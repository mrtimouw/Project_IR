Cowboy Bebop

The series, which ran for twenty-six episodes (dubbed "sessions"), is set in the year 2071, and follows the lives of a traveling bounty-hunting crew aboard a spaceship, the "Bebop". Although it incorporates a wide variety of genres, the series draws most heavily from science fiction, Western, and noir films. Its most prominent themes are existential boredom, loneliness, and the inability to escape one's past.

The series was dubbed into English by Animaze and ZRO Limit Productions, and was originally licensed in North America by Bandai Entertainment (and is now licensed by Crunchyroll) and in Britain by Beez Entertainment (now by Anime Limited); Madman Entertainment owns the license in Australia and New Zealand. In 2001, it became the first anime title to be broadcast on Adult Swim.

"Cowboy Bebop" has been hailed as one of the best animated television series of all time. It was a critical and commercial success both in Japanese and international markets, most notably in the United States. It garnered several major anime and science-fiction awards upon its release, and received acclaim from critics and audiences for its style, characters, story, voice acting, animation, and soundtrack. The English dub was particularly lauded and is regarded as one of the best anime English dubs. Credited with helping to introduce anime to a new wave of Western viewers in the early 2000s, "Cowboy Bebop" has also been called a gateway series for anime as a whole.

In the year 2071, roughly fifty years after an accident with a hyperspace gateway that made Earth almost uninhabitable, humanity has colonized most of the rocky planets and moons of the Solar System. Amid a rising crime rate, the Inter Solar System Police (ISSP) set up a legalized contract system, in which registered bounty hunters (also referred to as "Cowboys") chase criminals and bring them in alive in return for a reward. The series' protagonists are bounty-hunters working from the spaceship "Bebop". The original crew is Spike Spiegel, an exiled former hitman of the criminal Red Dragon Syndicate, and Jet Black, a former ISSP officer. They are later joined by Faye Valentine, an amnesiac con artist; Edward, an eccentric child, skilled in hacking; and Ein, a genetically-engineered Pembroke Welsh Corgi with human-like intelligence. Throughout the series, the team gets involved in disastrous mishaps leaving them without money, while often confronting faces and events from their past: These include Jet's reasons for leaving the ISSP and Faye's past as a young woman from Earth injured in an accident and cryogenically frozen to save her life.

While much of the show is episodic, the main story arc focuses on Spike and his deadly rivalry with Vicious, an ambitious criminal affiliated with the Red Dragon Syndicate. Spike and Vicious were once partners and friends. Still, when Spike begins an affair with Vicious's girlfriend Julia and resolves to leave the Syndicate with her, Vicious seeks to eliminate Spike by blackmailing Julia into killing him. Julia hides to protect herself and Spike, while Spike fakes his death to escape the Syndicate. In the present, Julia comes out of hiding and reunites with Spike, intending to complete their plan. Vicious, having staged a "coup d'état" and taken over the Syndicate, sends hitmen after the pair. Julia is killed, leaving Spike alone. Spike leaves the "Bebop" after finally apologizing to Faye and Jet. Upon infiltrating the syndicate, he finds Vicious on the top floor of the building and confronts him after dispatching the remaining Red Dragon members. The final battle ends with Spike killing Vicious, only to be seriously wounded himself in the ensuing confrontation. Looking up to the sky, Spike sees Julia. The series concludes as Spike descends the main staircase of the building into the rising sun before eventually falling to the ground.

Watanabe created a special tagline for the series to promote it during its original presentation, calling it "a new genre unto itself". The line was inserted before and after commercial breaks during its Japanese and US broadcasts. Later, Watanabe called the phrase an "exaggeration". The show is a hybrid of multiple genres, including westerns and pulp fiction. One reviewer described it as "space opera meets noir, meets comedy, meets cyberpunk". It has also been called a "genre-busting space Western".

The musical style was emphasized in many of the episode titles. Multiple philosophical themes are explored using the characters, including existentialism, existential boredom, loneliness, and the effect of the past on the protagonists. Other concepts referenced include environmentalism and capitalism. The series also makes specific references to or pastiches multiple films, including the works of John Woo and Bruce Lee, "Midnight Run", "", and "Alien". The series also includes extensive references and elements from science fiction, bearing strong similarities to the cyberpunk fiction of William Gibson. Several planets and space stations in the series are made in Earth's image. The streets of celestial objects such as Ganymede resemble a modern port city, while Mars features shopping malls, theme parks, casinos and cities. This setting has been described as "one part Chinese diaspora and two parts wild west".

The characters were created by Watanabe and character designed by Toshihiro Kawamoto. Watanabe envisioned each character as an extension of his own personality, or as an opposite person to himself. Each character, from the main cast to supporting characters, were designed to be outlaws unable to fit into society. Kawamoto designed the characters so they were easily distinguished from one another. All the main cast are characterized by a deep sense of loneliness or resignation to their fate and past. From the perspective of Brian Camp and Julie Davis, the main characters resemble the main characters of the anime series "Lupin III", if only superficially, given their more troubled pasts and more complex personalities.

The show focuses on the character of Spike Spiegel (voiced by Kōichi Yamadera), an iconic space cowboy with green hair and often seen wearing a blue suit, with the overall theme of the series being Spike's past and its karmic effect on him. Spike was portrayed as someone who had lost his expectations for the future, having lost the woman he loved, and so was in a near-constant lethargy. Spike's artificial eye was included as Watanabe wanted his characters to have flaws. He was originally going to be given an eyepatch, but this decision was vetoed by producers.

Jet (voiced by Unshō Ishizuka) is shown as someone who lost confidence in his former life and has become cynical about the state of society. Spike and Jet were designed to be opposites, with Spike being thin and wearing smart attire, while Jet was bulky and wore more casual clothing. The clothing, which was dark in color, also reflected their states of mind. Faye Valentine, Edward Wong (voiced by Aoi Tada), and Ein joined the crew in later episodes. Their designs were intended to contrast against Spike. Faye was described by her voice actress Megumi Hayashibara as initially being an "ugly" woman, with her defining traits being her liveliness, sensuality and humanity. To emphasize her situation when first introduced, she was compared to Poker Alice, a famous Western figure.

Edward and Ein were the only main characters to have real-life models. The former had her behavior based on the antics of Yoko Kanno as observed by Watanabe when he first met her. While generally portrayed as carefree and eccentric, Edward is motivated by a sense of loneliness after being abandoned by her father. Kawamoto initially based Ein's design on a friend's pet corgi, later getting one himself to use as a motion model.

"Cowboy Bebop" was developed by animation studio Sunrise and created by Hajime Yatate, the well-known pseudonym for the collective contributions of Sunrise's animation staff. The leader of the series' creative team was director Shinichirō Watanabe, most notable at the time for directing "Macross Plus" and "". Other leading members of Sunrise's creative team were screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical art designer Kimitoshi Yamane, composer Yoko Kanno, and producers Masahiko Minami and Yoshiyuki Takei. Most of them had previously worked together, in addition to having credits on other popular anime titles. Nobumoto had scripted "Macross Plus", Kawamoto had designed the characters for "Gundam", and Kanno had composed the music for "Macross Plus" and "The Vision of Escaflowne". Yamane had not worked with Watanabe yet, but his credits in anime included "Bubblegum Crisis" and "The Vision of Escaflowne". Minami joined the project as he wanted to do something different from his previous work on mecha anime.

"Cowboy Bebop" was Watanabe's first project as solo director, as he had been co-director in his previous works. His original concept was for a movie, and during production he treated each episode as a miniature movie. His main inspiration for "Cowboy Bebop" was of the anime "Lupin III", a crime drama focusing on the exploits of the series' titular character. When developing the series' story, Watanabe began by creating the characters first. He explained, "the first image that occurred to me was one of Spike, and from there I tried to build a story around him, trying to make him cool." While the original dialogue of the series was kept clean to avoid any profanities, its level of sophistication was made appropriate to adults in a criminal environment. Watanabe described "Cowboy Bebop" as "80% serious story and 20% humorous touch". The comical episodes were harder for the team to write than the serious ones, and though several events in them seemed random, they were carefully planned in advance. Watanabe conceived the series' ending early on, and each episode involving Spike and Vicious was meant to foreshadow their final confrontation. Some of the staff were unhappy about this approach as a continuation of the series would be difficult. While he considered altering the ending, he eventually settled with his original idea. The reason for creating the ending was that Watanabe did not want the series to become like "Star Trek", with him being tied to doing it for years.

The project had initially originated with Bandai's toy division as a sponsor, with the goal of selling spacecraft toys. Watanabe recalled his only instruction was "So long as there's a spaceship in it, you can do whatever you want." But upon viewing early footage, it became clear that Watanabe's vision for the series did not match Bandai's. Believing the series would never sell toy merchandise, Bandai pulled out of the project, leaving it in development hell until sister company Bandai Visual stepped in to sponsor it. Since there was no need to merchandise toys with the property any more, Watanabe had free rein in the development of the series. Watanabe wanted to design not just a space adventure series for adolescent boys but a program that would also appeal to sophisticated adults. During the making of "Bebop", Watanabe often attempted to rally the animation staff by telling them that the show would be something memorable up to three decades later. While some of them were doubtful of that at the time, Watanabe many years later expressed his happiness to have been proven right in retrospect. He joked that if Bandai Visual had not intervened then "you might be seeing me working the supermarket checkout counter right now."

The city locations were generally inspired by the cities of New York and Hong Kong. The atmospheres of the planets and the ethnic groups in "Cowboy Bebop" mostly originated from Watanabe's ideas, with some collaboration from set designers Isamu Imakake, Shoji Kawamori, and Dai Satō. The animation staff established the particular planet atmospheres early in the production of the series before working on the ethnic groups. It was Watanabe who wanted to have several groups of ethnic diversity appear in the series. Mars was the planet most often used in "Cowboy Bebop" storylines, with Satoshi Toba, the cultural and setting producer, explaining that the other planets "were unexpectedly difficult to use". He stated that each planet in the series had unique features, and the producers had to take into account the characteristics of each planet in the story. For the final episode, Toba explained that it was not possible for the staff to have the dramatic rooftop scene occur on Venus, so the staff "ended up normally falling back to Mars". In creating the backstory, Watanabe envisioned a world that was "multinational rather than stateless". In spite of certain American influences in the series, he stipulated that the country had been destroyed decades prior to the story, later saying the notion of the United States as the center of the world repelled him.

The music for "Cowboy Bebop" was composed by Yoko Kanno. Kanno formed the blues and jazz band Seatbelts to perform the series' music. According to Kanno, the music was one of the first aspects of the series to begin production, before most of the characters, story, or animation had been finalized. The genres she used for its composition were western, opera, and jazz. Watanabe noted that Kanno did not score the music exactly the way he told her to. He stated, "She gets inspired on her own, follows up on her own imagery, and comes to me saying 'this is the song we need for "Cowboy Bebop", and composes something completely on her own." Kanno herself was sometimes surprised at how pieces of her music were used in scenes, sometimes wishing it had been used elsewhere, though she also felt that none of their uses were "inappropriate". She was pleased with the working environment, finding the team very relaxed in comparison with other teams she had worked with.

Watanabe further explained that he would take inspiration from Kanno's music after listening to it and create new scenes for the story from it. These new scenes in turn would inspire Kanno and give her new ideas for the music and she would come to Watanabe with even more music. Watanabe cited as an example, "some songs in the second half of the series, we didn't even ask her for those songs, she just made them and brought them to us." He commented that while Kanno's method was normally "unforgivable and unacceptable", it was ultimately a "big hit" with "Cowboy Bebop". Watanabe described his collaboration with Kanno as "a game of catch between the two of us in developing the music and creating the TV series "Cowboy Bebop"". Since the series' broadcast, Kanno and the Seatbelts have released seven original soundtrack albums, two singles and extended plays, and two compilations through label Victor Entertainment.

The guns on the show were chosen by the director, Watanabe, and in discussion with set designer, Isamu Imakake, and mechanical designer, Kimitoshi Yamane. Setting producer, Satoshi Toba said, "They talked about how they didn't want common guns, because that wouldn't be very interesting, and so they decided on these guns."

"Cowboy Bebop" debuted on TV Tokyo, one of the main broadcasters of anime in Japan, airing from April 3 until June 26, 1998. Due to its 6:00 p.m. timeslot and depictions of graphic violence, the show's first run only included episodes 2, 3, 7 to 15, 18 and a special. Later that year, the series was shown in its entirety from October 24 until April 24, 1999, on satellite network Wowow. The full series has also been broadcast across Japan by anime television network Animax, which has also aired the series via its respective networks across Southeast Asia, South Asia and East Asia.

The first non-Asian country to air "Cowboy Bebop" was Italy. There, it was first aired on October 21, 1999, on MTV, where it inaugurated the 9:00–10:30 p.m. "Anime Night" programming block.

In the United States, "Cowboy Bebop" was one of the programs shown when Cartoon Network's late night block Adult Swim debuted on September 2, 2001, being the first anime shown on the block that night at midnight ET. During its original run on Adult Swim, episodes 6, 8, and 22 were skipped due to their violent themes in wake of the September 11 attacks. By the third run of the series, all these episodes had premiered for the first time. "Cowboy Bebop" was successful enough to be broadcast repeatedly for four years. It has been run at least once every year since 2007, and HD remasters of the show began broadcasting in 2015. In the United Kingdom, it was first broadcast in 2002 on the adult-oriented channel CNX. From November 6, 2007, it was repeated on AnimeCentral until the channel's closure in August 2008. In Australia, "Cowboy Bebop" was first broadcast on pay television in 2002 on Adult Swim, and on free-to-air-TV on ABC2 (the national digital public television channel) on January 2, 2007. It has been repeated several times, most recently starting in 2008. "Cowboy Bebop: The Movie" also aired on February 23, 2009, on SBS (a hybrid-funded Australian public broadcasting television network). In Canada, "Cowboy Bebop" was first broadcast on December 24, 2006, on Razer.

In Latin America, the series was first broadcast on pay-TV in 2001 on Locomotion. It aired again on January 9, 2016 on I.Sat.

"Cowboy Bebop" has been released in four separate editions in North America.

The first release was sold in VHS format either as a box set or as seven individual tapes. The tapes were sold through Anime Village, a division of Bandai.

In the late 1990s, Manga Entertainment purchased the rights to "Cowboy Bebop" with plans to release the English-dubbed PAL version on VHS; however, this was never realized.

The second release was sold in 2000 individually, and featured uncut versions of the original 26 episodes. In 2001, these DVDs were collected in the special edition "Perfect Sessions" which included the first 6 DVDs, the first "Cowboy Bebop" soundtrack, and a collector's box. At the time of release, the art box from the Perfect Sessions was made available for purchase on The Right Stuff International as a solo item for collectors who already owned the series.

The third release, "The Best Sessions", was sold in 2002 and featured what Bandai considered to be the best 6 episodes of the series remastered in Dolby Digital 5.1 and DTS surround sound.

The fourth release, "Cowboy Bebop Remix", was also distributed on 6 discs and included the original 26 uncut episodes, with sound remastered in Dolby Digital 5.1 and video remastered under the supervision of Shinichiro Watanabe. This release also included various extras that were not present in the original release. "Cowboy Bebop Remix" was itself collected as the "Cowboy Bebop Remix: The Complete Collection" in 2008.

In December 2012, newly founded distributor Anime Limited announced via Facebook and Twitter that they had acquired the home video license for the United Kingdom. Part 1 of the Blu-ray collection was released on July 29, 2013, while Part 2 was released on October 14. The standard DVD Complete Collection was originally meant to be released on September 23, 2013 with Part 2 of the Blu-ray release but due to mastering and manufacturing errors, the Complete Collection was delayed until November 27. Following the closure of Bandai Entertainment in 2012, Funimation and Sunrise had announced that they rescued "Cowboy Bebop", along with a handful of other former Bandai Entertainment properties, for home video and digital release. Funimation released the series on Blu-ray and DVD on December 16, 2014. The series was released in four separate editions: standard DVD, standard Blu-ray, an Amazon.com exclusive Blu-ray/DVD combo, and a Funimation.com exclusive Blu-ray/DVD combo.

Crunchyroll released a limited edition Blu-ray box set on April 4, 2023 for its 25th anniversary.

Netflix acquired the streaming rights to the original anime, with all 26 episodes available worldwide as of October 21, 2021. The series is also available on Hulu and Funimation in the United States. On March 1, 2022, the anime became available on Crunchyroll to consolidate both Funimation and Wakanim into the service.

Two "Cowboy Bebop" manga series adaptations have been released, both published by Kadokawa Shoten and serialized in "Asuka Fantasy DX".<ref name="DX 10/1997"></ref><ref name="DX 11/1998"></ref> The first manga series, titled "Cowboy Bebop: Shooting Star" and illustrated by Cain Kuga, was serialized from October issue 1997, before the anime series' release, to July issue 1998. It was collected into two volumes in 1998, the first one in May and the second one in September. The second manga series, simply titled "Cowboy Bebop" and illustrated by , was serialized from November issue 1998 to March issue 2000. It was collected into three volumes, the first two in April and October 1999 and the third one in April 2000. Both manga series were licensed by Tokyopop for release in North America.

A "Cowboy Bebop" video game, developed and published by Bandai, was released in Japan for the PlayStation on May 14, 1998. A PlayStation 2 video game, "", was released in Japan on August 25, 2005, and an English version had been set for release in North America. However, in January 2007, IGN reported that the release had likely been cancelled, speculating that it did not survive Bandai's merger with Namco to Bandai Namco Games.
In 2022, "Cowboy Bebop" made its debut in the Bandai Namco crossover game "Super Robot Wars T", which is traditionally focused on turn based mecha combat.

An anime film titled known in English as "Cowboy Bebop: The Movie", was released in Japan in September 2001 and in the United States in August 2002.

On July 22, 2008, "If" published an article on its website regarding a rumor of a live-action "Cowboy Bebop" movie in development by 20th Century Fox. Producer Erwin Stoff said that the film's development was in the early stages, and that they had "just signed it". Keanu Reeves was to play the role of Spike Spiegel. "Variety" confirmed on January 15, 2009 that production company Sunrise Animation would be "closely involved with the development of the English-language project". The site also confirmed Kenji Uchida, Shinichirō Watanabe and series writer Keiko Nobumoto as associate producers, series producer Masahiko Minami as a production consultant, and Peter Craig as screenwriter. This was lauded by various sources as a promising move for the potential quality of the film. At the time it was slated to release in 2011, but problems with the budget delayed its production. The submitted script was sent back for rewrite to reduce the cost and little has been heard about it since an interview with producer Joshua Long on October 15, 2010; the project currently languishes in development hell. On October 25, 2014, series director Watanabe was asked about the live-action film at the MCM London Comicon. He stated: "I'm afraid I don't know what they're thinking in Hollywood. Apparently the project hasn't come to a stop but I don't know how it's going to progress from here on. I hear that there are a lot of 'Hollywood' problems."

In 2017, it was announced that an American live-action adaptation of the series was being developed by Tomorrow Studios, a partnership between Marty Adelstein and ITV Studios, with executive production by Sunrise Inc. Christopher Yost was to write the series, and Netflix announced that it would distribute it. On April 4, 2019, Variety reported that John Cho, Mustafa Shakir, Daniella Pineda and Alex Hassell had been cast. Production was shut down in October 2019 due to a knee injury sustained by Cho, setting production back by more than six months. On April 17, 2020, it was revealed that the episodes would be an hour long. On May 19, 2020, Adelstein revealed that there were three finished episodes and that they had shot at least six episodes before Cho's knee injury. In the same interview it was revealed that the director of the anime series, Shinichirō Watanabe, had been hired as a creative consultant. Production in New Zealand resumed on September 30, 2020, following a COVID-19 lockdown in the country. The series was released on November 19, 2021 to mixed reviews. On December 9, 2021, it was announced that it would not be renewed for a second season, with Netflix cancelling it entirely.

An official side story titled "Cowboy Bebop: UT" tells the story of Ural and Victoria Terpsichore (V.T. from the session "Heavy Metal Queen") when they were bounty-hunters. The story was available in its own official site, however the website was closed and is currently available at the site mirror.

A deck-building board game, "Cowboy Bebop: Space Serenade", was released in 2019.

"Cowboy Bebop" received unanimous acclaim, beginning at the time of its initial broadcast. Beginning in 1998, Japanese critic Keith Rhee highlighted the series as a standout in an otherwise "run-of-the-mill" season, praising its overall production values, and singling out Kanno's soundtrack as "a much-welcome change from all the sugary J-pop tunes of most anime features". Rhee also highlighted the show's Japanese "all-star cast", which his colleague Mark L. Johnson described as being filled with "veteran voice talent", turning in even greater performances than those of their "above average" US counterparts. In 1999, Australian magazine "Hyper" reviewed the anime and rated it 9.5 out of 10.

"Anime News Network"s Mike Crandol gave the series an 'A+' rating for the dubbed version, and an 'A' rating for the subbed version. He characterized the series as "one of the most popular and respected anime titles in history", before adding that it was "a unique television show which skillfully transcends all kinds of genres". Crandol praised its characters as "some of the most endearing characters to ever grace an anime", and commended the voice acting, especially the "flawless English cast". He also complimented the series' "movie-quality" animation, "sophisticated" writing, and its "incredible" musical score. Crandol hailed "Cowboy Bebop" as a "landmark" anime "that will be remembered long after many others have been forgotten", and went on to call it "one of the greatest anime titles ever". Additionally, Michael Toole of "Anime News Network" named "Cowboy Bebop" as one of the most important anime of the 1990s.

T.H.E.M. Anime Reviews gave the entire series a perfect score of 5 out of 5 stars, with reviewer Christina Carpenter believing "Cowboy Bebop" as "one of the best [anime]" and touting it as a masterpiece that "puts most anime...and Hollywood, to shame". She described it as a "very stylish, beautifully crafted series that deserves much more attention than it gets". Carpenter praised the animation as "a rarity and a marvel to behold" and that it was "beyond superb", and the plot and characterization as having "a sophistication and subtlety that is practically one-of-a-kind". She also praised the soundtrack, and hailed the opening theme as one of the best intro pieces she had ever heard. Carpenter went to say that "Bebop" was a "must-have for any serious collector of Japanese animation".

In his article "Asteroid Blues: The Lasting Legacy of "Cowboy Bebop"", "The Atlantic" writer Alex Suskind states, "On paper, "Cowboy Bebop", the legendary cult anime series from Shinichirō Watanabe, reads like something John Wayne, Elmore Leonard, and Philip K. Dick came up with during a wild, all-night whiskey bender." He goes on to write, "The response from critics and fans may have sounded hyperbolic—the word 'masterpiece' was thrown around a great deal—but the praise was justified. First-time solo director Watanabe had created a gorgeous tale of morality, romance, and violence–a dark look at the lives of outlaws that's shot like an independent film."

In January 2015, television writer Kyle Mills of "DVD Talk" awarded the series five stars upon review. He stated, "Regardless of the medium, be it live action television, film, or animation, "Cowboy Bebop" is simply one of the finest examples of storytelling ever created." In his review, he describes the finale as "one of the best in television history", referring to it as a "widely revered" ending that "still sparks fan conversation, resonating with viewers 15 years on". He closes by writing, ""Cowboy Bebop" ends with a bang."

In his 2018 review of the series, "Paste" critic John Maher wrote, "It feels like a "magnum opus" produced at the pinnacle of a long career despite being, almost unbelievably, Watanabe's first series as a director. It is a masterwork that should justly rank among the best works of television of all time." It was also placed at #1 on the publication's list of the "50 Best Anime Series of All Time".

On review aggregator Rotten Tomatoes, the series has an approval rating of 100% based on 23 reviews, with an average rating of 8/10. The website's critical consensus reads, "Blending a head-spinning array of genres and references, "Cowboy Bebop" is an anime television classic that must be experienced."

In an April 2019 interview with Diego Molano, creator of "Victor & Valentino", he said that Cowboy Bebop was the first anime he "obsessed over", as he spent time tracking down VHS tapes of the show in high school. He also argued that this series showed him "how cinematic and emotional animation can be".

In the 1999 Anime Grand Prix awards for the anime of 1998, "Cowboy Bebop" won two first place awards: Spike Spiegel was awarded the best male character; and Megumi Hayashibara was awarded the best voice actor for her role as Faye Valentine. "Cowboy Bebop" also received rankings in other categories: the series itself was awarded the second best anime series; Faye Valentine and Ed were ranked the fifth and ninth best female characters respectively; "Tank!" and "The Real Folk Blues" were ranked the third and fifteenth best songs respectively; and "Ballad of Fallen Angels", "Speak Like a Child", "Jamming with Edward" and "Mish-Mash Blues" were ranked the second, eighth, eighteenth and 20th best episodes respectively.

In the 2000 Anime Grand Prix awards for the anime of 1999, "Cowboy Bebop" won the same two first place awards again: best male character for Spike Spiegel; and best voice actor for Megumi Hayashibara. Other rankings the series received are: second best anime series; sixth best female character for Faye Valentine; seventh and twelfth best song for "Tank!" and "Blue" respectively; and third and seventeenth best episode for "The Real Folk Blues (Part 2)" and "Hard Luck Woman" respectively. In the 2000 Seiun Awards, Cowboy Bebop was awarded for Best Media of the Year.

A 2004 poll in "Newtype USA", the US edition of the Japanese magazine "Newtype", asked its readers to vote the "Top 25 Anime Titles of All Time"; "Cowboy Bebop" ranked second on the list (after "Neon Genesis Evangelion"), placing it as one of the most socially relevant and influential anime series ever created. During that same year, "Cinefantastique" listed the anime as one of the "10 Essential Animations", citing the series' "gleeful mix of noir-style, culture-hopping inclusiveness and music". In 2007, the American Anime magazine "Anime Insider" listed the "50 Best Anime Ever" by compiling lists of industry regulars and magazine staff, and ranked "Cowboy Bebop" as the #1 anime of all time. In 2012, Madman Entertainment compiled the votes of fans online for "The Top 20 Madman Anime Titles" and ranked "Cowboy Bebop" at seventh.

"Cowboy Bebop" has been featured in several lists published by "IGN". In the 2009 "Top 100 Animated TV Series" list, "Cowboy Bebop", labelled as "a very original – and arguably one of the best – anime", was placed fourteenth, making it the second highest ranking anime on the list (after "Evangelion") and one of the most influential series of the 1990s. In 2011, "Bebop" was ranked 29th in the "Top 50 Sci-Fi TV Shows" list, once again being the second highest ranking anime on the list (after "Evangelion"). In 2006, "Cowboy Bebop"s soundtrack was ranked first in "Top Ten Anime Themes and Soundtracks of All-Time" list, with the series being commented as "one of the best anime ever and certainly is tops when it comes to music." Spike Spiegel was ranked fourth place in the "Top 25 Anime Characters of All Time" article. IGN Movies also placed "Cowboy Bebop" in their list of "10 Cartoon Adaptations We'd Like to See".

The series has been subject to study and analysis since its debut, with the main focus being on its style and mixture of genres. Miguel Douglas, describing the series style in a review, said that "the series distinctly establishes itself outside the realm of conventional Japanese animation and instead chooses to forge its own path. With a setting within the realm of science fiction, the series wisely offers a world that seems entirely realistic considering our present time. Free from many of the elements that accompany science fiction in general—whether that be space aliens, giant robots, or laser guns—the series delegates itself towards presenting a world that is quite similar to our own albeit showcasing some technological advances. Certainly not as pristine a future we would see in other series or films, "Cowboy Bebop" decides to deliver a future that closely reflects that of our own time. This aspect of familiarity does wonders in terms of relating to the viewer, and it presents a world that certainly resembles our very own." Daryl Surat of "Otaku USA", commenting on the series' appeal, said that it was "that rare breed of science-fiction: 'accessible'. Unlike many anime titles, viewers weren't expected to have knowledge of Japanese culture—character names, signs, and the like were primarily in English to begin with—or have seen any other anime series prior." Michelle Onley Pirkle, in her book "Science Fiction Film, Television, and Adaptation: Across the Screens", said that ""Cowboy Bebop" is taking a new take on genre, not by creating unique images and sounds, but by playing 'freely' with, 'remixing', or adapting the images and sounds of other familiar genres in a dynamic way." Robert Baigent, writing for the "Graduate Journal of Asia-Pacific Studies", said that the series' appeal likely stemmed from the trend in anime to emulate Western fiction.

In March 2009, the print and web editions of "The Onion"'s "The A.V. Club" called "Cowboy Bebop" "rightly a huge hit", and listed it as a gateway series to understanding the medium of anime as a whole. Suskind said: "It was unlike anything the genre had seen before. It even approached its music differently. The show kicked off with a wormhole of a theme song, and the soundtrack moves so seamlessly through genres, from rock to country to pop to jazz to funk, it's shocking to learn that one set of musicians is behind it all". In an interview, producer Sean Akins also states that the series "created a whole new world". "It's hard for me to quantify the impact that I think it has had. It changed anime. I think people began to think about what shows would be cool. I think it redefined cool within animation, not only in Japan but in the States". One of the series' main animators, Tensai Okamura, went on to create his own anime in 2007: "Darker than Black". Okamura used his experience from "Cowboy Bebop" to write the screenplay of "Darker than Black", leading to narratives composed of two episodes similar to Japanese dramas.

American film director, screenwriter, and producer Rian Johnson has cited "Cowboy Bebop" as a visual influence on his films, most notably "Brick". "Ender's Game" writer Orson Scott Card also praised the series. He states that the series is "better than most sci-fi films out there". He goes on to say that he "found this series brilliant, but what held me was a combination of strong relationship-based storytelling, a moody visual style that never got old and really smart dialogue".

After the creation of the series, an interviewer asked Watanabe if he had any plans to create more "Cowboy Bebop" material. Watanabe responded by saying that he does not believe that he "should just keep on making "Cowboy Bebop" sequels for the sake of it". Watanabe added that ending production and "to quit while we're ahead when people still want more" is more "in keeping with the "Bebop" spirit". In a more recent interview from 2006 with "The Daily Texan", Watanabe was asked if there would ever be more "Cowboy Bebop". Watanabe's answer was "someday...maybe, someday".

In May 2020, composer Mason Lieberman, who has never actually seen Cowboy Bebop, partnered with Sunrise and Funimation to produce an official "Cowboy Bebop" charity track for COVID-19 relief. This track was released on vinyl and featured the return of original series composer Yōko Kanno, original recording band The Seatbelts, and a collection of forty other special musical guests.



Clement of Alexandria

Titus Flavius Clemens, also known as Clement of Alexandria (; – ), was a Christian theologian and philosopher who taught at the Catechetical School of Alexandria. Among his pupils were Origen and Alexander of Jerusalem. A convert to Christianity, he was an educated man who was familiar with classical Greek philosophy and literature. As his three major works demonstrate, Clement was influenced by Hellenistic philosophy to a greater extent than any other Christian thinker of his time, and in particular, by Plato and the Stoics. His secret works, which exist only in fragments, suggest that he was familiar with pre-Christian Jewish esotericism and Gnosticism as well. In one of his works he argued that Greek philosophy had its origin among non-Greeks, claiming that both Plato and Pythagoras were taught by Egyptian scholars. 

Clement is usually regarded as a Church Father. He is venerated as a saint in Coptic Christianity, Eastern Catholicism, Ethiopian Christianity, and Anglicanism. He was revered in Western Catholicism until 1586, when his name was removed from the Roman Martyrology by Pope Sixtus V on the advice of Baronius. The Eastern Orthodox Church officially stopped any veneration of Clement of Alexandria in the 10th century. Nonetheless, he is still often referred to as "Saint Clement of Alexandria" by both Eastern Orthodox and Roman Catholic authors.

Neither Clement's birthdate or birthplace is known with any degree of certainty. It is speculated that he was born sometime around 150 AD. According to Epiphanius of Salamis, he was born in Athens, but there is also a tradition of an Alexandrian birth.

His parents were pagans and Clement was a convert to Christianity. In the "Protrepticus" he displays an extensive knowledge of Greek religion and mystery religions, which could have arisen only from the practice of his family's religion.

Having rejected paganism as a young man due to its perceived moral corruption, he travelled in Greece, Asia Minor, Palestine, and Egypt. Clement's journeys were primarily a religious undertaking. In Greece, he encountered an Ionian theologian, who has been identified as Athenagoras of Athens; while in the east, he was taught by an Assyrian, sometimes identified with Tatian, and a Jew, possibly Theophilus of Caesarea.

In around 180 AD, Clement reached Alexandria, where he met Pantaenus, who taught at the Catechetical School of Alexandria. Eusebius suggests that Pantaenus was the head of the school, but controversy exists about whether the institutions of the school were formalized in this way before the time of Origen. Clement studied under Pantaenus, and was ordained to the priesthood by Pope Julian before 189. Otherwise, virtually nothing is known of Clement's personal life in Alexandria. He may have been married, a conjecture supported by his writings.

During the Severian persecution of 202–203, Clement left Alexandria. In 211, Alexander of Jerusalem wrote a letter commending him to the Church of Antioch, which may imply that Clement was living in Cappadocia or Jerusalem at that time. He died at an unknown location.

Three of Clement's major works have survived in full and they are collectively referred to as a trilogy:

The "Protrepticus" (: "Exhortation to the Greeks") is, as its title suggests, an exhortation to the pagans of Greece to adopt Christianity. Within it, Clement demonstrates his extensive knowledge of pagan mythology and theology. It is chiefly important due to Clement's exposition of religion as an anthropological phenomenon. After a short philosophical discussion, it opens with a history of Greek religion in seven stages. Clement suggests that at first, humans mistakenly believed the Sun, the Moon, and other heavenly bodies to be deities. The next developmental stage was the worship of the products of agriculture, from which he contends the cults of Demeter and Dionysus arose. Humans then paid reverence to revenge and deified human feelings of love and fear, among others. In the following stage, the poets Hesiod and Homer attempt to enumerate the deities; Hesiod's Theogony giving the number of twelve. Finally, humans reached a stage when they proclaimed others, such as Asclepius and Heracles, as deities. Discussing idolatry, Clement contends that the objects of primitive religion were unshaped wood and stone, and idols thus arose when such natural items were carved. Following Plato, Clement is critical of all forms of visual art, suggesting that artworks are but illusions and "deadly toys".

Clement criticizes Greek paganism in the "Protrepticus" on the basis that its deities are both false and poor moral examples. He attacks the mystery religions for their ritualism and mysticism. In particular, the worshippers of Dionysus are ridiculed by him for their family-based rituals (such as the use of children's toys in ceremony). He suggests at some points that the pagan deities are based on humans, but at other times he suggests that they are misanthropic demons, and he cites several classical sources in support of this second hypothesis. Clement, like many pre-Nicene church fathers, writes favourably about Euhemerus and other rationalist philosophers, on the grounds that they at least saw the flaws in paganism. However, his greatest praise is reserved for Plato, whose apophatic views of God prefigure Christianity.

The figure of Orpheus is prominent throughout the Protrepticus narrative, and Clement contrasts the song of Orpheus, representing pagan superstition, with the divine Logos of Christ. According to Clement, through conversion to Christianity alone can one fully participate in the Logos, which is universal truth.
The title of "Paedagogus", translatable as "tutor", refers to Christ as the teacher of all humans, and it features an extended metaphor of Christians as children. It is not simply instructional: Clement intends to show how the Christian should respond to the Love of God authentically. Following Plato (Republic 4:441), he divides life into three elements: character, actions, and passions. The first having been dealt with in the "Protrepticus", he devotes the "Paedagogus" to reflections on Christ's role in teaching humans to act morally and to control their passions. Despite its explicitly Christian nature, Clement's work draws on Stoic philosophy and pagan literature; Homer, alone, is cited more than sixty times in the work.

Although Christ, like a human, is made in the image of God, he alone shares the likeness of God the Father. Christ is both sinless and apathetic, and thus by striving to imitate Christ, one can achieve salvation. To Clement, sin is involuntary, and thus irrational (), removed only through the wisdom of the Logos. God's guidance away from sin is thus a manifestation of God's universal love for mankind. The word play on and is characteristic of Clement's writing, and may be rooted in the Epicurean belief that relationships between words are deeply reflective of relationships between the objects they signify.

Clement argues for the equality of sexes, on the grounds that salvation is extended to all humans equally. Unusually, he suggests that Christ is neither female nor male, and that God the Father has both female and male aspects: the eucharist is described as milk from the breast (Christ) of the Father. Clement is supportive of women playing an active role in the leadership of the church and he provides a list of women he considers inspirational, which includes both Biblical and Classical Greek figures. It has been suggested that Clement's progressive views on gender as set out in the "Paedagogus" were influenced by Gnosticism, however, later in the work, he argues against the Gnostics that faith, not esoteric knowledge (), is required for salvation. According to Clement, it is through faith in Christ that one is enlightened and comes to know God.

In the second book, Clement provides practical rules on living a Christian life. He argues against overindulgence in food and in favour of good table manners. While prohibiting drunkenness, he promotes the drinking of alcohol in moderation following 1 Timothy 5:23. Clement argues for a simple way of life in accordance with the innate simplicity of Christian monotheism. He condemns elaborate and expensive furnishings and clothing, and argues against overly passionate music and perfumes, but Clement does not believe in the abandonment of worldly pleasures and argues that the Christian should be able to express joy in God's creation through gaiety and partying. He opposes the wearing of garlands, because the picking of the flowers ultimately kills a beautiful creation of God, and the garland resembles the crown of thorns.

Clement treats sex at some length. He argues that both promiscuity and sexual abstinence are unnatural, and that the main goal of human sexuality is procreation. He argues that adultery, coitus with pregnant women, concubinage, homosexuality, and prostitution all should be avoided as they will not contribute toward the generation of legitimate offspring.

In his third book, Clement continues along a similar vein, condemning cosmetics on the grounds that it is one's soul, not the body, one should seek to beautify. Clement also opposes the dyeing of men's hair and male depilation as being effeminate. He advises choosing one's company carefully, to avoid being corrupted by immoral people, and while arguing that material wealth is no sin in itself, it is too likely to distract one from the infinitely more important spiritual wealth that is found in Christ. The work finishes with selections of scripture supporting Clement's argument, and following a prayer, the lyrics of a hymn.

The contents of the "Stromata", as its title suggests, are miscellaneous. Its place in the trilogy is disputed – Clement initially intended to write the "Didasculus", a work that would complement the practical guidance of the "Paedagogus" with a more intellectual schooling in theology. The "Stromata" is less systematic and ordered than Clement's other works, and it has been theorized by André Méhat that it was intended for a limited, esoteric readership. Although Eusebius wrote of the eight books of the work, only seven undoubtedly survive. Photius, writing in the 9th century, found various text appended to manuscripts of the seven canonical books, which led Daniel Heinsius to suggest that the original eighth book is lost, and he identified the text purported to be from the eighth book as fragments of the "Hypotyposes".

The first book starts on the topic of Greek philosophy. Consistent with his other writing, Clement affirms that philosophy had a propaedeutic role for the Greeks, similar to the function of the law for the Jews. He then embarks on a discussion of the origins of Greek culture and technology, arguing that most of the important figures in the Greek world were foreigners, and (erroneously) that Jewish culture was the most significant influence on Greece. In an attempt to demonstrate the primacy of Moses, Clement gives an extended chronology of the world, wherein he dates the birth of Christ to 25 April or May, 4–2 BC, and the creation of the world to 5592 BC. The books ends with a discussion on the origin of languages and the possibility of a Jewish influence on Plato.

The second book is largely devoted to the respective roles of faith and philosophical argument. Clement contends that while both are important, the fear of God is foremost, because through faith one receives divine wisdom. To Clement, scripture is an innately true primitive philosophy that is complemented by human reason through the Logos. Faith is voluntary, and the decision to believe is a crucial fundamental step in becoming closer to God. It is never irrational, as it is founded on the knowledge of the truth of the Logos, but all knowledge proceeds from faith, as first principles are unprovable outside a systematic structure.

The third book covers asceticism. He discusses marriage, which is treated similarly in the "Paedagogus". Clement rejects the Gnostic opposition to marriage, arguing that only men who are uninterested in women should remain celibate, and that sex is a positive good if performed within marriage for the purposes of procreation. He argues that this has not always been so: the Fall occurred because Adam and Eve succumbed to their desire for each other, and copulated before the allotted time. He argues against the idea that Christians should reject their family for an ascetic life, which stems from Luke, contending that Jesus would not have contradicted the precept to "Honour thy Father and thy Mother", one of the Ten Commandments. Clement concludes that asceticism will only be rewarded if the motivation is Christian in nature, and thus the asceticism of non-Christians such as the gymnosophists is pointless.

Clement begins the fourth book with a belated explanation of the disorganized nature of the work, and gives a brief description of his aims for the remaining three or four books. The fourth book focuses on martyrdom. While all good Christians should be unafraid of death, Clement condemns those who actively seek out a martyr's death, arguing that they do not have sufficient respect for God's gift of life. He is ambivalent about whether any believing Christians can become martyrs by virtue of the manner of their death, or whether martyrdom is reserved for those who have lived exceptional lives. Marcionites cannot become martyrs, because they do not believe in the divinity of God the Father, so their sufferings are in vain. There is then a digression to the subject of theological epistemology. According to Clement, there is no way of empirically testing the existence of God the Father, because the Logos has revelatory, not analysable meaning, although Christ was an object of the senses. God had no beginning, and is the universal first principle.

The fifth book returns to the subject of faith. Clement argues that truth, justice, and goodness can be seen only by the mind, not the eye; faith is a way of accessing the unseeable. He stresses that knowledge of God can only be achieved through faith once one's moral faults have been corrected. This parallels Clement's earlier insistence that martyrdom can only be achieved by those who practice their faith in Christ through good deeds, not those who simply profess their faith. God transcends matter entirely, and thus the materialist cannot truly come to know God. Although Christ was God incarnate, it is spiritual, not physical comprehension of him that is important.

In the beginning of the sixth book, Clement intends to demonstrate that the works of Greek poets were derived from the prophetic books of the Bible. In order to reinforce his position that the Greeks were inclined toward plagiarism, he cites numerous instances of such inappropriate appropriation by classical Greek writers, reported second-hand from "On Plagiarism", an anonymous 3rd-century BC work sometimes ascribed to Aretades. Clement then digresses to the subject of sin and hell, arguing that Adam was not perfect when created, but given the potential to achieve perfection. He espouses broadly universalist doctrine, holding that Christ's promise of salvation is available to all, even those condemned to hell.

The final extant book begins with Clement arguing that Gnosticism is the true religion and states they should be an example of what a true Christian is, even saying they are "holy and pious" and "worships the true God in a manner worthy of him". Clement then gives a description of the nature of Christ, and that of the true Christian, who aims to be as similar as possible to both the Father and the Son. Clement then criticizes the simplistic anthropomorphism of most ancient religions, quoting Xenophanes' famous description of African, Thracian, and Egyptian deities. He indicates that the Greek deities may also have had their origins in the personification of material objects: Ares representing iron, and Dionysus wine. Prayer, and the relationship between love and knowledge are then discussed. Corinthians 13:8 seems to contradict the characterization of the true Christian as one who knows; but to Clement knowledge vanishes only in that it is subsumed by the universal love expressed by the Christian in reverence for the Creator. Following Socrates, he argues that vice arises from a state of ignorance, not from intention. The Christian is a "laborer in God's vineyard", responsible both for one's own path to salvation and that of one's neighbor. The work ends with an extended passage against the contemporary divisions and heresies within the church.

Besides the great trilogy, Clement's only other extant work is the treatise "Salvation for the Rich", also known as "Who is the Rich Man who is Saved?" written c. 203 AD Having begun with a scathing criticism of the corrupting effects of money and misguided servile attitudes toward the wealthy, Clement discusses the implications of Mark 10:25. The rich are either unconvinced by the promise of eternal life, or unaware of the conflict between the possession of material and spiritual wealth, and the good Christian has a duty to guide them toward a better life through the Gospel. Jesus' words are not to be taken literally — the supercelestial () meanings should be sought in which the true route to salvation is revealed. The holding of material wealth in itself is not a wrong, so long as it is used charitably, but Christians should be careful not to let their wealth dominate their spirit. It is more important to give up sinful passions than external wealth. If the rich are to be saved, all they must do is to follow the two commandments, and while material wealth is of no value to God, it can be used to alleviate the suffering of neighbors.

Other known works exist in fragments alone, including the four eschatological works in the secret tradition: "Hypotyposes", "Excerpta ex Theodoto", "Eclogae Propheticae", and the "Adumbraetiones". These cover Clement's celestial hierarchy, a complex schema in which the universe is headed by the Face of God, below which lie seven "protoctists", followed by archangels, angels, and humans. According to Jean Daniélou, this schema is inherited from a Judaeo-Christian esotericism, followed by the Apostles, which was only imparted orally to those Christians who could be trusted with such mysteries. The "proctocists" are the first beings created by God, and act as priests to the archangels. Clement identifies them both as the "Eyes of the Lord" and with the Thrones. Clement characterizes the celestial forms as entirely different from anything earthly, although he argues that members of each order only seem incorporeal to those of lower orders. According to the "Eclogae Propheticae", every thousand years every member of each order moves up a degree, and thus humans can become angels. Even the "protoctists" can be elevated, although their new position in the hierarchy is not clearly defined. The apparent contradiction between the fact that there can be only seven "protoctists" but also a vast number of archangels to be promoted to their order is problematical. One modern solution regards the story as an example of "interiorized apocalypticism": imagistic details are not to be taken literally, but as symbolizing interior transformation.

The titles of several lost works are known because of a list in Eusebius' "Ecclesiastical History", 6.13.1–3. They include the "Outlines", in eight books, and "Against Judaizers". Others are known only from mentions in Clement's own writings, including "On Marriage" and "On Prophecy", although few are attested by other writers and it is difficult to separate works that he intended to write from those that were completed.

The Mar Saba letter was attributed to Clement by Morton Smith, but there remains much debate today over whether it is an authentic letter from Clement, an ancient pseudepigraph, or a modern forgery. If authentic, its main significance would be in its relating that the Apostle Mark came to Alexandria from Rome and there, wrote a more spiritual Gospel, which he entrusted to the Church in Alexandria on his death; if genuine, the letter pushes back the tradition related by Eusebius connecting Mark with Alexandria by a century.

Eusebius, the fourth-century early church historian, is the first writer to provide an account of Clement's life and works, in his "Ecclesiastical History", 5.11.1–5, 6.6.1 He provides a list of Clement's works, biographical information, and an extended quotation from the "Stromata". From this and other accounts, it is evident that Clement was highly revered by his contemporaries and later patristic figures. As J.B. Mayor observes, “The piety and learning of Clement, his power as a teacher and philosopher, are spoken of in the highest terms by succeeding fathers.”

In the same work, Eusebius cites Alexander of Jerusalem (180-251) lauding “the holy Clement, who was both my master and benefactor,” describing him as one of the “blessed fathers who have trod the path before us,” while Eusebius himself is quoted as calling him “an incomparable master of Christian philosophy.” Jerome (342-420) calls Clement “the most learned of men,” recording that his writings are “full of eloquence and learning, both in sacred Scripture and in secular literature.” The aforementioned Alexander of Jerusalem is quoted by Jerome praising “the blessed presbyter Clement, a man illustrious and approved.” According to Theodoret (393-450), “he surpassed all others, and was a holy man.” Likewise, Cyril of Alexandria (376-444) says Clement was “a man admirably learned and skillful, and one that searched to the depths all the learning of the Greeks, with an exactness rarely attained before.” Maximus the Confessor (580-662) refers to him reverentially as “the great Clement.”

More recently, scholars have acknowledged Clement's primacy and importance in various respects. He has been called “the first Christian scholar” (Shelley), “the first systematic teacher of Christian doctrine” (Patrick), “the first great teacher of philosophical Christianity” (Hatch), “the first self-conscious theologian and ethicist” (Backhouse), “the first great Christian teacher in Alexandria” (Needham), “the founder of Christian philosophical theology” (Bray), “the true creator of ecclesiastical theology” (DeFaye), “the first major commentator on the Bible” (Bray), “the founder of Christian literature” (ANF), “the great founder of the Alexandrian School” (Coxe), a “pioneer of Christian scholarship” (ACCS), “an intellectual giant in the early church” (Kruger), “that man of genius who introduced Christianity to itself, as reflected in the burnished mirror of his intellect” (Coxe), and “the most inquisitive and independent spirit that has perhaps ever appeared in the Church” (DeFaye).

Stylistically, it has been noted that “his writings shine with a happy, peaceful, optimistic spirit; reading them can be a remarkably uplifting experience” (Needham). “He loves God’s creation and sees it as good; he gives us a warm, joyous picture of life; he is richly human, sane, and moderate” (Ferguson). Additionally, Clement's works “are a storehouse of curious ancient lore—a museum of the fossil remains of the beauties and monstrosities of the world of pagan antiquity, during all the epochs and phases of its history” (Wilson). “His prodigious erudition was unsurpassed even by that of Origen” (Cayre). “I do not know where we shall look for a purer or a truer man than this Clemens of Alexandria; he seems to me one of the old fathers whom we should all have reverenced most as a teacher, and loved best as a friend” (Maurice).

Nonetheless, there have been a few dissenting voices. Photios I of Constantinople writes polemically against Clement's theology in the "Bibliotheca", although he also is appreciative of Clement's learning and the literary merits of his work. In particular, he is highly critical of the "Hypotyposes", a work of biblical exegesis of which only a few fragments have survived. Photios compared Clement's treatise, which, like his other works, was highly syncretic, featuring ideas of Hellenistic, Jewish, and Gnostic origin, unfavorably against the prevailing orthodoxy of the 9th century. Amongst the particular ideas Photios deemed heretical were:
However, it is not clear that these are accurate representations of Clement's actual beliefs, since his extant writings appear to be mostly in line with what would come to be considered orthodox Christian theology. It has been suggested that Photios may have misunderstood Clement to be speaking for himself when he was often quoting from Gnostics and other sects without agreeing with their teachings.

As one of the earliest of the Church fathers whose works have survived, he is the subject of a significant amount of recent academic work, focusing on, among other things, his exegesis of scripture, his Logos-theology and pneumatology, his apparent belief in apokatastasis, the relationship between his thought and non-Christian philosophy, and his influence on Origen.
Up until the 17th century Clement was venerated as a saint in the Roman Catholic Church. His name was to be found in the martyrologies, and his feast fell on the fourth of December, but when the Roman Martyrology was revised by Pope Clement VIII his name was dropped from the calendar on the advice of Cardinal Baronius. Benedict XIV maintained this decision of his predecessor on the grounds that Clement's life was little known, that he had never obtained public cultus in the Church, and that some of his doctrines were, if not erroneous, at least suspect.

Although Clement is not widely venerated in Eastern Christianity, the Prologue of Ohrid repeatedly refers to him as a saint, as do various Orthodox authorities including the Greek Metropolitan Kallinikos of Edessa.

The Coptic tradition considers Clement a saint. Saint Clement Coptic Orthodox Christian Academy in Nashville, Tennessee, is specifically named after him. 

Clement is commemorated in Anglicanism. 

Clement taught that faith was the basis of salvation, however he also believed that faith was also the basis of "gnosis" which for him mean spiritual and mystical knowledge. Clement of Alexandria appropriated the word "gnosis" from what the Gnostics used, whom he opposed, but re-interpreted the word in a more Christian manner. Clement of Alexandria distinguished between two kinds of Christians, a pistic Christian who lives according to God's law, and the Christian gnostic who lives on the level of the gospel and responds by discipline and love. Clement's views of gnosis can be considered a forerunner of monasticism that began in Egypt after his death.

Clement suggested that philosophy was a preparatory discipline to the Greek world that would lead them to accept Christianity, and often sought to harmonize insights of Greek philosophy with biblical teaching. He defined philosophy as "the desire for true being and the studies which lead to it." Clement has been described as "the founder of what was to become the great tradition of Christian philosophical theology." He also was a forerunner to some views of Augustine, including arguably the just war theory and the theory of the two cities.

Clement is often regarded as one of the first Christian universalists, espousing belief in the eventual salvation of every person (though not with the level of systematic clarity of his disciple Origen). Clement understood divine punishment as corrective and remedial rather than merely retributive or destructive. He writes, "[God] destroys no one but gives salvation to all." "He bestows salvation on all mankind." "He indeed saves all universally—some as converted by punishments, others by voluntary submission with dignity of honor—that to Him every knee shall bow, both of beings in heaven, and on earth, and under the earth; that is, angels, and men, and souls departed this life." "God's punishments are saving and disciplinary, leading to conversion; choosing rather the repentance than the death of a sinner." "I will grant that He punishes the disobedient, for punishment is for the good and advantage of him who is punished, for it is the correction of a refractory subject." "For all things are arranged with a view to the salvation of the universe by the Lord of the universe, both generally and particularly."

For Clement, disciplining the body will help the Christian discipline his soul, which is why he gives detailed instructions on proper Christian conduct, decorum, and relationships in the second and third books of "The Instructor". Only once the passions are subject to the authority of the Word (or reason) can the Christian embark on an advanced course of philosophical study and contemplation.

Clement adopts a position that will give rise to a whole stream of later Christian thought: true philosophy and authentic human knowledge have their origin in the Logos, which is the unique source of all truth. He accepts the conception of παιδεία as he conducts the wisdom taught by the Logos through education in the sacred letters: on the one hand, the Greek παιδεία prepares the mind of the Christian to distinguish and defend the truth, and, on the other, the liberal arts help the new Christian to direct all his efforts towards the truly useful of each particular discipline, geometry, music, grammar and philosophy.

Notably (considering the time period), Clement seemed to advocate for the equality of women and men in the area of education, at least within the context of Christian spirituality and ethics. He wrote, "Let us recognize, too, that both men and women practice the same sort of virtue; surely, if there is but one God for both, then there is but one Educator for both."

Clement opposed a literal interpretation of the command "sell what you have and give to the poor," and argued that the Bible does not command every person to renounce all property, and that wealth can be used either for good or evil. Yet he seems to have done so tentatively (and perhaps reluctantly), to address the concerns of upper-class converts, while simultaneously warning of the dangers of wealth.

Clement believed that the days mentioned in Genesis are allegorical. Clement assumed a double creation, one of an invisible world and the second being material creation. He believed that formless matter existed before the creation of the world, being influenced by Plato. Clement tried to interpret Genesis 6 in harmony with the Book of Enoch.

The first person in church history to introduce a view of an invisible and a visible church is Clement of Alexandria. Because Clement saw the Protoevangelium of James as canonical, it could imply he believed in the perpetual virginity of Mary, though some have argued that he does not seem to believe in the sinlessness of Mary.

Clement of Alexandria interprets "Fire of Wisdom" which prevades the soul as by a baptism.

Clement of Alexandria used the word "symbol" to define the Eucharist, and interpreted "John 6" to be an allegory about faith, however his views on real presence are disputed.

Clement of Alexandria was apparently an amillennialist.




Cogito, ergo sum

The Latin , usually translated into English as "I think, therefore I am", is the "first principle" of René Descartes's philosophy. He originally published it in French as , in his 1637 "Discourse on the Method", so as to reach a wider audience than Latin would have allowed. It later appeared in Latin in his "Principles of Philosophy", and a similar phrase also featured prominently in his "Meditations on First Philosophy". The dictum is also sometimes referred to as the cogito. As Descartes explained in a margin note, "we cannot doubt of our existence while we doubt." In the posthumously published "The Search for Truth by Natural Light", he expressed this insight as ("I doubt, therefore I am — or what is the same — I think, therefore I am"). Antoine Léonard Thomas, in a 1765 essay in honor of Descartes presented it as ("I doubt, therefore I think, therefore I am").

Descartes's statement became a fundamental element of Western philosophy, as it purported to provide a certain foundation for knowledge in the face of radical doubt. While other knowledge could be a figment of imagination, deception, or mistake, Descartes asserted that the very act of doubting one's own existence served—at minimum—as proof of the reality of one's own mind; there must be a thinking entity—in this case the self—for there to be a thought.

One critique of the dictum, first suggested by Pierre Gassendi, is that it presupposes that there is an "I" which must be doing the thinking. According to this line of criticism, the most that Descartes was entitled to say was that "thinking is occurring", not that "I am thinking".

Descartes first wrote the phrase in French in his 1637 "Discourse on the Method". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 "Meditations on First Philosophy". The earliest written record of the phrase in Latin is in his 1644 "Principles of Philosophy", where, in a margin note (see below), he provides a clear explanation of his intent: "[W]e cannot doubt of our existence while we doubt". Fuller forms of the phrase are attributable to other authors.

The phrase first appeared (in French) in Descartes's 1637 "Discourse on the Method" in the first paragraph of its fourth part:

In 1641, Descartes published (in Latin) "Meditations on first philosophy" in which he referred to the proposition, though not explicitly as "cogito, ergo sum" in Meditation II:

In 1644, Descartes published (in Latin) his "Principles of Philosophy" where the phrase "ego cogito, ergo sum" appears in Part 1, article 7:

Descartes's margin note for the above paragraph is:

Descartes, in a lesser-known posthumously published work dated as written ca. 1647 and titled ("The Search for Truth by Natural Light"), provides his only known phrasing of the cogito as and admits that his insight is also expressible as dubito, ergo sum:

The proposition is sometimes given as . This form was penned by the French literary critic, Antoine Léonard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as "" ('Since I doubt, I think; since I think, I exist'). With rearrangement and compaction, the passage translates to "I doubt, therefore I think, therefore I am," or in Latin, ""dubito, ergo cogito, ergo sum"." This aptly captures Descartes's intent as expressed in his posthumously published "La Recherche de la Vérité par La Lumiere Naturale" as noted above: I doubt, therefore I am — or what is the same — I think, therefore I am.

A further expansion, ("…—a thinking thing") extends the "cogito" with Descartes's statement in the subsequent "Meditation", ("I am a thinking [conscious] thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many,-- who loves, hates, wills, refuses, who imagines likewise, and perceives"). This has been referred to as "the expanded "cogito"."

While the Latin "cōgitō" may be translated rather easily as "I think/ponder/visualize", does not indicate whether the verb form corresponds to the English simple present or progressive aspect. Technically speaking, the French lemma "pense" by itself is actually the result of numerous different conjugations of the verb "penser" (to think) – it could mean "I think... (something)"/"He thinks... (something)", "I think."/"He thinks.", or even "You (must) think... (something).", thereby necessitating the use of the wider context, or a pronoun, to understand the meaning. In the case of "je pense", a pronoun is already included, "je" or "I", but this still leaves the question of whether "I think..." or "I think." is intended. Therefore, translation needs a larger context to determine aspect.

Following John Lyons (1982), Vladimir Žegarac notes, "The temptation to use the simple present is said to arise from the lack of progressive forms in Latin and French, and from a misinterpretation of the meaning of "cogito" as habitual or generic" (cf. gnomic aspect). Also following Lyons, Ann Banfield writes, "In order for the statement on which Descartes's argument depends to represent certain knowledge,… its tense must be a true present—in English, a progressive,… not as 'I think' but as 'I am thinking, in conformity with the general translation of the Latin or French present tense in such nongeneric, nonstative contexts." Or in the words of Simon Blackburn, "Descartes's premise is not 'I think' in the sense of 'I ski', which can be true even if you are not at the moment skiing. It is supposed to be parallel to 'I am skiing'."

The similar translation "I am thinking, therefore I exist" of Descartes's correspondence in French (", ") appears in "The Philosophical Writings of Descartes" by Cottingham et al. (1988).

The earliest known translation as "I am thinking, therefore I am" is from 1872 by Charles Porterfield Krauth.

Fumitaka Suzuki writes "Taking consideration of Cartesian theory of continuous creation, which theory was developed especially in the Meditations and in the Principles, we would assure that 'I am thinking, therefore I am/exist' is the most appropriate English translation of 'ego cogito, ergo sum'."

Alexis Deodato S. Itao notes that is "literally 'I think, therefore I am'." Others differ: 1) "[A] precise English translation will read as 'I am thinking, therefore I exist'.; and 2) "[S]ince Descartes ... emphasized that existence is such an important 'notion,' a better translation is 'I am thinking, therefore I exist.'"

Descartes wrote this phrase as such only once, in the posthumously published lesser-known work noted above, "The Search for Truth by Natural Light". It appeared there mid-sentence, uncapitalized, and with a comma. (Commas were not used in Classical Latin but were a regular feature of scholastic Latin, the Latin Descartes "had learned in a Jesuit college at La Flèche.") Most modern reference works show it with a comma, but it is often presented without a comma in academic work and in popular usage. In Descartes's "Principia Philosophiae", the proposition appears as ego cogito, ergo sum.

As put succinctly by Krauth (1872), "That cannot doubt which does not think, and that cannot think which does not exist. I doubt, I think, I exist."

The phrase "cogito, ergo sum" is not used in Descartes's "Meditations on First Philosophy" but the term "the "cogito"" is used to refer to an argument from it. In the "Meditations", Descartes phrases the conclusion of the argument as "that the proposition, "I am, I exist," is necessarily true whenever it is put forward by me or conceived in my mind" ("Meditation" II). George Henry Lewes says Descartes "has told us that [his objective] was to find a starting point from which to reason—to find an irreversible certainty. And where did he find this? In his own consciousness. Doubt as I may, I cannot doubt of my own existence, because my very doubts reveal to me a something which doubts. You may call this an assumption, if you will; I point out the fact as one above and beyond all logic; which logic can neither prove nor disprove; but which must always remain an irreversible certainty, and as such a fitting basis of philosophy."

At the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt—his argument from the existence of a deceiving god—Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.

There are three important notes to keep in mind here. First, he claims only the certainty of "his own" existence from the first-person point of view — he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that "if he thinks", then necessarily he exists (see the instantiation principle). Third, this proposition "I am, I exist" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition. Descartes does not use this first certainty, the "cogito", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to discover further truths. As he puts it:

According to many Descartes specialists, including Étienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion — the immediate clarity and distinctiveness of self-evident propositions — to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes's thinking, therefore, is not so much in expressing the "cogito"—a feat accomplished by other predecessors, as we shall see—but on using the "cogito" as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.
Baruch Spinoza in ""Principia philosophiae cartesianae"" at its "Prolegomenon" identified "cogito ergo sum" the ""ego sum cogitans"" (I am a thinking being) as the thinking substance with his ontological interpretation.

Although the idea expressed in "cogito, ergo sum" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the "knowledge of knowledge" (Greek: νόησις νοήσεως, "nóesis noéseos") and Aristotle explains the idea in full length:

The Cartesian statement was interpreted to be an Aristotelian syllogism where the premise that all thinkers are also beings is not made explicit.

In the late sixth or early fifth century BC, Parmenides is quoted as saying "For to be aware and to be are the same". (Fragment B3)

In the early fifth century AD, Augustine of Hippo in "De Civitate Dei" (book XI, 26) affirmed his certain knowledge of his own existence, and added: "So far as these truths are concerned, I do not at all fear the arguments of the Academics when they say, What if you are mistaken? For if I am mistaken, I exist." This formulation () is sometimes called the Augustinian . In 1640, Descartes wrote to thank Andreas Colvius (a friend of Descartes's mentor, Isaac Beeckman) for drawing his attention to Augustine:

Another predecessor was Avicenna's "Floating Man" thought experiment on human self-awareness and self-consciousness.

The 8th century Hindu philosopher Adi Shankara wrote, in a similar fashion, that no one thinks 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt.

Spanish philosopher Gómez Pereira in his 1554 work "De Inmortalitate Animae", wrote ""nosco me aliquid noscere, & quidquid noscit, est, ergo ego sum"" ('I know that I know something, anyone who knows is, therefore I am').

In "Descartes, The Project of Pure Enquiry", Bernard Williams provides a history and full evaluation of this issue. The first to raise the "I" problem was Pierre Gassendi, who in his , as noted by Saul Fisher "points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. …[T]he only claim that is indubitable here is the agent-independent claim that there is cognitive activity present."

The objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: "thinking is occurring." That is, whatever the force of the "cogito", Descartes draws too much from it; the existence of a thinking thing, the reference of the "I," is more than the "cogito" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an "I", that there is such an activity as "thinking", and that "I" know what "thinking" is. He suggested a more appropriate phrase would be "it thinks" wherein the "it" could be an impersonal subject as in the sentence "It is raining."

The Danish philosopher Søren Kierkegaard calls the phrase a tautology in his "Concluding Unscientific Postscript". He argues that the "cogito" already presupposes the existence of "I", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise "I think" into the premises "'x' thinks" and "I am that 'x'", where "x" is used as a placeholder in order to disambiguate the "I" from the thinking thing.

Here, the "cogito" has already assumed the "I"'s existence as that which thinks. For Kierkegaard, Descartes is merely "developing the content of a concept", namely that the "I", which already exists, thinks. As Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.

Bernard Williams claims that what we are dealing with when we talk of thought, or when we say "I am thinking," is something conceivable from a third-person perspective—namely objective "thought-events" in the former case, and an objective thinker in the latter. He argues, first, that it is impossible to make sense of "there is thinking" without relativizing it to "something." However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness. The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.

As a critic of Cartesian subjectivity, Heidegger sought to ground human subjectivity in death as that certainty which individualizes and authenticates our being. As he wrote in 1925 in "History of the Concept of Time":

The Scottish philosopher John Macmurray rejects the "cogito" outright in order to place action at the center of a philosophical system he entitles the Form of the Personal. "We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality." The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost, thus dissolving the integrity of our selves, and destroying any connection with reality. In order to formulate a more adequate "cogito", Macmurray proposes the substitution of "I do" for "I think," ultimately leading to a belief in God as an agent to whom all persons stand in relation.



Carl Barks

Carl Barks (March 27, 1901 – August 25, 2000) was an American cartoonist, author, and painter. He is best known for his work in Disney comic books, as the writer and artist of the first Donald Duck stories and as the creator of Scrooge McDuck. He worked anonymously until late in his career; fans dubbed him The Duck Man and The Good Duck Artist. In 1987, Barks was one of the three inaugural inductees of the Will Eisner Comic Book Hall of Fame.

Barks worked for the Disney Studio and Western Publishing where he created Duckburg and many of its inhabitants, such as Scrooge McDuck (1947), Gladstone Gander (1948), the Beagle Boys (1951), The Junior Woodchucks (1951), Gyro Gearloose (1952), Cornelius Coot (1952), Flintheart Glomgold (1956), John D. Rockerduck (1961) and Magica De Spell (1961).

He has been named by animation historian Leonard Maltin as "the most popular and widely read artist-writer in the world". Will Eisner called him "the Hans Christian Andersen of comic books." Beginning especially in the 1980s, Barks' artistic contributions would be a primary source for animated adaptations such as "DuckTales" and its 2017 remake.

Barks was born in Merrill, Oregon, to William Barks and his wife, Arminta Johnson. He had an older brother named Clyde. His paternal grandparents were David Barks and his wife Ruth Shrum. Barks' maternal grandparents were Carl Johnson and his wife, Suzanna Massey, but little else is known about his ancestors. Barks was the descendant of Jacob Barks, who came to Missouri from North Carolina 1800. They lived in Marble Hill in Bollinger County. Jacob Barks' son Isaac was the father of the David Barks noted above.

According to Barks's description of his childhood, he was a rather lonely child. His parents owned of land that served as their farm. The nearest neighbor lived away, but he was more an acquaintance to Barks's parents than a friend. The closest school was about away and Barks had to walk that distance every day. The rural area had few children, though, and Barks later remembered that his school had only about eight or ten students including him. He had high praise for the quality of the education he received in that small school. "Schools were good in those days", he used to say.
The lessons lasted from nine o'clock in the morning to four o'clock in the afternoon and then he had to return to the farm. There he remembered not having anybody to talk to, as his parents were busy and he had little in common with his brother.

In 1908, William Barks (in an attempt to increase the family income) moved with his family to Midland, Oregon, some miles north of Merrill, to be closer to the new railway lines. He established a new stock-breeding farm and sold his produce to the local slaughterhouses.

Nine-year-old Clyde and seven-year-old Carl worked long hours there. But Carl later remembered that the crowd which gathered at Midland's market place made a strong impression on him. This was expected, as he was not used to crowds up until then. According to Barks, his attention was mostly drawn to the cowboys that frequented the market with their revolvers, strange nicknames for each other and sense of humor.

By 1911, they had been successful enough to move to Santa Rosa, California. There they started cultivating vegetables and set up some orchards. Unfortunately, the profits were not as high as William expected and they started having financial difficulties. William's anxiety over them was probably what caused his first nervous breakdown.

As soon as William recovered, he made the decision to move back to Merrill. The year was 1913, and Barks was already 12 years old; but, due to the constant moving, he had not yet managed to complete grade school. He resumed his education at this point and finally managed to graduate in 1916.

1916 served as a turning point in Barks's life for various reasons. First, Arminta, his mother, died in this year. Second, his hearing problems, which had already appeared earlier, had at the time become severe enough for him to have difficulties listening to his teachers talking. His hearing would continue to get worse later, but at that point he had not yet acquired a hearing aid. Later in life, he couldn't do without one. Third, the closest high school to their farm was away and even if he did enroll in it, his bad hearing was likely to contribute to his learning problems. He had to decide to stop his school education, much to his disappointment.

Barks started taking various jobs but had little success in such occupations as a farmer, woodcutter, turner, mule driver, cowboy and printer. From his jobs he learned, he later averred, how eccentric, stubborn and unpredictable men, animals and machines can be. At the same time he interacted with colleagues, fellow breadwinners who had satirical disposition towards even their worst troubles. Barks later declared that he was sure that if not for a little humor in their troubled lives, they would certainly go insane. It was an attitude towards life that Barks would adopt. Later he would say it was natural for him to satirize the secret yearnings and desires, the pompous style and the disappointments of his characters. According to Barks, this period of his life would later influence his best known fictional characters: Walt Disney's Donald Duck and his own Scrooge McDuck.

Donald's drifting from job to job was reportedly inspired by Barks's own experiences. So was his usual lack of success. And even in those that he was successful this would be temporary, just until a mistake or chance event caused another failure, another disappointment for the frustrated duck. Barks also reported that this was another thing he was familiar with.

Scrooge's main difference to Donald, according to Barks, was that he too had faced the same difficulties in his past but through intelligence, determination and hard work, he was able to overcome them. Or, as Scrooge himself would say to Huey, Dewey, and Louie: by being "tougher than the toughies and smarter than the smarties." In Barks's stories Scrooge would work to solve his many problems, even though the stories would often point out that his constant efforts seemed futile at the end.

Through both characters Barks would often exhibit his rather sarcastic sense of humor. It seems that this difficult period for the artist helped shape many of his later views in life that were expressed through his characters.

At the same time Barks had started thinking about turning a hobby that he always enjoyed into a profession: that of drawing. Since his early childhood he spent his free time by drawing on any material he could find. He had attempted to improve his style by copying the drawings of his favorite comic strip artists from the newspapers where he could find them. As he later said, he wanted to create his own facial expressions, figures and comical situations in his drawings but wanted to study the master comic artists' use of the pen and their use of color and shading.

Among his early favorites were Winsor McCay (mostly known for "Little Nemo") and Frederick Burr Opper (mostly known for "Happy Hooligan") but he would later study any style that managed to draw his attention.

At age 16, he was mostly self-taught but at this point he decided to take some lessons through correspondence. He only followed the first four lessons and then had to stop because his working left him with little free time. But as he later said, the lessons proved very useful in improving his style.

By December 1918, he left his father's home to attempt to find a job in San Francisco, California. He worked for a while in a small publishing house while attempting to sell his drawings to newspapers and other printed material with little success.

While he continued drifting through various jobs, he met Pearl Turner (1904–1987). In 1921 they married and had two daughters:

In 1923 he returned to his paternal farm in Merrill in an attempt to return to the life of a farmer, but that ended soon. He continued searching for a job while attempting to sell his drawings. He soon managed to sell some of them to "Judge" magazine and then started having success submitting to the Minneapolis-based "Calgary Eye-Opener", a racy men's cartoon magazine of the era. He was eventually hired as editor and scripted and drew most of the contents while continuing to sell occasional work to other magazines. His salary of $90 per month was considered respectable enough for the time. A facsimile of one of the racy magazines he did cartoons for in this period, "Coo Coo" #1, was published by Hamilton Comics in 1997.

Meanwhile, he had his first divorce. He and Pearl were separated in 1929 and divorced in 1930. After he moved to Minneapolis, Minnesota, where "Calgary-Eye-Opener" had its offices he met Clara Balken, who in 1938 became his second wife.

In November 1935, when he learned that Walt Disney was seeking more artists for his studio, Barks decided to apply. He was approved for a try-out which entailed a move to Los Angeles, California. He was one of two in his class of trainees who was hired. His starting salary was 20 dollars a week. He started at Disney Studios in 1935, more than a year after the debut of Donald Duck on June 9, 1934, in the short animated film "The Wise Little Hen".

Barks initially worked as an inbetweener. This involved being teamed and supervised by one of the head animators who did the key poses of character action (often known as extremes) for which the inbetweeners did the drawings between the extremes to create the illusion of movement. While an inbetweener, Barks submitted gag ideas for cartoon story lines being developed and showed such a knack for creating comical situations that by 1937 he was transferred to the story department. His first story sale was the climax of "Modern Inventions", for a sequence where a robot barber chair gives Donald Duck a haircut on his bottom.

In 1937, when Donald Duck became the star of his own series of cartoons instead of co-starring with Mickey Mouse and Goofy as previously, a new unit of storymen and animators was created devoted solely to this series. Though he originally just contributed gag ideas to some duck cartoons by 1937 Barks was (principally with partner Jack Hannah) originating story ideas that were storyboarded and (if approved by Walt) put into production. He collaborated on such cartoons as "Donald's Nephews" (1938), "Donald's Cousin Gus" (1939), "Mr. Duck Steps Out" (1940), "Timber" (1941), "The Vanishing Private" (1942) and "The Plastics Inventor" (1944).

Unhappy at the emerging wartime working conditions at Disney, and bothered by ongoing sinus problems caused by the studio's air conditioning, Barks quit in 1942. Shortly before quitting, he moonlighted as a comic book artist, contributing half the artwork for a one-shot comic book (the other half of the art being done by story partner Jack Hannah) titled "Donald Duck Finds Pirate Gold". This 64-page story was adapted by Donald Duck comic strip writer Bob Karp from an unproduced feature, and published in October 1942 in Dell Comics "Four Color Comics" #9. It was the first Donald Duck story originally produced for an American comic book and also the first involving Donald and his nephews in a treasure hunting expedition, in this case for the treasure of Henry Morgan. Barks would later use the treasure hunting theme in many of his stories. This actually was not his first work in comics, as earlier the same year Barks along with Hannah and fellow storyman Nick George scripted "Pluto Saves the Ship", which was among the first original Disney comic book stories published in the United States.

After quitting the Disney Studio, Barks relocated to the Hemet/San Jacinto area in the semi-desert Inland Empire region east of Los Angeles where he hoped to start a chicken farm.

When asked which of his stories was a favorite in several interviews Barks cited the ten-pager in "Walt Disney's Comics and Stories" #146 (Nov. 1952) in which Donald tells the story of the chain of unfortunate events that took place when he owned a chicken farm in a town which subsequently was renamed Omelet. Likely one reason it was a favorite is that it was inspired by Barks' own experiences in the poultry business.

But to earn a living in the meantime he inquired whether Western Publishing, which had published "Pirate Gold", had any need for artists for Donald Duck comic book stories. He was immediately assigned to illustrate the script for a ten-page Donald Duck story for the monthly "Walt Disney's Comics and Stories". At the publisher's invitation he revised the storyline and the improvements impressed the editor sufficiently to invite Barks to try his hand at contributing both the script and the artwork of his follow-up story. This set the pattern for Barks' career in that (with rare exceptions) he provided art (pencil, inking, solid blacks and lettering) and scripting for his stories.

"The Victory Garden", that initial ten-page story published in April, 1943 was the first of about 500 stories featuring the Disney ducks Barks would produce for Western Publishing over the next three decades, well into his purported retirement. These can be mostly divided into three categories:

Barks' artistic growth during his first decade in comics saw a transformation from rather rudimentary storytelling derived from his years as an animation artist and storyman into a virtuoso creator of complex narratives, notably in his longer adventure tales. According to critic Geoffrey Blum, the process that saw its beginnings in 1942's "Pirate Gold" first bore its full fruit in 1950's "Vacation Time", which he describes as "a visual primer for reading comics and understanding ... the form".

He surrounded Donald Duck and nephews Huey, Dewey, and Louie with a cast of eccentric and colorful characters, such as the aforementioned Scrooge McDuck, the wealthiest duck in the world; Gladstone Gander, Donald's obscenely lucky cousin; inventor Gyro Gearloose; the persistent Beagle Boys; the sorceress Magica De Spell; Scrooge's rivals Flintheart Glomgold and John D. Rockerduck; Daisy's nieces April, May and June; Donald's neighbor Jones, and The Junior Woodchucks organization.

Barks's stories (whether humorous adventures or domestic comedies) often exhibited a wry, dark irony born of hard experience. The ten-pagers showcased Donald as everyman, struggling against the cruel bumps and bruises of everyday life with the nephews often acting as a Greek chorus commenting on the unfolding disasters Donald wrought upon himself. Yet while seemingly defeatist in tone, the humanity of the characters shines through in their persistence despite the obstacles. These stories found popularity not only among young children but adults as well. Despite the fact that Barks had done little traveling his adventure stories often had the duck clan globe-trotting to the most remote or spectacular of places. This allowed Barks to indulge his penchant for elaborate backgrounds that hinted at his thwarted ambitions of doing realistic stories in the vein of Hal Foster's "Prince Valiant".

As Barks blossomed creatively, his marriage to Clara deteriorated. This is the period referred to in Barks' famed quip that he could feel his creative juices flowing while the whiskey bottles hurled at him by a tipsy Clara flew by his head. They were divorced in 1951, his second and last divorce. In this period Barks dabbled in fine art, exhibiting paintings at local art shows. It was at one of these in 1952 he became acquainted with fellow exhibitor Margaret Wynnfred Williams (1917 – March 10, 1993), nicknamed Garé. She was an accomplished landscape artist, some of whose paintings are in the collection of the Leanin' Tree Museum of Western Art. During her lifetime, and to this day, note cards of her paintings are available from Leanin' Tree. Her nickname appears as a store name in the story "Christmas in Duckburg", featured on page 1 of "Walt Disney's Christmas Parade" #9, published in 1958. Soon after they met, she started assisting Barks, handling the solid blacks and lettering, both of which he had found onerous. They married in 1954 and the union lasted until her death.

People who worked for Disney (and its comic book licensees) generally did so in relative anonymity; stories would only carry Walt Disney's name and (sometimes) a short identification number. Prior to 1960 Barks' identity remained a mystery to his readers. However, many readers recognized Barks' work and drawing style and began to call him the Good Duck Artist, a label that stuck even after his true identity was discovered by fans in the late 1950s. Malcolm Willits was the first person to learn Barks's name and address, but two brothers named John and Bill Spicer became the first fans to contact Barks after independently discovering the same information. After Barks received a 1960 visit from the Spicer brothers and Ron Leonard, he was no longer anonymous, as word of his identity spread through the emerging network of comic book fandom fanzines and conventions.

Carl Barks retired in 1966, but was persuaded by editor Chase Craig to continue to script stories for Western. The last new comic book story drawn by Carl Barks was a Daisy Duck tale ("The Dainty Daredevil") published in "Walt Disney Comics Digest" issue 5 (Nov. 1968). When bibliographer Michael Barrier asked Barks why he drew it, Barks' vague recollection was no one was available and he was asked to do it as a favor by Craig.

He wrote one Uncle Scrooge story, and three Donald Duck stories. From 1970 to 1974, Barks was the main writer for the Junior Woodchucks comic book (issues 6 through 25). The latter included environmental themes that Barks first explored in 1957 ["Land of the Pygmy Indians", "Uncle Scrooge" #18]. Barks also sold a few sketches to Western that were redrawn as covers. For a time the Barkses lived in Goleta, California, before returning to the Inland Empire by moving to Temecula.

To make a little extra money beyond what his pension and scripting earnings brought in, Barks started doing oil paintings to sell at the local art shows where he and Garé exhibited. Subjects included humorous depictions of life on the farm and portraits of Native American princesses. These skillfully rendered paintings encouraged fan Glenn Bray to ask Barks if he could commission a painting of the ducks ("A Tall Ship and a Star to Steer Her By", taken from the cover of "Walt Disney's Comics and Stories" #108 by Barks). This prompted Barks to contact George Sherman at Disney's Publications Department to request permission to produce and sell oil paintings of scenes from his stories. In July 1971 Barks was granted a royalty-free license by Disney. When word spread that Barks was taking commissions from those interested in purchasing an oil of the ducks, much to his astonishment the response quickly outstripped what he reasonably could produce in the next few years.

When Barks expressed dismay at coping with the backlog of orders he faced, fan/dealers Bruce Hamilton and Russ Cochran suggested Barks instead auction his paintings at conventions and via Cochran's catalog "Graphic Gallery". By September 1974 Barks had discontinued taking commissions.

At Boston's NewCon convention, in October 1975, the first Carl Barks oil painting auctioned at a comic book convention ("She Was Spangled and Flashy") sold for $2,500. Subsequent offerings saw an escalation in the prices realized.

In 1976, Barks and Garé went to Boston for the NewCon show, their first comic convention appearance. Among the other attendees was famed "Little Lulu" comic book scripter John Stanley; despite both having worked for Western Publishing this was the first time they met. The highlight of the convention was the auctioning of what was to that time the largest duck oil painting Barks had done, "July Fourth in Duckburg", which included depictions of several prominent Barks fans and collectors. It sold for a then record high amount: $6,400.

Soon thereafter a fan sold unauthorized prints of some of the Scrooge McDuck paintings, leading Disney to withdraw permission for further paintings. To meet demand for new work Barks embarked on a series of paintings of non-Disney ducks and fantasy subjects such as Beowulf and Xerxes. These were eventually collected in the limited-edition book "Animal Quackers".

As the result of heroic efforts by "Star Wars" producer Gary Kurtz and screenwriter Edward Summer, Disney relented and, in 1981, allowed Barks to do a now seminal oil painting called "Wanderers of Wonderlands" for a breakthrough limited edition book entitled "Uncle Scrooge McDuck: His Life and Times". The book collected 11 classic Barks stories of Uncle Scrooge colored by artist Peter Ledger along with a new Scrooge story by Barks done storybook style with watercolor illustrations, "Go Slowly, Sands of Time". After being turned down by every major publisher in New York City, Kurtz and Summer published the book through Celestial Arts, which Kurtz acquired partly for this purpose. The book went on to become the model for virtually every important collection of comic book stories. It was the first book of its kind ever reviewed in "Time" magazine and subsequently in "Newsweek", and the first book review in "Time" with large color illustrations.

In 1977 and 1982, Barks attended the San Diego Comic-Con. As with his appearance in Boston, the response to his presence was overwhelming, with long lines of fans waiting to meet Barks and get his autograph.

In 1981, Bruce Hamilton and Russ Cochran, two long-time Disney comics fans, decided to combine forces to bring greater recognition to the works of Carl Barks. Their first efforts went into establishing Another Rainbow Publishing, the banner under which they produced and issued the award-winning book "The Fine Art of Walt Disney's Donald Duck by Carl Barks", a comprehensive collection of the Disney duck paintings of this artist and storyteller. Not long after, the company began producing fine art lithographs of many of these paintings, in strictly limited editions, all signed by Barks, who eventually produced many original works for the series.

In 1983, Barks relocated one last time to Grants Pass, Oregon, near where he grew up, partly at the urging of friend and "Broom Hilda" artist Russell Myers, who lived in the area. The move also was motivated, Barks stated in another famous quip, by Temecula being too close to Disneyland and thus facilitating a growing torrent of drop-in visits by vacationing fans. In this period Barks made only one public appearance, at a comic book shop near Grants Pass.

In 1983, Another Rainbow took up the daunting task of collecting the entire Disney comic book oeuvre of Barks—over 500 stories in all—in the ten-set, thirty-volume "Carl Barks Library". These oversized hardbound volumes reproduced Barks' pages in pristine black and white line art, as close as possible to the way he would originally draw them, and included mountains of special features, articles, reminiscences, interviews, storyboards, critiques, and more than a few surprises. This monumental project was finally completed in mid-1990.

In 1985, a new division was founded, Gladstone Publishing, which took up the then-dormant Disney comic book license. Gladstone introduced a new generation of Disney comic book readers to the storytelling of Barks, Paul Murry, and Floyd Gottfredson, as well as presenting the first works of modern Disney comics artists Don Rosa and William Van Horn. Seven years after Gladstone's founding, the "Carl Barks Library" was revived as the "Carl Barks Library in Color", as full-color, high-quality squarebound comic albums (including the first-ever Carl Barks trading cards).

From 1993 to 1998, Barks' career was managed by the "Carl Barks Studio" (Bill Grandey and Kathy Morby—they had sold Barks original art since 1979). This involved numerous art projects and activities, including a tour of 11 European countries in 1994, Iceland being the first foreign country he ever visited. Barks appeared at the first of many Disneyana conventions in 1993. Silk screen prints of paintings along with high-end art objects (such as original water colors, bronze figurines and ceramic tiles) were produced based on designs by Barks.

During the summer of 1994 and until his death, Barks and his studio personally assigned Peter Reichelt, a museum exhibition producer from Mannheim, Germany, as his agent for Europe. Publisher "Edition 313" put out numerous lithographs. In 1997, tensions between Barks and the Studio eventually resulted in a lawsuit that was settled with an agreement that included the disbanding of the Studio. Barks never traveled to make another Disney appearance. He was represented by Ed Bergen, as he completed a final project. Gerry Tank and Jim Mitchell were to assist Barks in his final years.

During his Carl Barks Studio years, Barks created two more stories: the script for the final Uncle Scrooge story "Horsing Around with History", which was first published in Denmark in 1994 with Bill Van Horn art. The outlines for Barks' final Donald Duck story "Somewhere in Nowhere", were first published in 1997, in Italy, with art by Pat Block.

Austrian artist Gottfried Helnwein curated and organized the first solo museum-exhibition of Barks. Between 1994 and 1998 the retrospective was shown in ten European museums and seen by more than 400,000 visitors.

At the same time in spring 1994, Reichelt and Ina Brockmann designed a special museum exhibition tour about Barks' life and work. Also represented for the first time at this exhibition were Disney artists Al Taliaferro and Floyd Gottfredson. Since 1995, more than 500,000 visitors have attended the shows in Europe.

Reichelt also translated Michael Barrier's biography of Barks into German and published it in 1994.

Barks spent his final years in a new home in Grants Pass, Oregon, which he and Garé, who died in 1993, had built next door to their original home. In July 1999, he was diagnosed with chronic lymphocytic leukemia, a form of cancer arising from the white blood cells in the bone marrow, for which he received oral chemotherapy. However, as the disease progressed, causing him great discomfort, the ailing Barks decided to stop receiving treatment in June 2000. In spite of his terminal condition, Barks remained, according to caregiver Serene Hunicke, "funny up to the end".

The year before, Barks had told the university professor Donald Ault:

I have no apprehension, no fear of death. I do not believe in an afterlife. ... I think of death as total peace. You're beyond the clutches of all those who would crush you.

On August 25, 2000, shortly after midnight, Carl Barks died quietly in his sleep at the age of 99. He was interred in Hillcrest Memorial Cemetery in Grants Pass, beside Garé's grave.

"(A)n asteroid was named after the Duck Man in 1983 --- 2730 Barks, a carbonaceous C-type asteroid with a diameter of between 10 and 16 kilometers, an ordital period of six years and four months, and a rotation period of just over six hours." In a 1983 interview, Barks says that "," a story about the Ducks traveling to the asteroid belt to find a place Uncle Scrooge can store his money, was his favorite story.

Barks' Donald Duck stories were rated #7 on "The Comics Journal" list of 100 top comics; his Uncle Scrooge stories were rated #20.

Steven Spielberg and George Lucas have acknowledged that the rolling-boulder booby trap in the opening scene of "Raiders of the Lost Ark" was inspired by the 1954 Carl Barks Uncle Scrooge adventure "The Seven Cities of Cibola" ("Uncle Scrooge" #7). Lucas and Spielberg have also said that some of Barks' stories about space travel and the depiction of aliens had an influence on them. Lucas wrote the foreword to the 1982 "Uncle Scrooge McDuck: His Life and Times". In it he calls Barks' stories "cinematic" and "a priceless part of our literary heritage".

The Walt Disney Treasures DVD set "" includes a salute to Barks.

Carl Barks has an asteroid named after him, 2730 Barks.

In Almere, Netherlands, a street was named after him: Carl Barksweg. The same neighborhood also includes a Donald Ducklaan and a Goofystraat.

Japanese animator and cartoonist Osamu Tezuka, who created manga such as "Astro Boy" and "Black Jack", was a fan of Barks' work. "New Treasure Island", one of Tezuka's first works, was partly influenced by "Donald Duck Finds Pirate Gold".

A 1949 Donald Duck ten-pager features Donald raising a yacht from the ocean floor by filling it with ping pong balls. In December 1965 Karl Krøyer, a Dane, lifted the sunken freight vessel "Al Kuwait" in the Kuwait Harbor by filling the hull with 27 million tiny inflatable balls of polystyrene. Krøyer denies having been inspired by this Barks story. Some sources claim Krøyer was denied a Dutch patent registration (application number NL 6514306) for his invention on the grounds that the Barks story was a prior publication of the invention. Krøyer later successfully raised another ship off Greenland using the same method, and several other sunken vessels worldwide have since been raised by modified versions of this concept. The television show "MythBusters" also tested this method and was able to raise a small boat.

Don Rosa, one of the most popular living Disney artists, and possibly the one who has been most keen on connecting the various stories into a coherent universe and chronology, considers (with few exceptions) all Barks' duck stories as canon, and all others as apocryphal. Rosa has said that a number of novelists and movie-makers cite Carl Barks as their 'major influence and inspiration'.

When the news of Barks' passing was hardly covered by the press in America, "in Europe the sad news was flashed instantly across the airwaves and every newspaper — they realized the world had lost one of the most beloved, influential and well-known creators in international culture."

In 2010 Oregon Cartoon Institute produced a video about the influence of Carl Barks and Basil Wolverton on Robert Crumb.

The video game "" is dedicated to the memory of Carl Barks.

Carl Barks drew an early Andy Panda comic book story published in "New Funnies" #76, 1943. It is one of his few stories to feature humans interacting with talking animal characters (another is "Dangerous Disguise", "Four Color" #308, 1951). See List of Fictional Pandas.

The life story of Carl Barks, largely drawing upon his relationship with Disney and the phonetic similarity of his name to Karl Marx, serves as a loose inspiration to one of the subplots in "The Last Song of Manuel Sendero" by Ariel Dorfman.

The first image ever to be displayed on an Apple Macintosh was a scan of Carl Barks' Scrooge McDuck.


Films where Barks served as storyman or story director include:



Barks was an enthusiastic user of Esterbrook pens, and used a Nº 356 model to ink and letter his Donald Duck comic-book pages.




Centimetre–gram–second system of units

The centimetre–gram–second system of units (CGS or cgs) is a variant of the metric system based on the centimetre as the unit of length, the gram as the unit of mass, and the second as the unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways in which the CGS system was extended to cover electromagnetism.

The CGS system has been largely supplanted by the MKS system based on the metre, kilogram, and second, which was in turn extended and replaced by the International System of Units (SI). In many fields of science and engineering, SI is the only system of units in use, but there remain certain subfields where CGS is prevalent.

In measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 as and . For example, the CGS unit of force is the dyne, which is defined as , so the SI unit of force, the newton (), is equal to .

On the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is more subtle. Formulas for physical laws of electromagnetism (such as Maxwell's equations) take a form that depends on which system of units is being used, because the electromagnetic quantities are defined differently in SI and in CGS. Furthermore, within CGS, there are several plausible ways to define electromagnetic quantities, leading to different "sub-systems", including Gaussian units, "ESU", "EMU", and Heaviside–Lorentz units. Among these choices, Gaussian units are the most common today, and "CGS units" is often intended to refer to CGS-Gaussian units.

The CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1873, a committee of the British Association for the Advancement of Science, including physicists James Clerk Maxwell and William Thomson recommended the general adoption of centimetre, gram and second as fundamental units, and to express all derived electromagnetic units in these fundamental units, using the prefix "C.G.S. unit of ...".

The sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre–kilogram–second) system, which in turn developed into the modern SI standard.

Since the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide. SI units are predominantly used in engineering applications and physics education, while Gaussian CGS units are commonly used in theoretical physics, describing microscopic systems, relativistic electrodynamics, and astrophysics. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as "The Astrophysical Journal".

The units gram and centimetre remain useful as noncoherent units within the SI system, as with any other prefixed SI units.

In mechanics, the quantities in the CGS and SI systems are defined identically. The two systems differ only in the scale of the three base units (centimetre versus metre and gram versus kilogram, respectively), with the third unit (second) being the same in both systems.

There is a direct correspondence between the base units of mechanics in CGS and SI. Since the formulae expressing the laws of mechanics are the same in both systems and since both systems are coherent, the definitions of all coherent derived units in terms of the base units are the same in both systems, and there is an unambiguous relationship between derived units:

Thus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:

Expressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:

The conversion factors relating electromagnetic units in the CGS and SI systems are made more complex by the differences in the formulas expressing physical laws of electromagnetism as assumed by each system of units, specifically in the nature of the constants that appear in these formulas. This illustrates the fundamental difference in the ways the two systems are built: 

In each of these systems the quantities called "charge" etc. may be a different quantity; they are distinguished here by a superscript. The corresponding quantities of each system are related through a proportionality constant.

Maxwell's equations can be written in each of these systems as:

In the electrostatic units variant of the CGS system, (CGS-ESU), charge is defined as the quantity that obeys a form of Coulomb's law without a multiplying constant (and current is then defined as charge per unit time):

The ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in CGS-ESU, a franklin is equal to a centimetre times square root of dyne:
The unit of current is defined as:

In the CGS-ESU system, charge "q" is therefore has the dimension to MLT.

Other units in the CGS-ESU system include the statampere (1 statC/s) and statvolt (1 erg/statC).

In CGS-ESU, all electric and magnetic quantities are dimensionally expressible in terms of length, mass, and time, and none has an independent dimension. Such a system of units of electromagnetism, in which the dimensions of all electric and magnetic quantities are expressible in terms of the mechanical dimensions of mass, length, and time, is traditionally called an 'absolute system'.

All electromagnetic units in the CGS-ESU system that have not been given names of their own are named as the corresponding SI name with an attached prefix "stat" or with a separate abbreviation "esu", and similarly with the corresponding symbols.

In another variant of the CGS system, electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well).

The EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:
The unit of charge in CGS EMU is:

Dimensionally in the CGS-EMU system, charge "q" is therefore equivalent to ML. Hence, neither charge nor current is an independent physical quantity in the CGS-EMU system.

All electromagnetic units in the CGS-EMU system that do not have proper names are denoted by a corresponding SI name with an attached prefix "ab" or with a separate abbreviation "emu".

The practical CGS system is a hybrid system that uses the volt and the ampere as the units of voltage and current respectively. Doing this avoids the inconveniently large and small electrical units that arise in the esu and emu systems. This system was at one time widely used by electrical engineers because the volt and ampere had been adopted as international standard units by the International Electrical Congress of 1881. As well as the volt and ampere, the farad (capacitance), ohm (resistance), coulomb (electric charge), and henry (inductance) are consequently also used in the practical system and are the same as the SI units. The magnetic units are those of the emu system.

The electrical units, other than the volt and ampere, are determined by the requirement that any equation involving only electrical and kinematical quantities that is valid in SI should also be valid in the system. For example, since electric field strength is voltage per unit length, its unit is the volt per centimetre, which is one hundred times the SI unit.

The system is electrically rationalized and magnetically unrationalized; i.e., and , but the above formula for "λ" is invalid. A closely related system is the International System of Electric and Magnetic Units, which has a different unit of mass so that the formula for "λ"′ is invalid. The unit of mass was chosen to remove powers of ten from contexts in which they were considered to be objectionable (e.g., and ). Inevitably, the powers of ten reappeared in other contexts, but the effect was to make the familiar joule and watt the units of work and power respectively.

The ampere-turn system is constructed in a similar way by considering magnetomotive force and magnetic field strength to be electrical quantities and rationalizing the system by dividing the units of magnetic pole strength and magnetization by 4π. The units of the first two quantities are the ampere and the ampere per centimetre respectively. The unit of magnetic permeability is that of the emu system, and the magnetic constitutive equations are and . Magnetic reluctance is given a hybrid unit to ensure the validity of Ohm's law for magnetic circuits.

There were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These include the Gaussian units and the Heaviside–Lorentz units.

In this table, "c" = is the dimensionless numeric value of the speed of light in vacuum when expressed in units of centimetres per second. The symbol "≘" is used instead of "=" as a reminder that the quantities are "corresponding" but not in general "equal", even between CGS variants. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10 "c") cm in ESU; "but" it is incorrect to replace "1 F" with "(10 "c") cm" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is "always" correct to replace "1 m" with "100 cm" within an equation or formula.)

Lack of unique unit names leads to potential confusion: "15 emu" may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. With its system of uniquely named units, the SI removes any confusion in usage: 1 ampere is a fixed value of a specified quantity, and so are 1 henry, 1 ohm, and 1 volt.

In the CGS-Gaussian system, electric and magnetic fields have the same units, 4"πε" is replaced by 1, and the only dimensional constant appearing in the Maxwell equations is "c", the speed of light. The Heaviside–Lorentz system has these properties as well (with "ε" equaling 1).

In SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4"π", those concerning coils of current and straight wires contain 2"π" and those dealing with charged surfaces lack "π" entirely, which was the most convenient choice for applications in electrical engineering and relates directly to the geometric symmetry of the system being described by the equation.

Specialized unit systems are used to simplify formulas further than either SI or CGS do, by eliminating constants through a convention of normalizing quantities with respect to some system of natural units. For example, in particle physics a system is in use where every quantity is expressed by only one unit of energy, the electronvolt, with lengths, times, and so on all converted into units of energy by inserting factors of speed of light "c" and the reduced Planck constant "ħ". This unit system is convenient for calculations in particle physics, but is impractical in other contexts.



Christology

In Christianity, Christology (from the Greek and ), translated from Greek as 'the study of Christ', is a branch of theology that concerns Jesus. Different denominations have different opinions on questions such as whether Jesus was human, divine, or both, and as a messiah what his role would be in the freeing of the Jewish people from foreign rulers or in the prophesied Kingdom of God, and in the salvation from what would otherwise be the consequences of sin.

The earliest Christian writings gave several titles to Jesus, such as Son of Man, Son of God, Messiah, and , which were all derived from Hebrew scripture. These terms centered around two opposing themes, namely "Jesus as a preexistent figure who becomes human and then returns to God", versus adoptionism – that Jesus was human who was "adopted" by God at his baptism, crucifixion, or resurrection.

From the second to the fifth centuries, the relation of the human and divine nature of Christ was a major focus of debates in the early church and at the first seven ecumenical councils. The Council of Chalcedon in 451 issued a formulation of the hypostatic union of the two natures of Christ, one human and one divine, "united with neither confusion nor division". Most of the major branches of Western Christianity and Eastern Orthodoxy subscribe to this formulation, while many branches of Oriental Orthodox Churches reject it, subscribing to miaphysitism.

"Christology" (from the Greek and ), literally 'the understanding of Christ', is the study of the nature (person) and work (role in salvation) of Jesus Christ. It studies Jesus Christ's humanity and divinity, and the relation between these two aspects; and the role he plays in salvation.

"Ontological Christology" analyzes the nature or being of Jesus Christ. "Functional Christology" analyzes the works of Jesus Christ, while "soteriological Christology" analyzes the "salvific" standpoints of Christology.

Several approaches can be distinguished within Christology. The term "Christology from above" or "high Christology" refers to approaches that include aspects of divinity, such as Lord and Son of God, and the idea of the pre-existence of Christ as the "Logos" ('the Word'), as expressed in the . These approaches interpret the works of Christ in terms of his divinity. According to Pannenberg, Christology from above "was far more common in the ancient Church, beginning with Ignatius of Antioch and the second century Apologists." The term "Christology from below" or "low Christology" refers to approaches that begin with the human aspects and the ministry of Jesus (including the miracles, parables, etc.) and move towards his divinity and the mystery of incarnation.

A basic Christological teaching is that the person of Jesus Christ is both human and divine. The human and divine natures of Jesus Christ apparently ("prosopic") form a duality, as they coexist within one person ("hypostasis"). There are no direct discussions in the New Testament regarding the dual nature of the Person of Christ as both divine and human, and since the early days of Christianity, theologians have debated various approaches to the understanding of these natures, at times resulting in ecumenical councils, and schisms.

Some historical christological doctrines gained broad support:

Influential Christologies which were broadly condemned as heretical are:

Various church councils, mainly in the 4th and 5th centuries, resolved most of these controversies, making the doctrine of the Trinity orthodox in nearly all branches of Christianity. Among them, only the Dyophysite doctrine was recognized as true and not heretical, belonging to the Christian orthodoxy and deposit of faith.

In Christian theology, atonement is the method by which human beings can be reconciled to God through Christ's sacrificial suffering and death. Atonement is the forgiving or pardoning of sin in general and original sin in particular through the suffering, death and resurrection of Jesus, enabling the reconciliation between God and his creation. Due to the influence of Gustaf Aulèn's (1879–1978) (1931), the various theories or paradigmata of atonement are often grouped as "classical paradigm", "objective paradigm", and the "subjective paradigm":

Other theories are the "embracement theory" and the "shared atonement" theory.

The earliest christological reflections were shaped by both the Jewish background of the earliest Christians, and by the Greek world of the eastern Mediterranean in which they operated. The earliest Christian writings give several titles to Jesus, such as Son of Man, Son of God, Messiah, and "Kyrios", which were all derived from Hebrew scripture. According to Matt Stefon and Hans J. Hillerbrand:

Historically in the Alexandrian school of thought (fashioned on the Gospel of John), Jesus Christ is the eternal "Logos" who already possesses unity with the Father before the act of Incarnation. In contrast, the Antiochian school viewed Christ as a single, unified human person apart from his relationship to the divine.

The notion of pre-existence is deeply rooted in Jewish thought, and can be found in apocalyptic thought and among the rabbis of Paul's time, but Paul was most influenced by Jewish-Hellenistic wisdom literature, where Wisdom' is extolled as something existing before the world and already working in creation. According to Witherington, Paul "subscribed to the christological notion that Christ existed prior to taking on human flesh[,] founding the story of Christ[...] on the story of divine Wisdom".

The title "Kyrios" for Jesus is central to the development of New Testament Christology. In the Septuagint it translates the Tetragrammaton, the holy Name of God. As such, it closely links Jesus with God – in the same way a verse such as Matthew 28:19, "The Name (singular) of the Father, the Son, and the Holy Spirit".

"Kyrios" is also conjectured to be the Greek translation of Aramaic , which in everyday Aramaic usage was a very respectful form of polite address, which means more than just 'teacher' and was somewhat similar to 'rabbi'. While the term expressed the relationship between Jesus and his disciples during his life, the Greek "Kyrios" came to represent his lordship over the world.

The early Christians placed "Kyrios" at the center of their understanding, and from that center attempted to understand the other issues related to the Christian mysteries. The question of the deity of Christ in the New Testament is inherently related to the "Kyrios" title of Jesus used in the early Christian writings and its implications for the absolute lordship of Jesus. In early Christian belief, the concept of "Kyrios" included the pre-existence of Christ, for they believed if Christ is one with God, he must have been united with God from the very beginning.

Two fundamentally different Christologies developed in the early Church, namely a "low" or adoptionist Christology, and a "high" or "incarnation" Christology. The chronology of the development of these early Christologies is a matter of debate within contemporary scholarship.

The "low Christology" or "adoptionist Christology" is the belief "that God exalted Jesus to be his Son by raising him from the dead", thereby raising him to "divine status". According to the "evolutionary model" or evolutionary theories, the Christological understanding of Jesus developed over time, as witnessed in the Gospels, with the earliest Christians believing that Jesus was a human who was exalted, or else adopted as God's Son, when he was resurrected. Later beliefs shifted the exaltation to his baptism, birth, and subsequently to the idea of his pre-existence, as witnessed in the Gospel of John. This "evolutionary model" was proposed by proponents of the , especially Wilhelm Bousset's influential "Kyrios Christos" (1913). This evolutionary model was very influential, and the "low Christology" has long been regarded as the oldest Christology.

The other early Christology is "high Christology", which is "the view that Jesus was a pre-existent divine being who became a human, did the Father's will on earth, and then was taken back up into heaven whence he had originally come", and from where he appeared on earth. According to Bousset, this "high Christology" developed at the time of Paul's writing, under the influence of Gentile Christians, who brought their pagan Hellenistic traditions to the early Christian communities, introducing divine honours to Jesus. According to Casey and Dunn, this "high Christology" developed after the time of Paul, at the end of the first century CE when the Gospel of John was written.

Since the 1970s, these late datings for the development of a "high Christology" have been contested, and a majority of scholars argue that this "high Christology" existed already before the writings of Paul. According to the "New ", or the Early High Christology Club, which includes Martin Hengel, Larry Hurtado, N. T. Wright, and Richard Bauckham, this "incarnation Christology" or "high Christology" did not evolve over a longer time, but was a "big bang" of ideas which were already present at the start of Christianity, and took further shape in the first few decades of the church, as witnessed in the writings of Paul. Some 'Early High Christology' proponents scholars argue that this "high Christology" may go back to Jesus himself.

There is a controversy regarding whether Jesus himself claimed to be divine. In "Honest to God", then-Bishop of Woolwich, John A. T. Robinson, questioned the idea. John Hick, writing in 1993, mentioned changes in New Testament studies, citing "broad agreement" that scholars do not today support the view that Jesus claimed to be God, quoting as examples Michael Ramsey (1980), C. F. D. Moule (1977), James Dunn (1980), Brian Hebblethwaite (1985) and David Brown (1985). Larry Hurtado, who argues that the followers of Jesus within a very short period developed an exceedingly high level of devotional reverence to Jesus, at the same time rejects the view that Jesus made a claim to messiahship or divinity to his disciples during his life as "naive and ahistorical". According to Gerd Lüdemann, the broad consensus among modern New Testament scholars is that the proclamation of the divinity of Jesus was a development within the earliest Christian communities. N. T. Wright points out that arguments over the claims of Jesus regarding divinity have been passed over by more recent scholarship, which sees a more complex understanding of the idea of God in first century Judaism. However, Andrew Loke argues that if Jesus did not claim and show himself to be truly divine and rise from the dead, the earliest Christian leaders who were devout ancient monotheistic Jews would have regarded Jesus as merely a teacher or a prophet; they would not have come to the widespread agreement that he was truly divine, which they did.

The study of the various Christologies of the Apostolic Age is based on early Christian documents.

The oldest Christian sources are the writings of Paul. The central Christology of Paul conveys the notion of Christ's pre-existence and the identification of Christ as "Kyrios". Both notions already existed before him in the early Christian communities, and Paul deepened them and used them for preaching in the Hellenistic communities.

What exactly Paul believed about the nature of Jesus cannot be determined decisively. In Philippians 2, Paul states that Jesus was preexistent and came to Earth "by taking the form of a servant, being made in human likeness". This sounds like an incarnation Christology. In Romans 1:4, however, Paul states that Jesus "was declared with power to be the Son of God by his resurrection from the dead", which sounds like an adoptionistic Christology, where Jesus was a human being who was "adopted" after his death. Different views would be debated for centuries by Christians and finally settled on the idea that he was both fully human and fully divine by the middle of the 5th century in the Council of Ephesus. Paul's thoughts on Jesus' teachings, versus his nature and being, are more defined, in that Paul believed Jesus was sent as an atonement for the sins of everyone.

The Pauline epistles use "Kyrios" to identify Jesus almost 230 times, and express the theme that the true mark of a Christian is the confession of Jesus as the true Lord. Paul viewed the superiority of the Christian revelation over all other divine manifestations as a consequence of the fact that Christ is the Son of God.

The Pauline epistles also advanced the "cosmic Christology" later developed in the Gospel of John, elaborating the cosmic implications of Jesus' existence as the Son of God: "Therefore, if anyone is in Christ, he is a new creation. The old has passed away; behold, the new has come." Paul writes that Christ came to draw all back to God: "Through him God was pleased to reconcile to himself all things, whether on earth or in heaven" (Colossians 1:20); in the same epistle, he writes that "He is the image of the invisible God, the firstborn of all creation" (Colossians 1:15).

The synoptic Gospels date from after the writings of Paul. They provide episodes from the life of Jesus and some of his works, but the authors of the New Testament show little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life, and as in , the Gospels do not claim to be an exhaustive list of his works.

Christologies that can be gleaned from the three Synoptic Gospels generally emphasize the humanity of Jesus, his sayings, his parables, and his miracles. The Gospel of John provides a different perspective that focuses on his divinity. The first 14 verses of the Gospel of John are devoted to the divinity of Jesus as the "Logos", usually translated as "Word", along with his pre-existence, and they emphasize the cosmic significance of Christ, e.g.: "All things were made through him, and without him was not any thing made that was made." In the context of these verses, the Word made flesh is identical with the Word who was in the beginning with God, being exegetically equated with Jesus.

Following the Apostolic Age, from the second century onwards, a number of controversies developed about how the human and divine are related within the person of Jesus. As of the second century, a number of different and opposing approaches developed among various groups. In contrast to prevailing monoprosopic views on the Person of Christ, alternative dyoprosopic notions were also promoted by some theologians, but such views were rejected by the ecumenical councils. For example, Arianism did not endorse divinity, Ebionism argued Jesus was an ordinary mortal, while Gnosticism held docetic views which argued Christ was a spiritual being who only appeared to have a physical body. The resulting tensions led to schisms within the church in the second and third centuries, and ecumenical councils were convened in the fourth and fifth centuries to deal with the issues.

Although some of the debates may seem to various modern students to be over a theological iota, they took place in controversial political circumstances, reflecting the relations of temporal powers and divine authority, and certainly resulted in schisms, among others that separated the Church of the East from the Church of the Roman Empire.

In 325, the First Council of Nicaea defined the persons of the Godhead and their relationship with one another, decisions which were ratified at the First Council of Constantinople in 381. The language used was that the one God exists in three persons (Father, Son, and Holy Spirit); in particular, it was affirmed that the Son was "homoousios" (of the same being) as the Father. The Nicene Creed declared the full divinity and full humanity of Jesus. After the First Council of Nicaea in 325 the "Logos" and the second Person of the Trinity were being used interchangeably.

In 431, the First Council of Ephesus was initially called to address the views of Nestorius on Mariology, but the problems soon extended to Christology, and schisms followed. The 431 council was called because in defense of his loyal priest Anastasius, Nestorius had denied the "Theotokos" title for Mary and later contradicted Proclus during a sermon in Constantinople. Pope Celestine I (who was already upset with Nestorius due to other matters) wrote about this to Cyril of Alexandria, who orchestrated the council. During the council, Nestorius defended his position by arguing there must be two persons of Christ, one human, the other divine, and Mary had given birth only to a human, hence could not be called the "Theotokos", i.e. "the one who gives birth to God". The debate about the single or dual nature of Christ ensued in Ephesus.

The First Council of Ephesus debated miaphysitism (two natures united as one after the hypostatic union) versus dyophysitism (coexisting natures after the hypostatic union) versus monophysitism (only one nature) versus Nestorianism (two hypostases). From the Christological viewpoint, the council adopted ('but being made one', ) – Council of Ephesus, Epistle of Cyril to Nestorius, i.e. 'one nature of the Word of God incarnate' (, ). In 451, the Council of Chalcedon affirmed dyophysitism. The Oriental Orthodox rejected this and subsequent councils and continued to consider themselves as "miaphysite" according to the faith put forth at the Councils of Nicaea and Ephesus. The council also confirmed the "Theotokos" title and excommunicated Nestorius.

The 451 Council of Chalcedon was highly influential, and marked a key turning point in the christological debates. It is the last council which many Lutherans, Anglicans and other Protestants consider ecumenical.

The Council of Chalcedon fully promulgated the Western dyophysite understanding put forth by Pope Leo I of Rome of the "hypostatic union", the proposition that Christ has one human nature "(physis)" and one divine nature "(physis)", each distinct and complete, and united with neither confusion nor division. Most of the major branches of Western Christianity (Roman Catholicism, Anglicanism, Lutheranism, and Reformed), Church of the East, Eastern Catholicism and Eastern Orthodoxy subscribe to the Chalcedonian Christological formulation, while many branches of Oriental Orthodox Churches (Syrian Orthodoxy, Coptic Orthodoxy, Ethiopian Orthodoxy, and Armenian Apostolicism) reject it.

Although the Chalcedonian Creed did not put an end to all christological debate, it did clarify the terms used and became a point of reference for many future Christologies. But it also broke apart the church of the Eastern Roman Empire in the fifth century, and unquestionably established the primacy of Rome in the East over those who accepted the Council of Chalcedon. This was reaffirmed in 519, when the Eastern Chalcedonians accepted the Formula of Hormisdas, anathematizing all of their own Eastern Chalcedonian hierarchy, who died out of communion with Rome from 482 to 519.

The Second Council of Constantinople in 553 interpreted the decrees of Chalcedon, and further explained the relationship of the two natures of Jesus. It also condemned the alleged teachings of Origen on the pre-existence of the soul, and other topics.

The Third Council of Constantinople in 681 declared that Christ has two wills of his two natures, human and divine, contrary to the teachings of the Monothelites, with the divine will having precedence, leading and guiding the human will.

The Second Council of Nicaea was called under the Empress Regent Irene of Athens in 787, known as the second of Nicaea. It supports the veneration of icons while forbidding their worship. It is often referred to as "The Triumph of Orthodoxy".

The Franciscan piety of the 12th and 13th centuries led to "popular Christology". Systematic approaches by theologians, such as Thomas Aquinas, are called "scholastic Christology".

In the 13th century, Thomas Aquinas provided the first systematic Christology that consistently resolved a number of the existing issues. In his Christology from above, Aquinas also championed the principle of perfection of Christ's human attributes.

The Middle Ages also witnessed the emergence of the "tender image of Jesus" as a friend and a living source of love and comfort, rather than just the "Kyrios" image.

John Calvin maintained there was no human element in the Person of Christ which could be separated from the Person of The Word. Calvin also emphasized the importance of the "Work of Christ" in any attempt at understanding the Person of Christ and cautioned against ignoring the Works of Jesus during his ministry.

The 19th century saw the rise of Liberal Protestant theology, which questioned the dogmatic foundations of Christianity, and approached the Bible with critical-historical tools. The divinity of Jesus was problematized, and replaced with an emphasis on the ethical aspects of his teachings.

Catholic theologian Karl Rahner sees the purpose of modern Christology as to formulate the Christian belief that "God became man and that God-made-man is the individual Jesus Christ" in a manner that this statement can be understood consistently, without the confusions of past debates and mythologies. Rahner pointed out the coincidence between the Person of Christ and the Word of God, referring to and which state whoever is ashamed of the words of Jesus is ashamed of the Lord himself.

Hans von Balthasar argued the union of the human and divine natures of Christ was achieved not by the "absorption" of human attributes, but by their "assumption". Thus, in his view, the divine nature of Christ was not affected by the human attributes and remained forever divine.

The Nativity of Jesus impacted the Christological issues about his person from the earliest days of Christianity. Luke's Christology centers on the dialectics of the dual natures of the earthly and heavenly manifestations of existence of the Christ, while Matthew's Christology focuses on the mission of Jesus and his role as the savior. The salvific emphasis of later impacted the theological issues and the devotions to Holy Name of Jesus.

The accounts of the crucifixion and subsequent resurrection of Jesus provides a rich background for christological analysis, from the canonical Gospels to the Pauline Epistles.

A central element in the christology presented in the Acts of the Apostles is the affirmation of the belief that the death of Jesus by crucifixion happened "with the foreknowledge of God, according to a definite plan". In this view, as in , the cross is not viewed as a scandal, for the crucifixion of Jesus "at the hands of the lawless" is viewed as the fulfilment of the plan of God.

Paul's Christology has a specific focus on the death and resurrection of Jesus. For Paul, the crucifixion of Jesus is directly related to his resurrection and the term "the cross of Christ" used in Galatians 6:12 may be viewed as his abbreviation of the message of the Gospels. For Paul, the crucifixion of Jesus was not an isolated event in history, but a cosmic event with significant eschatological consequences, as in 1 Corinthians 2:8. In the Pauline view, Jesus, obedient to the point of death (Philippians 2:8), died "at the right time" (Romans 5:6) based on the plan of God. For Paul, the "power of the cross" is not separable from the resurrection of Jesus.

The threefold office (Latin ) of Jesus Christ is a Christian doctrine based upon the teachings of the Old Testament. It was described by Eusebius and more fully developed by John Calvin. It states that Jesus Christ performed three functions (or "offices") in his earthly ministry – those of prophet, priest, and king. In the Old Testament, the appointment of someone to any of these three positions could be indicated by anointing him or her by pouring oil over the head. Thus, the term "messiah", meaning "anointed one", is associated with the concept of the threefold office. While the office of king is that most frequently associated with the Messiah, the role of Jesus as priest is also prominent in the New Testament, being most fully explained in chapters 7 to 10 of the Book of Hebrews.

Some Christians, notably Roman Catholics, view Mariology as a key component of Christology. In this view, not only is Mariology a logical and necessary consequence of Christology, but without it, Christology is incomplete, since the figure of Mary contributes to a fuller understanding of who Christ is and what he did.

Protestants have criticized Mariology because many of its assertions lack any Biblical foundation. Strong Protestant reaction against Roman Catholic Marian devotion and teaching has been a significant issue for ecumenical dialogue.

Joseph Cardinal Ratzinger (later Pope Benedict XVI) expressed this sentiment about Roman Catholic Mariology when in two separate occasions he stated, "The appearance of a truly Marian awareness serves as the touchstone indicating whether or not the christological substance is fully present" and "It is necessary to go back to Mary, if we want to return to the truth about Jesus Christ."










Complaint

In legal terminology, a complaint is any formal legal document that sets out the facts and legal reasons (see: cause of action) that the filing party or parties (the plaintiff(s)) believes are sufficient to support a claim against the party or parties against whom the claim is brought (the defendant(s)) that entitles the plaintiff(s) to a remedy (either money damages or injunctive relief). For example, the Federal Rules of Civil Procedure (FRCP) that govern civil litigation in United States courts provide that a civil action is commenced with the filing or service of a pleading called a complaint. Civil court rules in states that have incorporated the Federal Rules of Civil Procedure use the same term for the same pleading. 

In Civil Law, a "complaint" is the first formal action taken to officially begin a lawsuit. This written document contains the allegations against the defense, the specific laws violated, the facts that led to the dispute, and any demands made by the plaintiff to restore justice.

In some jurisdictions, specific types of criminal cases may also be commenced by the filing of a complaint, also sometimes called a criminal complaint or felony complaint. Most criminal cases are prosecuted in the name of the governmental authority that promulgates criminal statutes and enforces the police power of the state with the goal of seeking criminal sanctions, such as the State (also sometimes called the People) or Crown (in Commonwealth realms). In the United States, the complaint is often associated with misdemeanor criminal charges presented by the prosecutor without the grand jury process. In most U.S. jurisdictions, the charging instrument presented to and authorized by a grand jury is referred to as an indictment.

Virtually every U.S. state has some forms available on the web for most common complaints for lawyers and self-representing litigants; if a petitioner cannot find an appropriate form in their state, they often can modify a form from another state to fit his or her request. Several United States federal courts publish general guidelines for the petitioners and Civil Rights complaint forms.

A complaint generally has the following structural elements:

After the complaint has been filed with the court, it has to be properly served to the opposite parties, but usually petitioners are not allowed to serve the complaint personally. The court also can issue a summons – an official summary document which the plaintiff needs to have served together with the complaint. The defendants have limited time to respond, depending on the State or Federal rules. A defendant's failure to answer a complaint can result in a default judgment in favor of the petitioner.

For example, in United States federal courts, any person who is at least 18 years old and not a party may serve a summons and complaint in a civil case. The defendant must submit an answer within 21 days after being served with the summons and complaint, or request a waiver, according to FRCP Rule 12. After the civil complaint has been served to the defendants, the plaintiff must, as soon as practicable initiate a conference between the parties to plan for the rest of the discovery process and then the parties should submit a proposed discovery plan to the judge within 14 days after the conference.

In many U.S. jurisdictions, a complaint submitted to a court must be accompanied by a Case Information Statement, which sets forth specific key information about the case and the lawyers representing the parties. This allows the judge to make determinations about which deadlines to set for different phases of the case, as it moves through the court system.

There are also freely accessible web search engines to assist parties in finding court decisions that can be cited in the complaint as an example or analogy to resolve similar questions of law. Google Scholar is the biggest database of full text state and federal courts decisions that can be accessed without charge. These web search engines often allow one to select specific state courts to search.

Federal courts created the Public Access to Court Electronic Records (PACER) system to obtain case and docket information from the United States district courts, United States courts of appeals, and United States bankruptcy courts. The system is managed by the Administrative Office of the United States Courts; it allows lawyers and self-represented clients to obtain documents entered in the case much faster than regular mail.

In addition to Federal Rules of Civil Procedure, many of the U.S. district courts have developed their own requirements included in Local Rules for filing with the Court. Local Rules can set up a limit on the number of pages, establish deadlines for motions and responses, explain whether it is acceptable to combine a motion petition with a response, specify if a judge needs an additional copy of the documents (called "judge’s copy"), etc. Local Rules can define page layout elements like: margins, text font/size, distance between lines, mandatory footer text, page numbering, and provide directions on how the pages need to be bound together – i.e. acceptable fasteners, number and location of fastening holes, etc. If the filed motion does not comply with the Local Rules then the judge can choose to strike the motion completely, or order the party to re-file its motion, or grant a special exception to the Local Rules.

According to Federal Rules of Civil Procedure (FRCP) , sensitive text like Social Security number, Taxpayer Identification Number, birthday, bank accounts and children’s names, should be redacted from the filings made with the court and accompanying exhibits, (exhibits normally do not need to be attached to the original complaint, but should be presented to Court after the discovery). The redacted text can be erased with black-out or white-out, and the page should have an indication that it was redacted - most often by stamping word "redacted" on the bottom. Alternately, the filing party may ask the court’s permission to file some exhibits completely under seal. A minor's name of the petitions should be replaced with initials.

A person making a redacted filing can file an unredacted copy under seal, or the Court can choose to order later that an additional filing be made under seal without redaction. Copies of both redacted and unredacted documents filed with court should be provided to the other parties in the case. Some courts also require that an additional electronic courtesy copy be emailed to the other parties.

Before filing the complaint, it is important for plaintiff(s) to remember that Federal courts can impose liability for the prevailing party's attorney fees to the losing party, if the judge considers the case frivolous or for purposes of harassment, even when the case was voluntarily dismissed. In the case of "Fox v. Vice", the U.S. Supreme Court held that reasonable attorneys' fees could be awarded to the defendant under 42 U.S.C. Sec. 1988, but only for costs that the defendant would not have incurred "but for the frivolous claims." Even when there is no actual trial or judgment, if there is only pre-trial motion practice such as motions to dismiss, attorney fee shifting still can be awarded under FRCP Rule 11 when the opposing party files a Motion for Sanctions and the court issue an order identifying the sanctioned conduct and the basis for the sanction. The losing party has a right to appeal any order for sanctions in the higher court. In the state courts, each party is generally responsible only for its own attorney fees, with certain exceptions.

In 1883, the Rules of the Supreme Court replaced the term "complaint" with "statement of claim". This was then replaced in 1998 with "particulars of claim" by the Civil Procedure Rules, which also replaced the word "plaintiff" with "claimant" as part of a drastic reform of English legal terminology. Thus, in England and Wales, a claimant now initiates a claim by filing a claim form (instead of a writ of summons), and either pleads particulars of claim on the claim form itself or as a separate document.



Casimir III the Great

Casimir III the Great (; 30 April 1310 – 5 November 1370) reigned as the King of Poland from 1333 to 1370. He also later became King of Ruthenia in 1340, and fought to retain the title in the Galicia-Volhynia Wars. He was the last Polish king from the Piast dynasty.

Casimir inherited a kingdom weakened by war and made it prosperous and wealthy. He reformed the Polish army and doubled the size of the kingdom. He reformed the judicial system and introduced a legal code, gaining the title "the Polish Justinian". Casimir built extensively and founded the Jagiellonian University (back then simply called the University of Krakow), the oldest Polish university and one of the oldest in the world. He also confirmed privileges and protections previously granted to Jews and encouraged them to settle in Poland in great numbers.

Casimir left no legitimate sons. When he died in 1370 from an injury received while hunting, his nephew, King Louis I of Hungary, succeeded him as king of Poland in personal union with Hungary.

Casimir was born on 30 April 1310 in Kowal, Kuyavia, the third son of Ladislaus the Short and Jadwiga of Kalisz. He had two brothers who died in infancy and three sisters: Kunegunda, Elżbieta, and Jadwiga. When Casimir attained the throne in 1333, his position was in danger, as his neighbours did not recognise his title and instead called him "king of Kraków". The kingdom was depopulated and exhausted by war, and the economy was ruined. In 1335, in the Treaty of Trentschin, Casimir was forced to relinquish his claims to Silesia "in perpetuity".

Casimir began to rebuild the country and strengthen its defenses. During his reign, nearly 30 towns were supplied with fortification walls and some 50 castles were constructed, including castles along the Trail of the Eagle's Nests. These achievements are still celebrated today, in a commonly-known ditty that translates as follows: "inherited wooden towns and left them fortified with stone and brick" (Kazimierz Wielki zastał Polskę drewnianą, a zostawił murowaną).

He organized a meeting of kings in Kraków in 1364 at which he exhibited the wealth of the Polish kingdom. Casimir is the only king in Polish history to both receive and retain the title of "Great", as Bolesław I is more commonly known as "the Brave".

Casimir ensured stability and great prospects for the future of the country. He established the Corona Regni Poloniae – the Crown of the Polish Kingdom, which certified the existence of the Polish lands independently from the monarch. Prior to that, the lands were only the property of the Piast dynasty.

At the Sejm in Wiślica, on 11 March 1347, Casimir introduced reforms to the Polish judicial system and sanctioned civil and criminal codes for Great and Lesser Poland, earning the title "the Polish Justinian". In 1364, having received permission from Pope Urban V, Casimir established the University of Kraków, now the oldest university in Poland. It was regarded as a rare distinction, since it was only the second university founded in Central Europe, after the Charles University in Prague.

Casimir demonstrated competence in foreign diplomacy and managed to double the size of his kingdom. He neutralized relations with potential enemies to the west and north, and began to expand his territory eastward. He conquered the Ruthenian kingdom of Halych and Volodymyr (a territory in the modern-day Ukraine), known in Polish history as Red Ruthenia and Volhynia. By extending the borders far south-east, the Polish kingdom gained access to the lucrative Black Sea trade.

In 1355, in Buda, Casimir designated his nephew Louis I of Hungary as his successor should he produce no male heir, just as his father had with Charles I of Hungary to gain help against Bohemia. In exchange Casimir gained a favourable Hungarian attitude, needed in disputes with the hostile Teutonic Order and the Kingdom of Bohemia. At the time Casimir was 45 years old, and so producing a son did not seem unreasonable.

Casimir left no legal son, however, begetting five daughters instead. He tried to adopt his grandson, Casimir IV, Duke of Pomerania, in his last will. The child had been born to his eldest daughter, Elisabeth, Duchess of Pomerania, in 1351. This part of the testament was invalidated by Louis I of Hungary, however, who had traveled to Kraków quickly after Casimir died (in 1370) and bribed the nobles with future privileges. Casimir III also had a son-in-law, Louis VI of Bavaria, Margrave and Prince-elector of Brandenburg, who was considered a possible successor, but he was deemed ineligible as his wife, Casimir's daughter Cunigunde, had died in 1357 without issue.
Thus King Louis I of Hungary became successor in Poland. Louis was proclaimed king upon Casimir's death in 1370, though Casimir's sister Elisabeth (Louis's mother) held much of the real power until her death in 1380.

Casimir was facetiously named "the Peasants' King". He introduced the codes of law of Greater and Lesser Poland as an attempt to end the overwhelming superiority of the nobility. During his reign all three major classes — the nobility, priesthood, and bourgeoisie — were more or less counterbalanced, allowing Casimir to strengthen his monarchic position. He was known for siding with the weak when the law did not protect them from nobles and clergymen. He reportedly even supported a peasant whose house had been demolished by his own mistress, after she had ordered it to be pulled down because it disturbed her enjoyment of the beautiful landscape.

His popularity with the peasants helped to rebuild the country, as part of the reconstruction program was funded by a land tax paid by the lower social class.

On 9 October 1334, Casimir confirmed the privileges granted to Jews in 1264 by Bolesław V the Chaste. Under penalty of death, he prohibited the kidnapping of Jewish children for the purpose of enforced Christian baptism, and he inflicted heavy punishment for the desecration of Jewish cemeteries. While Jews had lived in Poland since before his reign, Casimir allowed them to settle in Poland in great numbers and protected them as "people of the king". About 70 percent of the world's European Jews, or Ashkenazi, can trace their ancestry to Poland due to Casimir's reforms. Casimir's legendary Jewish mistress Esterka remains unconfirmed by direct historical evidence.

Casimir III was married four times:

On 30 April or 16 October 1325, Casimir married Aldona of Lithuania, daughter of Grand Duke Gediminas of Lithuania and Jewna. They had:

Aldona died on 26 May 1339. Casimir remained a widower for two years.

On 29 September 1341, Casimir married his second wife, Adelaide of Hesse. She was a daughter of Henry II, Landgrave of Hesse, and Elizabeth of Meissen. They had no children. Casimir started living separately from Adelaide soon after the marriage. Their loveless marriage lasted until 1356, when he declared himself divorced.

After Casimir "divorced" Adelaide he married his mistress Christina Rokiczana, the widow of Miklusz Rokiczani, a wealthy merchant. Her own origins are unknown. Following the death of her first husband she had entered the court of Bohemia in Prague as a lady-in-waiting. Casimir brought her with him from Prague and convinced the abbot of the Benedictine abbey of Tyniec to marry them. The marriage was held in a secret ceremony but soon became known. Queen Adelaide renounced it as bigamous and returned to Hesse. Casimir continued living with Christine despite complaints by Pope Innocent VI on behalf of Queen Adelaide. This marriage lasted until 1363–64 when Casimir again declared himself divorced. They had no children.

In about 1365, Casimir married his fourth wife Hedwig of Żagań. She was a daughter of Henry V of Iron, Duke of Żagań and Anna of Mazovia. They had three children:

As Adelheid was still alive (and possibly Christina as well), the marriage to Hedwig was also considered bigamous. Because of this, the legitimacy of his three young daughters was disputed. Casimir managed to have Anna and Kunigunde legitimated by Pope Urban V on 5 December 1369. Jadwiga the younger was legitimated by Pope Gregory XI on 11 October 1371 (after Casimir's death).

Casimir's full title was: "Casimir by the grace of God king of Poland and Rus' (Ruthenia), lord and heir of the land of Kraków, Sandomierz, Sieradz, Łęczyca, Kuyavia, Pomerania (Pomerelia)". The title in Latin was: "Kazimirus, Dei gratia rex Polonie et Russie, nec non Cracovie, Sandomirie, Siradie, Lancicie, Cuiavie, et Pomeranieque Terrarum et Ducatuum Dominus et Heres."






 

Complexity

Complexity characterises the behaviour of a system or model whose components interact in multiple ways and follow local rules, leading to non-linearity, randomness, collective dynamics, hierarchy, and emergence.

The term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.

The intuitive criterion of complexity can be formulated as follows: a system would be more complex if more parts could be distinguished, and if more connections between them existed.

, a number of approaches to characterizing complexity have been used in science; Zayed "et al."
reflect many of these. Neil Johnson states that "even among scientists, there is no unique definition of complexity – and the scientific notion has traditionally been conveyed using particular examples..." Ultimately Johnson adopts the definition of "complexity science" as "the study of the phenomena which emerge from a collection of interacting objects".

Definitions of complexity often depend on the concept of a "system" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.

Warren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.
Phenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront "dealing simultaneously with a sizable number of factors which are interrelated into an organic whole". Weaver's 1948 paper has influenced subsequent thinking about complexity.

The approaches that embody concepts of systems, multiple elements, multiple relational regimes, and state spaces might be summarized as implying that complexity arises from the number of distinguishable relational regimes (and their associated state spaces) in a defined system.

Some definitions relate to the algorithmic basis for the expression of a complex phenomenon or model or mathematical expression, as later set out herein.

One of the problems in addressing complexity issues has been formalizing the intuitive conceptual distinction between the large number of variances in relationships extant in random collections, and the sometimes large, but smaller, number of relationships between elements in systems where constraints (related to correlation of otherwise independent elements) simultaneously reduce the variations from element independence and create distinguishable regimes of more-uniform, or correlated, relationships, or interactions.

Weaver perceived and addressed this problem, in at least a preliminary way, in drawing a distinction between "disorganized complexity" and "organized complexity".

In Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a "disorganized complexity" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.

A prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits – the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.

Organized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity in regards to other systems, rather than the subject system, can be said to "emerge," without any "guiding hand".

The number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.

There are generally rules which can be invoked to explain the origin of complexity in a given system.

The source of disorganized complexity is the large number of parts in the system of interest, and the lack of correlation between elements in the system.

In the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.

Complexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.

In several scientific fields, "complexity" has a precise meaning:


Other fields introduce less precisely defined notions of complexity:


Complexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex – displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.

The use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting "stovepipes" and effective "integrated" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.

While this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains or social systems. One such interdisciplinary group of fields is relational order theories.

The behavior of a complex system is often said to be due to emergence and self-organization. Chaos theory has investigated the sensitivity of systems to variations in initial conditions as one cause of complex behaviour.

Recent developments in artificial life, evolutionary computation and genetic algorithms have led to an increasing emphasis on complexity and complex adaptive systems.

In social science, the study on the emergence of macro-properties from the micro-properties, also known as macro-micro view in sociology. The topic is commonly recognized as social complexity that is often related to the use of computer simulation in social science, i.e. computational sociology.

Systems theory has long been concerned with the study of complex systems (in recent times, "complexity theory" and "complex systems" have also been used as names of the field). These systems are present in the research of a variety disciplines, including biology, economics, social studies and technology. Recently, complexity has become a natural domain of interest of real world socio-cognitive systems and emerging systemics research. Complex systems tend to be high-dimensional, non-linear, and difficult to model. In specific circumstances, they may exhibit low-dimensional behaviour.

In information theory, algorithmic information theory is concerned with the complexity of strings of data.

Complex strings are harder to compress. While intuition tells us that this may depend on the codec used to compress a string (a codec could be theoretically created in any arbitrary language, including one in which the very small command "X" could cause the computer to output a very complicated string like "18995316"), any two Turing-complete languages can be implemented in each other, meaning that the length of two encodings in different languages will vary by at most the length of the "translation" language – which will end up being negligible for sufficiently large data strings.

These algorithmic measures of complexity tend to assign high values to random noise. However, under a certain understanding of complexity, arguably the most intuitive one, random noise is meaningless and so not complex at all.

Information entropy is also sometimes used in information theory as indicative of complexity, but entropy is also high for randomness. In the case of complex systems, information fluctuation complexity was designed so as not to measure randomness as complex and has been useful in many applications. More recently, a complexity metric was developed for images that can avoid measuring noise as complex by using the minimum description length principle.

There has also been interest in measuring the complexity of classification problems in supervised machine learning. This can be useful in meta-learning to determine for which data sets filtering (or removing suspected noisy instances from the training set) is the most beneficial and could be expanded to other areas.
For binary classification, such measures can consider the overlaps in feature values from differing classes, the separability of the classes, and measures of geometry, topology, and density of manifolds.

For non-binary classification problems, instance hardness is a bottom-up approach that first seeks to identify instances that are likely to be misclassified (assumed to be the most complex). The characteristics of such instances are then measured using supervised measures such as the number of disagreeing neighbors or the likelihood of the assigned class label given the input features.

A recent study based on molecular simulations and compliance constants describes molecular recognition as a phenomenon of organisation.
Even for small molecules like carbohydrates, the recognition process can not be predicted or designed even assuming that each individual hydrogen bond's strength is exactly known.

Driving from the law of requisite variety, Boisot and McKelvey formulated the ‘Law of Requisite Complexity’, that holds that, in order to be efficaciously adaptive, the internal complexity of a system must match the external complexity it confronts.

The application in project management of the Law of Requisite Complexity, as proposed by Stefan Morcov, is the analysis of positive, appropriate and negative complexity.

Project complexity is the property of a project which makes it difficult to understand, foresee, and keep under control its overall behavior, even when given reasonably complete information about the project system.

Maik Maurer considers complexity as a reality in engineering. He proposed a methodology for managing complexity in systems engineering :

                             1.           Define the system.

                             2.           Identify the type of complexity.

                             3.           Determine the strategy.

                             4.           Determine the method.

                             5.           Model the system.

                             6.           Implement the method.

Computational complexity theory is the study of the complexity of problems – that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved, as denoted in Big O notation, in time formula_1 (where "n" is the size of the network to visit – the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.

Even though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.

There exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.

There is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.

The concept of complexity is being increasingly used in the study of cosmology, big history, and cultural evolution with increasing granularity, as well as increasing quantification.

Eric Chaisson has advanced a cosmoglogical complexity metric which he terms Energy Rate Density. This approach has been expanded in various works, most recently applied to measuring evolving complexity of nation-states and their growing cities.


Chastity

Chastity, also known as purity, is a virtue related to temperance. Someone who is "chaste" refrains either from sexual activity that is considered immoral or from any sexual activity, according to their state of life. In some contexts, for example when making a vow of chastity, chastity means celibacy.

The words "chaste" and "chastity" stem from the Latin adjective ("cut off", "separated", "pure"). The words entered the English language around the middle of the 13th century. "Chaste" meant "virtuous", "pure from unlawful sexual intercourse" or (from the early 14th century on) as a noun, a virgin, while "chastity" meant "(sexual) purity".

Thomas Aquinas links (chastity) to the Latin verb ("chastise, reprimand, correct"), with a reference to Aristotle's "Nicomachean Ethics": "Chastity takes its name from the fact that reason 'chastises' concupiscence, which, like a child, needs curbing, as the Philosopher states".

For many Jews, Christians, and Muslims, people should restrict their acts of a sexual nature to the context of marriage. For unmarried people, chastity is equivalent to sexual abstinence. Sexual acts outside of or apart from marriage, such as adultery, fornication, masturbation, and prostitution, are considered immoral due to lust.

In many Christian traditions, chastity is synonymous with purity. The Catholic Church teaches that chastity involves, in the words of cardinal bishop Alfonso López Trujillo, "the successful integration of sexuality within the person and thus the inner unity of man in his bodily and spiritual being", which according to one's marital status requires either having no sexual relationship, or only having sexual relations with one's spouse. In Western Christian morality, chastity is placed opposite the deadly sin of lust, and is classified as one of seven virtues. The moderation of sexual desires is also required to be virtuous. Reason, will, and desire can harmoniously work together to do what is good.

As an emblem of inward chastity, some Christians choose to wear a cord, girdle or a cincture of one of the several Confraternities of the Cord or a purity ring. The cord is worn as a symbol of chastity in honour of a chaste saint whom the bearer asks for intercession. The purity ring is worn before holy matrimony by those who marry or for the rest of their lives by those who stay single.

In marriage, the spouses commit to a lifelong relationship that excludes sexual intimacy with other persons. A third form of chastity, often called "vidual chastity", is expected by the society for a period after the woman's husband dies. For example, Anglican Bishop Jeremy Taylor defined five rules in "Holy Living" (1650), including abstaining from marrying "so long as she is with child by her former husband" and "within the year of mourning".

In the Roman Catholic Church, members of the consecrated life vow or promise celibacy as one of the evangelical counsels. In 306, the Synod of Elvira proscribed clergy from marrying. This was unevenly enforced until the Second Lateran Council in 1139 when it found its way into canon law. Unmarried deacons promise celibacy to their local bishop when ordained.

Eastern Catholic priests are permitted to marry, provided they do so before ordination and outside monastic life.

"Vows of chastity" can be taken either as part of an organised religious life (such as Roman Catholic Beguines and Beghards in the past) or on an individual basis: as a voluntary act of devotion, or as part of an ascetic lifestyle (often devoted to contemplation), or both. Some Protestant religious communities, such as the Bruderhof, take vows of chastity as part of the church membership process.

Chastity is a central and pivotal concept in Roman Catholic praxis. Roman Catholic teaching regards chastity as essential in maintaining and cultivating the unity of body with spirit and thus the integrity of the human being. It is also fundamental to the practise of the Catholic life because it involves an "apprenticeship in self-mastery". By attaining mastery over one's passions, reason, will, and desire can harmoniously work together to do what is good.

The theology of the body of the Lutheran Churches emphasizes the role of the Holy Spirit, who sanctified the bodies of Christians to be God's temple.

Many Lutheran monks and Lutheran nuns practice celibacy, though in some Lutheran religious orders it is not compulsory.

In the Church of Jesus Christ of Latter-day Saints chastity is very important:

Physical intimacy between husband and wife is a beautiful and sacred part of God's plan for His children. It is an expression of love within marriage and allows husband and wife to participate in the creation of life. God has commanded that this sacred power be expressed only between a man and a woman who are legally married. The law of chastity applies to both men and women. It includes strict abstinence from sexual relations before marriage and complete fidelity and loyalty to one's spouse after marriage.

The law of chastity requires that sexual relations be reserved for marriage between a man and a woman.

In addition to reserving sexual intimacy for marriage, we obey the law of chastity by controlling our thoughts, words, and actions. Jesus Christ taught, "Ye have heard that it was said by them of old time, Thou shalt not commit adultery: but I say unto you, That whosoever looketh on a woman to lust after her hath committed adultery with her already in his heart" ()."

Teachings of the Church of Jesus Christ of Latter-Day Saints also include that sexual expression within marriage is an important dimension of spousal bonding apart from, but not necessarily avoiding, its procreative result.

The most famous personal example of chastity in the Quran is the Virgin Mary (Mariam):

Extramarital sex is forbidden. The Quran says:

In a list of commendable deeds the Quran says:

Because the sex desire is usually attained before a man is financially capable of marriage, the love to God and mindfulness of Him should be sufficient motive for chastity:

Chastity is mandatory in Islam. Sex outside legitimacy is prohibited, for both men and women, whether married or unmarried. The injunctions and forbiddings in Islam apply equally to men and women. The legal punishment for adultery is equal for men and women. 

The prophet's prescription to the youth was:
Chastity is an attitude and a way of life. In Islam it is both a personal and a social value. A Muslim society should not condone relations entailing or conducive to sexual license. Social patterns and practices calculated to inflame sexual desire are frowned upon by Islam, such incitements to immorality including permissive ideologies, titillating works of art, and the failure to inculcate sound moral principles in the young. At the heart of such a view of human sexuality lies the conviction that the notion of personal freedom should never be misconstrued as the freedom to flout God's laws by overstepping the bounds which, in His infinite wisdom, He has set upon the relations of the sexes.

Chastity is highly prized in the Baháʼí Faith. Similar to other Abrahamic religions, Baháʼí teachings call for the restriction of sexual activity to that between a wife and husband in Baháʼí marriage, and discourage members from using pornography or engaging in sexually explicit recreational activities. The concept of chastity is extended to include avoidance of alcohol and mind-altering drugs, profanity, and gaudy or immodest attire.

Hinduism's view on premarital sex is rooted in its concept of or the stages of life. The first of these stages, known as , roughly translates as chastity. Celibacy and chastity are considered the appropriate behavior for both male and female students during this stage, which precedes the stage of the married householder (). and Hindu monks or are also celibate as part of their ascetic discipline.

In Sikhism, premarital or extramarital sex is strictly forbidden. However, it is encouraged to marry and live as a family unit to provide and nurture children for the perpetual benefit of creation (as opposed to or living as a monk, which was, and remains, a common spiritual practice in India). A Sikh is encouraged not to live as a recluse, beggar, monk, nun, celibate, or in any similar vein.

The Jain ethical code contains the vow of (meaning "pure conduct"), which prescribes the expectations for Jains concerning sexual activity. is one of the five major and minor vows of Jainism, prescribing slightly different expectations for ascetics and laypeople, respectively.

Complete celibacy is expected only of Jain ascetics (who are also referred to as monks and nuns). For laypeople, chastity is expected, with extramarital sex and adultery being prohibited.

The teachings of Buddhism include the Noble Eightfold Path, comprising a division called right action. Under the Five Precepts ethical code, and lay followers should abstain from sexual misconduct, while and monastics should practice strict chastity.

The Five Precepts of the Taoist religion include "no sexual misconduct", which is interpreted as prohibiting extramarital sex for lay practitioners and marriage or sexual intercourse for monks and nuns.
In Iran, women are required to wear hijabs as part of that society's efforts to enforce chastity. In 2023 the Ministry of Culture and Islamic Guidance announced that there is a new Bureau of Chaste Living. 



Cosmic microwave background

The cosmic microwave background (CMB or CMBR) is microwave radiation that fills all space in the observable universe. It is a remnant that provides an important source of data on the primordial universe. With a standard optical telescope, the background space between stars and galaxies is almost completely dark. However, a sufficiently sensitive radio telescope detects a faint background glow that is almost uniform and is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of the CMB in 1965 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s.

CMB is landmark evidence of the Big Bang theory for the origin of the universe. In the Big Bang cosmological models, during the earliest periods, the universe was filled with an opaque fog of dense, hot plasma of sub-atomic particles. As the universe expanded, this plasma cooled to the point where protons and electrons combined to form neutral atoms of mostly hydrogen. Unlike the plasma, these atoms could not scatter thermal radiation by Thomson scattering, and so the universe became transparent. Known as the recombination epoch, this decoupling event released photons to travel freely through space – sometimes referred to as "relic radiation". However, the photons have grown less energetic due to the cosmological redshift associated with the expansion of the universe. The "surface of last scattering" refers to a shell at the right distance in space so photons are now received that were originally emitted at the time of decoupling.

The CMB is not completely smooth and uniform, showing a faint anisotropy that can be mapped by sensitive detectors. Ground and space-based experiments such as COBE and WMAP have been used to measure these temperature inhomogeneities. The anisotropy structure is determined by various interactions of matter and photons up to the point of decoupling, which results in a characteristic lumpy pattern that varies with angular scale. The distribution of the anisotropy across the sky has frequency components that can be represented by a power spectrum displaying a sequence of peaks and valleys. The peak values of this spectrum hold important information about the physical properties of the early universe: the first peak determines the overall curvature of the universe, while the second and third peak detail the density of normal matter and so-called dark matter, respectively. Extracting fine details from the CMB data can be challenging, since the emission has undergone modification by foreground features such as galaxy clusters.

Precise measurements of the CMB are critical to cosmology, since any proposed model of the universe must explain this radiation. The CMB has a thermal black body spectrum at a temperature of . The spectral radiance "dE"/"dν" peaks at 160.23 GHz, in the microwave range of frequencies, corresponding to a photon energy of about . Alternatively, if spectral radiance is defined as "dE"/"dλ", then the peak wavelength is 1.063 mm (282 GHz, photons). The glow is very nearly uniform in all directions, but the tiny residual variations show a very specific pattern, the same as that expected of a fairly uniformly distributed hot gas that has expanded to the current size of the universe. In particular, the spectral radiance at different angles of observation in the sky contains small anisotropies, or irregularities, which vary with the size of the region examined. They have been measured in detail, and match what would be expected if small thermal variations, generated by quantum fluctuations of matter in a very tiny space, had expanded to the size of the observable universe we see today. This is a very active field of study, with scientists seeking both better data (for example, the Planck spacecraft) and better interpretations of the initial conditions of expansion. Although many different processes might produce the general form of a black body spectrum, no model other than the Big Bang has yet explained the fluctuations. As a result, most cosmologists consider the Big Bang model of the universe to be the best explanation for the CMB.

The high degree of uniformity throughout the observable universe and its faint but measured anisotropy lend strong support for the Big Bang model in general and the ΛCDM ("Lambda Cold Dark Matter") model in particular. Moreover, the fluctuations are coherent on angular scales that are larger than the apparent cosmological horizon at recombination. Either such coherence is acausally fine-tuned, or cosmic inflation occurred.<ref name="hep-ph/0309057"></ref>

Other than the temperature and polarization anisotropy, the CMB frequency spectrum is expected to feature tiny departures from the black-body law known as spectral distortions. These are also at the focus of an active research effort with the hope of a first measurement within the forthcoming decades, as they contain a wealth of information about the primordial universe and the formation of structures at late time.

The cosmic microwave background radiation is an emission of uniform, black body thermal energy coming from all parts of the sky. The radiation is isotropic to roughly one part in 100,000: the root mean square variations are only 18 μK, after subtracting out a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Sun relative to the comoving cosmic rest frame as it moves at 369.82 ± 0.11 km/s towards the constellation Leo (galactic longitude 264.021 ± 0.011, galactic latitude 48.253 ± 0.005). The CMB dipole and aberration at higher multipoles have been measured, consistent with galactic motion.

In the Big Bang model for the formation of the universe, inflationary cosmology predicts that after about 10 seconds the nascent universe underwent exponential growth that smoothed out nearly all irregularities. The remaining irregularities were caused by quantum fluctuations in the inflaton field that caused the inflation event. Long before the formation of stars and planets, the early universe was smaller, much hotter and, starting 10 seconds after the Big Bang, filled with a uniform glow from its white-hot fog of interacting plasma of photons, electrons, and baryons.

As the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000 K or when the universe was approximately 379,000 years old. As photons did not interact with these electrically neutral atoms, the former began to travel freely through space, resulting in the decoupling of matter and radiation.

The color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to , it will continue to drop as the universe expands. The intensity of the radiation corresponds to black-body radiation at 2.726 K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called "the surface of last scattering". This represents the set of locations in space at which the decoupling event is estimated to have occurred and at a point in time such that the photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background, making up a fraction of roughly of the total density of the universe.

Two of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature.

The energy density of the CMB is which yields about 411 photons/cm.

The cosmic microwave background was first predicted in 1948 by Ralph Alpher and Robert Herman, in close relation to work performed by Alpher's PhD advisor George Gamow. Alpher and Herman were able to estimate the temperature of the cosmic microwave background to be 5 K, though two years later they re-estimated it at 28 K. This high estimate was due to a misestimate of the Hubble constant by Alfred Behr, which could not be replicated and was later abandoned for the earlier estimate. Although there were several previous estimates of the temperature of space, these estimates had two flaws. First, they were measurements of the temperature of space and did not suggest that space was filled with a thermal Planck spectrum. Next, they depend on our being at a special spot at the edge of the Milky Way galaxy and they did not suggest the radiation is isotropic. The estimates would yield very different predictions if Earth happened to be located elsewhere in the universe.
The 1948 results of Alpher and Herman were discussed in many physics settings through about 1955, when both left the Applied Physics Laboratory at Johns Hopkins University. The mainstream astronomical community, however, was not intrigued at the time by cosmology. Alpher and Herman's prediction was rediscovered by Yakov Zel'dovich in the early 1960s, and independently predicted by Robert Dicke at the same time. The first published recognition of the CMB radiation as a detectable phenomenon appeared in a brief paper by Soviet astrophysicists A. G. Doroshkevich and Igor Novikov, in the spring of 1964. In 1964, David Todd Wilkinson and Peter Roll, Dicke's colleagues at Princeton University, began constructing a Dicke radiometer to measure the cosmic microwave background. In 1964, Arno Penzias and Robert Woodrow Wilson at the Crawford Hill location of Bell Telephone Laboratories in nearby Holmdel Township, New Jersey had built a Dicke radiometer that they intended to use for radio astronomy and satellite communication experiments. On 20 May 1964 they made their first measurement clearly showing the presence of the microwave background, with their instrument having an excess 4.2K antenna temperature which they could not account for. After receiving a telephone call from Crawford Hill, Dicke said "Boys, we've been scooped." A meeting between the Princeton and Crawford Hill groups determined that the antenna temperature was indeed due to the microwave background. Penzias and Wilson received the 1978 Nobel Prize in Physics for their discovery.

The interpretation of the cosmic microwave background was a controversial issue in the 1960s with some proponents of the steady state theory arguing that the microwave background was the result of scattered starlight from distant galaxies. Using this model, and based on the study of narrow absorption line features in the spectra of stars, the astronomer Andrew McKellar wrote in 1941: "It can be calculated that the 'rotational temperature' of interstellar space is 2 K." However, during the 1970s the consensus was established that the cosmic microwave background is a remnant of the big bang. This was largely because new measurements at a range of frequencies showed that the spectrum was a thermal, black body spectrum, a result that the steady state model was unable to reproduce.

Harrison, Peebles, Yu and Zel'dovich realized that the early universe would require inhomogeneities at the level of 10 or 10. Rashid Sunyaev later calculated the observable imprint that these inhomogeneities would have on the cosmic microwave background. Increasingly stringent limits on the anisotropy of the cosmic microwave background were set by ground-based experiments during the 1980s. RELIKT-1, a Soviet cosmic microwave background anisotropy experiment on board the Prognoz 9 satellite (launched 1 July 1983) gave upper limits on the large-scale anisotropy. The NASA COBE mission clearly confirmed the primary anisotropy with the Differential Microwave Radiometer instrument, publishing their findings in 1992. The team received the Nobel Prize in physics for 2006 for this discovery.

Inspired by the COBE results, a series of ground and balloon-based experiments measured cosmic microwave background anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the scale of the first acoustic peak, which COBE did not have sufficient resolution to resolve. This peak corresponds to large scale density variations in the early universe that are created by gravitational instabilities, resulting in acoustical oscillations in the plasma. The first peak in the anisotropy was tentatively detected by the Toco experiment and the result was confirmed by the BOOMERanG and MAXIMA experiments. These measurements demonstrated that the geometry of the universe is approximately flat, rather than curved. They ruled out cosmic strings as a major component of cosmic structure formation and suggested cosmic inflation was the right theory of structure formation.

The second peak was tentatively detected by several experiments before being definitively detected by WMAP, which has tentatively detected the third peak. As of 2010, several experiments to improve measurements of the polarization and the microwave background on small angular scales are ongoing. These include DASI, WMAP, BOOMERanG, QUaD, Planck spacecraft, Atacama Cosmology Telescope, South Pole Telescope and the QUIET telescope.

There are challenges to the standard CMB intepretation within the big bang framework. The background temperature of space was predicted by Charles Édouard Guillaume, Arthur Eddington, Erich Regener, Walther Nernst, Gerhard Herzberg, Erwin Finlay-Freundlich, Max Born, and Anthony Peratt, based on a universe without expansion, and prior to the discovery of the CMB. Their predictions were more accurate than big bang models. The earliest estimation of the background temperature of “space” known to us is that of Guillaume (1896). It was published in 1896, before Gamow’s birth (1904).

This paper documents the history of predictions. 

Alternative interpretations also fit with the Plasma Universe model advocated by Anthony Peratt, and Eric Lerner, a plasma physicist at LPP Fusion. Lerner wrote The Big Bang Never Happened. He interprets the CMB as ""a radio fog of dense plasma filaments."" If Lerner is right and the CMB is not a remnant heat signature then it tells us nothing about the age of the universe.

The cosmic microwave background radiation and the cosmological redshift-distance relation are together regarded as the best available evidence for the Big Bang event. Measurements of the CMB have made the inflationary Big Bang model the Standard Cosmological Model. The discovery of the CMB in the mid-1960s curtailed interest in alternatives such as the steady state theory.

In the late 1940s Alpher and Herman reasoned that if there was a Big Bang, the expansion of the universe would have stretched the high-energy radiation of the very early universe into the microwave region of the electromagnetic spectrum, and down to a temperature of about 5 K. They were slightly off with their estimate, but they had the right idea. They predicted the CMB. It took another 15 years for Penzias and Wilson to discover that the microwave background was actually there.

According to standard cosmology, the CMB gives a snapshot of the hot early universe at the point in time when the temperature dropped enough to allow electrons and protons to form hydrogen atoms. This event made the universe nearly transparent to radiation because light was no longer being scattered off free electrons. When this occurred some 380,000 years after the Big Bang, the temperature of the universe was about 3,000 K. This corresponds to an ambient energy of about , which is much less than the ionization energy of hydrogen. This epoch is generally known as the "time of last scattering" or the period of recombination or decoupling.

Since decoupling, the color temperature of the background radiation has dropped by an average factor of 1,089 due to the expansion of the universe. As the universe expands, the CMB photons are redshifted, causing them to decrease in energy. The color temperature of this radiation stays inversely proportional to a parameter that describes the relative expansion of the universe over time, known as the scale length. The color temperature "T" of the CMB as a function of redshift, "z", can be shown to be proportional to the color temperature of the CMB as observed in the present day (2.725 K or 0.2348 meV):

The anisotropy, or directional dependency, of the cosmic microwave background is divided into two types: primary anisotropy, due to effects that occur at the surface of last scattering and before; and secondary anisotropy, due to effects such as interactions of the background radiation with intervening hot gas or gravitational potentials, which occur between the last scattering surface and the observer.

The structure of the cosmic microwave background anisotropies is principally determined by two effects: acoustic oscillations and diffusion damping (also called collisionless damping or Silk damping). The acoustic oscillations arise because of a conflict in the photon–baryon plasma in the early universe. The pressure of the photons tends to erase anisotropies, whereas the gravitational attraction of the baryons, moving at speeds much slower than light, makes them tend to collapse to form overdensities. These two effects compete to create acoustic oscillations, which give the microwave background its characteristic peak structure. The peaks correspond, roughly, to resonances in which the photons decouple when a particular mode is at its peak amplitude.

The peaks contain interesting physical signatures. The angular scale of the first peak determines the curvature of the universe (but not the topology of the universe). The next peak—ratio of the odd peaks to the even peaks—determines the reduced baryon density. The third peak can be used to get information about the dark-matter density.

The locations of the peaks give important information about the nature of the primordial density perturbations. There are two fundamental types of density perturbations called "adiabatic" and "isocurvature". A general density perturbation is a mixture of both, and different theories that purport to explain the primordial density perturbation spectrum predict different mixtures.

The CMB spectrum can distinguish between these two because these two types of perturbations produce different peak locations. Isocurvature density perturbations produce a series of peaks whose angular scales ("ℓ" values of the peaks) are roughly in the ratio 1 : 3 : 5 : ..., while adiabatic density perturbations produce peaks whose locations are in the ratio 1 : 2 : 3 : ... Observations are consistent with the primordial density perturbations being entirely adiabatic, providing key support for inflation, and ruling out many models of structure formation involving, for example, cosmic strings.

Collisionless damping is caused by two effects, when the treatment of the primordial plasma as fluid begins to break down:
These effects contribute about equally to the suppression of anisotropies at small scales and give rise to the characteristic exponential damping tail seen in the very small angular scale anisotropies.

The depth of the LSS refers to the fact that the decoupling of the photons and baryons does not happen instantaneously, but instead requires an appreciable fraction of the age of the universe up to that era. One method of quantifying how long this process took uses the "photon visibility function" (PVF). This function is defined so that, denoting the PVF by "P"("t"), the probability that a CMB photon last scattered between time "t" and is given by "P"("t")"dt".

The maximum of the PVF (the time when it is most likely that a given CMB photon last scattered) is known quite precisely. The first-year WMAP results put the time at which "P"("t") has a maximum as 372,000 years. This is often taken as the "time" at which the CMB formed. However, to figure out how it took the photons and baryons to decouple, we need a measure of the width of the PVF. The WMAP team finds that the PVF is greater than half of its maximal value (the "full width at half maximum", or FWHM) over an interval of 115,000 years. By this measure, decoupling took place over roughly 115,000 years, and when it was complete, the universe was roughly 487,000 years old.

Since the CMB came into existence, it has apparently been modified by several subsequent physical processes, which are collectively referred to as late-time anisotropy, or secondary anisotropy. When the CMB photons became free to travel unimpeded, ordinary matter in the universe was mostly in the form of neutral hydrogen and helium atoms. However, observations of galaxies today seem to indicate that most of the volume of the intergalactic medium (IGM) consists of ionized material (since there are few absorption lines due to hydrogen atoms). This implies a period of reionization during which some of the material of the universe was broken into hydrogen ions.

The CMB photons are scattered by free charges such as electrons that are not bound in atoms. In an ionized universe, such charged particles have been liberated from neutral atoms by ionizing (ultraviolet) radiation. Today these free charges are at sufficiently low density in most of the volume of the universe that they do not measurably affect the CMB. However, if the IGM was ionized at very early times when the universe was still denser, then there are two main effects on the CMB:

Both of these effects have been observed by the WMAP spacecraft, providing evidence that the universe was ionized at very early times, at a redshift more than 17. The detailed provenance of this early ionizing radiation is still a matter of scientific debate. It may have included starlight from the very first population of stars (population III stars), supernovae when these first stars reached the end of their lives, or the ionizing radiation produced by the accretion disks of massive black holes.

The time following the emission of the cosmic microwave background—and before the observation of the first stars—is semi-humorously referred to by cosmologists as the Dark Age, and is a period which is under intense study by astronomers (see 21 centimeter radiation).

Two other effects which occurred between reionization and our observations of the cosmic microwave background, and which appear to cause anisotropies, are the Sunyaev–Zeldovich effect, where a cloud of high-energy electrons scatters the radiation, transferring some of its energy to the CMB photons, and the Sachs–Wolfe effect, which causes photons from the Cosmic Microwave Background to be gravitationally redshifted or blueshifted due to changing gravitational fields.

The cosmic microwave background is polarized at the level of a few microkelvin. There are two types of polarization, called E-modes and B-modes. This is in analogy to electrostatics, in which the electric field ("E"-field) has a vanishing curl and the magnetic field ("B"-field) has a vanishing divergence. The E-modes arise naturally from Thomson scattering in a heterogeneous plasma. The B-modes are not produced by standard scalar type perturbations. Instead they can be created by two mechanisms: the first one is by gravitational lensing of E-modes, which has been measured by the South Pole Telescope in 2013; the second one is from gravitational waves arising from cosmic inflation. Detecting the B-modes is extremely difficult, particularly as the degree of foreground contamination is unknown, and the weak gravitational lensing signal mixes the relatively strong E-mode signal with the B-mode signal.

E-modes were first seen in 2002 by the Degree Angular Scale Interferometer (DASI).

Cosmologists predict two types of B-modes, the first generated during cosmic inflation shortly after the big bang, and the second generated by gravitational lensing at later times.

Primordial gravitational waves are gravitational waves that could be observed in the polarisation of the cosmic microwave background and having their origin in the early universe. Models of cosmic inflation predict that such gravitational waves should appear; thus, their detection would support the theory of inflation, and their strength can confirm and exclude different models of inflation. It is the result of three things: inflationary expansion, reheating after inflation, and turbulent fluid mixing of matter and radiation.
On 17 March 2014, it was announced that the BICEP2 instrument had detected the first type of B-modes, consistent with inflation and gravitational waves in the early universe at the level of , which is the amount of power present in gravitational waves compared to the amount of power present in other scalar density perturbations in the very early universe. Had this been confirmed it would have provided strong evidence for cosmic inflation and the Big Bang and against the ekpyrotic model of Paul Steinhardt and Neil Turok. However, on 19 June 2014, considerably lowered confidence in confirming the findings was reported
and on 19 September 2014, new results of the Planck experiment reported that the results of BICEP2 can be fully attributed to cosmic dust.

The second type of B-modes was discovered in 2013 using the South Pole Telescope with help from the Herschel Space Observatory. In October 2014, a measurement of the B-mode polarization at 150 GHz was published by the POLARBEAR experiment. Compared to BICEP2, POLARBEAR focuses on a smaller patch of the sky and is less susceptible to dust effects. The team reported that POLARBEAR's measured B-mode polarization was of cosmological origin (and not just due to dust) at a 97.2% confidence level.

Subsequent to the discovery of the CMB, hundreds of cosmic microwave background experiments have been conducted to measure and characterize the signatures of the radiation. The most famous experiment is probably the NASA Cosmic Background Explorer (COBE) satellite that orbited in 1989–1996 and which detected and quantified the large scale anisotropies at the limit of its detection capabilities. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory.

During the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum.

In June 2001, NASA launched a second CMB space mission, WMAP, to make much more precise measurements of the large scale anisotropies over the full sky. WMAP used symmetric, rapid-multi-modulated scanning, rapid switching radiometers to minimize non-sky signal noise. The first results from this mission, disclosed in 2003, were detailed measurements of the angular power spectrum at a scale of less than one degree, tightly constraining various cosmological parameters. The results are broadly consistent with those expected from cosmic inflation as well as various other competing theories, and are available in detail at NASA's data bank for Cosmic Microwave Background (CMB) (see links below). Although WMAP provided very accurate measurements of the large scale angular fluctuations in the CMB (structures about as broad in the sky as the moon), it did not have the angular resolution to measure the smaller scale fluctuations which had been observed by former ground-based interferometers.

A third space mission, the ESA (European Space Agency) Planck Surveyor, was launched in May 2009 and performed an even more detailed investigation until it was shut down in October 2013. Planck employed both HEMT radiometers and bolometer technology and measured the CMB at a smaller scale than WMAP. Its detectors were trialled in the Antarctic Viper telescope as ACBAR (Arcminute Cosmology Bolometer Array Receiver) experiment—which has produced the most precise measurements at small angular scales to date—and in the Archeops balloon telescope.

On 21 March 2013, the European-led research team behind the Planck cosmology probe released the mission's all-sky map (565x318 jpeg, 3600x1800 jpeg) of the cosmic microwave background. The map suggests the universe is slightly older than researchers expected. According to the map, subtle fluctuations in temperature were imprinted on the deep sky when the cosmos was about years old. The imprint reflects ripples that arose as early, in the existence of the universe, as the first nonillionth of a second. Apparently, these ripples gave rise to the present vast cosmic web of galaxy clusters and dark matter. Based on the 2013 data, the universe contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. On 5 February 2015, new data was released by the Planck mission, according to which the age of the universe is billion years old and the Hubble constant was measured to be .

Additional ground-based instruments such as the South Pole Telescope in Antarctica and the proposed Clover Project, Atacama Cosmology Telescope and the QUIET telescope in Chile will provide additional data not available from satellite observations, possibly including the B-mode polarization.

Raw CMBR data, even from space vehicles such as WMAP or Planck, contain foreground effects that completely obscure the fine-scale structure of the cosmic microwave background. The fine-scale structure is superimposed on the raw CMBR data but is too small to be seen at the scale of the raw data. The most prominent of the foreground effects is the dipole anisotropy caused by the Sun's motion relative to the CMBR background. The dipole anisotropy and others due to Earth's annual motion relative to the Sun and numerous microwave sources in the galactic plane and elsewhere must be subtracted out to reveal the extremely tiny variations characterizing the fine-scale structure of the CMBR background.

The detailed analysis of CMBR data to produce maps, an angular power spectrum, and ultimately cosmological parameters is a complicated, computationally difficult problem. Although computing a power spectrum from a map is in principle a simple Fourier transform, decomposing the map of the sky into spherical harmonics,
formula_1
where the formula_2 term measures the mean temperature and formula_3 term accounts for the fluctuation, where the formula_4 refers to a spherical harmonic, and "ℓ" is the multipole number while "m" is the azimuthal number.

By applying the angular correlation function, the sum can be reduced to an expression that only involves "ℓ" and power spectrum term formula_5 The angled brackets indicate the average with respect to all observers in the universe; since the universe is homogeneous and isotropic, therefore there is an absence of preferred observing direction. Thus, "C" is independent of "m". Different choices of "ℓ" correspond to multipole moments of CMB.

In practice it is hard to take the effects of noise and foreground sources into account. In particular, these foregrounds are dominated by galactic emissions such as Bremsstrahlung, synchrotron, and dust that emit in the microwave band; in practice, the galaxy has to be removed, resulting in a CMB map that is not a full-sky map. In addition, point sources like galaxies and clusters represent another source of foreground which must be removed so as not to distort the short scale structure of the CMB power spectrum.

Constraints on many cosmological parameters can be obtained from their effects on the power spectrum, and results are often calculated using Markov chain Monte Carlo sampling techniques.

When , the formula_3 term reduced to 1, and what we have left here is just the mean temperature of the CMB. This "mean" is called CMB monopole, and it is observed to have an average temperature of about with one standard deviation confidence. The accuracy of this mean temperature may be impaired by the diverse measurements done by different mapping measurements. Such measurements demand absolute temperature devices, such as the FIRAS instrument on the COBE satellite. The measured "kT" is equivalent to 0.234 meV or . The photon number density of a blackbody having such temperature is formula_7. Its energy density is \approx \mathrm{0.260\,eV{\cdot}cm^{-3}}</math>,}} and the ratio to the critical density is .

CMB dipole represents the largest anisotropy, which is in the first spherical harmonic (). When , the formula_3 term reduces to one cosine function and thus encodes amplitude fluctuation. The amplitude of CMB dipole is around . Since the universe is presumed to be homogeneous and isotropic, an observer should see the blackbody spectrum with temperature "T" at every point in the sky. The spectrum of the dipole has been confirmed to be the differential of a blackbody spectrum.

CMB dipole is frame-dependent. The CMB dipole moment could also be interpreted as the peculiar motion of the Earth toward the CMB. Its amplitude depends on the time due to the Earth's orbit about the barycenter of the solar system. This enables us to add a time-dependent term to the dipole expression. The modulation of this term is 1 year, which fits the observation done by COBE FIRAS. The dipole moment does not encode any primordial information.

From the CMB data, it is seen that the Sun appears to be moving at relative to the reference frame of the CMB (also called the CMB rest frame, or the frame of reference in which there is no motion through the CMB). The Local Group — the galaxy group that includes our own Milky Way galaxy — appears to be moving at in the direction of galactic longitude , . This motion results in an anisotropy of the data (CMB appearing slightly warmer in the direction of movement than in the opposite direction). The standard interpretation of this temperature variation is a simple velocity redshift and blueshift due to motion relative to the CMB, but alternative cosmological models can explain some fraction of the observed dipole temperature distribution in the CMB.

A 2021 study of Wide-field Infrared Survey Explorer questions the kinematic interpretation of CMB anisotropy with high statistical confidence.

The temperature variation in the CMB temperature maps at higher multipoles, or , is considered to be the result of perturbations of the density in the early Universe, before the recombination epoch. Before recombination, the Universe consisted of a hot, dense plasma of electrons and baryons. In such a hot dense environment, electrons and protons could not form any neutral atoms. The baryons in such early Universe remained highly ionized and so were tightly coupled with photons through the effect of Thompson scattering. These phenomena caused the pressure and gravitational effects to act against each other, and triggered fluctuations in the photon-baryon plasma. Quickly after the recombination epoch, the rapid expansion of the universe caused the plasma to cool down and these fluctuations are "frozen into" the CMB maps we observe today. The said procedure happened at a redshift of around .

With the increasingly precise data provided by WMAP, there have been a number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions.<ref name="arXiv:astro-ph/0511666"></ref><ref name="arXiv:astro-ph/0503213"></ref> The most longstanding of these is the low-"ℓ" multipole controversy. Even in the COBE map, it was observed that the quadrupole (, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole () modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes. A number of groups have suggested that this could be the signature of new physics at the greatest observable scales; other groups suspect systematic errors in the data.

Ultimately, due to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the "internal linear combination" map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and Bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole.

A full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%.
Recent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so instrumental error (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, "I do think there is a bit of a psychological effect; people want to find unusual things."

Assuming the universe keeps expanding and it does not suffer a Big Crunch, a Big Rip, or another similar fate, the cosmic microwave background will continue redshifting until it will no longer be detectable, and will be superseded first by the one produced by starlight, and perhaps, later by the background radiation fields of processes that may take place in the far future of the universe such as proton decay, evaporation of black holes, and positronium decay.







Comparative law

Comparative law is the study of differences and similarities between the law (legal systems) of different countries. More specifically, it involves the study of the different legal "systems" (or "families") in existence in the world, including the common law, the civil law, socialist law, Canon law, Jewish Law, Islamic law, Hindu law, and Chinese law. It includes the description and analysis of foreign legal systems, even where no explicit comparison is undertaken. The importance of comparative law has increased enormously in the present age of internationalism, economic globalization, and democratization.

The origins of modern Comparative Law can be traced back to Gottfried Wilhelm Leibniz in 1667 in his Latin-language book (New Methods of Studying and Teaching Jurisprudence). Chapter 7 (Presentation of Law as the Project for all Nations, Lands and Times) introduces the idea of classifying Legal Systems into several families. A few years later, Leibniz introduced an idea of Language families.

Although every Legal System is unique, Comparative Law through studies of their similarities and differences allows for classification of Legal Systems, wherein Law Families is the basic level of the classification. The main differences between Law Families are found in the source(s) of Law, the role of court precedents, the origin and development of the Legal System. Montesquieu is generally regarded as an early founding figure of comparative law. His comparative approach is obvious in the following excerpt from Chapter III of Book I of his masterpiece, "De l'esprit des lois" (1748; first translated by Thomas Nugent, 1750):

Also, in Chapter XI (entitled 'How to compare two different Systems of Laws') of Book XXIX, discussing the French and English systems for punishment of false witnesses, he advises that "to determine which of those systems is most agreeable to reason, we must take them each as a whole and compare them in their entirety." Yet another place where Montesquieu's comparative approach is evident is the following, from Chapter XIII of Book XXIX:

The modern founding figure of comparative and anthropological jurisprudence was Sir Henry Maine, a British jurist and legal historian. In his 1861 work "Ancient Law: Its Connection with the Early History of Society, and Its Relation to Modern Ideas", he set out his views on the development of legal institutions in primitive societies and engaged in a comparative discussion of Eastern and Western legal traditions. This work placed comparative law in its historical context and was widely read and influential.

The first university course on the subject was established at the University of Oxford in 1869, with Maine taking up the position of professor.

Comparative law in the US was brought by a legal scholar fleeing persecution in Germany, Rudolf Schlesinger. Schlesinger eventually became professor of comparative law at Cornell Law School helping to spread the discipline throughout the US.

Comparative law is an academic discipline that involves the study of legal systems, including their constitutive elements and how they differ, and how their elements combine into a system.

Several disciplines have developed as separate branches of comparative law, including comparative constitutional law, comparative administrative law, comparative civil law (in the sense of the law of torts, contracts, property and obligations), comparative commercial law (in the sense of business organisations and trade), and comparative criminal law. Studies of these specific areas may be viewed as micro- or macro-comparative legal analysis, i.e. detailed comparisons of two countries, or broad-ranging studies of several countries. Comparative civil law studies, for instance, show how the law of private relations is organised, interpreted and used in different systems or countries. The purposes of comparative law are:

Comparative law is different from general jurisprudence (i.e. legal theory) and from public and private international law. However, it helps inform all of these areas of normativity.

For example, comparative law can help international legal institutions, such as those of the United Nations System, in analyzing the laws of different countries regarding their treaty obligations. Comparative law would be applicable to private international law when developing an approach to interpretation in a conflicts analysis. Comparative law may contribute to legal theory by creating categories and concepts of general application. Comparative law may also provide insights into the question of legal transplants, i.e. the transplanting of law and legal institutions from one system to another. The notion of legal transplants was coined by Alan Watson, one of the world's renowned legal scholars specializing in comparative law. Gunther Teubner expanded the notion of legal transplantation to include "legal irritation": Rather than smoothly integrating into domestic legal systems, a foreign rule disrupts established norms and societal arrangements. This disruption sparks an evolution where the external rule's meaning is redefined and where significant transformations within the internal context are triggered. Lasse Schuldt added that irritation is not spontaneous, but requires institutional drivers.
Also, the usefulness of comparative law for sociology of law and law and economics (and vice versa) is very large. The comparative study of the various legal systems may show how different legal regulations for the same problem function in practice. Conversely, sociology of law and law & economics may help comparative law answer questions, such as:

René David proposed the classification of legal systems, according to the different ideology inspiring each one, into five groups or families:

Especially with respect to the aggregating by David of the Civil and Common laws into a single family, David argued that the antithesis between the Common law and Civil law systems, is of a technical rather than of an ideological nature. Of a different kind is, for instance, the antithesis between, say, Italian and American laws, and of a different kind than between the Soviet, Muslim, Hindu, or Chinese laws. According to David, the Civil law legal systems included those countries where legal science was formulated according to Roman law, whereas Common law countries are those dominated by judge-made law. The characteristics that he believed uniquely differentiate the Western legal family from the other four are:

Arminjon, Nolde, and Wolff believed that, for purposes of classifying the (then) contemporary legal systems of the world, it was required that those systems "per se" get studied, irrespective of external factors, such as geographical ones. They proposed the classification of legal system into seven groups, or so-called 'families', in particular the:

Konrad Zweigert and Hein Kötz propose a different, multidimensional methodology for categorizing laws, i.e. for ordering families of laws. They maintain that, to determine such families, five criteria should be taken into account, in particular: the historical background, the characteristic way of thought, the different institutions, the recognized sources of law, and the dominant ideology. Using the aforementioned criteria, they classify the legal systems of the world into six families:

Up to the second German edition of their introduction to comparative law, Zweigert and Kötz also used to mention Soviet or socialist law as another family of laws.

H. Patrick Glenn proposed the classification of legal systems places national laws in the broader context of major legal tradition:






CD (disambiguation)

A CD or compact disc is a thin plastic silvery disc for audio recordings.

CD or cd may also refer to:










Cyberspace

Cyberspace is an interconnected digital environment. It is a type of virtual world popularized with the rise of the Internet. The term entered popular culture from science fiction and the arts but is now used by technology strategists, security professionals, governments, military and industry leaders and entrepreneurs to describe the domain of the global technology environment, commonly defined as standing for the global network of interdependent information technology infrastructures, telecommunications networks and computer processing systems. Others consider cyberspace to be just a notional environment in which communication over computer networks occurs. The word became popular in the 1990s when the use of the Internet, networking, and digital communication were all growing dramatically; the term "cyberspace" was able to represent the many new ideas and phenomena that were emerging.
As a social experience, individuals can interact, exchange ideas, share information, provide social support, conduct business, direct actions, create artistic media, play games, engage in political discussion, and so on, using this global network. Cyberspace users are sometimes referred to as cybernauts. 

The term "cyberspace" has become a conventional means to describe anything associated with general computing, the Internet and the diverse Internet culture. The United States government recognizes the interconnected information technology and the interdependent network of information technology infrastructures operating across this medium as part of the US national critical infrastructure. Amongst individuals on cyberspace, there is believed to be a code of shared rules and ethics mutually beneficial for all to follow, referred to as cyberethics. Many view the right to privacy as most important to a functional code of cyberethics. Such moral responsibilities go hand in hand when working online with global networks, specifically, when opinions are involved with online social experiences.

According to Chip Morningstar and F. Randall Farmer, cyberspace is defined more by the social interactions involved rather than its technical implementation. In their view, the computational medium in cyberspace is an augmentation of the communication channel between real people; the core characteristic of cyberspace is that it offers an environment that consists of many participants with the ability to affect and influence each other. They derive this concept from the observation that people seek richness, complexity, and depth within a virtual world.

The term "cyberspace" first appeared in the visual arts in the late 1960s, when Danish artist Susanne Ussing (1940-1998) and her partner architect Carsten Hoff (b. 1934) constituted themselves as Atelier Cyberspace. Under this name the two made a series of installations and images entitled "sensory spaces" that were based on the principle of open systems adaptable to various influences, such as human movement and the behaviour of new materials.

Atelier Cyberspace worked at a time when the Internet did not exist and computers were more or less off-limit to artists and creative engagement. In a 2015-interview with Scandinavian art magazine Kunstkritikk, Carsten Hoff recollects, that although Atelier Cyberspace did try to implement computers, they had no interest in the virtual space as such:

And in the same interview Hoff continues:

The works of Atelier Cyberspace were originally shown at a number of Copenhagen venues and have later been exhibited at The National Gallery of Denmark in Copenhagen as part of the exhibition "What's Happening?"

The term "cyberspace" first appeared in fiction in the 1980s in the work of cyberpunk science fiction author William Gibson, first in his 1982 short story "Burning Chrome" and later in his 1984 novel "Neuromancer". In the next few years, the word became prominently identified with online computer networks. The portion of "Neuromancer" cited in this respect is usually the following:

Now widely used, the term has since been criticized by Gibson, who commented on the origin of the term in the 2000 documentary "No Maps for These Territories":

Don Slater uses a metaphor to define cyberspace, describing the "sense of a social setting that exists purely within a space of representation and communication ... it exists entirely within a computer space, distributed across increasingly complex and fluid networks." The term "Cyberspace" started to become a de facto synonym for the Internet, and later the World Wide Web, during the 1990s, especially in academic circles and activist communities. Author Bruce Sterling, who popularized this meaning, credits John Perry Barlow as the first to use it to refer to "the present-day nexus of computer and telecommunications networks". Barlow describes it thus in his essay to announce the formation of the Electronic Frontier Foundation (note the spatial metaphor) in June 1990:

As Barlow, and the EFF continued public education efforts to promote the idea of "digital rights", the term was increasingly used during the Internet boom of the late 1990s.

Although the present-day, loose use of the term "cyberspace" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location. It is for these reasons cyberspace has been described as the ultimate tax haven.

In 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.

Although several definitions of cyberspace can be found both in scientific literature and in official governmental sources, there is no fully agreed official definition yet. According to F. D. Kramer there are 28 different definitions of the term cyberspace.

The most recent draft definition is the following:

The Joint Chiefs of Staff of the United States Department of Defense define cyberspace as one of five interdependent domains, the remaining four being land, air, maritime, and space. "See United States Cyber Command"

While cyberspace should not be confused with the Internet, the term is often used to refer to objects and identities that exist largely within the communication network itself, so that a website, for example, might be metaphorically said to "exist in cyberspace". According to this interpretation, events taking place on the Internet are not happening in the locations where participants or servers are physically located, but "in cyberspace". The philosopher Michel Foucault used the term heterotopias, to describe such spaces which are simultaneously physical and mental.

Firstly, cyberspace describes the flow of digital data through the network of interconnected computers: it is at once not "real", since one could not spatially locate it as a tangible object, and clearly "real" in its effects. There have been several attempts to create a concise model about how cyberspace works since it is not a physical thing that can be looked at. Secondly, cyberspace is the site of computer-mediated communication (CMC), in which online relationships and alternative forms of online identity were enacted, raising important questions about the social psychology of Internet use, the relationship between "online" and "offline" forms of life and interaction, and the relationship between the "real" and the virtual. Cyberspace draws attention to remediation of culture through new media technologies: it is not just a communication tool but a social destination and is culturally significant in its own right. Finally, cyberspace can be seen as providing new opportunities to reshape society and culture through "hidden" identities, or it can be seen as borderless communication and culture.

The "space" in cyberspace has more in common with the abstract, mathematical meanings of the term (see space) than physical space. It does not have the duality of positive and negative volume (while in physical space, for example, a room has the negative volume of usable space delineated by positive volume of walls, Internet users cannot enter the screen and explore the unknown part of the Internet as an extension of the space they are in), but spatial meaning can be attributed to the relationship between different pages (of books as well as web servers), considering the unturned pages to be somewhere "out there." The concept of cyberspace, therefore, refers not to the content being presented to the surfer, but rather to the possibility of surfing among different sites, with feedback loops between the user and the rest of the system creating the potential to always encounter something unknown or unexpected.

Video games differ from text-based communication in that on-screen images are meant to be figures that actually occupy a space and the animation shows the movement of those figures. Images are supposed to form the positive volume that delineates the empty space. A game adopts the cyberspace metaphor by engaging more players in the game, and then figuratively representing them on the screen as avatars. Games do not have to stop at the avatar-player level, but current implementations aiming for more immersive playing space (i.e. Laser tag) take the form of augmented reality rather than cyberspace, fully immersive virtual realities remaining impractical.

Although the more radical consequences of the global communication network predicted by some cyberspace proponents (i.e. the diminishing of state influence envisioned by John Perry Barlow) failed to materialize and the word lost some of its novelty appeal, it remains current .

Some virtual communities explicitly refer to the concept of cyberspace, for example Linden Lab calling their customers "Residents" of Second Life, while all such communities can be positioned "in cyberspace" for explanatory and comparative purposes (as did Sterling in "The Hacker Crackdown", followed by many journalists), integrating the metaphor into a wider cyber-culture.

The metaphor has been useful in helping a new generation of thought leaders to reason through new military strategies around the world, led largely by the US Department of Defense (DoD). The use of cyberspace as a metaphor has had its limits, however, especially in areas where the metaphor becomes confused with physical infrastructure. It has also been critiqued as being unhelpful for falsely employing a spatial metaphor to describe what is inherently a network.

A forerunner of the modern ideas of cyberspace is the Cartesian notion that people might be deceived by an evil demon that feeds them a false reality. This argument is the direct predecessor of modern ideas of a brain in a vat and many popular conceptions of cyberspace take Descartes's ideas as their starting point.

Visual arts have a tradition, stretching back to antiquity, of artifacts meant to fool the eye and be mistaken for reality. This questioning of reality occasionally led some philosophers and especially theologians to distrust art as deceiving people into entering a world which was not real (see Aniconism). The artistic challenge was resurrected with increasing ambition as art became more and more realistic with the invention of photography, film (see "Arrival of a Train at La Ciotat"), and immersive computer simulations.

American counterculture exponents like William S. Burroughs (whose literary influence on Gibson and cyberpunk in general is widely acknowledged) and Timothy Leary were among the first to extol the potential of computers and computer networks for individual empowerment.

Some contemporary philosophers and scientists (e.g. David Deutsch in "The Fabric of Reality") employ virtual reality in various thought experiments. For example, Philip Zhai in "Get Real: A Philosophical Adventure in Virtual Reality" connects cyberspace to the Platonic tradition:

Note that this brain-in-a-vat argument conflates cyberspace with reality, while the more common descriptions of cyberspace contrast it with the "real world".

The “Geography of Notopia” (Papadimitriou, 2006) theorizes about the complex interplay of cyber-cultures and the geographical space. This interplay has several philosophical and psychological facets (Papadimitriou, 2009).

The technological convergence of the mass media is the result of a long adaptation process of their communicative resources to the evolutionary changes of each historical moment. Thus, the new media became (plurally) an extension of the traditional media in cyberspace, allowing to the public access information in a wide range of digital devices. In other words, it is a cultural virtualization of human reality as a result of the migration from physical to virtual space (mediated by the ICTs), ruled by codes, signs and particular social relationships. Forwards, arise instant ways of communication, interaction and possible quick access to information, in which we are no longer mere senders, but also producers, reproducers, co-workers and providers. New technologies also help to "connect" people from different cultures outside the virtual space, which was unthinkable fifty years ago. In this giant relationships web, we mutually absorb each other's beliefs, customs, values, laws and habits, cultural legacies perpetuated by a physical-virtual dynamics in constant metamorphosis (ibidem). In this sense, Professor Doctor Marcelo Mendonça Teixeira created, in 2013, a new model of communication to the virtual universe, based in Claude Elwood Shannon (1948) article "A Mathematical Theory of Communication".

Having originated among writers, the concept of cyberspace remains most popular in literature and film. Although artists working with other media have expressed interest in the concept, such as Roy Ascott, "cyberspace" in digital art is mostly used as a synonym for immersive virtual reality and remains more discussed than enacted.

Cyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.

In 2010, a five-level model was designed in France. According to this model, cyberspace is composed of five layers based on information discoveries: 1) language, 2) writing, 3) printing, 4) Internet, 5) Etc., i.e. the rest, e.g. noosphere, artificial life, artificial intelligence, etc., etc. This original model links the world of information to telecommunication technologies.




The Maritimes

The Maritimes, also called the Maritime provinces, is a region of Eastern Canada consisting of three provinces: New Brunswick, Nova Scotia, and Prince Edward Island. The Maritimes had a population of 1,899,324 in 2021, which makes up 5.1% of Canada's population. Together with Canada's easternmost province, Newfoundland and Labrador, the Maritime provinces make up the region of Atlantic Canada.

Located along the Atlantic coast, various aquatic sub-basins are located in the Maritimes, such as the Gulf of Maine and Gulf of St. Lawrence. The region is located northeast of New England in the United States, south and southeast of Quebec's Gaspé Peninsula, and southwest of the island of Newfoundland. The notion of a Maritime Union has been proposed at various times in Canada's history; the first discussions in 1864 at the Charlottetown Conference contributed to Canadian Confederation. This movement formed the larger Dominion of Canada. The Mi'kmaq, Maliseet and Passamaquoddy people are indigenous to the Maritimes, while Acadian and British settlements date to the 17th century.

The word maritime is an adjective that means "of the sea"; thus any land adjacent to the sea can be considered maritime. But the term "Maritimes" has historically been collectively applied to New Brunswick, Nova Scotia and Prince Edward Island, all of which border the Atlantic Ocean.

The pre-history of the Canadian Maritimes begins after the northerly retreat of glaciers at the end of the Wisconsin glaciation over 10,000 years ago; human settlement by First Nations began in the Maritimes with Paleo-Indians during the "Early Period", ending around 6,000 years ago.

The "Middle Period", starting 6,000 years ago, and ending 3,000 years ago, was dominated by rising sea levels from the melting glaciers in polar regions. This is also when what is called the "Laurentian tradition" started among Archaic Indians, the term used for First Nations peoples of the time. Evidence of Archaic Indian burial mounds and other ceremonial sites existing in the Saint John River valley has been uncovered.

The "Late Period" extended from 3,000 years ago until first contact with European settlers. This period was dominated by the organization of First Nations peoples into the Algonquian-speaking Abenaki Nation, which occupied territory largely in present-day interior Vermont, New Hampshire, and Maine, and the Mi'kmaq Nation, which inhabited all of Nova Scotia, Prince Edward Island, eastern New Brunswick and the southern Gaspé. The primarily agrarian Maliseet Nation settled throughout the Saint John River and Allagash River valleys of present-day New Brunswick and Maine. The Passamaquoddy Nation inhabited the northwestern coastal regions of the present-day Bay of Fundy. The Mi'kmaq Nation is also believed to have crossed the present-day Cabot Strait at around this time to settle on the south coast of Newfoundland, but they were a minority compared to the Beothuk Nation.

After Newfoundland, the Maritimes were the second area in Canada to be settled by Europeans. There is evidence that Viking explorers discovered and settled in the Vinland region around 1000 AD, which is when the L'Anse aux Meadows settlement in Newfoundland and Labrador has been dated. They may have made further exploration into the present-day Maritimes and northeastern United States.

Both Giovanni Caboto (John Cabot) and Giovanni da Verrazzano are reported to have sailed in or near Maritime waters during their voyages of discovery for England and France, respectively. Several Portuguese explorers / cartographers have also documented various parts of the Maritimes, namely Diogo Homem. However, it was French explorer Jacques Cartier who made the first detailed reconnaissance of the region for a European power and, in so doing, claimed the region for the King of France. Cartier was followed by nobleman Pierre Dugua, Sieur de Mons, who was accompanied by explorer / cartographer Samuel de Champlain in a 1604 expedition. During this they established the second permanent European settlement in what is now the United States and Canada, following Spain's settlement at St. Augustine in present-day Florida in the American South. Champlain's settlement at Saint Croix Island, later moved to Port Royal (Annapolis Royal), survived. By contrast, the ill-fated English settlement at Roanoke Colony off the southern American coast did not. The French settlement pre-dated the more successful English settlement at Jamestown in present-day Virginia by three years. Champlain was considered the founder of New France's province of Canada, which comprises much of the present-day lower St. Lawrence River valley in the province of Quebec.

Champlain's success in the region, which came to be called "Acadie", led to the fertile tidal marshes surrounding the southeastern and northeastern reaches of the Bay of Fundy being populated by French immigrants who called themselves "Acadien". The Acadians eventually built small settlements throughout what is today mainland Nova Scotia and New Brunswick, as well as Île-Saint-Jean (Prince Edward Island), Île-Royale (Cape Breton Island), and other shorelines of the Gulf of St. Lawrence in present-day Newfoundland and Labrador, and Quebec. Acadian settlements had primarily agrarian economies. Early examples of Acadian fishing settlements developed in southwestern Nova Scotia and in Île-Royale, as well as along the south and west coasts of Newfoundland, the Gaspé Peninsula, and the present-day Côte-Nord region of Quebec. Most Acadian fishing activities were overshadowed by the much larger seasonal European fishing fleets that were based out of Newfoundland and took advantage of proximity to the Grand Banks.

The growing English colonies along the American seaboard to the south and various European wars between England and France during the 17th and 18th centuries brought Acadia to the centre of world-scale geopolitical forces. In 1613, Virginian raiders captured Port-Royal, and in 1621 France ceded Acadia to Scotland's Sir William Alexander, who renamed it as "Nova Scotia".

By 1632, Acadia was returned from Scotland to France under the "Treaty of Saint-Germain-en-Laye." The Port Royale settlement was moved to the site of nearby present-day Annapolis Royal. More French immigrant settlers, primarily from the Brittany, Normandie, and Vienne regions of France, continued to populate the colony of Acadia during the latter part of the 17th and early part of the 18th centuries. Important settlements also began in the Beaubassin region of the present-day Isthmus of Chignecto, and in the Saint John River valley, as well as smaller communities on Île-Saint-Jean and Île-Royale.

In 1654, raiders from New England attacked Acadian settlements on the Annapolis Basin. Acadians lived with uncertainty throughout the English constitutional crises under Oliver Cromwell, and it was not until the Treaty of Breda in 1667 that France's claim to the region was reaffirmed. Colonial administration by France throughout the history of Acadia was of low priority. France's priorities were in settling and strengthening its claim on the larger territory of New France and the exploration and settlement of interior North America and the Mississippi River valley.

Over 74 years (1689–1763) there were six colonial wars, which involved continuous warfare between New England and Acadia (see the French and Indian Wars reflecting English and French tensions in Europe, as well as Father Rale's War (Dummer's War) and Father Le Loutre's War). Throughout these wars, New England was allied with the Iroquois Confederacy based around the southern Great Lakes and west of the Hudson River. Acadian settlers were allied with the Wabanaki Confederacy. In the first war, King William's War (the North American theatre of the Nine Years' War), natives from the Maritime region participated in numerous attacks with the French on the Acadia / New England border in southern Maine (e.g., Raid on Salmon Falls). New England retaliatory raids on Acadia, such as the Raid on Chignecto, were conducted by Benjamin Church. In the second war, Queen Anne's War (the North American theatre of the War of the Spanish Succession), the British conducted the Conquest of Acadia, while the region remained primarily in control of Maliseet militia, Acadia militia and Mi'kmaw militia.

In 1719, to further protect strategic interests in the Gulf of St. Lawrence and St. Lawrence River, France began the 20-year construction of a large fortress at Louisbourg on Île-Royale. Massachusetts was increasingly concerned over reports of the capabilities of this fortress, and of privateers staging out of its harbour to raid New England fishermen on the Grand Banks. In the fourth war, King George's War (the North American theatre of the War of the Austrian Succession), the British engaged successfully in the Siege of Louisbourg. The British returned control of Île-Royale to France with the fortress virtually intact three years later under the Treaty of Aix-la-Chapelle and the French reestablished their forces there.

In 1749, to counter the rising threat of Louisbourg, Halifax was founded and the Royal Navy established a major naval base and citadel. The founding of Halifax sparked Father Le Loutre's War.

During the sixth and final colonial war, the French and Indian War (the North American theatre of the Seven Years' War), the military conflicts in Nova Scotia continued. The British Conquest of Acadia happened in 1710. Over the next forty-five years, the Acadians refused to sign an unconditional oath of allegiance to Britain. During this time period Acadians participated in various militia operations against the British and maintained vital supply lines to the French Fortress of Louisbourg and Fort Beausejour. The British sought to neutralize any military threat Acadians posed and to interrupt the vital supply lines Acadians provided to Louisbourg by deporting Acadians from Acadia.

The British began the Expulsion of the Acadians with the Bay of Fundy campaign in 1775. Over the next nine years over 12,000 Acadians of 15,000 were removed from Nova Scotia.

In 1758, the fortress of Louisbourg was laid siege for a second time within 15 years, this time by more than 27,000 British soldiers and sailors with over 150 warships. After the French surrender, Louisbourg was thoroughly destroyed by British engineers to ensure it would never be reclaimed. With the fall of Louisbourg, French and Mi'kmaw resistance in the region crumbled. British forces seized remaining French control over Acadia in the coming months, with Île-Saint-Jean falling in 1759 to British forces on their way to Quebec City for the first siege of Quebec and the ensuing Battle of the Plains of Abraham.

The war ended and Britain had gained control over the entire Maritime region and the Indigenous people signed the Halifax Treaties.

Following the Seven Years' War, empty Acadian lands were settled first by 8,000 New England Planters and then by immigrants brought from Yorkshire. Île-Royale was renamed Cape Breton Island and incorporated into the Colony of Nova Scotia. Some of the Acadians who had been deported came back but went to the eastern coasts of New Brunswick.

Both the colonies of Nova Scotia (present-day Nova Scotia and New Brunswick) and St. John's Island (Prince Edward Island) were affected by the American Revolutionary War, largely by privateering against American shipping, but several coastal communities were also the targets of American raiders. Charlottetown, the capital of the new colony of St. John's Island, was ransacked in 1775 with the provincial secretary kidnapped and the Great Seal stolen. The largest military action in the Maritimes during the revolutionary war was the attack on Fort Cumberland (the renamed Fort Beauséjour) in 1776 by a force of American sympathizers led by Jonathan Eddy. The fort was partially overrun after a month-long siege, but the attackers were ultimately repelled after the arrival of British reinforcements from Halifax.

The most significant impact from this war was the settling of large numbers of Loyalist refugees in the region (34,000 to the 17,000 settlers already there), especially in Shelburne and Parrtown (Saint John). Following the Treaty of Paris in 1783, Loyalist settlers in what would become New Brunswick persuaded British administrators to split the Colony of Nova Scotia to create the new colony of New Brunswick in 1784. At the same time, another part of the Colony of Nova Scotia, Cape Breton Island, was split off to become the Colony of Cape Breton Island. The Colony of St. John's Island was renamed to Prince Edward Island on November 29, 1798.

The War of 1812 had some effect on the shipping industry in the Maritime colonies of New Brunswick, Nova Scotia, Prince Edward Island, and Cape Breton Island; however, the significant Royal Navy presence in Halifax and other ports in the region prevented any serious attempts by American raiders. Maritime and American privateers targeted unprotected shipping of both the United States and Britain respectively, further reducing trade. New Brunswick's section of the Canada–US border did not have any significant action during this conflict, although British forces did occupy a portion of coastal Maine at one point. The most significant incident from this war which occurred in the Maritimes was the British capture and detention of USS "Chesapeake", an American frigate in Halifax.

In 1820, the Colony of Cape Breton Island was merged back into the Colony of Nova Scotia for the second time by the British government.

British settlement of the Maritimes, as the colonies of Nova Scotia, New Brunswick and Prince Edward Island came to be known, accelerated throughout the late 18th century and into the 19th century with significant immigration to the region as a result of Scottish migrants displaced by the Highland Clearances and Irish escaping the Great Irish Famine (1845–1849). As a result, significant portions of the three provinces are influenced by Celtic heritages, with Scottish Gaelic (and to a lesser degree, Irish Gaelic) having been widely spoken, particularly in Cape Breton, although it is less prevalent today.

During the American Civil War, a significant number of Maritimers volunteered to fight for the armies of the Union, while a small handful joined the Confederate Army. However, the majority of the conflict's impact was felt in the shipping industry. Maritime shipping boomed during the war due to large-scale Northern imports of war supplies which were often carried by Maritime ships as Union ships were vulnerable to Confederate naval raiders. Diplomatic tensions between Britain and the Unionist North had deteriorated after some interests in Britain expressed support for the secessionist Confederate South. The Union Navy, although much smaller than the British Royal Navy and no threat to the Maritimes, did posture off Maritime coasts at times chasing Confederate naval ships which sought repairs and reprovisioning in Maritime ports, especially Halifax.

The immense size of the Union Army (the largest on the planet toward the end of the Civil War), however, was viewed with increasing concern by Maritimers throughout the early 1860s. Another concern was the rising threat of Fenian raids on border communities in New Brunswick by the Fenian Brotherhood seeking to end British rule in Ireland. This combination of events, coupled with an ongoing decline in British military and economic support to the region as the Home Office favoured newer colonial endeavours in Africa and elsewhere, led to a call among Maritime politicians for a conference on Maritime Union, to be held in early September 1864 in Charlottetown – chosen in part because of Prince Edward Island's reluctance to give up its jurisdictional sovereignty in favour of uniting with New Brunswick and Nova Scotia into a single colony. New Brunswick and Nova Scotia felt that if the union conference were held in Charlottetown, they might be able to convince Island politicians to support the proposal.

The Charlottetown Conference, as it came to be called, was also attended by a slew of visiting delegates from the neighbouring Crown colony, the Province of Canada, who had largely arrived at their own invitation with their own agenda. This agenda saw the conference dominated by discussions of creating an even larger union of the entire territory of British North America into a united colony. The Charlottetown Conference ended with an agreement to meet the following month in Quebec City, where more formal discussions ensued, culminating with meetings in London and the signing of the "British North America Act, 1867" (BNA Act). Of the Maritime provinces, only Nova Scotia and New Brunswick were initially party to the BNA Act: Prince Edward Island's reluctance, combined with a booming agricultural and fishing export economy having led to that colony opting not to sign on.

The major communities of the region include Halifax and Cape Breton in Nova Scotia, Moncton, Saint John, and Fredericton in New Brunswick, and Charlottetown in Prince Edward Island.

In spite of its name, The Maritimes has a humid continental climate of the warm-summer subtype. Especially in coastal Nova Scotia, differences between summers and winters are narrow compared to the rest of Canada. The inland climate of New Brunswick is in stark contrast during winter, resembling more continental areas. Summers are somewhat tempered by the marine influence throughout the provinces, but due to the southerly parallels still remain similar to more continental areas further west. Yarmouth in Nova Scotia has significant marine influence to have a borderline oceanic microclimate, but winter nights are still cold even in all coastal areas. The northernmost areas of New Brunswick are only just above subarctic with very cold continental winters.

The Maritimes were predominantly rural until recent decades, having resource-based economies of fishing, agriculture, forestry, and coal mining.

Maritimers are predominantly of west European origin: Scottish Canadians, Irish Canadians, English Canadians, and Acadians. New Brunswick, in general, differs from the other two Maritime provinces in that it has a much higher Francophone population. There was once a significant Canadian Gaelic speaking population. Helen Creighton recorded Celtic traditions of rural Nova Scotia in the mid-1900s.

There are Black Canadians who are mostly descendants of Black Loyalists or black refugees from the War of 1812. This Maritime population is mainly among Black Nova Scotians.

There are Mi'kmaq reserves in all three provinces, and a smaller population of the Maliseet in western New Brunswick. 

Given the small population of the region (compared with the Central Canadian provinces or the New England states), the regional economy is a net exporter of natural resources, manufactured goods, and services. The regional economy has long been tied to natural resources such as fishing, logging, farming, and mining activities. Significant industrialization in the second half of the 19th century brought steel to Trenton, Nova Scotia, and subsequent creation of a widespread industrial base to take advantage of the region's large underground coal deposits. After Confederation, however, this industrial base withered with technological change, and trading links to Europe and the U.S. were reduced in favour of those with Ontario and Quebec. In recent years, however, the Maritime regional economy has begun increased contributions from manufacturing again and the steady transition to a service economy.

Important manufacturing centres in the region include Pictou County, Truro, the Annapolis Valley and the South Shore, and the Strait of Canso area in Nova Scotia, as well as Summerside in Prince Edward Island, and the Miramichi area, the North Shore and the upper Saint John River valley of New Brunswick.

Some predominantly coastal areas have become major tourist centres, such as parts of Prince Edward Island, Cape Breton Island, the South Shore of Nova Scotia and the Gulf of St. Lawrence and Bay of Fundy coasts of New Brunswick. Additional service-related industries in information technology, pharmaceuticals, insurance and financial sectors—as well as research-related spin-offs from the region's numerous universities and colleges—are significant economic contributors.

Another important contribution to Nova Scotia's provincial economy is through spin-offs and royalties relating to off-shore petroleum exploration and development. Mostly concentrated on the continental shelf of the province's Atlantic coast in the vicinity of Sable Island, exploration activities began in the 1960s and resulted in the first commercial production field for oil beginning in the 1980s. Natural gas was also discovered in the 1980s during exploration work, and this is being commercially recovered, beginning in the late 1990s. Initial optimism in Nova Scotia about the potential of off-shore resources appears to have diminished with the lack of new discoveries, although exploration work continues and is moving farther off-shore into waters on the continental margin.

Regional transportation networks have also changed significantly in recent decades with port modernizations, with new freeway and ongoing arterial highway construction, the abandonment of various low-capacity railway branch lines (including the entire railway system of Prince Edward Island and southwestern Nova Scotia), and the construction of the Canso Causeway and the Confederation Bridge. There have been airport improvements at various centres providing improved connections to markets and destinations in the rest of North America and overseas.

Improvements in infrastructure and the regional economy notwithstanding, the three provinces remain one of the poorer regions of Canada. While urban areas are growing and thriving, economic adjustments have been harsh in rural and resource-dependent communities, and emigration has been an ongoing phenomenon for some parts of the region. Another problem is seen in the lower average wages and family incomes within the region. Property values are depressed, resulting in a smaller tax base for these three provinces, particularly when compared with the national average which benefits from central and western Canadian economic growth.

This has been particularly problematic with the growth of the welfare state in Canada since the 1950s, resulting in the need to draw upon equalization payments to provide nationally mandated social services. Since the 1990s the region has experienced an exceptionally tumultuous period in its regional economy with the collapse of large portions of the ground fishery throughout Atlantic Canada, the closing of coal mines and a steel mill on Cape Breton Island, and the closure of military bases in all three provinces. That being said, New Brunswick has one of the largest military bases in the Commonwealth of Nations (CFB Gagetown), which plays a significant role in the cultural and economic spheres of Fredericton, the province's capital city.

While the economic underperformance of the Maritime economy has been long lasting, it has not always been present. The mid-19th century, especially the 1850s and 1860s, has long been seen as a "Golden Age" in the Maritimes. Growth was strong, and the region had one of British North America's most extensive manufacturing sectors as well as a large international shipping industry. The question of why the Maritimes fell from being a centre of Canadian manufacturing to being an economic hinterland is thus a central one to the study of the region's pecuniary difficulties. The period in which the decline occurred had a great many potential culprits. In 1867 Nova Scotia and New Brunswick merged with the Canadas in Confederation, with Prince Edward Island joining them six years later in 1873. Canada was formed only a year after free trade with the United States (in the form of the Reciprocity Treaty) had ended. In the 1870s John A. Macdonald's National Policy was implemented, creating a system of protective tariffs around the new nation. Throughout the period there was also significant technological change both in the production and transportation of goods.

Several scholars have explored the so-called "Golden Age" of the Maritimes in the years just before Confederation. In Nova Scotia, the population grew steadily from 277,000 in 1851 to 388,000 in 1871, mostly from natural increase since immigration was slight. The era has been called a Golden Age, but that was a myth created in the 1930s to lure tourists to a romantic era of tall ships and antiques. Recent historians using census data have shown that is a fallacy. In 1851–1871 there was an overall increase in per capita wealth holding. However most of the gains went to the urban elite class, especially businessmen and financiers living in Halifax. The wealth held by the top 10% rose considerably over the two decades, but there was little improvement in the wealth levels in rural areas, which comprised the great majority of the population. Likewise Gwyn reports that gentlemen, merchants, bankers, colliery owners, shipowners, shipbuilders, and master mariners flourished. However the great majority of families were headed by farmers, fishermen, craftsmen and labourer. Most of them—and many widows as well—lived in poverty. Out migration became an increasingly necessary option. Thus the era was indeed a golden age but only for a small but powerful and highly visible elite.

The cause of economic malaise in the Maritimes is an issue of great debate and controversy among historians, economists, and geographers. The differing opinions can approximately be divided into the "structuralists", who argue that poor policy decisions are to blame, and the others, who argue that unavoidable technological and geographical factors caused the decline.

The exact date that the Maritimes began to fall behind the rest of Canada is difficult to determine. Historian Kris Inwood places the date very early, at least in Nova Scotia, finding clear signs that the Maritimes "Golden Age" of the mid-19th century was over by 1870, before Confederation or the National Policy could have had any significant impact. Richard Caves places the date closer to 1885. T.W. Acheson takes a similar view and provides considerable evidence that the early 1880s were in fact a booming period in Nova Scotia and this growth was only undermined towards the end of that decade. David Alexander argues that any earlier declines were simply part of the global Long Depression, and that the Maritimes first fell behind the rest of Canada when the great boom period of the early 20th century had little effect on the region. E.R. Forbes, however, emphasizes that the precipitous decline did not occur until after the First World War during the 1920s when new railway policies were implemented. Forbes also contends that significant Canadian defence spending during the Second World War favoured powerful political interests in Central Canada such as C. D. Howe, when major Maritime shipyards and factories, as well as Canada's largest steel mill, located in Cape Breton Island, fared poorly.

One of the most important changes, and one that almost certainly had an effect, was the revolution in transportation that occurred at this time. The Maritimes were connected to central Canada by the Intercolonial Railway in the 1870s, removing a longstanding barrier to trade. For the first time this placed the Maritime manufacturers in direct competition with those of Central Canada. Maritime trading patterns shifted considerably from mainly trading with New England, Britain, and the Caribbean, to being focused on commerce with the Canadian interior, enforced by the federal government's tariff policies.

Coincident with the construction of railways in the region, the age of the wooden sailing ship began to come to an end, being replaced by larger and faster steel steamships. The Maritimes had long been a centre for shipbuilding, and this industry was hurt by the change. The larger ships were also less likely to call on the smaller population centres such as Saint John and Halifax, preferring to travel to cities like New York and Montreal. Even the Cunard Line, founded by Maritime-born Samuel Cunard, stopped making more than a single ceremonial voyage to Halifax each year.

More controversial than the role of technology is the argument over the role of politics in the origins of the region's decline. Confederation and the tariff and railway freight policies that followed have often been blamed for having a deleterious effect on the Maritime economies. Arguments have been made that the Maritimes' poverty was caused by control over policy by Central Canada which used the national structures for its own enrichment. This was the central view of the Maritime Rights Movement of the 1920s, which advocated greater local control over the region's finances. T.W. Acheson is one of the main proponents of this theory. He notes the growth that was occurring during the early years of the National Policy in Nova Scotia demonstrates how the effects of railway fares and the tariff structure helped undermine this growth. Capitalists from Central Canada purchased the factories and industries of the Maritimes from their bankrupt local owners and proceeded to close down many of them, consolidating the industry in Central Canada.

The policies in the early years of Confederation were designed by Central Canadian interests, and they reflected the needs of that region. The unified Canadian market and the introduction of railroads created a relative weakness in the Maritime economies. Central to this concept, according to Acheson, was the lack of metropolises in the Maritimes.

Montreal and Toronto were well-suited to benefit from the development of large-scale manufacturing and extensive railway systems in Quebec and Ontario, these being the goals of the Macdonald and Laurier governments. In the Maritimes the situation was very different. Today New Brunswick has several mid-sized centres in Saint John, Moncton, and Fredericton but no significant population centre. Nova Scotia has a growing metropolitan area surrounding Halifax, but a contracting population in industrial Cape Breton County, and several smaller centres in Bridgewater, Kentville, Yarmouth, and Pictou County. Prince Edward Island's only significant population centres are in Charlottetown and Summerside. During the late 19th and early 20th centuries, just the opposite was the case with little to no population concentration in major industrial centres as the predominantly rural resource-dependent Maritime economy continued on the same path as it had since European settlement on the region's shores.

Despite the region's absence of economic growth on the same scale as other parts of the nation, the Maritimes has changed markedly throughout the 20th century, partly as a result of global and national economic trends, and partly as a result of government intervention. Each sub-region within the Maritimes has developed over time to exploit different resources and expertise. Saint John became a centre of the timber trade and shipbuilding and is currently a centre for oil refining and some manufacturing. The northern New Brunswick communities of Edmundston, Campbellton, Dalhousie, Bathurst, and Miramichi are focused on the pulp and paper industry and some mining activity. Moncton was a centre for railways and has changed its focus to becoming a multi-modal transportation centre with associated manufacturing and retail interests. The Halifax metropolitan area has come to dominate peninsular Nova Scotia as a retail and service centre, but that province's industries were spread out from the coal and steel industries of industrial Cape Breton and Pictou counties, the mixed farming of the North Shore and Annapolis Valley, and the fishing industry was primarily focused on the South Shore and Eastern Shore. Prince Edward Island is largely dominated by farming, fishing, and tourism.

Given the geographic diversity of the various sub-regions within the Maritimes, policies to centralize the population and economy were not initially successful, thus Maritime factories closed while those in Ontario and Quebec prospered.

The traditional staples thesis, advocated by scholars such as S.A. Saunders, looks at the resource endowments of the Maritimes and argues that it was the decline of the traditional industries of shipbuilding and fishing that led to Maritime poverty, since these processes were rooted in geography, and thus all but inevitable. Kris Inwood has revived the staples approach and looks at a number of geographic weaknesses relative to Central Canada. He repeats Acheson's argument that the region lacks major urban centres, but adds that the Maritimes were also lacking the great rivers that led to the cheap and abundant hydro-electric power, key to Quebec and Ontario's urban and manufacturing development, that the extraction costs of Maritime resources were higher (particularly in the case of Cape Breton coal), and that the soils of the region were poorer and thus the agricultural sector weaker.

The Maritimes are the only provinces in Canada which entered Confederation in the 19th century and have kept their original colonial boundaries. All three provinces have the smallest land base in the country and have been forced to make do with resources within. By comparison, the former colony of the Province of Canada (divided into the District of Canada East, and the District of Canada West) and the western provinces were dozens of times larger and in some cases were expanded to take in territory formerly held in British Crown grants to companies such as the Hudson's Bay Company; in particular the November 19, 1869 sale of Rupert's Land to the Government of Canada under the "Rupert's Land Act 1868" was facilitated in part by Maritime taxpayers. The economic riches of energy and natural resources held within this larger land base were only realized by other provinces during the 20th century.

The maritime provinces' main industry is fishing. Fishing can be found in any maritime province. This includes fishing for lobster, mackerel, tuna, salmon and many more kinds of fish. Oysters and salmonoid aquaculture is also increasingly important economically.

Nova Scotia is very strong in agriculture, forestry and fishing.

Tourism is important to the economy of PEI. "Anne of Green Gables" was written in PEI, and this attracts tourists to PEI. PEI is also known for its agriculture, mainly the potato, and fishing industries.

Agriculture and forestry are two prominent industries found in New Brunswick. Despite having an extensive coastline, New Brunswick's industrial sector has never been entirely reliant on the success of the fisheries. Likewise, the strong shipbuilding heritage of the province directly relates to its forest resources. Because of this, New Brunswickers tend to attribute their cultural heritage less with the sea and more with their forests and rivers.

Maritime conservatism since the Second World War has been very much part of the Red Tory tradition, key influences being former Premier of Nova Scotia and federal Progressive Conservative Party leader Robert Stanfield and New Brunswick Tory strategist Dalton Camp.

In recent years, the social democratic New Democratic Party (NDP) has made significant inroads both federally and provincially in the region. The NDP has elected members of parliament (MPs) from New Brunswick, but most of the focus of the party at the federal and provincial levels is currently in the Halifax area of Nova Scotia. Industrial Cape Breton has historically been a region of labour activism, electing Co-operative Commonwealth Federation (and later NDP) MPs, and even produced many early members of the Communist Party of Canada in the pre-Second World War era. In the 2004 federal election, the NDP captured 28.45% of the vote in Nova Scotia, more than any other province. In the 2009 provincial election the NDP formed a majority government, the first in the region.

In the 2004 federal election, the Conservatives had one of the worst showings in the region for a right-wing party, going back to Confederation, with the exception of the 1993 election. The Conservative party improved its seat count in the 2008 and elected 13 MPs in the 2011 election. However, in the 2015 election the Liberal Party won every seat in the region, defeating all of the Conservative (and NDP) challengers.

The Liberal Party of Canada has done well in the Maritimes in the past because of its interventionist policies. The Acadian Peninsula region of New Brunswick tends to vote for the Liberals or NDP for social political reasons, as well as treatment of the French by various parties. In the 1997 federal election, Prime Minister Jean Chrétien's Liberals endured a bitter defeat to the PCs and NDP in many ridings as a result of unpopular cuts to unemployment benefits for seasonal workers, as well as closures of several Canadian Forces bases, the refusal to honour a promise to rescind the Goods and Services Tax, cutbacks to provincial equalization payments, health care, post-secondary education and regional transportation infrastructure such as airports, fishing harbours, seaports, and railways . The Liberals held onto seats in Prince Edward Island and New Brunswick, while being shut out of Nova Scotia entirely, the second time in history (the only other time being the Diefenbaker sweep). In 2015 the Liberals won every seat in The Maritimes, defeating Conservative and NDP incumbents.

The Maritimes is currently represented in the Canadian Parliament by 25 Members of the House of Commons (Nova Scotia – 11, New Brunswick – 10, Prince Edward Island – 4) and 24 Senators (Nova Scotia and New Brunswick – 10 each, Prince Edward Island – 4). This level of representation was established at the time of Confederation when the Maritimes had a much larger proportion of the national population. The comparatively large population growth of western and central Canada during the immigration boom of the 20th century has reduced the Maritimes' proportion of the national population to less than 10%, resulting in an over-representation in Parliament, with some federal ridings having fewer than 35,000 people, compared to central and western Canada where ridings typically contain 100,000–120,000 people.

The Senate of Canada is structured along regional lines, giving an equal number of seats (24) to the Maritimes, Ontario, Quebec, and western Canada, in addition to the later entry of Newfoundland and Labrador, as well as the three territories. Enshrined in the Constitution, this model was developed to ensure that no area of the country is able to exert undue influence in the Senate. The Maritimes, with its much smaller proportion of the national population (compared to the time of Confederation) also have an over-representation in the Senate, particularly compared to the population growth of Ontario and the western provinces. This has led to calls to reform the Senate; however, such a move would entail constitutional changes.

Another factor related to the number of Senate seats is that a constitutional amendment in the early 20th century mandated that no province can have fewer Members of Parliament than it has senators. This court decision resulted from a complaint by the Government of Prince Edward Island after that province's number of MPs was proposed to change from 4 to 3, accounting for its declining proportion of the national population at that time. When PEI entered Confederation in 1873, it was accorded 6 MPs and 4 Senators; however this was reduced to 4 MPs by the early 20th century. Senators being appointed for life at this time, these coveted seats rarely went unfilled for a long period of time anywhere in Canada. As a result, PEI's challenge was accepted by the federal government, and its level of federal representation was secured. In the aftermath of the 1989 budget, which saw a filibuster by Liberal Senators in attempt to kill legislation creating the Goods and Services Tax, Prime Minister Brian Mulroney "stacked" the Senate by creating additional seats in several provinces across Canada, including New Brunswick; however, there was no attempt by these provinces to increase the number of MPs to reflect this change in Senate representation.



Cyril of Alexandria

Cyril of Alexandria (; or ⲡⲓ̀ⲁⲅⲓⲟⲥ Ⲕⲓⲣⲓⲗⲗⲟⲥ;  376–444) was the Patriarch of Alexandria from 412 to 444. He was enthroned when the city was at the height of its influence and power within the Roman Empire. Cyril wrote extensively and was a major player in the Christological controversies of the late-4th and 5th centuries. He was a central figure in the Council of Ephesus in 431, which led to the deposition of Nestorius as Patriarch of Constantinople.
Cyril is counted among the Church Fathers and also as a Doctor of the Church, and his reputation within the Christian world has resulted in his titles "Pillar of Faith" and "Seal of all the Fathers". 
The Nestorian bishops at their synod at the Council of Ephesus declared him a heretic, labelling him as a "monster, born and educated for the destruction of the church."

Cyril is well known for his dispute with Nestorius and his supporter, Patriarch John of Antioch, whom Cyril excluded from the Council of Ephesus for arriving late. He is also known for his expulsion of Novatians and Jews from Alexandria and for inflaming tensions that led to the murder of the Hellenistic philosopher Hypatia by a Christian mob. Historians disagree over the extent of his responsibility in this.

Cyril tried to oblige the pious Christian emperor Theodosius II (AD 408–450) to himself by dedicating his Paschal table to him. Cyril's Paschal table was provided with a Metonic basic structure in the form of a 19-year lunar cycle adopted by him around AD 425, which was very different from the first Metonic 19-year lunar cycle invented around AD 260 by Anatolius, but exactly equal to the lunar cycle which had been introduced around AD 412 by Annianus; the Julian equivalent of this Alexandrian cycle adopted by Cyril and nowadays referred to as the 'classical (Alexandrian) 19-year lunar cycle' would emerge a century later in Rome as the basic structure of Dionysius Exiguus’ Paschal table (AD 525).

The Catholic Church did not commemorate Saint Cyril in the Tridentine calendar: it added his feast only in 1882, assigning to it the date of 9 February. This date is used by the Western Rite Orthodox Church. Yet the 1969 Catholic Calendar revision moved it to 27 June, considered to be the day of the saint's death, as celebrated by the Coptic Orthodox Church. The same date has been chosen for the Lutheran calendar. The Eastern Orthodox and Byzantine Catholic Churches celebrate his feast day on 9 June and also, together with Pope Athanasius I of Alexandria, on 18 January.

Cyril is remembered in the Church of England with a commemoration on 27 June.

Little is known for certain of Cyril's early life. He was born circa 376, in the town of Didouseya, Egypt, modern-day El-Mahalla El-Kubra. A few years after his birth, his maternal uncle Theophilus rose to the powerful position of Patriarch of Alexandria. His mother remained close to her brother and under his guidance, Cyril was well educated. His writings show his knowledge of Christian writers of his day, including Eusebius, Origen, Didymus the Blind, and writers of the Church of Alexandria. He received the formal Christian education standard for his day: he studied grammar from age twelve to fourteen (390–392), rhetoric and humanities from fifteen to twenty (393–397) and finally theology and biblical studies (398–402).

In 403, he accompanied his uncle to attend the "Synod of the Oak" in Constantinople, which deposed John Chrysostom as Archbishop of Constantinople. The prior year, Theophilus had been summoned by the emperor to Constantinople to apologize before a synod, over which Chrysostom would preside, on account of several charges which were brought against him by certain Egyptian monks. Theophilus had them persecuted as Origenists. Placing himself at the head of soldiers and armed servants, Theophilus had marched against the monks, burned their dwellings, and ill-treated those whom he captured. Theophilus arrived at Constantinople with twenty-nine of his suffragan bishops, and conferring with those opposed to the Archbishop, drafted a long list of largely unfounded accusations against Chrysostom, who refused to recognize the legality of a synod in which his open enemies were judges. Chrysostom was subsequently deposed.

Theophilus died on 15 October 412, and Cyril was made Pope or Patriarch of Alexandria on 18 October 412, but only after a riot between his supporters and those of his rival Archdeacon Timotheus. According to Socrates Scholasticus, the Alexandrians were always rioting.

Thus, Cyril followed his uncle in a position that had become powerful and influential, rivalling that of the prefect in a time of turmoil and frequently violent conflict between the cosmopolitan city's pagan, Jewish, and Christian inhabitants. He began to exert his authority by causing the churches of the Novatianists to be closed and their sacred vessels to be seized.

Orestes, "Praefectus augustalis" of the Diocese of Egypt, steadfastly resisted Cyril's ecclesiastical encroachment upon secular prerogatives.

Tension between the parties increased when in 415, Orestes published an edict that outlined new regulations regarding mime shows and dancing exhibitions in the city, which attracted large crowds and were commonly prone to civil disorder of varying degrees. Crowds gathered to read the edict shortly after it was posted in the city's theater. Cyril sent the "grammaticus" Hierax to discover the content of the edict. The edict angered Christians as well as Jews. At one such gathering, Hierax read the edict and applauded the new regulations, prompting a disturbance. Many people felt that Hierax was attempting to incite the crowd—particularly the Jews—into sedition. Orestes had Hierax tortured in public in a theatre. This order had two aims: one to quell the riot, the other to mark Orestes' authority over Cyril.

Socrates Scholasticus recounts that upon hearing of Hierex's severe and public punishment, Cyril threatened to retaliate against the Jews of Alexandria with "the utmost severities" if the harassment of Christians did not cease immediately. In response to Cyril's threat, the Jews of Alexandria grew even more furious, eventually resorting to violence against the Christians. They plotted to flush the Christians out at night by running through the streets claiming that the Church of Alexander was on fire. When Christians responded to what they were led to believe was the burning down of their church, "the Jews immediately fell upon and slew them" by using rings to recognize one another in the dark and killing everyone else in sight. When the morning came, Cyril, along with many of his followers, took to the city's synagogues in search of the perpetrators of the massacre.

According to Socrates, after Cyril rounded up all the Jews in Alexandria he ordered them to be stripped of all possessions, banished them from Alexandria, and allowed their goods to be pillaged by the remaining citizens of Alexandria. Scholasticus alleges that all the Jews of Alexandria were banished, while John of Nikiû says it was only those involved in the ambush and massacre. Susan Wessel says that, while it is not clear whether Scholasticus was a Novationist (whose churches Cyril had closed), he was apparently sympathetic towards them, and repeatedly accuses Cyril of abusing his episcopal power by infringing on the rights and duties of the secular authorities. Wessel says, however, "...Socrates probably does not provide accurate and unambiguous information about Cyril's relationship to imperial authority".

Nonetheless, with Cyril's banishment of the Jews, however many, "Orestes [...] was filled with great indignation at these transactions, and was excessively grieved that a city of such magnitude should have been suddenly bereft of so large a portion of its population." Because of this, the feud between Cyril and Orestes intensified, and both men wrote to the emperor regarding the situation. Eventually, Cyril attempted to reach out to Orestes through several peace overtures, including attempted mediation and, when that failed, showed him the Gospels, which he interpreted to indicate that the religious authority of Cyril would require Orestes' acquiescence in the bishop's policy. Nevertheless, Orestes remained unmoved by such gestures.

This refusal almost cost Orestes his life. Nitrian monks came from the desert and instigated a riot against Orestes among the population of Alexandria. These monks had resorted to violence 15 years before, during a controversy between Theophilus (Cyril's uncle) and the "Tall Brothers"; the monks assaulted Orestes and accused him of being a pagan. Orestes rejected the accusations, showing that he had been baptised by the Archbishop of Constantinople. A monk named Ammonius threw a stone hitting Orestes in the head. The prefect had Ammonius tortured to death, whereupon the Patriarch allegedly honored him as a martyr. However, at least according to Scholasticus, the Christian community displayed a general lack of enthusiasm for Ammonius's case for martyrdom. The prefect then wrote to the emperor Theodosius II, as did Cyril.

The Prefect Orestes enjoyed the political backing of Hypatia, an astronomer, philosopher and mathematician who had considerable moral authority in the city of Alexandria, and who had extensive influence. Indeed, many students from wealthy and influential families came to Alexandria purposely to study privately with Hypatia, and many of these later attained high posts in government and the Church. Several Christians thought that Hypatia's influence had caused Orestes to reject all conciliatory offerings by Cyril. Modern historians think that Orestes had cultivated his relationship with Hypatia to strengthen a bond with the pagan community of Alexandria, as he had done with the Jewish one, in order to better manage the tumultuous political life of the Egyptian capital.

According to Socrates Scholasticus during the Christian season of Lent in March 415, a mob of Christians under the leadership of a lector named Peter, raided Hypatia's carriage as she was travelling home. They dragged her into a building known as the "Kaisarion", a former pagan temple and center of the Roman imperial cult in Alexandria that had been converted into a Christian church. There, the mob stripped Hypatia naked and murdered her using "ostraka", which can either be translated as "roof tiles" or "oyster shells". Later historian John of Nikiû also tells a similar story. Even later historian Byzantinist Fr. Adrian Fortescue, says that the mob of Christian Parabalanies and Peter, cruelly tore her to pieces on the "steps" of a church. Damascius adds that they also cut out her eyeballs. They tore her body into pieces and dragged her limbs through the town to a place called Cinarion, where they set them on fire. According to Watts, this was in line with the traditional manner in which Alexandrians carried the bodies of the "vilest criminals" outside the city limits to cremate them as a way of symbolically purifying the city.

Although Socrates Scholasticus never explicitly identifies Hypatia's murderers, they are commonly assumed to have been members of the "parabalani". Christopher Haas disputes this identification, arguing that the murderers were more likely "a crowd of Alexandrian laymen". Socrates Scholasticus unequivocally condemns the actions of the mob, declaring, "Surely nothing can be farther from the spirit of Christianity than the allowance of massacres, fights, and transactions of that sort."

Neoplatonist historian Damascius ( 458 –  538) was "anxious to exploit the scandal of Hypatia's death", and attributed responsibility for her murder to Bishop Cyril and his Christian followers. Damascius's account of the Christian murder of Hypatia is the sole historical source naming Bishop Cyril. Some modern studies, as well as the 2009 Hypatia biopic "Agora" represent Hypatia as falling casualty to a conflict between two Christian factions, one peaceful and moderate and led by Orestes, with the support of Hypatia, and fundamentalist faction enforced by Parabalani and led by Patriarch Cyril. According to lexicographer William Smith, "She was accused of too much familiarity with Orestes, prefect of Alexandria, and the charge spread among the clergy, who took up the notion that she interrupted the friendship of Orestes with their archbishop, Cyril." Scholasticus, alleges that Hypatia fell "victim to the political jealousy which at the time prevailed" and that news of Hypatia's murder, "brought no small disgrace", not only to Patriarch Cyril but to the whole Christian Church in Alexandria, "for murder and slaughter and all such things are altogether opposed to the Christian religion."

After the murder, a deputation of citizens went to Constantinople to petition the Emperor for an investigation so as to prevent such horrors in the future and to put down the disorderly Parabalani, however they urged for the Patriarch to be allowed to remain in the city (Orestes wanted him banished). One could deduce from this that there were some who didn't think Cyril responsible for this or that even his own followers thought he went too far. However, according to Damascius, Cyril himself allegedly only managed to escape even more serious punishment by bribing one of Theodosius's officials. Indeed, the investigation resulted in the emperors Honorius and Theodosius II issuing an edict in autumn of 416, which attempted to remove the "parabalani" from Cyril's power and instead place them under the authority of Orestes. The edict restricted the parabalani from attending "any public spectacle whatever" or entering "the meeting place of a municipal council or a courtroom." It also severely restricted their recruitment by limiting the total number of parabalani to no more than five hundred.

A further argument in favour of Cyril's exoneration is that if ever a man had bitter enemies it was Cyril. Wilful murder was considered just as unsuitable conduct for bishops in the fifth century as it is now. Why, during all the fierce conflict with the Nestorians, when they brought up every possible charge against him, did no one think of calling him Hypatia's murderer? Although to accuse a Saint of this horrid story is a gross calumny, there is no doubt that in other ways he did give annoyance to the government."

Another major conflict was between the Alexandrian and Antiochian schools of ecclesiastical reflection, piety, and discourse. This long running conflict widened with the third canon of the First Council of Constantinople which granted the see of Constantinople primacy over the older sees of Alexandria and Antioch. Thus, the struggle between the sees of Alexandria and Antioch now included Constantinople. The conflict came to a head in 428 after Nestorius, who originated in Antioch, was made Archbishop of Constantinople.

Cyril gained an opportunity to restore Alexandria's pre-eminence over both Antioch and Constantinople when an Antiochine priest who was in Constantinople at Nestorius' behest began to preach against calling Mary the "Mother of God" ("Theotokos"). As the term "Mother of God" had long been attached to Mary, the laity in Constantinople complained against the priest. Rather than repudiating the priest, Nestorius intervened on his behalf. Nestorius argued that Mary was neither a "Mother of Man" nor "Mother of God" as these referred to Christ's two natures; rather, Mary was the "Mother of Christ" (Greek: "Christotokos"). Christ, according to Nestorius, was the conjunction of the Godhead with his "temple" (which Nestorius was fond of calling his human nature). The controversy seemed to be centered on the issue of the suffering of Christ. Cyril maintained that the Son of God or the divine Word, truly suffered "in the flesh." However, Nestorius claimed that the Son of God was altogether incapable of suffering, even within his union with the flesh. Eusebius of Dorylaeum went so far as to accuse Nestorius of adoptionism. By this time, news of the controversy in the capital had reached Alexandria. At Easter 429 A.D., Cyril wrote a letter to the Egyptian monks warning them of Nestorius's views. A copy of this letter reached Constantinople where Nestorius preached a sermon against it. This began a series of letters between Cyril and Nestorius which gradually became more strident in tone. Finally, Emperor Theodosius II convoked the Council of Ephesus (in 431) to solve the dispute. Cyril selected Ephesus as the venue since it supported the veneration of Mary. The council was convoked before Nestorius's supporters from Antioch and Syria had arrived and thus Nestorius refused to attend when summoned. Predictably, the Council ordered the deposition and exile of Nestorius for heresy.

However, when John of Antioch and the other pro-Nestorius bishops finally reached Ephesus, they assembled their own Council, condemned Cyril for heresy, deposed him from his see, and labelled him as a "monster, born and educated for the destruction of the church". Theodosius, by now old enough to hold power by himself, annulled the verdict of the Council and arrested Cyril, but Cyril eventually escaped. Having fled to Egypt, Cyril bribed Theodosius's courtiers, and sent a mob led by Dalmatius, a hermit, to besiege Theodosius's palace, and shout abuse; the emperor eventually gave in, sending Nestorius into minor exile (Upper Egypt).
Cyril died about 444, but the controversies were to continue for decades, from the "Robber Synod" of Ephesus (449) to the Council of Chalcedon (451) and beyond.

Cyril regarded the embodiment of God in the person of Jesus Christ to be so mystically powerful that it spread out from the body of the God-man into the rest of the race, to reconstitute human nature into a graced and deified condition of the saints, one that promised immortality and transfiguration to believers. Nestorius, on the other hand, saw the incarnation as primarily a moral and ethical example to the faithful, to follow in the footsteps of Jesus. Cyril's constant stress was on the simple idea that it was God who walked the streets of Nazareth (hence Mary was "Theotokos", meaning "God bearer", which became in Latin "Mater Dei or Dei Genitrix", or Mother of God), and God who had appeared in a transfigured humanity. Nestorius spoke of the distinct "Jesus the man" and "the divine Logos" in ways that Cyril thought were too dichotomous, widening the ontological gap between man and God in a way that some of his contemporaries believed would annihilate the person of Christ.

The main issue that prompted this dispute between Cyril and Nestorius was the question which arose at the Council of Constantinople: What exactly was the being to which Mary gave birth? Cyril affirmed that the Holy Trinity consists of a singular divine nature, essence, and being ("ousia") in three distinct aspects, instantiations, or subsistencies of being ("hypostases"). These distinct hypostases are the Father, the Son or Word ("Logos"), and the Holy Spirit. Then, when the Son became flesh and entered the world, the pre-Incarnate divine nature and assumed human nature both remained, but became "united" in the person of Jesus. This resulted in the miaphysite slogan "One Nature united out of two" being used to encapsulate the theological position of this Alexandrian bishop.

According to Cyril's theology, there were two states for the Son of God: the state that existed "prior" to the Son (or Word/Logos) becoming enfleshed in the person of Jesus and the state that actually became enfleshed. The Logos Incarnate suffered and died on the Cross, and therefore the Son was able to suffer without suffering. Cyril passionately argued for the continuity of a single subject, God the Word, from the pre-Incarnate state to the Incarnate state. The divine Logos was really present in the flesh and in the world—not merely bestowed upon, semantically affixed to, or morally associated with the man Jesus, as the adoptionists and, he believed, Nestorius had taught.

Cyril of Alexandria became noted in Church history because of his spirited fight for the title "Theotokos" during the First Council of Ephesus (431), establishing the ecclesiastically settled basis for all subsequent mariological developments. Prior to the controversy over the theology of Nestorius, Cyril rarely if ever used the Mariological title, but theo-political circumstances compelled him as Archbishop of one of the Empire's chief sees, to become involved and develop his theology."

Beginning in 429 Cyril wrote a series of letters to various ecclesiastical authorities in which he espoused the orthodoxy of "Theotokos". The propriety of the term was justified through appeals to earlier theologians who had used it, like Athanasius, the Cappadocians, and Atticus. Following an epistolary exchange with the increasingly unpopular archbishop of Constantinople, in 430 Cyril wrote his famous 12 Anathemas in which anyone who refused to call Mary Theotokos was condemned. The following year over 100 bishops met in council at Ephesus to rule on the disputes. In between sessions at the Council Cyril delivered a number of sermons; some of those attributed to his hand are of disputed authorship, but 6 are recognised as genuine. Homily IV delivered upon the late arrival of western delegates is a particularly striking example of Cyril's developed Mariology. It is the foremost expression of Cyril's devotion to Mary, and is one of the first historical attestations of the salutation Χαῖρε ("Hail") being used to invoke the Virgin, a practice later standardised in Byzantine homiletics and hymnography such as the sermons of Chrysippus and Basil of Selecucia, and the Akathist hymn. Mary, who is credited with calling the council fathers together, embodies for Cyril the paradoxes of orthodox Christology, "container of the uncontained" and "the place for the infinite", among other lauded descriptions. Cyril's notions of the identity of Christ, therefore, have direct bearing on the identity of Mary. Wessell explains how "Cyril's spatial metaphors construed Mary as a sacred place" and how he "applied metaphors depicting royalty and exaltation to Mary: she was the treasure of the world, the crown of virginity, and the sceptre of orthodoxy." Subsequently, such praise would become normative in Marian theology.

In several of his works, Cyril focuses on the love of Jesus to his mother. On the Cross, he overcomes his pain and thinks of his mother. At the wedding in Cana, he bows to her wishes. The conflict with Nestorius was mainly over this issue, and some have argued that it has often been misunderstood. "[T]he debate was not so much about Mary as about Jesus. The question was not what honors were due to Mary, but how one was to speak of the birth of Jesus." Wessell notes that in Homily V delivered at the council, Cyril argued that Nestorius' refusal to acknowledge God's incarnate birth from Mary was a blasphemy against Christ. At the same time, the close relationship between Christological and Mariological formulations going back to the Cappadocian Fathers created a climate wherein intellectual argumentation over disputed theology overlapped with blossoming lay piety. When the Council of Ephesus convened under Cyril's presidency it did so in the newly constructed Church of Mary, a venue that contributed to the devotional matrix of the debates. Whereas in the past scholars have often argued that Marian piety and theology only developed in the wake of the conciliar decrees, Shoemaker considers this to be refuted by the picture emerging from liturgical and archaeological evidence. The substance of Cyril's arumentation was Christological in orientation. His mature Mariology was chiefly in service to this, and to the end of discrediting Nestorius. Yet Wessel, quoting Homily IV, notes that the enthusiastic praises go beyond the strictly Christological. "She was not only valuable as a vessel storing something sacred but was herself precious and venerated: ‘Is it even possible for people to speak of the celebrated Mary? The virginal womb; O thing of wonder! The marvel strikes me with awe!’" Such sentiments served to further distinguish what Cyril believed to be orthodox theology from that which Nestorius taught, characterising the latter as subversive to both church and empire. As "scepter of orthodoxy", Mary became the standard of Christological fidelity in Cyril's theology; Nestorius's denial of "Theotokos" became the identifiable sign of his impugning of the divinity of Jesus.

St. Cyril received an important recognition of his preachings by the Second Council of Constantinople (553 d.C.) which declared;

Cyril was a scholarly archbishop and a prolific writer. In the early years of his active life in the Church he wrote several exegetical documents. Among these were: "Commentaries on the Old Testament", "Thesaurus", "Discourse Against Arians", "Commentary on St. John's Gospel", and "Dialogues on the Trinity". In 429 as the Christological controversies increased, the output of his writings was so extensive that his opponents could not match it. His writings and his theology have remained central to the tradition of the Fathers and to all Orthodox to this day.





Cyril of Jerusalem

Cyril of Jerusalem (, "Kýrillos A Ierosolýmon"; ; 386) was a theologian of the Early Church. About the end of AD 350, he succeeded Maximus as Bishop of Jerusalem, but was exiled on more than one occasion due to the enmity of Acacius of Caesarea, and the policies of various emperors. Cyril left important writings documenting the instruction of catechumens and the order of the Liturgy in his day.

Cyril is venerated as a saint within the Roman Catholic Church, the Eastern Orthodox Church, Oriental Orthodox Church, and the Anglican Communion. In 1883, Cyril was declared a Doctor of the Church by Pope Leo XIII.

Cyril is remembered in the Church of England with a commemoration on 18 March.

Little is known of his life before he became a bishop; the assignment of his birth to the year 315 rests on conjecture. According to Butler, Cyril was born at or near the city of Jerusalem, and was apparently well-read in both the writings of the early Christian theologians and the Greek philosophers.

Cyril was ordained a deacon by Bishop Macarius of Jerusalem in about 335 and a priest some eight years later by Bishop Maximus. Around the end of 350 he succeeded Maximus in the See of Jerusalem, although the evidence for this relies on the "Catecheses" written by Cyril where he refers to himself as "bishop". Jerome also suggests Cyril was an Arian at this stage.

Cyril is described as preacher and liturgist by the pilgrim Egeria.

Relations between Metropolitan Acacius of Caesarea and Cyril became strained. Acacius is presented as a leading Arian by the orthodox historians, and his opposition to Cyril in the 350s is attributed by these writers to this. Sozomen also suggests that the tension may have been increased by Acacius's jealousy of the importance assigned to Cyril's See by the Council of Nicaea, as well as by the threat posed to Caesarea by the rising influence of the seat of Jerusalem as it developed into the prime Christian holy place and became a centre of pilgrimage.

Acacius charged Cyril with selling church property. The city of Jerusalem had suffered drastic food shortages at which point church historians Sozomen and Theodoret report "Cyril secretly sold sacramental ornaments of the church and a valuable holy robe, fashioned with gold thread that the emperor Constantine had once donated for the bishop to wear when he performed the rite of Baptism", possibly to keep people from starving.

For two years, Cyril resisted Acacius' summons to account for his actions, but a church council held under Acacius's influence in 357 deposed Cyril in his absence, and Cyril took refuge with Silvanus, Bishop of Tarsus. The following year, 359, in an atmosphere more hostile to Acacius, the Council of Seleucia reinstated Cyril and deposed Acacius. In 360 this was reversed by Emperor Constantius again, and Cyril suffered another year's exile from Jerusalem until the Emperor Julian's accession allowed him to return in 361.

Cyril was once again banished from Jerusalem by the Arian Emperor Valens in 367, but was able to return again after Valens's death in 378, after which he remained undisturbed until his death in 386. In 380, Gregory of Nyssa came to Jerusalem on the recommendation of a council held at Antioch in the preceding year. He seemingly found the faith in good shape, but worried that the city was prey to parties and corrupt in morals. Cyril's jurisdiction over Jerusalem was expressly confirmed by the First Council of Constantinople (381), at which he was present. At that council he voted for acceptance of the term "homoousios" (which defined the nature between "God the Father", and "God the Son"), having been finally convinced that there was no better alternative. His story is perhaps best representative of those Eastern bishops (perhaps a majority) initially mistrustful of Nicaea, who came to accept the creed of that council, and the doctrine of the "homoousion".

Though his theology was at first somewhat indefinite in phraseology, he undoubtedly gave a thorough adhesion to the Nicene Orthodoxy. Even if he did avoid the debatable term "homoousios", he expressed its sense in many passages, which exclude equally Patripassianism, Sabellianism, and the formula "there was a time when the Son was not" attributed to Arius. In other points he takes the ordinary ground of the Eastern Fathers, as in the emphasis he lies on the freedom of the will, the "autexousion" (αὐτεξούσιον), and in his view of the nature of sin. To him sin is the consequence of freedom, not a natural condition. The body is not the cause, but the instrument of sin. The remedy for it is repentance, on which he insists. Like many of the Eastern Fathers, he focuses on high moral living as essential to true Christianity. His doctrine of the Resurrection is not quite so realistic as that of other Fathers; but his conception of the Church is decidedly empirical: the existing Church form is the true one, intended by Christ, the completion of the Church of the Old Testament. His interpretation of the Eucharist is disputed. Some argue he sometimes seems to approach the symbolic view, though he professes a strong realistic doctrine. The bread and wine are not mere elements, but the body and blood of Christ.

Cyril's writings are filled with the loving and forgiving nature of God which was somewhat uncommon during his time period. Cyril fills his writings with great lines of the healing power of forgiveness and the Holy Spirit, like "The Spirit comes gently and makes himself known by his fragrance. He is not felt as a burden for God is light, very light. Rays of light and knowledge stream before him as the Spirit approaches. The Spirit comes with the tenderness of a true friend to save, to heal, to teach, to counsel, to strengthen and to console". Cyril himself followed God's message of forgiveness many times throughout his life. This is most clearly seen in his two major exiles where Cyril was disgraced and forced to leave his position and his people behind. He never wrote or showed any ill will towards those who wronged him. Cyril stressed the themes of healing and regeneration in his catechesis.

Cyril's famous twenty-three lectures given to catechumens in Jerusalem being prepared for, and after, baptism are best considered in two parts: the first eighteen lectures are commonly known as the "Catechetical Lectures", "Catechetical Orations" or "Catechetical Homilies", while the final five are often called the "Mystagogic Catecheses" (μυσταγωγικαί), because they deal with the "mysteries" (μυστήρια) i.e. Sacraments of Baptism, Confirmation and the Eucharist.

His catechetical lectures (Greek Κατηχήσεις, "Katēchēseis") are generally assumed, on the basis of limited evidence, to have been delivered either in Cyril's early years as a bishop, around 350, or perhaps in 348, while Cyril was still a priest, deputising for his bishop, Maximus. The "Catechetical Lectures" were given in the "Martyrion", the basilica erected by Constantine. They contain instructions on the principal topics of Christian faith and practice, in a popular rather than scientific manner, full of a warm pastoral love and care for the catechumens to whom they were delivered. Each lecture is based upon a text of Scripture, and there is an abundance of Scriptural quotation throughout. In the "Catechetical Lectures", parallel with the exposition of the Creed as it was then received in the Church of Jerusalem are vigorous polemics against pagan, Jewish, and heretical errors. They are of great importance for the light which they throw upon the method of instruction usual of that age, as well as upon the liturgical practises of the period, of which they give the fullest account extant.

It is not only among us, who are marked with the name of Christ, that the dignity of faith is great; all the business of the world, even of those outside the Church, is accomplished by faith. By faith, marriage laws join in union persons who were strangers to one another. By faith, agriculture is sustained; for a man does not endure the toil involved unless he believes he will reap a harvest. By faith, seafaring men, entrusting themselves to a tiny wooden craft, exchange the solid element of the land for the unstable motion of the waves."

In the 13th lecture, Cyril of Jerusalem discusses the Crucifixion and burial of Jesus Christ. The main themes that Cyril focuses on in these lectures are Original sin and Jesus' sacrificing himself to save us from our sins. Also, the burial and Resurrection which occurred three days later proving the divinity of Jesus Christ and the loving nature of the Father. Cyril was very adamant about the fact that Jesus went to his death with full knowledge and willingness. Not only did he go willingly but throughout the process he maintained his faith and forgave all those who betrayed him and engaged in his execution. Cyril writes "who did not sin, neither was deceit found in his mouth, who, when he was reviled, did not revile, when he suffered did not threaten". This line by Cyril shows his belief in the selflessness of Jesus especially in this last final act of Love. The lecture also gives a sort of insight to what Jesus may have been feeling during the execution from the whippings and beatings, to the crown of thorns, to the nailing on the cross. Cyril intertwines the story with the messages Jesus told throughout his life before his execution relating to his final act. For example, Cyril writes "I gave my back to those who beat me and my cheeks to blows; and my face I did not shield from the shame of spitting". This clearly reflects the teachings of Jesus to turn the other cheek and not raising your hands against violence because violence just begets violence begets violence. The segment of the Catechesis really reflects the voice Cyril maintained in all of his writing. The writings always have the central message of the Bible; Cyril is not trying to add his own beliefs in reference to religious interpretation and remains grounded in true biblical teachings.

Danielou sees the baptism rite as carrying eschatological overtones, in that "to inscribe for baptism is to write one's name in the register of the elect in heaven".

Oded Irshai observed that Cyril lived in a time of intense apocalyptic expectation, when Christians were eager to find apocalyptic meaning in every historical event or natural disaster. Cyril spent a good part of his episcopacy in intermittent exile from Jerusalem. Abraham Malherbe argued that when a leader's control over a community is fragile, directing attention to the imminent arrival of the antichrist effectively diverts attention from that fragility.

Soon after his appointment, Cyril in his "Letter to Constantius" of 351 recorded the appearance of a cross of light in the sky above Golgotha, witnessed by the whole population of Jerusalem. The Greek church commemorates this miracle on 7 May. Though in modern times the authenticity of the "Letter" has been questioned, on the grounds that the word "homoousios" occurs in the final blessing, many scholars believe this may be a later interpolation, and accept the letter's authenticity on the grounds of other pieces of internal evidence.

Cyril interpreted this as both a sign of support for Constantius, who was soon to face the usurper Magnentius, and as announcing the Second Coming, which was soon to take place in Jerusalem. Not surprisingly, in Cyril's eschatological analysis, Jerusalem holds a central position.

Matthew 24:6 speaks of "wars and reports of wars", as a sign of the End Times, and it is within this context that Cyril read Julian's war with the Persians. Matthew 24:7 speaks of "earthquakes from place to place", and Jerusalem experienced an earthquake in 363 at a time when Julian was attempting to rebuild the temple in Jerusalem. Embroiled in a rivalry with Acacius of Caesarea over the relative primacy of their respective sees, Cyril saw even ecclesial discord a sign of the Lord's coming. Catechesis 15 would appear to cast Julian as the antichrist, although Irshai views this as a later interpolation.

"In His first coming, He endured the Cross, despising shame; in His second, He comes attended by a host of Angels, receiving glory. We rest not then upon His first advent only, but look also for His second." He looked forward to the Second Advent which would bring an end to the world and then the created world to be made anew. At the Second Advent he expected to rise in the resurrection if it came after his time on earth.

There has been considerable controversy over the date and authorship of the "Mystagogic Catecheses", addressed to the newly baptized, in preparation for the reception of Holy Communion, with some scholars having attributed them to Cyril's successor as Bishop of Jerusalem, John. Many scholars would currently view the "Mystagogic Catecheses" as being written by Cyril, but in the 370s or 380s, rather than at the same time as the "Catechetical Lectures".

According to the Spanish pilgrim Egeria, these "mystagogical catecheses" were given to the newly baptised in the Church of the "Anastasis" in the course of Easter Week.







Hanukkah

Hanukkah (; "Ḥănukkā" ) is a Jewish festival commemorating the recovery of Jerusalem and subsequent rededication of the Second Temple at the beginning of the Maccabean Revolt against the Seleucid Empire in the 2nd century BCE.

Hanukkah is observed for eight nights and days, starting on the 25th day of Kislev according to the Hebrew calendar, which may occur at any time from late November to late December in the Gregorian calendar. The festival is observed by lighting the candles of a candelabrum with nine branches, commonly called a menorah or hanukkiah. One branch is typically placed above or below the others and its candle is used to light the other eight candles. This unique candle is called the "shammash" (, "attendant"). Each night, one additional candle is lit by the "shammash" until all eight candles are lit together on the final night of the festival. 

Other Hanukkah festivities include singing Hanukkah songs, playing the game of dreidel and eating oil-based foods, such as latkes and sufganiyot, and dairy foods. Since the 1970s, the worldwide Chabad Hasidic movement has initiated public menorah lightings in open public places in many countries.

Originally instituted as a feast "in the manner of Sukkot (Booths)", it does not come with the corresponding obligations, and is therefore a relatively minor holiday in strictly religious terms. Nevertheless, Hanukkah has attained major cultural significance in North America and elsewhere, especially among secular Jews, due to often occurring around the same time as Christmas during the festive season.

The name "Hanukkah" derives from the Hebrew verb "", meaning "to dedicate". On Hanukkah, the Maccabean Jews regained control of Jerusalem and rededicated the Temple.

Many homiletical explanations have been given for the name:

In Hebrew, the word Hanukkah is written or (). It is most commonly transliterated to English as "Hanukkah" or "". The spelling "Hanukkah", which is based on using characters of the English alphabet as symbols to re-create the word's correct spelling in Hebrew, is the most common and the preferred choice of Merriam–Webster, "Collins English Dictionary", the "Oxford Style Manual", and the style guides of "The New York Times" and "The Guardian". The sound represented by "Ch" (, similar to the Scottish pronunciation of "loch") is not native to the English language, although it is native to the Welsh language. Furthermore, the letter "ḥeth" (), which is the first letter in the Hebrew spelling, is pronounced differently in modern Hebrew (voiceless uvular fricative) from in classical Hebrew (voiceless pharyngeal fricative ), and neither of those sounds is unambiguously representable in English spelling. However, its original sound is closer to the English "H" than to the Scottish "Ch", and "Hanukkah" more accurately represents the spelling in the Hebrew alphabet. Moreover, the 'kaf' consonant is geminate in classical (but not modern) Hebrew. Adapting the classical Hebrew pronunciation with the geminate and pharyngeal can lead to the spelling "Hanukkah", while adapting the modern Hebrew pronunciation with no gemination and uvular leads to the spelling .

In Modern Hebrew, Hanukkah may also be called the Festival of Lights (, ), based on a comment by Josephus in "Antiquities of the Jews", καὶ ἐξ ἐκείνου μέχρι τοῦ δεῦρο τὴν ἑορτὴν ἄγομεν καλοῦντες αὐτὴν φῶτα "And from then on we celebrate this festival, and we call it Lights". The first Hebrew translation of "Antiquities" (1864) used () "Festival of Lamps", but the translation "Festival of Lights" () appeared by the end of the nineteenth century.

The story of Hanukkah is told in the books of the First and Second Maccabees, which describe in detail the rededication of the Temple in Jerusalem and the lighting of the menorah. These books, however, are not a part of the canonized Masoretic Text version of the Tanakh (Hebrew and Aramaic language Jewish Bible) used and accepted by normative Rabbinical Judaism and therefore modern Jews (as copied, edited and distributed by a group of Jews known as the Masoretes between the 7th and 10th centuries of the Common Era). However, the books of Maccabees were included among the deuterocanonical books added to the Septuagint, a Jewish scholarly Greek-language translation of the Hebrew Bible originally compiled in the mid-3rd century BCE. The Roman Catholic and Orthodox Churches consider the books of Maccabees as a canonical part of the Old Testament. 

The eight-day rededication of the temple is described in 1 Maccabees, though the miracle of the oil does not appear here. A story similar in character, and older in date, is the one alluded to in 2 Maccabees according to which the relighting of the altar fire by Nehemiah was due to a miracle which occurred on the 25th of Kislev, and which appears to be given as the reason for the selection of the same date for the rededication of the altar by Judah Maccabee. The above account in 1 Maccabees, as well as 2 Maccabees portrays the feast as a delayed observation of the eight-day Feast of Booths (Sukkot); similarly 2 Maccabees explains the length of the feast as "in the manner of the Feast of Booths".

Megillat Taanit (1st century) contains a list of festive days on which fasting or eulogizing is forbidden. It specifies, "On the 25th of [Kislev] is Hanukkah of eight days, and one is not to eulogize" and then references the story of the rededication of the Temple.

The Mishna (late 2nd century) mentions Hanukkah in several places, but never describes its laws in detail and never mentions any aspect of the history behind it. To explain the Mishna's lack of a systematic discussion of Hanukkah, Nissim ben Jacob postulated that information on the holiday was so commonplace that the Mishna felt no need to explain it. Modern scholar Reuvein Margolies suggests that as the Mishnah was redacted after the Bar Kochba revolt, its editors were reluctant to include explicit discussion of a holiday celebrating another relatively recent revolt against a foreign ruler, for fear of antagonizing the Romans.

The miracle of the one-day supply of oil miraculously lasting eight days is described in the Talmud, committed to writing about 600 years after the events described in the books of Maccabees. The Talmud says that after the forces of Antiochus IV had been driven from the Temple, the Maccabees discovered that almost all of the ritual olive oil had been profaned. They found only a single container that was still sealed by the High Priest, with enough oil to keep the menorah in the Temple lit for a single day. They used this, yet it burned for eight days (the time it took to have new oil pressed and made ready).

The Talmud presents three options:

Except in times of danger, the lights were to be placed outside one's door, on the opposite side of the mezuza, or in the window closest to the street. Rashi, in a note to "Shabbat 21b," says their purpose is to publicize the miracle. The blessings for Hanukkah lights are discussed in tractate "Succah," p. 46a.

Megillat Antiochus (probably composed in the 2nd century) concludes with the following words:

The Al HaNissim prayer is recited on Hanukkah as an addition to the Amidah prayer, which was formalized in the late 1st century. "Al HaNissim" describes the history of the holiday as follows:

The Jewish historian Titus Flavius Josephus narrates in his book, Jewish Antiquities XII, how the victorious Judas Maccabeus ordered lavish yearly eight-day festivities after rededicating the Temple in Jerusalem that had been profaned by Antiochus IV Epiphanes. Josephus does not say the festival was called Hanukkah but rather the "Festival of Lights":

In the New Testament, John 10:22–23 says, "Then came the Festival of Dedication at Jerusalem. It was winter, and Jesus was in the temple courts walking in Solomon's Colonnade" (NIV). The Greek noun used appears in the neuter plural as "the renewals" or "the consecrations" (; ). The same root appears in 2 Esdras 6:16 in the Septuagint to refer specifically to Hanukkah. This Greek word was chosen because the Hebrew word for 'consecration' or 'dedication' is "Hanukkah" (). The Aramaic New Testament uses the Aramaic word (a close synonym), which literally means 'renewal' or 'to make new'.

After the death of Alexander the Great in 323 BCE, Judea became part of the Ptolemaic Kingdom of Egypt until 200 BCE, when King Antiochus III the Great of Syria defeated King Ptolemy V Epiphanes of Egypt at the Battle of Panium. Judea then became part of the Seleucid Empire of Syria. King Antiochus III the Great, wanting to conciliate his new Jewish subjects, guaranteed their right to "live according to their ancestral customs" and to continue to practice their religion in the Temple of Jerusalem. The Seleucids, like the Ptolemies before them, held a suzerainty over Judea, where they respected Jewish culture and protected Jewish institutions. This policy was drastically reversed by Antiochus IV Epiphanes, the son of Antiochus III, seemingly after what was either a dispute over leadership of the Temple in Jerusalem and the office of High Priest, or possibly a revolt whose nature was lost to time after being crushed. In 175 BCE, Antiochus IV invaded Judea at the request of the sons of Tobias. The Tobiads, who led the Hellenizing Jewish faction in Jerusalem, were expelled to Syria around 170 BCE when the high priest Onias and his pro-Egyptian faction wrested control from them. The exiled Tobiads lobbied Antiochus IV Epiphanes to recapture Jerusalem. As Flavius Josephus relates:
When the Second Temple in Jerusalem was looted and services stopped, Judaism was outlawed. In 167 BCE, Antiochus ordered an altar to Zeus erected in the Temple. He banned brit milah (circumcision) and ordered pigs to be sacrificed at the altar of the temple.

Antiochus's actions provoked a large-scale revolt. Mattathias (Mattityahu), a Jewish priest, and his five sons Jochanan, Simeon, Eleazar, Jonathan, and Judah led a rebellion against Antiochus. It started with Mattathias killing first a Jew who wanted to comply with Antiochus's order to sacrifice to Zeus, and then a Greek official who was to enforce the government's behest (1 Mac. 2, 24–25). Judah became known as Yehuda HaMakabi ("Judah the Hammer"). By 166 BCE, Mattathias had died, and Judah took his place as leader. By 164 BCE, the Jewish revolt against the Seleucid monarchy was successful. The Temple was liberated and rededicated. The festival of Hanukkah was instituted to celebrate this event. Judah ordered the Temple to be cleansed, a new altar to be built in place of the polluted one and new holy vessels to be made. According to the Talmud,"For when the Greeks entered the Sanctuary, they defiled all the oils therein, and when the Hasmonean dynasty prevailed against and defeated them, they made search and found only one cruse of oil which lay with the seal of the kohen gadol (high priest), but which contained sufficient [oil] for one day's lighting only; yet a miracle was wrought therein, and they lit [the lamp] therewith for eight days. The following year these [days] were appointed a Festival with [the recital of] Hallel and thanksgiving."
—Shabbat 21b
Tertiary sources in the Jewish tradition make reference to this account.

Maimonides (12th century) described Hanukkah as follows:

When, on the twenty-fifth of Kislev, the Jews had emerged victorious over their foes and destroyed them, they re-entered the Temple where they found only one jar of pure oil, enough to be lit for only a single day; yet they used it for lighting the required set of lamps for eight days, until they managed to press olives and produce pure oil. Because of this, the sages of that generation ruled that the eight days beginning with the twenty-fifth of Kislev should be observed as days of rejoicing and praising the Lord. Lamps are lit in the evening over the doors of the homes, on each of the eight nights, so as to display the miracle. These days are called Hanukkah, when it is forbidden to lament or to fast, just as it is on the days of Purim. Lighting the lamps during the eight days of Hanukkah is a religious duty imposed by the sages.
Some modern scholars, following the account in 2 Maccabees, observe that the king was intervening in an internal civil war between the Maccabean Jews and the Hellenized Jews in Jerusalem. These competed violently over who would be the High Priest, with traditionalists with Hebrew/Aramaic names like Onias contesting with Hellenizing High Priests with Greek names like Jason and Menelaus. In particular, Jason's Hellenistic reforms would prove to be a decisive factor leading to eventual conflict within the ranks of Judaism. Other authors point to possible socioeconomic reasons in addition to the religious reasons behind the civil war.
What began in many respects as a civil war escalated when the Hellenistic kingdom of Syria sided with the Hellenizing Jews in their conflict with the traditionalists. As the conflict escalated, Antiochus took the side of the Hellenizers by prohibiting the religious practices the traditionalists had rallied around. This may explain why the king, in a total departure from Seleucid practice in all other places and times, banned a traditional religion.

The miracle of the oil is widely regarded as a legend and its authenticity has been questioned since the Middle Ages. However, given the famous question Joseph Karo (1488-1575) posed concerning why Hanukkah is celebrated for eight days when the miracle was only for seven days (since there was enough oil for one day), it was clear that writing in the 16th century CE, he believed it to be a historical event. This belief has been adopted by most of Orthodox Judaism, in as much as Karo's "Shulchan Aruch" is a main code of Jewish Law. The menorah first began to be used as a symbol of Judaism in the Hasmonean period – appearing on coins issued by Hasmonean king Mattathias Antigonus between 40 and 37 BCE, but these coins depict a seven-branched Temple menorah, not a nine-branched vessel that would correspond to an eight-day oil miracle.


Selected battles between the Maccabees and the Seleucid Syrian-Greeks:


Hanukkah is celebrated with a series of rituals that are performed every day throughout the eight-day holiday, some are family-based and others communal. There are special additions to the daily prayer service, and a section is added to the blessing after meals.

Hanukkah is not a "Sabbath-like" holiday, and there is no obligation to refrain from activities that are forbidden on the Sabbath, as specified in the "Shulkhan Arukh". Adherents go to work as usual but may leave early in order to be home to kindle the lights at nightfall. There is no religious reason for schools to be closed, although in Israel schools close from the second day for the whole week of Hanukkah. Many families exchange gifts each night, such as books or games, and "Hanukkah Gelt" is often given to children. Fried foods—such as latkes (potato pancakes), jelly doughnuts (sufganiyot) and Sephardic bimuelos—are eaten to commemorate the importance of oil during the celebration of Hanukkah. Some also have a custom of eating dairy products to remember Judith and how she overcame Holofernes by feeding him cheese, which made him thirsty, and giving him wine to drink. When Holofernes became very drunk, Judith cut off his head.

Each night throughout the eight-day holiday, a candle or oil-based light is lit. As a universally practiced "beautification" (hiddur mitzvah) of the mitzvah, the number of lights lit is increased by one each night. An extra light called a "shammash", meaning "attendant" or "sexton," is also lit each night, and is given a distinct location, usually higher, lower, or to the side of the others.

Among Ashkenazim the tendency is for every male member of the household (and in many families, girls as well) to light a full set of lights each night, while among Sephardim the prevalent custom is to have one set of lights for the entire household.

The purpose of the "shammash" is to adhere to the prohibition, specified in the Talmud, against using the Hanukkah lights for anything other than publicizing and meditating on the Hanukkah miracle. This differs from Sabbath candles which are meant to be used for illumination and lighting. Hence, if one were to need extra illumination on Hanukkah, the "shammash" candle would be available, and one would avoid using the prohibited lights. Some, especially Ashkenazim, light the "shammash" candle first and then use it to light the others. So altogether, including the "shammash", two lights are lit on the first night, three on the second and so on, ending with nine on the last night, for a total of 44 (36, excluding the "shammash"). It is Sephardic custom not to light the shammash first and use it to light the rest. Instead, the shammash candle is the last to be lit, and a different candle or a match is used to light all the candles. Some Hasidic Jews follow this Sephardic custom as well.

The lights can be candles or oil lamps. Electric lights are sometimes used and are acceptable in places where open flame is not permitted, such as a hospital room, or for the very elderly and infirm; however, those who permit reciting a blessing over electric lamps only allow it if it is incandescent and battery operated (an incandescent flashlight would be acceptable for this purpose), while a blessing may not be recited over a plug-in menorah or lamp. Most Jewish homes have a special candelabrum referred to as either a "Chanukiah" (the modern Israeli term) or a "menorah" (the traditional name, simply Hebrew for 'lamp'). Many families use an oil lamp (traditionally filled with olive oil) for Hanukkah. Like the candle Chanukiah, it has eight wicks to light plus the additional "shammash" light.

In the United States, Hanukkah became a more visible festival in the public sphere from the 1970s when Rabbi Menachem M. Schneerson called for public awareness and observance of the festival and encouraged the lighting of public menorahs.

The reason for the Hanukkah lights is not for the "lighting of the house within", but rather for the "illumination of the house without," so that passersby should see it and be reminded of the holiday's miracle (i.e. that the sole cruse of pure oil found which held enough oil to burn for one night actually burned for eight nights). Accordingly, lamps are set up at a prominent window or near the door leading to the street. It is customary amongst some Ashkenazi Jews to have a separate menorah for each family member (customs vary), whereas most Sephardi Jews light one for the whole household. Only when there was danger of antisemitic persecution were lamps supposed to be hidden from public view, as was the case in Persia under the rule of the Zoroastrians, or in parts of Europe before and during World War II. However, most Hasidic groups light lamps near an inside doorway, not necessarily in public view. According to this tradition, the lamps are placed on the opposite side from the "mezuzah", so people passing through the door are surrounded by the holiness of "mitzvot" (the commandments).

Generally, women are exempt in Jewish law from time-bound positive commandments, although the Talmud requires that women engage in the mitzvah of lighting Hanukkah candles "for they too were involved in the miracle."

Some Jews in North America and Israel have taken up environmental concerns in relation to Hanukkah's "miracle of the oil", emphasizing reflection on energy conservation and energy independence. An example of this is the Coalition on the Environment and Jewish Life's renewable energy campaign.

Hanukkah lights should usually burn for at least half an hour after it gets dark. Many light at sundown and those who do so should be careful to have enough oil or wax to last until half an hour after dark. Most Hasidim and many other communities light later, generally around nightfall. Many Hasidic Rebbes light much later to fulfill the obligation of publicizing the miracle by the presence of their Hasidim when they kindle the lights.

Inexpensive small wax candles sold for Hanukkah burn for approximately half an hour so should be lit no earlier than nightfall. Friday night presents a problem, however. Since candles may not be lit on Shabbat itself, the candles must be lit before sunset. However, they must remain lit through the lighting of the Shabbat candles. Therefore, the Hanukkah menorah is lit first with larger candles than usual, followed by the Shabbat candles. At the end of the Shabbat, there are those who light the Hanukkah lights before Havdalah and those who make Havdalah before the lighting Hanukkah lights.

If for whatever reason one didn't light at sunset or nightfall, the lights should be kindled later, as long as there are people in the streets. Later than that, the lights should still be kindled, but the blessings should be recited only if there is at least somebody else awake in the house and present at the lighting of the Hannukah lights.

Typically two blessings ("brachot"; singular: "brachah") are recited during this eight-day festival when lighting the candles. On the first night only, the shehecheyanu blessing is added, making a total of three blessings.

The first blessing is recited before the candles are lit, and while most recite the other blessing(s) beforehand as well, some have the custom to recite them after. On the first night of Hanukkah one light (candle or oil) is lit on the right side of the menorah, on the following night a second light is placed to the left of the first but it is lit first, and so on, proceeding from placing candles right to left but lighting them from left to right over the eight nights.

Transliteration: 

Translation: "Blessed are You, our God, King of the universe, Who has sanctified us with His commandments and commanded us to kindle the Hanukkah light[s]."

Transliteration: 

Translation: "Blessed are You, LORD our God, King of the universe, Who performed miracles for our ancestors in those days at this time..."

After the lights are kindled the hymn "Hanerot Halalu" is recited. There are several different versions; the version presented here is recited in many Ashkenazic communities:

In the Ashkenazi tradition, each night after the lighting of the candles, the hymn Ma'oz Tzur is sung. The song contains six stanzas. The first and last deal with general themes of divine salvation, and the middle four deal with events of persecution in Jewish history, praising God for survival despite these tragedies (the exodus from Egypt, the Babylonian captivity, the miracle of the holiday of Purim, the Hasmonean victory) and expressing a longing for the days when Judea will finally triumph over Rome.

The song was composed in the thirteenth century by a poet only known through the acrostic found in the first letters of the original five stanzas of the song: Mordechai. The familiar tune is most probably a derivation of a German Protestant church hymn or a popular folk song.

After lighting the candles and Ma'oz Tzur, singing other Hanukkah songs is customary in many Jewish homes. Some Hasidic and Sephardi Jews recite Psalms, such as Psalm 30, Psalm 67, and Psalm 91. In North America and in Israel it is common to exchange presents or give children presents at this time. In addition, many families encourage their children to give tzedakah (charity) in lieu of presents for themselves.

An addition is made to the ""hoda'ah"" (thanksgiving) benediction in the Amidah (thrice-daily prayers), called "Al HaNissim" ("On/about the Miracles"). This addition refers to the victory achieved over the Syrians by the Hasmonean Mattathias and his sons.

The same prayer is added to the grace after meals. In addition, the "Hallel" (praise) Psalms are sung during each morning service and the "Tachanun" penitential prayers are omitted.

The Torah is read every day in the shacharit morning services in synagogue, on the first day beginning from Numbers 6:22 (according to some customs, Numbers 7:1), and the last day ending with Numbers 8:4. Since Hanukkah lasts eight days it includes at least one, and sometimes two, Jewish Sabbaths (Saturdays). The weekly Torah portion for the first Sabbath is almost always "Miketz", telling of Joseph's dream and his enslavement in Egypt. The "Haftarah" reading for the first Sabbath Hanukkah is Zechariah 2:14 – Zechariah 4:7. When there is a second Sabbath on Hanukkah, the "Haftarah" reading is from 1 Kings 7:40–50.

The Hanukkah "menorah" is also kindled daily in the synagogue, at night with the blessings and in the morning without the blessings.

The menorah is not lit during Shabbat, but rather prior to the beginning of Shabbat as described above and not at all during the day.
During the Middle Ages "Megillat Antiochus" was read in the Italian synagogues on Hanukkah just as the Book of Esther is read on Purim. It still forms part of the liturgy of the Yemenite Jews.

The last day of Hanukkah is known by some as "Zot Hanukkah" and by others as "Chanukat HaMizbeach", from the verse read on this day in the synagogue Numbers 7:84, "Zot Hanukkat Hamizbe'ach": "This was the dedication of the altar". According to the teachings of Kabbalah and Hasidism, this day is the final "seal" of the High Holiday season of Yom Kippur and is considered a time to repent out of love for God. In this spirit, many Hasidic Jews wish each other "Gmar chatimah tovah" ("may you be sealed totally for good"), a traditional greeting for the Yom Kippur season. It is taught in Hasidic and Kabbalistic literature that this day is particularly auspicious for the fulfillment of prayers.

It is customary for women not to work for at least the first half-hour of the candles' burning, and some have the custom not to work for the entire time of burning. It is also forbidden to fast or to eulogize during Hanukkah.

Some Hasidic scholars teach that the Hanukkah is in fact the final conclusion of God's judgment extending High Holy Days of Rosh Hashana when humanity is judged and Yom Kippur when the judgment is sealed:

Hanukkah songs (in Hebrew except where indicated) include ""Ma'oz Tzur"" (Rock of Ages), ""Latke'le Latke'le"" (Yiddish: "Little Latke, Little Latke"), ""Hanukkiah Li Yesh"" ("I Have a Hanukkah Menorah"), ""Ocho Kandelikas"" (Judeo-Spanish: "Eight Little Candles"), ""Kad Katan"" ("A Small Jug"), ""S'vivon Sov Sov Sov"" ("Dreidel, Spin and Spin"), ""Haneirot Halolu"" ("These Candles Which We Light"), ""Mi Yimalel"" ("Who Can Retell") and ""Ner Li, Ner Li"" ("I have a Candle").

Among the best known songs in English-speaking countries are "Dreidel, Dreidel, Dreidel" and "Oh Chanukah".

In the Nadvorna Hasidic dynasty, it is customary for the rebbes to play violin after the menorah is lit.

Penina Moise's Hannukah Hymn published in the 1842 "Hymns Written for the Use of Hebrew Congregations" was instrumental in the beginning of Americanization of Hanukkah.

There is a custom of eating foods fried or baked in oil (preferably olive oil) to commemorate the miracle of a small flask of oil keeping the Second Temple's Menorah alight for eight days. Traditional foods include potato pancakes, known as "latkes" in Yiddish, especially among Ashkenazi families. Sephardi, Polish, and Israeli families eat jam-filled doughnuts ( "pontshkes"), bimuelos (fritters) and sufganiyot which are deep-fried in oil. Italkim and Hungarian Jews traditionally eat cheese pancakes known as "cassola" or "cheese latkes".
Latkes are not popular in Israel, having been largely replaced by sufganiyot due to local economic factors, convenience and the influence of trade unions. Bakeries in Israel have popularized many new types of fillings for "sufganiyot" besides the traditional strawberry jelly filling, including chocolate cream, vanilla cream, caramel, cappuccino and others. In recent years, downsized, "mini" sufganiyot containing half the calories of the regular, 400-to-600-calorie version, have become popular.

Rabbinic literature also records a tradition of eating cheese and other dairy products during Hanukkah. This custom, as mentioned above, commemorates the heroism of Judith during the Babylonian captivity of the Jews and reminds us that women also played an important role in the events of Hanukkah. The deuterocanonical book of Judith (Yehudit or Yehudis in Hebrew), which is not part of the Tanakh, records that Holofernes, an Assyrian general, had surrounded the village of Bethulia as part of his campaign to conquer Judea. After intense fighting, the water supply of the Jews was cut off and the situation became desperate. Judith, a pious widow, told the city leaders that she had a plan to save the city. Judith went to the Assyrian camps and pretended to surrender. She met Holofernes, who was smitten by her beauty. She went back to his tent with him, where she plied him with cheese and wine. When he fell into a drunken sleep, Judith beheaded him and escaped from the camp, taking the severed head with her (the beheading of Holofernes by Judith has historically been a popular theme in art). When Holofernes' soldiers found his corpse, they were overcome with fear; the Jews, on the other hand, were emboldened and launched a successful counterattack. The town was saved, and the Assyrians defeated.

Roast goose has historically been a traditional Hanukkah food among Eastern European and American Jews, although the custom has declined in recent decades.

Indian Jews traditionally consume gulab jamun, fried dough balls soaked in a sweet syrup, similar to teiglach or bimuelos, as part of their Hanukkah celebrations. Italian Jews eat fried chicken, cassola (a ricotta cheese latke almost similar to a cheesecake), and "fritelle de riso par Hanukkah" (a fried sweet rice pancake). Romanian Jews eat pasta latkes as a traditional Hanukkah dish, and Syrian Jews consume Kibbet Yatkeen, a dish made with pumpkin and bulgur wheat similar to latkes, as well as their own version of keftes de prasa spiced with allspice and cinnamon.

After lighting the candles, it is customary to play (or spin) the dreidel. The dreidel, or "sevivon" in Hebrew, is a four-sided spinning top that children play with during Hanukkah. Each side is imprinted with a Hebrew letter which is an abbreviation for the Hebrew words (, "A great miracle happened there"), referring to the miracle of the oil that took place in the Beit Hamikdash. The fourth side of some dreidels sold in Israel are inscribed with the letter "(Pe)", rendering the acronym (, "A great miracle happened here"), referring to the fact that the miracle occurred in the land of Israel, although this is a relatively recent innovation. Stores in Haredi neighborhoods sell the traditional "Shin" dreidels as well, because they understand "there" to refer to the Temple and not the entire Land of Israel, and because the Hasidic Masters ascribe significance to the traditional letters.

Chanukkah gelt (Yiddish for "Chanukkah money"), known in Israel by the Hebrew translation , is often distributed to children during the festival of Hanukkah. The giving of Hanukkah gelt also adds to the holiday excitement. The amount is usually in small coins, although grandparents or relatives may give larger sums. The tradition of giving Chanukah "gelt" dates back to a long-standing East European custom of children presenting their teachers with a small sum of money at this time of year as a token of gratitude. One minhag favors the fifth night of Hanukkah for giving Hanukkah gelt. Unlike the other nights of Hanukkah, the fifth does not ever fall on the Shabbat, hence never conflicting with the Halachic injunction against handling money on the Shabbat.

The earliest Hanukkah link with the White House occurred in 1951 when Israeli Prime Minister David Ben-Gurion presented United States President Harry Truman with a Hanukkah menorah. In 1979 President Jimmy Carter took part in the first public Hanukkah candle-lighting ceremony of the National Menorah held across the White House lawn. In 1989, President George H. W. Bush displayed a menorah in the White House. In 1993, President Bill Clinton invited a group of schoolchildren to the Oval Office for a small ceremony.

The United States Postal Service has released several Hanukkah-themed postage stamps. In 1996, the United States Postal Service (USPS) issued a 32 cent Hanukkah stamp as a joint issue with Israel. In 2004, after eight years of reissuing the menorah design, the USPS issued a dreidel design for the Hanukkah stamp. The dreidel design was used through 2008. In 2009 a Hanukkah stamp was issued with a design featured a photograph of a menorah with nine lit candles. In 2008, President George W. Bush held an official Hanukkah reception in the White House where he linked the occasion to the 1951 gift by using that menorah for the ceremony, with a grandson of Ben-Gurion and a grandson of Truman lighting the candles.

In December 2014, two Hanukkah celebrations were held at the White House. The White House commissioned a menorah made by students at the Max Rayne school in Israel and invited two of its students to join U.S. President Barack Obama and First Lady Michelle Obama as they welcomed over 500 guests to the celebration. The students' school in Israel had been subjected to arson by extremists. President Obama said these "students teach us an important lesson for this time in our history. The light of hope must outlast the fires of hate. That's what the Hanukkah story teaches us. It's what our young people can teach us – that one act of faith can make a miracle, that love is stronger than hate, that peace can triumph over conflict." Rabbi Angela Warnick Buchdahl, in leading prayers at the ceremony commented on the how special the scene was, asking the President if he believed America's founding fathers could possibly have pictured that a female Asian-American rabbi would one day be at the White House leading Jewish prayers in front of the African-American president.

The dates of Hanukkah are determined by the Hebrew calendar. Hanukkah begins at the 25th day of Kislev and concludes on the second or third day of Tevet (Kislev can have 29 or 30 days). The Jewish day begins at sunset. Hanukkah dates for recent and upcoming:

In 2013, on 28 November, the American holiday of Thanksgiving fell during Hanukkah for only the third time since Thanksgiving was declared a national holiday by President Abraham Lincoln. The last time was 1899, and due to the nature of the Gregorian and Jewish calendars being slightly out of sync with each other, it will not happen again in the foreseeable future. This rare convergence prompted the creation of the neologism Thanksgivukkah.

Major Jewish holidays are those when all forms of work are forbidden, and that feature traditional holiday meals, kiddush, holiday candle-lighting, etc. Only biblical holidays fit these criteria, and Chanukah was instituted some two centuries after the Hebrew Bible was completed. Nevertheless, though Chanukah is of rabbinic origin, it is traditionally celebrated in a major and very public fashion. The requirement to position the menorah, or Chanukiah, at the door or window, symbolizes the desire to give the Chanukah miracle a high profile.

Some Jewish historians suggest a different explanation for the rabbinic reluctance to laud the militarism. First, the rabbis wrote after Hasmonean leaders had led Judea into Rome's grip and so may not have wanted to offer the family much praise. Second, they clearly wanted to promote a sense of dependence on God, urging Jews to look toward the divine for protection. They likely feared inciting Jews to another revolt that might end in disaster, as the Bar Kochba revolt did.

The emergence of Jewish nationalism and the Zionist movement in the late 19th and early 20th centuries had a profound impact on the celebration and reinterpretation of Jewish holidays. These developments resulted in increased emphasis on certain Jewish celebrations, of which Hanukkah and Tu BiShvat are prominent examples.

Traditionally, Hanukkah was a minor event, but took on a new meaning following the rise of Jewish nationalism as a nationalist holiday, symbolizing the struggle of the Jewish people against foreign oppression and their desire for national re-creation. Hanukkah served as a common ground where both religious and secular Zionists could unite around their nationalist agenda. Rabbi Shmuel Mohilever, an early religious Zionist, proposed making Hanukkah the official holiday of the proto-Zionist organization Hovevei Zion in Russia in 1881. Public celebrations of Hanukkah gained prominence in the early 20th century, with parades and public events becoming common. Schools in Mandate Palestine played an early role in promoting these celebrations.

With the advent of Zionism and the state of Israel, the themes of militarism were reconsidered. In modern Israel, the national and military aspects of Hanukkah became, once again, more dominant.

While Hanukkah is a relatively minor Jewish holiday, as indicated by the lack of religious restrictions on work other than a few minutes after lighting the candles, in North America, Hanukkah in the 21st century has taken a place equal to Passover as a symbol of Jewish identity. Both the Israeli and North American versions of Hanukkah emphasize resistance, focusing on some combination of national liberation and religious freedom as the defining meaning of the holiday.

Diane Ashton attributed the increased visibility and reinvention of Hanukkah by some of the American Jewish community as a way to adapt to American life, re-inventing the festival in "the language of individualism and personal conscience derived from both Protestantism and the Enlightenment".

In the Catholic Church, Christmastide has its own Octave, being eight days especially set aside to celebrate Christmas from December 25th to January 1st. This is seen as a Christian fulfillment of the original text's demand for Hanukkah to be eight days, "And they kept the eight days with gladness, as in the feast of the tabernacles, remembering that not long afore they had held the feast of the tabernacles" (2 Macc 10:6). Advent is considered as the season of darkness preceding the season of light, Christmas, so for this reason, Christmas can be said to be the "New Hanukkah," or its fulfillment through the Nativity of Christ. This is similar to the Easter Octave being the solemn eight days of the Passover of Exodus.

In North America, Hanukkah became increasingly important to many Jewish individuals and families during the latter part of the 20th century, including a large number of secular Jews, who wanted to celebrate a Jewish alternative to the Christmas celebrations which frequently overlap with Hanukkah. Diane Ashton argues that Jewish immigrants to America raised the profile of Hanukkah as a kid-centered alternative to Christmas as early as the 1800s. This in parts mirrors the ascendancy of Christmas, which like Hanukkah increased in importance in the 1800s. During this time period, Jewish leaders (especially Reform) such as Max Lilienthal and Isaac Mayer Wise made an effort to rebrand Hanukkah and started creating Hanukkah celebration for kids at their synagogues, which included candy and singing songs. By the 1900s, it started to become a commercial holiday like Christmas, with Hanukkah gifts and decorations appearing in stores and Jewish Women's magazines printing articles on holiday decorations, children's celebrations, and gift giving. Ashton says that Jewish families did this in order to maintain a Jewish identity which is distinct from mainline Christian culture, on the other hand, the mirroring of Hanukkah and Christmas made Jewish families and kids feel that they were American. Though it was traditional for Ashkenazi Jews to give "gelt" or money to children during Hanukkah, in many families, this tradition has been supplemented with the giving of other gifts so that Jewish children can enjoy receiving gifts just like their Christmas-celebrating peers do. Children play a big role in Hanukkah, and Jewish families with children are more likely to celebrate it than childless Jewish families, and sociologists hypothesize that this is because Jewish parents do not want their kids to be alienated from their non-Jewish peers who celebrate Christmas. Recent celebrations have also seen the presence of the Hanukkah bush, which is considered a Jewish counterpart to the Christmas tree. Today, the presence of Hanukkah bushes is generally discouraged by most rabbis, but some Reform, Reconstructionist and more liberal Conservative rabbis do not object, they also do not object to the presence of Christmas trees.

In December 2022, New York City Mayor Eric Adams, Reverends Al Sharpton and Conrad Tillard, businessman Robert F. Smith, Rabbi Shmuley Boteach, and Elisha Wiesel joined to celebrate Hanukkah and Kwanzaa together, and combat racism and antisemitism, at Carnegie Hall.




Christian views on marriage

From the earliest days of the Christian faith, Christians have viewed marriage as a divinely blessed, lifelong, monogamous union between a man and a woman. However, while many Christians might agree with the traditional definition, the terminology and theological views of marriage have varied through time in different countries, and among Christian denominations.

Many Protestants consider marriage to be a sacred institution or "holy ordinance" of God. Catholic and Eastern Orthodox Christians consider marriage as a holy sacrament or sacred mystery. However, there have been differing attitudes among denominations and individual Christians towards not only the concept of Christian marriage, but also concerning divorce, remarriage, gender roles, family authority (the "headship" of the husband), the legal status of married women, birth control, marriageable age, cousin marriage, marriage of in-laws, interfaith marriage, same-sex marriage, and polygamy, among other topics, so that in the 21st century there cannot be said to be a single, uniform, worldwide view of marriage among all who profess to be Christians.

Christian teaching has never held that marriage is necessary for everyone; for many centuries in Western Europe, priestly or monastic celibacy was valued as highly as, if not higher than, marriage. Christians who did not marry were expected to refrain from all sexual activity, as were those who took holy orders or monastic vows.

In some Western countries, a separate and secular civil wedding ceremony is required for recognition by the state, while in other Western countries, couples must merely obtain a marriage license from a local government authority and can be married by Christian or other clergy if they are authorized by law to conduct weddings. In this case, the state recognizes the religious marriage as a civil marriage as well; and Christian couples married in this way have all the rights of civil marriage, including, for example, divorce, even if their church forbids divorce.

Christians believe that marriage is considered in its ideal according to the purpose of God, and that at the heart of God's design for marriage is companionship and intimacy.

The biblical picture of marriage expands into something much broader, with the husband and wife relationship illustrating the relationship between Christ and the church.

It is also considered in its actual occurrence, sometimes involving failure. Therefore, the Bible speaks on the subject of divorce. The New Testament recognizes a place for singleness. Salvation within Christianity is not dependent on the continuation of a biological lineage.

Christians interpret the Genesis creation account as telling the story of when God instituted marriage, which they hold to have taken place after the creation of the first woman, Eve, from Adam, the first man.

Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Old Testament, yet scholars doubt that it was common among average Israelites because of the wealth needed to practice it. Both the biblical patriarchs and kings of Israel are described as engaged in polygamous relationships. Despite the various polygynous relationships in the Bible, Old Testament scholar Peter Gentry has said that it does not mean that God condones polygyny. He also made note of the various problems that polygynous relationships present with the examples of Abraham, Jacob, David, and Solomon in the Bible. Alternatively, this could be a case of graded absolutism.

Betrothal (), which is merely a binding promise to get married, is distinct from marriage itself (), with the time between these events varying substantially. Nonetheless, when a couple is betrothed, they are held accountable to the laws against adultery, like an officially married couple. From this, it is implied that a couple is considered to be married even they have only been betrothed. Since a wife was regarded as property in biblical times, the betrothal () was effected simply by purchasing her from her father (or guardian) (i.e. paying the bride price to the woman and her father); the woman's consent is not explicitly required by any biblical law. Nonetheless, in one Biblical story, Rebecca was asked whether she agreed to be married before the marriage took place. Additionally, according to French anthropologist Philippe Rospabé, the payment of the bride price does not entail the purchase of a woman, as was thought in the early twentieth century. Instead, it is a purely symbolic gesture acknowledging (but never paying off) the husband's permanent debt to the wife's parents.
Like the adjacent Arabic culture (in the pre-Islamic period), the act of marriage appears mainly to have consisted of the groom fetching the bride, although among the Israelites the procession was a festive occasion, accompanied by music, dancing, and lights. To celebrate the marriage, week-long feasts were sometimes held.

In Old Testament times, a wife was submissive to her husband, which may interpreted as Israelite society viewing wives as the chattel of husbands. The descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and bigamous men were expected to ensure that they give their first wife food, clothing, and sexual activity.

Since a wife was regarded as property, her husband was originally free to divorce her with little restriction, at any time. A divorced couple could get back together unless the wife had married someone else after her divorce.

The Bible clearly addresses marriage and divorce. Those in troubled marriages are encouraged to seek counseling and restoration because, according to some advocates of traditional marriage ethics, most divorces are neither necessary nor unavoidable.

In the gospels of both Matthew and Mark, Jesus appealed to God's will in creation. He builds upon the narratives in where male and female are created together and for one another. Thus Jesus takes a firm stance on the permanence of marriage in the original will of God. This corresponds closely with the position of the Pharisee school of thought led by Shammai, at the start of the first millennium, with which Jesus would have been familiar. By contrast, Rabbinic Judaism subsequently took the opposite view, espoused by Hillel, the leader of the other major Pharisee school of thought at the time; in Hillel's view, men were allowed to divorce their wives for any reason.

Some hold that marriage vows are unbreakable, so that even in the distressing circumstances in which a couple separates, they are still married from God's point of view. This is the Roman Catholic church's position, although occasionally the church will declare a marriage to be "null" (in other words, it never really was a marriage). William Barclay (1907-1978) has written:

Jesus brought together two passages from Genesis, reinforcing the basic position on marriage found in Jewish scripture. Thus, he implicitly emphasized that it is God-made ("God has joined together"), "male and female," lifelong ("let no one separate"), and monogamous ("a man...his wife").

Jesus used the image of marriage and the family to teach the basics about the Kingdom of God. He inaugurated his ministry by blessing the wedding at Cana. In the Sermon on the Mount he set forth a new commandment concerning marriage, teaching that lustful looking constitutes adultery. He also superseded a Mosaic Law allowing divorce with his teaching that "anyone who divorces his wife, except for sexual immorality (Greek ), causes her to become an adulteress, and anyone who marries the divorced woman commits adultery". Similar Pauline teachings are found in 1 Corinthians 7. The exception clause—"except for"—uses the Greek word which is variously translated "fornication" (KJV), "marital unfaithfulness" (NIV 1984), "sexual immorality" (NIV 2011), "unchastity" (RSV), "et al". "The KJV New Testament Greek Lexicon, KJV" says "porneia" includes a variety of sexual "deviations" to include "illicit sexual intercourse, adultery, fornication, homosexuality, lesbianism, intercourse with animals, etc., sexual intercourse with close relatives..."

Theologian Frank Stagg says that manuscripts disagree as to the presence in the original text of the phrase "except for fornication". Stagg writes: "Divorce always represents failure...a deviation from God's will... There is grace and redemption where there is contrition and repentance... There is no clear authorization in the New Testament for remarriage after divorce." Stagg interprets the chief concern of Matthew 5 as being "to condemn the criminal act of the man who divorces an innocent wife... Jesus was rebuking the husband who victimizes an innocent wife and thinks that he makes it right with her by giving her a divorce". He points out that Jesus refused to be trapped by the Pharisees into choosing between the strict and liberal positions on divorce as held at the time in Judaism. When they asked him, "Is it lawful for a man to divorce his wife for any cause?" he answered by reaffirming God's will as stated in Genesis, that in marriage husband and wife are made "one flesh", and what God has united man must not separate.

There is no evidence that Jesus himself ever married, and considerable evidence that he remained single. In contrast to Judaism and many other traditions, he taught that there is a place for voluntary singleness in Christian service. He believed marriage could be a distraction from an urgent mission, that he was living in a time of crisis and urgency where the Kingdom of God would be established where there would be no marriage nor giving in marriage:

In Matthew 22, Jesus is asked about the continuing state of marriage after death and he affirms that at the resurrection "people neither marry nor be given in marriage; they are like the angels in heaven.".

The Apostle Paul quoted passages from Genesis almost verbatim in two of his New Testament books. He used marriage not only to describe the kingdom of God, as Jesus had done, but to define also the nature of the 1st-century Christian church. His theological view was a Christian development of the Old Testament parallel between marriage and the relationship between God and Israel. He analogized the church as a bride and Christ as the bridegroom─drawing parallels between Christian marriage and the relationship between Christ and the Church.

There is no hint in the New Testament that Jesus was ever married, and no clear evidence that Paul was ever married. However, both Jesus and Paul seem to view marriage as a legitimate calling from God for Christians. Paul elevates singleness to that of the preferable position, but does offer a caveat suggesting this is "because of the impending crisis"—which could itself extend to present times (see also Pauline privilege). Paul's primary issue was that marriage adds concerns to one's life that detract from their ability to serve God without distraction.

Some scholars have speculated that Paul may have been a widower since prior to his conversion to Christianity he was a Pharisee and member of the Sanhedrin, positions in which the social norm of the day required the men to be married. But it is just as likely that he never married at all.

Yet, Paul acknowledges the mutuality of marital relations, and recognizes that his own singleness is "a particular gift from God" that others may not necessarily have. He writes: "Now to the unmarried and the widows I say: It is good for them to stay unmarried, as I am. But if they cannot control themselves, they should marry, for it is better to marry than to burn with passion."

Paul indicates that bishops, deacons, and elders must be "husbands of one wife", and that women must have one husband. This is usually understood to legislate against polygamy rather than to require marriage:

In the Roman Age, female widows who did not remarry were considered more pure than those who did. Such widows were known as "one man woman" ("enos andros gune") in the epistles of Paul. Paul writes:
Paul allowed widows to remarry. Paul says that only "one-man women" older than 60 years can make the list of Christian widows who did special tasks in the community, but that younger widows should remarry to hinder sin.

Building on what they saw the example of Jesus and Paul advocating, some early Church Fathers placed less value on the family and saw celibacy and freedom from family ties as a preferable state.

Nicene Fathers such as Augustine believed that marriage was a sacrament because it was a symbol used by Paul to express Christ's love of the Church. However, there was also an apocalyptic dimension in his teaching, and he was clear that if everybody stopped marrying and having children that would be an admirable thing; it would mean that the Kingdom of God would return all the sooner and the world would come to an end. Such a view reflects the Manichaean past of Augustine.

While upholding the New Testament teaching that marriage is "honourable in all and the bed undefiled," Augustine believed that "yet, whenever it comes to the actual process of generation, the very embrace which is lawful and honourable cannot be effected without the ardour of lust...This is the carnal concupiscence, which, while it is no longer accounted sin in the regenerate, yet in no case happens to nature except from sin."

Both Tertullian and Gregory of Nyssa were church fathers who were married. They each stressed that the happiness of marriage was ultimately rooted in misery. They saw marriage as a state of bondage that could only be cured by celibacy. They wrote that at the very least, the virgin woman could expect release from the "governance of a husband and the chains of children."

Tertullian argued that second marriage, having been freed from the first by death,"will have to be termed no other than a species of fornication," partly based on the reasoning that this involves desiring to marry a woman out of sexual ardor, which a Christian convert is to avoid.

Also advocating celibacy and virginity as preferable alternatives to marriage, Jerome wrote: "It is not disparaging wedlock to prefer virginity. No one can make a comparison between two things if one is good and the other evil." On First Corinthians 7:1 he reasons, "It is good, he says, for a man not to touch a woman. If it is good not to touch a woman, it is bad to touch one: for there is no opposite to goodness but badness. But if it be bad and the evil is pardoned, the reason for the concession is to prevent worse evil."

St. John Chrysostom wrote: "...virginity is better than marriage, however good... Celibacy is...an imitation of the angels. Therefore, virginity is as much more honorable than marriage, as the angel is higher than man. But why do I say angel? Christ, Himself, is the glory of virginity."

Cyprian, Bishop of Carthage, said that the first commandment given to men was to increase and multiply, but now that the earth was full there was no need to continue this process of multiplication.

This view of marriage was reflected in the lack of any formal liturgy formulated for marriage in the early Church. No special ceremonial was devised to celebrate Christian marriage—despite the fact that the Church had produced liturgies to celebrate the Eucharist, Baptism and Confirmation. It was not important for a couple to have their nuptials blessed by a priest. People could marry by mutual agreement in the presence of witnesses.

At first, the old Roman pagan rite was used by Christians, although modified superficially. The first detailed account of a Christian wedding in the West dates from the 9th century. This system, known as Spousals, persisted after the Reformation.

Today all Christian denominations regard marriage as a sacred institution, a covenant. Roman Catholics consider it to be a sacrament. Marriage was officially recognized as a sacrament at the 1184 Council of Verona. Before then, no specific ritual was prescribed for celebrating a marriage: "Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime."

In the decrees on marriage of the Council of Trent (twenty-fourth session from 1563), the validity of marriage was made dependent upon the wedding taking place before a priest and two witnesses, although the lack of a requirement for parental consent ended a debate that had proceeded from the 12th century. In the case of a divorce, the right of the innocent party to marry again was denied so long as the other party was alive, even if the other party had committed adultery.

The Catholic Church allowed marriages to take place inside churches only starting with the 16th century, beforehand religious marriages happened on the porch of the church.

The Roman Catholic Church teaches that God himself is the author of the sacred institution of marriage, which is His way of showing love for those He created. Marriage is a divine institution that can never be broken, even if the husband or wife legally divorce in the civil courts; as long as they are both alive, the Church considers them bound together by God. Holy Matrimony is another name for sacramental marriage. Marriage is intended to be a faithful, exclusive, lifelong union of a man and a woman. Committing themselves completely to each other, a Catholic husband and wife strive to sanctify each other, bring children into the world, and educate them in the Catholic way of life. Man and woman, although created differently from each other, complement each other. This complementarity draws them together in a mutually loving union.

The valid marriage of baptized Christians is one of the seven Roman Catholic sacraments. The sacrament of marriage is the only sacrament that a priest does not administer directly; a priest, however, is the chief witness of the husband and wife's administration of the sacrament to each other at the wedding ceremony in a Catholic church.

The Roman Catholic Church views that Christ himself established the sacrament of marriage at the wedding feast of Cana; therefore, since it is a divine institution, neither the Church nor state can alter the basic meaning and structure of marriage. Husband and wife give themselves totally to each other in a union that lasts until death.

Priests are instructed that marriage is part of God's natural law and to support the couple if they do choose to marry. Today it is common for Roman Catholics to enter into a "mixed marriage" between a Catholic and a baptized non-Catholic. Couples entering into a mixed marriage are usually allowed to marry in a Catholic church provided their decision is of their own accord and they intend to remain together for life, to be faithful to each other, and to have children which are brought up in the Catholic faith.

In Roman Catholic teaching, marriage has two objectives: the good of the spouses themselves, and the procreation and education of children (1983 code of canon law, c.1055; 1994 catechism, par.2363). Hence "entering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment." It is normal procedure for a priest to ask the prospective bride and groom about their plans to have children before officiating at their wedding. The Roman Catholic Church may refuse to marry anyone unwilling to have children, since procreation by "the marriage act" is a fundamental part of marriage. Thus usage of any form of contraception, in vitro fertilization, or birth control besides natural family planning is a grave offense against the sanctity of marriage and ultimately against God.

Most Protestant denominations hold marriage to be ordained by God for the union between a man and a woman. They see the primary purposes of this union as intimate companionship, rearing children and mutual support for both husband and wife to fulfill their life callings. Protestant Christian denominations consider marital sexual pleasure to be a gift of God, though they vary on their position on birth control, ranging from the acceptance of the use of contraception to only allowing natural family planning to teaching Quiverfull doctrine—that birth control is sinful and Christians should have large families. Conservative Protestants consider marriage a solemn covenant between wife, husband and God. Most view sexual relations as appropriate only within a marriage. Protestant Churches discourage divorce though the way it is addressed varies by denomination; for example, the Reformed Church in America permits divorce and remarriage, while connexions such as the Evangelical Methodist Church Conference forbid divorce except in the case of fornication and do not allow for remarriage in any circumstance.

Many Methodist Christians teach that marriage is "God's gift and covenant intended to imitate God's covenant with humankind" that "Christians enter in their baptism." For example, the rite used in the Free Methodist Church proclaims that marriage is "more than a legal contract, being a bond of union made in heaven, into which you enter discreetly and reverently."

Roles and responsibilities of husband and wives now vary considerably on a continuum between the long-held male dominant/female submission view and a shift toward equality (without sameness) of the woman and the man. There is considerable debate among many Christians today—not just Protestants—whether equality of husband and wife or male headship is the biblically ordained view, and even if it is biblically permissible. The divergent opinions fall into two main groups: Complementarians (who call for husband-headship and wife-submission) and Christian Egalitarians (who believe in full partnership equality in which couples can discover and negotiate roles and responsibilities in marriage).

There is no debate that Ephesians 5 presents a historically benevolent husband-headship/wife-submission model for marriage. The questions are (a) how these New Testament household codes are to be reconciled with the calls earlier in Chapter 5 (cf. verses 1, 18, 21) for mutual submission among all believers, and (b) the meaning of "head" in v.23. It is important to note that verse 22 contains no verb in the original manuscripts, which were also not divided into verses:

Ephesians 5 (NIV)

In the Eastern Orthodox Church, marriage is treated as a Sacred Mystery (sacrament), and as an ordination. It serves to unite a woman and a man in eternal union before God. It refers to the 1st centuries of the church, where spiritual union of spouses in the first sacramental marriage was eternal. Therefore, it is considered a martyrdom as each spouse learns to die to self for the sake of the other. Like all Mysteries, Orthodox marriage is more than just a celebration of something which already exists: it is the creation of something new, the imparting to the couple of the grace which transforms them from a 'couple' into husband and wife within the Body of Christ.

Marriage is an icon (image) of the relationship between Jesus and the Church. This is somewhat akin to the Old Testament prophets' use of marriage as an analogy to describe the relationship between God and Israel. Marriage is the simplest, most basic unity of the church: a congregation where "two or three are gathered together in Jesus' name." The home is considered a consecrated space (the ritual for the Blessing of a House is based upon that of the Consecration of a Church), and the husband and wife are considered the ministers of that congregation. However, they do not "perform" the Sacraments in the house church; they "live" the Sacrament of Marriage. Because marriage is considered to be a pilgrimage wherein the couple walk side by side toward the Kingdom of Heaven, marriage to a non-Orthodox partner is discouraged, though it may be permitted.

Unlike Western Christianity, Eastern Christians do not consider the sacramental aspect of the marriage to be conferred by the couple themselves. Rather, the marriage is conferred by the action of the Holy Spirit acting through the priest. Furthermore, no one besides a bishop or priest—not even a deacon—may perform the Sacred Mystery.

The external sign of the marriage is the placing of wedding crowns upon the heads of the couple, and their sharing in a "Common Cup" of wine. Once crowned, the couple walk a circle three times in a ceremonial "dance" in the middle of the church, while the choir intones a joyous three-part antiphonal hymn, "Dance, Isaiah"

The sharing of the Common Cup symbolizes the transformation of their union from a common marriage into a sacred union. The wedding is usually performed after the Divine Liturgy at which the couple receives Holy Communion. Traditionally, the wedding couple would wear their wedding crowns for eight days, and there is a special prayer said by the priest at the removal of the crowns.

Divorce is discouraged. Sometimes out of "economia" (mercy) a marriage may be dissolved if there is no hope whatever for a marriage to fulfill even a semblance of its intended sacramental character. The standard formula for remarriage is that the Orthodox Church joyfully blesses the first marriage, merely performs the second, barely tolerates the third, and invariably forbids the fourth. "On the basis of the ideal of the first marriage as an image of the glory of God the question is which significance such a second marriage has and whether it can be regarded as Mysterion. Even though there are opinions (particularly in the west) which deny the sacramental character to the second marriage, in the orthodox literature almost consistently either a reduced or even a full sacramentality is attributed to it. The investigation of the second marriage rite shows that both positions affirming the sacramentality to a second marriage can be justified."

Early church texts forbid marriage between an Orthodox Christian and a heretic or schismatic (which would include all non-Orthodox Christians). Traditional Orthodox Christians forbid mixed marriages with other denominations. More liberal ones perform them, provided that the couple formally commit themselves to rearing their children in the Orthodox faith.

All people are called to celibacy—human beings are all born into virginity, and Orthodox Christians are expected by Sacred Tradition to remain in that state unless they are called into marriage and that call is sanctified. The church blesses two paths on the journey to salvation: monasticism and marriage. Mere celibacy, without the sanctification of monasticism, can fall into selfishness and tends to be regarded with disfavour by the Church.

Orthodox priests who serve in parishes are usually married. They must marry prior to their ordination. If they marry after they are ordained they are not permitted to continue performing sacraments. If their wife dies, they are forbidden to remarry; if they do, they may no longer serve as a priest. A married man may be ordained as a priest or deacon. However, a priest or deacon is not permitted to enter into matrimony after ordination. Bishops must always be monks and are thus celibate. However, if a married priest is widowed, he may receive monastic tonsure and thus become eligible for the episcopate.

The Eastern Orthodox Church believes that marriage is an eternal union of spouses, but in Heaven there will not be a procreative bond of marriage.

The Non-Chalcedonian Churches of Oriental Orthodoxy hold views almost identical to those of the (Chalcedonian) Eastern Orthodox Church. The Coptic Orthodox Church allows second marriages only in cases of adultery or death of spouse.

In the teachings of the Church of Jesus Christ of Latter-day Saints (LDS Church), celestial (or eternal) marriage is a covenant between a man, a woman, and God performed by a priesthood authority in a temple of the church. Celestial marriage is intended to continue forever into the afterlife if the man and woman do not break their covenants. Thus, eternally married couples are often referred to as being "sealed" to each other. Sealed couples who keep their covenants are also promised to have their posterity sealed to them in the afterlife. (Thus, "families are forever" is a common phrase in the LDS Church.) A celestial marriage is considered a requirement for exaltation.

In some countries, celestial marriages can be recognized as civil marriages; in other cases, couples are civilly married outside of the temple and are later sealed in a celestial marriage. (The church will no longer perform a celestial marriage for a couple unless they are first or simultaneously legally married.) The church encourages its members to be in good standing with it so that they may marry or be sealed in the temple. A celestial marriage is not annulled by a civil divorce: a "cancellation of a sealing" may be granted, but only by the First Presidency, the highest authority in the church. Civil divorce and marriage outside the temple carries somewhat of a stigma in the Mormon culture; the church teaches that the "gospel of Jesus Christ—including repentance, forgiveness, integrity, and love—provides the remedy for conflict in marriage." Regarding marriage and divorce, the church instructs its leaders: "No priesthood officer is to counsel a person whom to marry. Nor should he counsel a person to divorce his or her spouse. Those decisions must originate and remain with the individual. When a marriage ends in divorce, or if a husband and wife separate, they should always receive counseling from Church leaders."

In church temples, members of the LDS Church perform vicarious celestial marriages for deceased couples who were legally married.

The New Church teaches that marital love (or "conjugial love") is "the precious jewel of human life and the repository of the Christian religion" because the love shared between a husband and a wife is the source of all peace and joy. Emanuel Swedenborg coined the term "conjugial" (rather than the more usual adjective in reference to marital union, "conjugal") to describe the special love experienced by married partners. When a husband and wife work together to build their marriage on earth, that marriage continues after the deaths of their bodies and they live as angels in heaven into eternity. Swedenborg claimed to have spoken with angelic couples who had been married for thousands of years. Those who never married in the natural world will, if they wish, find a spouse in heaven.

The Jehovah's Witnesses view marriage to be a permanent arrangement with the only possible exception being adultery. Divorce is strongly discouraged even when adultery is committed since the wronged spouse is free to forgive the unfaithful one. There are provisions for a domestic separation in the event of "failure to provide for one's household" and domestic violence, or spiritual resistance on the part of a partner. Even in such situations though divorce would be considered grounds for loss of privileges in the congregation. Remarrying after death or a proper divorce is permitted. Marriage is the only situation where any type of sexual interaction is acceptable, and even then certain restrictions apply to acts such as oral and anal sex. Married persons who are known to commit such acts may in fact lose privileges in the congregation as they are supposed to be setting a good example to the congregation.

In Christianity, an interdenominational marriage (also known as an ecumenical marriage) is a marriage between two baptized Christians who belong to different Christian denominations, e.g. a wedding between a Lutheran Christian man and a Catholic Christian woman. Nearly all Christian denominations permit interdenominational marriages.

In Methodism, ¶81 of the 2014 "Discipline" of the Allegheny Wesleyan Methodist Connection, states with regard to interdenominational marriages: "We do not prohibit our people from marrying persons who are not of our connection, provided such persons have the form and are seeking the power of godliness; but we are determined to discourage their marrying persons who do not come up to this description."

The Catholic Church recognizes as sacramental, (1) the marriages between two baptized Protestants or between two baptized Orthodox Christians, as well as (2) marriages between baptized non-Catholic Christians and Catholic Christians, although in the latter case, consent from the diocesan bishop must be obtained, with this being termed "permission to enter into a mixed marriage". To illustrate (1), for example, "if two Lutherans marry in the Lutheran Church in the presence of a Lutheran minister, the Catholic Church recognizes this as a valid sacrament of marriage." Weddings in which both parties are Catholic Christians are ordinarily held in a Catholic church, while weddings in which one party is a Catholic Christian and the other party is a non-Catholic Christian can be held in a Catholic church or a non-Catholic Christian church.

In Christianity, an interfaith marriage is a marriage between a baptized Christian and a non-baptized person, e.g. a wedding between a Christian man and Jewish woman.

In the Presbyterian Church (USA), the local church congregation is tasked with supporting and including an interfaith couple with one being a baptized Presbyterian Christian and the other being a non-Christian, in the life of the Church, "help[ing] parents make and live by commitments about the spiritual nurture of their children", and being inclusive of the children of the interfaith couple. The pastor is to be available to help and counsel the interfaith couple in their life journey.

Although the Catholic Church recognizes as natural marriages weddings between two non-Christians or those between a Catholic Christian and a non-Christian, these are not considered to be sacramental, and in the latter case, the Catholic Christian must seek permission from his/her bishop for the marriage to occur; this permission is known as "dispensation from disparity of cult".

In Methodist Christianity, the 2014 "Discipline" of the Allegheny Wesleyan Methodist Connection discourages interfaith marriages, stating "Many Christians have married unconverted persons. This has produced bad effects; they have either been hindered for life, or have turned back to perdition." Though the United Methodist Church authorizes its clergy to preside at interfaith marriages, it notes that Corinthians 6 has been interpreted "as at least an ideal if not an absolute ban on such [interfaith] marriages as an issue of scriptural faithfulness, if not as an issue of Christian survival." At the same time, for those already in an interfaith marriage (including cases in which there is a non-Christian couple and one party converts to Christianity after marriage), the Church notes that Saint Paul "addresses persons married to unbelievers and encourages them to stay married."

Anglican denominations such as the Episcopal Church in United States the Anglican Church of Canada, the Anglican Church in Aotearoa, New Zealand and Polynesia, the Anglican Episcopal Church of Brazil, the Scottish Episcopal Church in Scotland and mainline Protestant denominations such as the United Church of Christ, the United Church of Canada, the Metropolitan Community Church, the Presbyterian Church (USA), the Quakers, the United Reformed Church in United Kingdom, the Church of Scotland, the Methodist Church of Great Britain, the Church of Iceland, the Church of Sweden, the Church of Denmark, the Church of Norway, the United Protestant Church in Belgium, the Protestant Church in Baden, the Evangelical Church in Berlin, Brandenburg and Silesian Upper Lusatia, the Evangelical Church of Bremen, the Evangelical Lutheran Church in Brunswick, the Evangelical Church of Hesse Electorate-Waldeck, the Evangelical Lutheran Church in Oldenburg, the Evangelical Lutheran Church of Hanover, the Church of Lippe, the Evangelical Reformed Church in Bavaria and Northwestern Germany, the Evangelical Church in the Rhineland, the Protestant Church in Hesse and Nassau, the Evangelical Lutheran Church in Northern Germany the Protestant Church of the Palatinate, the Evangelical Church of Westphalia, the Mennonite Church in the Netherlands the United Protestant Church of France, the Catholic Diocese of the Old Catholics in Germany, the Christian Catholic Church of Switzerland, some Reformed churches in Federation of Swiss Protestant Churches for example the Reformed Church of Aargau, the Protestant Church of Geneva or the Evangelical Reformed Church of the Canton of Zürich and some non-trinitarian denominations such as the Unity Church and the Unitarians, some international evangelical denominations, such as the Association of Welcoming and Affirming Baptists and Affirming Pentecostal Church International perform weddings between same-sex couples.

The Evangelical Lutheran Church of America, the Evangelical Lutheran Church in Canada, some Lutheran and united churches in Evangelical Church in Germany, some Reformed churches in Federation of Swiss Protestant Churches, and the Protestant Church in the Netherlands do not administer sacramental marriage to same-sex couples, but blesses same-sex unions through the use of a specific liturgy.

The Catholic Church, Eastern Orthodox Church, and other more conservative Protestant denominations do not perform or recognize same-sex marriage because they do not consider it as marriage at all, and considering any homosexual sexual activity to be sinful. The Global Anglican Future Conference (GAFCON) consisting of the Church of Nigeria, Anglican Church of Kenya, Anglican Church of Tanzania, Rwanda and Uganda; Anglican Church of South America, Australia, parts of England, Canada, USA and Church of India through the Jerusalem Conference clearly asserted "the unchangeable standard of Christian marriage between one man and one woman as the proper place for sexual intimacy."

With respect to religion, historic Christian belief emphasizes that Christian weddings should occur in a church as Christian marriage should begin where one also starts their faith journey (Christians receive the sacrament of baptism in church in the presence of their congregation). Catholic Christian weddings must "take place in a church building" as holy matrimony is a sacrament; sacraments normatively occur in the presence of Christ in the house of God, and "members of the faith community [should be] present to witness the event and provide support and encouragement for those celebrating the sacrament." Bishops never grant permission "to those requesting to be married in a garden, on the beach, or some other place outside of the church" and a dispensation is only granted "in extraordinary circumstances (for example, if a bride or groom is ill or disabled and unable to come to the church)." Marriage in the church, for Christians, is seen as contributing to the fruit of the newlywed couple regularly attending church each Lord's Day and raising children in the faith.

Christians seek to uphold the seriousness of wedding vows. Yet, they respond with compassion to deep hurts by recognizing that divorce, though less than the ideal, is sometimes necessary to relieve one partner of intolerable hardship, unfaithfulness or desertion. While the voice of God had said, "I hate divorce", some authorities believe the divorce rate in the church is nearly comparable to that of the culture at large.

Christians today hold three competing views as to what is the biblically ordained relationship between husband and wife. These views range from Christian egalitarianism that interprets the New Testament as teaching complete equality of authority and responsibility between the man and woman in marriage, all the way to Patriarchy that calls for a "return to complete patriarchy" in which relationships are based on male-dominant power and authority in marriage:

1. Christian Egalitarians believe in an equal partnership of the wife and husband with neither being designated as the leader in the marriage or family. Instead, the wife and husband share a fully equal partnership in both their marriage and in the family. Its proponents teach "the fundamental biblical principle of the equality of all human beings before God".

"There is neither Jew nor Gentile, neither slave nor free, nor is there male and female, for you are all one in Christ Jesus."

According to this principle, there can be no moral or theological justification for permanently granting or denying status, privilege, or prerogative solely on the basis of a person's race, class, or gender.

2. Christian Complementarians prescribe husband-headship—a male-led hierarchy. This view's core beliefs call for a husband's "loving, humble headship" and the wife's "intelligent, willing submission" to his headship. They believe women have "different but complementary roles and responsibilities in marriage".

3. Biblical patriarchy, though not at all popular among mainstream Christians, prescribes a strict male-dominant hierarchy. A very strong view makes the husband the ruler over his wife and his household. Their organization's first tenet is that "God reveals Himself as masculine, not feminine. God is the eternal Father and the eternal Son, the Holy Spirit is also addressed as He, and Jesus Christ is a male". They consider the husband-father to be sovereign over his household—the family leader, provider, and protector. They call for a wife to be obedient to her head (her husband).

Some Christian authorities permit the practice polygamy (specifically polygyny), but this practice, besides being illegal in Western cultures, is now considered to be out of the Christian mainstream in most parts of the globe; the Lutheran World Federation hosted a regional conference in Africa, in which the acceptance of polygamists and their wives into full membership by the Lutheran Church in Liberia was defended as being permissible. While the Lutheran Church in Liberia permits men to retain their wives if they married them prior to being received into the Church, it does not permit polygamists who have become Christians to marry more wives after they have received the sacrament of Holy Baptism.

Much of the dispute hinges on how one interprets the New Testament household code "(Haustafel)", a term coined by Martin Luther, which has as its main focus hierarchical relationships between three pairs of social classes that were controlled by Roman law: husbands/wives, parents/children, and masters/slaves. The apostolic teachings, with variations, that constitute what has been termed the "household code" occurs in four epistles (letters) by the Apostle Paul and in 1  Peter.

In the early Roman Republic, long before the time of Christ, the law of "manus" along with the concept of "patria potestas" (rule of the fathers), gave the husband nearly absolute autocratic power over his wife, children, and slaves, including the power of life and death. In practice, the extreme form of this right was seldom exercised, and it was eventually limited by law.

Theologian Frank Stagg finds the basic tenets of the code in Aristotle's discussion of the household in Book 1 of "Politics" and in Philo's "Hypothetica 7.14". Serious study of the New Testament Household Code "(Haustafel)" began with Martin Dilbelius in 1913, with a wide range of studies since then. In a Tübingen dissertation, by James E. Crouch concludes that the early Christians found in Hellenistic Judaism a code which they adapted and Christianized.

The Staggs believe the several occurrences of the New Testament household code in the Bible were intended to meet the needs for "order" within the churches and in the society of the day. They maintain that the New Testament household code is an attempt by Paul and Peter to Christianize the concept of family relationships for Roman citizens who had become followers of Christ. The Staggs write that there is some suggestion in scripture that because Paul had taught that they had newly found freedom "in Christ", wives, children, and slaves were taking improper advantage of the "Haustafel" both in the home and the church. 
"The form of the code stressing reciprocal social duties is traced to Judaism's own Oriental background, with its strong moral/ethical demand but also with a low view of woman... At bottom is probably to be seen the perennial tension between freedom and order... What mattered to (Paul) was 'a new creation' and 'in Christ' there is 'not any Jew not Greek, not any slave nor free, not any male and female'.

Two of these Christianized codes are found in Ephesians 5 (which contains the phrases "husband is the head of the wife" and "wives, submit to your husband") and in Colossians 3, which instructs wives to subordinate themselves to their husbands.

The importance of the meaning of "head" as used by the Apostle Paul is pivotal in the conflict between the Complementarian position and the Egalitarian view. The word Paul used for "head", transliterated from Greek, is "kephalē". Today's English word "cephalic" ( ) stems from the Greek "kephalē" and means "of or relating to the head; or located on, in, or near the head." A thorough concordance search by Catherine Kroeger shows that the most frequent use of "head" "(kephalē)" in the New Testament is to refer to "the anatomical head of a body". She found that its second most frequent use in the New Testament was to convey the metaphorical sense of "source". Other Egalitarian authors such as Margaret Howe agree with Kroeger, writing that "The word 'head' must be understood not as 'ruler' but as 'source.

Wayne Grudem criticizes commonly rendering "kephalē" in those same passages only to mean "source", and argues that it denotes "authoritative head" in such texts as Corinthians 11. They interpret that verse to mean that God the father is the authoritative head over the Son, and in turn Jesus is the authoritative head over the church, not simply its source. By extension, they then conclude that in marriage and in the church, the man is the authoritative head over the woman.

Another potential way to define the word "head", and hence the relationship between husband and wife as found in the Bible, is through the example given in the surrounding context in which the word is found. In that context the husband and wife are compared to Christ and his church. The context seems to imply an authority structure based on a man sacrificing himself for his wife, as Christ did for the church; a love-based authority structure, where submission is not required but freely given based on the care given to the wife.

Some biblical references on this subject are debated depending on one's school of theology. The historical grammatical method is a hermeneutic technique that strives to uncover the meaning of the text by taking into account not just the grammatical words, but also the syntactical aspects, the cultural and historical background, and the literary genre. Thus references to a patriarchal Biblical culture may or may not be relevant to other societies. What is believed to be a timeless truth to one person or denomination may be considered a cultural norm or minor opinion to another.

Christian Egalitarians (from the French word "égal" meaning "equal") believe that Christian marriage is intended to be a marriage without any hierarchy—a full and equal partnership between the wife and husband. They emphasize that nowhere in the New Testament is there a requirement for a wife to "obey" her husband. While "obey" was introduced into marriage vows for much of the church during the Middle Ages, its only New Testament support is found in Peter 3, with that only being by implication from Sarah's obedience to Abraham. Scriptures such as state that in Christ, right relationships are restored and in him, "there is neither Jew nor Greek, slave nor free, male nor female."

Christian Egalitarians interpret scripture to mean that God intended spouses to practice "mutual submission", each in equality with the other. The phrase "mutual submission" comes from a verse in Ephesians 5 which precedes advice for the three domestic relationships of the day, including slavery. It reads, "Submit to one another ('mutual submission') out of reverence for Christ", wives to husbands, children to parents, and slaves to their master. Christian Egalitarians believe that full partnership in marriage is the most biblical view, producing the most intimate, wholesome, and reciprocally fulfilling marriages.

The Christian Egalitarian view of marriage asserts that gender, in and of itself, neither privileges nor curtails a believer's gifting or calling to any ministry in the church or home. It does not imply that women and men are identical or undifferentiated, but affirms that God designed men and women to complement and benefit one another. A foundational belief of Christian Egalitarians is that the husband and wife are created equally and are ordained of God to "become one", a biblical principle first ordained by God in Genesis 2, reaffirmed by Jesus in Matthew 19 and Mark 10, and by the Apostle Paul in Ephesians 5. Therefore, they see that "oneness" as pointing to gender equality in marriage. They believe the biblical model for Christian marriages is therefore for the spouses to share equal responsibility within the family—not one over the other nor one under the other.

David Dykes, theologian, author, and pastor of a 15,000-member Baptist church, sermonized that "When you are in Christ, you have full equality with all other believers". In a sermon he entitled "The Ground Is Level at the Foot of the Cross", he said that some theologians have called one particular Bible verse the Christian "Magna Carta". The Bible verse reads: "There is neither Jew nor Gentile, neither slave nor free, nor is there male and female, for you are all one in Christ Jesus." Acknowledging the differences between men and women, Dykes writes that "in Christ, these differences don't define who we are. The only category that really matters in the world is whether you are in Christ. At the cross, Jesus destroyed all the made-made barriers of hostility:" ethnicity, social status, and gender.

Those of the egalitarian persuasion point to the biblical instruction that all Christian believers, irrespective of gender, are to submit or be subject "to one another in the fear of God" or "out of reverence for Christ". Gilbert Bilezikian writes that in the highly debated Ephesians 5 passage, the verb "to be subject" or "to be submitted" appears in verse 21 which he describes as serving as a "hinge" between two different sections. The first section consists of verses 18–20, verse 21 is the connection between the two, and the second section consists of verses 22–33. When discussion begins at verse 22 in Ephesians 5, Paul appears to be reaffirming a chain of command principle within the family. However,

Advocates of Christian egalitarianism believe that this model has firm biblical support:

The egalitarian paradigm leaves it up to the couple to decide who is responsible for what task or function in the home. Such decisions should be made rationally and wisely, not based on gender or tradition. Examples of a couple's decision logic might include:

Complementarians hold to a hierarchical structure between husband and wife. They believe men and women have different gender-specific roles that allow each to "complement" the other, hence the designation "Complementarians". The Complementarian view of marriage holds that while the husband and wife are of equal worth before God, husbands and wives are given different functions and responsibilities by God that are based on gender, and that male leadership is biblically ordained so that the husband is always the senior authority figure. They state they "observe with deep concern" "accompanying distortions or neglect of the glad harmony portrayed in Scripture between the intelligent, humble leadership of redeemed husbands and the loving, willing support of that leadership by redeemed wives". They believe "the Bible presents a clear chain of authority—above all authority and power is God; God is the head of Christ. Then in descending order, Christ is the head of man, man is the head of woman, and parents are the head of their children." Complementarians teach that God intended men to lead their wives as "heads" of the family. Wayne Grudem, in an article that interprets the "mutual submission" of Ephesians 5 as being hierarchical, writes that it means "being considerate of one another, and caring for one another’s needs, and being thoughtful of one another, and sacrificing for one another."

Scriptures such as 1 Corinthians 11:3: "But I would have you know, that the head of every man is Christ; and the head of the woman is the man; and the head of Christ is God", (KJV) is understood as meaning the wife is to be subject to her husband, if not unconditionally.

According to Complementarian authors John Piper, Wayne Grudem, and others, historically, but to a significantly lesser extent in most of Christianity today, the predominant position in both Catholicism and conservative Protestantism places the male as the "head" in the home and in the church. They hold that women are commanded to be in subjection to male leadership, with a wife being obedient to her head (husband), based upon Old Testament precepts and principles. This view holds that, "God has created men and women equal in their essential dignity and human personhood, but different and complementary in function with male headship in the home and in the Church."

Grudem also acknowledges exceptions to the submission of wives to husbands where moral issues are involved. Rather than unconditional obedience, Complementarian authors such as Piper and Grudem are careful to caution that a wife's submission should never cause her to "follow her husband into sin."

Roman Catholic Church teaching on the role of women includes that of Pope Leo XIII in his 1880 encyclical "Arcanum" which states:
Though each of their churches is autonomous and self-governed, the official position of the Southern Baptist Convention (the largest Protestant denomination in the United States) is:
The patriarchal model of marriage is clearly the oldest one. It characterized the theological understanding of most Old Testament writers. It mandates the supremacy, at times the ultimate domination, of the husband-father in the family. In the first century Roman Empire, in the time of Jesus, Paul, and Peter, it was the law of the land and gave the husband absolute authority over his wife, children, and slaves—even the power of life or death. It subordinates all women.

Biblical patriarchy is similar to Complementarianism but with differences of degree and emphasis. Biblical patriarchists carry the husband-headship model considerably further and with more militancy. While Complementarians also hold to exclusively male leadership in both the home and the church, Biblical patriarchy extends that exclusion to the civic sphere as well, so that women should not be civil leaders and indeed should not have careers outside the home.

Patriarchy is based on authoritarianism—complete obedience or subjection to male authority as opposed to individual freedom. Patriarchy gives preeminence to the male in essentially all matters of religion and culture. It explicitly deprives all women of social, political, and economic rights. The marriage relationship simply reinforced this dominance of women by men, providing religious, cultural, and legal structures that clearly favor patriarchy to the exclusion of even basic human dignity for wives.

Historically in classical patriarchy, the wives and children were always legally dependent upon the father, as were the slaves and other servants. It was the way of life throughout most of the Old Testament, religiously, legally, and culturally. However, it was not unique to Hebrew thought. With only minor variations, it characterized virtually every pagan culture of that day—including all Pre-Christian doctrine and practice.

While Scripture allowed this approach in Old Testament times, nowhere does the Bible ordain it. In the Hebrew nation, patriarchy seems to have evolved as an expression of male dominance and supremacy, and of a double standard that prevailed throughout much of the Old Testament. Its contemporary advocates insist that it is the only biblically valid model for marriage today. They argue that it was established at Creation, and thus is a firm, unalterable decree of God about the relative positions of men and women.

Biblical patriarchists see what they describe as a crisis of this era being what they term to be a systematic attack on the "timeless truths of biblical patriarchy". They believe such an attack includes the movement to "subvert the biblical model of the family, and redefine the very meaning of fatherhood and motherhood, masculinity, femininity, and the parent and child relationship." Arguing from the biblical presentation of God revealing himself "as masculine, not feminine", they believe God ordained distinct gender roles for man and woman as part of the created order. They say "Adam’s headship over Eve was established at the beginning, before sin entered the world". Their view is that the male has God-given authority and mandate to direct "his" household in paths of obedience to God. They refer to man's "dominion" beginning within the home, and a man's qualification to lead and ability to lead well in the public square is based upon his prior success in "ruling his household".

Thus, William Einwechter refers to the traditional Complementarian view as "two-point Complementarianism" (male leadership in the family and church), and regards the biblical patriarchy view as "three-point" or "full" complementarianism (male leadership in family, church "and society").

The patriarchists teach that "the woman was created as a helper to her husband, as the bearer of children, and as a "keeper at home", concluding that the God-ordained and proper sphere of dominion for a wife is the household. Biblical patriarchists consider that "faithfulness to Christ requires that (Biblical patriarchy) be believed, taught, and lived". They claim that the "man is...the image and glory of God in terms of authority, while the woman is the glory of man". They teach that a wife is to be "obedient" to her "head" (husband), based upon Old Testament teachings and models.

See Christian feminism




Class (computer programming)

In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).

When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called "instance variables", to contrast with the "class variables" shared across the class.

In certain languages, classes are, as a matter of fact, only a compile time feature (new classes cannot be declared at run-time), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type or similar). In these languages, a class that creates classes within itself is called a metaclass.

In object-oriented programming (OOP), an instance is a concrete occurrence of any object, existing usually during the runtime of a computer program. Formally, "instance" is synonymous with "object" as they are each a particular value (realization), and these may be called an instance object; "instance" emphasizes the distinct identity of the object. The creation of an instance is called instantiation.

An object may be varied in many ways. Each realized variation of that object is an instance of its class. That is, it is a member of a given class that has specified values rather than variables. In a non-programming context, you could think of "dog" as a type and your particular dog as an instance of that class.

In class-based programming, objects are created as instances of classes by subroutines called constructors, and destroyed by destructors. An object is an instance of a class as it can access to all data types(primitive as well as non primitive), and methods etc., of a class. Therefore, objects may be called a class instances or class objects. Object instantiation is known as construction. Not all classes can be instantiated abstract classes cannot be instantiated, while classes that can be instantiated are called concrete classes. In prototype-based programming, instantiation is instead done by copying (cloning) a prototype instance.

In its most casual usage, people often refer to the "class" of an object, but narrowly speaking objects have "type": the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In the terms of type theory, a class is an implementationa "concrete" data structure and collection of subroutineswhile a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type might be implemented with two classes (fast for small stacks, but scales poorly) and (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.

Class types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a type might represent the properties and functionality of bananas in general, while the and classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The class could then produce particular bananas: instances of the class would be objects of type . Often only a single implementation of a type is given, in which case the class name is often identical with the type name.

Classes are composed of structural and behavioral constituents. Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.

A class contains data field descriptions (or "properties", "fields", "data members", or "attributes"). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.

Some programming languages such as Eiffel support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.

The behavior of a class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods—such as constructors, destructors, and conversion operators—are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.

Every class "implements" (or "realizes") an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing an implementation.

Languages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from.

For example, if "class A" inherits from "class B" and if "class B" implements the interface "interface B" then "class A" also inherits the functionality(constants and methods declaration) provided by "interface B".

In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.

Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object. 

The buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the "power" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together compose the interface (other television sets that are the same model as yours would have the same interface). In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.

A television set also has a myriad of "attributes", such as size and whether it supports color, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).

Getting the total number of televisions manufactured could be a "static method" of the television class. This method is associated with the class, yet is outside the domain of each instance of the class. A static method that finds a particular instance out of the set of all television objects is another example.

The following is a common set of access specifiers:


Although many object-oriented languages support the above access specifiers, their semantics may differ.

Object-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants—constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.

Access specifiers do not necessarily control "visibility", in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from the client code will be prevented by the type checker.

The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.

Some languages feature other accessibility schemes:

In addition to the design of standalone classes, programming languages may support more advanced class designs based on relationships between classes. The inter-class relationship design capabilities commonly provided are "compositional" and "hierarchical".

Classes can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a "has-a" relationship. For example, a class "Car" could be composed of and contain a class "Engine". Therefore, a Car "has an" Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:
@interface Car : NSObject

@property NSString *name;
@property Engine *engine
@property NSArray *tires;

@end
This class "has" an instance of (a string object), , and (an array object).

Classes can be "derived" from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes ("base classes", "parent classes" or ') and the derived class ("child class" or "subclass") . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a"' Control. Structural and behavioral members of the parent classes are "inherited" by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they "inherit" and are therefore "specializations" of their superclasses. Also, derived classes can override inherited methods if the language allows.

Not all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.

Example (Simplified Objective-C 2.0 code, from iPhone SDK):

@interface UIResponder : NSObject //...
@interface UIView : UIResponder //...
@interface UIScrollView : UIView //...
@interface UITableView : UIScrollView //...

In this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.

Conceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve as a superclass of and , while would be a subclass of . These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.

A common conceptual error is to mistake a "part of" relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model an engine or body as a subclass of a car.

In object-oriented modeling these kinds of relations are typically modeled as object properties. In this example, the class would have a property called . would be typed to hold a collection of objects, such as instances of , , , etc.
Object modeling languages such as UML include capabilities to model various aspects of "part of" and other kinds of relations – data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code besides the basic data definitions for the objects, such as error checking on get and set methods.

One important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets, it would be rare to find sets that did not intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.

Most modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.

However, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.

A similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rationale is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.

Although class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as "object-based languages", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.

In object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.

An association role is given end of an association and describes the role of the corresponding class. For example, a "subscriber" role describes the way instances of the class "Person" participate in a "subscribes-to" association with the class "Magazine". Also, a "Magazine" has the "subscribed magazine" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are "0..1", "1..1", "1..*" and "0..*", where the "*" specifies any number of instances.

There are many categories of classes, some of which overlap.

In a language that supports inheritance, an abstract class, or abstract base class (ABC), is a class that cannot be directly instantiated. By contrast, a concrete class is a class that be directly instantiated. Instantiation of an abstract class can occur only indirectly, via a concrete class.

An abstract class is either labeled as such explicitly or it may simply specify "abstract methods" (or "virtual methods"). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.

Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword "abstract" is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).

A class consisting of only pure virtual methods is called a pure abstract base class (or pure ABC) in C++ and is also known as an "interface" by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.
In some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.

An inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on the language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is "inner types", also known as "inner data type" or "nested type", which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via "typedef" declarations).

Another type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.

Metaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.

In some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language.
The Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.

Non-subclassable classes or sealed classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited (a stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to "abstract" classes, which imply, encourage, and require derivation to be used at all. A non-subclassable class is implicitly "concrete".

A non-subclassable class is created by declaring the class as in C# or as in Java or PHP. For example, Java's class is designated as "final".

Non-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes.

An open class can be changed. Typically, an executable program cannot be changed by customers. Developers can often change some classes, but typically cannot change standard or built-in ones. In Ruby, all classes are open. In Python, classes can be created at runtime, and all can be modified afterward. Objective-C categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code.

Some languages have special support for mixins, though, in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class might provide a method called when included in classes and that do not share a common parent.

In languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.

The primary motivation for the introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it "unifies" all the parts of a partial class. Then, compilation can proceed as usual.

Other benefits and effects of the partial class feature include:


Partial classes have existed in Smalltalk under the name of "Class Extensions" for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.

Uninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.

For example, in C#, a class marked "static" can not be instantiated, can only have static members (fields, methods, other), may not have "instance constructors", and is "sealed".
An unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.

The benefits of organizing software into object classes fall into three categories:


Object classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.

Object classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.

Software reuse is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailoring some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.

As a data type, a class is usually considered as a compile-time construct. A language or library may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.

For example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.




Canterbury (disambiguation)

Canterbury is a city located in the county of Kent in southeast England. It may also refer to:













Color blindness

Color blindness or color vision deficiency (CVD) is the decreased ability to see color or differences in color. The severity of color blindness ranges from mostly unnoticeable to full absence of color perception. Color blindness is usually an inherited problem or variation in the functionality of one or more of the three classes of cone cells in the retina, which mediate color vision. The most common form is caused by a genetic condition called congenital red–green color blindness (including protan and deutan types), which affects "up to" 1 in 12 males (8%) and 1 in 200 females (0.5%). The condition is more prevalent in males, because the opsin genes responsible are located on the X chromosome. Rarer genetic conditions causing color blindness include congenital blue–yellow color blindness (tritan type), blue cone monochromacy, and achromatopsia. Color blindness can also result from physical or chemical damage to the eye, the optic nerve, parts of the brain, or from medication toxicity. Color vision also naturally degrades in old age.
Diagnosis of color blindness is usually done with a color vision test, such as the Ishihara test. There is no cure for most causes of color blindness, however there is ongoing research into gene therapy for some severe conditions causing color blindness. Minor forms of color blindness do not significantly affect daily life and the color blind automatically develop adaptations and coping mechanisms to compensate for the deficiency. However, diagnosis may allow an individual, or their parents/teachers to actively accommodate the condition. Color blind glasses (e.g. "EnChroma") may help the red–green color blind at some color tasks, but they do not grant the wearer "normal color vision" or the ability to see "new" colors. Some mobile apps can use a device's camera to identify colors.
Depending on the jurisdiction, the color blind are ineligible for certain careers, such as aircraft pilots, train drivers, police officers, firefighters, and members of the armed forces. The effect of color blindness on artistic ability is controversial, but a number of famous artists are believed to have been color blind.

A color blind person will have decreased (or no) color discrimination along the red–green axis, blue–yellow axis, or both. However, the vast majority of the color blind are only affected on their red–green axis.

The first indication of color blindness generally consists of a person using the wrong color for an object, such as when painting, or calling a color by the wrong name. The colors that are confused are very consistent among people with the same type of color blindness.

Confusion colors are pairs or groups of colors that will often be mistaken by the color blind. Confusion colors for red–green color blindness include:
Confusion colors for tritan include:
These colors of confusion are defined quantitatively by straight confusion lines plotted in CIEXYZ, usually plotted on the corresponding chromaticity diagram. The lines all intersect at a "copunctal point", which varies with the type of color blindness. Chromaticities along a confusion line will appear metameric to dichromats of that type. Anomalous trichromats of that type will see the chromaticities as metameric if they are close enough, depending on the strength of their CVD. For two colors on a confusion line to be metameric, the chromaticities first have to be made "isoluminant", meaning equal in lightness. Also, colors that may be isoluminant to the standard observer may not be isoluminant to a person with dichromacy.

Cole describes four color tasks, all of which are impeded to some degree by color blindness:

The following sections describe specific color tasks with which the color blind typically have difficulty.

Color blindness causes difficulty with the "connotative" color tasks associated with selecting or preparing food. Selecting food for ripeness can be difficult; the green–yellow transition of bananas is particularly hard to identify. It can also be difficult to detect bruises, mold, or rot on some foods, to determine when meat is done by color, to distinguish some varietals, such as a Braeburn vs. a Granny Smith apple, or to distinguish colors associated with artificial flavors (e.g. jelly beans, sports drinks).

Changes in skin color due to bruising, sunburn, rashes or even blushing are easily missed by the red–green color blind.

The colors of traffic lights can be difficult for the red–green color blindness. This difficulty includes distinguishing red/amber lights from sodium street lamps, distinguishing green lights (closer to cyan) from normal white lights, and distinguishing red from amber lights, especially when there are no positional clues available (see image).
The main coping mechanism to overcome these challenges is to memorize the position of lights. The order of the common triplet traffic light is standardized as red–amber–green from top to bottom or left to right. Cases that deviate from this standard are rare. One such case is a traffic light in Tipperary Hill in Syracuse, New York, which is upside-down (green–amber–red top to bottom) due to the sentiments of its Irish American community. However, the light has been criticized due to the potential hazard it poses for color blind drivers.
There are other several features of traffic lights available that help accommodate the color blind. British Rail signals use more easily identifiable colors: The red is blood red, the amber is yellow and the green is a bluish color. Most British road traffic lights are mounted vertically on a black rectangle with a white border (forming a "sighting board"), so that drivers can more easily look for the position of the light. In the eastern provinces of Canada, traffic lights are sometimes differentiated by shape in addition to color: square for red, diamond for yellow, and circle for green (see image).

Navigation lights in marine and aviation settings employ red and green lights to signal the relative position of other ships or aircraft. Railway signal lights also rely heavily on red–green–yellow colors. In both cases, these color combinations can be difficult for the red–green color blind. Lantern Tests are a common means of simulating these light sources to determine not necessarily whether someone is color blind, but whether they can functionally distinguish these specific signal colors. Those who cannot pass this test are generally completely restricted from working on aircraft, ships or rail, for example.

Color analysis is the analysis of color in its use in fashion, to determine personal color combinations that are most aesthetically pleasing. Colors to combine can include clothing, accessories, makeup, hair color, skin color, eye color, etc. Color analysis involves many aesthetic and comparative color task that can be difficult for the color blind.

Inability to distinguish color does not necessarily preclude the ability to become a celebrated artist. The 20th century expressionist painter Clifton Pugh, three-time winner of Australia's Archibald Prize, on biographical, gene inheritance and other grounds has been identified as a person with protanopia. 19th century French artist Charles Méryon became successful by concentrating on etching rather than painting after he was diagnosed as having a red–green deficiency. Jin Kim's red–green color blindness did not stop him from becoming first an animator and later a character designer with Walt Disney Animation Studios.

Deuteranomals are better at distinguishing shades of khaki, which may be advantageous when looking for predators, food, or camouflaged objects hidden among foliage. Dichromats tend to learn to use texture and shape clues and so may be able to penetrate camouflage that has been designed to deceive individuals with normal color vision.

Some tentative evidence finds that the color blind are better at penetrating certain color camouflages. Such findings may give an evolutionary reason for the high rate of red–green color blindness. There is also a study suggesting that people with some types of color blindness can distinguish colors that people with normal color vision are not able to distinguish. In World War II, color blind observers were used to penetrate camouflage.

In the presence of chromatic noise, the color blind are more capable of seeing a luminous signal, as long as the chromatic noise appears metameric to them. This is the effect behind most "reverse" Pseudoisochromatic plates (e.g. ""hidden digit"" Ishihara plates) that are discernible to the color blind but unreadable to people with typical color vision.

Color codes are useful tools for designers to convey information. The interpretation of this information requires users to perform a variety of Color Tasks, usually comparative but also sometimes connotative or denotative. However, these tasks are often problematic for the color blind when design of the color code has not followed best practices for accessibility. For example, one of the most ubiquitous connotative color codes is the "red means bad and green means good" or similar systems, based on the classic signal light colors. However, this color coding will almost always be undifferentiable to deutans or protans, and therefore should be avoided or supplemented with a parallel connotative system (symbols, smileys, etc.).

Good practices to ensure design is accessible to the color blind include:

A common task for designers is to select a subset of colors ("qualitative" colormap) that are as mutually differentiable as possible (salient). For example, player pieces in a board game should be as different as possible.

Classic advice suggests using Brewer palettes, but several of these are "not" actually accessible to the color blind.

Unfortunately, the colors with the greatest contrast to the red–green color blind tend to be colors of confusion to the blue–yellow color blind, and vice versa. However, since red–green is much more prevalent than blue–yellow CVD, design should generally prioritize those users (deutans then protans).

A common task for data visualization is to represent a color scale, or "sequential" colormap, often in the form of a heat map or choropleth. Several scales are designed with special consideration for the color blind and are widespread in academia, including Cividis, Viridis and Parula. These comprise a light-to-dark scale superimposed on a yellow-to-blue scale, making them monotonic and perceptually uniform to all forms of color vision.

Much terminology has existed and does exist for the classification of color blindness, but the typical classification for color blindness follows the von Kries classifications, which uses severity and affected cone for naming.

Based on clinical appearance, color blindness may be described as total or partial. Total color blindness (monochromacy) is much less common than partial color blindness. Partial color blindness includes dichromacy and anomalous trichromacy, but is often clinically defined as mild, moderate or strong.

Monochromacy is often called "total color blindness" since there is no ability to see color. Although the term may refer to acquired disorders such as cerebral achromatopsia, it typically refers to congenital color vision disorders, namely rod monochromacy and blue cone monochromacy).

In cerebral achromatopsia, a person cannot perceive colors even though the eyes are capable of distinguishing them. Some sources do not consider these to be true color blindness, because the failure is of perception, not of vision. They are forms of visual agnosia.

Monochromacy is the condition of possessing only a single channel for conveying information about color. Monochromats are unable to distinguish any colors and perceive only variations in brightness. Congenital monochromacy occurs in two primary forms:

Dichromats can match any color they see with some mixture of just two primary colors (in contrast to those with normal sight (trichromats) who can distinguish three primary colors). Dichromats usually know they have a color vision problem, and it can affect their daily lives. Dichromacy in humans includes protanopia, deuteranopia, and tritanopia. Out of the male population, 2% have severe difficulties distinguishing between red, orange, yellow, and green (orange and yellow are different combinations of red and green light). Colors in this range, which appear very different to a normal viewer, appear to a dichromat to be the same or a similar color. The terms protanopia, deuteranopia, and tritanopia come from Greek, and respectively mean "inability to see ("anopia") with the first ("prot-"), second ("deuter-"), or third ("trit-") [cone]".

Anomalous trichromacy is the mildest type of color deficiency, but the severity ranges from almost dichromacy (strong) to almost normal trichromacy (mild). In fact, many mild anomalous trichromats have very little difficulty carrying out tasks that require normal color vision and some may not even be aware that they have a color vision deficiency. The types of anomalous trichromacy include protanomaly, deuteranomaly and tritanomaly. It is approximately three times more common than dichromacy. Anomalous trichromats exhibit trichromacy, but the color matches they make differ from normal trichromats. In order to match a given spectral yellow light, protanomalous observers need more red light in a red/green mixture than a normal observer, and deuteranomalous observers need more green. This difference can be measured by an instrument called an Anomaloscope, where red and green lights are mixed by a subject to match a yellow light.

There are two major types of color blindness: difficulty distinguishing between red and green, and difficulty distinguishing between blue and yellow. These definitions are based on the phenotype of the partial color blindness. Clinically, it is more common to use a genotypical definition, which describes which cone/opsin is affected.

Red–green color blindness includes protan and deutan CVD. Protan CVD is related to the L-cone and includes protanomaly (anomalous trichromacy) and protanopia (dichromacy). Deutan CVD is related to the M-cone and includes deuteranomaly (anomalous trichromacy) and deuteranopia (dichromacy). The phenotype (visual experience) of deutans and protans is quite similar. Common colors of confusion include red/brown/green/yellow as well as blue/purple. Both forms are almost always symptomatic of congenital red–green color blindness, so affects males disproportionately more than females. This form of color blindness is sometimes referred to as "daltonism" after John Dalton, who had red–green dichromacy. In some languages, "daltonism" is still used to describe red–green color blindness.



Blue–yellow color blindness includes tritan CVD. Tritan CVD is related to the S-cone and includes tritanomaly (anomalous trichromacy) and tritanopia (dichromacy). Blue–yellow color blindness is much less common than red–green color blindness, and more often has acquired causes than genetic. Tritans have difficulty discerning between bluish and greenish hues. Tritans have a neutral point at 571 nm (yellowish).



The below table shows the cone complements for different types of human color vision, including those considered color blindness, normal color vision and 'superior' color vision. The cone complement contains the types of cones (or their opsins) expressed by an individual.

Color blindness is any deviation of color vision from normal trichromatic color vision (often as defined by the standard observer) that produces a reduced gamut. Mechanisms for color blindness are related to the functionality of cone cells, and often to the expression of photopsins, the photopigments that 'catch' photons and thereby convert light into chemical signals.

Color vision deficiencies can be classified as inherited or acquired.

Color blindness is typically an inherited genetic disorder. The most common forms of color blindness are associated with the Photopsin genes, but the mapping of the human genome has shown there are many causative mutations that do not directly affect the opsins. Mutations capable of causing color blindness originate from at least 19 different chromosomes and 56 different genes (as shown online at the Online Mendelian Inheritance in Man [OMIM]).

By far the most common form of color blindness is congenital red–green color blindness (Daltonism), which includes protanopia/protanomaly and deuteranopia/deuteranomaly. These conditions are mediated by the OPN1LW and OPN1MW genes, respectively, both on the X chromosome. An 'affected' gene is either missing (as in Protanopia and Deuteranopia - Dichromacy) or is a chimeric gene (as in Protanomaly and Deuteranomaly).

Since the OPN1LW and OPN1MW genes are on the X chromosome, they are sex-linked, and therefore affect males and females disproportionately. Because the color blind 'affected' alleles are recessive, color blindness specifically follows X-linked recessive inheritance. Males have only one X chromosome (XY), and females have two (XX); Because the male only has one of each gene, if it is affected, the male will be color blind. Because a female has two alleles of each gene (one on each chromosome), if only one gene is affected, the dominant normal alleles will "override" the affected, recessive allele and the female will have normal color vision. However, if the female has two mutated alleles, she will still be color blind. This is why there is a disproportionate prevalence of color blindness, with ~8% of males exhibiting color blindness and ~0.5% of females.

Congenital blue–yellow color blindness is a much rarer form of color blindness including tritanopia/tritanomaly. These conditions are mediated by the OPN1SW gene on Chromosome 7 which encodes the S-opsin protein and follows autosomal dominant inheritance. The cause of blue–yellow color blindness is not analogous to the cause of red–green color blindness, i.e. the peak sensitivity of the S-opsin does not shift to longer wavelengths. Rather, there are 6 known point mutations of OPN1SW that degrade the performance of the S-cones. The OPN1SW gene is almost invariant in the human population. Congenital tritan defects are often progressive, with nearly normal trichromatic vision in childhood (e.g. mild tritanomaly) progressing to dichromacy (tritanopia) as the S-cones slowly die. Tritanomaly and tritanopia are therefore different penetrance of the same disease, and some sources have argued that tritanomaly therefore be referred to as incomplete tritanopia.

Several inherited diseases are known to cause color blindness, including achromatopsia, cone dystrophy, Leber's congenital amaurosis and retinitis pigmentosa. These can be congenital or commence in childhood or adulthood. They can be static/stationary or progressive. Progressive diseases often involve deterioration of the retina and other parts of the eye, so often progress from color blindness to more severe visual impairments, up to and including total blindness.

Physical trauma can cause color blindness, either neurologically – brain trauma which produces swelling of the brain in the occipital lobe – or retinally, either acute (e.g. from laser exposure) or chronic (e.g. from ultraviolet light exposure).

Color blindness may also present itself as a symptom of degenerative diseases of the eye, such as cataract and age-related macular degeneration, and as part of the retinal damage caused by diabetes. Vitamin A deficiency may also cause color blindness.

Color blindness may be a side effect of prescription drug use. For example, red–green color blindness can be caused by ethambutol, a drug used in the treatment of tuberculosis. Blue–yellow color blindness can be caused by sildenafil, an active component of Viagra. Hydroxychloroquine can also lead to hydroxychloroquine retinopathy, which includes various color defects. Exposure to chemicals such as styrene or organic solvents can also lead to color vision defects.

Simple colored filters can also create mild color vision deficiencies. John Dalton's original hypothesis for his deuteranopia was actually that the vitreous humor of his eye was discolored:

An autopsy of his eye after his death in 1844 showed this to be definitively untrue, though other filters are possible. Actual physiological examples usually affect the blue–yellow opponent channel and are named Cyanopsia and Xanthopsia, and are most typically an effect of yellowing or removal of the lens.

The opponent channels can also be affected by the prevalence of certain cones in the retinal mosaic. The cones are not equally prevalent and not evenly distributed in the retina. When the number of one of these cone types is significantly reduced, this can also lead to or contribute to a color vision deficiency. This is one of the causes of tritanomaly.

Some people are also unable to distinct between blue and green, which appears to be a combination of culture and exposure to UV-light.

The main method for diagnosing a color vision deficiency is in testing the color vision directly. The Ishihara color test is the test most often used to detect red–green deficiencies and most often recognized by the public. Some tests are clinical in nature, designed to be fast, simple, and effective at identifying broad categories of color blindness. Others focus on precision and are generally available only in academic settings.


While genetic testing cannot directly evaluate a subject's color vision (phenotype), most congenital color vision deficiencies are well-correlated with genotype. Therefore, the genotype can be directly evaluated and used to predict the phenotype. This is especially useful for progressive forms that do not have a strongly color deficient phenotype at a young age. However, it can also be used to sequence the L- and M-Opsins on the X-chromosome, since the most common alleles of these two genes are known and have even been related to exact spectral sensitivities and peak wavelengths. A subject's color vision can therefore be classified through genetic testing, but this is just a prediction of the phenotype, since color vision can be affected by countless non-genetic factors such as your cone mosaic.

Despite much recent improvement in gene therapy for color blindness, there is currently no FDA approved treatment for any form of CVD, and otherwise no cure for CVD currently exists. Management of the condition by using lenses to alleviate symptoms or smartphone apps to aid with daily tasks is possible.

There are three kinds of lenses that an individual can wear that can increase their accuracy in some color related tasks (although none of these will ""fix"" color blindness or grant the wearer normal color vision):


Many mobile and computer applications have been developed to aid color blind individuals in completing color tasks:


In 2003, a cybernetic device called eyeborg was developed to allow the wearer to hear sounds representing different colors. Achromatopsic artist Neil Harbisson was the first to use such a device in early 2004; the eyeborg allowed him to start painting in color by memorizing the sound corresponding to each color. In 2012, at a TED Conference, Harbisson explained how he could now perceive colors outside the ability of human vision.

Color blindness affects a large number of individuals, with protans and deutans being the most common types. In individuals with Northern European ancestry, as many as 8 percent of men and 0.4 percent of women experience congenital color deficiency. Interestingly, even Dalton's very first paper already arrived upon this 8% number:

During the 17th and 18th century, several philosophers hypothesized that not all individuals perceived colors in the same way:

The phenomenon only came to be scientifically studied in 1794, when English chemist John Dalton gave the first account of color blindness in a paper to the Manchester Literary and Philosophical Society, which was published in 1798 as "Extraordinary Facts relating to the Vision of Colours: With Observations". Genetic analysis of Dalton's preserved eyeball confirmed him as having deuteranopia in 1995, some 150 years after his death.

Influenced by Dalton, German writer J. W. von Goethe studied color vision abnormalities in 1798 by asking two young subjects to match pairs of colors.

In 1875, the Lagerlunda train crash in Sweden brought color blindness to the forefront. Following the crash, Professor Alarik Frithiof Holmgren, a physiologist, investigated and concluded that the color blindness of the engineer (who had died) had caused the crash. Professor Holmgren then created the first test for color vision using multicolored skeins of wool to detect color blindness and thereby exclude the color blind from jobs in the transportation industry requiring color vision to interpret safety signals. However, there is a claim that there is no firm evidence that color deficiency did cause the collision, or that it might have not been the sole cause.

In 1920, Frederick William Edridge-Green devised an alternative theory of color vision and color blindness based on Newton's classification of 7 fundamental colors (ROYGBIV). Edridge-Green classified color vision based on how many distinct colors a subject could see in the spectrum. Normal subjects were termed "hexachromic" as they could not discern Indigo. Subjects with superior color vision, who could discern indigo, were "heptachromic". The color blind were therefore "dichromic" (equivalent to dichromacy) or "tri-", "tetra-" or "pentachromic" (anomalous trichromacy).

In the United States, under federal anti-discrimination laws such as the Americans with Disabilities Act, color vision deficiencies have not been found to constitute a disability that triggers protection from workplace discrimination.

A Brazilian court ruled that the color blind are protected by the Inter-American Convention on the Elimination of All Forms of Discrimination against Person with Disabilities. At trial, it was decided that the carriers of color blindness have a right of access to wider knowledge, or the full enjoyment of their human condition.

Color blindness may make it difficult or impossible for a person to engage in certain activities. Persons with color blindness may be legally or practically barred from occupations in which color perception is an essential part of the job ("e.g.," mixing paint colors), or in which color perception is important for safety ("e.g.," operating vehicles in response to color-coded signals). This occupational safety principle originates from the aftermath of the 1875 Lagerlunda train crash, which Alarik Frithiof Holmgren blamed on the color blindness of the engineer and created the first occupational screening test (Holmgren's wool test) against the color blind.

Color vision is important for occupations using telephone or computer networking cabling, as the individual wires inside the cables are color-coded using green, orange, brown, blue and white colors. Electronic wiring, transformers, resistors, and capacitors are color-coded as well, using black, brown, red, orange, yellow, green, blue, violet, gray, white, silver, and gold.

Participation, officiating and viewing sporting events can be impacted by color blindness. Professional football players Thomas Delaney and Fabio Carvalho have discussed the difficulties when color clashes occur, and research undertaken by FIFA has shown that enjoyment and player progression can be hampered by issues distinguishing the difference between the pitch and training objects or field markings. Snooker World Champions Mark Williams and Peter Ebdon sometimes need to ask the referee for help distinguishing between the red and brown balls due to their color blindness. Both have played foul shots on notable occasions by the wrong ball.

Red–green color blindness can make it difficult to drive, primarily due to the inability to differentiate red–amber–green traffic lights. Protans are further disadvantaged due to the darkened perception of reds, which can make it more difficult to quickly recognize brake lights. In response, some countries have refused to grant driver's licenses to individuals with color blindness:

Although many aspects of aviation depend on color coding, only a few of them are critical enough to be interfered with by some milder types of color blindness. Some examples include color-gun signaling of aircraft that have lost radio communication, color-coded glide-path indications on runways, and the like. Some jurisdictions restrict the issuance of pilot credentials to persons with color blindness for this reason. Restrictions may be partial, allowing color-blind persons to obtain certification but with restrictions, or total, in which case color-blind persons are not permitted to obtain piloting credentials at all.

In the United States, the Federal Aviation Administration requires that pilots be tested for normal color vision as part of their medical clearance in order to obtain the required medical certificate, a prerequisite to obtaining a pilot's certification. If testing reveals color blindness, the applicant may be issued a license with restrictions, such as no night flying and no flying by color signals—such a restriction effectively prevents a pilot from holding certain flying occupations, such as that of an airline pilot, although commercial pilot certification is still possible, and there are a few flying occupations that do not require night flight and thus are still available to those with restrictions due to color blindness (e.g., agricultural aviation). The government allows several types of tests, including medical standard tests ("e.g.," the Ishihara, Dvorine, and others) and specialized tests oriented specifically to the needs of aviation. If an applicant fails the standard tests, they will receive a restriction on their medical certificate that states: "Not valid for night flying or by color signal control". They may apply to the FAA to take a specialized test, administered by the FAA. Typically, this test is the "color vision light gun test". For this test an FAA inspector will meet the pilot at an airport with an operating control tower. The color signal light gun will be shone at the pilot from the tower, and they must identify the color. If they pass they may be issued a waiver, which states that the color vision test is no longer required during medical examinations. They will then receive a new medical certificate with the restriction removed. This was once a Statement of Demonstrated Ability (SODA), but the SODA was dropped, and converted to a simple waiver (letter) early in the 2000s.

Research published in 2009 carried out by the City University of London's Applied Vision Research Centre, sponsored by the UK's Civil Aviation Authority and the U.S. Federal Aviation Administration, has established a more accurate assessment of color deficiencies in pilot applicants' red/green and yellow–blue color range which could lead to a 35% reduction in the number of prospective pilots who fail to meet the minimum medical threshold.



Computer security

Computer security, cybersecurity, digital security or information technology security (IT security) is the protection of computer systems and networks from attacks by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.

The field is significant due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi. Also, due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.

A vulnerability is a weakness in the design, implementation, operation, or internal control of a computer or system. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database. An "exploitable" vulnerability is one for which at least one working attack or "exploit" exists. Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts.

Various people or parties are vulnerable to cyber attacks; however, different groups are likely to experience different types of attacks more than others.

In April 2023, the United Kingdom Department for Science, Innovation & Technology released a report on cyber attacks over the last 12 months. They surveyed 2,263 UK businesses, 1,174 UK registered charities and 554 education institutions. The research found that "32% of businesses and 24% of charities overall recall any breaches or attacks from the last 12 months." These figures were much higher for "medium businesses (59%), large businesses (69%) and high-income charities with £500,000 or more in annual income (56%)." Yet, although medium or large businesses are more often the victims, since larger companies have generally improved their security over the last decade, small and midsize businesses (SMBs) have also become increasingly vulnerable as they often "do not have advanced tools to defend the business." SMBs are most likely to be affected by malware, ransomware, phishing, man-in-the-middle attacks, and Denial-of Service (DoS) Attacks.

Normal internet users are most likely to be affected by untargeted cyber attacks. These are where attackers indiscriminately target as many devices, services or users as possible. They do this using techniques that take advantage of the openness of the Internet. These strategies mostly include phishing, ransomware, water holing and scanning.

To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of the following categories:

A backdoor in a computer system, a cryptosystem, or an algorithm, is any secret method of bypassing normal authentication or security controls. These weaknesses may exist for many reasons, including original design or poor configuration. Due to the nature of backdoors, they are of greater concern to companies and databases as opposed to individuals.

Backdoors may be added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons. Criminals often use malware to install backdoors, giving them remote administrative access to a system. Once they have access, cybercriminals can "modify files, steal personal information, install unwanted software, and even take control of the entire computer."

Backdoors can be very hard to detect, and are usually discovered by someone who has access to the application source code or intimate knowledge of the operating system of the computer.

Denial-of-service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim's account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of distributed denial-of-service (DDoS) attacks are possible, where the attack comes from a large number of points. In this case defending against these attacks is much more difficult. Such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques, including distributed reflective denial-of-service (DRDoS), where innocent systems are fooled into sending traffic to the victim. With such attacks, the amplification factor makes the attack easier for the attacker because they have to use little bandwidth themselves. To understand why attackers may carry out these attacks, see the 'attacker motivation' section.

A direct-access attack is when an unauthorized user (an attacker) gains physical access to a computer, most likely to directly copy data from it or to steal information. Attackers may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphones. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.

Direct service attackers are related in concept to direct memory attacks which allow an attacker to gain direct access to a computer's memory. The attacks "take advantage of a feature of modern computers that allows certain devices, such as external hard drives, graphics cards or network cards, to access the computer's memory directly."

To help prevent these attacks, computer users must ensure that they have strong passwords, that their computer is locked at all times when they are not using it, and that they keep their computer with them at all times when traveling.

Eavesdropping is the act of surreptitiously listening to a private computer conversation (communication), usually between hosts on a network. It typically occurs when a user connects to a network where traffic is not secured or encrypted and sends sensitive business data to a colleague, which, when listened to by an attacker, could be exploited. Data transmitted across an "open network" allows an attacker to exploit a vulnerability and intercept it via various methods.

Unlike malware, direct-access attacks, or other forms of cyber attacks, eavesdropping attacks are unlikely to negatively affect the performance of networks or devices, making them difficult to notice. In fact, "the attacker does not need to have any ongoing connection to the software at all. The attacker can insert the software onto a compromised device, perhaps by direct insertion or perhaps by a virus or other malware, and then come back some time later to retrieve any data that is found or trigger the software to send the data at some determined time."

Using a virtual private network (VPN), which encrypts data between two points, is one of the most common forms of protection against eavesdropping. Using the best form of encryption possible for wireless networks is best practice, as well as using HTTPS instead of an unencrypted HTTP.

Programs such as Carnivore and NarusInSight have been used by the Federal Bureau of Investigation (FBI) and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact with the outside world) can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware. TEMPEST is a specification by the NSA referring to these attacks.

Malicious software (malware) is any software code or computer program "intentionally written to harm a computer system or its users." Once present on a computer, it can leak sensitive details such as personal information, business information and passwords, can give control of the system to the attacker, and can corrupt or delete data permanently. Another type of malware is ransomware, which is when "malware installs itself onto a victim's machine, encrypts their files, and then turns around and demands a ransom (usually in Bitcoin) to return that data to the user."

Types of malware include some of the following:

Man-in-the-middle Attacks involve a malicious attacker trying to intercept, surveil and/or modify communications between two parties by spoofing one or both party's identities and injecting themselves in-between.
Types of MITM Attacks include:

Surfacing in 2017, a new class of multi-vector, polymorphic cyber threats combine several types of attacks and change form to avoid cybersecurity controls as they spread.

Multi-vector polymorphic attacks, as the name describes, are both multi-vectored and polymorphic. Firstly, they are a singular attack that involves multiple methods of attack. In this sense, they are “multi-vectored (i.e. the attack can use multiple means of propagation such as via the Web, email and applications." However, they are also multi-staged, meaning that “they can infiltrate networks and move laterally inside the network.” The attacks can be polymorphic, meaning that the cyberattacks used such as viruses, worms or trojans “constantly change (“morph”) making it nearly impossible to detect them using signature-based defences.”

Phishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users. Phishing is typically carried out by email spoofing, instant messaging, text message, or on a phone call. They often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. The fake website often asks for personal information, such as login details and passwords. This information can then be used to gain access to the individual's real account on the real website.

Preying on a victim's trust, phishing can be classified as a form of social engineering. Attackers can use creative ways to gain access to real accounts. A common scam is for attackers to send fake electronic invoices to individuals showing that they recently purchased music, apps, or others, and instructing them to click on a link if the purchases were not authorized. A more strategic type of phishing is spear-phishing which leverages personal or organization-specific details to make the attacker appear like a trusted source. Spear-phishing attacks target specific individuals, rather than the broad net cast by phishing attempts.

Privilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become "root" and have full unrestricted access to a system. The severity of attacks can range from attacks simply sending an unsolicited email to a ransomware attack on large amounts of data. Privilege escalation usually starts with social engineering techniques, often phishing.

Privilege escalation can be separated into two strategies, horizontal and vertical privilege escalation:


Any computational system affects its environment in some form. This effect it has on its environment includes a wide range of criteria, which can range from electromagnetic radiation to residual effect on RAM cells which as a consequence make a Cold boot attack possible, to hardware implementation faults that allow for access and or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios, the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure.

Social engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer. This generally involves exploiting people's trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. One of the main techniques of social engineering are phishing attacks.

In early 2016, the FBI reported that such business email compromise (BEC) scams had cost US businesses more than $2 billion in about two years.

In May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.

Spoofing is an act of pretending to be a valid entity through the falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. Spoofing is closely related to phishing. There are several types of spoofing, including:
In 2018, the cybersecurity firm Trellix published research on the life-threatening risk of spoofing in the healthcare industry.

Tampering describes a malicious modification or alteration of data. An intentional but unauthorized act resulting in the modification of a system, components of systems, its intended behavior, or data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.

HTML smuggling allows an attacker to "smuggle" a malicious code inside a particular HTML or web page. HTML files can carry payloads concealed as benign, inert data in order to defeat content filters. These payloads can be reconstructed on the other side of the filter.

When a target user opens the HTML, the malicious code is activated; the web browser then "decodes" the script, which then unleashes the malware onto the target's device.

Employee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization. Information security culture is the "...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds."

Andersson and Reimers (2014) found that employees often do not see themselves as part of their organization's information security effort and often take actions that impede organizational changes. Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cybersecurity incidents involved internal actors within a company. Research shows information security culture needs to be improved continuously. In "Information Security Culture from Analysis to Change", authors commented, "It's a never-ending process, a cycle of evaluation and change or maintenance." To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.

In computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.

Some common countermeasures are listed in the following sections:

Security by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered a main feature.

The UK government's National Cyber Security Centre separate secure cyber design principles into five sections:


These design principles of security by design can include some of the following techniques:

Security architecture can be defined as the "practice of designing computer systems to achieve security goals." These goals have overlap with the principles of "security by design" explored above, including to "make initial compromise of the system difficult," and to "limit the impact of any compromise." In practice, the role of a security architect would be to ensure the structure of a system reinforces the security of the system, and that new changes are safe and meet the security requirements of the organisation.

Similarly, Techopedia defines security architecture as "a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible." The key attributes of security architecture are:

Practicing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.

A state of computer security is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:

Today, computer security consists mainly of preventive measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet. They can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking. Another implementation is a so-called "physical firewall", which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.

Some organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.

In order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security. To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.

Vulnerability management is the cycle of identifying, fixing or mitigating vulnerabilities, especially in software and firmware. Vulnerability management is integral to computer security and network security.

Vulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities, such as open ports, insecure software configuration, and susceptibility to malware. In order for these tools to be effective, they must be kept up to date with every new update the vendor release. Typically, these updates will scan for the new vulnerabilities that were introduced recently.

Beyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.

The act of assessing and reducing vulnerabilities to cyber attacks is commonly referred to as information technology security assessments. They aims to assess systems for risk and to predict and test for their vulnerabilities. While formal verification of the correctness of computer systems is possible, it is not yet common. Operating systems formally verified include seL4, and SYSGO's PikeOS – but these make up a very small percentage of the market.

It is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates and/or hiring people with expertise in security. Large companies with significant threats can hire Security Operations Centre (SOC) Analysts. These are specialists in cyber defences, with their role ranging from "conducting threat analysis to investigating reports of any new issues and preparing and testing disaster recovery plans."

Whilst no measures can completely guarantee the prevention of an attack, these measures can help mitigate the damage of possible attacks. The effects of data loss/damage can be also reduced by careful backing up and insurance.

Outside of formal assessments, there are various methods of reducing vulnerabilities. Two factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires "something you know"; a password or PIN, and "something you have"; a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.

Protecting against social engineering and direct computer access (physical) attacks can only be occur by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk by improving people's knowledge of how to protect themselves and by increasing people's awareness of threats. However, even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.

Inoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.

Hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.

One use of the term "computer security" refers to technology that is used to implement secure operating systems. Using secure operating systems is a good way of ensuring computer security. These are systems that have achieved certification from an external security-auditing organization, the most popular evaluations are Common Criteria (CC).

In software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are "secure by design". Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;
important for cryptographic protocols for example.

Within computer systems, two of the main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).

An access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.

Role-based access control is an approach to restricting system access to authorized users, used by the majority of enterprises with more than 500 employees, and can implement mandatory access control (MAC) or discretionary access control (DAC).

A further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.

The end-user is widely recognized as the weakest link in the security chain and it is estimated that more than 90% of security incidents and breaches involve some kind of human error. Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments. A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites. This is a gift to attackers who have obtained access to a machine by some means. The risk may be mitigated by the use of two-factor authentication.

As the human component of cyber risk is particularly relevant in determining the global cyber risk an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.

The focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.

Related to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks. Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.

The most common acts of digital hygiene can include updating malware protection, cloud back-ups, passwords, and ensuring restricted admin rights and network firewalls. As opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline or education. It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal and/or collective digital security. As such, these measures can be performed by laypeople, not just security experts.

Cyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term "computer virus" was coined almost simultaneously with the creation of the first working computer viruses, the term "cyber hygiene" is a much later invention, perhaps as late as 2000 by Internet pioneer Vint Cerf. It has since been adopted by the Congress and Senate of the United States, the FBI, EU institutions and heads of state.

Responding to attempted security breaches is often very difficult for a variety of reasons, including:

Where an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.

The growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there are an increasing number of systems at risk.

The computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains. Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market. In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.

The UCLA Internet Report: Surveying the Digital Future (2000) found that the privacy of personal data created barriers to online sales and that more than nine out of 10 internet users were somewhat or very concerned about credit card security.

The most common web technologies for improving security between browsers and websites are named SSL (Secure Sockets Layer), and its successor TLS (Transport Layer Security), identity management and authentication services, and domain name services allow companies and consumers to engage in secure communications and commerce. Several versions of SSL and TLS are commonly used today in applications such as web browsing, e-mail, internet faxing, instant messaging, and VoIP (voice-over-IP). There are various interoperable implementations of these technologies, including at least one implementation that is open source. Open source allows anyone to view the application's source code, and look for and report vulnerabilities.

The credit card companies Visa and MasterCard cooperated to develop the secure EMV chip which is embedded in credit cards. Further developments include the Chip Authentication Program where banks give customers hand-held card readers to perform online secure transactions. Other developments in this arena include the development of technology such as Instant Issuance which has enabled shopping mall kiosks acting on behalf of banks to issue on-the-spot credit cards to interested customers.

Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.

The aviation industry is very reliant on a series of complex systems which could be attacked. A simple power outage at one airport can cause repercussions worldwide, much of the system relies on radio transmissions which could be disrupted, and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. There is also potential for attack from within an aircraft.

Implementing fixes in aerospace systems poses a unique challenge because efficient air transportation is heavily affected by weight and volume. Improving security by adding physical devices to airplanes could increase their unloaded weight, and could potentially reduce cargo or passenger capacity.

In Europe, with the (Pan-European Network Service) and NewPENS, and in the US with the NextGen program, air navigation service providers are moving to create their own dedicated networks.

Many modern passports are now biometric passports, containing an embedded microchip that stores a digitized photograph and personal information such as name, gender, and date of birth. In addition, more countries are introducing facial recognition technology to reduce identity-related fraud. The introduction of the ePassport has assisted border officials in verifying the identity of the passport holder, thus allowing for quick passenger processing. Plans are under way in the US, the UK, and Australia to introduce SmartGate kiosks with both retina and fingerprint recognition technology. The airline industry is moving from the use of traditional paper tickets towards the use of electronic tickets (e-tickets). These have been made possible by advances in online credit card transactions in partnership with the airlines. Long-distance bus companies are also switching over to e-ticketing transactions today.

The consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.

Desktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.

The increasing number of home automation devices such as the Nest thermostat are also potential targets.

Today many health-care providers and health insurance companies use the internet to provide enhanced products and services, for example through use of tele-health to potentially offer better quality and access to healthcare, or fitness trackers to lower insurance premiums.

The health care company Humana partners with WebMD, Oracle Corporation, EDS and Microsoft to enable its members to access their health care records, as well as to provide an overview of health care plans. Patient records are increasingly being placed on secure in-house networks, alleviating the need for extra storage space.

Large corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include the loss of millions of clients' credit card and financial details by Home Depot, Staples, Target Corporation, and Equifax.

Medical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale. Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.

Not all attacks are financially motivated, however: security firm HBGary Federal had a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group, and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.

Vehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network. Self-driving cars are expected to be even more complex. All of these systems carry some security risks, and such issues have gained wide attention.

Simple examples of risk include a malicious compact disc being used as an attack vector, and the car's onboard microphones being used for eavesdropping. However, if access is gained to a car's internal controller area network, the danger is much greater – and in a widely publicized 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.

Manufacturers are reacting in numerous ways, with Tesla in 2016 pushing out some security fixes "over the air" into its cars' computer systems. In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.

Additionally, e-Drivers' licenses are being developed using the same technology. For example, Mexico's licensing authority (ICV) has used a smart card platform to issue the first e-Drivers' licenses to the city of Monterrey, in the state of Nuevo León.

Shipping companies have adopted RFID (Radio Frequency Identification) technology as an efficient, digitally secure, tracking device. Unlike a barcode, RFID can be read up to 20 feet away. RFID is used by FedEx and UPS.

Government and military computer systems are commonly attacked by activists and foreign powers. Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, as well as student records.

The FBI, CIA, and Pentagon, all utilize secure controlled access technology for any of their buildings. However, the use of this form of technology is spreading into the entrepreneurial world. More and more companies are taking advantage of the development of digitally secure controlled access technology. GE's ACUVision, for example, offers a single panel platform for access control, alarm monitoring and digital recording.

The Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data. Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.

While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,
it also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat. If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.

An attack that targets physical infrastructure and/or human lives is sometimes referred to as a cyber-kinetic attack. As IoT devices and appliances gain currency, cyber-kinetic attacks can become pervasive and significantly damaging.

Medical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps. There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks, Windows XP exploits, viruses, and data breaches of sensitive data stored on hospital servers. On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices – but no structure for enforcement.

In distributed generation systems, the risk of a cyber attack is real, according to "Daily Energy Insider". An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would "allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid."

Perhaps the most widely known digitally secure telecommunication device is the SIM (Subscriber Identity Module) card, a device that is embedded in most of the world's cellular devices before any service can be obtained. The SIM card is just the beginning of this digitally secure environment.

The Smart Card Web Servers draft standard (SCWS) defines the interfaces to an HTTP server in a smart card. Tests are being conducted to secure OTA ("over-the-air") payment and credit card information from and to a mobile phone. 
Combination SIM/DVD devices are being developed through Smart Video Card technology which embeds a DVD-compliant optical disc into the card body of a regular SIM card.

Other telecommunication developments involving digital security include mobile signatures, which use the embedded SIM card to generate a legally binding electronic signature.

Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal."

However, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).

As with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in "The Cuckoo's Egg".

Attackers motivations can vary for all types of attacks from pleasure to for political goals. For example, "hacktivists" may target a company a company or organisation that carries out activities they do not agree with. This would be to create bad publicity for the company by having its website crash.

High capability hackers, often with larger backing or state sponsorship, may attack based on the demands of their financial backers. These attacks are more likely to attempt more serious attack. An example of a more serious attack was the 2015 Ukraine power grid hack, which reportedly utilised the spear-phising, destruction of files, and denial-of-service attacks to carry out the full attack.

Additionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas. The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference.

A standard part of threat modeling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.

Computer security incident management is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.
Incident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization's response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization's response and resolution.

There are four key components of a computer security incident response plan:

Some illustrative examples of different types of computer security breaches are given below.
In 1988, 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet computer worm. The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris who said "he wanted to count how many machines were connected to the Internet".

In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.

In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.

In 2010, the computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges. It did so by disrupting industrial programmable logic controllers (PLCs) in a targeted attack. This is generally believed to have been launched by Israel and the United States to disrupt Iran's nuclear program – although neither has publicly admitted this.

In early 2013, documents provided by Edward Snowden were published by "The Washington Post" and "The Guardian" exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption. This standard was later withdrawn due to widespread criticism. The NSA additionally were revealed to have tapped the links between Google's data centers.

A Ukrainian hacker known as Rescator broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. "The malware utilized is absolutely unsophisticated and uninteresting," says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.

In April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5 million personnel records handled by the office. The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States. Data targeted in the breach included personally identifiable information such as Social Security numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check. It is believed the hack was perpetrated by Chinese hackers.

In July 2015, a hacker group is known as The Impact Team successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained to function.

In June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast.

International legal issues of cyber attacks are complicated in nature. There is no global base of common rules to judge, and eventually punish, cybercrimes and cybercriminals - and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. Proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies. "Computer viruses switch from one country to another, from one jurisdiction to another – moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world." The use of techniques such as dynamic DNS, fast flux and bullet proof servers add to the difficulty of investigation and enforcement.

The role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.

The government's regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today's libertarian blockchain and bitcoin discussions.

Many government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the "industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through." On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.

On 22 May 2020, the UN Security Council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace. According to UN Secretary-General António Guterres, new technologies are too often used to violate rights.

Many different teams and organizations exist, including:

On 14 April 2016, the European Parliament and the Council of the European Union adopted the General Data Protection Regulation (GDPR). The GDPR, which came into force on 25 May 2018, grants individuals within the European Union (EU) and the European Economic Area (EEA) the right to the protection of personal data. The regulation requires that any entity that processes personal data incorporate data protection by design and by default. It also requires that certain organizations appoint a Data Protection Officer (DPO).

Most countries have their own computer emergency response team to protect network security.

Since 2010, Canada has had a cybersecurity strategy. This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure. The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online. There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.

The Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond & recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors. It posts regular cybersecurity bulletins & operates an online reporting tool where individuals and organizations can report a cyber incident.

To inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations, and launched the Cyber Security Cooperation Program. They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.

Public Safety Canada aims to begin an evaluation of Canada's cybersecurity strategy in early 2015.

Australian federal government announced an $18.2 million investment to fortify the cybersecurity resilience of small and medium enterprises (SMEs) and enhance their capabilities in responding to cyber threats. This financial backing is an integral component of the soon-to-be-unveiled 2023-2030 Australian Cyber Security Strategy, slated for release within the current week. A substantial allocation of $7.2 million is earmarked for the establishment of a voluntary cyber health check program, facilitating businesses in conducting a comprehensive and tailored self-assessment of their cybersecurity upskill.

This avant-garde health assessment serves as a diagnostic tool, enabling enterprises to ascertain the robustness of Australia's cyber security regulations. Furthermore, it affords them access to a repository of educational resources and materials, fostering the acquisition of skills necessary for an elevated cybersecurity posture. This groundbreaking initiative was jointly disclosed by Minister for Cyber Security Clare O'Neil and Minister for Small Business Julie Collins.
The United States has its first fully formed cyber plan in 15 years, as a result of the release of this National Cyber plan. In this policy, the US says it will: Protect the country by keeping networks, systems, functions, and data safe; Promote American wealth by building a strong digital economy and encouraging strong domestic innovation; Peace and safety should be kept by making it easier for the US to stop people from using computer tools for bad things, working with friends and partners to do this; and Increase the United States' impact around the world to support the main ideas behind an open, safe, reliable, and compatible Internet. The new U.S. cyber strategy seeks to allay some of those concerns by promoting responsible behavior in cyberspace, urging nations to adhere to a set of norms, both through international law and voluntary standards. It also calls for specific measures to harden U.S. government networks from attacks, like the June 2015 intrusion into the U.S. Office of Personnel Management (OPM), which compromised the records of about 4.2 million current and former government employees. And the strategy calls for the U.S. to continue to name and shame bad cyber actors, calling them out publicly for attacks when possible, along with the use of economic sanctions and diplomatic pressure.

Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.

The National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard "information, such as personal information (of web users), financial and banking information and sovereign data". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).

The Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.

Following cyberattacks in the first half of 2013, when the government, news media, television stations, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.

The 1986 , the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of "protected computers" as defined in . Although various other measures have been proposed – none has succeeded.

In 2013, executive order "Improving Critical Infrastructure Cybersecurity" was signed, which prompted the creation of the NIST Cybersecurity Framework.

In response to the Colonial Pipeline ransomware attack President Joe Biden signed Executive Order 14028 on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.

The General Services Administration (GSA) has standardized the "penetration test" service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).
The Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division. The division is home to US-CERT operations and the National Cyber Alert System. The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.

The third priority of the FBI is to: "Protect the United States against cyber-based attacks and high-technology crimes", and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.

In addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.

The Computer Crime and Intellectual Property Section (CCIPS) operates in the United States Department of Justice Criminal Division. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks. In 2017, CCIPS published A Framework for a Vulnerability Disclosure Program for Online Systems to help organizations "clearly describe authorized vulnerability disclosure and discovery conduct, thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the Computer Fraud and Abuse Act (18 U.S.C. § 1030)."

The United States Cyber Command, also known as USCYBERCOM, "has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners." It has no role in the protection of civilian networks.

The U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.

The Food and Drug Administration has issued guidance for medical devices, and the National Highway Traffic Safety Administration is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office, and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System. Concerns have also been raised about the future Next Generation Air Transportation System.

The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.

"Computer emergency response team" is a name given to expert groups that handle computer security incidents. In the US, two distinct organizations exist, although they do work closely together.

In the context of U.S. nuclear power plants, the U.S. Nuclear Regulatory Commission (NRC) outlines cybersecurity requirements under 10 CFR Part 73, specifically in §73.54.

The Nuclear Energy Institute's NEI 08-09 document, "Cyber Security Plan for Nuclear Power Reactors", outlines a comprehensive framework for cybersecurity in the nuclear power industry. Drafted with input from the U.S. NRC, this guideline is instrumental in aiding licensees to comply with the Code of Federal Regulations (CFR), which mandates robust protection of digital computers and equipment and communications systems at nuclear power plants against cyber threats.

There is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from "The Christian Science Monitor" wrote in a 2015 article titled "The New Cyber Arms Race":

This has led to new terms such as "cyberwarfare" and "cyberterrorism". The United States Cyber Command was created in 2009 and many other countries have similar forces.

There are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.

Cybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breaches. According to research from the Enterprise Strategy Group, 46% of organizations say that they have a "problematic shortage" of cybersecurity skills in 2016, up from 28% in 2015. Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail. However, the use of the term "cybersecurity" is more prevalent in government job descriptions.

Typical cybersecurity job titles and descriptions include:

Student programs are also available for people interested in beginning a career in cybersecurity. Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts. A wide range of certified courses are also available.

In the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government's cybersecurity strategy in order to encourage start-ups and innovation and to address the skills gap identified by the U.K Government.

In Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities.

The following terms used with regards to computer security are explained below:


Since the Internet's arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject in both our professional and personal lives. Cybersecurity and cyber threats have been consistently present for the last 60 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization of organized attacks such as distributed denial of service. This led to the formalization of cybersecurity as a professional discipline.

The April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security. Ware's work straddled the intersection of material, cultural, political, and social concerns.

A 1977 NIST publication introduced the "CIA triad" of confidentiality, integrity, and availability as a clear and simple way to describe key security goals. While still relevant, many more elaborate frameworks have since been proposed.

However, in the 1970s and 1980s, there were no grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. More often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. By the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.

One of the earliest examples of an attack on a computer network was the computer worm Creeper written by Bob Thomas at BBN, which propagated through the ARPANET in 1971. The program was purely experimental in nature and carried no malicious payload. A later program, Reaper, was created by Ray Tomlinson in 1972 and used to destroy Creeper.

Between September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage. The group hacked into American defense contractors, universities, and military base networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June 1987. He was convicted of espionage (along with two co-conspirators) on 15 Feb 1990.

In 1988, one of the first computer worms, called the Morris worm, was distributed via the Internet. It gained significant mainstream media attention.

In 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993. Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched Version 2.0.

The National Security Agency (NSA) is responsible for the protection of U.S. information systems and also for collecting foreign intelligence. The agency analyzes commonly used software and system configurations to find security flaws, which it can use for offensive purposes against competitors of the United States.

NSA contractors created and sold "click-and-shoot" attack tools to US agencies and close allies, but eventually, the tools made their way to foreign adversaries. In 2016, NSAs own hacking tools were hacked, and they have been used by Russia and North Korea. NSA's employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare. In 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials. Iran responded by heavily investing in their own cyberwarfare capability, which it began using against the United States.


Chris Cunningham

Chris Cunningham (born 15 October 1970) is a British video artist and music video director who directed music videos for electronic musicians such as Autechre, Squarepusher, and Aphex Twin on videos for "Windowlicker" and "Come to Daddy", and Björk's "All is Full of Love". All were used in Chris' chapter in Director's Label. 

He has also created art installations and directed short movies. He was approached to direct a movie version of William Gibson's cyberpunk novel "Neuromancer"; the project has been in development hell for more than two decades. In the 2000s, Cunningham began doing music production work, and has also designed album artwork for a variety of musicians.

After seeing Cunningham's work on the 1995 film version of "Judge Dredd", Stanley Kubrick head-hunted Cunningham to design and supervise animatronic tests of the central robot child character in his version of the film "A.I. Artificial Intelligence". Cunningham worked for over a year on the film before leaving to pursue a career as a director.

Earlier work in film included model-making, prosthetic make-up and concept illustrations for "Hardware" and "Dust Devil" for director Richard Stanley, work on "Nightbreed" for Clive Barker, and on "Alien" for David Fincher. Between 1990 and 1992, he contributed the occasional cover painting and strip to "Judge Dredd Megazine", working under the pseudonym "Chris Halls"; Halls is his stepfather's surname.

Cunningham has had close ties to Warp Records since his first video for Autechre, "Second Bad Vilbel" in 1995 and Squarepusher's "Come On My Selector" in 1998, which received airplay on MTV's "Amp" and MTV's Chill Out Zone in Europe. Videos for Aphex Twin's "Come to Daddy" and "Windowlicker" are perhaps his best known. His video for Björk's "All Is Full of Love" won multiple awards, including an MTV music video award for Breakthrough Video and was nominated for a Grammy for Best Short Form Music Video. It was also the first ever music video to win a Gold Pencil at the D&AD Awards. It can still be seen at the Museum of Modern Art in New York. His video for Aphex Twin's "Windowlicker" was nominated for the "Best Video" award at the Brit Awards 2000. He also directed Madonna's "Frozen" video which became an international hit and won the award for Best Special Effects at the 1998 MTV Music Video Awards. Cunningham also came out of a seven-year hiatus from making music videos to direct the video for "Sheena Is a Parasite" by the Horrors.

His video installation "Flex" was first shown in 2000 at the Royal Academy of Arts, and subsequently at the Anthony d'Offay Gallery and other art galleries. "Flex" was commissioned by the Anthony d'Offay Gallery for the exhibition curated by Norman Rosenthal and Max Wigram at the Royal Academy of Arts in 2000.

The Anthony d'Offay Gallery also commissioned "Monkey Drummer", a 2½ minute piece intended for exhibition as a companion to "Flex" at the 2000 "Apocalypse" exhibition at the Royal Academy of Arts: however, the piece was not finished in time. In it an automaton with nine appendages and the head of a monkey plays the drums to "Mt Saint Michel + Saint Michaels Mount", the 10th track on Aphex Twin's 2001 album "drukqs". "Monkey Drummer" debuted as part of Cunningham's installation at the 49th International Exhibition of Art at the 2001 Venice Biennale, which consisted of a loop of "Monkey Drummer", "Flex", and his video for Björk's "All Is Full of Love". In 2002 both "Flex" and "Monkey Drummer" were exhibited by 5th Gallery in Dublin, Ireland, in an exhibition curated by Artist/Curator Paul Murnaghan,

In 2007, an excerpt from "Flex" was shown in the Barbican's exhibition Seduced: Art and Sex from Antiquity to Now curated by Martin Kemp, Marina Wallace and Joanne Bernstein. alongside other pieces by Bacon, Klimt, Rembrandt, Rodin and Picasso.

In 2005, Cunningham released the short film "Rubber Johnny" as a DVD accompanied by a book of photographs and drawings. "Rubber Johnny", a six-minute experimental short film cut to a soundtrack by Aphex Twin remixed by Cunningham, was shot between 2001 and 2004. Shot on DV night-vision, it was made in Cunningham's own time as a home movie of sorts, and took three and half years of weekends to complete. "The Telegraph" called it "like a Looney Tunes short for a generation raised on video nasties and rave music".

During this period Cunningham also made another short film for Warp Films, "Spectral Musicians", which remains unreleased. The short film was set to Squarepusher's "My Fucking Sound" from his album "Go Plastic"; and to a piece called "Mutilation Colony" which was written especially for the short, and was released on the studio album "Do You Know Squarepusher".

Cunningham has directed a handful of commercials for companies and brands, including Gucci, Sony (PlayStation), Levi's, Telecom Italia, Nissan, and Orange.

In 2004/2005, Cunningham took a sabbatical from filmmaking to learn about music production and recording and to develop his own music projects. In December 2007 Cunningham produced two tracks, "Three Decades" and "Primary Colours", for "Primary Colours", the second album by the Horrors. In the summer of 2008, due to scheduling conflicts with his feature film script writing he could not work on the rest of the album which was subsequently recorded by Geoff Barrow from Portishead.

In 2008, he produced and arranged a new version of 'I Feel Love' for the Gucci commercial that he also directed. He travelled to Nashville to work with Donna Summer to record a brand new vocal for it.

In 2005, Cunningham played a 45-minute audio visual piece performed live in Tokyo and Osaka in front of 30,000+ fans over the two nights at the Japanese electronic music festival . These performances evolved into "Chris Cunningham Live", a 55-minute long performance piece combining original and remixed music and film. It features remixed, unreleased and brand new videos and music dynamically edited together into a new live piece spread over three screens. The sound accompanying these images includes Cunningham's first publicly performed compositions interspersed with his remixes of other artist's work. "Chris Cunningham Live" debuted as one of the headline attractions at Warp 20 in Paris on 8 May 2009 with other performances scheduled at festivals in UK, and a number of European cities later in the year. "Chris Cunningham Live" continued in June 2011, with performances in London, Barcelona, and Sydney, Australia.

Cunningham has created photography and cover artwork for various people including Björk's "All Is Full of Love", Aphex Twin's "Windowlicker" and "Come to Daddy".

In 2008, Cunningham produced a fashion shoot for "Dazed & Confused" using Grace Jones as a model to create "Nubian versions" of Rubber Johnny. In an interview for BBC's "The Culture Show", it was suggested that the collaboration may expand into a video project. In regards to the collaboration, Cunningham stated "For me, Grace has the strongest iconography of any artist in music. She’s definitely the most inspiring person I’ve worked with so far".

In November 2008, Cunningham followed on with another photoshoot for "Vice Magazine".

In an August 1999 "Spike Magazine" interview, cyberpunk author William Gibson stated "He (Chris) was brought to my attention by someone else. We were told, third-hand, that he was extremely wary of the Hollywood process, and wouldn't return calls. But someone else told us that "Neuromancer" had been his "The Wind in the Willows", that he'd read it when he was a kid. I went to London and we met." Gibson is also quoted in the article as saying "Chris is my own 100 percent personal choice...My only choice. The only person I've met who I thought might have a hope in hell of doing it right. I went back to see him in London just after he'd finished the Bjork video, and I sat on a couch beside this dead sex little Bjork robot, except it was wearing Aphex Twin's head. We talked."

In 2000, Cunningham and William Gibson began work on the script for Gibson's 1984 novel "Neuromancer". However, because "Neuromancer" was due to be a big budget studio film, it is rumoured that Cunningham pulled out due to being a first time director without final cut approval. He also felt that too much of the original book's ideas had been cannibalised by other recent films.

On 18 November 2004, in the FAQ on the William Gibson Board, Gibson was asked:

Cunningham was married to Warpaint's bassist Jenny Lee Lindberg. They are currently no longer together.

The video collection "The Work of Director Chris Cunningham" was released in November 2004 as part of the Directors Label set. This DVD includes selected highlights from 1995 to 2000.


Centaur

A centaur ( ; ; ), occasionally hippocentaur, also called Ixionidae (), is a creature from Greek mythology with the upper body of a human and the lower body and legs of a horse that was said to live in the mountains of Thessaly. In one version of the myth, the centaurs were named after Centaurus, and, through his brother Lapithes, were kin to the legendary tribe of the Lapiths.

Centaurs are thought of in many Greek myths as being as wild as untamed horses, and were said to have inhabited the region of Magnesia and Mount Pelion in Thessaly, the Foloi oak forest in Elis, and the Malean peninsula in southern Laconia. Centaurs are subsequently featured in Roman mythology, and were familiar figures in the medieval bestiary. They remain a staple of modern fantastic literature.

The Greek word "kentauros" is generally regarded as being of obscure origin. The etymology from "ken" + "tauros", 'piercing bull', was a euhemerist suggestion in Palaephatus' rationalizing text on Greek mythology, "On Incredible Tales" (Περὶ ἀπίστων), which included mounted archers from a village called "Nephele" eliminating a herd of bulls that were the scourge of Ixion's kingdom. Another possible related etymology can be "bull-slayer".

The centaurs were usually said to have been born of Ixion and Nephele. As the story goes, Nephele was a cloud made into the likeness of Hera in a plot to trick Ixion into revealing his lust for Hera to Zeus. Ixion seduced Nephele and from that relationship centaurs were created. Another version, however, makes them children of Centaurus, a man who mated with the Magnesian mares. Centaurus was either himself the son of Ixion and Nephele (inserting an additional generation) or of Apollo and the nymph Stilbe. In the latter version of the story, Centaurus's twin brother was Lapithes, ancestor of the Lapiths.

Another tribe of centaurs was said to have lived on Cyprus. According to Nonnus, the Cyprian Centaurs were fathered by Zeus, who, in frustration after Aphrodite had eluded him, spilled his seed on the ground of that land. Unlike those of mainland Greece, the Cyprian centaurs were ox-horned.

There were also the Lamian Pheres, twelve rustic daimones (spirits) of the Lamos river. They were set by Zeus to guard the infant Dionysos, protecting him from the machinations of Hera, but the enraged goddess transformed them into ox-horned Centaurs unrelated to the Cyprian Centaurs. The Lamian Pheres later accompanied Dionysos in his campaign against the Indians.

The centaur's half-human, half-horse composition has led many writers to treat them as liminal beings, caught between the two natures they embody in contrasting myths; they are both the embodiment of untamed nature, as in their battle with the Lapiths (their kin), and conversely, teachers like Chiron.

The Centaurs are best known for their fight with the Lapiths who, according to one origin myth, would have been cousins to the centaurs. The battle, called the Centauromachy, was caused by the centaurs' attempt to carry off Hippodamia and the rest of the Lapith women on the day of Hippodamia's marriage to Pirithous, who was the king of the Lapithae and a son of Ixion. Theseus, a hero and founder of cities, who happened to be present, threw the balance in favour of the Lapiths by assisting Pirithous in the battle. The Centaurs were driven off or destroyed. Another Lapith hero, Caeneus, who was invulnerable to weapons, was beaten into the earth by Centaurs wielding rocks and the branches of trees. In her article "The Centaur: Its History and Meaning in Human Culture", Elizabeth Lawrence claims that the contests between the centaurs and the Lapiths typify the struggle between civilization and barbarism.

The Centauromachy is most famously portrayed in the metopes of the Parthenon by Phidias and in the "Battle of the Centaurs", a relief by Michelangelo.


The most common theory holds that the idea of centaurs came from the first reaction of a non-riding culture, as in the Minoan Aegean world, to nomads who were mounted on horses. The theory suggests that such riders would appear as half-man, half-animal. Bernal Díaz del Castillo reported that the Aztecs also had this misapprehension about Spanish cavalrymen. The Lapith tribe of Thessaly, who were the kinsmen of the Centaurs in myth, were described as the inventors of horse-riding by Greek writers. The Thessalian tribes also claimed their horse breeds were descended from the centaurs.

Robert Graves (relying on the work of Georges Dumézil, who argued for tracing the centaurs back to the Indian Gandharva), speculated that the centaurs were a dimly remembered, pre-Hellenic fraternal earth cult who had the horse as a totem. A similar theory was incorporated into Mary Renault's "The Bull from the Sea."

Though female centaurs, called centaurides or centauresses, are not mentioned in early Greek literature and art, they do appear occasionally in later antiquity. A Macedonian mosaic of the 4th century BC is one of the earliest examples of the centauress in art. Ovid also mentions a centauress named Hylonome who committed suicide when her husband Cyllarus was killed in the war with the Lapiths.
The Kalibangan cylinder seal, dated to be around 2600–1900 BC, found at the site of Indus-Valley civilization shows a battle between men in the presence of centaur-like creatures. Other sources claim the creatures represented are actually half human and half tigers, later evolving into the Hindu Goddess of War. These seals are also evidence of Indus-Mesopotamia relations in the 3rd millennium BC.

In a popular legend associated with Pazhaya Sreekanteswaram Temple in Thiruvananthapuram, the curse of a saintly Brahmin transformed a handsome Yadava prince into a creature having a horse's body and the prince's head, arms, and torso in place of the head and neck of the horse.

Kinnaras, another half-man, half-horse mythical creature from Indian mythology, appeared in various ancient texts, arts, and sculptures from all around India. It is shown as a horse with the torso of a man where the horse's head would be, and is similar to a Greek centaur.

A centaur-like half-human, half-equine creature called "Polkan" appeared in Russian folk art and lubok prints of the 17th–19th centuries. Polkan is originally based on "Pulicane", a half-dog from Andrea da Barberino's poem "I Reali di Francia", which was once popular in the Slavonic world in prosaic translations.

The extensive Mycenaean pottery found at Ugarit included two fragmentary Mycenaean terracotta figures which have been tentatively identified as centaurs. This finding suggests a Bronze Age origin for these creatures of myth. A painted terracotta centaur was found in the "Hero's tomb" at Lefkandi, and by the Geometric period, centaurs figure among the first representational figures painted on Greek pottery. An often-published Geometric period bronze of a warrior face-to-face with a centaur is at the Metropolitan Museum of Art.

In Greek art of the Archaic period, centaurs are depicted in three different forms.


At a later period, paintings on some "amphorae" depict winged centaurs.

Centaurs were also frequently depicted in Roman art. One example is the pair of centaurs drawing the chariot of Constantine the Great and his family in the Great Cameo of Constantine ("circa" AD 314–16), which embodies wholly pagan imagery, and contrasts sharply with the popular image of Constantine as the patron of early Christianity.

Centaurs preserved a Dionysian connection in the 12th-century Romanesque carved capitals of Mozac Abbey in the Auvergne. Other similar capitals depict harvesters, boys riding goats (a further Dionysiac theme), and griffins guarding the chalice that held the wine. Centaurs are also shown on a number of Pictish carved stones from north-east Scotland erected in the 8th–9th centuries AD (e.g., at Meigle, Perthshire). Though outside the limits of the Roman Empire, these depictions appear to be derived from Classical prototypes.

The John C. Hodges library at The University of Tennessee hosts a permanent exhibit of a "Centaur from Volos" in its library. The exhibit, made by sculptor Bill Willers by combining a study human skeleton with the skeleton of a Shetland pony, is entitled "Do you believe in Centaurs?". According to the exhibitors, it was meant to mislead students in order to make them more critically aware.

Centaurs are common in European heraldry, although more frequent in continental than in British arms. A centaur holding a bow is referred to as a sagittarius.

Jerome's version of the "Life" of St Anthony the Great, written by Athanasius of Alexandria about the hermit monk of Egypt, was widely disseminated in the Middle Ages; it relates Anthony's encounter with a centaur who challenged the saint, but was forced to admit that the old gods had been overthrown. The episode was often depicted in "The Meeting of St Anthony Abbot and St Paul the Hermit" by the painter Stefano di Giovanni, who was known as "Sassetta". Of the two episodic depictions of the hermit Anthony's travel to greet the hermit Paul, one is his encounter with the demonic figure of a centaur along the pathway in a wood.

Lucretius, in his first-century BC philosophical poem "On the Nature of Things," denied the existence of centaurs, based on the differing rates of growth of human and equine anatomies. Specifically, he states that at the age of three years, horses are in the prime of their life while humans at the same age are still little more than babies, making hybrid animals impossible.

Centaurs are among the creatures which 14th-century Italian poet Dante placed as guardians in his "Inferno". In Canto XII, Dante and his guide Virgil meet a band led by Chiron and Pholus, guarding the bank of Phlegethon in the seventh circle of Hell, a river of boiling blood in which the violent against their neighbours are immersed, shooting arrows into any who move to a shallower spot than their allotted station. The two poets are treated with courtesy, and Nessus guides them to a ford. In Canto XXIV, in the eighth circle, in Bolgia 7, a ditch where thieves are confined, they meet but do not converse with Cacus (who is a giant in the ancient sources), wreathed in serpents and with a fire-breathing dragon on his shoulders, arriving to punish a sinner who has just cursed God. In his "Purgatorio", an unseen spirit on the sixth terrace cites the centaurs ("the drunken double-breasted ones who fought Theseus") as examples of the sin of gluttony.

C.S. Lewis' "The Chronicles of Narnia" series depicts centaurs as the wisest and noblest of creatures. Narnian Centaurs are gifted at stargazing, prophecy, healing, and warfare; a fierce and valiant race always faithful to the High King Aslan the Lion.

In J.K. Rowling's "Harry Potter" series, centaurs live in the Forbidden Forest close to Hogwarts, preferring to avoid contact with humans. They live in societies called herds and are skilled at archery, healing, and astrology, but like in the original myths, they are known to have some wild and barbarous tendencies.

With the exception of Chiron, the centaurs in Rick Riordan's "Percy Jackson & the Olympians" are seen as wild party-goers who use a lot of American slang. Chiron retains his mythological role as a trainer of heroes and is skilled in archery. In Riordan's subsequent series, "Heroes of Olympus", another group of centaurs are depicted with more animalistic features (such as horns) and appear as villains, serving the Gigantes.

Philip Jose Farmer's "World of Tiers" series (1965) includes centaurs, called Half-Horses or Hoi Kentauroi. His creations address several of the metabolic problems of such creatures—how could the human mouth and nose intake sufficient air to sustain both itself and the horse body and, similarly, how could the human ingest sufficient food to sustain both parts.

Brandon Mull's "Fablehaven" series features centaurs that live in an area called Grunhold. The centaurs are portrayed as a proud, elitist group of beings that consider themselves superior to all other creatures. The fourth book also has a variation on the species called an Alcetaur, which is part man, part moose.

The myth of the centaur appears in John Updike's novel "The Centaur". The author depicts a rural Pennsylvanian town as seen through the optics of the myth of the centaur. An unknown and marginalized local school teacher, just like the mythological Chiron did for Prometheus, gave up his life for the future of his son who had chosen to be an independent artist in New York.
Other hybrid creatures appear in Greek mythology, always with some liminal connection that links Hellenic culture with archaic or non-Hellenic cultures:

Also,

Additionally, "Bucentaur", the name of several historically important Venetian vessels, was linked to a posited ox-centaur or "βουκένταυρος" "(boukentauros)" by fanciful and likely spurious folk-etymology.




Chemotaxis

Chemotaxis (from "chemo-" + "taxis") is the movement of an organism or entity in response to a chemical stimulus. Somatic cells, bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming toward the highest concentration of food molecules, or to flee from poisons (e.g., phenol). In multicellular organisms, chemotaxis is critical to early development (e.g., movement of sperm towards the egg during fertilization) and development (e.g., migration of neurons or lymphocytes) as well as in normal function and health (e.g., migration of leukocytes during injury or infection). In addition, it has been recognized that mechanisms that allow chemotaxis in animals can be subverted during cancer metastasis. The aberrant chemotaxis of leukocytes and lymphocytes also contribute to inflammatory diseases such as atherosclerosis, asthma, and arthritis. Sub-cellular components, such as the polarity patch generated by mating yeast, may also display chemotactic behavior.

"Positive" chemotaxis occurs if the movement is toward a higher concentration of the chemical in question; "negative" chemotaxis if the movement is in the opposite direction. Chemically prompted kinesis (randomly directed or nondirectional) can be called chemokinesis.

Although migration of cells was detected from the early days of the development of microscopy by Leeuwenhoek, a Caltech lecture regarding chemotaxis propounds that 'erudite description of chemotaxis was only first made by T. W. Engelmann (1881) and W. F. Pfeffer (1884) in bacteria, and H. S. Jennings (1906) in ciliates'. The Nobel Prize laureate I. Metchnikoff also contributed to the study of the field during 1882 to 1886, with investigations of the process as an initial step of phagocytosis. The significance of chemotaxis in biology and clinical pathology was widely accepted in the 1930s, and the most fundamental definitions underlying the phenomenon were drafted by this time. The most important aspects in quality control of chemotaxis assays were described by H. Harris in the 1950s. In the 1960s and 1970s, the revolution of modern cell biology and biochemistry provided a series of novel techniques that became available to investigate the migratory responder cells and subcellular fractions responsible for chemotactic activity. The availability of this technology led to the discovery of C5a, a major chemotactic factor involved in acute inflammation. The pioneering works of J. Adler modernized Pfeffer's capillary assay and represented a significant turning point in understanding the whole process of intracellular signal transduction of bacteria.

Some bacteria, such as "E. coli", have several flagella per cell (4–10 typically). These can rotate in two ways:
The directions of rotation are given for an observer outside the cell looking down the flagella toward the cell.

The overall movement of a bacterium is the result of alternating tumble and swim phases, called run-and-tumble motion. As a result, the trajectory of a bacterium swimming in a uniform environment will form a random walk with relatively straight swims interrupted by random tumbles that reorient the bacterium. Bacteria such as "E. coli" are unable to choose the direction in which they swim, and are unable to swim in a straight line for more than a few seconds due to rotational diffusion; in other words, bacteria "forget" the direction in which they are going. By repeatedly evaluating their course, and adjusting if they are moving in the wrong direction, bacteria can direct their random walk motion toward favorable locations.

In the presence of a chemical gradient bacteria will chemotax, or direct their overall motion based on the gradient. If the bacterium senses that it is moving in the correct direction (toward attractant/away from repellent), it will keep swimming in a straight line for a longer time before tumbling; however, if it is moving in the wrong direction, it will tumble sooner. Bacteria like "E. coli" use temporal sensing to decide whether their situation is improving or not, and in this way, find the location with the highest concentration of attractant, detecting even small differences in concentration.

This biased random walk is a result of simply choosing between two methods of random movement; namely tumbling and straight swimming. The helical nature of the individual flagellar filament is critical for this movement to occur. The protein structure that makes up the flagellar filament, flagellin, is conserved among all flagellated bacteria. Vertebrates seem to have taken advantage of this fact by possessing an immune receptor (TLR5) designed to recognize this conserved protein. 

As in many instances in biology, there are bacteria that do not follow this rule. Many bacteria, such as "Vibrio", are monoflagellated and have a single flagellum at one pole of the cell. Their method of chemotaxis is different. Others possess a single flagellum that is kept inside the cell wall. These bacteria move by spinning the whole cell, which is shaped like a corkscrew.

Chemical gradients are sensed through multiple transmembrane receptors, called methyl-accepting chemotaxis proteins (MCPs), which vary in the molecules that they detect. Thousands of MCP receptors are known to be encoded across the bacterial kingdom. These receptors may bind attractants or repellents directly or indirectly through interaction with proteins of periplasmatic space. The signals from these receptors are transmitted across the plasma membrane into the cytosol, where "Che proteins" are activated. The Che proteins alter the tumbling frequency, and alter the receptors.

The proteins CheW and CheA bind to the receptor. The absence of receptor activation results in autophosphorylation in the histidine kinase, CheA, at a single highly conserved histidine residue. CheA, in turn, transfers phosphoryl groups to conserved aspartate residues in the response regulators CheB and CheY; CheA is a histidine kinase and it does not actively transfer the phosphoryl group, rather, the response regulator CheB takes the phosphoryl group from CheA. This mechanism of signal transduction is called a two-component system, and it is a common form of signal transduction in bacteria. CheY induces tumbling by interacting with the flagellar switch protein FliM, inducing a change from counter-clockwise to clockwise rotation of the flagellum. Change in the rotation state of a single flagellum can disrupt the entire flagella bundle and cause a tumble.

CheB, when activated by CheA, acts as a methylesterase, removing methyl groups from glutamate residues on the cytosolic side of the receptor; it works antagonistically with CheR, a methyltransferase, which adds methyl residues to the same glutamate residues. If the level of an attractant remains high, the level of phosphorylation of CheA (and, therefore, CheY and CheB) will remain low, the cell will swim smoothly, and the level of methylation of the MCPs will increase (because CheB-P is not present to demethylate). The MCPs no longer respond to the attractant when they are fully methylated; therefore, even though the level of attractant might remain high, the level of CheA-P (and CheB-P) increases and the cell begins to tumble. The MCPs can be demethylated by CheB-P, and, when this happens, the receptors can once again respond to attractants. The situation is the opposite with regard to repellents: fully methylated MCPs respond best to repellents, while least-methylated MCPs respond worst to repellents. This regulation allows the bacterium to 'remember' chemical concentrations from the recent past, a few seconds, and compare them to those it is currently experiencing, thus 'know' whether it is traveling up or down a gradient.

Chemoattractants and chemorepellents are inorganic or organic substances possessing chemotaxis-inducer effect in motile cells. These chemotactic ligands create chemical concentration gradients that organisms, prokaryotic and eukaryotic, move toward or away from, respectively.

Effects of chemoattractants are elicited via chemoreceptors such as methyl-accepting chemotaxis proteins (MCP). MCPs in E.coli include Tar, Tsr, Trg and Tap. Chemoattracttants to Trg include ribose and galactose with phenol as a chemorepellent. Tap and Tsr recognize dipeptides and serine as chemoattractants, respectively.

Chemoattractants or chemorepellents bind MCPs at its extracellular domain; an intracellular signaling domain relays the changes in concentration of these chemotactic ligands to downstream proteins like that of CheA which then relays this signal to flagellar motors via phosphorylated CheY (CheY-P). CheY-P can then control flagellar rotation influencing the direction of cell motility.

For "E.coli", "S. meliloti", and "R. spheroides," the binding of chemoattractants to MCPs inhibit CheA and therefore CheY-P activity, resulting in smooth runs, but for "B. substilis", CheA activity increases. Methylation events in "E.coli" cause MCPs to have lower affinity to chemoattractants which causes increased activity of CheA and CheY-P resulting in tumbles. In this way cells are able to adapt to the immediate chemoattractant concentration and detect further changes to modulate cell motility.

Chemoattractants in eukaryotes are well characterized for immune cells. Formyl peptides, such as fMLF, attract leukocytes such as neutrophils and macrophages, causing movement toward infection sites. Non-acylated methioninyl peptides do not act as chemoattractants to neutrophils and macrophages. Leukocytes also move toward chemoattractants C5a, a complement component, and pathogen-specific ligands on bacteria.

Mechanisms concerning chemorepellents are less known than chemoattractants. Although chemorepellents work to confer an avoidance response in organisms, "Tetrahymena thermophila" adapt to a chemorepellent, Netrin-1 peptide, within 10 minutes of exposure; however, exposure to chemorepellents such as GTP, PACAP-38, and nociceptin show no such adaptations. GTP and ATP are chemorepellents in micro-molar concentrations to both "Tetrahymena" and "Paramecium". These organisms avoid these molecules by producing avoiding reactions to re-orient themselves away from the gradient.

The mechanism of chemotaxis that eukaryotic cells employ is quite different from that in the bacteria "E. coli"; however, sensing of chemical gradients is still a crucial step in the process. Due to their small size and other biophysical constraints, "E. coli" cannot directly detect a concentration gradient. Instead, they employ temporal gradient sensing, where they move over larger distances several times their own width and measure the rate at which perceived chemical concentration changes.

Eukaryotic cells are much larger than prokaryotes and have receptors embedded uniformly throughout the cell membrane. Eukaryotic chemotaxis involves detecting a concentration gradient spatially by comparing the asymmetric activation of these receptors at the different ends of the cell. Activation of these receptors results in migration towards chemoattractants, or away from chemorepellants. In mating yeast, which are non-motile, patches of polarity proteins on the cell cortex can relocate in a chemotactic fashion up pheromone gradients.

It has also been shown that both prokaryotic and eukaryotic cells are capable of chemotactic memory. In prokaryotes, this mechanism involves the methylation of receptors called methyl-accepting chemotaxis proteins (MCPs). This results in their desensitization and allows prokaryotes to "remember" and adapt to a chemical gradient. In contrast, chemotactic memory in eukaryotes can be explained by the Local Excitation Global Inhibition (LEGI) model. LEGI involves the balance between a fast excitation and delayed inhibition which controls downstream signaling such as Ras activation and PIP3 production.

Levels of receptors, intracellular signalling pathways and the effector mechanisms all represent diverse, eukaryotic-type components. In eukaryotic unicellular cells, amoeboid movement and cilium or the eukaryotic flagellum are the main effectors (e.g., Amoeba or Tetrahymena). Some eukaryotic cells of higher vertebrate origin, such as immune cells also move to where they need to be. Besides immune competent cells (granulocyte, monocyte, lymphocyte) a large group of cells—considered previously to be fixed into tissues—are also motile in special physiological (e.g., mast cell, fibroblast, endothelial cells) or pathological conditions (e.g., metastases). Chemotaxis has high significance in the early phases of embryogenesis as development of germ layers is guided by gradients of signal molecules.

Unlike motility in bacterial chemotaxis, the mechanism by which eukaryotic cells physically move is unclear. There appear to be mechanisms by which an external chemotactic gradient is sensed and turned into an intracellular PIP3 gradient, which results in a gradient and the activation of a signaling pathway, culminating in the polymerisation of actin filaments. The growing distal end of actin filaments develops connections with the internal surface of the plasma membrane via different sets of peptides and results in the formation of anterior pseudopods and posterior uropods.
Cilia of eukaryotic cells can also produce chemotaxis; in this case, it is mainly a Ca-dependent induction of the microtubular system of the basal body and the beat of the 9 + 2 microtubules within cilia. The orchestrated beating of hundreds of cilia is synchronized by a submembranous system built between basal bodies.
The details of the signaling pathways are still not totally clear.

Chemotaxis refers to the directional migration of cells in response to chemical gradients; several variations of chemical-induced migration exist as listed below. 

In general, eukaryotic cells sense the presence of chemotactic stimuli through the use of 7-transmembrane (or serpentine) heterotrimeric G-protein-coupled receptors, a class representing a significant portion of the genome. Some members of this gene superfamily are used in eyesight (rhodopsins) as well as in olfaction (smelling). The main classes of chemotaxis receptors are triggered by:
However, induction of a wide set of membrane receptors (e.g., cyclic nucleotides, amino acids, insulin, vasoactive peptides) also elicit migration of the cell.

While some chemotaxis receptors are expressed in the surface membrane with long-term characteristics, as they are determined genetically, others have short-term dynamics, as they are assembled "ad hoc" in the presence of the ligand. The diverse features of the chemotaxis receptors and ligands allows for the possibility of selecting chemotactic responder cells with a simple chemotaxis assay By chemotactic selection, we can determine whether a still-uncharacterized molecule acts via the long- or the short-term receptor pathway. The term "chemotactic selection" is also used to designate a technique that separates eukaryotic or prokaryotic cells according to their chemotactic responsiveness to selector ligands.

The number of molecules capable of eliciting chemotactic responses is relatively high, and we can distinguish primary and secondary chemotactic molecules. The main groups of the primary ligands are as follows:

Chemotactic responses elicited by ligand-receptor interactions vary with the concentration of the ligand. Investigations of ligand families (e.g. amino acids or oligopeptides) demonstrates that chemoattractant activity occurs over a wide range, while chemorepellent activities have narrow ranges.

A changed migratory potential of cells has relatively high importance in the development of several clinical symptoms and syndromes.
Altered chemotactic activity of extracellular (e.g., Escherichia coli) or intracellular (e.g., Listeria monocytogenes) pathogens itself represents a significant clinical target. Modification of endogenous chemotactic ability of these microorganisms by pharmaceutical agents can decrease or inhibit the ratio of infections or spreading of infectious diseases.
Apart from infections, there are some other diseases wherein impaired chemotaxis is the primary etiological factor, as in Chédiak–Higashi syndrome, where giant intracellular vesicles inhibit normal migration of cells.

Several mathematical models of chemotaxis were developed depending on the type of

Although interactions of the factors listed above make the behavior of the solutions of mathematical models of chemotaxis rather complex, it is possible to describe the basic phenomenon of chemotaxis-driven motion in a straightforward way. Indeed, let us denote with formula_1 the spatially non-uniform concentration of the chemo-attractant and formula_2 as its gradient. Then the chemotactic cellular flow (also called current) formula_3 that is generated by the chemotaxis is linked to the above gradient by the law:where formula_4 is the spatial density of the cells and formula_5 is the so-called ’Chemotactic coefficient’ - formula_6 is often not constant, but a decreasing function of the chemo-attractant. For some quantity formula_7 that is subject to total flux formula_8 and generation/destruction term formula_9, it is possible to formulate a continuity equation:

where formula_11 is the divergence. This general equation applies to both the cell density and the chemo-attractant. Therefore, incorporating a diffusion flux into the total flux term, the interactions between these quantities are governed by a set of coupled reaction-diffusion partial differential equations describing the change in formula_12 and formula_13: &= f(C) + \nabla\cdot \left[D_{C}\nabla C - C\chi(\varphi)\nabla\varphi \right ] \\
\end{aligned} </math>|border colour=#0073CF|background colour=#F5FFFA}}where formula_14 describes the growth in cell density, formula_15 is the kinetics/source term for the chemo-attractant, and the diffusion coefficients for cell density and the chemo-attractant are respectively formula_16 and formula_17.

Spatial ecology of soil microorganisms is a function of their chemotactic sensitivities towards substrate and fellow organisms. The chemotactic behavior of the bacteria was proven to lead to non-trivial population patterns even in the absence of environmental heterogeneities. The presence of structural pore scale heterogeneities has an extra impact on the emerging bacterial patterns.

A wide range of techniques is available to evaluate chemotactic activity of cells or the chemoattractant and chemorepellent character of ligands.
The basic requirements of the measurement are as follows:

Despite the fact that an ideal chemotaxis assay is still not available, there are several protocols and pieces of equipment that offer good correspondence with the conditions described above. The most commonly used are summarised in the table below:
"Chemical robots" that use artificial chemotaxis to navigate autonomously have been designed. Applications include targeted delivery of drugs in the body. More recently, enzyme molecules have also shown positive chemotactic behavior in the gradient of their substrates. The thermodynamically-favorable binding of enzymes to their specific substrates is recognized as the origin of enzymatic chemotaxis. Additionally, enzymes in cascades have also shown substrate-driven chemotactic aggregation.

Apart from active enzymes, non-reacting molecules also show chemotactic behavior. This has been demonstrated by using dye molecules that move directionally in gradients of polymer solution through favorable hydrophobic interactions.



Cheshire

Cheshire ( ) is a ceremonial county in North West England. It is bordered by Merseyside to the north-west, Greater Manchester to the north-east, Derbyshire to the east, Staffordshire to the south-east, and Shropshire to the south; to the west it is bordered by the Welsh counties of Flintshire and Wrexham, and has a short coastline on the Dee Estuary. Warrington is the largest settlement, and the city of Chester is the county town.

The county has an area of and had a population of 1,095,500 at the 2021 census. After Warrington (212,000), the largest settlements are Chester (86,011) and Crewe (71,722). The south and east of the county are primarily rural, while the north is more densely populated and includes the settlements of Runcorn, Widnes, and Ellesmere Port. For local government purposes Cheshire comprises four unitary authority areas: Cheshire East, Cheshire West and Chester, Halton, and Warrington. The county historically included all of the Wirral Peninsula and southern Greater Manchester but excluded Widnes and Warrington, which were part of Lancashire.

The landscape of the county is dominated by the Cheshire Plain, an area of relatively flat land divided by the Mid-Cheshire Ridge. To the west, Cheshire contains the south of the Wirral Peninsula, and to the east the landscape rises to the Pennines, where the county contains part of the Peak District National Park. The River Mersey runs through the north of Cheshire before broadening into its wide estuary; the River Dee forms part of the county's border with Wales, then fully enters England and flows through the city of Chester before re-entering Wales upstream of its estuary. Red Triassic sandstone forms the bedrock of much of the county, and was used in the construction of many of its buildings.

The culture of Cheshire has impacted global pop culture by producing actors such as Daniel Craig, Tim Curry, and Pete Postlethwaite; athletes such as Shauna Coxsey, Tyson Fury, and Paula Radcliffe; authors such as Lewis Carroll; comedians such as John Bishop and Ben Miller, and musicians such as Gary Barlow, Ian Curtis, and Harry Styles. Most places are involved in agriculture and chemistry, leading to Cheshire's reputation for the production of chemicals, Cheshire cheese, salt, and silk.

Cheshire's name was originally derived from an early name for Chester, and was first recorded as "Legeceasterscir" in the "Anglo-Saxon Chronicle", meaning "the shire of the city of legions". Although the name first appears in 980, it is thought that the county was created by Edward the Elder around 920. In the Domesday Book, Chester was recorded as having the name "Cestrescir" (Chestershire), derived from the name for Chester at the time. Through the next few centuries a series of changes that occurred in the English language, which have included simplifications and elision, has resulted in the name Cheshire.

Because of the historically close links with the land bordering Cheshire to the west, which became modern Wales, there is a history of interaction between Cheshire and North Wales. The Domesday Book records Cheshire as having two complete Hundreds (Atiscross and Exestan) that later became the principal part of Flintshire. Additionally, another large portion of the Duddestan Hundred later became known as English Maelor (Maelor Saesneg) when it was transferred to North Wales. For this and other reasons, the Welsh language name for Cheshire, , is sometimes used.

After the Norman conquest of 1066 by William I, dissent and resistance continued for many years after the invasion. In 1069 local resistance in Cheshire was finally put down using draconian measures as part of the Harrying of the North. The ferocity of the campaign against the English populace was enough to end all future resistance. Examples were made of major landowners such as Earl Edwin of Mercia, their properties confiscated and redistributed amongst Norman barons.

The earldom was sufficiently independent from the kingdom of England that the 13th-century Magna Carta did not apply to the shire of Chester, so the earl wrote up his own Chester Charter at the petition of his barons.

William I made Cheshire a county palatine and gave Gerbod the Fleming the new title of Earl of Chester. When Gerbod returned to Normandy in about 1070, the king used his absence to declare the earldom forfeit and gave the title to Hugh d'Avranches (nicknamed Hugh Lupus, or "wolf"). Because of Cheshire's strategic location on the Welsh Marches, the Earl had complete autonomous powers to rule on behalf of the king in the county palatine.

Cheshire in the Domesday Book (1086) is recorded as a much larger county than it is today. It included two hundreds, Atiscross and Exestan, that later became part of North Wales. At the time of the Domesday Book, it also included as part of Duddestan Hundred the area of land later known as English Maelor (which used to be a detached part of Flintshire) in Wales. The area between the Mersey and Ribble (referred to in the Domesday Book as "Inter Ripam et Mersam") formed part of the returns for Cheshire. Although this has been interpreted to mean that at that time south Lancashire was part of Cheshire, more exhaustive research indicates that the boundary between Cheshire and what was to become Lancashire remained the River Mersey. With minor variations in spelling across sources, the complete list of hundreds of Cheshire at this time are: Atiscross, Bochelau, Chester, Dudestan, Exestan, Hamestan, Middlewich, Riseton, Roelau, Tunendune, Warmundestrou and Wilaveston.

There were 8 feudal baronies in Chester, the barons of Kinderton, Halton, Malbank, Mold, Shipbrook, Dunham-Massey, and the honour of Chester itself. Feudal baronies or baronies by tenure were granted by the Earl as forms of feudal land tenure within the palatinate in a similar way to which the king granted English feudal baronies within England proper. An example is the barony of Halton. One of Hugh d'Avranche's barons has been identified as Robert Nicholls, Baron of Halton and Montebourg.

In 1182, the land north of the Mersey became administered as part of the new county of Lancashire, resolving any uncertainty about the county in which the land "Inter Ripam et Mersam" was. Over the years, the ten hundreds consolidated and changed names to leave just seven—Broxton, Bucklow, Eddisbury, Macclesfield, Nantwich, Northwich and Wirral.

In 1397 the county had lands in the march of Wales added to its territory, and was promoted to the rank of principality. This was because of the support the men of the county had given to King Richard II, in particular by his standing armed force of about 500 men called the "Cheshire Guard". As a result, the King's title was changed to "King of England and France, Lord of Ireland, and Prince of Chester". No other English county has been honoured in this way, although it lost the distinction on Richard's fall in 1399.

Through the Local Government Act 1972, which came into effect on 1 April 1974, some areas in the north became part of the metropolitan counties of Greater Manchester and Merseyside. Stockport (previously a county borough), Altrincham, Hyde, Dukinfield and Stalybridge in the north-east became part of Greater Manchester. Much of the Wirral Peninsula in the north-west, including the county boroughs of Birkenhead and Wallasey, joined Merseyside as the Metropolitan Borough of Wirral. At the same time the Tintwistle Rural District was transferred to Derbyshire. The area of south Lancashire not included within either the Merseyside or Greater Manchester counties, including Widnes and the county borough of Warrington, was added to the new non-metropolitan county of Cheshire.

Halton and Warrington became unitary authorities independent of Cheshire County Council on 1 April 1998, but remain part of Cheshire for ceremonial purposes and also for fire and policing. Halton is part of Liverpool City Region combined authority, which also includes the five metropolitan boroughs of Merseyside.

A referendum for a further local government reform connected with an elected regional assembly was planned for 2004, but was abandoned.

As part of the local government restructuring in April 2009, Cheshire County Council and the Cheshire districts were abolished and replaced by two new unitary authorities, Cheshire East and Cheshire West and Chester. The existing unitary authorities of Halton and Warrington were not affected by the change.

Cheshire has no county-wide elected local council, but it does have a Lord Lieutenant under the Lieutenancies Act 1997 and a High Sheriff under the Sheriffs Act 1887.

Local government functions apart from the Police and Fire/Rescue services are carried out by four smaller unitary authorities: Cheshire East, Cheshire West and Chester, Halton, and Warrington. All four unitary authority areas have borough status.

Policing and fire and rescue services are still provided across the county as a whole. The Cheshire Fire Authority consist of members of the four councils, while governance of Cheshire Constabulary is performed by the elected Cheshire Police and Crime Commissioner.

Winsford is a major administrative hub for Cheshire with the Police and Fire & Rescue Headquarters based in the town as well as a majority of Cheshire West and Chester Council. It was also home to the former Vale Royal Borough Council and Cheshire County Council.

From 1 April 1974 the area under the control of the county council was divided into eight local government districts; Chester, Congleton, Crewe and Nantwich, Ellesmere Port and Neston, Halton, Macclesfield, Vale Royal and Warrington. Halton (which includes the towns of Runcorn and Widnes) and Warrington became unitary authorities in 1998. The remaining districts and the county were abolished as part of local government restructuring on 1 April 2009. The Halton and Warrington boroughs were not affected by the 2009 restructuring.

On 25 July 2007, the Secretary of State Hazel Blears announced she was 'minded' to split Cheshire into two new unitary authorities, Cheshire West and Chester, and Cheshire East. She confirmed she had not changed her mind on 19 December 2007 and therefore the proposal to split two-tier Cheshire into two would proceed. Cheshire County Council leader Paul Findlow, who attempted High Court legal action against the proposal, claimed that splitting Cheshire would only disrupt excellent services while increasing living costs for all. On 31 January 2008 "The Standard", Cheshire and district's newspaper, announced that the legal action had been dropped. Members against the proposal were advised that they may be unable to persuade the court that the decision of Hazel Blears was "manifestly absurd".

The Cheshire West and Chester unitary authority covers the area formerly occupied by the City of Chester and the boroughs of Ellesmere Port and Neston and Vale Royal; Cheshire East now covers the area formerly occupied by the boroughs of Congleton, Crewe and Nantwich, and Macclesfield. The changes were implemented on 1 April 2009.

Congleton Borough Council pursued an appeal against the judicial review it lost in October 2007. The appeal was dismissed on 4 March 2008.

A plain of glacial till and other glacio-fluvial sediments extends across much of Cheshire, separating the hills of North Wales and the Pennines. Known as the Cheshire Plain, it was formed following the retreat of a Quaternary ice sheet which left the area dotted with kettle holes, those which hold water being referred to as meres. The bedrock of this region is almost entirely Triassic sandstone, outcrops of which have long been quarried, notably at Runcorn, providing the distinctive red stone for Liverpool Cathedral and Chester Cathedral.

The eastern half of the county is Upper Triassic Mercia Mudstone laid down with large salt deposits which were mined for hundreds of years around Winsford. Separating this area from Lower Triassic Sherwood Sandstone to the west is a prominent sandstone ridge known as the Mid Cheshire Ridge. A footpath, the Sandstone Trail, follows this ridge from Frodsham to Whitchurch passing Delamere Forest, Beeston Castle and earlier Iron Age forts.

The western fringes of the Peak District - the southernmost extent of the Pennine range - form the eastern part of the county. The highest point (county top) in the historic county of Cheshire was Black Hill () near Crowden in the Cheshire Panhandle, a long eastern projection of the county which formerly stretched along the northern side of Longdendale and on the border with the West Riding of Yorkshire. Black Hill is now the highest point in the ceremonial county of West Yorkshire.

Within the current ceremonial county and the unitary authority of Cheshire East the highest point is Shining Tor on the Derbyshire/Cheshire border between Macclesfield and Buxton, at above sea level. After Shining Tor, the next highest point in Cheshire is Shutlingsloe, at above sea level. Shutlingsloe lies just to the south of Macclesfield Forest and is sometimes humorously referred to as the "Matterhorn of Cheshire" thanks to its distinctive steep profile.

Cheshire contains portions of two green belt areas surrounding the large conurbations of Merseyside and Greater Manchester (North Cheshire Green Belt, part of the North West Green Belt) and Stoke-on-Trent (South Cheshire Green Belt, part of the Stoke-on-Trent Green Belt), these were first drawn up from the 1950s. Contained primarily within Cheshire East and Chester West & Chester, with small portions along the borders of the Halton and Warrington districts, towns and cities such as Chester, Macclesfield, Alsager, Congleton, Northwich, Ellesmere Port, Knutsford, Warrington, Poynton, Disley, Neston, Wilmslow, Runcorn, and Widnes are either surrounded wholly, partially enveloped by, or on the fringes of the belts. The North Cheshire Green Belt is contiguous with the Peak District Park boundary inside Cheshire.

The ceremonial county borders Merseyside, Greater Manchester, Derbyshire, Staffordshire and Shropshire in England along with Flintshire and Wrexham in Wales, arranged by compass directions as shown in the table. below. Cheshire also forms part of the North West England region.

In July 2022, beavers bred in Cheshire for the first time in 400 years, following a reintroduction scheme.

Based on the Census of 2001, the overall population of Cheshire East and Cheshire West and Chester is 673,781, of which 51.3% of the population were male and 48.7% were female. Of those aged between 0–14 years, 51.5% were male and 48.4% were female; and of those aged over 75 years, 62.9% were female and 37.1% were male. This increased to 699,735 at the 2011 Census. The population for 2021 is forecast to be 708,000.

In 2001, the population density of Cheshire East and Cheshire West and Chester was 32 people per km, lower than the North West average of 42 people/km and the England and Wales average of 38 people/km. Ellesmere Port and Neston had a greater urban density than the rest of the county with 92 people/km.

In 2001, ethnic white groups accounted for 98% (662,794) of the population, and 10,994 (2%) in ethnic groups other than white.

Of the 2% in non-white ethnic groups:


In the 2001 Census, 81% of the population (542,413) identified themselves as Christian; 124,677 (19%) did not identify with any religion or did not answer the question; 5,665 (1%) identified themselves as belonging to other major world religions; and 1,033 belonged to other religions.

The boundary of the Church of England Diocese of Chester follows most closely the pre-1974 county boundary of Cheshire, so it includes all of Wirral, Stockport, and the Cheshire panhandle that included Tintwistle Rural District council area. In terms of Roman Catholic church administration, most of Cheshire falls into the Roman Catholic Diocese of Shrewsbury.

Cheshire has a diverse economy with significant sectors including agriculture, automotive, bio-technology, chemical, financial services, food and drink, ICT, and tourism. The county is famous for the production of Cheshire cheese, salt and silk. The county has seen a number of inventions and firsts in its history.

A mainly rural county, Cheshire has a high concentration of villages. Agriculture is generally based on the dairy trade, and cattle are the predominant livestock. Land use given to agriculture has fluctuated somewhat, and in 2005 totalled 1558 km over 4,609 holdings. Based on holdings by EC farm type in 2005, 8.51 km was allocated to dairy farming, with another 11.78 km allocated to cattle and sheep.
The chemical industry in Cheshire was founded in Roman times, with the mining of salt in Middlewich and Northwich. Salt is still mined in the area by British Salt. The salt mining has led to a continued chemical industry around Northwich, with Brunner Mond based in the town. Other chemical companies, including Ineos (formerly ICI), have plants at Runcorn. The Essar Refinery (formerly Shell Stanlow Refinery) is at Ellesmere Port. The oil refinery has operated since 1924 and has a capacity of 12 million tonnes per year.

Crewe was once the centre of the British railway industry, and remains a major railway junction. The Crewe railway works, built in 1840, employed 20,000 people at its peak, although the workforce is now less than 1,000. Crewe is also the home of Bentley cars. Also within Cheshire are manufacturing plants for Jaguar and Vauxhall Motors in Ellesmere Port.

The county also has an aircraft industry, with the BAE Systems facility at Woodford Aerodrome, part of BAE System's Military Air Solutions division. The facility designed and constructed Avro Lancaster and Avro Vulcan bombers and the Hawker-Siddeley Nimrod. On the Cheshire border with Flintshire is the Broughton aircraft factory, more recently associated with Airbus.

Tourism in Cheshire from within the UK and overseas continues to perform strongly. Over 8 million nights of accommodation (both UK and overseas) and over 2.8 million visits to Cheshire were recorded during 2003.

At the start of 2003, there were 22,020 VAT-registered enterprises in Cheshire, an increase of 7% since 1998, many in the business services (31.9%) and wholesale/retail (21.7%) sectors. Between 2002 and 2003 the number of businesses grew in four sectors: public administration and other services (6.0%), hotels and restaurants (5.1%), construction (1.7%), and business services (1.0%). The county saw the largest proportional reduction between 2001 and 2002 in employment in the energy and water sector and there was also a significant reduction in the manufacturing sector. The largest growth during this period was in the other services and distribution, hotels and retail sectors.

Cheshire is considered to be an affluent county. However, towns such as Crewe and Winsford have significant deprivation. The county's proximity to the cities of Manchester and Liverpool means counter urbanisation is common. Cheshire West has a fairly large proportion of residents who work in Liverpool and Manchester, while the town of Northwich and area of Cheshire East falls more within Manchester's sphere of influence.

All four local education authorities in Cheshire operate only comprehensive state school systems. When Altrincham, Sale and Bebington were moved from Cheshire to Trafford and Merseyside in 1974, they took some former Cheshire selective schools. There are two universities based in the county, the University of Chester and the Chester campus of The University of Law. The Crewe campus of Manchester Metropolitan University was scheduled to close in 2019.

Cheshire has produced musicians such as Joy Division members Ian Curtis and Stephen Morris, One Direction member Harry Styles, the members of The 1975, Take That member Gary Barlow, The Cult member Ian Astbury, Catfish and the Bottlemen member Van McCann, Girls Aloud member Nicola Roberts, Stephen Hough, John Mayall, The Charlatans member Tim Burgess, and Nigel Stonier.

Actors from Cheshire include Russ Abbot, Warren Brown, Julia Chan, Ray Coulthard, Daniel Craig, Tim Curry, Wendy Hiller, Tom Hughes, Tim McInnerny, Ben Miller, Pete Postlethwaite, Adam Rickitt, John Steiner, and Ann Todd. The most famous author from the county is Lewis Carroll, who wrote "Alice's Adventures in Wonderland" and named the Cheshire Cat character after it. Other notable Cheshire writers include Hall Caine, Alan Garner, and Elizabeth Gaskell. Artists from Cheshire include ceramic artist Emma Bossons and sculptor/photographer Andy Goldsworthy.

Local news and television programmes are provided by BBC North West and ITV Granada. Television signals are received from the Winter Hill TV transmitter.

Local radio stations in the county include Chester's Dee Radio, Capital North West and Wales, Smooth Wales, Cheshire's Silk 106.9 and Signal 1. It is one of only four counties in the country (along with County Durham, Dorset, and Rutland) that does not have its own designated BBC radio station; the south and parts of the east are covered by BBC Radio Stoke, while BBC Radio Merseyside tends to cover the west, and BBC Radio Manchester covers the north and parts of the east. The BBC directs readers to Stoke and Staffordshire when Cheshire is selected on their website. There were plans to launch BBC Radio Cheshire, but those were shelved in 2007 after the BBC license fee settlement was lower than expected.

Athletes native to Cheshire include sailor Ben Ainslie, cricketer Ian Botham, rock climber Shauna Coxsey, boxer Tyson Fury, oarsman Matt Langridge, mountaineer George Mallory, marathon runner Paula Radcliffe, cyclist Sarah Storey, and hurdler Shirley Strong. It has also been home to numerous athletes from outside the county. Many Premier League footballers have relocated there over the years upon joining nearby teams such as Manchester United FC, Manchester City FC, Everton FC, and Liverpool FC. These include Dean Ashton, Seth Johnson, Jesse Lingard, Michael Owen, and Wayne Rooney. The "Cheshire Golden Triangle" is the collective name for a group of adjacent Cheshire villages where the amount of footballers, actors, and entrepreneurs moving in over the years led to the average house prices becoming some of the most expensive in the UK.

Cheshire has one Football League team, Crewe Alexandra, which plays in League One. Chester F.C., a phoenix club formed in 2010 after ex-Football League club Chester City was dissolved, competes in the National League North. Northwich Victoria, another ex-League team which was a founding member of the Football League Division Two in 1892/1893, now represents Cheshire in the Northern Premier League along with Nantwich Town, Warrington Town, and Witton Albion. Macclesfield Town another former League club, went into liquidation in 2020; a phoenix club, Macclesfield F.C., was formed in 2021.

The Warrington Wolves and Widnes Vikings are the premier rugby league teams in Cheshire; the former plays in the Super League, while the latter plays in the Championship. There are also numerous junior clubs in the county, including Chester Gladiators. Cheshire County Cricket Club is one of the clubs that make up the minor counties of English and Welsh cricket. Cheshire also is represented in the highest level basketball league in the UK, the BBL, by Cheshire Phoenix (formerly Cheshire Jets). Europe's largest motorcycle event, the Thundersprint, is held in Northwich every May.

The Royal Cheshire Show, an annual agricultural show, has taken place since the 1800s.

Cheshire also produced a military hero in Norman Cyril Jones, a World War I flying ace who won the Distinguished Flying Cross.

As part of a 2002 marketing campaign, the plant conservation charity Plantlife chose the cuckooflower as the county flower. Previously, a sheaf of golden wheat was the county emblem, a reference to the Earl of Chester's arms in use from the 12th century.

Prehistoric burial grounds have been discovered at The Bridestones near Congleton (Neolithic) and Robin Hood's Tump near Alpraham (Bronze Age). The remains of Iron Age hill forts are found on sandstone ridges at several locations in Cheshire. Examples include Maiden Castle on Bickerton Hill, Helsby Hillfort and Woodhouse Hillfort at Frodsham. The Roman fortress and walls of Chester, perhaps the earliest building works in Cheshire remaining above ground, are constructed from purple-grey sandstone.

The distinctive local red sandstone has been used for many monumental and ecclesiastical buildings throughout the county: for example, the medieval Beeston Castle, Chester Cathedral and numerous parish churches. Occasional residential and industrial buildings, such as Helsby railway station (1849), are also in this sandstone.

Many surviving buildings from the 15th to 17th centuries are timbered, particularly in the southern part of the county. Notable examples include the moated manor house Little Moreton Hall, dating from around 1450, and many commercial and residential buildings in Chester, Nantwich and surrounding villages.

Early brick buildings include Peover Hall near Macclesfield (1585), Tattenhall Hall (pre-1622), and the Pied Bull Hotel in Chester (17th-century). From the 18th century, orange, red or brown brick became the predominant building material used in Cheshire, although earlier buildings are often faced or dressed with stone. Examples from the Victorian period onwards often employ distinctive brick detailing, such as brick patterning and ornate chimney stacks and gables. Notable examples include Arley Hall near Northwich, Willington Hall near Chester (both by Nantwich architect George Latham) and Overleigh Lodge, Chester. From the Victorian era, brick buildings often incorporate timberwork in a mock Tudor style, and this hybrid style has been used in some modern residential developments in the county. Industrial buildings, such as the Macclesfield silk mills (for example, Waters Green New Mill), are also usually in brick.

The county is home to some of the most affluent areas of northern England, including Alderley Edge, Wilmslow, Prestbury, Tarporley and Knutsford, named in 2006 as the most expensive place to buy a house in the north of England. The former Cheshire town of Altrincham was in second place. The area is sometimes referred to as The Golden Triangle on account of the area in and around the aforementioned towns and villages.

The cities and towns in Cheshire are:

Some settlements which were historically part of the county now fall under the counties of Derbyshire, Merseyside and Greater Manchester:

The main railway line through the county is the West Coast Main Line. Trains on the main London to Scotland line call at Crewe (in the south of the county) and Warrington Bank Quay (in the north of the county). Trains stop at Crewe and Runcorn on the Liverpool branch of the WCML; Crewe and Macclesfield are each hourly stops on the two Manchester branches. The major interchanges are:

In the east of Cheshire, Macclesfield station is served by Avanti West Coast, CrossCountry and Northern, on the Manchester–London line. Services from Manchester to the south coast frequently stop at Macclesfield. Neston on the Wirral Peninsula is served by a railway station on the Borderlands line between Bidston and Wrexham.

Cheshire has of roads, including of the M6, M62, M53 and M56 motorways; there are 23 interchanges and four service areas. It also has the A580 "East Lancashire Road" at its border with Greater Manchester at Leigh. The M6 motorway at the Thelwall Viaduct carries 140,000 vehicles every 24 hours.

Bus transport in Cheshire is provided by various operators. The major bus operator in the Cheshire area is D&G Bus. Other operators in Cheshire include Stagecoach Chester & Wirral and Network Warrington.

There are also several operators based outside of Cheshire, who either run services wholly within the area or services which start from outside the area. Companies include Arriva Buses Wales, Aimee's Travel, High Peak, First Greater Manchester, D&G bus and Stagecoach Manchester.

Some services are run under contract to Cheshire West and Chester, Cheshire East, Borough of Halton and Warrington Councils.

The Cheshire canal system includes several canals originally used to transport the county's industrial products (mostly chemicals). Nowadays they are mainly used for tourist traffic. The Cheshire Ring is formed from the Rochdale, Ashton, Peak Forest, Macclesfield, Trent and Mersey and Bridgewater canals.

The Manchester Ship Canal is a wide, stretch of water opened in 1894. It consists of the rivers Irwell and Mersey made navigable to Manchester for seagoing ships leaving the Mersey estuary. The canal passes through the north of the county via Runcorn and Warrington. Rivers and canals in the county are:



County town

In Great Britain and Ireland, a county town is the most important town or city in a county. It is usually the location of administrative or judicial functions within a county, and the place where public representatives are elected to parliament. Following the establishment of county councils in England 1889, the headquarters of the new councils were usually established in the county town of each county; however, the concept of a county town pre-dates these councils.

The concept of a county town is ill-defined and unofficial. Some counties in Great Britain have their administrative bodies housed elsewhere. For example, Lancaster is the county town of Lancashire, but the county council is in Preston. Some county towns in Great Britain are no longer within the administrative county because of changes in the county's boundaries. For example, Nottingham is administered by a unitary authority separately from the rest of Nottinghamshire.

This list shows towns or cities which held county functions at various points in time.

Following the Norman invasion of Wales, the Cambro-Normans created the historic shire system (also known as ancient counties). Many of these counties were named for the centre of Norman power within the new county (Caernarfonshire named for Caernarfon, Monmouthshire named for Monmouth) others were named after the previous medieval Welsh kingdoms (Ceredigon becomes Cardigan, Morgannwg becomes Glamorgan). The 1535 Laws in Wales Act established the historic counties in English law, but in Wales they were later replaced with eight preserved counties for ceremonial purposes and the twenty two principal areas are used for administrative purposes. Neither of these subdivisions use official county towns, although their administrative headquarters and ceremonial centres are often located in the historic county town.

With the creation of elected county councils in 1889, the administrative headquarters in some cases moved away from the traditional county town. Furthermore, in 1965 and 1974 there were major boundary changes in England and Wales and administrative counties were replaced with new metropolitan and non-metropolitan counties. The boundaries underwent further alterations between 1995 and 1998 to create unitary authorities, and some of the ancient counties and county towns were restored. (Note: not all headquarters are or were called County Halls or Shire Halls e.g.: Cumbria County Council's HQ up until 2016 was called "The Courts" and has since moved to Cumbria House.) Before 1974, many of the county halls were in towns and cities that had the status of a county borough i.e. a borough outside the county council's jurisdiction.


The follow lists the location of the administration of each of the 31 local authorities in the Republic of Ireland, with 26 of the traditional counties.
Note – Despite the fact that Belfast is the capital of Northern Ireland, it is not the county town of any county. Greater Belfast straddles two counties – Antrim and Down.

Jamaica's three counties were established in 1758 to facilitate the holding of courts along the lines of the British county court system, with each county having a county town. The counties have no current administrative relevance.


Constitution of Canada

The Constitution of Canada () is the supreme law in Canada. It outlines Canada's system of government and the civil and human rights of those who are citizens of Canada and non-citizens in Canada. Its contents are an amalgamation of various codified acts, treaties between the Crown and Indigenous Peoples (both historical and modern), uncodified traditions and conventions. Canada is one of the oldest constitutional monarchies in the world.

According to subsection 52(2) of the "Constitution Act, 1982", the Canadian Constitution consists of the "Canada Act 1982" (which includes the "Constitution Act, 1982"), acts and orders referred to in its schedule (including in particular the "Constitution Act, 1867", formerly the "British North America Act, 1867"), and any amendments to these documents. The Supreme Court of Canada has held that the list is not exhaustive and also includes a number of pre-confederation acts and unwritten components as well. See list of Canadian constitutional documents for details.

The first semblance of a constitution for Canada was the Royal Proclamation of 1763. The act renamed the northeasterly portion of the former French province of New France as Province of Quebec, roughly coextensive with the southern third of contemporary Quebec. The proclamation, which established an appointed colonial government, was the constitution of Quebec until 1774 when the British parliament passed the Quebec Act, which expanded the province's boundaries to the Ohio and Mississippi Rivers (one of the grievances listed in the United States Declaration of Independence). Significantly, the Quebec Act also replaced French criminal law with the English common law system; but the French civil law system was retained for non-criminal matters.
The Treaty of Paris of 1783 ended the American War of Independence and sent a wave of British loyalist refugees northward to Quebec and Nova Scotia. In 1784, the two provinces were divided: Nova Scotia was split into Nova Scotia, Cape Breton Island (rejoined to Nova Scotia in 1820), Prince Edward Island, and New Brunswick, while Quebec was split into Lower Canada (southern Quebec) and Upper Canada (southern through lower northern Ontario). The winter of 1837–38 saw rebellion in both Canadas, contributing to their re-union as the Province of Canada in 1841.

The "British North America Act, 1867" established the Dominion of Canada as a federation of provinces. Initially, on 1 July 1867, four provinces entered into confederation as "One dominion under the name of Canada": Canada West (former Upper Canada, now Ontario), Canada East (former Lower Canada, now Quebec), Nova Scotia, and New Brunswick. Title to the Northwest Territories was transferred by the Hudson's Bay Company in 1870, out of which the province of Manitoba (the first to be established by the Parliament of Canada) was created. British Columbia joined Confederation in 1871, followed by Prince Edward Island in 1873. The Yukon Territory was created by Parliament in 1898, followed by Alberta and Saskatchewan in 1905 (all out of parts of the Northwest Territories). Newfoundland, Britain's oldest colony in the Americas and by then also a Dominion, joined Confederation in 1949. Nunavut was created in 1999 from the Northwest Territories.

An Imperial Conference in 1926 that included the leaders of all Dominions and representatives from India (which then included Burma, Bangladesh, and Pakistan), led to the eventual enactment of the Statute of Westminster 1931. The statute, an essential transitory step from the British Empire to the Commonwealth of Nations, provided that existing Dominions became fully sovereign of the United Kingdom and any new Dominions would be fully sovereign upon the grant of Dominion status. Although listed, Newfoundland never ratified the statute so was still subject to imperial authority when its entire system of government and economy collapsed in the mid-1930s. Canada did ratify the statute but with a requested exception—the Canadian federal and provincial governments could not agree on an amending formula for the Canadian constitution. It would be another 50 years before this was achieved. In the interim, the British parliament periodically passed constitutional amendments when requested by the government of Canada. This was never anything but a rubber stamp.

The patriation of the Canadian constitution was achieved in 1982 when the British parliament, with the request and assent of the Canadian parliament, passed the "Canada Act 1982", which included in its schedules the "Constitution Act, 1982". The United Kingdom thus renounced any remaining responsibility for, or jurisdiction over, Canada. In a formal ceremony on Parliament Hill in Ottawa, Queen Elizabeth II proclaimed the "Constitution Act, 1982" into law on 17 April 1982.

The "Constitution Act, 1982", includes the "Canadian Charter of Rights and Freedoms". Before the Charter, various statutes protected an assortment of civil rights and obligations but nothing was enshrined in the constitution until 1982. The Charter has thus placed a strong focus upon individual and collective rights of the people of Canada. The enactment of the Charter of Rights and Freedoms has fundamentally changed much of Canadian constitutional law. The act also codified many previously oral constitutional conventions and made amendment of the constitution in general significantly more difficult. Previously, the Canadian constitution could be formally amended by an act of the British parliament, or by informal agreement between the federal and provincial governments, or even simply by adoption as the custom of an oral convention or performance that shows precedential but unwritten tradition. Since the act, textual amendments must now conform to certain specified provisions in the written portion of the Canadian constitution.

This was an Act of the British parliament, originally called the "British North America Act, 1867". It outlined Canada's system of government, which combines Britain's Westminster model of parliamentary government with the division of sovereignty (federalism). Although it is the first of 20 "British North America Acts", it is the most famous as the primary document of Canadian Confederation. With the patriation of the Constitution in 1982, this Act was renamed "Constitution Act, 1867". In recent years, the 1867 document has mainly served as the basis on which the division of powers between the provinces and the federal government is analyzed.

Endorsed by all provincial governments except that of Quebec, this was the formal Act of Parliament that effected Canada's full legislative independence from the United Kingdom. Part V of this act established an amending formula for the Canadian constitution, the lack of which (due to more than 50 years of disagreement between the federal and provincial governments) meant Canada's constitutional amendments still required enactment by the British parliament after Statute of Westminster in 1931.

The "Constitution Act, 1982" was enacted as a schedule to the "Canada Act 1982", a British Act of Parliament which was introduced at the request of a joint address to Queen Elizabeth II by the Senate and House of Commons of Canada. The version of the "Canada Act 1982" which is in force in Britain is in English only, but the version of the act in force in Canada is bilingual, English and French. In addition to enacting the "Constitution Act, 1982", the "Canada Act 1982" provides that no further British acts of Parliament will apply to Canada as part of its law, finalizing Canada's legislative independence.

As noted above, this is Part I of the "Constitution Act, 1982". The Charter is the constitutional guarantee of the civil rights and liberties of every citizen in Canada, such as freedom of expression, of religion, and of mobility. Part II addresses the rights of Aboriginal peoples in Canada.

It is written in plain language to ensure accessibility to the average citizen. It applies only to government and government actions to prevent the government from creating unconstitutional laws.

Instead of the usual parliamentary procedure, which includes the monarch's formal royal assent for enacting legislation, amendments to any of the acts that collectively form the constitution must be done in accordance with Part V of the "Constitution Act, 1982", which provides for five different amending formulae. Amendments can be brought forward under section 46(1) by any province or the federal legislature. The general formula set out in section 38(1), known as the "7/50 formula", requires: (a) assent from both the House of Commons and the Senate; (b) the approval of two-thirds of the provincial legislatures (at least seven provinces) representing at least 50 per cent of the population of the provinces (effectively, this would include at least Quebec or Ontario, as they are the most populous provinces). This formula specifically applies to amendments related to the proportionate representation in Parliament, powers, selection, and composition of the Senate, the Supreme Court and the addition of provinces or territories.

The other amendment formulae are for particular cases as provided by the act. An amendment related to the Office of the King, the use of either official language (subject to section 43), the amending formula itself, or the composition of the Supreme Court, must be adopted by unanimous consent of all the provinces in accordance with section 41. In the case of an amendment related to provincial boundaries or the use of an official language within a province alone, the amendment must be passed by the legislatures affected by the amendment (section 43). In the case of an amendment that affects the federal government only, the amendment does not need the approval of the provinces (section 44). The same applies to amendments affecting the provincial government alone (section 45).

Canada's constitution has roots going back to the thirteenth century, including England's Magna Carta and the first English Parliament of 1275. Canada's constitution is composed of several individual statutes. There are three general methods by which a statute becomes entrenched in the Constitution:

The existence of unwritten constitutional components was reaffirmed in 1998 by the Supreme Court in "Reference re Secession of Quebec".
The Constitution is more than a written text. It embraces the entire global system of rules and principles which govern the exercise of constitutional authority. A superficial reading of selected provisions of the written constitutional enactment, without more, may be misleading.

In practice, there have been three sources of unwritten constitutional law:


Unlike in most federations, Canadian provinces do not have written provincial constitutions. Provincial constitutions are instead a combination of uncodified constitution, provisions of the Constitution of Canada, and provincial statutes.

Overall structures of provincial governments (like the legislature and cabinet) are described in parts of the Constitution of Canada. Governmental structure of the original four provinces are described in Part V of the "Constitution Act, 1867". The three colonies that joined Canada after Confederation had existing UK legislation which described their governmental structure, and this was affirmed in each colony's "Terms of Union", which now form part of Canada's Constitution. The remaining three provinces were created by federal statute. Their constitutional structures are described in those statutes, which now form part of Canada's Constitution.

All provinces have enacted legislation that establishes other rules for the structure of government. For example, every province (and territory) has an act governing elections to the legislature, and another governing procedure in the legislature. Two provinces have explicitly listed such acts as being part of their provincial constitution; see "Constitution of Quebec" and "Constitution Act" (British Columbia). However, these acts do not, generally, supersede other legislation and do not require special procedures to amend, and so they function as regular statutes rather than constitutional statutes.

A small number of non-constitutional provincial laws do supersede all other provincial legislation, as a constitution would. This is referred to as quasi-constitutionality. Quasi-constitutionality is often applied to human rights laws, allowing those laws to act as a "de facto" constitutional charter of rights. For example, laws preventing discrimination in employment, housing, and services have clauses making them quasi-constitutional in ten of thirteen jurisdictions.

Section 45 of the "Constitution Act, 1982" allows each province to amend its own constitution. This applies, for example, to provincial statute laws like "Constitution of Quebec" and "Constitution Act (British Columbia)". However, if the desired change would require an amendment to any documents that form part of the Constitution of Canada, it would require the consent of the Senate and House of Commons under section 43. This was done, for example, by the "Constitution Amendment, 1998", when Newfoundland asked the federal government to amend the "Terms of Union of Newfoundland" to allow it to end denominational quotas for religion classes.

A small number of statutes within provincial constitutions cannot be amended by a simple majority of the legislative assembly, despite section 45. For example, section 7 of the "Constitution of Alberta Amendment Act, 1990" requires plebiscites of Metis settlement members before that Act can be amended. Courts have not yet ruled about whether this kind of language really would bind future legislatures, but it might do so if the higher bar was met when creating the law.

Three amendments to provincial constitutions in the 2020s have been controversially framed as amendments to the "Constitution Act 1867". These are Quebec statutes purporting to add sections 90Q and 128Q and a Saskatchewan statute purporting to add section 90S. Because the Senate and House of Commons did not authorise these amendments, they would only have effect if they are amendments to provincial constitutions under the section 45 amending procedure. Constitutional scholars are divided on the validity of an amendment to a provincial constitution framed as an addition to part of the Constitution of Canada.

In 1983, Peter Greyson, an art student, entered Ottawa's National Archives (known today as Library and Archives Canada) and poured red paint mixed with glue over a copy of the proclamation of the 1982 constitutional amendment. He said he was displeased with the federal government's decision to allow United States missile testing in Canada and had wanted to "graphically illustrate to Canadians" how wrong he believed the government to be. Greyson was charged with public mischief and sentenced to 89 days in jail, 100 hours of community work, and two years of probation. A grapefruit-sized stain remains on the original document; restoration specialists opted to leave most of the paint intact, fearing that removal attempts would only cause further damage.



Crochet

Crochet (; ) is a process of creating textiles by using a crochet hook to interlock loops of yarn, thread, or strands of other materials. The name is derived from the French term "croc", meaning 'hook'. Hooks can be made from a variety of materials, such as metal, wood, bamboo, bone or plastic. The key difference between crochet and knitting, beyond the implements used for their production, is that each stitch in crochet is completed before the next one is begun, while knitting keeps many stitches open at a time. Some variant forms of crochet, such as Tunisian crochet and broomstick lace, do keep multiple crochet stitches open at a time.

The word crochet is derived from the French "crochet", a diminutive of "croche", in turn from the Germanic "croc", both meaning "hook". It was used in 17th-century French lace-making, where the term "crochetage" designated a stitch used to join separate pieces of lace. The word "crochet" subsequently came to describe both the specific type of textile, and the hooked needle used to produce it.

In 1567, the tailor of Mary, Queen of Scots, Jehan de Compiegne, supplied her with silk thread for sewing and crochet, ""soye à coudre et crochetz"".

Knitted textiles survive from as early as the 11th century CE, but the first substantive evidence of crocheted fabric emerges in Europe during the 19th century. Earlier work identified as crochet was commonly made by nålebinding, a different looped yarn technique.

The first known published instructions for crochet explicitly using that term to describe the craft in its present sense appeared in the Dutch magazine "Penélopé" in 1823. This includes a colour plate showing five styles of purse, of which three were intended to be crocheted with silk thread. The first is "simple open crochet" ("crochet simple ajour"), a mesh of chain-stitch arches. The second (illustrated here) starts in a semi-open form ("demi jour"), where chain-stitch arches alternate with equally long segments of slip-stitch crochet, and closes with a star made with "double-crochet stitches" ("dubbelde hekelsteek": double-crochet in British terminology; single-crochet in US). The third purse is made entirely in double-crochet. The instructions prescribe the use of a tambour needle (as illustrated below) and introduce a number of decorative techniques.

The earliest dated reference in English to garments made of cloth produced by looping yarn with a hook—"shepherd's knitting"—is in "The Memoirs of a Highland Lady" by Elizabeth Grant (1797–1830). The journal entry, itself, is dated 1812 but was not recorded in its subsequently published form until some time between 1845 and 1867, and the actual date of publication was first in 1898. Nonetheless, the 1833 volume of "Penélopé" describes and illustrates a shepherd's hook, and recommends its use for crochet with coarser yarn.

In 1844, one of the numerous books discussing crochet that began to appear in the 1840s states:

Two years later, the same author writes:

An instruction book from 1846 describes "Shepherd or single crochet" as what in current British usage is either called single crochet or slip-stitch crochet, with U.S. American terminology always using the latter (reserving single crochet for use as noted above). It similarly equates "Double" and "French crochet".
Notwithstanding the categorical assertion of a purely British origin, there is solid evidence of a connection between French tambour embroidery and crochet. French tambour embroidery was illustrated in detail in 1763 in Diderot's Encyclopedia. The tip of the needle shown there is indistinguishable from that of a present-day inline crochet hook and the chain stitch separated from a cloth support is a fundamental element of the latter technique. The 1823 "Penélopé" instructions unequivocally state that the tambour tool was used for crochet and the first of the 1840s instruction books uses the terms "tambour" and "crochet" as synonyms. This equivalence is retained in the 4th edition of that work, 1847.
The strong taper of the shepherd's hook eases the production of slip-stitch crochet but is less amenable to stitches that require multiple loops on the hook at the same time. Early yarn hooks were also continuously tapered but gradually enough to accommodate multiple loops. The design with a cylindrical shaft that is commonplace today was largely reserved for tambour-style steel needles. Both types gradually merged into the modern form that appeared toward the end of the 19th century, including both tapered and cylindrical segments, and the continuously tapered bone hook remained in industrial production until World War II.

The early instruction books make frequent reference to the alternative use of 'ivory, bone, or wooden hooks' and 'steel needles in a handle', as appropriate to the stitch being made. Taken with the synonymous labeling of shepherd's- and single crochet, and the similar equivalence of French- and double crochet, there is a strong suggestion that crochet is rooted both in tambour embroidery and shepherd's knitting, leading to thread and yarn crochet respectively; a distinction that is still made. The locus of the fusion of all these elements—the "invention" noted above—has yet to be determined, as does the origin of shepherd's knitting.

Shepherd's hooks are still being made for local slip-stitch crochet traditions. The form in the accompanying photograph is typical for contemporary production. A longer continuously tapering design intermediate between it and the 19th-century tapered hook was also in earlier production, commonly being made from the handles of forks and spoons.

In the 19th century, as Ireland was facing the Great Irish Famine (1845–1849), crochet lace work was introduced as a form of famine relief (the production of crocheted lace being an alternative way of making money for impoverished Irish workers). Men, women, children joined a co-operative in order to crochet and produce products to help with famine relief during the Great Irish Famine. Schools to teach crocheting were started. Teachers were trained and sent across Ireland to teach this craft. When the Irish immigrated to the Americas, they were able to take with them crocheting. Mademoiselle Riego de la Branchardiere is generally credited with the invention of Irish Crochet, publishing the first book of patterns in 1846. Irish lace became popular in Europe and America, and was made in quantity until the first World War.

Fashions in crochet changed with the end of the Victorian era in the 1890s. Crocheted laces in the new Edwardian era, peaking between 1910 and 1920, became even more elaborate in texture and complicated stitching.
The strong Victorian colours disappeared, though, and new publications called for white or pale threads, except for fancy purses, which were often crocheted of brightly colored silk and elaborately beaded. After World War I, far fewer crochet patterns were published, and most of them were simplified versions of the early 20th-century patterns. After World War II, from the late 1940s until the early 1960s, there was a resurgence in interest in home crafts, particularly in the United States, with many new and imaginative crochet designs published for colorful doilies, potholders, and other home items, along with updates of earlier publications. These patterns called for thicker threads and yarns than in earlier patterns and included variegated colors. The craft remained primarily a homemaker's art until the late 1960s and early 1970s, when the new generation picked up on crochet and popularized granny squares, a motif worked in the round and incorporating bright colors.
Although crochet underwent a subsequent decline in popularity, the early 21st century has seen a revival of interest in handcrafts and DIY, as well as improvement of the quality and varieties of yarn. As well as books and classes, there are YouTube tutorials and TikTok videos to help people who may need a clearer explanation to learn how to crochet.

Filet crochet, Tunisian crochet, tapestry crochet, broomstick lace, hairpin lace, cro-hooking, and Irish crochet are all variants of the basic crochet method.

Crochet has experienced a revival on the catwalk as well. Christopher Kane's Fall 2011 Ready-to-Wear collection makes intensive use of the granny square, one of the most basic of crochet motifs. In addition, crochet has been utilized many times by designers on the reality show "Project Runway". Websites such as Etsy and Ravelry have made it easier for individual hobbyists to sell and distribute their patterns or projects across the internet.

Basic materials required for crochet are a hook and some type of material that will be crocheted, most commonly yarn or thread. Yarn, one of the most commonly used materials for crocheting, has varying weights which need to be taken into consideration when following patterns. Acrylic can also be used when crocheting, as it is synthetic and an alternative for wool. Additional tools are convenient for making related accessories. Examples of such tools include cardboard cutouts, which can be used to make tassels, fringe, and many other items; a pom-pom circle, used to make pom-poms; a tape measure and a gauge measure, both used for measuring crocheted work and counting stitches; a row counter; and occasionally plastic rings, which are used for special projects.
In recent years, yarn selections have moved beyond synthetic and plant and animal-based fibers to include bamboo, qiviut, hemp, and banana stalks, to name a few. Many advanced crocheters have also incorporated recycled materials into their work in an effort to "go green" and experiment with new textures by using items such as plastic bags, old t-shirts or sheets, VCR or Cassette tape, and ribbon.

The crochet hook comes in many sizes and materials, such as bone, bamboo, aluminium, plastic, and steel. Because sizing is categorized by the diameter of the hook's shaft, a crafter aims to create stitches of a certain size in order to reach a particular gauge specified in a given pattern. If gauge is not reached with one hook, another is used until the stitches made are the needed size. Crafters may have a preference for one type of hook material over another due to aesthetic appeal, yarn glide, or hand disorders such as arthritis, where bamboo or wood hooks are favored over metal for the perceived warmth and flexibility during use. Hook grips and ergonomic hook handles are also available to assist crafters.

Steel crochet hooks range in size from 0.4 to 3.5 millimeters, or from 00 to 16 in American sizing. These hooks are used for fine crochet work such as doilies and lace.

Aluminium, bamboo, and plastic crochet hooks are available from 2.5 to 19 millimeters in size, or from B to S in American sizing.

Artisan-made hooks are often made of hand-turned woods, sometimes decorated with semi-precious stones or beads.

Crochet hooks used for Tunisian crochet are elongated and have a stopper at the end of the handle, while double-ended crochet hooks have a hook on both ends of the handle. There is also a double hooked apparatus called a Cro-hook that has become popular.
While this is not in itself a hook, it is a device used in conjunction with a crochet hook to produce stitches.

Yarn for crochet is usually sold as balls, or skeins (hanks), although it may also be wound on spools or cones. Skeins and balls are generally sold with a "yarn band", a label that describes the yarn's weight, length, dye lot, fiber content, washing instructions, suggested needle size, likely gauge, etc. It is a common practice to save the yarn band for future reference, especially if additional skeins must be purchased. Crocheters generally ensure that the yarn for a project comes from a single dye lot. The dye lot specifies a group of skeins that were dyed together and thus have precisely the same color; skeins from different dye lots, even if very similar in color, are usually slightly different and may produce a visible stripe when added onto existing work. If insufficient yarn of a single dye lot is bought to complete a project, additional skeins of the same dye lot can sometimes be obtained from other yarn stores or online.

The thickness or weight of the yarn is a significant factor in determining how many stitches and rows are required to cover a given area for a given stitch pattern. This is also termed the gauge. Thicker yarns generally require large-diameter crochet hooks, whereas thinner yarns may be crocheted with thick or thin hooks. Hence, thicker yarns generally require fewer stitches, and therefore less time, to work up a given project. The recommended gauge for a given ball of yarn can be found on the label that surrounds the skein when buying in stores. Patterns and motifs are coarser with thicker yarns and produce bold visual effects, whereas thinner yarns are best for refined or delicate pattern-work. Yarns are standardly grouped by thickness into six categories: superfine, fine, light, medium, bulky and superbulky. Quantitatively, thickness is measured by the number of wraps per inch (WPI). The related "weight per unit length" is usually measured in tex or denier.

Before use, hanks are wound into balls in which the yarn emerges from the center, making crocheting easier by preventing the yarn from becoming easily tangled. The winding process may be performed by hand or done with a ball winder and swift.

A yarn's usefulness is judged by several factors, such as its "loft" (its ability to trap air), its "resilience" (elasticity under tension), its washability and colorfastness, its "hand" (its feel, particularly softness vs. scratchiness), its durability against abrasion, its resistance to pilling, its "hairiness" (fuzziness), its tendency to twist or untwist, its overall weight and drape, its blocking and felting qualities, its comfort (breathability, moisture absorption, wicking properties) and its appearance, which includes its color, sheen, smoothness and ornamental features. Other factors include allergenicity, speed of drying, resistance to chemicals, moths, and mildew, melting point and flammability, retention of static electricity, and the propensity to accept dyes. Desirable properties may vary for different projects, so there is no one "best" yarn.
Although crochet may be done with ribbons, metal wire or more exotic filaments, most yarns are made by spinning fibers. In spinning, the fibers are twisted so that the yarn resists breaking under tension; the twisting may be done in either direction, resulting in a Z-twist or S-twist yarn. If the fibers are first aligned by combing them and the spinner uses a worsted type drafting method such as the short forward draw, the yarn is smoother and called a "worsted"; by contrast, if the fibers are carded but not combed and the spinner uses a woolen drafting method such as the long backward draw, the yarn is fuzzier and called "woolen-spun". The fibers making up a yarn may be continuous "filament" fibers such as silk and many synthetics, or they may be "staples" (fibers of an average length, typically a few inches); naturally filament fibers are sometimes cut up into staples before spinning. The strength of the spun yarn against breaking is determined by the amount of twist, the length of the fibers and the thickness of the yarn. In general, yarns become stronger with more twist (also called "worst"), longer fibers and thicker yarns (more fibers); for example, thinner yarns require more twist than do thicker yarns to resist breaking under tension. The thickness of the yarn may vary along its length; a "slub" is a much thicker section in which a mass of fibers is incorporated into the yarn.

The spun fibers are generally divided into animal fibers, plant and synthetic fibers. These fiber types are chemically different, corresponding to proteins, carbohydrates and synthetic polymers, respectively. Animal fibers include silk, but generally are long hairs of animals such as sheep (wool), goat (angora, or cashmere goat), rabbit (angora), llama, alpaca, dog, cat, camel, yak, and muskox (qiviut). Plants used for fibers include cotton, flax (for linen), bamboo, ramie, hemp, jute, nettle, raffia, yucca, coconut husk, banana trees, soy and corn. Rayon and acetate fibers are also produced from cellulose mainly derived from trees. Common synthetic fibers include acrylics, polyesters such as dacron and ingeo, nylon and other polyamides, and olefins such as polypropylene. Of these types, wool is generally favored for crochet, chiefly owing to its superior elasticity, warmth and (sometimes) felting; however, wool is generally less convenient to clean and some people are allergic to it. It is also common to blend different fibers in the yarn, e.g., 85% alpaca and 15% silk. Even within a type of fiber, there can be great variety in the length and thickness of the fibers; for example, Merino wool and Egyptian cotton are favored because they produce exceptionally long, thin (fine) fibers for their type.

A single spun yarn may be crochet as is, or braided or plied with another. In plying, two or more yarns are spun together, almost always in the opposite sense from which they were spun individually; for example, two Z-twist yarns are usually plied with an S-twist. The opposing twist relieves some of the yarns' tendency to curl up and produces a thicker, "balanced" yarn. Plied yarns may themselves be plied together, producing "cabled yarns" or "multi-stranded yarns". Sometimes, the yarns being plied are fed at different rates, so that one yarn loops around the other, as in bouclé. The single yarns may be dyed separately before plying, or afterwards to give the yarn a uniform look.

The dyeing of yarns is a complex art. Yarns need not be dyed; or they may be dyed one color, or a great variety of colors. Dyeing may be done industrially, by hand or even hand-painted onto the yarn. A great variety of synthetic dyes have been developed since the synthesis of indigo dye in the mid-19th century; however, natural dyes are also possible, although they are generally less brilliant. The color-scheme of a yarn is sometimes called its colorway. Variegated yarns can produce interesting visual effects, such as diagonal stripes.

Crocheted fabric is begun by placing a slip-knot loop on the hook (though other methods, such as a magic ring or simple folding over of the yarn may be used), pulling another loop through the first loop, and repeating this process to create a chain of a suitable length. The chain is either turned and worked in rows, or joined to the beginning of the row with a slip stitch and worked in rounds. Rounds can also be created by working many stitches into a single loop. Stitches are made by pulling one or more loops through each loop of the chain. At any one time at the end of a stitch, there is only one loop left on the hook. Tunisian crochet, however, draws all of the loops for an entire row onto a long hook before working them off one at a time. Like knitting, crochet can be worked either flat (back and forth in rows) or in the round (in spirals, such as when making tubular pieces). 

There are six main types of basic stitches (the following description uses US crochet terminology which differs from the terminology used in the UK and Europe).
While the horizontal distance covered by these basic stitches is the same, they differ in height and thickness.

The more advanced stitches are often combinations of these basic stitches, or are made by inserting the hook into the work in unusual locations. More advanced stitches include the "shell stitch", "V stitch", "spike stitch", "Afghan stitch", "butterfly stitch", "popcorn stitch", "cluster stitch", and "crocodile stitch".

In the English-speaking crochet world, basic stitches have different names that vary by country. The differences are usually referred to as UK/US or British/American. Crochet is traditionally worked off a written pattern in which stitches and placement are communicated using textual abbreviations. To help counter confusion when reading patterns, a diagramming system using a standard international notation has come into use (illustration, left). In the United States, crochet terminology and sizing guidelines, as well as standards for yarn and hook labeling, are primarily regulated by the Craft Yarn Council.

Another terminological difference is known as "tension" (UK) and "gauge" (US). Individual crocheters work yarn with a loose or a tight hold and, if unmeasured, these differences can lead to significant size changes in finished garments that have the same number of stitches. In order to control for this inconsistency, printed crochet instructions include a standard for the number of stitches across a standard swatch of fabric. An individual crocheter begins work by producing a test swatch and compensating for any discrepancy by changing to a smaller or larger hook. North Americans call this "gauge", referring to the result of these adjustments; British crocheters speak of "tension", which refers to the crafter's grip on the yarn while producing stitches.

One of the more obvious differences is that crochet uses one hook while much knitting uses two needles. In most crochet, the artisan usually has only one live stitch on the hook (with the exception being Tunisian crochet), while a knitter keeps an entire row of stitches active simultaneously. Dropped stitches, which can unravel a knitted fabric, rarely interfere with crochet work, due to a second structural difference between knitting and crochet. In knitting, each stitch is supported by the corresponding stitch in the row above and it supports the corresponding stitch in the row below, whereas crochet stitches are only supported by and support the stitches on either side of it. If a stitch in a finished crocheted item breaks, the stitches above and below remain intact, and because of the complex looping of each stitch, the stitches on either side are unlikely to come loose unless heavily stressed.

Round or cylindrical patterns are simple to produce with a regular crochet hook, but cylindrical knitting requires either a set of circular needles or three to five special double-ended needles. Many crocheted items are composed of individual motifs which are then joined, either by sewing or crocheting, whereas knitting is usually composed of one fabric, such as entrelac.

Freeform crochet is a technique that can create interesting shapes in three dimensions because new stitches can be made independently of previous stitches almost anywhere in the crocheted piece. It is generally accomplished by building shapes or structural elements onto existing crocheted fabric at any place the crafter desires.

Knitting can be accomplished by machine, while many crochet stitches can only be crafted by hand. The height of knitted and crocheted stitches is also different: a single crochet stitch is twice the height of a knit stitch in the same yarn size and comparable diameter tools, and a double crochet stitch is about four times the height of a knit stitch.

While most crochet is made with a hook, there is also a method of crocheting with a knitting loom. This is called "loomchet". Slip stitch crochet is very similar to knitting. Each stitch in slip stitch crochet is formed the same way as a knit or purl stitch which is then bound off. A person working in slip stitch crochet can follow a knitted pattern with knits, purls, and cables, and get a similar result.

It is a common perception that crochet produces a thicker fabric than knitting, tends to have less "give" than knitted fabric, and uses approximately a third more yarn for a comparable project than knitted items. Although this is true when comparing a single crochet swatch with a stockinette swatch, both made with the same size yarn and needle/hook, it is not necessarily true for crochet in general. Most crochet uses far less than 1/3 more yarn than knitting for comparable pieces, and a crocheter can get similar feel and drape to knitting by using a larger hook or thinner yarn. Tunisian crochet and slip stitch crochet can in some cases use less yarn than knitting for comparable pieces. According to sources claiming to have tested the 1/3 more yarn assertion, a single crochet stitch (sc) uses approximately the same amount of yarn as knit garter stitch, but more yarn than stockinette stitch. Any stitch using yarnovers uses less yarn than single crochet to produce the same amount of fabric. Cluster stitches, which are in fact multiple stitches worked together, will use the most length.

Standard crochet stitches like sc and dc also produce a thicker fabric, more like knit garter stitch. This is part of why they use more yarn. Slip stitch can produce a fabric much like stockinette that is thinner and therefore uses less yarn.

Any yarn can be either knitted or crocheted, provided needles or hooks of the correct size are used, but the cord's properties should be taken into account. For example, lofty, thick woolen yarns tend to function better when knitted, which does not crush their airy structure, while thin and tightly spun yarn helps to achieve the firm texture required for Amigurumi crochet.

It has been very common for people and groups to crochet clothing and other garments and then donate them to soldiers during war. People have also crocheted clothing and then donated it to hospitals, for sick patients and also for newborn babies. Sometimes groups will crochet for a specific charity purpose, such as crocheting for homeless shelters, nursing homes, etc.

It is becoming increasingly popular to crochet hats (commonly referred to as "chemo caps") and donate them to cancer treatment centers, for those undergoing chemotherapy and therefore losing hair. During October pink hats and scarves are made and proceeds are donated to breast cancer funds. Organizations dedicated to using crochet as a way to help others include Knots of Love, Crochet for Cancer, and Soldiers' Angels. These organizations offer warm useful items for people in need.

In 2020, people around the world banded together to help save the wildlife affected by the Australian bushfires by crocheting kangaroo pouches, koala mittens and wildlife nests. This was an international effort to help during the particularly bad bushfire season which devastated local ecological systems.

A group started in 2005 to create crochet versions of coral reefs grew by 2022 to over 20,000 contributors in what became the Crochet Coral Reef Project. To promote awareness of the effects of global warming, their creations have been displayed in galleries and museums by an estimated 2 million people. Many creations apply hyperbolic (curved) geometric shapes—distinguished from Euclidean (flat) geometry—to emulate natural structures.

Crochet has been used to illustrate shapes in hyperbolic space that are difficult to reproduce using other media or are difficult to understand when viewed two-dimensionally.

Mathematician Daina Taimiņa first used crochet in 1997 to create strong, durable models of hyperbolic space after finding paper models were delicate and hard to create. These models enable one to turn, fold, and otherwise manipulate space to more fully grasp ideas such as how a line can appear curved in hyperbolic space yet actually be straight. Her work received an exhibition by the Institute For Figuring.
Examples in nature of organisms that show hyperbolic structures include lettuces, sea slugs, flatworms and coral. Margaret Wertheim and Christine Wertheim of the Institute For Figuring created a travelling art installation of a coral reef using Taimina's method. Local artists are encouraged to create their own "satellite reefs" to be included alongside the original display.

As hyperbolic and mathematics-based crochet has become more popular, there have been several events highlighting work from various fiber artists. Two shows were "Sant Ocean Hall" at the Smithsonian in Washington, D.C., and "Sticks, Hooks, and the Mobius: Knit and Crochet Go Cerebral" at Lafayette College in Pennsylvania.

In "Style in the technical arts", Gottfried Semper looks at the textile with great promise and historical precedent. In Section 53, he writes of the "loop stitch, or Noeud Coulant: a knot that, if untied, causes the whole system to unravel." In the same section, Semper confesses his ignorance of the subject of crochet but believes strongly that it is a technique of great value as a textile technique and possibly something more.

There are a small number of architects currently interested in the subject of crochet as it relates to architecture. The following publications, explorations and thesis projects can be used as a resource to see how crochet is being used within the capacity of architecture.

In the past few years, a practice called yarn bombing, or the use of knitted or crocheted cloth to modify and beautify one's (usually outdoor) surroundings, emerged in the US and spread worldwide. Yarn bombers sometimes target existing pieces of graffiti for beautification. In 2010, an entity dubbed "the Midnight Knitter" hit West Cape May. Residents awoke to find knit cozies hugging tree branches and sign poles. In September 2015, Grace Brett was named "The World's Oldest Yarn Bomber". She is part of a group of yarn graffiti-artists called the Souter Stormers, who beautify their local town in Scotland.




Electromagnetic coil

An electromagnetic coil is an electrical conductor such as a wire in the shape of a coil (spiral or helix). Electromagnetic coils are used in electrical engineering, in applications where electric currents interact with magnetic fields, in devices such as electric motors, generators, inductors, electromagnets, transformers, and sensor coils. Either an electric current is passed through the wire of the coil to generate a magnetic field, or conversely, an external "time-varying" magnetic field through the interior of the coil generates an EMF (voltage) in the conductor.

A current through any conductor creates a circular magnetic field around the conductor due to Ampere's law. The advantage of using the coil shape is that it increases the strength of the magnetic field produced by a given current. The magnetic fields generated by the separate turns of wire all pass through the center of the coil and add (superpose) to produce a strong field there. The greater the number of turns of wire, the stronger the field produced. Conversely, a "changing" external magnetic flux induces a voltage in a conductor such as a wire, due to Faraday's law of induction. The induced voltage can be increased by winding the wire into a coil because the field lines intersect the circuit multiple times.

The direction of the magnetic field produced by a coil can be determined by the right hand grip rule. If the fingers of the right hand are wrapped around the magnetic core of a coil in the direction of conventional current through the wire, the thumb will point in the direction the magnetic field lines pass through the coil. The end of a magnetic core from which the field lines emerge is defined to be the North pole.

There are many different types of coils used in electric and electronic equipment.

The wire or conductor which constitutes the coil is called the winding. The hole in the center of the coil is called the core area or "magnetic axis". Each loop of wire is called a turn. In windings in which the turns touch, the wire must be insulated with a coating of nonconductive insulation such as plastic or enamel to prevent the current from passing between the wire turns. The winding is often wrapped around a "coil form" made of plastic or other material to hold it in place. The ends of the wire are brought out and attached to an external circuit. Windings may have additional electrical connections along their length; these are called taps. A winding that has a single tap in the center of its length is called center-tapped.

Coils can have more than one winding, insulated electrically from each other. When there are two or more windings around a common magnetic axis, the windings are said to be inductively coupled or magnetically coupled. A time-varying current through one winding will create a time-varying magnetic field that passes through the other winding, which will induce a time-varying voltage in the other windings. This is called a transformer. The winding to which current is applied, which creates the magnetic field, is called the "primary winding". The other windings are called "secondary windings".

Many electromagnetic coils have a magnetic core, a piece of ferromagnetic material like iron in the center to increase the magnetic field. The current through the coil magnetizes the iron, and the field of the magnetized material adds to the field produced by the wire. This is called a ferromagnetic-core or iron-core coil. A ferromagnetic core can increase the magnetic field and inductance of a coil by hundreds or thousands of times over what it would be without the core. A ferrite core coil is a variety of coil with a core made of ferrite, a ferrimagnetic ceramic compound. Ferrite coils have lower core losses at high frequencies.
A coil without a ferromagnetic core is called an air-core coil. This includes coils wound on plastic or other nonmagnetic forms, as well as coils which actually have empty air space inside their windings.

Coils can be classified by the frequency of the current they are designed to operate with:

Coils can be classified by their function:

Electromagnets are coils that generate a magnetic field for some external use, often to exert a mechanical force on something. or remove existing background fields. A few specific types:

Inductors or reactors are coils which generate a magnetic field which interacts with the coil itself, to induce a back EMF which opposes changes in current through the coil. Inductors are used as circuit elements in electrical circuits, to temporarily store energy or resist changes in current. A few types:

A transformer is a device with two or more magnetically coupled windings (or sections of a single winding). A time varying current in one coil (called the primary winding) generates a magnetic field which induces a voltage in the other coil (called the secondary winding). A few types:
Electric machines such as motors and generators have one or more windings which interact with moving magnetic fields to convert electrical energy to mechanical energy. Often a machine will have one winding through which passes most of the power of the machine (the "armature"), and a second winding which provides the magnetic field of the rotating element ( the "field winding") which may be connected by brushes or slip rings to an external source of electric current. In an induction motor, the "field" winding of the rotor is energized by the slow relative motion between the rotating winding and the rotating magnetic field produced by the stator winding, which induces the necessary exciting current in the rotor.

These are coils used to translate time-varying magnetic fields to electric signals, and vice versa. A few types:

There are also types of coil which don't fit into these categories.




Charles I of England

Charles I (19 November 1600 – 30 January 1649) was King of England, Scotland, and Ireland from 27 March 1625 until his execution in 1649.

Charles was born into the House of Stuart as the second son of King James VI of Scotland, but after his father inherited the English throne in 1603, he moved to England, where he spent much of the rest of his life. He became heir apparent to the kingdoms of England, Scotland, and Ireland in 1612 upon the death of his elder brother, Henry Frederick, Prince of Wales. An unsuccessful and unpopular attempt to marry him to Infanta Maria Anna of Spain culminated in an eight-month visit to Spain in 1623 that demonstrated the futility of the marriage negotiation. Two years later, shortly after his accession, he married Henrietta Maria of France.

After his succession in 1625, Charles quarrelled with the English Parliament, which sought to curb his royal prerogative. He believed in the divine right of kings, and was determined to govern according to his own conscience. Many of his subjects opposed his policies, in particular the levying of taxes without parliamentary consent, and perceived his actions as those of a tyrannical absolute monarch. His religious policies, coupled with his marriage to a Roman Catholic, generated antipathy and mistrust from Reformed religious groups such as the English Puritans and Scottish Covenanters, who thought his views too Catholic. He supported high church Anglican ecclesiastics and failed to aid continental Protestant forces successfully during the Thirty Years' War. His attempts to force the Church of Scotland to adopt high Anglican practices led to the Bishops' Wars, strengthened the position of the English and Scottish parliaments, and helped precipitate his own downfall.

From 1642, Charles fought the armies of the English and Scottish parliaments in the English Civil War. After his defeat in 1645 at the hands of the Parliamentarian New Model Army, he fled north from his base at Oxford. Charles surrendered to a Scottish force and after lengthy negotiations between the English and Scottish parliaments he was handed over to the Long Parliament in London. Charles refused to accept his captors' demands for a constitutional monarchy, and temporarily escaped captivity in November 1647. Re-imprisoned on the Isle of Wight, he forged an alliance with Scotland, but by the end of 1648, the New Model Army had consolidated its control over England. Charles was tried, convicted, and executed for high treason in January 1649. The monarchy was abolished and the Commonwealth of England was established as a republic. The monarchy would be restored to Charles's son Charles II in 1660.

The second son of King James VI of Scotland and Anne of Denmark, Charles was born in Dunfermline Palace, Fife, on 19 November 1600. At a Protestant ceremony in the Chapel Royal of Holyrood Palace in Edinburgh on 23 December 1600, he was baptised by David Lindsay, Bishop of Ross, and created Duke of Albany, the traditional title of the second son of the king of Scotland, with the subsidiary titles of Marquess of Ormond, Earl of Ross and Lord Ardmannoch.

James VI was the first cousin twice removed of Queen Elizabeth I of England, and when she died childless in March 1603, he became King of England as James I. Charles was a weak and sickly infant, and while his parents and older siblings left for England in April and early June that year, due to his fragile health, he remained in Scotland with his father's friend Lord Fyvie appointed as his guardian.

By 1604, when Charles was three-and-a-half, he was able to walk the length of the great hall at Dunfermline Palace without assistance, and it was decided that he was strong enough to journey to England to be reunited with his family. In mid-July 1604, he left Dunfermline for England, where he was to spend most of the rest of his life. In England, Charles was placed under the charge of Elizabeth, Lady Carey, the wife of courtier Sir Robert Carey, who put him in boots made of Spanish leather and brass to help strengthen his weak ankles. His speech development was also slow, and he had a stammer for the rest of his life.

In January 1605, Charles was created Duke of York, as is customary in the case of the English sovereign's second son, and made a Knight of the Bath. Thomas Murray, a presbyterian Scot, was appointed as a tutor. Charles learnt the usual subjects of classics, languages, mathematics and religion. In 1611, he was made a Knight of the Garter.

Eventually, Charles apparently conquered his physical infirmity, which might have been caused by rickets. He became an adept horseman and marksman, and took up fencing. Even so, his public profile remained low in contrast to that of his physically stronger and taller elder brother, Henry Frederick, Prince of Wales, whom Charles adored and attempted to emulate. But in early November 1612, Henry died at the age of 18 of what is suspected to have been typhoid (or possibly porphyria). Charles, who turned 12 two weeks later, became heir apparent. As the eldest surviving son of the sovereign, he automatically gained several titles, including Duke of Cornwall and Duke of Rothesay. In November 1616, he was created Prince of Wales and Earl of Chester.

In 1613, Charles's sister Elizabeth married Frederick V, Elector Palatine, and moved to Heidelberg. In 1617, the Habsburg Archduke Ferdinand of Austria, a Catholic, was elected king of Bohemia. The next year, the Bohemians rebelled, defenestrating the Catholic governors. In August 1619, the Bohemian diet chose Frederick, who led the Protestant Union, as their monarch, while Ferdinand was elected Holy Roman Emperor in the imperial election. Frederick's acceptance of the Bohemian crown in defiance of the emperor marked the beginning of the turmoil that would develop into the Thirty Years' War. The conflict, originally confined to Bohemia, spiralled into a wider European war, which the English Parliament and public quickly grew to see as a polarised continental struggle between Catholics and Protestants. In 1620, King Frederick was defeated at the Battle of White Mountain near Prague and his hereditary lands in the Electoral Palatinate were invaded by a Habsburg force from the Spanish Netherlands. James, however, had been seeking marriage between Prince Charles and Ferdinand's niece, Infanta Maria Anna of Spain, and began to see the Spanish match as a possible diplomatic means of achieving peace in Europe.

Unfortunately for James, negotiation with Spain proved unpopular with both the public and James's court. The English Parliament was actively hostile towards Spain and Catholicism, and thus, when called by James in 1621, the members hoped for an enforcement of recusancy laws, a naval campaign against Spain, and a Protestant marriage for the Prince of Wales. James's Lord Chancellor, Francis Bacon, was impeached before the House of Lords for corruption. The impeachment was the first since 1459 without the king's official sanction in the form of a bill of attainder. The incident set an important precedent as the process of impeachment would later be used against Charles and his supporters the Duke of Buckingham, Archbishop William Laud, and the Earl of Strafford. James insisted that the House of Commons be concerned exclusively with domestic affairs, while the members protested that they had the privilege of free speech within the Commons' walls, demanding war with Spain and a Protestant princess of Wales. Like his father, Charles considered discussion of his marriage in the Commons impertinent and an infringement of his father's royal prerogative. In January 1622, James dissolved Parliament, angry at what he perceived as the members' impudence and intransigence.

Charles and Buckingham, James's favourite and a man who had great influence over the prince, travelled incognito to Spain in February 1623 to try to reach agreement on the long-pending Spanish match. The trip was an embarrassing failure. The "infanta" thought Charles little more than an infidel, and the Spanish at first demanded that he convert to Catholicism as a condition of the match. They insisted on toleration of Catholics in England and the repeal of the English penal laws, which Charles knew Parliament would not agree to, and that the "infanta" remain in Spain for a year after any wedding to ensure that England complied with all the treaty's terms. A personal quarrel erupted between Buckingham and the Count of Olivares, the Spanish chief minister, and so Charles conducted the ultimately futile negotiations personally. When he returned to London in October, without a bride and to a rapturous and relieved public welcome, he and Buckingham pushed the reluctant James to declare war on Spain.

With the encouragement of his Protestant advisers, James summoned the English Parliament in 1624 to request subsidies for a war. Charles and Buckingham supported the impeachment of the Lord Treasurer, Lionel Cranfield, 1st Earl of Middlesex, who opposed war on grounds of cost and quickly fell in much the same manner Bacon had. James told Buckingham he was a fool, and presciently warned Charles that he would live to regret the revival of impeachment as a parliamentary tool. An underfunded makeshift army under Ernst von Mansfeld set off to recover the Palatinate, but it was so poorly provisioned that it never advanced beyond the Dutch coast.

By 1624, the increasingly ill James was finding it difficult to control Parliament. By the time of his death in March 1625, Charles and Buckingham had already assumed "de facto" control of the kingdom.

With the failure of the Spanish match, Charles and Buckingham turned their attention to France. On 1 May 1625 Charles was married by proxy to the 15-year-old French princess Henrietta Maria in front of the doors of Notre Dame de Paris. He had seen her in Paris while en route to Spain. They met in person on 13 June 1625 in Canterbury. Charles delayed the opening of his first Parliament until after the marriage was consummated, to forestall any opposition. Many members of the Commons opposed his marriage to a Catholic, fearing that he would lift restrictions on Catholic recusants and undermine the official establishment of the reformed Church of England. Charles told Parliament that he would not relax religious restrictions, but promised to do exactly that in a secret marriage treaty with his brother-in-law Louis XIII of France. Moreover, the treaty loaned to the French seven English naval ships that were used to suppress the Protestant Huguenots at La Rochelle in September 1625. Charles was crowned on 2 February 1626 at Westminster Abbey, but without his wife at his side, because she refused to participate in a Protestant religious ceremony.

Distrust of Charles's religious policies increased with his support of a controversial anti-Calvinist ecclesiastic, Richard Montagu, who was in disrepute among the Puritans. In his pamphlet "A New Gag for an Old Goose" (1624), a reply to the Catholic pamphlet "A New Gag for the New Gospel", Montagu argued against Calvinist predestination, the doctrine that God preordained salvation and damnation. Anti-Calvinistsknown as Arminiansbelieved that people could accept or reject salvation by exercising free will. Arminian divines had been one of the few sources of support for Charles's proposed Spanish marriage. With King James's support, Montagu produced another pamphlet, "Appello Caesarem", published in 1625 shortly after James's death and Charles's accession. To protect Montagu from the stricture of Puritan members of Parliament, Charles made him a royal chaplain, heightening many Puritans' suspicions that Charles favoured Arminianism as a clandestine attempt to aid Catholicism's resurgence.

Rather than direct involvement in the European land war, the English Parliament preferred a relatively inexpensive naval attack on Spanish colonies in the New World, hoping for the capture of the Spanish treasure fleets. Parliament voted to grant a subsidy of £140,000, an insufficient sum for Charles's war plans. Moreover, the House of Commons limited its authorisation for royal collection of tonnage and poundage (two varieties of customs duties) to a year, although previous sovereigns since Henry VI had been granted the right for life. In this manner, Parliament could delay approval of the rates until after a full-scale review of customs revenue. The bill made no progress in the House of Lords past its first reading. Although no Parliamentary Act for the levy of tonnage and poundage was obtained, Charles continued to collect the duties.

A poorly conceived and executed naval expedition against Spain under Buckingham's leadership went badly, and the House of Commons began proceedings for the impeachment of the duke. In May 1626, Charles nominated Buckingham as Chancellor of Cambridge University in a show of support, and had two members who had spoken against BuckinghamDudley Digges and Sir John Eliotarrested at the door of the House. The Commons was outraged by the imprisonment of two of their members, and after about a week in custody, both were released. On 12 June 1626, the Commons launched a direct protestation attacking Buckingham, stating, "We protest before your Majesty and the whole world that until this great person be removed from intermeddling with the great affairs of state, we are out of hope of any good success; and do fear that any money we shall or can give will, through his misemployment, be turned rather to the hurt and prejudice of this your kingdom than otherwise, as by lamentable experience we have found those large supplies formerly and lately given." Despite the protests, Charles refused to dismiss his friend, dismissing Parliament instead.

Meanwhile, domestic quarrels between Charles and Henrietta Maria were souring the early years of their marriage. Disputes over her jointure, appointments to her household, and the practice of her religion culminated in the king expelling the vast majority of her French attendants in August 1626. Despite Charles's agreement to provide the French with English ships as a condition of marrying Henrietta Maria, in 1627 he launched an attack on the French coast to defend the Huguenots at La Rochelle. The action, led by Buckingham, was ultimately unsuccessful. Buckingham's failure to protect the Huguenotsand his retreat from Saint-Martin-de-Réspurred Louis XIII's siege of La Rochelle and furthered the English Parliament's and people's detestation of the duke.

Charles provoked further unrest by trying to raise money for the war through a "forced loan": a tax levied without parliamentary consent. In November 1627, the test case in the King's Bench, the "Five Knights' Case", found that the king had a prerogative right to imprison without trial those who refused to pay the forced loan. Summoned again in March 1628, Parliament adopted a Petition of Right on 26 May, calling upon Charles to acknowledge that he could not levy taxes without Parliament's consent, impose martial law on civilians, imprison them without due process, or quarter troops in their homes. Charles assented to the petition on 7 June, but by the end of the month he had prorogued Parliament and reasserted his right to collect customs duties without authorisation from Parliament.

On 23 August 1628, Buckingham was assassinated. Charles was deeply distressed. According to Edward Hyde, 1st Earl of Clarendon, he "threw himself upon his bed, lamenting with much passion and with abundance of tears". He remained grieving in his room for two days. In contrast, the public rejoiced at Buckingham's death, accentuating the gulf between the court and the nation and between the Crown and the Commons. Buckingham's death effectively ended the war with Spain and eliminated his leadership as an issue, but it did not end the conflicts between Charles and Parliament. It did, however, coincide with an improvement in Charles's relationship with his wife, and by November 1628 their old quarrels were at an end. Perhaps Charles's emotional ties were transferred from Buckingham to Henrietta Maria. She became pregnant for the first time, and the bond between them grew stronger. Together, they embodied an image of virtue and family life, and their court became a model of formality and morality.

In January 1629, Charles opened the second session of the English Parliament, which had been prorogued in June 1628, with a moderate speech on the tonnage and poundage issue. Members of the House of Commons began to voice opposition to Charles's policies in light of the case of John Rolle, a Member of Parliament whose goods had been confiscated for failing to pay tonnage and poundage. Many MPs viewed the imposition of the tax as a breach of the Petition of Right. When Charles ordered a parliamentary adjournment on 2 March, members held the Speaker, Sir John Finch, down in his chair so that the session could be prolonged long enough for resolutions against Catholicism, Arminianism and tonnage and poundage to be read out and acclaimed by the chamber. The provocation was too much for Charles, who dissolved Parliament and had nine parliamentary leaders, including Sir John Eliot, imprisoned over the matter, thereby turning the men into martyrs and giving popular cause to their protest.

Personal rule necessitated peace. Without the means in the foreseeable future to raise funds from Parliament for a European war, or Buckingham's help, Charles made peace with France and Spain. The next 11 years, during which Charles ruled England without a Parliament, are known as the Personal Rule or the "eleven years' tyranny". Ruling without Parliament was not exceptional, and was supported by precedent. But only Parliament could legally raise taxes, and without it Charles's capacity to acquire funds for his treasury was limited to his customary rights and prerogatives.

A large fiscal deficit had arisen during the reigns of Elizabeth I and James I. Notwithstanding Buckingham's short-lived campaigns against both Spain and France, Charles had little financial capacity to wage wars overseas. Throughout his reign, he was obliged to rely primarily on volunteer forces for defence and on diplomatic efforts to support his sister Elizabeth and his foreign policy objective for the restoration of the Palatinate. England was still the least taxed country in Europe, with no official excise and no regular direct taxation. To raise revenue without reconvening Parliament, Charles resurrected an all-but-forgotten law called the "Distraint of Knighthood", in abeyance for over a century, which required any man who earned £40 or more from land each year to present himself at the king's coronation to be knighted. Relying on this old statute, Charles fined those who had failed to attend his coronation in 1626.

The chief tax Charles imposed was a feudal levy known as ship money, which proved even more unpopular, and lucrative, than tonnage and poundage before it. Previously, collection of ship money had been authorised only during wars, and only on coastal regions. But Charles argued that there was no legal bar to collecting the tax for defence during peacetime and throughout the whole of the kingdom. Ship money, paid directly to the Treasury of the Navy, provided between £150,000 to £200,000 annually between 1634 and 1638, after which yields declined. Opposition to ship money steadily grew, but England's 12 common law judges ruled the tax within the king's prerogative, though some of them had reservations. The prosecution of John Hampden for non-payment in 1637–38 provided a platform for popular protest, and the judges found against Hampden only by the narrow margin of 7–5.

Charles also derived money by granting monopolies, despite a statute forbidding such action, which, though inefficient, raised an estimated £100,000 a year in the late 1630s. One such monopoly was for soap, pejoratively referred to as "popish soap" because some of its backers were Catholics. Charles also raised funds from the Scottish nobility, at the price of considerable acrimony, by the Act of Revocation (1625), whereby all gifts of royal or church land made to the nobility since 1540 were revoked, with continued ownership being subject to an annual rent. In addition, the boundaries of the royal forests in England were restored to their ancient limits as part of a scheme to maximise income by exploiting the land and fining land users within the reasserted boundaries for encroachment. The programme's focus was disafforestation and sale of forest lands for conversion to pasture and arable farming, or in the case of the Forest of Dean, development for the iron industry. Disafforestation frequently caused riots and disturbances, including those known as the Western Rising.

Against the background of this unrest, Charles faced bankruptcy in mid-1640. The City of London, preoccupied with its own grievances, refused to make any loans to him, as did foreign powers. In this extremity, in July Charles seized silver bullion worth £130,000 held in trust at the mint in the Tower of London, promising its later return at 8% interest to its owners. In August, after the East India Company refused to grant a loan, Lord Cottington seized the company's stock of pepper and spices and sold it for £60,000 (far below its market value), promising to refund the money with interest later.

Throughout Charles's reign, the English Reformation was in the forefront of political debate. Arminian theology emphasised clerical authority and the individual's ability to reject or accept salvation, which opponents viewed as heretical and a potential vehicle for the reintroduction of Catholicism. Puritan reformers considered Charles too sympathetic to Arminianism, and opposed his desire to move the Church of England in a more traditional and sacramental direction. In addition, his Protestant subjects followed the European war closely and grew increasingly dismayed by Charles's diplomacy with Spain and his failure to support the Protestant cause abroad effectively.

In 1633, Charles appointed William Laud Archbishop of Canterbury. They initiated a series of reforms to promote religious uniformity by restricting non-conformist preachers, insisting the liturgy be celebrated as prescribed by the "Book of Common Prayer", organising the internal architecture of English churches to emphasise the sacrament of the altar, and reissuing King James's Declaration of Sports, which permitted secular activities on the sabbath. The Feoffees for Impropriations, an organisation that bought benefices and advowsons so that Puritans could be appointed to them, was dissolved. Laud prosecuted those who opposed his reforms in the Court of High Commission and the Star Chamber, the two most powerful courts in the land. The courts became feared for their censorship of opposing religious views and unpopular among the propertied classes for inflicting degrading punishments on gentlemen. For example, in 1637 William Prynne, Henry Burton and John Bastwick were pilloried, whipped and mutilated by cropping and imprisoned indefinitely for publishing anti-episcopal pamphlets.

When Charles attempted to impose his religious policies in Scotland he faced numerous difficulties. Although born in Scotland, Charles had become estranged from it; his first visit since early childhood was for his Scottish coronation in 1633. To the dismay of the Scots, who had removed many traditional rituals from their liturgical practice, Charles insisted that the coronation be conducted using the Anglican rite. In 1637, he ordered the use of a new prayer book in Scotland that was almost identical to the English "Book of Common Prayer", without consulting either the Scottish Parliament or the Kirk. Although it had been written, under Charles's direction, by Scottish bishops, many Scots resisted it, seeing it as a vehicle to introduce Anglicanism to Scotland. On 23 July, riots erupted in Edinburgh upon the first Sunday of the prayer book's usage, and unrest spread throughout the Kirk. The public began to mobilise around a reaffirmation of the National Covenant, whose signatories pledged to uphold the reformed religion of Scotland and reject any innovations not authorised by Kirk and Parliament. When the General Assembly of the Church of Scotland met in November 1638, it condemned the new prayer book, abolished episcopal church government by bishops, and adopted presbyterian government by elders and deacons.

Charles perceived the unrest in Scotland as a rebellion against his authority, precipitating the First Bishops' War in 1639. He did not seek subsidies from the English Parliament to wage war, instead raising an army without parliamentary aid and marching to Berwick-upon-Tweed, on the Scottish border. The army did not engage the Covenanters, as the king feared the defeat of his forces, whom he believed to be significantly outnumbered by the Scots. In the Treaty of Berwick, Charles regained custody of his Scottish fortresses and secured the dissolution of the Covenanters' interim government, albeit at the decisive concession that both the Scottish Parliament and General Assembly of the Scottish Church were called.

The military failure in the First Bishops' War caused a financial and diplomatic crisis for Charles that deepened when his efforts to raise funds from Spain while simultaneously continuing his support for his Palatine relatives led to the public humiliation of the Battle of the Downs, where the Dutch destroyed a Spanish bullion fleet off the coast of Kent in sight of the impotent English navy.

Charles continued peace negotiations with the Scots in a bid to gain time before launching a new military campaign. Because of his financial weakness, he was forced to call Parliament into session in an attempt to raise funds for such a venture. Both the English and Irish parliaments were summoned in the early months of 1640. In March 1640, the Irish Parliament duly voted in a subsidy of £180,000 with the promise to raise an army 9,000 strong by the end of May. But in the English general election in March, court candidates fared badly, and Charles's dealings with the English Parliament in April quickly reached stalemate. The earls of Northumberland and Strafford attempted to broker a compromise whereby the king would agree to forfeit ship money in exchange for £650,000 (although the cost of the coming war was estimated at £1 million). Nevertheless, this alone was insufficient to produce consensus in the Commons. The Parliamentarians' calls for further reforms were ignored by Charles, who still retained the support of the House of Lords. Despite the protests of the Earl of Northumberland, the Short Parliament (as it came to be known) was dissolved in May 1640, less than a month after it assembled.

By this stage the Earl of Strafford, Lord Deputy of Ireland since 1632, had emerged as Charles's right-hand man and, together with Archbishop Laud, pursued a policy that he termed "Thorough", which aimed to make central royal authority more efficient and effective at the expense of local or anti-government interests. Although originally a critic of the king, Strafford defected to royal service in 1628, in part due to the Duke of Buckingham's persuasion, and had since emerged, alongside Laud, as the most influential of Charles's ministers.

Bolstered by the failure of the English Short Parliament, the Scottish Parliament declared itself capable of governing without the king's consent, and in August 1640 the Covenanter army moved into the English county of Northumberland. Following the illness of Lord Northumberland, who was the king's commander-in-chief, Charles and Strafford went north to command the English forces, despite Strafford being ill himself with a combination of gout and dysentery. The Scottish soldiery, many of whom were veterans of the Thirty Years' War, had far greater morale and training than their English counterparts. They met virtually no resistance until reaching Newcastle upon Tyne, where they defeated the English forces at the Battle of Newburn and occupied the city, as well as the neighbouring county of Durham.

As demands for a parliament grew, Charles took the unusual step of summoning a great council of peers. By the time it met, on 24 September at York, Charles had resolved to follow the almost universal advice to call a parliament. After informing the peers that a parliament would convene in November, he asked them to consider how he could acquire funds to maintain his army against the Scots in the meantime. They recommended making peace. A cessation of arms was negotiated in the humiliating Treaty of Ripon, signed in October 1640. This stated that the Scots would continue to occupy Northumberland and Durham and be paid £850 per day indefinitely until a final settlement was negotiated and the English Parliament recalled, which would be required to raise sufficient funds to pay the Scottish forces. Consequently, Charles summoned what later became known as the Long Parliament. Once again, his supporters fared badly at the polls. Of the 493 members of the Commons returned in November, over 350 were opposed to the king.

The Long Parliament proved just as difficult for Charles as had the Short Parliament. It assembled on 3 November 1640 and quickly began proceedings to impeach the king's leading counsellors for high treason. Strafford was taken into custody on 10 November; Laud was impeached on 18 December; Finch, now Lord Keeper of the Great Seal, was impeached the next day, and consequently fled to The Hague with Charles's permission on 21 December. To prevent the king from dissolving it at will, Parliament passed the Triennial Act, which required Parliament to be summoned at least every three years, and permitted the Lord Keeper and 12 peers to summon Parliament if the king failed to do so. The Act was coupled with a subsidy bill, and to secure the latter, Charles grudgingly granted royal assent in February 1641.

Strafford had become the principal target of the Parliamentarians, particularly John Pym, and he went on trial for high treason on 22 March 1641. But the key allegation by Sir Henry Vane that Strafford had threatened to use the Irish army to subdue England was not corroborated, and on 10 April Pym's case collapsed. Pym and his allies immediately launched a bill of attainder, which simply declared Strafford guilty and pronounced the sentence of death.

Charles assured Strafford that "upon the word of a king you shall not suffer in life, honour or fortune", and the attainder could not succeed if Charles withheld assent. Furthermore, many members and most peers opposed the attainder, not wishing, in the words of one, to "commit murder with the sword of justice". But increased tensions and an attempted coup by royalist army officers in support of Strafford and in which Charles was involved began to sway the issue. The Commons passed the bill on 20 April by a large margin (204 in favour, 59 opposed, and 230 abstained), and the Lords acquiesced (by 26 votes to 19, with 79 absent) in May. On 3 May, Parliament's Protestation attacked the "wicked counsels" of Charles's "arbitrary and tyrannical government". While those who signed the petition undertook to defend the king's "person, honour and estate", they also swore to preserve "the true reformed religion", Parliament, and the "rights and liberties of the subjects". Fearing for his family's safety in the face of unrest, Charles reluctantly assented to Strafford's attainder on 9 May after consulting his judges and bishops. Strafford was beheaded three days later.

Also in early May, Charles assented to an unprecedented Act that forbade the dissolution of the English Parliament without its consent. In the following months, ship money, fines in distraint of knighthood and excise without parliamentary consent were declared unlawful, and the Courts of Star Chamber and High Commission were abolished. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. The House of Commons also launched bills attacking bishops and episcopacy, but these failed in the Lords.

Charles had made important concessions in England, and temporarily improved his position in Scotland by signing a final settlement of the Bishops' Wars, then securing the Scots' favour on a visit from August to November 1641 during which he conceded to the official establishment of presbyterianism in Scotland. But after an attempted royalist coup in Scotland, known as the Incident, Charles's credibility was significantly undermined.

Ireland's population was split into three main sociopolitical groups: the Gaelic Irish, who were Catholic; the Old English, who were descended from medieval Normans and also predominantly Catholic; and the New English, who were Protestant settlers from England and Scotland aligned with the English Parliament and the Covenanters. Strafford's administration had improved the Irish economy and boosted tax revenue, but had done so by heavy-handedly imposing order. He had trained up a large Catholic army in support of the king and weakened the Irish Parliament's authority, while continuing to confiscate land from Catholics for Protestant settlement at the same time as promoting a Laudian Anglicanism that was anathema to presbyterians. As a result, all three groups had become disaffected. Strafford's impeachment provided a new departure for Irish politics whereby all sides joined to present evidence against him. In a similar manner to the English Parliament, the Old English members of the Irish Parliament argued that while opposed to Strafford they remained loyal to Charles. They argued that the king had been led astray by malign counsellors, and that, moreover, a viceroy such as Strafford could emerge as a despotic figure instead of ensuring that the king was directly involved in governance.

Strafford's fall from power weakened Charles's influence in Ireland. The dissolution of the Irish army was unsuccessfully demanded three times by the English Commons during Strafford's imprisonment, until lack of money eventually forced Charles to disband the army at the end of Strafford's trial. Disputes over the transfer of land ownership from native Catholic to settler Protestant, particularly in relation to the plantation of Ulster, coupled with resentment at moves to ensure the Irish Parliament was subordinate to the Parliament of England, sowed the seeds of rebellion. When armed conflict arose between the Gaelic Irish and New English in late October 1641, the Old English sided with the Gaelic Irish while simultaneously professing their loyalty to the king.

In November 1641, the House of Commons passed the Grand Remonstrance, a long list of grievances against actions by Charles's ministers committed since the beginning of his reign (that were asserted to be part of a grand Catholic conspiracy of which the king was an unwitting member), but it was in many ways a step too far by Pym and passed by only 11 votes, 159 to 148. Furthermore, the Remonstrance had very little support in the House of Lords, which the Remonstrance attacked. The tension was heightened by news of the Irish rebellion, coupled with inaccurate rumours of Charles's complicity. Throughout November, a series of alarmist pamphlets published stories of atrocities in Ireland, including massacres of New English settlers by the native Irish who could not be controlled by the Old English lords. Rumours of "papist" conspiracies circulated in England, and English anti-Catholic opinion was strengthened, damaging Charles's reputation and authority. The English Parliament distrusted Charles's motivations when he called for funds to put down the Irish rebellion; many members of the Commons suspected that forces he raised might later be used against Parliament itself. Pym's Militia Bill was intended to wrest control of the army from the king, but it did not have the support of the Lords, let alone Charles. Instead, the Commons passed the bill as an ordinance, which they claimed did not require royal assent. The Militia Ordinance appears to have prompted more members of the Lords to support the king. In an attempt to strengthen his position, Charles generated great antipathy in London, which was already fast falling into lawlessness, when he placed the Tower of London under the command of Colonel Thomas Lunsford, an infamous, albeit efficient, career officer. When rumours reached Charles that Parliament intended to impeach his wife for supposedly conspiring with the Irish rebels, he decided to take drastic action.

Charles suspected, probably correctly, that some members of the English Parliament had colluded with the invading Scots. On 3 January 1642, Charles directed Parliament to give up five specific members of the Commons—Pym, John Hampden, Denzil Holles, William Strode and Sir Arthur Haselrig—and one peer, Lord Mandeville, on the grounds of high treason. When Parliament refused, it was possibly Henrietta Maria who persuaded Charles to arrest the five members by force, which he resolved to do personally. But news of the warrant reached Parliament ahead of him, and the wanted men slipped away by boat shortly before Charles entered the House of Commons with an armed guard on 4 January. Having displaced Speaker William Lenthall from his chair, the king asked him where the MPs had fled. Lenthall, on his knees, famously replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." Charles abjectly declared "all my birds have flown", and was forced to retire empty-handed.

The botched arrest attempt was politically disastrous for Charles. No English sovereign had ever entered the House of Commons, and his unprecedented invasion of the chamber to arrest its members was considered a grave breach of parliamentary privilege. In one stroke Charles destroyed his supporters' efforts to portray him as a defence against innovation and disorder.

Parliament quickly seized London, and Charles fled the capital for Hampton Court Palace on 10 January, moving two days later to Windsor Castle. After sending his wife and eldest daughter to safety abroad in February, he travelled northwards, hoping to seize the military arsenal at Hull. To his dismay, he was rebuffed by the town's Parliamentary governor, Sir John Hotham, who refused him entry in April, and Charles was forced to withdraw.

In mid-1642, both sides began to arm. Charles raised an army using the medieval method of commission of array, and Parliament called for volunteers for its militia. The negotiations proved futile, and Charles raised the royal standard in Nottingham on 22 August 1642. By then, his forces controlled roughly the Midlands, Wales, the West Country and northern England. He set up his court at Oxford. Parliament controlled London, the south-east and East Anglia, as well as the English navy.

After a few skirmishes, the opposing forces met in earnest at Edgehill, on 23 October 1642. Charles's nephew Prince Rupert of the Rhine disagreed with the battle strategy of the royalist commander Lord Lindsey, and Charles sided with Rupert. Lindsey resigned, leaving Charles to assume overall command assisted by Lord Forth. Rupert's cavalry successfully charged through the parliamentary ranks, but instead of swiftly returning to the field, rode off to plunder the parliamentary baggage train. Lindsey, acting as a colonel, was wounded and bled to death without medical attention. The battle ended inconclusively as the daylight faded.

In his own words, the experience of battle had left Charles "exceedingly and deeply grieved". He regrouped at Oxford, turning down Rupert's suggestion of an immediate attack on London. After a week, he set out for the capital on 3 November, capturing Brentford on the way while simultaneously continuing to negotiate with civic and parliamentary delegations. At Turnham Green on the outskirts of London, the royalist army met resistance from the city militia, and faced with a numerically superior force, Charles ordered a retreat. He overwintered in Oxford, strengthening the city's defences and preparing for the next season's campaign. Peace talks between the two sides collapsed in April.

The war continued indecisively over the next couple of years, and Henrietta Maria returned to Britain for 17 months from February 1643. After Rupert captured Bristol in July 1643, Charles visited the port city and laid siege to Gloucester, further up the river Severn. His plan to undermine the city walls failed due to heavy rain, and on the approach of a parliamentary relief force, Charles lifted the siege and withdrew to Sudeley Castle. The parliamentary army turned back towards London, and Charles set off in pursuit. The two armies met at Newbury, Berkshire, on 20 September. Just as at Edgehill, the battle stalemated at nightfall, and the armies disengaged. In January 1644, Charles summoned a Parliament at Oxford, which was attended by about 40 peers and 118 members of the Commons; all told, the Oxford Parliament, which sat until March 1645, was supported by the majority of peers and about a third of the Commons. Charles became disillusioned by the assembly's ineffectiveness, calling it a "mongrel" in private letters to his wife.

In 1644, Charles remained in the southern half of England while Rupert rode north to relieve Newark and York, which were under threat from parliamentary and Scottish Covenanter armies. Charles was victorious at the battle of Cropredy Bridge in late June, but the royalists in the north were defeated at the battle of Marston Moor just a few days later. The king continued his campaign in the south, encircling and disarming the parliamentary army of the Earl of Essex. Returning northwards to his base at Oxford, he fought at Newbury for a second time before the winter closed in; the battle ended indecisively. Attempts to negotiate a settlement over the winter, while both sides rearmed and reorganised, were again unsuccessful.

At the battle of Naseby on 14 June 1645, Rupert's horsemen again mounted a successful charge against the flank of Parliament's New Model Army, but elsewhere on the field, opposing forces pushed Charles's troops back. Attempting to rally his men, Charles rode forward, but as he did so, Lord Carnwath seized his bridle and pulled him back, fearing for the king's safety. The royalist soldiers misinterpreted Carnwath's action as a signal to move back, leading to a collapse of their position. The military balance tipped decisively in favour of Parliament. There followed a series of defeats for the royalists, and then the siege of Oxford, from which Charles escaped (disguised as a servant) in April 1646. He put himself into the hands of the Scottish presbyterian army besieging Newark, and was taken northwards to Newcastle upon Tyne. After nine months of negotiations, the Scots finally arrived at an agreement with the English Parliament: in exchange for £100,000, and the promise of more money in the future, the Scots withdrew from Newcastle and delivered Charles to the parliamentary commissioners in January 1647.

Parliament held Charles under house arrest at Holdenby House in Northamptonshire until Cornet George Joyce took him by threat of force from Holdenby on 3 June in the name of the New Model Army. By this time, mutual suspicion had developed between Parliament, which favoured army disbandment and presbyterianism, and the New Model Army, which was primarily officered by congregationalist Independents, who sought a greater political role. Charles was eager to exploit the widening divisions, and apparently viewed Joyce's actions as an opportunity rather than a threat. He was taken first to Newmarket, at his own suggestion, and then transferred to Oatlands and subsequently Hampton Court, while more fruitless negotiations took place. By November, he determined that it would be in his best interests to escape—perhaps to France, Southern England or Berwick-upon-Tweed, near the Scottish border. He fled Hampton Court on 11 November, and from the shores of Southampton Water made contact with Colonel Robert Hammond, Parliamentary Governor of the Isle of Wight, whom he apparently believed to be sympathetic. But Hammond confined Charles in Carisbrooke Castle and informed Parliament that Charles was in his custody.

From Carisbrooke, Charles continued to try to bargain with the various parties. In direct contrast to his previous conflict with the Scottish Kirk, on 26 December 1647 he signed a secret treaty with the Scots. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles's behalf and restore him to the throne on condition that presbyterianism be established in England for three years.

The royalists rose in May 1648, igniting the Second Civil War, and as agreed with Charles, the Scots invaded England. Uprisings in Kent, Essex, and Cumberland, and a rebellion in South Wales, were put down by the New Model Army, and with the defeat of the Scots at the Battle of Preston in August 1648, the royalists lost any chance of winning the war.

Charles's only recourse was to return to negotiations, which were held at Newport on the Isle of Wight. On 5 December 1648, Parliament voted 129 to 83 to continue negotiating with the king, but Oliver Cromwell and the army opposed any further talks with someone they viewed as a bloody tyrant and were already taking action to consolidate their power. Hammond was replaced as Governor of the Isle of Wight on 27 November, and placed in the custody of the army the following day. In Pride's Purge on 6 and 7 December, the members of Parliament out of sympathy with the military were arrested or excluded by Colonel Thomas Pride, while others stayed away voluntarily. The remaining members formed the Rump Parliament. It was effectively a military coup.

Charles was moved to Hurst Castle at the end of 1648, and thereafter to Windsor Castle. In January 1649, the Rump House of Commons indicted him for treason; the House of Lords rejected the charge. The idea of trying a king was novel. The Chief Justices of the three common law courts of England—Henry Rolle, Oliver St John and John Wilde—all opposed the indictment as unlawful. The Rump Commons declared itself capable of legislating alone, passed a bill creating a separate court for Charles's trial, and declared the bill an act without the need for royal assent. The High Court of Justice established by the Act consisted of 135 commissioners, but many either refused to serve or chose to stay away. Only 68 (all firm Parliamentarians) attended Charles's trial on charges of high treason and "other high crimes" that began on 20 January 1649 in Westminster Hall. John Bradshaw acted as President of the Court, and the prosecution was led by Solicitor General John Cook.

Charles was accused of treason against England by using his power to pursue his personal interest rather than the good of the country. The charge stated that he was devising "a wicked design to erect and uphold in himself an unlimited and tyrannical power to rule according to his will, and to overthrow the rights and liberties of the people". In carrying this out he had "traitorously and maliciously levied war against the present Parliament, and the people therein represented", and that the "wicked designs, wars, and evil practices of him, the said Charles Stuart, have been, and are carried on for the advancement and upholding of a personal interest of will, power, and pretended prerogative to himself and his family, against the public interest, common right, liberty, justice, and peace of the people of this nation." Presaging the modern concept of command responsibility, the indictment held him "guilty of all the treasons, murders, rapines, burnings, spoils, desolations, damages and mischiefs to this nation, acted and committed in the said wars, or occasioned thereby." An estimated 300,000 people, or 6% of the population, died during the war.

Over the first three days of the trial, whenever Charles was asked to plead, he refused, stating his objection with the words: "I would know by what power I am called hither, by what lawful authority...?" He claimed that no court had jurisdiction over a monarch, that his own authority to rule had been given to him by God and by the traditional laws of England, and that the power wielded by those trying him was only that of force of arms. Charles insisted that the trial was illegal, explaining that, The court, by contrast, challenged the doctrine of sovereign immunity and proposed that "the King of England was not a person, but an office whose every occupant was entrusted with a limited power to govern 'by and according to the laws of the land and not otherwise'."

At the end of the third day, Charles was removed from the court, which then heard over 30 witnesses against him in his absence over the next two days, and on 26 January condemned him to death. The next day, the king was brought before a public session of the commission, declared guilty, and sentenced. The judgement read, "For all which treasons and crimes this court doth adjudge that he, the said Charles Stuart, as a tyrant, traitor, murderer, and public enemy to the good people of this nation, shall be put to death by the severing of his head from his body." Fifty-nine of the commissioners signed Charles's death warrant.

Charles's execution was scheduled for Tuesday, 30 January 1649. Two of his children remained in England under the control of the Parliamentarians: Elizabeth and Henry. They were permitted to visit him on 29 January, and he bade them a tearful farewell. The next morning, he called for two shirts to prevent the cold weather causing any noticeable shivers that the crowd could have mistaken for fear: "the season is so sharp as probably may make me shake, which some observers may imagine proceeds from fear. I would have no such imputation."

He walked under guard from St James's Palace, where he had been confined, to the Palace of Whitehall, where an execution scaffold had been erected in front of the Banqueting House. Charles was separated from spectators by large ranks of soldiers, and his last speech reached only those with him on the scaffold. He blamed his fate on his failure to prevent the execution of his loyal servant Strafford: "An unjust sentence that I suffered to take effect, is punished now by an unjust sentence on me." He declared that he had desired the liberty and freedom of the people as much as any, "but I must tell you that their liberty and freedom consists in having government ... It is not their having a share in the government; that is nothing appertaining unto them. A subject and a sovereign are clean different things." He continued, "I shall go from a corruptible to an incorruptible Crown, where no disturbance can be."

At about 2:00 p.m., Charles put his head on the block after saying a prayer and signalled the executioner when he was ready by stretching out his hands; he was then beheaded in one clean stroke. According to observer Philip Henry, a moan "as I never heard before and desire I may never hear again" rose from the assembled crowd, some of whom then dipped their handkerchiefs in the king's blood as a memento.

The executioner was masked and disguised, and there is debate over his identity. The commissioners approached Richard Brandon, the common hangman of London, but he refused, at least at first, despite being offered £200 – a considerably large sum for the time. It is possible he relented and undertook the commission after being threatened with death, but others have been named as potential candidates, including George Joyce, William Hulet and Hugh Peters. The clean strike, confirmed by an examination of the king's body at Windsor in 1813, suggests that the execution was carried out by an experienced headsman.

It was common practice for the severed head of a traitor to be held up and exhibited to the crowd with the words "Behold the head of a traitor!" Charles's head was exhibited, but those words were not used, possibly because the executioner did not want his voice recognised. On the day after the execution, the king's head was sewn back onto his body, which was then embalmed and placed in a lead coffin.

The commission refused to allow Charles's burial at Westminster Abbey, so his body was conveyed to Windsor on the night of 7 February. He was buried in private on 9 February 1649 in the Henry VIII vault in the chapel's quire, alongside the coffins of Henry VIII and Henry's third wife, Jane Seymour, in St George's Chapel, Windsor Castle. The king's son, Charles II, later planned for an elaborate royal mausoleum to be erected in Hyde Park, London, but it was never built.

Ten days after Charles's execution, on the day of his interment, a memoir purportedly written by him appeared for sale. This book, the "Eikon Basilike" (Greek for the "Royal Portrait"), contained an "apologia" for royal policies, and proved an effective piece of royalist propaganda. John Milton wrote a Parliamentary rejoinder, the "Eikonoklastes" ("The Iconoclast"), but the response made little headway against the pathos of the royalist book. Anglicans and royalists fashioned an image of martyrdom, and in the Convocations of Canterbury and York of 1660 King Charles the Martyr was added to the Church of England's liturgical calendar. High church Anglicans held special services on the anniversary of his death. Churches, such as those at Falmouth and Tunbridge Wells, and Anglican devotional societies such as the Society of King Charles the Martyr, were founded in his honour.

With the monarchy overthrown, England became a republic or "Commonwealth". The House of Lords was abolished by the Rump Commons, and executive power was assumed by a Council of State. All significant military opposition in Britain and Ireland was extinguished by the forces of Oliver Cromwell in the Anglo-Scottish War and the Cromwellian conquest of Ireland. Cromwell forcibly disbanded the Rump Parliament in 1653, thereby establishing the Protectorate with himself as Lord Protector. Upon his death in 1658, he was briefly succeeded by his ineffective son, Richard. Parliament was reinstated, and the monarchy was restored to Charles I's eldest son, Charles II, in 1660.

Charles's unprecedented 1642 invasion of the House of Commons' chamber, a grave violation of the liberties of Parliament, and his unsuccessful attempt to arrest five Members of Parliament is commemorated annually at the State Opening of Parliament.

Partly inspired by his visit to the Spanish court in 1623, Charles became a passionate and knowledgeable art collector, amassing one of the finest art collections ever assembled. In Spain, he sat for a sketch by Velázquez, and acquired works by Titian and Correggio, among others. In England, his commissions included the ceiling of the Banqueting House, Whitehall, by Rubens and paintings by other artists from the Low Countries such as van Honthorst, Mytens, and van Dyck. His close associates, including the Duke of Buckingham and the Earl of Arundel, shared his interest and have been dubbed the Whitehall Group. In 1627 and 1628, Charles purchased the entire collection of the Duke of Mantua, which included work by Titian, Correggio, Raphael, Caravaggio, del Sarto and Mantegna. His collection grew further to encompass Bernini, Bruegel, Leonardo, Holbein, Hollar, Tintoretto and Veronese, and self-portraits by both Dürer and Rembrandt. By Charles's death, there were an estimated 1,760 paintings, most of which were sold and dispersed by Parliament.

In the words of John Philipps Kenyon, "Charles Stuart is a man of contradictions and controversy". Revered by high Tories who considered him a saintly martyr, he was condemned by Whig historians, such as Samuel Rawson Gardiner, who thought him duplicitous and delusional. In recent decades, most historians have criticised him, the main exception being Kevin Sharpe, who offered a more sympathetic view that has not been widely adopted. Sharpe argued that the king was a dynamic man of conscience, but Barry Coward thought Charles "the most incompetent monarch of England since Henry VI", a view shared by Ronald Hutton, who called him "the worst king we have had since the Middle Ages".

Archbishop William Laud, whom Parliament beheaded during the war, described Charles as "A mild and gracious prince who knew not how to be, or how to be made, great." Charles was more sober and refined than his father, but he was intransigent. He deliberately pursued unpopular policies that brought ruin on himself. Both Charles and James were advocates of the divine right of kings, but while James's ambitions concerning absolute prerogative were tempered by compromise and consensus with his subjects, Charles believed he had no need to compromise or even to explain his actions. He thought he was answerable only to God. "Princes are not bound to give account of their actions," he wrote, "but to God alone".


The official style of Charles I as king in England was "Charles, by the Grace of God, King of England, Scotland, France and Ireland, Defender of the Faith, etc." The style "of France" was only nominal, and was used by every English monarch from Edward III to George III, regardless of the amount of French territory actually controlled. The authors of his death warrant called him "Charles Stuart, King of England".


As Duke of York, Charles bore the royal arms of the kingdom differenced by a label Argent of three points, each bearing three torteaux Gules. As the Prince of Wales, he bore the royal arms differenced by a plain label Argent of three points. As king, Charles bore the royal arms undifferenced: Quarterly, I and IV Grandquarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland). In Scotland, the Scottish arms were placed in the first and fourth quarters with the English and French arms in the second quarter.

Charles had nine children, two of whom eventually succeeded as king, and two of whom died at or shortly after birth.




Counter-Strike (video game)

Counter-Strike (also known as Half-Life: Counter-Strike or Counter-Strike 1.6) is a tactical first-person shooter game developed by Valve. It was initially developed and released as a "Half-Life" modification by Minh "Gooseman" Le and Jess Cliffe in 1999, before Le and Cliffe were hired and the game's intellectual property acquired. "Counter-Strike" was released by Valve for Microsoft Windows in November 2000, and is the first installment in the "Counter-Strike" series. Several remakes and ports were released on Xbox, as well as OS X and Linux.

Set in various locations around the globe, players assume the roles of counter-terrorist forces and terrorist militants opposing them. During each round of gameplay, the two teams are tasked with defeating the other by the means of either achieving the map's objectives or eliminating all of the enemy combatants. Each player may customize their arsenal of weapons and accessories at the beginning of every match, with currency being earned after the end of each round.

"Counter-Strike" is an objective-based first-person shooter in which players are divided into the terrorist team and the counter-terrorist team. Each team attempts to complete their mission objective or eliminate the opposing team. Each round starts with the two teams spawning simultaneously. 

The objectives vary depending on the type of map, and these are the most common:

Players are generally given a few seconds before the round begins to prepare and buy equipment, during which they cannot attack or move. Once the round has ended, surviving players retain their equipment for use in the next round; players who were killed begin the next round with the basic default starting equipment and must buy new weapons.

Standard monetary bonuses are awarded for winning or losing a round, killing an enemy, being the first to instruct a hostage to follow, rescuing a hostage, planting the bomb (as Terrorist) or defusing the bomb (as Counter-Terrorist).

"Counter-Strike" began as a mod of "Half-Life"s engine GoldSrc. Minh Le, the mod's co-creator, had started his last semester at university, and wanted to do something in game development to help give him better job prospects. Throughout university, Le had worked on mods with the "Quake" engine, and on looking for this latest project, wanted to try something new and opted for GoldSrc. At the onset, Valve had not yet released the software development kit (SDK) for GoldSrc but affirmed it would be available in a few months, allowing Le to work on the character models in the interim. Once the GoldSrc SDK was available, Le estimated it took him about a month and a half to complete the programming and integrate his models for "Beta One" of "Counter-Strike". To assist, Le had help from Jess Cliffe who managed the game's website and community, and had contacts within level map making community to help build some of the levels for the game. The theme of countering terrorists was inspired by Le's own interest in guns and the military, and from games like "Rainbow Six" and "Spec Ops".

Le and Cliffe continued to release Betas on a frequent basis for feedback. The initial few Betas, released starting in June 1999, had limited audiences but by the fifth one, interest in the project dramatically grew. The interest in the game drew numerous players to the website, which helped Le and Cliffe to make revenue from ads hosted on the site. Around 2000 at the time of Beta 5's release, the two were approached by Valve, offering to buy the "Counter-Strike" intellectual property and offering both jobs to continue its development. Both accepted the offer, and by September 2000, Valve released the first non-beta version of the game. While Cliffe stayed with Valve, Le did some additional work towards a "Counter-Strike 2.0" based on Valve's upcoming Source engine, but left to start his own studio after Valve opted to shelve the sequel.

"Counter-Strike" itself is a mod, and it has developed its own community of script writers and mod creators. Some mods add bots, while others remove features of the game, and others create different modes of play. Some mods, often called "admin plugins", give server administrators more flexible and efficient control over their servers. There are some mods which affect gameplay heavily, such as Gun Game, where players start with a basic pistol and must score kills to receive better weapons, and Zombie Mod, where one team consists of zombies and must "spread the infection" by killing the other team (using only the knife). There are also Superhero mods which mix the first-person gameplay of "Counter-Strike" with an experience system, allowing a player to become more powerful as they continue to play. The game is highly customizable on the player's end, allowing the user to install or even create their own custom skins, HUDs, spray graphics, sprites, and sound effects, given the proper tools.

"Counter-Strike" has been a target for cheating in online games since its release. In-game, cheating is often referred to as "hacking" in reference to programs or "hacks" executed by the client. Valve has implemented an anti-cheat system called Valve Anti-Cheat (VAC). Players cheating on a VAC-enabled server risk having their account permanently banned from all VAC-secured servers.

With the first version of VAC, a ban took hold almost instantly after being detected and the cheater had to wait two years to have the account unbanned. Since VAC's second version, cheaters are not banned automatically. With the second version, Valve instituted a policy of 'delayed bans,' the theory being that if a new hack is developed which circumvents the VAC system, it will spread amongst the 'cheating' community. By delaying the initial ban, Valve hopes to identify and ban as many cheaters as possible. Like any software detection system, some cheats are not detected by VAC. To remedy this, some servers implement a voting system, in which case players can call for a vote to kick or ban the accused cheater. VAC's success at identifying cheats and banning those who use them has also provided a boost in the purchasing of private cheats. These cheats are updated frequently to minimize the risk of detection, and are generally only available to a trusted list of recipients who collectively promise not to reveal the underlying design. Even with private cheats however, some servers have alternative anticheats to coincide with VAC itself. This can help with detecting some cheaters, but most paid for cheats are designed to bypass these alternative server-based anticheats.

When "Counter-Strike" was published by Sierra Studios, it was bundled with "Team Fortress Classic", "" multiplayer, and the "Wanted", "Half-Life: Absolute Redemption" and "Firearms" mods.

On March 24, 1999, Planet Half-Life opened its "Counter-Strike" section. Within two weeks, the site had received 10,000 hits. On June 19, 1999, the first public beta of "Counter-Strike" was released, followed by numerous further "beta" releases. On April 12, 2000, Valve announced that the "Counter-Strike" developers and Valve had teamed up. In January 2013, Valve began testing a version of "Counter-Strike" for OS X and Linux, eventually releasing the update to all users in April 2013.

An unofficial browser version was released in 2023 on a Russian website.

Upon its retail release, "Counter-Strike" received highly favorable reviews. In 2003, "Counter-Strike" was inducted into GameSpot's list of the greatest games of all time. "The New York Times" reported that E-Sports Entertainment ESEA League started the first professional fantasy e-sports league in 2004 with the game "Counter-Strike". Some credit the move into professional competitive team play with prizes as a major factor in "Counter-Strike" longevity and success.

Global retail sales of "Counter-Strike" surpassed 250,000 units by July 2001. The game sold 1.5 million by February 2003 and generated $40 million in revenue. In the United States, its retail version sold 550,000 copies and earned $15.7 million by August 2006, after its release in November 2000. It was the country's 22nd best-selling PC game between January 2000 and August 2006.

The Xbox version sold 1.5 million copies in total.

On January 17, 2008, a Brazilian federal court order prohibiting all sales of "Counter-Strike" and "EverQuest" began to be enforced. The federal Brazilian judge Carlos Alberto Simões de Tomaz ordered the ban in October 2007 because, as argued by the judge, the games "bring imminent stimulus to the subversion of the social order, attempting against the democratic state and the law and against public security." As of June 18, 2009, a regional federal court order lifting the prohibition on the sale of "Counter-Strike" was published. The game is now being sold again in Brazil.

The original "Counter-Strike" has been played in tournaments since 2000 with the first major being hosted in 2001 at the Cyberathlete Professional League Winter Championship. The first official sequel was "", released on November 1, 2004. The game was criticized by the competitive community, who believed the game's skill ceiling was significantly lower than that of CS 1.6. This caused a divide in the competitive community as to which game to play competitively.

Following the success of the first "Counter-Strike", Valve went on to make multiple sequels to the game. "", a game using "Counter-Strike"s GoldSrc engine, was released in 2004. "", a remake of the original "Counter-Strike", was the first in the series to use Valve's Source engine and was also released in 2004, eight months after the release of "Counter-Strike: Condition Zero". The next game in the "Counter-Strike" series to be developed primarily by Valve was "", released for Windows, OS X, Linux, PlayStation 3, and Xbox 360 in 2012.

The game spawned multiple spin-offs for the Asian gaming market. The first, "Counter-Strike Neo", was an arcade game developed by Namco and released in Japan in 2003. In 2008, Nexon Corporation released "Counter-Strike Online", a free-to-play instalment in the series monetized via microtransactions. "Counter-Strike Online" was followed by "Counter-Strike Online 2" in 2013. In 2014, Nexon released "" worldwide via Steam.


Camille Pissarro

Jacob Abraham Camille Pissarro ( , ; 10 July 1830 – 13 November 1903) was a Danish-French Impressionist and Neo-Impressionist painter born on the island of St Thomas (now in the US Virgin Islands, but then in the Danish West Indies). His importance resides in his contributions to both Impressionism and Post-Impressionism. Pissarro studied from great forerunners, including Gustave Courbet and Jean-Baptiste-Camille Corot. He later studied and worked alongside Georges Seurat and Paul Signac when he took on the Neo-Impressionist style at the age of 54.

In 1873 he helped establish a collective society of fifteen aspiring artists, becoming the "pivotal" figure in holding the group together and encouraging the other members. Art historian John Rewald called Pissarro the "dean of the Impressionist painters", not only because he was the oldest of the group, but also "by virtue of his wisdom and his balanced, kind, and warmhearted personality". Paul Cézanne said "he was a father for me. A man to consult and a little like the good Lord", and he was also one of Paul Gauguin's masters. Pierre-Auguste Renoir referred to his work as "revolutionary", through his artistic portrayals of the "common man", as Pissarro insisted on painting individuals in natural settings without "artifice or grandeur".

Pissarro is the only artist to have shown his work at all eight Paris Impressionist exhibitions, from 1874 to 1886. He "acted as a father figure not only to the Impressionists" but to all four of the major Post-Impressionists, Cézanne, Seurat, Gauguin, and van Gogh.

Jacob Abraham Camille Pissarro was born on 10 July 1830 on the island of St. Thomas to Frederick Abraham Gabriel Pissarro and Rachel Manzano-Pomié. His father was of Portuguese Jewish descent and held French nationality. His mother was from a French-Jewish family from St. Thomas. His father was a merchant who came to the island from France to deal with the hardware store of a deceased uncle, Isaac Petit, and married his widow. The marriage caused a stir within St. Thomas's small Jewish community because she was previously married to Frederick's uncle and according to Jewish law a man is forbidden from marrying his aunt. In subsequent years his four children attended the all-black primary school. Upon his death, his will specified that his estate be split equally between the synagogue and St. Thomas' Protestant church.

When Pissarro was twelve his father sent him to boarding school in France. He studied at the Savary Academy in Passy near Paris. While a young student, he developed an early appreciation of the French art masters. Monsieur Savary himself gave him a strong grounding in drawing and painting and suggested he draw from nature when he returned to St. Thomas.

After his schooling, Pissarro returned to St. Thomas at the age of sixteen or seventeen, where his father advocated Pissarro to work in his business as a port clerk. Nevertheless, Pissarro took every opportunity during those next five years at the job to practice drawing during breaks and after work.

Visual theorist Nicholas Mirzoeff claims that the young Pissarro was inspired by the artworks of James Gay Sawkins, a British painter and geologist who lived in Charlotte Amalie, St. Thomas circa 1847. Pissarro may have attended art classes taught by Sawkins and seen Sawkins's paintings of Mitla, Mexico. Mirzoeff states, "A formal analysis suggests that [Sawkins's] work influenced the young Pissarro, who had just returned to the island from his school in France. Soon afterward, Pissarro began his own drawings of the local African population in apparent imitation of Sawkins," creating "sketches for a postslavery imagination."

When Pissarro turned twenty-one, Danish artist Fritz Melbye, then living on St. Thomas, inspired him to take on painting as a full-time profession, becoming his teacher and friend. Pissarro then chose to leave his family and job and live in Venezuela, where he and Melbye spent the next two years working as artists in Caracas and La Guaira. He drew everything he could, including landscapes, village scenes, and numerous sketches, enough to fill up multiple sketchbooks.

In 1855, Pissarro moved back to Paris where he began working as an assistant to Anton Melbye, Fritz Melbye's brother and also a painter. He also studied paintings by other artists whose style impressed him: Courbet, Charles-François Daubigny, Jean-François Millet, and Corot. He also enrolled in various classes taught by masters, at schools such as École des Beaux-Arts and Académie Suisse. But Pissarro eventually found their teaching methods "stifling," states art historian John Rewald. This prompted him to search for alternative instruction, which he requested and received from Corot.

His initial paintings were in accord with the standards at the time to be displayed at the Paris Salon, the official body whose academic traditions dictated the kind of art that was acceptable. The Salon's annual exhibition was essentially the only marketplace for young artists to gain exposure. As a result, Pissarro worked in the traditional and prescribed manner to satisfy the tastes of its official committee.

In 1859 his first painting was accepted and exhibited. His other paintings during that period were influenced by Camille Corot, who tutored him. He and Corot both shared a love of rural scenes painted from nature. It was by Corot that Pissarro was inspired to paint outdoors, also called "plein air" painting. Pissarro found Corot, along with the work of Gustave Courbet, to be "statements of pictorial truth," writes Rewald. He discussed their work often. Jean-François Millet was another whose work he admired, especially his "sentimental renditions of rural life".

During this period Pissarro began to understand and appreciate the importance of expressing on canvas the beauties of nature without adulteration. After a year in Paris, he therefore began to leave the city and paint scenes in the countryside to capture the daily reality of village life. He found the French countryside to be "picturesque," and worthy of being painted. It was still mostly agricultural and sometimes called the "golden age of the peasantry". Pissarro later explained the technique of painting outdoors to a student:

Corot would complete his paintings back in his studio, often revising them according to his preconceptions. Pissarro, however, preferred to finish his paintings outdoors, often at one sitting, which gave his work a more realistic feel. As a result, his art was sometimes criticised as being "vulgar," because he painted what he saw: "rutted and edged hodgepodge of bushes, mounds of earth, and trees in various stages of development." According to one source, such details were equivalent to today's art showing garbage cans or beer bottles on the side of a street. This difference in style created disagreements between Pissarro and Corot.

In 1859, while attending the free school, the Académie Suisse, Pissarro became friends with a number of younger artists who likewise chose to paint in the more realistic style. Among them were Claude Monet, Armand Guillaumin and Paul Cézanne. What they shared in common was their dissatisfaction with the dictates of the Salon. Cézanne's work had been mocked at the time by the others in the school, and, writes Rewald, in his later years Cézanne "never forgot the sympathy and understanding with which Pissarro encouraged him." As a part of the group, Pissarro was comforted from knowing he was not alone, and that others similarly struggled with their art.

Pissarro agreed with the group about the importance of portraying individuals in natural settings, and expressed his dislike of any artifice or grandeur in his works, despite what the Salon demanded for its exhibits. In 1863 almost all of the group's paintings were rejected by the Salon, and French Emperor Napoleon III instead decided to place their paintings in a separate exhibit hall, the Salon des Refusés. However, only works of Pissarro and Cézanne were included, and the separate exhibit brought a hostile response from both the officials of the Salon and the public.

In subsequent Salon exhibits of 1865 and 1866, Pissarro acknowledged his influences from Melbye and Corot, whom he listed as his masters in the catalogue. But in the exhibition of 1868 he no longer credited other artists as an influence, in effect declaring his independence as a painter. This was noted at the time by art critic and author Émile Zola, who offered his opinion:

Another writer tries to describe elements of Pissarro's style:
And though, on orders from the hanging Committee and the Marquis de Chennevières, Pissarro's paintings of Pontoise for example had been skyed, hung near the ceiling, this did not prevent Jules-Antoine Castagnary from noting that the qualities of his paintings had been observed by art lovers. At the age of thirty-eight, Pissarro had begun to win himself a reputation as a landscapist to rival Corot and Daubigny.

In the late 1860s or early 1870s, Pissarro became fascinated with Japanese prints, which influenced his desire to experiment in new compositions. He described the art to his son Lucien:

In 1871 in Croydon, England, he married his mother's maid, Julie Vellay, a vineyard grower's daughter, with whom he had seven children, six of which would become painters: Lucien Pissarro (1863–1944), Georges Henri Manzana Pissarro (1871–1961), Félix Pissarro (1874–1897), (1878–1952), (1881–1948), and Paul-Émile Pissarro (1884–1972). They lived outside Paris in Pontoise and later in Louveciennes, both of which places inspired many of his paintings including scenes of village life, along with rivers, woods, and people at work. He also kept in touch with the other artists of his earlier group, especially Monet, Renoir, Cézanne, and Frédéric Bazille.

After the outbreak of the Franco-Prussian War of 1870–71, having only Danish nationality and being unable to join the army, he moved his family to Norwood, then a village on the edge of London. However, his style of painting, which was a forerunner of what was later called "Impressionism", did not do well. He wrote to his friend, Théodore Duret, that "my painting doesn't catch on, not at all ..."

Pissarro met the Paris art dealer Paul Durand-Ruel, in London, who became the dealer who helped sell his art for most of his life. Durand-Ruel put him in touch with Monet who was likewise in London during this period. They both viewed the work of British landscape artists John Constable and J. M. W. Turner, which confirmed their belief that their style of open air painting gave the truest depiction of light and atmosphere, an effect that they felt could not be achieved in the studio alone. Pissarro's paintings also began to take on a more spontaneous look, with loosely blended brushstrokes and areas of impasto, giving more depth to the work.

Through the paintings Pissarro completed at this time, he records Sydenham and the Norwoods at a time when they were just recently connected by railways, but prior to the expansion of suburbia. One of the largest of these paintings is a view of "St. Bartholomew's Church" at Lawrie Park Avenue, commonly known as "", in the collection of the National Gallery in London. Twelve oil paintings date from his stay in Upper Norwood and are listed and illustrated in the catalogue raisonné prepared jointly by his fifth child Ludovic-Rodolphe Pissarro and Lionello Venturi and published in 1939. These paintings include "Lower Norwood Under Snow", and "Lordship Lane Station", views of The Crystal Palace relocated from Hyde Park, "Dulwich College", "Sydenham Hill", "All Saints Church Upper Norwood", and a lost painting of St. Stephen's Church.

Returning to France, Pissarro lived in Pontoise from 1872 to 1884. In 1890 he again visited England and painted some ten scenes of central London. He came back again in 1892, painting in Kew Gardens and Kew Green, and also in 1897, when he produced several oils described as being of Bedford Park, Chiswick, but in fact all being of the nearby Stamford Brook area except for one of Bath Road, which runs from Stamford Brook along the south edge of Bedford Park.

When Pissarro returned to his home in France after the war, he discovered that of the 1,500 paintings he had done over 20 years, which he was forced to leave behind when he moved to London, only 40 remained. The rest had been damaged or destroyed by the soldiers, who often used them as floor mats outside in the mud to keep their boots clean. It is assumed that many of those lost were done in the Impressionist style he was then developing, thereby "documenting the birth of Impressionism." Armand Silvestre, a critic, went so far as to call Pissarro "basically the inventor of this [Impressionist] painting"; however, Pissarro's role in the Impressionist movement was "less that of the great man of ideas than that of the good counselor and appeaser ..." "Monet ... could be seen as the guiding force."

He soon reestablished his friendships with the other Impressionist artists of his earlier group, including Cézanne, Monet, Manet, Renoir, and Degas. Pissarro now expressed his opinion to the group that he wanted an alternative to the Salon so their group could display their own unique styles.

To assist in that endeavour, in 1873 he helped establish a separate collective, called the "Société Anonyme des Artistes, Peintres, Sculpteurs et Graveurs," which included fifteen artists. Pissarro created the group's first charter and became the "pivotal" figure in establishing and holding the group together. One writer noted that with his prematurely grey beard, the forty-three-year-old Pissarro was regarded as a "wise elder and father figure" by the group. Yet he was able to work alongside the other artists on equal terms due to his youthful temperament and creativity. Another writer said of him that "he has unchanging spiritual youth and the look of an ancestor who remained a young man".

The following year, in 1874, the group held their First Impressionist Exhibition, which shocked and "horrified" the critics, who primarily appreciated only scenes portraying religious, historical, or mythological settings. They found fault with the Impressionist paintings on many grounds:

Pissarro showed five of his paintings, all landscapes, at the exhibit, and again Émile Zola praised his art and that of the others. In the Impressionist exhibit of 1876, however, art critic Albert Wolff complained in his review, "Try to make M. Pissarro understand that trees are not violet, that sky is not the color of fresh butter ..." Journalist and art critic Octave Mirbeau on the other hand, writes, "Camille Pissarro has been a revolutionary through the revitalized working methods with which he has endowed painting".
According to Rewald, Pissarro had taken on an attitude more simple and natural than the other artists. He writes:

In later years, Cézanne also recalled this period and referred to Pissarro as "the first Impressionist". In 1906, a few years after Pissarro's death, Cézanne, then 67 and a role model for the new generation of artists, paid Pissarro a debt of gratitude by having himself listed in an exhibition catalogue as "Paul Cézanne, pupil of Pissarro".

Pissarro, Degas, and American impressionist Mary Cassatt planned a journal of their original prints in the late 1870s, a project that nevertheless came to nothing when Degas withdrew. Art historian and the artist's great-grandson Joachim Pissarro notes that they "professed a passionate disdain for the Salons and refused to exhibit at them." Together they shared an "almost militant resolution" against the Salon, and through their later correspondences it is clear that their mutual admiration "was based on a kinship of ethical as well as aesthetic concerns".

Cassatt had befriended Degas and Pissarro years earlier when she joined Pissarro's newly formed French Impressionist group and gave up opportunities to exhibit in the United States. She and Pissarro were often treated as "two outsiders" by the Salon since neither were French or had become French citizens. However, she was "fired up with the cause" of promoting Impressionism and looked forward to exhibiting "out of solidarity with her new friends". Towards the end of the 1890s she began to distance herself from the Impressionists, avoiding Degas at times as she did not have the strength to defend herself against his "wicked tongue". Instead, she came to prefer the company of "the gentle Camille Pissarro", with whom she could speak frankly about the changing attitudes toward art. She once described him as a teacher "that could have taught the stones to draw correctly."

By the 1880s, Pissarro began to explore new themes and methods of painting to break out of what he felt was an artistic "mire". As a result, Pissarro went back to his earlier themes by painting the life of country people, which he had done in Venezuela in his youth. Degas described Pissarro's subjects as "peasants working to make a living".

However, this period also marked the end of the Impressionist period due to Pissarro's leaving the movement. As Joachim Pissarro points out:

"Once such a die-hard Impressionist as Pissarro had turned his back on Impressionism, it was apparent that Impressionism had no chance of surviving ..."

It was Pissarro's intention during this period to help "educate the public" by painting people at work or at home in realistic settings, without idealising their lives. Pierre-Auguste Renoir, in 1882, referred to Pissarro's work during this period as "revolutionary," in his attempt to portray the "common man." Pissarro himself did not use his art to overtly preach any kind of political message, however, although his preference for painting humble subjects was intended to be seen and purchased by his upper class clientele. He also began painting with a more unified brushwork along with pure strokes of color.

In 1885 he met Georges Seurat and Paul Signac, both of whom relied on a more "scientific" theory of painting by using very small patches of pure colours to create the illusion of blended colours and shading when viewed from a distance. Pissarro then spent the years from 1885 to 1888 practising this more time-consuming and laborious technique, referred to as pointillism. The paintings that resulted were distinctly different from his Impressionist works, and were on display in the 1886 Impressionist Exhibition, but under a separate section, along with works by Seurat, Signac, and his son Lucien.

All four works were considered an "exception" to the eighth exhibition. Joachim Pissarro notes that virtually every reviewer who commented on Pissarro's work noted "his extraordinary capacity to change his art, revise his position and take on new challenges." One critic writes:

Pissarro explained the new art form as a "phase in the logical march of Impressionism", but he was alone among the other Impressionists with this attitude, however. Joachim Pissarro states that Pissarro thereby became the "only artist who went from Impressionism to Neo-Impressionism".

In 1884, art dealer Theo van Gogh asked Pissarro if he would take in his older brother, Vincent, as a boarder in his home. Lucien Pissarro wrote that his father was impressed by Van Gogh's work and had "foreseen the power of this artist", who was 23 years younger. Although Van Gogh never boarded with him, Pissarro did explain to him the various ways of finding and expressing light and color, ideas which he later used in his paintings, notes Lucien.

Pissarro eventually turned away from Neo-Impressionism, claiming its system was too artificial. He explains in a letter to a friend:

However, after reverting to his earlier style, his work became, according to Rewald, "more subtle, his color scheme more refined, his drawing firmer ... So it was that Pissarro approached old age with an increased mastery."

But the change also added to Pissarro's continual financial hardship which he felt until his 60s. His "headstrong courage and a tenacity to undertake and sustain the career of an artist", writes Joachim Pissarro, was due to his "lack of fear of the immediate repercussions" of his stylistic decisions. In addition, his work was strong enough to "bolster his morale and keep him going", he writes. His Impressionist contemporaries, however, continued to view his independence as a "mark of integrity", and they turned to him for advice, referring to him as "Père Pissarro" (father Pissarro).

In his older age Pissarro suffered from a recurring eye infection that prevented him from working outdoors except in warm weather. As a result of this disability, he began painting outdoor scenes while sitting by the window of hotel rooms. He often chose hotel rooms on upper levels to get a broader view. He moved around northern France and painted from hotels in Rouen, Paris, Le Havre and Dieppe. On his visits to London, he would do the same.

Pissarro died in Paris on 13 November 1903 and was buried in Père Lachaise Cemetery.

During the period Pissarro exhibited his works, art critic Armand Silvestre had called Pissarro the "most real and most naive member" of the Impressionist group. His work has also been described by art historian Diane Kelder as expressing "the same quiet dignity, sincerity, and durability that distinguished his person." She adds that "no member of the group did more to mediate the internecine disputes that threatened at times to break it apart, and no one was a more diligent proselytizer of the new painting."

According to Pissarro's son, Lucien, his father painted regularly with Cézanne beginning in 1872. He recalls that Cézanne walked a few miles to join Pissarro at various settings in Pontoise. While they shared ideas during their work, the younger Cézanne wanted to study the countryside through Pissarro's eyes, as he admired Pissarro's landscapes from the 1860s. Cézanne, although only nine years younger than Pissarro, said that "he was a father for me. A man to consult and a little like the good Lord."

Lucien Pissarro was taught painting by his father, and described him as a "splendid teacher, never imposing his personality on his pupil." Gauguin, who also studied under him, referred to Pissarro "as a force with which future artists would have to reckon". Art historian Diane Kelder notes that it was Pissarro who introduced Gauguin, who was then a young stockbroker studying to become an artist, to Degas and Cézanne. Gauguin, near the end of his career, wrote a letter to a friend in 1902, shortly before Pissarro's death:

The American impressionist Mary Cassatt, who at one point lived in Paris to study art, and joined his Impressionist group, noted that he was "such a teacher that he could have taught the stones to draw correctly."

Caribbean author and scholar Derek Walcott based his book-length poem, "Tiepolo's Hound" (2000), on Pissarro's life.

During the early 1930s throughout Europe, Jewish owners of numerous fine art masterpieces found themselves forced to give up or sell off their collections for minimal prices due to anti-Jewish laws created by the new Nazi regime. Many Jews were forced to flee Germany starting in 1933, and then, as the Nazis expanded their hold over all of Europe, Austria, France, Holland, Poland, Italy and other countries. The Nazis created special looting organizations like the Reichsleiter Rosenberg Taskforce whose mission it was to seize Jewish property notably valuable artworks. When those forced into exile or deported to extermination camps owned valuables, including artwork, they were often sold to finance the Nazi war effort, sent to Hitler's personal museum, traded or seized by officials for personal gain. Several artworks by Pissarro were looted from their Jewish owners in Germany, France and elsewhere by the Nazis.

Pissarro's "Shepherdess Bringing Home the Sheep" (La Bergère Rentrant des Moutons") was looted from the Jewish art collectors Yvonne et Raoul Meyer in France in 1941 and transited via Switzerland and New York before entering the Fred Jones Jr Museum at the University of Oklahoma. In 2014, Meyer's daughter, Léonie-Noëlle Meyer filed a restitution claim which resulted in years of court battle. The lawsuit resulted in the recognition of Meyer's ownership and its transfer to France for five years, coupled with an agreement to shuttle the painting back and forth between Paris and Oklahoma every three years after that. However, in 2020 Meyer filed suit in a French court to challenge the accord. After Fred Jones Jr Museum sued Meyer requesting heavy financial penalties, the Holocaust survivor abandoned her effort to recover the Pissarro, saying, "I have no other choice.

Pissarro's Picking Peas (La Cueillette) was looted from Jewish businessman Simon Bauer, in addition to 92 other artworks seized in 1943 by the Vichy collaborationist regime in France.

Pissarro's "Sower And Ploughman," was owned by Dr Henri Hinrichsen, a Jewish music publisher from Leipzig, until 11 January 1940, when he was forced to relinquish the painting to Hildebrand Gurlitt in Nazi-occupied Brussels, before being murdered in Auschwitz in September 1942.

Pissarro's "Le Quai Malaquais, Printemps", owned by German Jewish publisher Samuel Fischer, founder of the famous S. Fischer Verlag, passed through the hands of infamous Nazi art looter Bruno Lohse.

Pissarro's "Le Boulevard de Montmartre, Matinée de Printemps", owned by Max Silberberg, a German Jewish industrialist whose renowned art collection was considered "one of the best in pre-war Germany", was seized and sold in a forced auction before Silberberg and his wife Johanna were murdered in Auschwitz.

In the decades after World War II, many art masterpieces were found on display in various galleries and museums in Europe and the United States, often with false provenances and labels missing. Some, as a result of legal action, were later returned to the families of the original owners. Many of the recovered paintings were then donated to the same or other museums as a gift.

One such lost piece, Pissarro's 1897 oil painting, "Rue St. Honoré, Apres Midi, Effet de Pluie", was discovered hanging at Madrid's government-owned museum, the Museo Thyssen-Bornemisza. In January 2011 the Spanish government denied a request by the US ambassador to return the painting. At the subsequent trial in Los Angeles, the court ruled that the Thyssen-Bornemisza Collection Foundation was the rightful owner. In 1999, Pissarro's 1897 "Le Boulevard de Montmartre, Matinée de Printemps" appeared in the Israel Museum in Jerusalem, its donor having been unaware of its pre-war provenance. In January 2012, "Le Marché aux Poissons" (The Fish Market), a color monotype, was returned after 30 years.

During his lifetime, Camille Pissarro sold few of his paintings. By the 21st century, however, his paintings were selling for millions. An auction record for the artist was set on 6 November 2007 at Christie's in New York, where a group of four paintings, "Les Quatre Saisons" (the Four Seasons), sold for $14,601,000 (estimate $12,000,000 – $18,000,000). In November 2009 "Le Pont Boieldieu et la Gare d'Orléans, Rouen, Soleil" sold for $7,026,500 at Sotheby's in New York.

In February 2014 the 1897 "Le Boulevard de Montmartre, Matinée de Printemps", originally owned by the German industrialist and Holocaust victim Max Silberberg (), sold at Sotheby's in London for £19.9M, nearly five times the previous record.

In October 2021 Berlin's Alte Nationalgalerie restituted Pissarro's "A Square in La Roche-Guyon" (1867) to the heirs of Armand Dorville, a French Jewish art collector whose family was persecuted by the Nazis and whose paintings had been sold at a 1942 auction in Nice that was overseen by the Commissariat Général aux Questions Juives. The museum then purchased the Pissarro back.

Camille's son Lucien was an Impressionist and Neo-impressionist painter as were his second and third sons Georges Henri Manzana Pissarro and Félix Pissarro. Lucien's daughter Orovida Pissarro was also a painter. Camille's great-grandson, Joachim Pissarro, became Head Curator of Drawing and Painting at the Museum of Modern Art in New York City and a professor in Hunter College's Art Department. Camille's great-granddaughter, Lélia Pissarro, has had her work exhibited alongside her great-grandfather. Another great-granddaughter, Julia Pissarro, a Barnard College graduate, is also active in the art scene. From the only daughter of Camille, Jeanne Pissarro, other painters include Henri Bonin-Pissarro (1918–2003) and Claude Bonin-Pissarro (born 1921), who is the father of the Abstract artist Frédéric Bonin-Pissarro (born 1964).

The grandson of Camille Pissarro, Hugues Claude Pissarro (dit Pomié), was born in 1935 in the western section of Paris, Neuilly-sur-Seine, and began to draw and paint as a young child under his father's tutelage. During his adolescence and early twenties he studied the works of the great masters at the Louvre. His work has been featured in exhibitions in Europe and the United States, and he was commissioned by the White House in 1959 to paint a portrait of U.S. President Dwight Eisenhower. He now lives and paints in Donegal, Ireland, with his wife Corinne also an accomplished artist and their children.





In June 2006, a three-volume work of 1,500 pages was published, titled "Pissarro: Critical Catalogue of Paintings". It was compiled by Joachim Pissarro, descendant of the painter, and Claire Durand-Ruel Snollaerts, descendant of the French art dealer Paul Durand-Ruel. The work is the most comprehensive collection of Pissarro paintings to date. It contains accompanying images of drawings and studies, as well as photographs of Pissarro and his family that had not previously been published.


Cardiology diagnostic tests and procedures

The diagnostic tests in cardiology are methods of identifying heart conditions associated with healthy vs. unhealthy, pathologic heart function.

Obtaining a medical history is always the first "test", part of understanding the likelihood of significant disease, as detectable within the current limitations of clinical medicine. Yet heart problems often produce no symptoms until very advanced, and many symptoms, such as palpitations and sensations of extra or missing heart beats correlate poorly with relative heart health "vs" disease. Hence, a history alone is rarely sufficient to diagnose a heart condition.

"Auscultation" employs a stethoscope to more easily hear various normal and abnormal sounds, such as normal heart beat sounds and the usual heart beat sound changes associated with breathing versus heart murmurs.

A variety of "blood tests" are available for analyzing cholesterol transport behavior, HDL, LDL, triglycerides, lipoprotein little a, homocysteine, C-reactive protein, blood sugar control: fasting, after eating or averages using glycated albumen or hemoglobin, myoglobin, creatine kinase, troponin, brain-type natriuretic peptide, etc. to assess the evolution of coronary artery disease and evidence of existing damage. A great many more physiologic markers related to atherosclerosis and heart function are used and being developed and evaluated in research.
(*) due to the high cost, LDL is usually calculated instead of being measured directly<br>
source: Beyond Cholesterol, Julius Torelli MD, 2005 

"Electrocardiography" (ECG/EKG in German vernacular. Elektrokardiogram) monitors electrical activity of the heart, primarily as recorded from the skin surface. A 12 lead recording, recording the electrical activity in three planes, anterior, posterior, and lateral is the most commonly used form. The ECG allows observation of the heart electrical activity by visualizing waveform beat origin (typically from the sinoatrial or SA node) following down the bundle of HIS and ultimately stimulating the ventricles to contract forcing blood through the body. Much can be learned by observing the QRS morphology (named for the respective portions of the polarization/repolarization waveform of the wave, P,Q,R,S,T wave). Rhythm abnormalities can also be visualized as in slow heart rate bradycardia, or fast heart rate tachycardia.

A "Fasegraphy" allows expanding the system of Electrocardiography diagnostic features, based on the evaluation of the speed characteristics of the process, and thereby increasing the sensitivity and specificity of ECG-diagnostics.

Fasegraphy allows determining the initial features of changes in the cardiac muscle, even on a single-channel ECG, which are underestimated in traditional ECG diagnostics.

A "Holter monitor" records a continuous EKG rhythm pattern (rarely a full EKG) for 24 hours or more. These monitors are used for suspected frequent rhythm abnormalities, especially ones the wearer may not recognize by symptoms. They are more expensive than event monitors.

An "event monitor" records short term EKG rhythm patterns, generally storing the last 2 to 5 minutes, adding in new and discarding old data, for 1 to 2 weeks or more. There are several different types with different capabilities. When the wearer presses a button on the monitor, it quits discarding old and continues recording for a short additional period. The wearer then plays the recording, via a standard phone connection, to a center with compatible receiving and rhythm printing equipment, after which the monitor is ready to record again. These monitors are used for suspected infrequent rhythm abnormalities, especially ones the wearer does recognize by symptoms. They are less expensive than Holter monitors.

"Cardiac stress testing" is used to determine to assess cardiac function and to disclose evidence of exertion-related cardiac hypoxia. Radionuclide testing using thallium or technetium can be used to demonstrate areas of perfusion abnormalities. With a maximal stress test the level of exercise is increased until the person's heart rate will not increase any higher, despite increased exercise. A fairly accurate estimate of the target heart rate, based on extensive clinical research, can be estimated by the formula 220 beats per minute minus patient's age. This linear relation is accurate up to about age 30, after which it mildly underestimates typical maximum attainable heart rates achievable by healthy individuals. Other formulas exist, such as that by Miller (217 - (0.85 × Age)) and others. Achieving a high enough heart rate at the end of exercise is critical to improving the sensitivity of the test to detect high grade heart artery stenosis. High frequency analysis of the QRS complex may be useful for detection of coronary artery disease during an exercise stress test.

The electrophysiology study or EP study is the end all of electrophysiological tests of the heart. It involves a catheter with electrodes probing the endocardium, the inside of the heart, and testing the conduction pathways and electrical activity of individual areas of the heart.

Cardiac imaging techniques include coronary catheterization, echocardiogram, intravascular ultrasound, retinal vessel analysis and the coronary calcium scan.


Carlo Collodi

Carlo Lorenzini (24 November 1826 – 26 October 1890), better known by the pen name Carlo Collodi (), was an Italian author, humourist, and journalist, widely known for his fairy tale novel "The Adventures of Pinocchio".

Collodi was born in Florence on 24 November 1826. His mother Angiolina Orzali Lorenzini was a seamstress from Collodi, the town from which he later took the pen name, and his father Domenico Lorenzini was a cook. Both parents worked for the ' Ginori Lisci. Carlo was the eldest child in the family and he had ten siblings; seven died at a young age. He spent most of his childhood in the town of Collodi where his mother was born. He lived there with his maternal grandmother. After attending primary school, he was sent to study at a theological seminary in Colle Val d’Elsa. An account at the seminary shows that the ' had offered financial aid, but the boy found that he did not want to be a priest so he continued his education at the College of the Scolopi Fathers in Florence. In 1844, he started working at the Florentine bookstore Libreria Piatti, where he assisted Giuseppe Aiazzi, a prominent Italian manuscript specialist.

During the Italian Wars of Independence in 1848 and 1860, Collodi served as a volunteer with the Tuscan army. His active interest in political matters can be seen in his earliest literary works, as well as in the founding of the satirical newspaper in 1853. This newspaper was censored by order of the Grand Duke of Tuscany. In 1854, he published his second newspaper, ' ("The Controversy"). Lorenzini's first publications were in his periodicals. A debut came in 1856 with the play ' and parodic guidebook , both in 1856. By 1860, he published his first notable work called "" (Mr. Alberi Is Right!), which outlined his political and cultural vision of Italy. This is the text where Lorenzini started using the Collodi pseudonym, which was taken from his mother's hometown.

Collodi had also begun intense activity on other political newspapers such as '; at the same time he was employed by the Censorship Commission for the Theatre. During this period he composed various satirical sketches and stories (sometimes simply by collating earlier articles), including ' (1880), ' (1881), and ' (1887).

Collodi became disenchanted with Italian politics afterwards, so he turned to children's literature and his first works involved translating French fairy tales into Italian. In 1875, for instance, he completed ', a translation of French fairy tales by Charles Perrault. In 1876, Collodi wrote ' (inspired by Alessandro Luigi Parravicini's "Giannetto"), the ', and ', a pedagogic series which explored the unification of Italy through the ironic thoughts and actions of the character Giannettino.

Lorenzini became fascinated by the idea of using an amiable, rascally character as a means of expressing his own convictions through allegory. In 1880, he began writing ' ("Story of a Marionette"), also called "Le avventure di Pinocchio", which was published weekly in '. "Pinocchio" was adapted into a 1940 film by Disney that is considered to be one of Disney's greatest films.

Collodi died suddenly in Florence on 26 October 1890 at the age of 63 and is interred at Cimitero Monumentale Delle Porte Sante in Florence. The National Carlo Collodi Foundation was established in 1962 to promote education and the works of Collodi, and Pinocchio Park, which was opened in 1956 in the town of Collodi and remains a popular attraction today.


Constructible number

In geometry and algebra, a real number formula_1 is constructible if and only if, given a line segment of unit length, a line segment of length formula_2 can be constructed with compass and straightedge in a finite number of steps. Equivalently, formula_1 is constructible if and only if there is a closed-form expression for formula_1 using only integers and the operations for addition, subtraction, multiplication, division, and square roots.

The geometric definition of constructible numbers motivates a corresponding definition of constructible points, which can again be described either geometrically or algebraically. A point is constructible if it can be produced as one of the points of a compass and straight edge construction (an endpoint of a line segment or crossing point of two lines or circles), starting from a given unit length segment. Alternatively and equivalently, taking the two endpoints of the given segment to be the points (0, 0) and (1, 0) of a Cartesian coordinate system, a point is constructible if and only if its Cartesian coordinates are both constructible numbers. Constructible numbers and points have also been called ruler and compass numbers and ruler and compass points, to distinguish them from numbers and points that may be constructed using other processes.

The set of constructible numbers forms a field: applying any of the four basic arithmetic operations to members of this set produces another constructible number. This field is a field extension of the rational numbers and in turn is contained in the field of algebraic numbers. It is the Euclidean closure of the rational numbers, the smallest field extension of the rationals that includes the square roots of all of its positive numbers.

The proof of the equivalence between the algebraic and geometric definitions of constructible numbers has the effect of transforming geometric questions about compass and straightedge constructions into algebra, including several famous problems from ancient Greek mathematics. The algebraic formulation of these questions led to proofs that their solutions are not constructible, after the geometric formulation of the same problems previously defied centuries of attack.

Let formula_5 and formula_6 be two given distinct points in the Euclidean plane, and define formula_7 to be the set of points that can be constructed with compass and straightedge starting with formula_5 and formula_6. Then the points of formula_7 are called constructible points. formula_5 and formula_6 are, by definition, elements of formula_7. To more precisely describe the remaining elements of formula_7, make the following two definitions:
Then, the points of formula_7, besides formula_5 and formula_6 are:

As an example, the midpoint of constructed segment formula_22 is a constructible point. One construction for it is to construct two circles with formula_22 as radius, and the line through the two crossing points of these two circles. Then the midpoint of segment formula_22 is the point where this segment is crossed by the constructed line.

The starting information for the geometric formulation can be used to define a Cartesian coordinate system in which the point formula_5 is associated to the origin having coordinates formula_26 and in which the point formula_6 is associated with the coordinates formula_28. The points of formula_7 may now be used to link the geometry and algebra by defining a constructible number to be a coordinate of a constructible point.

Equivalent definitions are that a constructible number is the formula_30-coordinate of a constructible point formula_31 or the length of a constructible line segment. In one direction of this equivalence, if a constructible point has coordinates formula_32, then the point formula_31 can be constructed as its perpendicular projection onto the formula_30-axis, and the segment from the origin to this point has length formula_30. In the reverse direction, if formula_30 is the length of a constructible line segment, then intersecting the formula_30-axis with a circle centered at formula_5 with radius formula_30 gives the point formula_31. It follows from this equivalence that every point whose Cartesian coordinates are geometrically constructible numbers is itself a geometrically constructible point. For, when formula_30 and formula_42 are geometrically constructible numbers, point formula_32 can be constructed as the intersection of lines through formula_31 and formula_45, perpendicular to the coordinate axes.

The algebraically constructible real numbers are the subset of the real numbers that can be described by formulas that combine integers using the operations of addition, subtraction, multiplication, multiplicative inverse, and square roots of positive numbers. Even more simply, at the expense of making these formulas longer, the integers in these formulas can be restricted to be only 0 and 1. For instance, the square root of 2 is constructible, because it can be described by the formulas formula_46 or formula_47.

Analogously, the algebraically constructible complex numbers are the subset of complex numbers that have formulas of the same type, using a more general version of the square root that is not restricted to positive numbers but can instead take arbitrary complex numbers as its argument, and produces the principal square root of its argument. Alternatively, the same system of complex numbers may be defined as the complex numbers whose real and imaginary parts are both constructible real numbers. For instance, the complex number formula_48 has the formulas formula_49 or formula_50, and its real and imaginary parts are the constructible numbers 0 and 1 respectively.

These two definitions of the constructible complex numbers are equivalent. In one direction, if formula_51 is a complex number whose real part formula_30 and imaginary part formula_42 are both constructible real numbers, then replacing formula_30 and formula_42 by their formulas within the larger formula formula_56 produces a formula for formula_57 as a complex number. In the other direction, any formula for an algebraically constructible complex number can be transformed into formulas for its real and imaginary parts, by recursively expanding each operation in the formula into operations on the real and imaginary parts of its arguments, using the expansions

The algebraically constructible points may be defined as the points whose two real Cartesian coordinates are both algebraically constructible real numbers. Alternatively, they may be defined as the points in the complex plane given by algebraically constructible complex numbers. By the equivalence between the two definitions for algebraically constructible complex numbers, these two definitions of algebraically constructible points are also equivalent.

If formula_64 and formula_65 are the non-zero lengths of geometrically constructed segments then elementary compass and straightedge constructions can be used to obtain constructed segments of lengths formula_66, formula_67, formula_68, and formula_69. The latter two can be done with a construction based on the intercept theorem. A slightly less elementary construction using these tools is based on the geometric mean theorem and will construct a segment of length formula_70 from a constructed segment of length formula_64. It follows that every algebraically constructible number is geometrically constructible, by using these techniques to translate a formula for the number into a construction for the number. 

In the other direction, a set of geometric objects may be specified by algebraically constructible real numbers: coordinates for points, slope and formula_42-intercept for lines, and center and radius for circles. It is possible (but tedious) to develop formulas in terms of these values, using only arithmetic and square roots, for each additional object that might be added in a single step of a compass-and-straightedge construction. It follows from these formulas that every geometrically constructible number is algebraically constructible.

The definition of algebraically constructible numbers includes the sum, difference, product, and multiplicative inverse of any of these numbers, the same operations that define a field in abstract algebra. Thus, the constructible numbers (defined in any of the above ways) form a field. More specifically, the constructible real numbers form a Euclidean field, an ordered field containing a square root of each of its positive elements. Examining the properties of this field and its subfields leads to necessary conditions on a number to be constructible, that can be used to show that specific numbers arising in classical geometric construction problems are not constructible.

It is convenient to consider, in place of the whole field of constructible numbers, the subfield formula_73 generated by any given constructible number formula_74, and to use the algebraic construction of formula_74 to decompose this field. If formula_74 is a constructible real number, then the values occurring within a formula constructing it can be used to produce a finite sequence of real numbers formula_77 such that, for each formula_48, formula_79 is an extension of formula_80 of degree 2. Using slightly different terminology, a real number is constructible if and only if it lies in a field at the top of a finite tower of real quadratic extensions,
formula_81
starting with the rational field formula_82 where formula_74 is in formula_84 and for all formula_85, formula_86. It follows from this decomposition that the degree of the field extension formula_87 is formula_88, where formula_1 counts the number of quadratic extension steps.

Analogously to the real case, a complex number is constructible if and only if it lies in a field at the top of a finite tower of complex quadratic extensions. More precisely, formula_74 is constructible if and only if there exists a tower of fields
formula_91
where formula_74 is in formula_93, and for all formula_94, formula_95. The difference between this characterization and that of the real constructible numbers is only that the fields in this tower are not restricted to being real. Consequently, if a complex number formula_74 is constructible, then formula_87 is a power of two. However, this necessary condition is not sufficient: there exist field extensions whose degree is a power of two that cannot be factored into a sequence of quadratic extensions.

The fields that can be generated in this way from towers of quadratic extensions of formula_82 are called "iterated quadratic extensions" of formula_82. The fields of real and complex constructible numbers are the unions of all real or complex iterated quadratic extensions of formula_82.

Trigonometric numbers are the cosines or sines of angles that are rational multiples of formula_101. These numbers are always algebraic, but they may not be constructible. The cosine or sine of the angle formula_102 is constructible only for certain special numbers formula_103:
Thus, for example, formula_104 is constructible because 15 is the product of the Fermat primes 3 and 5; but formula_105 is not constructible (not being the product of "distinct" Fermat primes) and neither is formula_106 (being a non-Fermat prime).

The ancient Greeks thought that certain problems of straightedge and compass construction they could not solve were simply obstinate, not unsolvable. However, the non-constructibility of certain numbers proves that these constructions are logically impossible to perform. (The problems themselves, however, are solvable using methods that go beyond the constraint of working only with straightedge and compass, and the Greeks knew how to solve them in this way. One such example is Archimedes' Neusis construction solution of the problem of Angle trisection.)

In particular, the algebraic formulation of constructible numbers leads to a proof of the impossibility of the following construction problems:


The birth of the concept of constructible numbers is inextricably linked with the history of the three impossible compass and straightedge constructions: doubling the cube, trisecting an angle, and squaring the circle. The restriction of using only compass and straightedge in geometric constructions is often credited to Plato due to a passage in Plutarch. According to Plutarch, Plato gave the duplication of the cube (Delian) problem to Eudoxus and Archytas and Menaechmus, who solved the problem using mechanical means, earning a rebuke from Plato for not solving the problem using pure geometry. However, this attribution is challenged, due, in part, to the existence of another version of the story (attributed to Eratosthenes by Eutocius of Ascalon) that says that all three found solutions but they were too abstract to be of practical value. Proclus, citing Eudemus of Rhodes, credited Oenopides (circa 450 BCE) with two ruler and compass constructions, leading some authors to hypothesize that Oenopides originated the restriction. The restriction to compass and straightedge is essential to the impossibility of the classic construction problems. Angle trisection, for instance, can be done in many ways, several known to the ancient Greeks. The Quadratrix of Hippias of Elis, the conics of Menaechmus, or the marked straightedge (neusis) construction of Archimedes have all been used, as has a more modern approach via paper folding.
Although not one of the classic three construction problems, the problem of constructing regular polygons with straightedge and compass is often treated alongside them. The Greeks knew how to construct regular with formula_137 (for any integer formula_138), 3, 5, or the product of any two or three of these numbers, but other regular eluded them. In 1796 Carl Friedrich Gauss, then an eighteen-year-old student, announced in a newspaper that he had constructed a regular 17-gon with straightedge and compass. Gauss's treatment was algebraic rather than geometric; in fact, he did not actually construct the polygon, but rather showed that the cosine of a central angle was a constructible number. The argument was generalized in his 1801 book "Disquisitiones Arithmeticae" giving the "sufficient" condition for the construction of a regular Gauss claimed, but did not prove, that the condition was also necessary and several authors, notably Felix Klein, attributed this part of the proof to him as well. Alhazen's problem is also not one of the classic three problems, but despite being named after Ibn al-Haytham (Alhazen), a medieval Islamic mathematician, it already appear's in Ptolemy's work on optics from the second century.

are impossible to solve if one uses only compass and straightedge. In the same paper he also solved the problem of determining which regular polygons are constructible:
a regular polygon is constructible if and only if the number of its sides is the product of a power of two and any number of distinct Fermat primes (i.e., the sufficient conditions given by Gauss are also necessary). An attempted proof of the impossibility of squaring the circle was given by James Gregory in "Vera Circuli et Hyperbolae Quadratura" (The True Squaring of the Circle and of the Hyperbola) in 1667. Although his proof was faulty, it was the first paper to attempt to solve the problem using algebraic properties of . It was not until 1882 that Ferdinand von Lindemann rigorously proved its impossibility, by extending the work of Charles Hermite and proving that is a transcendental number. Alhazen's problem was not proved impossible to solve by compass and straightedge until the work of .

The study of constructible numbers, per se, was initiated by René Descartes in La Géométrie, an appendix to his book "Discourse on the Method" published in 1637. Descartes associated numbers to geometrical line segments in order to display the power of his philosophical method by solving an ancient straightedge and compass construction problem put forth by Pappus.



Carson City, Nevada

Carson City is an independent city and the capital of the U.S. state of Nevada. As of the 2020 census, the population was 58,639, making it the 6th most populous city in the state. The majority of the city's population lives in Eagle Valley, on the eastern edge of the Carson Range, a branch of the Sierra Nevada, about south of Reno. The city is named after the mountain man Kit Carson. The town began as a stopover for California-bound immigrants, but developed into a city with the Comstock Lode, a silver strike in the mountains to the northeast. The city has served as Nevada's capital since statehood in 1864; for much of its history it was a hub for the Virginia and Truckee Railroad, although the tracks were removed in 1950.

Before 1969, Carson City was the county seat of Ormsby County. That year the state legislature abolished the county and included its territory into a revised city charter for a Consolidated Municipality of Carson City. With the consolidation, the city limits extend west across the Sierra Nevada to the California state line in the middle of Lake Tahoe. Like other independent cities in the United States, it is treated as a county-equivalent for census purposes.

The Washoe people have inhabited the valley and surrounding areas for about 6,000 years.

The first European Americans to arrive in what is now known as Eagle Valley were John C. Frémont and his exploration party in January 1843. Fremont named the river flowing through the valley Carson River in honor of Kit Carson, the mountain man and scout he had hired for his expedition. Later, settlers named the area Washoe, in reference to the indigenous people.

By 1851, the Eagle Station ranch along the Carson River was a trading post and stopover for travelers on the California Trail's Carson Branch, which ran through Eagle Valley. The valley and trading post received their name from a bald eagle that was hunted and killed by one of the early settlers and was featured on a wall inside the post.

As the area was part of the Utah Territory, it was governed from Salt Lake City, where the territorial government was headquartered. Early settlers bristled at the control by Mormon-influenced officials and desired the creation of the Nevada territory. A vigilante group of influential settlers, headed by Abraham Curry, sought a site for a capital city for the envisioned territory. In 1858, Abraham Curry bought Eagle Station and the settlement was thereafter renamed Carson City. Curry and several other partners had Eagle Valley surveyed for development. Curry decided Carson City would someday serve as the capital city and left a plot in the center of town for a capitol building.

After gold and silver were discovered in 1859 on nearby Comstock Lode, Carson City's population began to grow. Curry built the Warm Springs Hotel a mile to the east of the city center. When territorial governor James W. Nye traveled to Nevada, he chose Carson City as the territorial capital, influenced by Carson City lawyer William Stewart, who escorted him from San Francisco to Nevada. As such, Carson City bested Virginia City and American Flat. Curry loaned the Warm Springs Hotel to the territorial Legislature as a meeting hall. The Legislature named Carson City to be the seat of Ormsby County and selected the hotel as the territorial prison, with Curry serving as its first warden. Today, the property is still part of the state prison.

When Nevada became a state in 1864 during the American Civil War, Carson City was confirmed as Nevada's permanent capital. Carson City's development was no longer dependent on the mining industry and instead became a thriving commercial center. The Virginia and Truckee Railroad was built between Virginia City and Carson City. A log flume was also built from the Sierra Nevada into Carson City. The current capitol building was constructed from 1870 to 1871. The United States Mint operated the Carson City Mint between the years 1870 and 1893, which struck gold and silver coins. People came from China during that time, many to work on the railroad. Some of them owned businesses and taught school. By 1880, almost a thousand Chinese people, "one for every five Caucasians", lived in Carson City.

Carson City's population and transportation traffic decreased when the Central Pacific Railroad built a line through Donner Pass, too far to the north to benefit Carson City. The city was slightly revitalized with the mining booms in Tonopah and Goldfield. The US federal building (now renamed the Paul Laxalt Building) was completed in 1890 as was the Stewart Indian School. Even these developments could not prevent the city's population from dropping to just over 1,500 people by 1930. Carson City resigned itself to small city status, advertising itself as "America's smallest capital". The city slowly grew after World War II; by 1960, it had reached its 1880 boom-time population.

As early as the late 1940s, discussions began about merging Ormsby County and Carson City. By this time, the county was little more than Carson City and a few hamlets to the west. However, the effort did not pay off until 1966, when a statewide referendum approved the merger. The required constitutional amendment was passed in 1968. On April 1, 1969, Ormsby County and Carson City officially merged as the Consolidated Municipality of Carson City. With this consolidation, Carson City absorbed former town sites such as Empire City, which had grown up in the 1860s as a milling center along the Carson River and current U.S. Route 50. Carson City could now advertise itself as one of America's largest state capitals with its of city limits.

In 1991, the city adopted a downtown master plan, specifying no building within of the capitol would surpass it in height. This plan effectively prohibited future high-rise development in the center of downtown. The Ormsby House is the tallest building in downtown Carson City, at a height of . The structure was completed in 1972.

Most of the city proper resides in the Eagle Valley. The Carson River flows from Douglas County through the southwestern edge of both the valley and Carson City. Since the consolidation, the city limits today include several small populated areas outside of this valley. Today the city limits include several peaks in the Sierra Nevada, small portions of both the Virginia Range and the Pine Nut Mountains and portions of Marlette Lake and Lake Tahoe. The highest elevation in city limits is Snow Valley Peak at an elevation of . Carson City is one of two state capitals that border another state, the other being Trenton, New Jersey.

Carson City features a cold semi-arid climate (Koppen: "BSk") with cold winters and hot summers. The city is in a high desert river valley approximately above sea level. There are four fairly distinct seasons. Winters see typically light to moderate snowfall, with an average of . Most precipitation occurs in winter and spring, with summer and fall being fairly dry, drier than neighboring California. There are 39.5 days of + highs annually, with + temperatures occurring 1.2 days per year.

The average temperature in Carson City increased by between 1984 and 2014, a greater change than in any other city in the United States.




Carson City is the smallest of the United States' 366 metropolitan statistical areas.

As of the 2010 census, there were 55,274 people, 20,171 households, and 13,252 families residing in the city. The population density was . There were 21,283 housing units at an average density of . The racial makeup of the city was 81.1% White, 1.9% Black or African American, 2.4% Native American, 2.1% Asian, 0.2% Pacific Islander, 9.4% from other races, and 2.9% from two or more races. 21% of the population were Hispanic or Latino of any race.

As of the 2000 census, there were 20,171 households, out of which 29.8% had children under the age of 18 living with them, 50.0% were married couples living together, 11.0% had a female householder with no husband present, and 34.3% were non-families. 27.8% of all households were made up of individuals, and 11.00% had someone living alone who was 65 years of age or older. The average household size was 2.44 and the average family size was 2.97. The city's age distribution was: 23.4% under the age of 18, 7.9% from 18 to 24, 28.9% from 25 to 44, 24.9% from 45 to 64, and 14.9% who were 65 years of age or older. The median age was 39 years. For every 100 females, there were 106.9 males. For every 100 females age 18 and over, there were 108.2 males.

Data from the 2000 census indicates the median income for a household in the city was $41,809, and the median income for a family was $49,570. Males had a median income of $35,296 versus $27,418 for females. The per capita income for the city was $20,943. 10.0% of the population and 6.9% of families were below the poverty line. Out of the total population, 13.7% of those under the age of 18 and 5.8% of those 65 and older were living below the poverty line.

As of 2010, 82.3% (42,697) of Carson City residents age 5 and older spoke English at home as a first language, while 14.1% (7,325) spoke Spanish, 0.6% (318) French, and numerous Indo-Aryan languages were spoken as a main language by 0.5% (261) of the population over the age of five. In total, 17.7% (9,174) of Carson City's population age 5 and older spoke a first language other than English.

Ormsby County consolidated with Carson City in 1969, and the county simultaneously dissolved. The city is now governed by a five-member board of supervisors, consisting of a mayor and four supervisors. All members are elected at-large, but each of the four supervisors must reside in respective wards, numbered 1 through 4. The mayor and supervisors serve four year terms. Elections are staggered so the mayor and the supervisors from Wards 2 and Ward 4 are elected in presidential election years, and the supervisors from Wards 1 and 3 are elected in the even-numbered years in between (i.e., the same year as gubernatorial elections).

The city is generally considered a Republican stronghold, often voting for Republicans by wide margins. In 2004, George W. Bush defeated John Kerry 57–40%. In 2008, however, Barack Obama became the first Democrat since 1964 to win Ormsby County/Carson City, defeating John McCain 49–48%, by 204 votes, a margin of under 1%.

Carson City, being the state capital, has seen many political protests and demonstrations.

In an attempt to either make a proposed spent nuclear fuel storage facility at Yucca Mountain prohibitively expensive (by raising property tax rates to the maximum allowed) or to allow the state to collect the potential federal payments of property taxes on the facility, the state government in 1987 carved Yucca Mountain out of Nye County and created a new county with no residents out of the area surrounding Yucca called Bullfrog County. Carson City became the county seat of Bullfrog County, even though it is not in Bullfrog County and is more than from Yucca Mountain. A state judge found the process unconstitutional in 1989, and Bullfrog County's territory was retroceded to Nye County.

Carson City has never hosted any professional team sports. However, a variety of sports are offered at parks and recreation. Many neighborhood parks offer a wide variety of features including picnic tables, beaches, restrooms, fishing, softball, basketball hoops, ponds, tennis, and volleyball. The largest park is Mills Park, which has a total land area of and includes the narrow-gauge Carson & Mills Park Railroad.
While there are no ski slopes within Carson City, the city is near the Heavenly Mountain Resort, Diamond Peak and Mount Rose Ski Tahoe skiing areas.

Carson City has served as one of the state's centers for politics and business. Every state governor since Denver S. Dickerson has resided in the Governor's Mansion in Carson City. The following personalities took up residence in Carson City at some point in their lives.

The following is a list of notable employers in Carson City from the fourth quarter of 2012:

1,000–1,499 employees
500–999 employees
200–499 employees
100–199 employees

There are four highways in the city: Nevada State Route 28, U.S. Route 395, U.S. Route 50, and Interstate 580, its only freeway. Phase 1 of the Carson City Freeway Project from US 395, just north of the city, to US 50 was completed in February 2006, and Phase 2A, extending from Rt. 50 to Fairview Drive, was officially opened on September 24, 2009. Phase 2B, Fairview Drive to Rt. 50, was completed in August 2017. Prior to 2012, Carson City was one of only five state capitals not directly served by an interstate highway; the city lost this distinction when I-580 was extended into the city limits.

Carson City's first modern bus system, Jump Around Carson, or JAC, opened to the public in October 2005. JAC uses a smaller urban bus ideal for Carson City. Tahoe Transportation District connects Gardnerville with Carson City.

However, there is virtually no ground public transportation to other destinations. Passenger trains have not served Carson City since 1950, when the Virginia and Truckee Railroad was shut down. Greyhound Lines stopped their bus services to the town in 2006 and Amtrak discontinued their connecting thruway bus to Sacramento, California, in 2008. There is now only a limited Monday – Friday RTC bus service, to Reno which is still served by both Greyhound and Amtrak, as well as Eastern Sierra Transit Authority service from Lone Pine to Reno.

Carson City is also served by the Carson Airport, which is a regional airport in the northern part of the city. Reno–Tahoe International Airport, which is away, handles domestic commercial flights.
The Carson City School District, the sole public school district of the city, operates ten schools there. The six elementary schools are Bordewich-Bray Elementary School, Empire Elementary School, Fremont Elementary School, Fritsch Elementary School, Mark Twain Elementary School, and Al Seeliger Elementary School. The two middle schools are Carson Middle School and Eagle Valley Middle School. Carson High School and the alternative Pioneer High School serve high school students. Carson High is on Saliman Road.

The district sponsors Carson Montessori School, a public charter school serving grades K-6. Students residing in any Nevada county may enroll. Carson Montessori School is the only school in district operating with a balanced budget. In 2019 Carson Montessori School received the Governor's STEM Schools Designation, an official recognition given to 25 schools statewide which causes a short ceremony attended by the governor during which receiving schools are assigned a 10-foot banner.

Western Nevada College (WNC) is a regionally accredited, two-year and four-year institution which is part of the Nevada System of Higher Education. The college offers many programs including education, arts and science.

Carson City has a public library, the Carson City Library.



Classification of finite simple groups

In mathematics, the classification of finite simple groups is a result of group theory stating that every finite simple group is either cyclic, or alternating, or belongs to a broad infinite class called the groups of Lie type, or else it is one of twenty-six or twenty-seven exceptions, called sporadic. The proof consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004.

Simple groups can be seen as the basic building blocks of all finite groups, reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference from integer factorization is that such "building blocks" do not necessarily determine a unique group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.

Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.

The classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.

Daniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.

 wrote two volumes outlining the low rank and odd characteristic part of the proof, and 
wrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:

The simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.

The simple groups of small 2-rank include:
The classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.

All groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)

A group is said to be of component type if for some centralizer "C" of an involution, "C"/"O"("C") has a component (where "O"("C") is the core of "C", the maximal normal subgroup of odd order).
These are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.
A major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of "C"/"O"("C") is the image of a component of "C".

The idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.

A group is of characteristic 2 type if the generalized Fitting subgroup "F"*("Y") of every 2-local subgroup "Y" is a 2-group.
As the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.

The rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.

Groups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.
The three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of "standard type" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.
The general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.

The main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster group totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.

In 1972 announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:

Many of the items in the table below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the "wrong" order.
The proof of the theorem, as it stood around 1985 or so, can be called "first generation". Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called "revisionism", was originally led by Daniel Gorenstein.

, ten volumes of the second generation proof have been published (Gorenstein, Lyons & Solomon 1994, 1996, 1998, 1999, 2002, 2005, 2018a, 2018b; & Capdeboscq, 2021, 2023). In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from the second generation proof being written in a more relaxed style.) However, with the publication of volume 9 of the GLS series, and including the Aschbacher–Smith contribution, this estimate was already reached, with several more volumes still in preparation (the rest of what was originally intended for volume 9, plus projected volumes 10 and 11). Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.

Gorenstein and his collaborators have given several reasons why a simpler proof is possible.

Gorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups.


This section lists some results that have been proved using the classification of finite simple groups.





Chalcolithic

The Chalcolithic (also called the Copper Age and Eneolithic) was an archaeological period characterized by the increasing use of smelted copper. It followed the Neolithic and preceded the Bronze Age. It occurred at different periods in different areas, but was absent in some parts of the world, such as Russia, where there was no well-defined Copper Age between the Stone and Bronze ages. Stone tools were still predominantly used during this period.

The archaeological site of Belovode, on Rudnik mountain in Serbia, has the world's oldest securely dated evidence of copper smelting at high temperature, from  . The transition from Copper Age to Bronze Age in Europe occurred between the late 5th and the late In the Ancient Near East the Copper Age covered about the same period, beginning in the late and lasting for about a millennium before it gave rise to the Early Bronze Age.

A study in the journal "Antiquity" from 2013 reported the discovery of a tin bronze foil from the Pločnik archaeological site dated to , as well as 14 other artefacts from Bulgaria and Serbia dated to before 4,000 BC, showed that early tin bronze was more common than previously thought and developed independently in Europe 1,500 years before the first tin bronze alloys in the Near East. In Britain, the Chalcolithic is a short period between about 2,500 and 2,200 BC, characterized by the first appearance of objects of copper and gold, a new ceramic culture and the immigration of Beaker culture people, heralding the end of the local late Neolithic.

The multiple names result from multiple definitions of the period. Originally, the term Bronze Age meant that either copper or bronze was being used as the chief hard substance for the manufacture of tools and weapons. Ancient writers, who provided the essential cultural references for educated people during the 19th century, used the same name for both copper- and bronze-using ages.

The concept of the Copper Age was put forward by Hungarian scientist Ferenc Pulszky in the 1870s, when, on the basis of the significant number of large copper objects unearthed within the Carpathian Basin, he suggested that the previous threefold division of the Prehistoric Age – the Stone, Bronze and Iron Ages – should be further divided with the introduction of the Copper Age.

In 1881, John Evans recognized that use of copper often preceded the use of bronze, and distinguished between a "transitional Copper Age" and the "Bronze Age proper". He did not include the transitional period in the Bronze Age, but described it separately from the customary stone / bronze / iron system, at the Bronze Age's beginning. He did not, however, present it as a fourth age but chose to retain the tripartite system.

In 1884, Gaetano Chierici, perhaps following the lead of Evans, renamed it in Italian as the "eneo-litica", or "bronze–stone" transition. The phrase was never intended to mean that the period was the only one in which both bronze and stone were used. The Copper Age features the use of copper, excluding bronze; moreover, stone continued to be used throughout both the Bronze Age and the Iron Age. The part "-litica" simply names the Stone Age as the point from which the transition began and is not another "-lithic" age.

Subsequently, British scholars used either Evans's "Copper Age" or the term "Eneolithic" (or Æneolithic), a translation of Chierici's "eneo-litica". After several years, a number of complaints appeared in the literature that "Eneolithic" seemed to the untrained eye to be produced from "e-neolithic", "outside the Neolithic", clearly not a definitive characterization of the Copper Age. Around 1900, many writers began to substitute "Chalcolithic" for Eneolithic, to avoid the false segmentation. The term chalcolithic is a combination of two words- Chalco+Lithic, derived from the Greek words "khalkos" meaning "copper", and "líthos" meaning "stone".

But "chalcolithic" could also mislead: For readers unfamiliar with the Italian language, "chalcolithic" seemed to suggest another "-lithic" age, paradoxically part of the Stone Age despite the use of copper. Today, "Copper Age", "Eneolithic", and "Chalcolithic" are used synonymously to mean Evans's original definition of Copper Age.

The emergence of metallurgy may have occurred first in the Fertile Crescent. The earliest use of lead is documented here from the late Neolithic settlement of Yarim Tepe in Iraq:
The earliest lead (Pb) finds in the ancient Near East are a bangle from Yarim Tepe in northern Iraq and a slightly later conical lead piece from Halaf period Arpachiyah, near Mosul. As native lead is extremely rare, such artifacts raise the possibility that lead smelting may have begun even before copper smelting.
Copper smelting is also documented at this site at about the same time period (soon after 6000 BC), although the use of lead seems to precede copper smelting. Early metallurgy is also documented at the nearby site of Tell Maghzaliyah, which seems to be dated even earlier, and completely lacks pottery.

The Timna Valley contains evidence of copper mining in 7000–5000 BC. The process of transition from Neolithic to Chalcolithic in the Middle East is characterized in archaeological stone tool assemblages by a decline in high quality raw material procurement and use. This dramatic shift is seen throughout the region, including the Tehran Plain, Iran. Here, analysis of six archaeological sites determined a marked downward trend in not only material quality, but also in aesthetic variation in the lithic artefacts. Fazeli & Coningham use these results as evidence of the loss of craft specialisation caused by increased use of copper tools.

The Tehran Plain findings illustrate the effects of the introduction of copper working technologies on the in-place systems of lithic craft specialists and raw materials. Networks of exchange and specialized processing and production that had evolved during the Neolithic seem to have collapsed by the Middle Chalcolithic () and been replaced by the use of local materials by a primarily household-based production of stone tools.

Arsenical copper or bronze was clearly produced in eastern Turkey (Malatya Province) at two ancient sites, Norşuntepe and Değirmentepe, around 4200 BC. According to Boscher (2016), hearths or natural draft furnaces, slag, ore, and pigment had been recovered throughout these sites. This was in the context of Ubaid period architectural complexes typical of southern Mesopotamian architecture. Norşuntepe site demonstrates that some form of arsenic alloying was indeed taking place by the 4th millennium BC. Since the slag identified at Norşuntepe contains no arsenic, this means that arsenic in some form was added separately.

A copper axe found at Prokuplje, Serbia contains the oldest securely dated evidence of copper-making, (7,500 years ago). The find in June 2010 extends the known record of copper smelting by about 800 years, and suggests that copper smelting may have been invented in separate parts of Asia and Europe at that time rather than spreading from a single source.

Knowledge of the use of copper was far more widespread than the metal itself. The European Battle Axe culture used stone axes modeled on copper axes, even with moulding carved in the stone. Ötzi the Iceman, who was found in the Ötztal Alps in 1991 and whose remains have been dated to about 3300 BC, was found with a Mondsee copper axe.

Examples of Chalcolithic cultures in Europe include Vila Nova de São Pedro and Los Millares on the Iberian Peninsula. Pottery of the Beaker people has been found at both sites, dating to several centuries after copper-working began there. According to radiocarbon dating, the Pre-Bell Beaker Chalcolithic began on the Northern Iberian Plateau in 3000 cal. BC and the Bell Beaker Chalcolithic appeared around 2500 cal. BC. The Beaker culture appears to have spread copper and bronze technologies in Europe, along with Indo-European languages. In Britain, copper was used between the 25th and , but some archaeologists do not recognise a British Chalcolithic because production and use was on a small scale.

Ceramic similarities between the Indus Valley civilisation, southern Turkmenistan, and northern Iran during 4300–3300 BC of the Chalcolithic period suggest considerable mobility and trade.

The term "Chalcolithic" has also been used in the context of the South Asian Stone Age.

In Bhirrana, the earliest Indus civilization site, copper bangles and arrowheads were found. The inhabitants of Mehrgarh in present-day Pakistan fashioned tools with local copper ore between 7000 and 3300 BC.

The Nausharo site was a pottery workshop in province of Balochistan, Pakistan, that dates to 4,500 years ago; 12 blades and blade fragments were excavated there. These blades are long, wide, and relatively thin. Archaeological experiments show that these blades were made with a copper indenter and functioned as a potter's tool to trim and shape unfired pottery. Petrographic analysis indicates local pottery manufacturing, but also reveals the existence of a few exotic black-slipped pottery items from the Indus Valley.

In India, Chalcolithic culture flourished in mainly four farming communities – Ahar or Banas, Kayatha, Malwa, and Jorwe. These communities had some common traits like painted pottery and use of copper, but they had a distinct ceramic design tradition. Banas culture (2000–1600 BC) had ceramics with red, white, and black design. Kayatha culture (2450–1700 BC) had ceramics painted with brown colored design. Malwa culture (1900–1400 BC) had profusely decorated pottery with red or black colored design. Jorwe culture (1500–900 BC) had ceramics with matte surface and black-on-red design.

Pandu Rajar Dhibi (2000–1600 BC) is a Chalcolithic site in the eastern part of the Indian subcontinent. It is located on the south bank of Ajay River in West Bengal. Black ware, painted Koshi ware, pottery, various ornaments made of pearl and copper, various types of tools, pieces of fabric woven from Shimul cotton thread, human and various animal skeletons, burnt clay fragments have been found at the site.

In March 2018, archaeologists had discovered three carts and copper artifacts including weapons dating to 1800 BC in Sanauli village of Uttar Pradesh. The artifacts belongs to Ochre Coloured Pottery culture.

Andean civilizations in South America appear to have independently invented copper smelting.

The term "Chalcolithic" is also applied to American civilizations that already used copper and copper alloys thousands of years before Europeans immigrated. Besides cultures in the Andes and Mesoamerica, the Old Copper Complex mined and fabricated copper as tools, weapons, and personal ornaments in an area centered in the upper Great Lakes region: Present-day Michigan and Wisconsin.

The evidence of smelting or alloying that has been found in North America is subject to some dispute and a common assumption by archaeologists is that objects were cold-worked into shape. Artifacts from some of these sites have been dated to 6500–1000 BC, making them some of the oldest Chalcolithic sites in the world. Some archaeologists find artifactual and structural evidence of casting by Hopewellian and Mississippian peoples to be demonstrated in the archaeological record.

In the 5th millennium BC copper artifacts start to appear in East Asia, such as in the Jiangzhai and Hongshan cultures, but those metal artifacts were not widely used during this early stage.

Copper manufacturing gradually appeared in the Yangshao period (5000–3000 BC). Jiangzhai is the only site where copper artifacts were found in the Banpo culture. Archaeologists have found remains of copper metallurgy in various cultures from the late fourth to the early third millennia BC. These include the copper-smelting remains and copper artifacts of the Hongshan culture (4700–2900) and copper slag at the Yuanwozhen site. This indicates that inhabitants of the Yellow River valley had already learned how to make copper artifacts by the later Yangshao period.

In the region of the Aïr Mountains, Niger, independent copper smelting developed between 3000 and 2500 BC. The process was not in a developed state, indicating smelting was not foreign. It became mature about 1500 BC.



Circumcision and law

Laws restricting, regulating, or banning circumcision, some dating back to ancient times, have been enacted in many countries and communities. In modern states, circumcision is generally presumed to be legal, but laws pertaining to assault or child custody have been applied in cases involving circumcision. In the case of non-therapeutic circumcision of children, proponents of laws in favor of the procedure often point to the rights of the parents or practitioners, namely the right of freedom of religion. Those against the procedure point to the boy's right of freedom from religion. In several court cases, judges have pointed to the irreversible nature of the act, the grievous harm to the boy's body, and the right to self-determination, and bodily integrity.

There are ancient religious requirements for circumcision. The Hebrew Bible commands Jews to circumcise their male children on the eighth day of life, and to circumcise their male slaves.

Laws which ban circumcision are also ancient. The ancient Greeks prized the foreskin and disapproved of the Jewish custom of circumcision. 1 Maccabees, 1:60–61 states that King Antiochus IV of Syria, the occupying power of Judea in 170 BCE, outlawed circumcision on penalty of death, one of the grievances leading to the Maccabean Revolt.

According to the "Historia Augusta", the Roman emperor Hadrian issued a decree which banned circumcision in the empire, and some modern scholars argue that this was a main cause of the Jewish Bar Kokhba revolt of 132 CE. The Roman historian Cassius Dio, however, made no mention of such a law, instead, he blamed the Jewish uprising on Hadrian's decision to rebuild Jerusalem and rename it Aelia Capitolina, a city dedicated to Jupiter.

Antoninus Pius permitted Jews to circumcise their own sons. However, he forbade the circumcision of non-Jewish males who were either foreign-born slaves of Jews and the circumcision of non-Jewish males who were members of Jewish households, in violation of Genesis 17:12. He also banned non-Jewish men from converting to Judaism. Antoninus Pius exempted the Egyptian priesthood from the otherwise universal ban on circumcision.

Constantine the Great made it illegal to circumcise Christian slaves, and punished the owners who allowed it by freeing the Christian from slavery.

Circumcision has also played a major role in Christian history and theology. The Council of Jerusalem in the early Christian Church declared that circumcision was not necessary for Christians; covenant theology largely views the Christian sacrament of baptism as fulfilling the Israelite practice of circumcision, both being signs and seals of the covenant of grace. Though mainstream Christian denominations maintain a neutral position on routine circumcision, it is widely practiced in many Christian communities. 

Historically, the Lutheran Churches have also not practiced circumcision among their communicants. Currently the Catholic Church maintains a neutral position on the practice of non-religious circumcision. Today, many Christian denominations are neutral about ritual male circumcision, not requiring it for religious observance, but neither forbidding it for cultural or other reasons. 

On the other hand, in Oriental Christianity, the Coptic Orthodox Church and Ethiopian Orthodox Church and Eritrean Orthodox Church require that their male members undergo circumcision.

Before glasnost, according to an article in "The Jewish Press", Jewish ritual circumcision was forbidden in the Soviet Union. However, David E. Fishman, professor of Jewish History at the Jewish Theological Seminary of America, states that, whereas the "heder" and "yeshiva", the organs of Jewish education, "were banned by virtue of the law separating church and school, and subjected to tough police and administrative actions", circumcision was not proscribed by law or suppressed by executive measures.
Jehoshua A. Gilboa writes that while circumcision was not officially or explicitly banned, pressure was exerted to make it difficult. "Mohels" in particular were concerned that they could be punished for any health issue that might develop, even if it arose some time after the circumcision.

In 1967, all religion in Communist Albania was banned, along with the practice of circumcision. The practice was driven underground and many boys were secretly circumcised.

On 1 October 2013, the Parliamentary Assembly of the Council of Europe adopted a non-binding resolution in which they state they are "particularly worried about a category of violation of the physical integrity of children", and included in this category "circumcision of young boys for religious reasons". On 7 October, Israel's president Shimon Peres wrote a personal missive to the Secretary General of the Council of Europe, Thorbjørn Jagland, to stop the "ban", arguing: "The Jewish communities across Europe would be greatly afflicted to see their cultural and religious freedom impeded upon by the Council of Europe, an institution devoted to the protection of these very rights." Two days later, Jagland clarified that the resolution was non-binding and that "Nothing in the body of our legally binding standards would lead us to put on equal footing the issue of female genital mutilation and the circumcision of young boys for religious reasons."

A study commissioned by the European Parliament Committee on Civil Liberties, Justice and Home Affairs published in February 2013 stated that "Male circumcision for non-therapeutic reasons appears to be practiced with relative regularity and frequency throughout Europe," and said it was "the only scenario, among the topics discussed in the present chapter, in which the outcome of the balancing between the right to physical integrity and religious freedom is in favour of the latter." The study recommended that "the best interests of children should be paramount, while acknowledging the relevance of this practice for Muslims and Jews. Member States should ensure that circumcision of underage children is performed according to the medical profession's art and under conditions that do not put the health of minors at risk. The introduction of regulations by the Member States in order to set the conditions and the appropriate medical training for those called to perform it is warranted."

On 30 September 2013, the children's ombudsmen of all five Nordic countries – Denmark, Finland, Iceland, Norway, and Sweden – together with the children's spokesperson from Greenland and representatives of associations of Nordic paediatricians and paediatric surgeons, gathered in Oslo to discuss the issue, and released a joint declaration proposing a ban on non-therapeutic circumcision of male minors:

As of February 2018, no European country has a ban on male circumcision.

Whereas child custody regulations have been applied to cases involving circumcision, there seems to be no state which currently unequivocally bans infant male circumcision for non-therapeutic reasons, albeit the legality of such circumcision is disputed in some legislations.

The present table provides a non-exhaustive overview comparing legal restrictions and requirements on non-therapeutic infant circumcision in several countries. Some countries require one or both parents to consent to the operation; some of these (Finland, United Kingdom) have experienced legal battles between parents when one of them had their son's circumcision carried out or planned without the other's consent. Some countries require the procedure to be performed by or supervised by a qualified physician (or a qualified nurse in Sweden), and with (local) anaesthesia applied to the boy or man.

The Royal Australasian College of Physicians (RACP) finds that routine infant circumcision is not warranted in Australia and New Zealand and that, since circumcision involves physical injury, physicians ought to raise and consider with parents and considered the option of leaving circumcision until later, when the boy is old enough to make a decision for himself: 

In 1993, a non-binding research paper of the Queensland Law Reform Commission ("Circumcision of Male Infants") concluded that "On a strict interpretation of the assault provisions of the Queensland Criminal Code, routine circumcision of a male infant could be regarded as a criminal act," and that doctors who perform circumcision on male infants may be liable to civil claims by that child at a later date. No prosecutions have occurred in Queensland, and circumcisions continue to be performed.

In 1999, a Perth man won A$360,000 in damages after a doctor admitted he botched a circumcision operation at birth which left the man with a badly deformed penis.

In 2002, Queensland police charged a father with grievous bodily harm for having his two sons, then aged nine and five, circumcised without the knowledge and against the wishes of the mother. The mother and father were in a family court dispute. The charges were dropped when the police prosecutor revealed that he did not have all family court paperwork in court and the magistrate refused to grant an adjournment.

Cosmetic circumcision for newborn males is currently banned in all Australian public hospitals, South Australia being the last state to adopt the ban in 2007; the procedure was not forbidden from being performed in private hospitals. In the same year, the Tasmanian President of the Australian Medical Association, Haydn Walters, stated that they would support a call to ban circumcision for non-medical, non-religious reasons. In 2009, the Tasmanian Law Reform Institute released its Issues Paper investigating the law relating to male circumcision in Tasmania, it "highlights the uncertainty in relation to whether doctors can legally perform circumcision on infant males".

The Tasmania Law Reform Institute released its recommendations for reform of Tasmanian law relative to male circumcision on 21 August 2012. The report makes fourteen recommendations for reform of Tasmanian law relative to male circumcision.

The Belgian Advisory Committee on Bioethics finds that circumcision is a radical operation, and that physical integrity of the child takes precedence over parents' belief systems.

In 2012, "Le Soir" reported a 21% increase in the amount of circumcisions in Belgium from 2006 and 2011. In the previous 25 years, one in three Belgian-born boys had allegedly been circumcised. A questionnaire to hospitals in Wallonia and Brussels showed that about 80 to 90% of the procedures had religious or cultural motives. The Ministry of Health stressed the importance of safe circumstances, physicians warned that "no surgical procedure is without risk" and that circumcision was "not a necessary procedure".

In 2017, it was estimated that about 15% of Belgian men were circumcised. The incidence has been gradually rising: in 2002, about 17,800 boys or men underwent circumcision, which increased to almost 26,200 in 2016. The expenses of undergoing circumcision are covered by the National Institute for Disease and Disability Insurance (RIZIV/INAMI), costing about 2.7 million euros in 2016. After inquiries were submitted to the Belgian Bioethics Advisory Committee in early 2014, an ethics commission was set up to review the morality of covering the costs of medically unnecessary surgery through taxpayer money, especially considering that many taxpayers regard the practice as immoral. By July 2017, the commission reportedly reached consensus on discontinuing the financial coverage of non-medical circumcision, but was still debating whether to advise the government to institute a total ban of the practice. The commission's final (non-binding) recommendation, presented on 19 September 2017, was to cease public funding for non-medical circumcision, and to not circumcise anyone underage until they can consent or reject the procedure after being properly informed. This was in line with the 1990 Convention on the Rights of the Child, and mirrors the 2013 non-binding Parliamentary Assembly of the Council of Europe's resolution against underage non-therapeutic circumcision. However, Health Minister Maggie De Block rejected the commission's advice, arguing the RIZIV "cannot know whether there is a medical motive or not" when parents request a circumcision, and when they are denied a professional procedure, chances are parents will have a non-expert perform it, leading to worse results for the children. The Health Minister's response was received with mixed reactions.

The Canadian Paediatric Society does not recommend routine circumcision, finding that medical necessity has not been clearly established, and as such, that it should be deferred until the individual concerned is able to make his own choices.

According to the College of Physicians and Surgeons of British Columbia:

Circumcision is legal in Denmark, and each year 1,000 to 2,000 boys are circumcised for non-medical reasons, the Danish Health Authority estimated in 2013, with most circumcisions being performed on Muslim or Jewish boys in private clinics or private homes. For boys below the age of 15, circumcision requires consent from the parents, while the boy can consent when he is 15 years or older. Circumcision is classified as an operation and reserved for doctors, though the responsible doctor can delegate the actual operation to non-medical person, as long as the doctor is present. The operation requires "sufficient pain relief (analgesic) and sedation (Anesthesia)" The doctor is responsible for having the necessary qualifications (both for the operation and the pain relief) and for being informed about the newest scientific developments in the area.

The current guidelines for non-medical circumcision are from 2013, and , a committee under the Danish Patient Health Authority are in the process of updating them. In August 2020, the Danish Society of Anaesthesiology and Intensive Care Medicine withdrew from the committee, because they disagreed with the Authority's opinion that local anaesthesia was sufficient, instead saying the scientific literature showed that general anaesthesia was necessary. Other professional organizations followed them, and according to DR, only the Authority and two private clinics that perform circumcisions remain in the committee.

The Danish population overwhelmingly support a ban on non-medical circumcision of boys below the age of 18. A 2020 survey measured the support at 86%, while surveys in 2018, 2016 and 2014 measured the support at 83%, 87% and 74%, respectively In 2018, a citizen's initiative calling for such a ban reached the threshold of 50.000 signatures to be put forward in the Folketing. It was subsequently found compliant with the Danish Constitution, in particularly §67 on religious freedom. The Danish Medical Association believes boys should decide for themselves after they turn 18 years old, but does not call for a ban. Politicians are hesitant in supporting a ban, with protection of religious freedom, in particular the Jewish practice of circumcision, and potential foreign policy and national security ramifications mentioned as some of the reasons. , the Social Democrats and Venstre, who together hold a majority in the Folketing, oppose a ban, while the Danish People's Party, the Socialist People's Party, Red-Green Alliance, The Alternative, The New Right and Liberal Alliance favour a ban. The Conservative and the Social Liberal Party have no official opinion on the question.




With a two-thirds majority against, the Folketing voted against a ban on circumcision in May 2021.

The Finnish Ombudsman for Equality finds that circumcising young boys without a medical reason is legally highly questionable, The Finnish Supreme Court found that non-therapeutic circumcision of boys is assault, and the Finnish Ombudsman for Children proposed that Finland should ban non-therapeutic circumcision of young boys: 
In August 2006, a Finnish court ruled that the circumcision of a four-year-old boy arranged by his mother, who is Muslim, to be an illegal assault. The boy's father, who had not been consulted, reported the incident to the police. A local prosecutor stated that the prohibition of circumcision is not gender-specific in Finnish law. A lawyer for the Ministry of Social Affairs and Health stated that there is neither legislation nor prohibition on male circumcision, and that "the operations have been performed on the basis of common law." The case was appealed and in October 2008 the Finnish Supreme Court ruled that the circumcision, "carried out for religious and social reasons and in a medical manner, did not have the earmarks of a criminal offence. It pointed out in its ruling that the circumcision of Muslim boys is an established tradition and an integral part of the identity of Muslim men". In 2008, the Finnish government was reported to be considering a new law to legalise circumcision if the practitioner is a doctor and if the child consents. In December 2011, Helsinki District Court said that the Supreme Court's decision does not mean that circumcision is legal for any non-medical reasons. The court referred to the Convention on Human rights and Biomedicine of the Council of Europe, which was ratified in Finland in 2010.

In February 2010, a Jewish couple were fined for causing bodily harm to their then infant son who was circumcised in 2008 by a mohel brought in from the UK. Normal procedure for persons of Jewish faith in Finland is to have a locally certified mohel who works in Finnish healthcare perform the operation. In the 2008 case, the infant was not anesthetized and developed complications that required immediate hospital care. The parents were ordered to pay 1500 euros in damages to their child.

In November 2020, the Finnish Parliament passed a new law on female genital mutilation. An earlier version of the draft law could also have criminalised nonmedical infant circumcision, but due to intense lobbying by several Islamic and Jewish organisations including the Central Council of Finnish Jewish Communities, Milah UK, and the European Jewish Congress, the wording was changed and instead, the law passed in Parliament now states that the issue of circumcision of boys should be "clarified" in the future.



The German Association of Pediatricians (BVKJ) finds no medical reason for non-therapeutic circumcision and that the AAP (2012) recommendation scientifically unsustainable, and that boys should have the same constitutional legal right to physical integrity as girls: 

In October 2006, a Turkish national who performed ritual circumcisions on seven boys was convicted of causing dangerous bodily harm by the state court in Düsseldorf.

In September 2007, a Frankfurt am Main appeals court found that the circumcision of an 11-year-old boy without his approval was an unlawful personal injury. The boy, whose parents were divorced, was visiting his Muslim father during a vacation when his father forced him to be ritually circumcised. The boy had planned to sue his father for .

In May 2012, the Cologne regional appellate court ruled that religious circumcision of male children amounts to bodily injury, and is a criminal offense in the area under its jurisdiction. The decision based on the article "Criminal Relevance of Circumcising Boys. A Contribution to the Limitation of Consent in Cases of Care for the Person of the Child" published by Holm Putzke, a German law professor at the University of Passau. The court arrived at its judgment by application of the human rights provisions of the Basic Law, a section of the Civil Code, and some sections of the Criminal Code to non-therapeutic circumcision of male children. Some observers said it could set a legal precedent that criminalizes the practice. Jewish and Muslim groups were outraged by the ruling, viewing it as trampling on freedom of religion.

The German ambassador to Israel, Andreas Michaelis, told Israeli lawmakers that Germany was working to resolve the issue and that it does not apply at a national level, but instead only to the local jurisdiction of the court in Cologne. The Council of the Coordination of Muslims in Germany condemned the ruling, stating that it is "a serious attack on religious freedom". Ali Kizilkaya, a spokesman of the council, stated that, "The ruling does not take everything into account, religious practice concerning circumcision of young Muslims and Jews has been carried out over the millennia on a global level." The Roman Catholic archbishop of Aachen, Heinrich Mussinghoff, said that the ruling was "very surprising", and the contradiction between "basic rights on freedom of religion and the well-being of the child brought up by the judges is not convincing in this very case". Hans Ulrich Anke, the head of the Protestant Church in Germany, said the ruling should be appealed since it did not "sufficiently" consider the religious significance of the rite. A spokesman, Steffen Seibert, for German Chancellor Angela Merkel stated that Jewish and Muslim communities will be free to practice circumcision responsibly, and the government would find a way around the local ban in Cologne. The spokesman stated "For everyone in the government it is absolutely clear that we want to have Jewish and Muslim religious life in Germany. Circumcision carried out in a responsible manner must be possible in this country without punishment."

In July 2012, a group of rabbis, imams, and others said that they view the ruling against circumcision "an affront on our basic religious and human rights". The joint statement was signed by leaders of groups including Germany's Turkish-Islamic Union for Religious Affairs, the Islamic Center Brussels, the Rabbinical Centre of Europe, the European Jewish Parliament and the European Jewish Association, who met with members of European Parliament from Germany, Finland, Belgium, Italy, and Poland. European rabbis, who urged Jews to continue circumcision, planned further talks with Muslim and Christian leaders to determine how they can oppose the ban together. The Jewish Hospital of Berlin suspended the practice of male circumcision. On 19 July 2012, a joint resolution of the CDU/CSU, SPD and FDP factions in the Bundestag requesting the executive branch to draft a law permitting circumcision of boys to be performed without unnecessary pain in accordance with best medical practice carried with a broad majority.

"The New York Times" reported that the German Medical Association "condemned the ruling for potentially putting children at risk by taking the procedure out of the hands of doctors, but it also warned surgeons note to perform circumcisions for religious reasons until legal clarity was established". The ruling was supported by Deutsche Kinderhilfe, a German child rights organization, which asked for a two-year moratorium to discuss the issue and pointed out that religious circumcision may contravene the Convention on the Rights of the Child (Article 24.3: "States Parties shall take all effective and appropriate measures with a view to abolishing traditional practices prejudicial to the health of children.").

The German Academy for Pediatric and Adolescent Medicine (Deutsche Akademie für Kinder- und Jugendmedizin e.V., DAKJ), the German Association for Pediatric Surgery (Deutsche Gesellschaft für Kinderchirurgie, DGKCH) and the Professional Association of Pediatric and Adolescent Physicians (Berufsverband der Kinder- und Jugendärzte) took a firm stand against non-medical routine infant circumcision.

In July, in Berlin, a criminal complaint was lodged against Rabbi Yitshak Ehrenberg for "causing bodily harm" by performing religious circumcision, and for vocal support of the continuation of the practice. In September, the prosecutors dismissed the complaint, concluding that "there is no proof to establish that the rabbi's conduct met the 'condition of a criminal' violation".

In September, Reuters reported "Berlin's senate said doctors could legally circumcise infant boys for religious reasons in its region, given certain conditions."

On 12 December 2012, following a series of hearings and consultations, the Bundestag adopted the proposed law explicitly permitting non-therapeutic circumcision to be performed under certain conditions; it is now §1631(d) in the German Civil Code. The vote tally was 434 ayes, 100 noes, and 46 abstentions. Following approval by the Bundesrat and signing by the Bundespräsident, the new law became effective on 28 December 2012 a day after its publication in the Federal Gazette.

In May 2005, Iceland amended its General Penal Code to criminalise female genital mutilation

In February 2018, the Progressive Party proposed a bill that would change the words "girl child" to "child" and "her sexual organs" to "[their] sexual organs", thereby making Iceland the first European country to ban male circumcision for non-medical reasons. The bill was ultimately put on hold later that year following pressure from the United States, Israel, and various lobbyist groups.



In October 2005 a Nigerian man was cleared of a charge of reckless endangerment over the death of a baby from hemorrhage and shock after he had circumcised the child. The judge directed the jury not to "bring what he called their white western values to bear when they were deciding this case" and effectively imposed a not guilty verdict on the jury. After deliberating for an hour and a half they found the defendant not guilty.

In Israel, Jewish circumcision is entirely legal. The circumcision rate is very high in Israel, although some limited data suggests the practice is slowly declining. According to an online survey by the parents' portal Mamy in 2006, the rate was 95%, while earlier estimates put it at 98–99%. Ben Shalem, an organisation dedicated to the abolition of circumcision, petitioned the Supreme Court in 1999 on the grounds that circumcision violated human dignity, children's rights and criminal law. The petition was rejected. In 2013, a Rabbinical court in Israel ordered a mother in the midst of divorce proceedings to circumcise her son in accordance with the father's wishes, or pay a fine of 500 Israeli Shekel for every day that the child is not circumcised. She appealed against the Rabbinical court ruling and the High Court ruled in her favour stating, among other considerations, the basic right of freedom from religion.

The Royal Dutch Medical Association (KNMG) finds non-therapeutic circumcision of male minors to be in conflict with children's right to autonomy and physical integrity, and that there are good reasons for its legal prohibition, as exists for female genital mutilation: 
In May 2008 a father who had his two sons, aged 3 and 6, circumcised against the will of their mother was found not guilty of abuse as the circumcision was performed by a physician and due to the court's restraint in setting a legal precedent; instead he was given a 6-week suspended jail sentence for taking the boys away from their mother against her will.

The parquet of the Supreme Court of the Netherlands made an elaborate statement on the legal status of circumcision on 5 July 2011 in the course of a criminal case. First, the parquet notes that there is no law that specifically prohibits the circumcision of boys, nor that the practice falls under the more general crime of "(zware) mishandeling" ('(grave) assault'). "Genital mutilation of girls in any case undoubtedly falls under "(zware) mishandeling" (Art. 300–303 Dutch Criminal Code). Whereas most forms of genital cutting of girls are generally marked as genital mutilation, a similar communis opinio regarding genital cutting of boys does not yet exist so far." The Supreme Court acknowledged that society's attitudes on genital cutting of boys had been gradually shifting over the course of years, and that "the increasing concern [in the medical world] about the harm and the risk of complications during a circumcision is indeed relevant", but that overall there were not enough reasons yet to proceed to criminalisation. Neither could intentional infliction of grave bodily harm (Art. 82 Dutch Criminal Code) be applied to the normal circumstances of a competently and hygienically performed circumcision in a clinic. And because young children are incapable of exercising the right to self-determination, parents ought to do this on their behalf. They can both request a circumcision to be performed, as well as consent to it being performed, on the grounds of their parental authority. However, it is important that both parents consent to the procedure.



The Norwegian Ombudsman for Children ("Barneombudet") opposes circumcising children, and stated on 29 September 2013 that it is right to wait until children are old enough to decide for themselves: 
In June 2012, the centre-right Centre Party proposed a ban on circumcision on males under eighteen, after an Oslo infant died in May following a circumcision.

A bill on ritual circumcision of boys was passed (against two votes) in the Norwegian Parliament in June 2014, with the new law going into effect on 1 January 2015. This law explicitly allows Jews to practice brit milah and obligates the Norwegian Health Care regions to offer the Muslim minority a safe and affordable procedure. Local anaesthesia needs to be applied and a licensed physician needs to be present at the circumcision, which hospitals started to perform in March 2015.

In May 2017, the right-wing Progress Party proposed to ban circumcision for males under sixteen.


The Children's Act 2005 makes the circumcision of male children under 16 unlawful except for religious or medical reasons. In the Eastern Cape province the Application of Health Standards in Traditional Circumcision Act, 2001, regulates traditional circumcision, which causes the death or mutilation of many youths by traditional surgeons each year. Among other provisions, the minimum age for circumcision is age 18.

In 2004, a 22-year-old Rastafarian convert was forcibly circumcised by a group of Xhosa tribal elders and relatives. When he first fled, two police returned him to those who had circumcised him. In another case, a medically circumcised Xhosa man was forcibly recircumcised by his father and community leaders. He laid a charge of unfair discrimination on the grounds of his religious beliefs, seeking an apology from his father and the Congress of Traditional Leaders of South Africa. According to South African newspapers, the subsequent trial became "a landmark case around forced circumcision". In October 2009, the Eastern Cape High Court at Bhisho (sitting as an Equality Court) clarified that circumcision is unlawful unless done with the full consent of the initiate.

The Slovenian Human Rights Ombudsman found in February 2012, after consulting various relevant expert bodies and studying relevant constitutional and legal stipulations, that circumcision for non-medical reasons is a violation of children's rights, that ritual circumcision for religious reasons is unacceptable in Slovenia for both legal and ethical reasons and should not be performed by doctors: 

In 2001, the Parliament of Sweden enacted a law allowing only persons certified by the National Board of Health to circumcise infants. It requires a medical doctor or an anesthesia nurse to accompany the circumciser and for anaesthetic to be applied beforehand. After the first two months of life circumcisions can only be performed by a physician. The stated purpose of the law was to increase the safety of the procedure.

Swedish Jews and Muslims objected to the law, and in 2001, the World Jewish Congress called it "the first legal restriction on Jewish religious practice in Europe since the Nazi era". The requirement for an anaesthetic to be administered by a medical professional is a major issue, and the low degree of availability of certified professionals willing to conduct circumcision has also been subject to criticism. According to a survey, two out of three paediatric surgeons said they refuse to perform non-therapeutic circumcision, and less than half of all county councils offer it in their hospitals. However, in 2006, the U.S. State Department stated, in a report on Sweden, that most Jewish mohels had been certified under the law and 3000 Muslim and 40–50 Jewish boys were circumcised each
year. An estimated 2000 of these are performed by persons who are neither physicians nor have officially recognised certification.

The Swedish National Board of Health and Welfare reviewed the law in 2005 and recommended that it be maintained, but found that the law had failed with regard to the intended consequence of increasing the safety of circumcisions. A later report by the Board criticised the low level of availability of legal circumcisions, partly due to reluctance among health professionals. To remedy this, the report suggested a new law obliging all county councils to offer non-therapeutic circumcision in their hospitals, but this was later abandoned in favour of a non-binding recommendation.

In January 2014, the Swedish Medical Association (SLF) found no known medical benefits to circumcision of children, and thus strong reasons to wait until the boy is old and mature enough (12 or 13 years old) to give informed consent, aiming at ceasing all non-medically justified circumcision without prior consent:

In October 2018, the right-wing populist Sweden Democrats party submitted a draft motion to parliament calling for a ban. At the annual conference of the Centre Party in September 2019, 314 to 166 commissioners voted in favor of prohibiting boys' circumcision. Several Jewish and Islamic organisations voiced their opposition to a potential ban. The Left Party has also expressed support for a prohibition on circumcising boys before the age of 18; other parties have so far not backed a potential ban, though the Green Party found the practice "problematic".



According to a July 2012 survey by "20 Minuten" involving 8,000 participants, 64% of the Swiss population wanted religious circumcision to be banned. 67% of men and 56% of women were in favour. 93% of Muslim respondents and 75% of Jewish respondents opposed a ban. Over 25% of male respondents were themselves circumcised; 96% of Muslim men and 89% of Jewish men in the survey said they were circumcised, while 20% of circumcised men belonged to neither religion. Almost a third of circumcised men favoured a ban, with 12% wishing in hindsight that they had not been circumcised.

Male circumcision has traditionally been presumed to be legal under British law, however some authors have argued that there is no solid foundation for this view in English law.

While legal, the British Medical Association finds it ethically unacceptable to circumcise a child or young person, either with or without competence, who refuses the procedure, irrespective of the parents' wishes, and that parental preference alone does not constitute sufficient grounds for performing NTMC on a child unable to express his own view: 

The passage of the Human Rights Act 1998 has led to some speculation that the lawfulness of the circumcision of male children is unclear.

One 1999 case, "Re "J" (child's religious upbringing and circumcision)" said that circumcision in Britain required the consent of all those with parental responsibility (however this comment was not part of the reason for the judgement and therefore is not legally binding), or the permission of the court, acting for the best interests of the child, and issued an order prohibiting the circumcision of a male child of a non-practicing Muslim father and non-practicing Christian mother with custody. The reasoning included evidence that circumcision carried some medical risk; that the operation would be likely to weaken the relationship of the child with his mother, who strongly objected to circumcision without medical necessity; that the child may be subject to ridicule by his peers as the odd one out and that the operation might irreversibly reduce sexual pleasure, by permanently removing some sensory nerves, even though cosmetic foreskin restoration might be possible. The court did not rule out circumcision against the consent of one parent. It cited a hypothetical case of a Jewish mother and an agnostic father with a number of sons, all of whom, by agreement, had been circumcised as infants in accordance with Jewish laws; the parents then have another son who is born after they have separated; the mother wishes him to be circumcised like his brothers; the father for no good reason, refuses his agreement. In such a case, a decision in favor of circumcision was said to be likely.

In 2001 the General Medical Council had found a doctor who had botched circumcision operations guilty of abusing his professional position and that he had acted "inappropriately and irresponsibly", and struck him off the register. A doctor who had referred patients to him, and who had pressured a mother into agreeing to the surgery, was also condemned. He was put on an 18-month period of review and retraining, and was allowed to resume unrestricted practice as a doctor in March 2003, after a committee found that he had complied with conditions it placed on him. According to the "Northern Echo", he "told the committee he has now changed his approach to circumcision referrals, accepting that most cases can be treated without the need for surgery".

Fox and Thomson (2005) argue that consent cannot be given for non-therapeutic circumcision. They say there is "no compelling legal authority for the common view that circumcision is lawful".

In 2005 a Muslim man had his son circumcised against the wishes of the child's mother who was the custodial parent.

In 2009 it was reported that a 20-year-old man whose father had him ritually circumcised as a baby is preparing to sue the doctor who circumcised him. This is believed to be the first time a person who was circumcised as an infant has made a claim in the UK. The case is expected to be heard in 2010.

In a 2015 case regarding female circumcision, a judge concluded that non-therapeutic circumcision of male children is a "significant harm". In 2016, the Family Court in Exeter ruled that a Muslim father could not have his two sons (aged 6 and 4) circumcised after their mother disagreed. Mrs Justice Roberts declared that the boys should first grow old enough "to the point where each of the boys themselves will make their individual choices once they have the maturity and insight to appreciate the consequences and longer-term effects of the decisions which they reach".

In June 2017, Nottinghamshire Police arrested three people on suspicion of "conspiracy to commit grievous bodily harm". The alleged victim was purportedly circumcised while in its Muslim father's care at his grandparents' in July 2013 without the consent of his mother (a non-religious white British woman who conceived the child after a casual affair with the man, whom she had separated from after the incident). The mother first contacted social services and eventually the police in November 2014. The police initially dismissed the complaint, but after the mother got help from the anti-circumcision group Men Do Complain and leading human rights lawyer Saimo Chahal QC, they reopened the case, and ended up arresting three suspects involved. In November 2017, the Crown Prosecution Service explained to the mother in a letter they were not going to prosecute the doctor, who claimed he was unaware of the mother's non-consent. However, Chahal appealed this decision, which she said "lacks any semblance of a considered and reasoned decision and is flawed and irrational", and threatened to bring the case to court. The by then 29-year-old mother finally sued the doctor in April 2018. Niall McCrae, mental health expert from King's College London, argued that this case could mean "the end of ritual male circumcision in the UK", drawing comparisons with earlier rulings against female genital mutilation.

Circumcision of adults who grant personal informed consent for the surgical operation is legal.

In the United States, non-therapeutic circumcision of male children has long been assumed to be lawful in every jurisdiction provided that one parent grants surrogate informed consent. Adler (2013) has recently challenged the validity of this assumption. As with every country, doctors who circumcise children must take care that all applicable rules regarding informed consent and safety are satisfied.

While anti-circumcision groups have occasionally proposed legislation banning non-therapeutic child circumcision, it has not been supported in any legislature. After a failed attempt to adopt a local ordinance banning circumcision on a San Francisco ballot, the state of California enacted in October 2011 a law protecting circumcision from local attempts to ban the practice.

In 2012, New York City required those performing "metzitzah b'peh", the oral suction of the open circumcision wound required by Hasidim, to obey stringent consent requirements, including documentation. Agudath Israel of America and other Jewish groups have planned to sue the city in response.

Disputes between parents

Occasionally the courts are asked to make a ruling when parents cannot agree on whether or not to circumcise a child.

In January 2001 a dispute between divorcing parents in New Jersey was resolved when the mother, who sought to have the boy circumcised withdrew her request. The boy had experienced two instances of foreskin inflammation and she wanted to have him circumcised. The father, who had experienced a traumatic circumcision as a child, objected and they turned to the courts for a decision. The Medical Society of New Jersey and the Urological Society of New Jersey both opposed any court ordered medical treatment. As the parties came to an agreement, no precedent was set. In June 2001 a Nevada court settled a dispute over circumcision between two parents but put a strict gag order on the terms of the settlement. In July 2001 a dispute between parents in Kansas over circumcision was resolved when the mother's request to have the infant circumcised was withdrawn. In this case the father opposed circumcision while the mother asserted that not circumcising the child was against her religious beliefs. (The woman's pastor had stated that circumcision was "important" but was not necessary for salvation.) On 24 July 2001 the parents reached agreement that the infant would not be circumcised.

On 14 July 2004 a mother appealed to the Missouri Supreme Court to prevent the circumcision of her son after a county court and the Court of Appeals had denied her a writ of prohibition. However, in early August 2004, before the Supreme Court had given its ruling, the father, who had custody of the boy, had him circumcised.

In October 2006 a judge in Chicago granted an injunction blocking the circumcision of a 9-year-old boy. In granting the injunction the judge stated that "the boy could decide for himself whether to be circumcised when he turns 18."

In November 2007, the Oregon Supreme Court heard arguments from a divorced Oregon couple over the circumcision of their son. The father wanted his son, who turned 13 on 2 March 2008, to be circumcised in accordance with the father's religious views; the child's mother opposes the procedure. The parents dispute whether the boy is in favor of the procedure. A group opposed to circumcision filed briefs in support of the mother's position, while some Jewish groups filed a brief in support of the father. On 25 January 2008, the Court returned the case to the trial court with instructions to determine whether the child agrees or objects to the proposed circumcision. The father appealed to the US Supreme Court to allow him to have his son circumcised but his appeal was rejected. The case then returned to the trial court. When the trial court interviewed the couple's son, now 14 years old, the boy stated that he did not want to be circumcised. This also provided the necessary circumstances to allow the boy to change residence to live with his mother. The boy was not circumcised.

Other disputes

In September 2004 the North Dakota Supreme Court rejected a mother's attempt to prosecute her doctor for circumcising her child without fully informing her of the consequences of the procedure. The judge and jury found that the plaintiffs were adequately informed of possible complications, and the jury further found that it is not incumbent on the doctors to describe every "insignificant" risk.

In March 2009 a Fulton County, GA, State Court jury awarded $2.3 million in damages to a 4-year-old boy and his mother for a botched circumcision in which too much tissue was removed causing permanent disfigurement.

In August 2010 an eight-day-old boy was circumcised in a Florida hospital against the stated wishes of the parents. The hospital admitted that the boy was circumcised by mistake; the mother has sued the hospital and the doctor involved in the case.



Called to Common Mission

Called to Common Mission (CCM) is an agreement between The Episcopal Church (ECUSA) and the Evangelical Lutheran Church in America (ELCA) in the United States, establishing full communion between them. It was ratified by the ELCA in 1999, the ECUSA in 2000, after the narrow failure of a previous agreement. Its principal author on the Episcopal side was theological professor J. Robert Wright. Under the agreement, they recognize the validity of each other's baptisms and ordinations. The agreement provided that the ELCA would accept the historical episcopate and the "threefold ministry" of bishop - priest (or pastor) - deacon with respect to ministers of communicant churches serving ELCA congregations; the installation of the ELCA presiding bishop was performed through the laying on of hands by Lutheran bishops in the historic episcopate. This provision was opposed by some in the ELCA, which after its founding merger in 1988, held a lengthy study of the ministry which was undertaken with divided opinions. In response to concerns about the meaning of the CCM, synod bishops in the ELCA drafted the Tucson resolution which presented the official ELCA position. It made clear that there is no requirement to ordain deacons or accept their ministry. It also provided assurance that the ELCA did not and was not required by CCM to change its own theological stance.

Lutheran churches of Scandinavian origin, such as the Church of Sweden and Church in Kenya, affirm apostolic succession and are in the historical episcopate; nevertheless, some within the ELCA argued that the historical episcopate would contradict the doctrine that the church exists wherever the Word of God is preached and sacraments are practiced. The traditional ELCA doctrine is affirmed by the Tucson resolution. Others objected on the grounds that adopting the Episcopalian / Anglican view on priestly orders and hierarchical structure was contrary to the Evangelical Lutheran concept of the "priesthood of all believers", which holds that all Christians stand on equal footing before God. They argued that the Old Covenant required a priest to mediate between God and humanity, but that New Covenant explicitly abolishes the need for priestly role by making every Christian a priest with direct access to God's grace. The Tucson resolution explained that the ELCA had not adopted the Episcopal view, but ECUSA or Reformed ordinands accepted by ELCA congregations would follow ELCA practice. Still others objected because of the implied directive that the use of a lay presidency would be abolished. This was a particularly issue for rural congregations that periodically "called" a congregation member to conduct communion services consecrating the elements (of bread and wine for service) in the interim period or with the absence of ordained clergy (pastor). The Tucson resolution explicitly affirmed the continued use of lay ministry.



Context menu

A context menu (also called contextual, shortcut, and pop up or pop-up menu) is a menu in a graphical user interface (GUI) that appears upon user interaction, such as a right-click mouse operation. A context menu offers a limited set of choices that are available in the current state, or context, of the operating system or application to which the menu belongs. Usually the available choices are actions related to the selected object. From a technical point of view, such a context menu is a graphical control element.

Context menus first appeared in the Smalltalk environment on the Xerox Alto computer, where they were called "pop-up menus"; they were invented by Dan Ingalls in the mid-1970s.

Microsoft Office v3.0 introduced the context menu for copy and paste functionality in 1990. Borland demonstrated extensive use of the context menu in 1991 at the Second Paradox Conference in Phoenix Arizona. Lotus 1-2-3/G for OS/2 v1.0 added additional formatting options in 1991. Borland Quattro Pro for Windows v1.0 introduced the Properties context menu option in 1992.

Context menus are opened via various forms of user interaction that target a region of the GUI that supports context menus. The specific form of user interaction and the means by which a region is targeted vary:



Windows mouse click behavior is such that the context menu doesn't open while the mouse button is pressed, but only opens the menu when the button is released, so the user has to click again to select a context menu item. This behavior differs from that of macOS and most free software GUIs.

Context menus are sometimes hierarchically organized, allowing navigation through different levels of the menu structure. The implementations differ: Microsoft Word was one of the first applications to only show sub-entries of some menu entries after clicking an arrow icon on the context menu, otherwise executing an action associated with the parent entry. This makes it possible to quickly repeat an action with the parameters of the previous execution, and to better separate options from actions.

The following window managers provide context menu functionality:


Context menus have received some criticism from usability analysts when improperly used, as some applications make certain features "only" available in context menus, which may confuse even experienced users (especially when the context menus can only be activated in a limited area of the application's client window).

Context menus usually open in a fixed position under the pointer, but when the pointer is near a screen edge the menu will be displaced - thus reducing consistency and impeding use of muscle memory. If the context menu is being triggered by keyboard, such as by using Shift + F10, the context menu appears near the focused widget instead of the position of the pointer, to save recognition efforts.

Microsoft's guidelines call for always using the term "context menu", and explicitly deprecate "shortcut menu".


Jews as the chosen people

In Judaism, the concept of the Jews as chosen people ( "ha-ʿam ha-nivḥar , IPA: haʕam hanivħar") is the belief that the Jews as a subset, via partial descent from the ancient Israelites, are also chosen people, i.e. selected to be in a covenant with God. However, Israelites being properly the chosen people of God is found directly in the Book of Deuteronomy as the verb "baḥar" (בָּחַר), and is alluded to elsewhere in the Hebrew Bible using other terms such as "holy people". Much is written about these topics in rabbinic literature. The three largest Jewish denominations—Orthodox Judaism, Conservative Judaism and Reform Judaism—maintain the belief that the Jews have been chosen by God for a purpose. Sometimes this choice is seen as charging the Jewish people with a specific mission—to be a light unto the nations, and to exemplify the covenant with God as described in the Torah.

While the concept of "chosenness" may be understood by some to connote ethnic supremacy, the status as a "chosen people" within Judaism does not preclude a belief that God has a relationship with other peoples—rather, Judaism holds that God had entered into a covenant with "all" humankind, and that Jews and non-Jews alike have a relationship with God. Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh", the Tanakh (Hebrew Bible) also identifies prophets outside the community of Israel and the prophet Jonah is explicitly told to go prophesize to the non-Jewish people of Nineveh. Based on these statements and stories, some rabbis theorized that, in the words of Natan'el al-Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others ... [and] God sends a prophet to every people according to their own language." (Levine, 1907/1966) The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other" (Mishnah Sanhedrin 4:5). The Tosefta, an important supplement to the Mishnah, also states: "Righteous people of all nations have a share in the world to come" (Sanhedrin 105a).

According to the Israel Democracy Institute, approximately two thirds of Israeli Jews believe that Jews are the "chosen people".

According to Deuteronomy, when the Lord delivers the Israelites to the land, the other nations will be cast out, and "thou shalt make no covenant with them, nor show mercy unto them" Deuteronomy 7:5-7:6,

A similar passage exalting Israel as the chosen people follows prohibitions on baldness and cutting yourself in mourning, "For thou art a holy people".
The Torah also says,
God promises that he will never exchange his people with any other:

Other Torah verses about chosenness,

The obligation imposed upon the Israelites was emphasized by the prophet Amos:

Sometimes this choice is seen as charging the Jewish people with a specific mission—to be a light unto the nations, and to exemplify the covenant with God as described in the Torah. This view, however, does not always preclude a belief that God has a relationship with other peoples—rather, Judaism held that God had entered into a covenant with all humankind, and that Jews and non-Jews alike have a relationship with God.
Biblical references as well as rabbinic literature support this view: Moses refers to the "God of the spirits of all flesh", and the Tanakh also identifies prophets outside the community of Israel. Based on these statements, some rabbis theorized that, in the words of Natan'el al-Fayyumi, a Yemenite Jewish theologian of the 12th century, "God permitted to every people something he forbade to others...<nowiki>[and]</nowiki> God sends a prophet to every people according to their own language." The Mishnah states that "Humanity was produced from one man, Adam, to show God's greatness. When a man mints a coin in a press, each coin is identical. But when the King of Kings, the Holy One, blessed be He, creates people in the form of Adam not one is similar to any other." The Tosefta, a collection of important post-Talmudic discourses, also states: "Righteous people of all nations have a share in the world to come."

Most Jewish texts do not state that "God chose the Jews" by itself. Rather, this is usually linked with a mission or purpose, such as proclaiming God's message among all the nations, even though Jews cannot become "unchosen" if they shirk their mission. This implies a special duty, which evolves from the belief that Jews have been pledged by the covenant which God concluded with the biblical patriarch Abraham, their ancestor, and again with the entire Jewish nation at Mount Sinai. In this view, Jews are charged with living a holy life as God's priest-people.

In the Jewish prayerbook (the Siddur), chosenness is referred to in a number of ways. The blessing for reading the Torah reads, "Praised are You, Lord our God, King of the Universe, Who has chosen us out of all the nations and bestowed upon us His Torah." In the "Kiddush", a prayer of sanctification, in which the Sabbath is inaugurated over a cup of wine, the text reads, "For you have chosen us and sanctified us out of all the nations, and have given us the Sabbath as an inheritance in love and favour. Praised are you, Lord, who hallows the Sabbath." In the "Kiddush" recited on festivals it reads, "Blessed are You ... who have chosen us from among all nations, raised us above all tongues, and made us holy through His commandments." The Aleinu prayer refers to the concept of Jews as a chosen people: "It is our duty to praise the Master of all, to exalt the Creator of the Universe, who has not made us like the nations of the world and has not placed us like the families of the earth; who has not designed our destiny to be like theirs, nor our lot like that of all their multitude. We bend the knee and bow and acknowledge before the Supreme King of Kings, the Holy One, blessed be he, that it is he who stretched forth the heavens and founded the earth. His seat of glory is in the heavens above; his abode of majesty is in the lofty heights.

According to the Rabbis, "Israel is of all nations the most willful or headstrong one, and the Torah was to give it the right scope and power of resistance, or else the world could not have withstood its fierceness."

"The Lord offered the Law to all nations; but all refused to accept it except Israel."

How do we understand "A Gentile who consecrates his life to the study and observance of the Law ranks as high as the high priest", says R. Meïr, by deduction from Lev. xviii. 5; II Sam. vii. 19; Isa. xxvi. 2; Ps. xxxiii. 1, cxviii. 20, cxxv. 4, where all stress is laid not on Israel, but on man or the righteous one.
Maimonides states: "It is now abundantly clear that the pledges Hashem made to Avraham and his descendants would be fulfilled exclusively first in Yitzchak and then in Yaakov, Yitzchak son. This is confirmed by a passage that states, "He is ever mindful of His covenant ... that He made with Avraham, swore to Yitzchak, and confirmed in a decree for Yaakov, for Yisrael, as an eternal covenant.""

The Gemara states this regarding a non-Jew who studies Torah [his 7 mitzvot] and regarding this, see Shita Mekubetzes, Bava Kama 38a who says that this is an exaggeration. In any case, this statement was not extolling the non-Jew. The Rishonim explain that it is extolling the Torah.

Tosfos explains that it uses the example of a "kohen gadol" (high priest), because this statement is based on the verse, ""y'kara hi mipnimim"" (it is more precious than pearls). This is explained elsewhere in the Gemara to mean that the Torah is more precious "pnimim" (translated here as "inside" instead of as "pearls"; thus that the Torah is introspectively absorbed into the person), which refers to "lifnai v'lifnim" (translated as "the most inner of places"), that is the Holy of Holies where the "kahon gadol" went.

In any case, in Midrash Rabba this statement is made with an important addition: a non-Jew who converts and studies Torah etc.

The Nation of Israel is likened to the olive. Just as this fruit yields its precious oil only after being much pressed and squeezed, so Israel's destiny is one of great oppression and hardship, in order that it may thereby give forth its illuminating wisdom. Poverty is the quality most befitting Israel as the chosen people. Only on account of its good works is Israel among the nations "as the lily among thorns", or "as wheat among the chaff."

Rabbi Lord Immanuel Jakobovits, former Chief Rabbi of the United Synagogue of Great Britain (Modern Orthodox Judaism), described chosenness in this way: "Yes, I do believe that the chosen people concept as affirmed by Judaism in its holy writ, its prayers, and its millennial tradition. In fact, I believe that every people—and indeed, in a more limited way, every individual—is "chosen" or destined for some distinct purpose in advancing the designs of Providence. Only, some fulfill their mission and others do not. Maybe the Greeks were chosen for their unique contributions to art and philosophy, the Romans for their pioneering services in law and government, the British for bringing parliamentary rule into the world, and the Americans for piloting democracy in a pluralistic society. The Jews were chosen by God to be 'peculiar unto Me' as the pioneers of religion and morality; that was and is their national purpose."

Modern Orthodox theologian Michael Wyschogrod wrote:

"[T]he initial election of Abraham himself was not earned. ... We are simply told that God commanded Abraham to leave his place of birth and go to a land that God would show him. He is also promised that his descendants will become a numerous people. But nowhere does the Bible tell us why Abraham rather than someone else was chosen. The implication is that God chooses whom He wishes and that He owes no accounting to anyone for His choices."

Rabbi Norman Lamm, a leader of Modern Orthodox Judaism, wrote: "The chosenness of Israel relates exclusively to its spiritual vocation embodied in the Torah; the doctrine, indeed, was announced at Sinai. Whenever it is mentioned in our liturgy—such as the blessing immediately preceding the Shema...it is always related to Torah or Mitzvot ("commandments"). This spiritual vocation consists of two complementary functions, described as "Goy Kadosh", that of a holy nation, and "Mamlekhet Kohanim", that of a kingdom of priests. The first term denotes the development of communal separateness or differences in order to achieve a collective self-transcendence. ... The second term implies the obligation of this brotherhood of the spiritual elite toward the rest of mankind; priesthood is defined by the prophets as fundamentally a teaching vocation."

Conservative Judaism views the concept of chosenness in this way: "Few beliefs have been subject to as much misunderstanding as the 'Chosen People' doctrine. The Torah and the Prophets clearly stated that this does not imply any innate Jewish superiority. In the words of Amos (3:2) 'You alone have I singled out of all the families of the earth—that is why I will call you to account for your iniquities.' The Torah tells us that we are to be "a kingdom of priests and a holy nation" with obligations and duties which flowed from our willingness to accept this status. Far from being a license for special privilege, it entailed additional responsibilities not only toward God but to our fellow human beings. As expressed in the blessing at the reading of the Torah, our people have always felt it to be a privilege to be selected for such a purpose. For the modern traditional Jew, the doctrine of the election and the covenant of Israel offers a purpose for Jewish existence which transcends its own self interests. It suggests that because of our special history and unique heritage we are in a position to demonstrate that a people that takes seriously the idea of being covenanted with God can not only thrive in the face of oppression, but can be a source of blessing to its children and its neighbors. It obligates us to build a just and compassionate society throughout the world and especially in the land of Israel where we may teach by example what it means to be a 'covenant people, a light unto the nations.'"

Rabbi Reuven Hammer comments on the excised sentence in the Aleinu prayer mentioned above:

"Originally the text read that God has not made us like the nations who "bow down to nothingness and vanity, and pray to an impotent god", ... In the Middle Ages these words were censored, since the church believed they were an insult to Christianity. Omitting them tends to give the impression that the Aleinu teaches that we are both different and better than others. The actual intent is to say that we are thankful that God has enlightened us so that, unlike the pagans, we worship the true God and not idols. There is no inherent superiority in being Jewish, but we do assert the superiority of monotheistic belief over paganism. Although paganism still exists today, we are no longer the only ones to have a belief in one God."

Reform Judaism views the concept of chosenness as follows: "Throughout the ages it has been Israel's mission to witness to the Divine in the face of every form of paganism and materialism. We regard it as our historic task to cooperate with all men in the establishment of the kingdom of God, of universal brotherhood, Justice, truth and peace on earth. This is our Messianic goal." In 1999 the Reform movement stated, "We affirm that the Jewish people are bound to God by an eternal covenant, as reflected in our varied understandings of Creation, Revelation and Redemption. ... We are Israel, a people aspiring to holiness, singled out through our ancient covenant and our unique history among the nations to be witnesses to God's presence. We are linked by that covenant and that history to all Jews in every age and place."

Many Kabbalistic sources, notably the Tanya, contain statements to the effect that the Jewish soul is qualitatively different from the non-Jewish soul. A number of known Chabad rabbis offered alternative readings of the Tanya, did not take this teaching literally, and even managed to reconcile it with the leftist ideas of internationalism and class struggle. The original text of the Tanya refers to the "idol worshippers" and does not mention the "nations of the world" at all, although such interpretation was endorsed by Menachem Mendel Schneerson and is popular in contemporary Chabad circles. Hillel of Parich, an early Tanya commentator, wrote that the souls of righteous Gentiles are more similar to the Jewish souls, and are generally good and not egoistic. This teaching was accepted by Schneerson and is considered normative in Chabad.

According to the author of the Tanya himself, a righteous non-Jew can achieve a high level of spiritually, similar to an angel, although his soul is still fundamentally different in character, but not value, from a Jewish one. Tzemach Tzedek, the third rebbe of Chabad, wrote that the Muslims are naturally good-hearted people. Rabbi Yosef Jacobson, a popular contemporary Chabad lecturer, teaches that in today's world most non-Jews belong to the category of righteous Gentiles, effectively rendering the Tanya's attitude anachronistic.

An anti-Zionist interpretation of Tanya was offered by Abraham Yehudah Khein, a prominent Ukrainian Chabad rabbi, who supported anarchist communism and considered Peter Kropotkin a great Tzaddik. Khein basically read the Tanya backwards; since the souls of idol worshipers are known to be evil, according to the Tanya, while the Jewish souls are known to be good, he concluded that truly altruistic people are really Jewish, in a spiritual sense, while Jewish nationalists and class oppressors are not. By this logic, he claimed that Vladimir Solovyov and Rabindranath Tagore probably have Jewish souls, while Leon Trotsky and other totalitarians do not, and many Zionists, whom he compared to apes, are merely "Jewish by birth certificate".

Nachman of Breslov also believed that Jewishness is a level of consciousness, and not an intrinsic inborn quality. He wrote that, according to the Book of Malachi, one can find "potential Jews" among all nations, whose souls are illuminated by the leap of "holy faith", which "activated" the Jewishness in their souls. These people would otherwise convert to Judaism, but prefer not to do so. Instead, they recognize the Divine unity within their pagan religions.

Isaac Arama, an influential philosopher and mystic of the 15th century, believed that righteous non-Jews are spiritually identical to the righteous Jews. Rabbi Menachem Meiri, a famous Catalan Talmudic commentator and Maimonidian philosopher, considered all people, who sincerely profess an ethical religion, to be part of a greater "spiritual Israel". He explicitly included Christians and Muslims in this category. Meiri rejected all Talmudic laws that discriminate between the Jews and non-Jews, claiming that they only apply to the ancient idolators, who had no sense of morality. The only exceptions are a few laws related directly or indirectly to intermarriage, which Meiri did recognize.

Meiri applied his idea of "spiritual Israel" to the Talmudic statements about unique qualities of the Jewish people. For example, he believed that the famous saying that Israel is above astrological predestination ("Ein Mazal le-Israel") also applied to the followers of other ethical faiths. He also considered countries, inhabited by decent moral non-Jews, such as Languedoc, as a spiritual part of the Holy Land.

One Jewish critic of chosenness was the philosopher Baruch Spinoza. In the third chapter of his "Theologico-Political Treatise", Spinoza mounts an argument against a naive interpretation of God's choice of the Jews. Bringing evidence from the Bible itself, he argues that God's choice of Israel was not unique (he had chosen other nations before choosing the Hebrew nation) and that the choice of the Jews is neither inclusive (it does not include all of the Jews, but only the 'pious' ones) nor exclusive (it also includes 'true gentile prophets'). Finally, he argues that God's choice is not unconditional. Recalling the numerous times God threatened the complete destruction of the Hebrew nation, he asserts that this choice is neither absolute, nor eternal, nor necessary.

In a German-language letter to philosopher Eric Gutkind, dated 3 January 1954, the physicist Albert Einstein wrote:
The word God is for me nothing more than the expression and product of human weaknesses, the Bible a collection of honorable, but still primitive legends which are nevertheless pretty childish. No interpretation no matter how subtle can (for me) change this... For me the Jewish religion like all other religions is an incarnation of the most childish superstitions. And the Jewish people to whom I gladly belong and with whose mentality I have a deep affinity have no different quality for me than all other people... I cannot see anything “chosen” about them.

Reconstructionist Judaism rejects the concept of chosenness. Its founder, Rabbi Mordecai Kaplan, said that the idea that God chose the Jewish people leads to racist beliefs among Jews, and thus must be excised from Jewish theology. This rejection of chosenness is made explicit in the movement's siddurim (prayer books). For example, the original blessing recited before reading from the Torah contains the phrase, "asher bahar banu mikol ha’amim"—"Praised are you Lord our God, ruler of the Universe, "who has chosen us from among all peoples" by giving us the Torah." The Reconstructionist version is rewritten as "asher kervanu la’avodato", "Praised are you Lord our God, ruler of the Universe, "who has drawn us to your service" by giving us the Torah." In the mid-1980s, the Reconstructionist movement issued its "Platform on Reconstructionism". It states that the idea of chosenness is "morally untenable", because anyone who has such beliefs "implies the superiority of the elect community and the rejection of others."

Not all Reconstructionists accept this view. The newest siddur of the movement, "Kol Haneshamah", includes the traditional blessings as an option, and some modern Reconstructionist writers have opined that the traditional formulation is not racist, and should be embraced.

An original prayer book by Reconstructionist feminist poet Marcia Falk, "The Book of Blessings", has been widely accepted by both Reform and Reconstructionist Jews. Falk rejects all concepts relating to hierarchy or distinction; she sees any distinction as leading to the acceptance of other kinds of distinctions, thus leading to prejudice. She writes that as a politically liberal feminist, she must reject distinctions made between men and women, homosexuals and heterosexuals, Jews and non-Jews, and to some extent even distinctions between the Sabbath and the other six days of the week. She thus rejects the idea of chosenness as unethical. She also rejects Jewish theology in general, and instead holds to a form of religious humanism. Falk writes: "The idea of Israel as God's chosen people ... is a key concept in rabbinic Judaism. Yet it is particularly problematic for many Jews today, in that it seems to fly in the face of monotheistic belief that all humanity is created in the divine image—and hence, all humanity is equally loved and valued by God. ... I find it difficult to conceive of a feminist Judaism that would incorporate it in its teaching: the valuing of one people "over and above" others is all too analogous to the privileging of one sex over another." Reconstructionist author Judith Plaskow also criticises the idea of chosenness, for many of the same reasons as Falk. A politically liberal lesbian, Plaskow rejects most distinctions made between men and women, homosexuals and heterosexuals, and Jews and non-Jews. In contrast to Falk, Plaskow does not reject all concepts of difference as inherently leading to unethical beliefs, and holds to a more classical form of Jewish theism than Falk.

A number of responses to these views have been made by Reform and Conservative Jews; they hold that these criticisms are against teachings that do not exist within liberal forms of Judaism, and which are rare in Orthodox Judaism (outside certain Haredi communities, such as Chabad). A separate criticism stems from the very existence of feminist forms of Judaism in all denominations of Judaism, which do not have a problem with the concepts of chosenness.

The children of Israel enjoy a special status in the Islamic book, the Quran (2:47 and 2:122). However, Muslim scholars point out that this status did not confer upon Israelites any racial superiority, and was only valid so long as the Israelites maintain their covenant with God.

Some Christians believe that the Jews were God's chosen people, but because of Jewish rejection of Jesus, the Christians in turn received that special status. This doctrine is known as Supersessionism.

Other Christians, such as the Christadelphians, believe that God has not rejected Israel as his chosen people and that the Jews will in fact accept Jesus as their Messiah at his Second Coming, resulting in their salvation.

Augustine criticized Jewish chosenness as "carnal." He reasoned that Israel was chosen "according to the flesh."

The Jamieson-Fausset-Brown Bible Commentary similarly argues that God made Israel the "holy nation" to exclusively uphold the promises made to their "pious forefathers". They argue that Jewish supremacist views are unsound, with Jews being frequently described as a small people that engaged in "perverse" moral conduct in the Bible.

Avi Beker, an Israeli scholar and former Secretary General of the World Jewish Congress, regarded the idea of the chosen people as Judaism's defining concept and "the central unspoken psychological, historical, and theological problem which is at the heart of Jewish-Gentile relations." In his book "The Chosen: The History of an Idea, and the Anatomy of an Obsession", Beker expresses the view that the concept of chosenness is the driving force behind Jewish-Gentile relations, explaining both the admiration and, more pointedly, the envy and the hatred which the world has felt towards the Jews in both religious and secular terms. Beker argues that while Christianity has modified its doctrine on the displacement of the Jews, Islam has neither reversed nor reformed its theology concerning the succession of both the Jews and the Christians. According to Beker, this presents a major barrier to conflict resolution in the Arab-Israeli conflict.

The Israeli philosopher Ze'ev Levy writes that chosenness can be "(partially) justified only from the historical angle" with respect to its spiritual and moral contribution to Jewish life through the centuries as "a powerful agent of consolation and hope". He points out, however, that modern anthropological theories "do not merely proclaim the inherent universal equality of all people [as] human beings; they also stress the "equivalence" [emphasis in original] of all human cultures." He continues that "there are no inferior and superior people or cultures but only different, "other", ones." He concludes that the concept of chosenness entails ethnocentrism, "which does not go hand in hand with otherness, that is, with unconditional respect of otherness".

Some people have said that Judaism's chosen people concept is racist because it implies that Jews are superior to non-Jews. The Anti-Defamation League asserts that the concept of a chosen people within Judaism has nothing to do with racial superiority.





Christian persecution

Christian persecution may refer to:






Chaparral

Chaparral ( ) is a shrubland plant community found primarily in California, in southern Oregon and in the northern portion of the Baja California Peninsula in Mexico. It is shaped by a Mediterranean climate (mild wet winters and hot dry summers) and infrequent, high-intensity crown fires.

Many chaparral shrubs have hard sclerophyllous evergreen leaves, as contrasted with the associated soft-leaved, drought-deciduous, scrub community of coastal sage scrub, found often on drier, southern facing slopes.

Three other closely related chaparral shrubland systems occur in central Arizona, western Texas, and along the eastern side of central Mexico's mountain chains, all having summer rains in contrast to the Mediterranean climate of other chaparral formations. Chaparral comprises 9% of California's wildland vegetation and contains 20% of its plant species.

The name comes from the Spanish word , which translates to "place of the scrub oak".

In its natural state, chaparral is characterized by infrequent fires, with natural fire return intervals ranging between 30 years and over 150 years. Mature chaparral (at least 60 years since time of last fire) is characterized by nearly impenetrable, dense thickets (except the more open desert chaparral). These plants are flammable during the late summer and autumn months when conditions are characteristically hot and dry. They grow as woody shrubs with thick, leathery, and often small leaves, contain green leaves all year (are evergreen), and are typically drought resistant (with some exceptions). After the first rains following a fire, the landscape is dominated by small flowering herbaceous plants, known as fire followers, which die back with the summer dry period.

Similar plant communities are found in the four other Mediterranean climate regions around the world, including the Mediterranean Basin (where it is known as ), central Chile (where it is called ), the South African Cape Region (known there as ), and in Western and Southern Australia (as ). According to the California Academy of Sciences, Mediterranean shrubland contains more than 20 percent of the world's plant diversity. The word "chaparral" is a loanword from Spanish , meaning place of the scrub oak, which itself comes from a Basque word, , that has the same meaning.

Conservation International and other conservation organizations consider chaparral to be a biodiversity hotspot – a biological community with a large number of different species – that is under threat by human activity.

The California chaparral and woodlands ecoregion, of the Mediterranean forests, woodlands, and scrub biome, has three sub-ecoregions with ecosystem—plant community subdivisions:

For the numerous individual plant and animal species found within the California chaparral and woodlands ecoregion, see:

Some of the indicator plants of the California chaparral and woodlands ecoregion include:
Chaparral soils and nutrient composition

Chaparral characteristically is found in areas with steep topography and shallow stony soils, while adjacent areas with clay soils, even where steep, tend to be colonized by annual plants and grasses. Some chaparral species are adapted to nutrient-poor soils developed over serpentine and other ultramafic rock, with a high ratio of magnesium and iron to calcium and potassium, that are also generally low in essential nutrients such as nitrogen.

Another phytogeography system uses two California chaparral and woodlands subdivisions: the cismontane chaparral and the transmontane (desert) chaparral.

Cismontane chaparral ("this side of the mountain") refers to the chaparral ecosystem in the Mediterranean forests, woodlands, and scrub biome in California, growing on the western (and coastal) sides of large mountain range systems, such as the western slopes of the Sierra Nevada in the San Joaquin Valley foothills, western slopes of the Peninsular Ranges and California Coast Ranges, and south-southwest slopes of the Transverse Ranges in the Central Coast and Southern California regions.

In Central and Southern California chaparral forms a dominant habitat. Members of the chaparral biota native to California, all of which tend to regrow quickly after fires, include:

The complex ecology of chaparral habitats supports a very large number of animal species. The following is a short list of birds which are an integral part of the cismontane chaparral ecosystems.


Transmontane chaparral or desert chaparral —"transmontane" ("the other side of the mountain") "chaparral"—refers to the desert shrubland habitat and chaparral plant community growing in the rainshadow of these ranges. Transmontane chaparral features xeric desert climate, not Mediterranean climate habitats, and is also referred to as desert chaparral. Desert chaparral is a regional ecosystem subset of the deserts and xeric shrublands biome, with some plant species from the California chaparral and woodlands ecoregion. Unlike cismontane chaparral, which forms dense, impenetrable stands of plants, desert chaparral is often open, with only about 50 percent of the ground covered. Individual shrubs can reach up to in height.
Transmontane chaparral or desert chaparral is found on the eastern slopes of major mountain range systems on the western sides of the deserts of California. The mountain systems include the southeastern Transverse Ranges (the San Bernardino and San Gabriel Mountains) in the Mojave Desert north and northeast of the Los Angeles basin and Inland Empire; and the northern Peninsular Ranges (San Jacinto, Santa Rosa, and Laguna Mountains), which separate the Colorado Desert (western Sonoran Desert) from lower coastal Southern California. It is distinguished from the cismontane chaparral found on the coastal side of the mountains, which experiences higher winter rainfall. Naturally, desert chaparral experiences less winter rainfall than cismontane chaparral. Plants in this community are characterized by small, hard (sclerophyllic) evergreen (non-deciduous) leaves. Desert chaparral grows above California's desert cactus scrub plant community and below the pinyon-juniper woodland. It is further distinguished from the deciduous sub-alpine scrub above the pinyon-juniper woodlands on the same side of the Peninsular ranges.

Due to the lower annual rainfall (resulting in slower plant growth rates) when compared to cismontane chaparral, desert chaparral is more vulnerable to biodiversity loss and the invasion of non-native weeds and grasses if disturbed by human activity and frequent fire.

Transmontane (desert) chaparral typically grows on the lower ( elevation) northern slopes of the southern Transverse Ranges (running east to west in San Bernardino and Los Angeles counties) and on the lower () eastern slopes of the Peninsular Ranges (running south to north from lower Baja California to Riverside and Orange counties and the Transverse Ranges). It can also be found in higher-elevation sky islands in the interior of the deserts, such as in the upper New York Mountains within the Mojave National Preserve in the Mojave Desert.

The California transmontane (desert) chaparral is found in the rain shadow deserts of the following:


There is overlap of animals with those of the adjacent desert and pinyon-juniper communities.

Chaparral is a coastal biome with hot, dry summers and mild, rainy winters. The chaparral area receives about of precipitation a year. This makes the chaparral most vulnerable to fire in the late summer and fall.

The chaparral ecosystem as a whole is adapted to be able to recover from naturally infrequent, high-intensity fire (fires occurring between 30 and 150 years or more apart); indeed, chaparral regions are known culturally and historically for their impressive fires. (This does create a conflict with human development adjacent to and expanding into chaparral systems.) Additionally, Native Americans burned chaparral near villages on the coastal plain to promote plant species for textiles and food. Before a major fire, typical chaparral plant communities are dominated by manzanita, chamise "Adenostoma fasciculatum" and "Ceanothus" species, toyon (which can sometimes be interspersed with scrub oaks), and other drought-resistant shrubs with hard (sclerophyllous) leaves; these plants resprout (see resprouter) from underground burls after a fire.

Plants that are long-lived in the seed bank or serotinous with induced germination after fire include chamise", Ceanothus," and fiddleneck"." Some chaparral plant communities may grow so dense and tall that it becomes difficult for large animals and humans to penetrate, but may be teeming with smaller fauna in the understory. The seeds of many chaparral plant species are stimulated to germinate by some fire cue (heat or the chemicals from smoke or charred wood). During the time shortly after a fire, chaparral communities may contain soft-leaved herbaceous, fire following annual wildflowers and short-lived perennials that dominate the community for the first few years – until the burl resprouts and seedlings of chaparral shrub species create a mature, dense overstory. Seeds of annuals and shrubs lie dormant until the next fire creates the conditions needed for germination.

Several shrub species such as "Ceanothus" fix nitrogen, increasing the availability of nitrogen compounds in the soil.

Because of the hot, dry conditions that exist in the California summer and fall, chaparral is one of the most fire-prone plant communities in North America. Some fires are caused by lightning, but these are usually during periods of high humidity and low winds and are easily controlled. Nearly all of the very large wildfires are caused by human activity during periods of hot, dry easterly Santa Ana winds. These human-caused fires are commonly ignited by power line failures, vehicle fires and collisions, sparks from machinery, arson, or campfires.

Though adapted to infrequent fires, chaparral plant communities can be eliminated by frequent fires. A high frequency of fire (less than 10-15 years apart) will result in the loss of obligate seeding shrub species such as "Manzanita" spp. This high frequency disallows seeder plants to reach their reproductive size before the next fire and the community shifts to a sprouter-dominance. If high frequency fires continue over time, obligate resprouting shrub species can also be eliminated by exhausting their energy reserves below-ground. Today, frequent accidental ignitions can convert chaparral from a native shrubland to non-native annual grassland and drastically reduce species diversity, especially under drought brought about by climate change.

There are two older hypotheses relating to California chaparral fire regimes that caused considerable debate in the past within the fields of wildfire ecology and land management. Research over the past two decades have rejected these hypotheses:

The perspective that older chaparral is unhealthy or unproductive may have originated during the 1940s when studies were conducted measuring the amount of forage available to deer populations in chaparral stands. However, according to recent studies, California chaparral is extraordinarily resilient to very long periods without fire and continues to maintain productive growth throughout pre-fire conditions. Seeds of many chaparral plants actually require 30 years or more worth of accumulated leaf litter before they will successfully germinate (e.g., scrub oak, "Quercus berberidifolia"; toyon, "Heteromeles arbutifolia"; and holly-leafed cherry, "Prunus ilicifolia"). When intervals between fires drop below 10 to 15 years, many chaparral species are eliminated and the system is typically replaced by non-native, invasive, weedy grassland.

The idea that older chaparral is responsible for causing large fires was originally proposed in the 1980s by comparing wildfires in Baja California and southern California. It was suggested that fire suppression activities in southern California allowed more fuel to accumulate, which in turn led to larger fires. This is similar to the observation that fire suppression and other human-caused disturbances in dry, ponderosa pine forests in the Southwest of the United States has unnaturally increased forest density. Historically, mixed-severity fires likely burned through these forests every decade or so, burning understory plants, small trees, and downed logs at low-severity, and patches of trees at high-severity. However, chaparral has a high-intensity crown-fire regime, meaning that fires consume nearly all the above ground growth whenever they burn, with a historical frequency of 30 to 150 years or more. A detailed analysis of historical fire data concluded that fire suppression activities have been ineffective at excluding fire from southern California chaparral, unlike in ponderosa pine forests. In addition, the number of fires is increasing in step with population growth and exacerbated by human-caused climate change. Chaparral stand age does not have a significant correlation to its tendency to burn.

Large, infrequent, high-intensity wildfires are part of the natural fire regime for California chaparral. Extreme weather conditions (low humidity, high temperature, high winds), drought, and low fuel moisture are the primary factors in determining how large a chaparral fire becomes.




Clinker

Clinker may refer to:


Clinker may also refer to:


Clipper

A clipper was a type of mid-19th-century merchant sailing vessel, designed for speed. Clippers were generally narrow for their length, small by later 19th-century standards, could carry limited bulk freight, and had a large total sail area. "Clipper" does not refer to a specific sailplan; clippers may be schooners, brigs, brigantines, etc., as well as full-rigged ships. Clippers were mostly constructed in British and American shipyards, although France, Brazil, the Netherlands, and other nations also produced some. Clippers sailed all over the world, primarily on the trade routes between the United Kingdom and China, in transatlantic trade, and on the New York-to-San Francisco route around Cape Horn during the California Gold Rush. Dutch clippers were built beginning in the 1850s for the tea trade and passenger service to Java.

The boom years of the clipper era began in 1843 in response to a growing demand for faster delivery of tea from China and continued with the demand for swift passage to gold fields in California and Australia beginning in 1848 and 1851, respectively. The era ended with the opening of the Suez Canal in 1869.

The term "clipper" most likely derives from the verb "clip", which in former times meant, among other things, to run or fly swiftly. Dryden, the English poet, used the word "clip" to describe the swift flight of a falcon in the 17th century when he said, "And, with her eagerness the quarry missed, Straight flies at check, and clips it down the wind." The ships appeared to clip along the ocean water. The term "clip" became synonymous with "speed" and was also applied to fast horses and sailing ships. "To clip it", and "going at a good clip", are remaining expressions.

The first application of the term "clipper", in a nautical sense, is uncertain. At first, fast sailing vessels were referred to as "Virginia-built" or "pilot-boat model", with the name "Baltimore-built" appearing during the War of 1812. In the final days of the slave trade ("circa" 1835–1850)just as the type was dying outthe term, Baltimore clipper, became common and remained current in the last quarter of the 18th century through to the first half of the 19th century. The retrospective application of the word "clipper" to these vessels has caused confusion.

The Oxford English Dictionary's earliest quote (referring to the Baltimore clipper) is from 1824. The dictionary cites Royal Navy officer and novelist Frederick Marryat as using the term in 1830. British newspaper usage of the term can be found as early as 1832 and in shipping advertisements from 1835. A US court case of 1834 has evidence that discusses a clipper being faster than a brig.

A clipper is a sailing vessel designed for speed, a priority that takes precedence over cargo-carrying capacity or building or operating costs. It is not restricted to any one rig (while many were fully rigged ships, others were barques, brigs, or schooners), nor was the term restricted to any one hull type. Howard Chapelle lists three basic hull types for clippers. The first was characterised by the sharp and ends found in the Baltimore clipper. The second was a hull with a full midsection and modest deadrise, but sharp endsthis was a development of the hull form of transatlantic packets. The third was more experimental, with deadrise and sharpness being balanced against the need to carry a profitable quantity of cargo. A clipper carried a large sail area and a fast hull; by the standards of any other type of sailing ship, a clipper was greatly over-canvassed. The last defining feature of a clipper, in the view of maritime historian David MacGregor, was a captain who had the courage, skill, and determination to get the fastest speed possible out of her.

In assessing the hull of a clipper, different maritime historians use different criteria to measure "sharpness", "fine lines" or "fineness", a concept which is explained by comparing a rectangular cuboid with the underwater shape of a vessel's hull. The more material one has to carve off the cuboid to achieve the hull shape, the sharper the hull. Ideally, a maritime historian would be able to look at either the block coefficient of fineness or the prismatic coefficient of various clippers, but measured drawings or accurate half models may not exist to calculate either of these figures. An alternative measure of sharpness for hulls of a broadly similar shape is the coefficient of underdeck tonnage, as used by David MacGregor in comparing tea clippers. This could be calculated from the measurements taken to determine the registered tonnage, so can be applied to more vessels.

An extreme clipper has a hull of great fineness, as judged either by the prismatic coefficient, the coefficient of underdeck tonnage, or some other technical assessment of hull shape. This term has been misapplied in the past, without reference to hull shape. As commercial vessels, these are totally reliant on speed to generate a profit for their owners, as their sharpness limits their cargo-carrying capacity.

A medium clipper has a cargo-carrying hull that has some sharpness. In the right conditions and with a capable captain, some of these achieved notable quick passages. They were also able to pay their way when the high freight rates often paid to a fast sailing ship were not available (in a fluctuating market).

The term "clipper" applied to vessels between these two categories. They often made passages as fast as extreme clippers, but had less difficulty in making a living when freight rates were lower.

The first ships to which the term "clipper" seems to have been applied were the Baltimore clippers, developed in the Chesapeake Bay before the American Revolution, and reached their zenith between 1795 and 1815. They were small, rarely exceeding 200 tons OM. Their hulls were sharp ended and displayed much deadrise. They were rigged as schooners, brigs, or brigantines.
In the War of 1812, some were lightly armed, sailing under letters of marque and reprisal, when the typeexemplified by "Chasseur", launched at Fells Point, Baltimore in 1814became known for her incredible speed; the deep draft enabled the Baltimore clipper to sail close to the wind. Clippers, running the British blockade of Baltimore, came to be recognized for speed rather than cargo space.

The type existed as early as 1780. A 1789 drawing of purchased by the Royal Navy in 1780 in the West Indiesrepresents the earliest draught of what became known as the Baltimore clipper.

Vessels of the Baltimore clipper type continued to be built for the slave trade, being useful for escaping enforcement of the British and American legislation prohibiting the trans-Atlantic slave trade. Some of these Baltimore clippers were captured when working as slavers, condemned by the appropriate court, and sold to owners who then used them as opium clippersmoving from one illegal international trade to another.

"Ann McKim", built in Baltimore in 1833 by the Kennard & Williamson shipyard, is considered by some to be the original clipper ship. (Maritime historians Howard I. Chapelle and David MacGregor decry the concept of the "first" clipper, preferring a more evolutionary, multiple-step development of the type.) She measured 494 tons OM, and was built on the enlarged lines of a Baltimore clipper, with sharply raked stem, counter stern, and square rig. Although "Ann McKim" was the first large clipper ship ever constructed, she cannot be said to have founded the clipper ship era, or even that she directly influenced shipbuilders, since no other ship was built like her, but she may have suggested the clipper design in vessels of ship rig. She did, however, influence the building of "Rainbow" in 1845, the first extreme clipper ship.

In Aberdeen, Scotland, shipbuilders Alexander Hall and Sons developed the "Aberdeen" clipper bow in the late 1830s; the first was "Scottish Maid" launched in 1839. "Scottish Maid", 150 tons OM, was the first British clipper ship. ""Scottish Maid" was intended for the Aberdeen-London trade, where speed was crucial to compete with steamships. The Hall brothers tested various hulls in a water tank and found the clipper design most effective. The design was influenced by tonnage regulations. Tonnage measured a ship's cargo capacity and was used to calculate tax and harbour dues. The new 1836 regulations measured depth and breadth with length measured at half midship depth. Extra length above this level was tax-free and became a feature of clippers. "Scottish Maid" proved swift and reliable and the design was widely copied." The earliest British clipper ships were built for trade within the British Isles ("Scottish Maid" was built for the Aberdeen to London trade). Then followed the vast clipper trade of tea, opium, spices, and other goods from the Far East to Europe, and the ships became known as "tea clippers".

From 1839, larger American clipper ships started to be built beginning with "Akbar", 650 tons OM, in 1839, and including the 1844-built "Houqua", 581 tons OM. These larger vessels were built predominantly for use in the China tea trade and known as "tea clippers".

Then in 1845 "Rainbow", 757 tons OM, the first extreme clipper, was launched in New York. These American clippers were larger vessels designed to sacrifice cargo capacity for speed. They had a bow lengthened above the water, a drawing out and sharpening of the forward body, and the greatest breadth further aft. Extreme clippers were built in the period 1845 to 1855.

In 1851, shipbuilders in Medford, Massachusetts, built what is sometimes called one of the first medium clippers, the "Antelope", often called the "Antelope of Boston" to distinguish her from other ships of the same name. A contemporary ship-design journalist noted that "the design of her model was to combine large stowage capacity with good sailing qualities." "Antelope" was relatively flat-floored and had only an 8-inch deadrise at half-floor.

The medium clipper, though still very fast, could carry more cargo. After 1854, extreme clippers were replaced in American shipbuilding yards by medium clippers.

The "Flying Cloud" was a clipper ship built in 1851 that established the fastest passage between New York and San Francisco within weeks of her launching, then broke her own records three years later, which stood at 89 days 8 hours until 1989. (The other contender for this "blue ribbon" title was the medium clipper "Andrew Jackson"an unresolvable argument exists over timing these voyages "from pilot to pilot"). "Flying Cloud" was the most famous of the clippers built by Donald McKay. She was known for her extremely close race with the "Hornet" in 1853; for having a woman navigator, Eleanor Creesy, wife of Josiah Perkins Creesy, who skippered the "Flying Cloud" on two record-setting voyages from New York to San Francisco; and for sailing in the Australia and timber trades.

Clipper ships largely ceased being built in American shipyards in 1859 when, unlike the earlier boom years, only four clipper ships were built; a few were built in the 1860s. 
British clipper ships continued to be built after 1859. From 1859, a new design was developed for British clipper ships that was nothing like the American clippers; these ships continued to be called extreme clippers. The new design had a sleek, graceful appearance, less sheer, less freeboard, lower bulwarks, and smaller breadth. They were built for the China tea trade, starting with "Falcon" in 1859, and continuing until 1870. The earlier ships were made from wood, though some were made from iron, just as some British clippers had been made from iron prior to 1859. In 1863, the first tea clippers of composite construction were brought out, combining the best of both worlds. Composite clippers had the strength of an iron hull framework but with wooden planking that, with properly insulated fastenings, could use copper sheathing without the problem of galvanic corrosion. Copper sheathing prevented fouling and teredo worm, but could not be used on iron hulls. The iron framework of composite clippers was less bulky and lighter, so allowing more cargo in a hull of the same external shape.

After 1869, with the opening of the Suez Canal that greatly advantaged steam vessels (see Decline below), the tea trade collapsed for clippers. From the late 1860s until the early 1870s, the clipper trade increasingly focused on the Britain to Australia and New Zealand route, carrying goods and immigrants, services that had begun earlier with the Australian Gold Rush of the 1850s. British-built clipper ships and many American-built, British-owned ships were used. Even in the 1880s, sailing ships were still the main carriers of cargo between Britain, and Australia and New Zealand. This trade eventually became unprofitable, and the ageing clipper fleet became unseaworthy.
Before the early 18th century, the East India Company paid for its tea mainly in silver. When the Chinese emperor chose to embargo European-manufactured commodities and demand payment for all Chinese goods in silver, the price rose, restricting trade. The East India Company began to produce opium in India, something desired by the Chinese as much as tea was by the British. This had to be smuggled into China on smaller, fast-sailing ships, called "opium clippers". Some of these were built specifically for the purposemostly in India and Britain, such as the 1842-built "Ariel", 100 tons OM. Some fruit schooners were bought for this trade, as were some Baltimore clippers.

Among the most notable clippers were the China clippers, also called tea clippers, designed to ply the trade routes between Europe and the East Indies. The last example of these still in reasonable condition is "Cutty Sark", preserved in dry dock at Greenwich, United Kingdom. Damaged by fire on 21 May 2007 while undergoing conservation, the ship was permanently elevated 3.0 m above the dry dock floor in 2010 as part of a plan for long-term preservation.

Clippers were built for seasonal trades such as tea, where an early cargo was more valuable, or for passenger routes. One passenger ship survives, the "City of Adelaide" designed by William Pile of Sunderland. The fast ships were ideally suited to low-volume, high-profit goods, such as tea, opium, spices, people, and mail. The return could be spectacular. The "Challenger" returned from Shanghai with "the most valuable cargo of tea and silk ever to be laden in one bottom". Competition among the clippers was public and fierce, with their times recorded in the newspapers.

The last China clippers had peak speeds over , but their average speeds over a whole voyage were substantially less. The joint winner of the Great Tea Race of 1866 logged about 15,800 nautical miles on a 99-day trip. This gives an average speed slightly over . The key to a fast passage for a tea clipper was getting across the China Sea against the monsoon winds that prevailed when the first tea crop of the season was ready. These difficult sailing conditions (light and/or contrary winds) dictated the design of tea clippers. The US clippers were designed for the strong winds encountered on their route around Cape Horn.

Donald McKay's "Sovereign of the Seas" reported the highest speed ever achieved by a sailing ship of the era, , made while running her easting down to Australia in 1854. (John Griffiths' first clipper, the "Rainbow", had a top speed of 14 knots.) Eleven other instances are reported of a ship's logging or over. Ten of these were recorded by American clippers.
Besides the breath-taking day's run of the "Champion of the Seas", 13 other cases are known of a ship's sailing over in 24 hours.
With few exceptions, though, all the port-to-port sailing records are held by the American clippers.
The 24-hour record of the "Champion of the Seas", set in 1854, was not broken until 1984 (by a multihull), or 2001 (by another monohull).

The American clippers sailing from the East Coast to the California goldfields were working in a booming market. Freight rates were high everywhere in the first years of the 1850s. This started to fade in late 1853. The ports of California and Australia reported that they were overstocked with goods that had been shipped earlier in the year. This gave an accelerating fall in freight rates that was halted, however, by the start of the Crimean War in March 1854, as many ships were now being chartered by the French and British governments. The end of the Crimean War in April 1856 released all this capacity back on the world shipping marketsthe result being a severe slump. The next year had the Panic of 1857, with effects on both sides of the Atlantic. The United States was just starting to recover from this in 1861 when the American Civil War started, causing significant disruption to trade in both Union and Confederate states.
As the economic situation deteriorated in 1853, American shipowners either did not order new vessels, or specified an ordinary clipper or a medium clipper instead of an extreme clipper. No extreme clipper was launched in an American shipyard after the end of 1854 and only a few medium clippers after 1860.

By contrast, British trade recovered well at the end of the 1850s. Tea clippers had continued to be launched during the depressed years, apparently little affected by the economic downturn. The long-distance route to China was not realistically challenged by steamships in the early part of the 1860s. No true steamer (as opposed to an auxiliary steamship) had the fuel efficiency to carry sufficient cargo to make a profitable voyage. The auxiliary steamships struggled to make any profit.
The situation changed in 1866 when the Alfred Holt-designed and owned SS "Agamemnon" made her first voyage to China. Holt had persuaded the Board of Trade to allow higher steam pressures in British merchant vessels. Running at 60 psi instead of the previously permitted 25 psi, and using an efficient compound engine, "Agamemnon" had the fuel efficiency to steam at 10 knots to China and back, with coaling stops at Mauritius on the outward and return legscrucially carrying sufficient cargo to make a profit.

In 1869, the Suez Canal opened, giving steamships a route about shorter than that taken by sailing ships round the Cape of Good Hope. Despite initial conservatism by tea merchants, by 1871, tea clippers found strong competition from steamers in the tea ports of China. A typical passage time back to London for a steamer was 58 days, while the fastest clippers could occasionally make the trip in less than 100 days; the average was 123 days in the 1867–68 tea season. The freight rate for a steamer in 1871 was roughly double that paid to a sailing vessel. Some clipper owners were severely caught out by this; several extreme clippers had been launched in 1869, including "Cutty Sark", "Norman Court" and "Caliph".

Of the many clipper ships built during the mid-19th century, only two are known to survive. The only intact survivor is "Cutty Sark", which was preserved as a museum ship in 1954 at Greenwich for public display. The other known survivor is "City of Adelaide"; unlike "Cutty Sark", she was reduced to a hulk over the years. She eventually sank at her moorings in 1991, but was raised the following year, and remained on dry land for years. "Adelaide" (or S.V. "Carrick") is the older of the two survivors, and was transported to Australia for conservation.

The clipper legacy appears in collectible cards and in the name of a basketball team.

Departures of clipper ships, mostly from New York and Boston to San Francisco, were advertised by clipper-ship sailing cards. These cards, slightly larger than today's postcards, were produced by letterpress and wood engraving on coated card stock. Most clipper cards were printed in the 1850s and 1860s, and represented the first pronounced use of color in American advertising art. Perhaps 3,500 cards survive. With their rarity and importance as artifacts of nautical, Western, and printing history, clipper cards are valued by both private collectors and institutions.

The Los Angeles Clippers of the National Basketball Association take their name from the type of ship. After the Buffalo Braves moved to San Diego, California in 1978, a contest was held to choose a new name. The winning name highlighted the city's connection with the clippers that frequented San Diego Bay. The team retained the name in its 1984 move to Los Angeles.

The airline Pan Am named its aircraft beginning with the word 'Clipper' and used Clipper as its callsign. This was intended to evoke an image of speed and glamour.








Clive Anderson

Clive Stuart Anderson (born 10 December 1952) is an English television and radio presenter, comedy writer, and former barrister. Winner of a British Comedy Award in 1991, Anderson began experimenting with comedy and writing comedic scripts during his 15-year legal career, before becoming host of "Whose Line Is It Anyway?", initially a radio show on BBC Radio 4 in 1988, before moving to television on Channel 4 from 1988 to 1999. He was also host of his own chat show "Clive Anderson Talks Back", which changed its name to "Clive Anderson All Talk" in 1996, from 1989 to 2001. He has also hosted many radio programmes, and made guest appearances on "Have I Got News for You", "Mock the Week" and "QI".

Anderson's mother was English and his parents met while serving in the RAF. He was educated at Stanburn Primary School and Harrow County School for Boys then a grammar school which closed in 1975. His group of contemporaries included Geoffrey Perkins and Michael Portillo. His Scottish father originally from Glasgow was promoted to manager of the Bradford & Bingley's Building Society, Wembley branch. Anderson attended Selwyn College, Cambridge, where, from 1974 to 1975, he was President of the Cambridge Footlights. He was called to the bar at the Middle Temple in 1976 and became a practising barrister, specialising in criminal law. While still practising law, he continued performing, including taking a show to the Edinburgh Fringe in 1981 with Griff Rhys Jones.

Anderson was involved in the fledgling alternative comedy scene in the early 1980s and was the first act to appear at The Comedy Store when it opened in 1979. He made his name as host of the original UK version of the improvised television comedy show "Whose Line Is It Anyway?", which ran for 10 series on Channel 4 from 1988 to 1999.

Anderson hosted his own chat show "Clive Anderson Talks Back", which ran for 10 series on Channel 4 from 1989 to 1996. The show then moved to the BBC, with the name changed to "Clive Anderson All Talk", running for 4 series from 1996 to 1999. In one incident in 1997, Anderson interviewed the Bee Gees. Throughout the interview, he repeatedly joked about their songs from the "Saturday Night Fever" era, and when discussing the names the band had before becoming 'The Bee Gees', one of them was 'Les Tosseurs', with Anderson's comment, "You'll always be les tossers to me", ultimately prompting the band to walk out of the interview. Anderson once had a glass of water poured over his head by a perturbed Richard Branson, to which Anderson remarked "I'm used to that; I've flown Virgin". When singer and actress Cher appeared on the show, Anderson alluded to her alleged cosmetic surgery, asking her "You look like a million dollars – is that how much it cost?" He also said to author and politician Jeffrey Archer, in response to his derogatory comment about the show, "You're a critic too... there's no beginning to your talents". Archer retorted that "The old ones are always the best" for Anderson to reply "Yes, I've read your books".

He has made ten appearances on "Have I Got News for You". In 1996, a heated exchange occurred on the show when he joked to fellow guest Piers Morgan that the "Daily Mirror" was now, thanks to Morgan (then its editor), almost as good as "The Sun". When asked by Morgan, "What do you know about editing newspapers?" he swiftly replied "About as much as you do". Anderson has also frequently appeared on "QI". In 2007, he featured as a regular panellist on the ITV comedy show "News Knight". From 2019 to 2020 he co-hosted the television series "Mystic Britain" on the Sky television channel Smithsonian. 

In 2005, he presented the short-lived quiz "Back in the Day" for Channel 4. On 25 February 2008, he started to present "Brainbox Challenge", a new game show, for BBC Two. Later that year, he presented a talent show-themed reality TV series produced by the BBC entitled "Maestro", starring eight celebrities. In 2009, Anderson was the television host of the BBC's "Last Night of the Proms".

In November 2023, Clive appeared on TV game show "Richard Osman's House of Games", winning the show by one point.

Anderson presents legal show "Unreliable Evidence" on BBC Radio 4. He also covered the Sunday morning 11 a.m. to 1 p.m. show on BBC Radio 2 until the end of January 2008.

In early 1988, Anderson hosted the original radio version of "Whose Line Is It Anyway?", which ran for 6 episodes on BBC Radio 4 before the show moved to television later that year.

It was announced in April 2008 that Anderson, who had previously filled in for host Ned Sherrin from 2006 until Sherrin's death in 2007, would be taking over as permanent host of "Loose Ends". He also hosted six series of "Clive Anderson's Chat Room" on BBC Radio 2 from 2004 to 2009. Anderson has appeared on BBC Radio 4's "The Unbelievable Truth" hosted by David Mitchell.

Anderson also presented the radio show "The Guessing Game" on BBC Radio Scotland. Anderson has also appeared on BBC Radio 5 Live's "Fighting Talk".

Anderson is a comedy sketch writer who has written for Frankie Howerd, "Not the Nine O'Clock News", and Griff Rhys Jones and Mel Smith. One of his early comedy writing projects was "Black Cinderella Two Goes East" with Rory McGrath for BBC Radio 4 in 1978. As well as writing comedy, Anderson is also a frequent contributor to newspapers and was a regular columnist for "The Sunday Correspondent".

Anderson lives in Highbury, North London, with his consultant wife, Jane Anderson, a physician who has spent her career in managing HIV/AIDS. The couple has three children.

He supports Arsenal, and Rangers football teams. He is President of the Woodland Trust and became Vice Patron of the Solicitors' Benevolent Association, a registered charity.

The show "Whose Line is it Anyway?" won a BAFTA award in 1990. Later, Anderson won both the "Top Entertainment Presenter" and "Top Radio Comedy Personality" at the British Comedy Awards in 1991. In 2023 he was made an Honorary Fellow of Selwyn College, Cambridge.


Cold fusion

Cold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. It would contrast starkly with the "hot" fusion that is known to take place naturally within stars and artificially in hydrogen bombs and prototype fusion reactors under immense pressure and at temperatures of millions of degrees, and be distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model that would allow cold fusion to occur.

In 1989, two electrochemists, Martin Fleischmann and Stanley Pons, reported that their apparatus had produced anomalous heat ("excess heat") of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention and raised hopes of a cheap and abundant source of energy.

Many scientists tried to replicate the experiment with the few details available. Hopes faded with the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion. Presently, since articles about cold fusion are rarely published in peer-reviewed mainstream scientific journals, they do not attract the level of scrutiny expected for mainstream scientific publications.

Nevertheless, some interest in cold fusion has continued through the decades—for example, a Google-funded failed replication attempt was published in a 2019 issue of "Nature". A small community of researchers continues to investigate it, often under the alternative designations "low-energy nuclear reactions" ("LENR") or "condensed matter nuclear science" ("CMNS").

Nuclear fusion is normally understood to occur at temperatures in the tens of millions of degrees. This is called "thermonuclear fusion". Since the 1920s, there has been speculation that nuclear fusion might be possible at much lower temperatures by catalytically fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before the majority of scientists criticized their claim as incorrect after many found they could not replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of researchers who believe that such reactions happen and hope to gain wider recognition for their experimental evidence.

The ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian-born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, saying that the helium they measured was due to background from the air.

In 1927, Swedish scientist John Tandberg reported that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for "a method to produce helium and useful reaction energy". Due to Paneth and Peters's retraction and his inability to explain the physical process, his patent application was denied. After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.

The term "cold fusion" was used as early as 1956 in an article in "The New York Times" about Luis Alvarez's work on muon-catalyzed fusion. Paul Palmer and then Steven Jones of Brigham Young University used the term "cold fusion" in 1986 in an investigation of "geo-fusion", the possible existence of fusion involving hydrogen isotopes in a planetary core. In his original paper on this subject with Clinton Van Siclen, submitted in 1985, Jones had coined the term "piezonuclear fusion".

The most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program from the US to France after the controversy erupted.

Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.

In 1988, Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled "Cold nuclear fusion" that had been published in "Scientific American" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable "excess energy", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to publish their results simultaneously, though their accounts of their 6 March meeting differ.

In mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to "Nature" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, disclosing their work at a press conference on 23 March (they claimed in the press release that it would be published in "Nature" but instead submitted their paper to the "Journal of Electroanalytical Chemistry"). Jones, upset, faxed in his paper to "Nature" after the press conference.

Fleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. Many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.

The announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: "What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics."

Although the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to "Nature" reproducing excess heat, although it passed peer review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal "Fusion Technology". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that "essentially all" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree Celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as "cold fusion" or "fusion confusion" in the news.

In April 1989, Fleischmann and Pons published a "preliminary note" in the "Journal of Electroanalytical Chemistry". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.

Nevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.

On 30 April 1989, cold fusion was declared dead by "The New York Times". The "Times" called it a circus the same day, and the "Boston Herald" attacked cold fusion the following day.

On 1 May 1989, the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of ""the incompetence and delusion of Pons and Fleischmann,"" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.

On 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.

From 8 May, only the A&M tritium results kept cold fusion afloat.

In July and November 1989, "Nature" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including "Science", "Physical Review Letters", and "Physical Review C" (nuclear physics).

In August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.

The United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of "focused experiments within the general funding system". Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of "pathological science".

In March 1990, Michael H. Salamon, a physicist from the University of Utah, and nine co-authors reported negative results. University faculty were then "stunned" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.

In early May 1990, one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in "Science" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.

On 30 June 1991, the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.

On 1 January 1991, Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischmann resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.

Mostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.

A 1991 review by a cold fusion proponent had calculated "about 600 scientists" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis setups in spite of the rejection by the mainstream community. "The Boston Globe" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India. For example, it was reported in "Nature", in May, 2019, that Google had spent approximately $10 million on cold fusion research. A group of scientists at well-known research labs (e.g., MIT, Lawrence Berkeley National Lab, and others) worked for several years to establish experimental protocols and measurement techniques in an effort to re-evaluate cold fusion to a high standard of scientific rigor. Their reported conclusion: no cold fusion.

In 2021, following "Nature's" 2019 publication of anomalous findings that might only be explained by some localized fusion, scientists at the Naval Surface Warfare Center, Indian Head Division announced that they had assembled a group of scientists from the Navy, Army and National Institute of Standards and Technology to undertake a new, coordinated study. With few exceptions, researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR), Chemically Assisted Nuclear Reactions (CANR), Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) or Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with "cold fusion". The new names avoid making bold implications, like implying that fusion is actually occurring.

The researchers who continue their investigations acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated increased attention from mainstream researchers and described cold fusion as:

United States Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002 they released a two-volume report, "Thermal and nuclear aspects of the Pd/DO system", with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.

In August 2003, the U.S. Secretary of Energy, Spencer Abraham, ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were "split approximately evenly" on whether the experiments had produced energy in the form of heat, but "most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented'". In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they did not recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research "could be helpful in resolving some of the controversies in the field". They summarized its conclusions thus:

Cold fusion researchers placed a "rosier spin" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused "a huge upswing in interest in funding cold fusion research". However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as "cold fusion" even if they are not, in order to attract the attention of journalists.

In February 2012, millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news show "60 Minutes", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum under extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which a professor associated with the project, Mark Prelas, says bursts of millions of neutrons a second were recorded, which was stopped because "his research account had been frozen". He claims that the new experiment has already seen "neutron emissions at similar levels to the 1991 observation".

In May 2016, the United States House Committee on Armed Services, in its report on the 2017 National Defense Authorization Act, directed the Secretary of Defense to "provide a briefing on the military utility of recent U.S. industrial base LENR advancements to the House Committee on Armed Services by September 22, 2016".

Since the Fleischmann and Pons announcement, the Italian national agency for new technologies, energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomenon to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA started a research program which claimed to have found excess power of up to 500 percent, and in 2009, ENEA hosted the 15th cold fusion conference.

Between 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a "New Hydrogen Energy (NHE)" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated "We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future." In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher was Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and Michael McKubre at SRI.

In the 1990s, India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended that the Indian government revive this research. Projects were commenced at Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research has stalled since the 1990s. A special section in the Indian multidisciplinary journal "Current Science" published 33 cold fusion papers in 2015 by major cold fusion researchers including several Indian researchers.

A cold fusion experiment usually includes:

Electrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also "heat-after-death" experiments, where the evolution of heat is monitored after the electric current is turned off.

The most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear—this is known as the "loading time," the time required to saturate the palladium electrode with hydrogen (see "Loading ratio" section).

The Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.

An excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, an electrolysis cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.

Unable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments. In 1993, after their original report, Fleischmann reported "heat-after-death" experiments—where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report has also become part of subsequent cold fusion claims.

Known instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detection of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 10 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.

Several medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the United States Department of Energy (DOE) in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.

In response to doubts about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for He, with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.

One of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and nuclear transmutations. Some researchers also claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.

Researchers in the field do not agree on a theory for cold fusion. One proposal considers that hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes. However, the reduction in separation is not enough to create the fusion rates claimed in the original experiment, by a factor of ten. It was also proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations not convincing and inconsistent with current physics theories.

Criticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility that fusion reactions have occurred in electrolysis setups or criticizing the excess heat measurements as being spurious, erroneous, or due to poor methodology or controls. There are several reasons why known fusion reactions are an unlikely explanation for the excess heat and associated cold fusion claims.

Because nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this charged repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat. In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.

Paneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium, which at the time was needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.

This was also the belief of geologist Palmer, who convinced Steven Jones that the helium-3 occurring naturally in Earth perhaps came from fusion involving hydrogen isotopes inside catalysts like nickel and palladium. This led their team in 1986 to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had much the same belief, but they calculated the pressure to be of 10 atmospheres, when cold fusion experiments achieve a loading ratio of only one to one, which has only between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.

Conventional deuteron fusion is a two-step process, in which an unstable high-energy intermediary is formed:
Experiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:
Only about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (6.242 × 10 MeV/s) of nuclear power were produced from ~2.2575 × 10 deuteron fusion individual reactions each second consistent with known branching ratios, the resulting neutron and tritium (H) production would be easily measured. Some researchers reported detecting He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.

The known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then there would be measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment cause only small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.

Cold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and have asserted that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Critics have further taken issue with what they describe as mistakes or errors of interpretation that cold fusion researchers have made in calorimetry analyses and energy budgets.

In 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups, however, reported successful reproductions of cold fusion during this time. In July 1989, an Indian group from the Bhabha Atomic Research Centre (P. K. Iyengar and M. Srinivasan) and in October 1989, John Bockris' group from Texas A&M University reported on the creation of tritium. In December 1990, professor Richard Oriani of the University of Minnesota reported excess heat.

Groups that did report successes found that some of their cells were producing the effect, while other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:

Cold fusion researchers (McKubre since 1994, ENEA in 2011) have speculated that a cell that is loaded with a deuterium/palladium ratio lower than 100% (or 1:1) will not produce excess heat. Since most of the negative replications from 1989 to 1990 did not report their ratios, this has been proposed as an explanation for failed reproducibility. This loading ratio is hard to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells; there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier now uses a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.

Some research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.

The calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.

One assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid DO into gaseous D and O. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.

Another assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.

The ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper "Cold Fusion: A Hypothesis" in "Physical Review Letters", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of "PRL") in protest.

The number of papers sharply declined after 1990 because of two simultaneous phenomena: first, scientists abandoned the field; second, journal editors declined to review new papers. Consequently, cold fusion fell off the ISI charts. Researchers who got negative results turned their backs on the field; those who continued to publish were simply ignored. A 1993 paper in "Physics Letters A" was the last paper published by Fleischmann, and "one of the last reports [by Fleischmann] to be formally challenged on technical grounds by a cold fusion skeptic."

The "Journal of Fusion Technology" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.

The decline of publications in cold fusion has been described as a "failed information epidemic". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to "normal" science more difficult.

Cold fusion reports continued to be published in a few journals like "Journal of Electroanalytical Chemistry" and "Il Nuovo Cimento". Some papers also appeared in "Journal of Physical Chemistry", "Physics Letters A", "International Journal of Hydrogen Energy", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, "Naturwissenschaften" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board. In 2015 the Indian multidisciplinary journal "Current Science" published a special section devoted entirely to cold fusion related papers.

In the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as "Fusion Facts", "Cold Fusion Magazine", "Infinite Energy Magazine" and "New Energy Times" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.

Cold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The International Conference on Cold Fusion (ICCF) was first held in 1990 and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics, thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science—for reasons that are detailed in the subsequent research section above—but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as "low-energy nuclear reactions", or LENR, but according to sociologist Bart Simon the "cold fusion" label continues to serve a social function in creating a collective identity for the field.

Since 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include "invited symposium(s)" on cold fusion. An ACS program chair, Gopal Coimbatore, said that without a proper forum the matter would never be discussed and, "with the world facing an energy crisis, it is worth exploring all possibilities."

On 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis setup and a CR-39 detector, a result previously published in "Naturwissenschaften". The authors claim that these neutrons are indicative of nuclear reactions. Without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.

Although details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from 5 to 12 April. An MIT graduate student applied for a patent but was reportedly rejected by the USPTO in part by the citation of the "negative" MIT Plasma Fusion Center's cold fusion experiment of 1989. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and in March 1998 it said that it would no longer defend its patents.

The U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is "useful", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being "inoperative" are rare, since such rejections need to demonstrate "proof of total incapacity", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.

A U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more on the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving "new nuclear physics" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of "cold fusion" from the patent description to avoid having it rejected outright.

At least one patent related to cold fusion has been granted by the European Patent Office.

A patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.

A 1990 Michael Winner film "Bullseye!", starring Michael Caine and Roger Moore, referenced the Fleischmann and Pons experiment. The film – a comedy – concerned conmen trying to steal scientists' purported findings. However, the film had a poor reception, described as "appallingly unfunny".

In "Undead Science", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in "Murphy Brown" and "The Simpsons". It was adopted as a software product name Adobe ColdFusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.

The plot of "The Saint", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. In "Undead Science", Simon posits that film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.

Similarly, the tenth episode of 2000 science fiction TV drama "Life Force" ("Paradise Island") is also based around cold fusion, specifically the efforts of eccentric scientist Hepzibah McKinley (Amanda Walker), who is convinced she has perfected it based on her father's incomplete research into the subject. The episode explores its potential benefits and viability within the ongoing post-apocalyptic global warming scenario of the series.

In the 2023 video game "Atomic Heart", cold fusion is responsible for nearly all of the technological advances.



Coal tar

Coal tar is a thick dark liquid which is a by-product of the production of coke and coal gas from coal. It is a type of creosote. It has both medical and industrial uses. Medicinally it is a topical medication applied to skin to treat psoriasis and seborrheic dermatitis (dandruff). It may be used in combination with ultraviolet light therapy. Industrially it is a railroad tie preservative and used in the surfacing of roads. Coal tar was listed as a known human carcinogen in the first Report on Carcinogens from the U.S. Federal Government, issued in 1980.
Coal tar was discovered circa 1665 and used for medical purposes as early as the 1800s. Circa 1850, the discovery that it could be used as the main raw material for the synthesis of dyes engendered an entire industry. It is on the World Health Organization's List of Essential Medicines. Coal tar is available as a generic medication and over the counter.
Side effects include skin irritation, sun sensitivity, allergic reactions, and skin discoloration. It is unclear if use during pregnancy is safe for the baby and use during breastfeeding is not typically recommended. The exact mechanism of action is unknown. It is a complex mixture of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds. It demonstrates antifungal, anti-inflammatory, anti-itch, and antiparasitic properties.

Coal tar is produced through thermal destruction (pyrolysis) of coal. Its composition varies with the process and type of coal used – lignite, bituminous or anthracite.

Coal tar is a mixture of approximately 10,000 chemicals, of which only about 50% have been identified. Most of the chemical compounds are polycyclic aromatic hydrocarbon:


Others: benzene, toluene, xylenes, cumenes, coumarone, indene, benzofuran, naphthalene and methyl-naphthalenes, acenaphthene, fluorene, phenol, cresols, pyridine, picolines, phenanthracene, carbazole, quinolines, fluoranthene. Many of these constituents are known carcinogens.

Various phenolic coal tar derivatives have analgesic (pain-killer) properties. These included acetanilide, phenacetin, and paracetamol aka acetaminophen. Paracetamol may be the only coal-tar derived analgesic still in use today. Industrial phenol is now usually synthesized from crude oil rather than coal tar.

Coal tar derivatives are contra-indicated for people with the inherited red cell blood disorder glucose-6-phosphate dehydrogenase deficiency (G6PD deficiency), as they can cause oxidative stress leading to red blood cell breakdown.

The exact mechanism of action is unknown. Coal tar is a complex mixture of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds.

It is a keratolytic agent, which reduces the growth rate of skin cells and softens the skin's keratin.

Coal tar is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. Coal tar is generally available as a generic medication and over the counter.

Coal tar is used in medicated shampoo, soap and ointment. It demonstrates antifungal, anti-inflammatory, anti-itch, and antiparasitic properties. It may be applied topically as a treatment for dandruff and psoriasis, and to kill and repel head lice. It may be used in combination with ultraviolet light therapy.

Coal tar may be used in two forms: crude coal tar () or a coal tar solution () also known as liquor carbonis detergens (LCD). Named brands include Denorex, Balnetar, Psoriasin, Tegrin, T/Gel, and Neutar. When used in the extemporaneous preparation of topical medications, it is supplied in the form of coal tar topical solution USP, which consists of a 20% w/v solution of coal tar in alcohol, with an additional 5% w/v of polysorbate 80 USP; this must then be diluted in an ointment base such as petrolatum.

Coal tar was a component of the first sealed roads. In its original development by Edgar Purnell Hooley, tarmac was tar covered with granite chips. Later the filler used was industrial slag. Today, petroleum derived binders and sealers are more commonly used. These sealers are used to extend the life and reduce maintenance cost associated with asphalt pavements, primarily in asphalt road paving, car parks and walkways.

Coal tar is incorporated into some parking-lot sealcoat products used to protect the structural integrity of the underlying pavement. Sealcoat products that are coal-tar based typically contain 20 to 35 percent coal-tar pitch. Research shows it is used throughout the United States of America, however several areas have banned its use in sealcoat products, including the District of Columbia; the city of Austin, Texas; Dane County, Wisconsin; the state of Washington; and several municipalities in Minnesota and others.

In modern times, coal tar is mostly traded as fuel and an application for tar, such as roofing. The total value of the trade in coal tar is around US$20 billion each year.








Side effects of coal tar products include skin irritation, sun sensitivity, allergic reactions, and skin discoloration. It is unclear if use during pregnancy is safe for the baby and use during breastfeeding is not typically recommended.

According to the National Psoriasis Foundation, coal tar is a valuable, safe and inexpensive treatment option for millions of people with psoriasis and other scalp or skin conditions. According to the FDA, coal tar concentrations between 0.5% and 5% are considered safe and effective for psoriasis.

Long-term, consistent exposure to coal tar likely increases the risk of non-melanoma skin cancers. Evidence is inconclusive whether medical coal tar, which does not remain on the skin for the long periods seen in occupational exposure, causes cancer, because there is insufficient data to make a judgment. While coal tar consistently causes cancer in cohorts of workers with chronic occupational exposure, animal models, and mechanistic studies, the data on short-term use as medicine in humans has so far failed to show any consistently significant increase in rates of cancer.

Coal tar contains many polycyclic aromatic hydrocarbons, and it is believed that their metabolites bind to DNA, damaging it. The PAHs found in coal tar and air pollution induce immunosenescence and cytotoxicity in epidermal cells. It's possible that the skin can repair itself from this damage after short-term exposure to PAHs but not after long-term exposure. Long-term skin exposure to these compounds can produce "tar warts", which can progress to squamous cell carcinoma.

Coal tar was one of the first chemical substances proven to cause cancer from occupational exposure, during research in 1775 on the cause of chimney sweeps' carcinoma. Modern studies have shown that working with coal tar pitch, such as during the paving of roads or when working on roofs, increases the risk of cancer.

The International Agency for Research on Cancer lists coal tars as Group 1 carcinogens, meaning they directly cause cancer. The U.S. Department of Health and Human Services lists coal tars as known human carcinogens.

In response to public health concerns regarding the carcinogenicity of PAHs some municipalities, such as the city of Milwaukee, have banned the use of common coal tar-based road and driveway sealants citing concerns of elevated PAH content in groundwater.

Coal tar causes increased sensitivity to sunlight, so skin treated with topical coal tar preparations should be protected from sunlight.

The residue from the distillation of high-temperature coal tar, primarily a complex mixture of three or more membered condensed ring aromatic hydrocarbons, was listed on 13 January 2010 as a substance of very high concern by the European Chemicals Agency.

Exposure to coal tar pitch volatiles can occur in the workplace by breathing, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit) to 0.2 mg/m benzene-soluble fraction over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m cyclohexane-extractable fraction over an 8-hour workday. At levels of 80 mg/m, coal tar pitch volatiles are immediately dangerous to life and health.

When used as a medication in the United States, coal tar preparations are considered over-the-counter drug pharmaceuticals and are subject to regulation by the Food and Drug Administration (FDA).



Cobbler

Cobbler(s) may refer to:







