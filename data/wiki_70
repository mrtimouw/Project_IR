Dada

Dada () or Dadaism was an art movement of the European avant-garde in the early 20th century, with early centres in Zürich, Switzerland, at the Cabaret Voltaire (in 1916), founded by Hugo Ball with his companion Emmy Hennings, and in Berlin in 1917. New York Dada began , and after 1920 Dada flourished in Paris. Dadaist activities lasted until the mid 1920s.
Developed in reaction to World War I, the Dada movement consisted of artists who rejected the logic, reason, and aestheticism of modern capitalist society, instead expressing nonsense, irrationality, and anti-bourgeois protest in their works. The art of the movement began primarily as performance art, but eventually spanned visual, literary, and sound media, including collage, sound poetry, cut-up writing, and sculpture. Dadaist artists expressed their discontent toward violence, war, and nationalism and maintained political affinities with radical politics on the left-wing and far-left politics.

There is no consensus on the origin of the movement's name; a common story is that the German artist Richard Huelsenbeck slid a paper knife (letter-opener) at random into a dictionary, where it landed on "dada", a colloquial French term for a hobby horse. Jean Arp wrote that Tristan Tzara invented the word at 6 p.m. on 6 February 1916, in the Café de la Terrasse in Zürich. Others note that it suggests the first words of a child, evoking a childishness and absurdity that appealed to the group. Still others speculate that the word might have been chosen to evoke a similar meaning (or no meaning at all) in any language, reflecting the movement's internationalism.The roots of Dada lie in pre-war avant-garde. The term anti-art, a precursor to Dada, was coined by Marcel Duchamp around 1913 to characterize works that challenge accepted definitions of art. Cubism and the development of collage and abstract art would inform the movement's detachment from the constraints of reality and convention. The work of French poets, Italian Futurists and the German Expressionists would influence Dada's rejection of the tight correlation between words and meaning. Works such as "Ubu Roi" (1896) by Alfred Jarry and the ballet "Parade" (1916–17) by Erik Satie would also be characterized as proto-Dadaist works. The Dada movement's principles were first collected in Hugo Ball's Dada Manifesto in 1916. Ball is seen as the founder of the Dada movement.

The Dadaist movement included public gatherings, demonstrations, and publication of art/literary journals; passionate coverage of art, politics, and culture were topics often discussed in a variety of media. Key figures in the movement included Jean Arp, Johannes Baader, Hugo Ball, Marcel Duchamp, Max Ernst, Elsa von Freytag-Loringhoven, George Grosz, Raoul Hausmann, John Heartfield, Emmy Hennings, Hannah Höch, Richard Huelsenbeck, Francis Picabia, Man Ray, Hans Richter, Kurt Schwitters, Sophie Taeuber-Arp, Tristan Tzara, and Beatrice Wood, among others. The movement influenced later styles like the avant-garde and downtown music movements, and groups including Surrealism, "nouveau réalisme", pop art and Fluxus.

Dada was an informal international movement, with participants in Europe and North America. The beginnings of Dada correspond with the outbreak of World War I. For many participants, the movement was a protest against the bourgeois nationalist and colonialist interests, which many Dadaists believed were the root cause of the war, and against the cultural and intellectual conformity—in art and more broadly in society—that corresponded to the war.

Avant-garde circles outside France knew of pre-war Parisian developments. They had seen (or participated in) Cubist exhibitions held at Galeries Dalmau, Barcelona (1912), Galerie Der Sturm in Berlin (1912), the Armory Show in New York (1913), SVU Mánes in Prague (1914), several Jack of Diamonds exhibitions in Moscow and at Moderne Kunstkring, Amsterdam (between 1911 and 1915). Futurism developed in response to the work of various artists. Dada subsequently combined these approaches.

Many Dadaists believed that the 'reason' and 'logic' of bourgeois capitalist society had led people into war. They expressed their rejection of that ideology in artistic expression that appeared to reject logic and embrace chaos and irrationality. For example, George Grosz later recalled that his Dadaist art was intended as a protest "against this world of mutual destruction".

According to Hans Richter Dada was not art: it was "anti-art". Dada represented the opposite of everything which art stood for. Where art was concerned with traditional aesthetics, Dada ignored aesthetics. If art was to appeal to sensibilities, Dada was intended to offend.

Additionally, Dada attempted to reflect onto human perception and the chaotic nature of society. Tristan Tzara proclaimed, "Everything is Dada, too. Beware of Dada. Anti-dadaism is a disease: selfkleptomania, man's normal condition, is Dada. But the real Dadas are against Dada".

As Hugo Ball expressed it, "For us, art is not an end in itself ... but it is an opportunity for the true perception and criticism of the times we live in."

A reviewer from the "American Art News" stated at the time that "Dada philosophy is the sickest, most paralyzing and most destructive thing that has ever originated from the brain of man." Art historians have described Dada as being, in large part, a "reaction to what many of these artists saw as nothing more than an insane spectacle of collective homicide".

Years later, Dada artists described the movement as "a phenomenon bursting forth in the midst of the postwar economic and moral crisis, a savior, a monster, which would lay waste to everything in its path... [It was] a systematic work of destruction and demoralization... In the end it became nothing but an act of sacrilege."

To quote Dona Budd's "The Language of Art Knowledge", Dada was born out of negative reaction to the horrors of the First World War. This international movement was begun by a group of artists and poets associated with the Cabaret Voltaire in Zürich. Dada rejected reason and logic, prizing nonsense, irrationality and intuition. The origin of the name Dada is unclear; some believe that it is a nonsensical word. Others maintain that it originates from the Romanian artists Tristan Tzara's and Marcel Janco's frequent use of the words "da, da," meaning "yes, yes" in the Romanian language. Another theory says that the name "Dada" came during a meeting of the group when a paper knife stuck into a French–German dictionary happened to point to 'dada', a French word for 'hobbyhorse'. The movement primarily involved visual arts, literature, poetry, art manifestos, art theory, theatre, and graphic design, and concentrated its anti-war politics through a rejection of the prevailing standards in art through anti-art cultural works.

The creations of Duchamp, Picabia, Man Ray, and others between 1915 and 1917 eluded the term Dada at the time, and "New York Dada" came to be seen as a post facto invention of Duchamp. At the outset of the 1920s the term Dada flourished in Europe with the help of Duchamp and Picabia, who had both returned from New York. Notwithstanding, Dadaists such as Tzara and Richter claimed European precedence. Art historian David Hopkins notes:

Ironically, though, Duchamp's late activities in New York, along with the machinations of Picabia, re-cast Dada's history. Dada's European chroniclers—primarily Richter, Tzara, and Huelsenbeck—would eventually become preoccupied with establishing the pre-eminence of Zürich and Berlin at the foundations of Dada, but it proved to be Duchamp who was most strategically brilliant in manipulating the genealogy of this avant-garde formation, deftly turning New York Dada from a late-comer into an originating force.

Dada emerged from a period of artistic and literary movements like Futurism, Cubism and Expressionism; centered mainly in Italy, France and Germany respectively, in those years. However, unlike the earlier movements Dada was able to establish a broad base of support, giving rise to a movement that was international in scope. Its adherents were based in cities all over the world including New York, Zürich, Berlin, Paris and others. There were regional differences like an emphasis on literature in Zürich and political protest in Berlin.

Prominent Dadaists published manifestos, but the movement was loosely organized and there was no central hierarchy. On 14 July 1916, Ball originated the seminal Dada Manifesto. Tzara wrote a second Dada manifesto, considered important Dada reading, which was published in 1918. Tzara's manifesto articulated the concept of "Dadaist disgust"—the contradiction implicit in avant-garde works between the criticism and affirmation of modernist reality. In the Dadaist perspective modern art and culture are considered a type of fetishization where the objects of consumption (including organized systems of thought like philosophy and morality) are chosen, much like a preference for cake or cherries, to fill a void.

The shock and scandal the movement inflamed was deliberate; Dadaist magazines were banned and their exhibits closed. Some of the artists even faced imprisonment. These provocations were part of the entertainment but, over time, audiences' expectations eventually outpaced the movement's capacity to deliver. As the artists' well-known "sarcastic laugh" started to come from the audience, the provocations of Dadaists began to lose their impact. Dada was an active movement during years of political turmoil from 1916 when European countries were actively engaged in World War I, the conclusion of which, in 1918, set the stage for a new political order.

There is some disagreement about where Dada originated. The movement is commonly accepted by most art historians and those who lived during this period to have identified with the Cabaret Voltaire (housed inside the "Holländische Meierei" bar in Zürich) co-founded by poet and cabaret singer Emmy Hennings and Hugo Ball. Some sources propose a Romanian origin, arguing that Dada was an offshoot of a vibrant artistic tradition that transposed to Switzerland when a group of Jewish modernist artists, including Tristan Tzara, Marcel Janco, and Arthur Segal settled in Zürich. Before World War I, similar art had already existed in Bucharest and other Eastern European cities; it is likely that Dada's catalyst was the arrival in Zürich of artists like Tzara and Janco.

The name "Cabaret Voltaire" was a reference to the French philosopher Voltaire, whose novel "Candide" mocked the religious and philosophical dogmas of the day. Opening night was attended by Ball, Tzara, Jean Arp, and Janco. These artists along with others like Sophie Taeuber, Richard Huelsenbeck and Hans Richter started putting on performances at the Cabaret Voltaire and using art to express their disgust with the war and the interests that inspired it. 

Having left Germany and Romania during World War I, the artists arrived in politically neutral Switzerland. They used abstraction to fight against the social, political, and cultural ideas of that time. They used shock art, provocation, and "vaudevilleian excess" to subvert the conventions they believed had caused the Great War. The Dadaists believed those ideas to be a byproduct of bourgeois society that was so apathetic it would wage war against itself rather than challenge the "status quo":

Ball said that Janco's mask and costume designs, inspired by Romanian folk art, made "the horror of our time, the paralyzing background of events" visible. According to Ball, performances were accompanied by a "balalaika orchestra playing delightful folk-songs". Influenced by African music, arrhythmic drumming and jazz were common at Dada gatherings.

After the cabaret closed down, Dada activities moved on to a new gallery, and Hugo Ball left for Bern. Tzara began a relentless campaign to spread Dada ideas. He bombarded French and Italian artists and writers with letters, and soon emerged as the Dada leader and master strategist. The Cabaret Voltaire re-opened, and is still in the same place at the Spiegelgasse 1 in the Niederdorf.

Zürich Dada, with Tzara at the helm, published the art and literature review "Dada" beginning in July 1917, with five editions from Zürich and the final two from Paris.

Other artists, such as André Breton and Philippe Soupault, created "literature groups to help extend the influence of Dada".

After the fighting of the First World War had ended in the armistice of November 1918, most of the Zürich Dadaists returned to their home countries, and some began Dada activities in other cities. Others, such as the Swiss native Sophie Taeuber, would remain in Zürich into the 1920s.

"Berlin was a city of tightened stomachers, of mounting, thundering hunger, where hidden rage was transformed into a boundless money lust, and men's minds were concentrating more and more on questions of naked existence... Fear was in everybody's bones" – Richard Hülsenbeck

Raoul Hausmann, who helped establish Dada in Berlin, published his manifesto "Synthethic Cino of Painting" in 1918 where he attacked Expressionism and the art critics who promoted it. Dada is envisioned in contrast to art forms, such as Expressionism, that appeal to viewers' emotional states: "the exploitation of so-called echoes of the soul". In Hausmann's conception of Dada, new techniques of creating art would open doors to explore new artistic impulses. Fragmented use of real world stimuli allowed an expression of reality that was radically different from other forms of art:
The groups in Germany were not as strongly anti-art as other groups. Their activity and art were more political and social, with corrosive manifestos and propaganda, satire, public demonstrations and overt political activities. The intensely political and war-torn environment of Berlin had a dramatic impact on the ideas of Berlin Dadaists. Conversely, New York's geographic distance from the war spawned its more theoretically-driven, less political nature. According to Hans Richter, a Dadaist who was in Berlin yet “aloof from active participation in Berlin Dada”, several distinguishing characteristics of the Dada movement there included: “its political element and its technical discoveries in painting and literature”; “inexhaustible energy”; “mental freedom which included the abolition of everything”; and “members intoxicated with their own power in a way that had no relation to the real world”, who would “turn their rebelliousness even against each other”.

In February 1918, while the Great War was approaching its climax, Huelsenbeck gave his first Dada speech in Berlin, and he produced a Dada manifesto later in the year. Following the October Revolution in Russia, by then out of the war, Hannah Höch and George Grosz used Dada to express communist sympathies. Grosz, together with John Heartfield, Höch and Hausmann developed the technique of photomontage during this period. Johannes Baader, the uninhibited Oberdada, was the “crowbar” of the Berlin movement's direct action according to Hans Richter and is credited with creating the first giant collages, according to Raoul Hausmann.

After the war, the artists published a series of short-lived political magazines and held the First International Dada Fair, 'the greatest project yet conceived by the Berlin Dadaists', in the summer of 1920. As well as work by the main members of Berlin Dada – Grosz, Raoul Hausmann, Hannah Höch, Johannes Baader, Huelsenbeck and Heartfield – the exhibition also included the work of Otto Dix, Francis Picabia, Jean Arp, Max Ernst, Rudolf Schlichter, Johannes Baargeld and others. In all, over 200 works were exhibited, surrounded by incendiary slogans, some of which also ended up written on the walls of the Nazi's "Entartete Kunst" exhibition in 1937. Despite high ticket prices, the exhibition lost money, with only one recorded sale.

The Berlin group published periodicals such as "Club Dada", "Der Dada", "Everyman His Own Football", and "Dada Almanach". They also established a political party, the Central Council of Dada for the World Revolution.

In Cologne, Ernst, Baargeld, and Arp launched a controversial Dada exhibition in 1920 which focused on nonsense and anti-bourgeois sentiments. Cologne's Early Spring Exhibition was set up in a pub, and required that participants walk past urinals while being read lewd poetry by a woman in a communion dress. The police closed the exhibition on grounds of obscenity, but it was re-opened when the charges were dropped.

Like Zürich, New York City was a refuge for writers and artists from the First World War. Soon after arriving from France in 1915, Marcel Duchamp and Francis Picabia met American artist Man Ray. By 1916 the three of them became the center of radical anti-art activities in the United States. American Beatrice Wood, who had been studying in France, soon joined them, along with Elsa von Freytag-Loringhoven. Arthur Cravan, fleeing conscription in France, was also in New York for a time. Much of their activity centered in Alfred Stieglitz's gallery, 291, and the home of Walter and Louise Arensberg.

The New Yorkers, though not particularly organized, called their activities "Dada," but they did not issue manifestos. They issued challenges to art and culture through publications such as "The Blind Man", "Rongwrong", and "New York Dada" in which they criticized the traditionalist basis for "museum" art. New York Dada lacked the disillusionment of European Dada and was instead driven by a sense of irony and humor. In his book "Adventures in the arts: informal chapters on painters, vaudeville and poets" Marsden Hartley included an essay on "".

During this time Duchamp began exhibiting "readymades" (everyday objects found or purchased and declared art) such as a bottle rack, and was active in the Society of Independent Artists. In 1917 he submitted the now famous "Fountain", a urinal signed R. Mutt, to the Society of Independent Artists exhibition but they rejected the piece. First an object of scorn within the arts community, the "Fountain" has since become almost canonized by some as one of the most recognizable modernist works of sculpture. Art world experts polled by the sponsors of the 2004 Turner Prize, Gordon's gin, voted it "the most influential work of modern art". 

As recent scholarship documents, the work is still controversial. Duchamp indicated in a 1917 letter to his sister that a female friend was centrally involved in the conception of this work: "One of my female friends who had adopted the pseudonym Richard Mutt sent me a porcelain urinal as a sculpture." The piece is in line with the scatological aesthetics of Duchamp's neighbour, the Baroness Elsa von Freytag-Loringhoven. In an attempt to "pay homage to the spirit of Dada" a performance artist named Pierre Pinoncelli made a crack in a replica of "The Fountain" with a hammer in January 2006; he also urinated on it in 1993.

Picabia's travels tied New York, Zürich and Paris groups together during the Dadaist period. For seven years he also published the Dada periodical "391" in Barcelona, New York City, Zürich, and Paris from 1917 through 1924.

By 1921, most of the original players moved to Paris where Dada had experienced its last major incarnation.

The French avant-garde kept abreast of Dada activities in Zürich with regular communications from Tristan Tzara (whose pseudonym means "sad in country," a name chosen to protest the treatment of Jews in his native Romania), who exchanged letters, poems, and magazines with Guillaume Apollinaire, André Breton, Max Jacob, Clément Pansaers, and other French writers, critics and artists.

Paris had arguably been the classical music capital of the world since the advent of musical Impressionism in the late 19th century. One of its practitioners, Erik Satie, collaborated with Picasso and Cocteau in a mad, scandalous ballet called "Parade". First performed by the Ballets Russes in 1917, it succeeded in creating a scandal but in a different way than Stravinsky's "Le Sacre du printemps" had done almost five years earlier. This was a ballet that was clearly parodying itself, something traditional ballet patrons would obviously have serious issues with.

Dada in Paris surged in 1920 when many of the originators converged there. Inspired by Tzara, Paris Dada soon issued manifestos, organized demonstrations, staged performances and produced a number of journals (the final two editions of "Dada", "Le Cannibale", and "Littérature" featured Dada in several editions.)

The first introduction of Dada artwork to the Parisian public was at the "Salon des Indépendants" in 1921. Jean Crotti exhibited works associated with Dada including a work entitled, "Explicatif" bearing the word "Tabu". In the same year Tzara staged his Dadaist play "The Gas Heart" to howls of derision from the audience. When it was re-staged in 1923 in a more professional production, the play provoked a theatre riot (initiated by André Breton) that heralded the split within the movement that was to produce Surrealism. Tzara's last attempt at a Dadaist drama was his "ironic tragedy" "Handkerchief of Clouds" in 1924.

In the Netherlands the Dada movement centered mainly around Theo van Doesburg, best known for establishing the "De Stijl" movement and magazine of the same name. Van Doesburg mainly focused on poetry, and included poems from many well-known Dada writers in "De Stijl" such as Hugo Ball, Hans Arp and Kurt Schwitters. Van Doesburg and (a cordwainer and artist in Drachten) became friends of Schwitters, and together they organized the so-called "Dutch Dada campaign" in 1923, where van Doesburg promoted a leaflet about Dada (entitled "What is Dada?"), Schwitters read his poems, Vilmos Huszár demonstrated a mechanical dancing doll and Nelly van Doesburg (Theo's wife), played avant-garde compositions on piano.
Van Doesburg wrote Dada poetry himself in "De Stijl", although under a pseudonym, I.K. Bonset, which was only revealed after his death in 1931. 'Together' with I.K. Bonset, he also published a short-lived Dutch Dada magazine called "Mécano" (1922–23). Another Dutchman identified by K. Schippers in his study of the movement in the Netherlands was the Groningen typographer H. N. Werkman, who was in touch with van Doesburg and Schwitters while editing his own magazine, "The Next Call" (1923–6). Two more artists mentioned by Schippers were German-born and eventually settled in the Netherlands. These were Otto van Rees, who had taken part in the liminal exhibitions at the Café Voltaire in Zürich, and Paul Citroen.

Though Dada itself was unknown in Georgia until at least 1920, from 1917 until 1921 a group of poets called themselves Le Degré 41", or "Le Degré Quarante et Un" (English, "The 41st Degree") (referring both to the latitude of Tbilisi, Georgia and to the Celsius temperature of a high fever [equal to 105.8 Fahrenheit]) organized along Dadaist lines. The most important figure in this group was Iliazd (Ilia Zdanevich), whose radical typographical designs visually echo the publications of the Dadaists. 

After his flight to Paris in 1921, he collaborated with Dadaists on publications and events. For example, when Tristan Tzara was banned from holding seminars in Théâtre Michel in 1923, Iliazd booked the venue on his behalf for the performance, "The Bearded Heart Soirée", and designed the flyer.

In Yugoslavia, alongside the new art movement Zenitism, there was significant Dada activity between 1920 and 1922, run mainly by Dragan Aleksić and including work by Mihailo S. Petrov, Ljubomir Micić and Branko Ve Poljanski. Aleksić used the term "Yougo-Dada" and is known to have been in contact with Raoul Hausmann, Kurt Schwitters, and Tristan Tzara.

The Dada movement in Italy, based in Mantua, was met with distaste and failed to make a significant impact in the world of art. It published a magazine for a short time and held an exhibition in Rome, featuring paintings, quotations from Tristan Tzara, and original epigrams such as "True Dada is against Dada". One member of this group was Julius Evola, who went on to become an eminent scholar of occultism, as well as a right-wing philosopher.

A prominent Dada group in Japan was Mavo, founded in July 1923 by Tomoyoshi Murayama, and Yanase Masamu later joined by Tatsuo Okada. Other prominent artists were Jun Tsuji, Eisuke Yoshiyuki, Shinkichi Takahashi and Katué Kitasono.
In Tsuburaya Productions's "Ultra Series", an alien named Dada was inspired by the Dadaism movement, with said character first appearing in episode 28 of the 1966 tokusatsu series, "Ultraman", its design by character artist Toru Narita. Dada's design is primarily monochromatic, and features numerous sharp lines and alternating black and white stripes, in reference to the movement and, in particular, to chessboard and Go patterns. On May 19, 2016, in celebration to the 100 year anniversary of Dadaism in Tokyo, the Ultra Monster was invited to meet the Swiss Ambassador Urs Bucher.

Butoh, the Japanese dance-form originating in 1959, can be considered to have direct connections to the spirit of the Dada movement, as Tatsumi Hijikata, one of Butoh's founders, "was influenced early in his career by Dadaism".

Dada in itself was relatively unknown in Russia, however, avant-garde art was widespread due to the Bolsheviks' revolutionary agenda. The , a literary group sharing Dadaist ideals achieved infamy after one of its members suggested that Vladimir Mayakovsky should go to the "Pampushka" (Pameatnik Pushkina – Pushkin monument) on the "Tverbul" (Tverskoy Boulevard) to clean the shoes of anyone who desired it, after Mayakovsky declared that he was going to cleanse Russian literature. For more information on Dadaism's influence upon Russian avant-garde art, see the book "Russian Dada 1914–1924".

Often overlooked when discussing the history and foundations of Dada, it is necessary to shed light on the female artists who created and inspired art and artists alike. These women were often times in platonic or romantic relationships with the male Dadaists mentioned above but are rarely written past the relative ties. However, each artist made vital contributions to the movement. Other notable mentions that do not include the artists below are: Suzanne Duchamp, Elsa von Freytag-Loringhoven, Emmy Hennings, Beatrice Wood, Clara Tice, and Ella Bergmann-Michel.

Hannah Höch of Berlin is considered to be the only female Dadaist in Berlin at the time of the movement. During this time, she was in a relationship with Raoul Hausmann who also was a Dada artist. She channeled the same anti-war and anti-government (Weimar Republic) in her works but brought out a feminist lens on the themes. With her works primarily of collage and photomontage, she often used precise placement or detailed titles to callout the misogynistic ways she and other women were treated.

Sophie Taeuber-Arp was a Swiss artist, teacher, and dancer who produced various types of fine art and handicraft pieces. While married to Dadaist Jean Arp, Taeuber-Arp was known in the Dada community for her performative dancing. As such, she worked with choreographer Rudolf von Laban and was written by Tristan Tarza for her dancing skills.

London-born Mina Loy was known for being active in the literary sector of the New York Dada scene. She spent time writing poetry, creating Dada magazines, and acting and writing in plays. She contributed writing to Dada journal "The Blind Man" and Marchel Duchamp's "Rongwrong".

Dadists used shock, nihilism, negativity, paradox, randomness, subconscious forces and antinomianism to subvert established traditions in the aftermath of the Great War. Tzara's 1920 manifesto proposed cutting words from a newspaper and randomly selecting fragments to write poetry, a process in which the synchronous universe itself becomes an active agent in creating the art. A poem written using this technique would be a "fruit" of the words that were clipped from the article.

In literary arts Dadaists focused on poetry, particularly the so-called sound poetry invented by Hugo Ball. Dadaist poems attacked traditional conceptions of poetry, including structure, order, as well as the interplay of sound and the meaning of language. For Dadaists, the existing system by which information is articulated robs language of its dignity. The dismantling of language and poetic conventions are Dadaist attempts to restore language to its purest and most innocent form: "With these sound poem, we wanted to dispense with a language which journalism had made desolate and impossible."

Simultaneous poems (or "poèmes simultanés") were recited by a group of speakers who, collectively, produced a chaotic and confusing set of voices. These poems are considered manifestations of modernity including advertising, technology, and conflict. Unlike movements such as Expressionism, Dadaism did not take a negative view of modernity and the urban life. The chaotic urban and futuristic world is considered natural terrain that opens up new ideas for life and art.

Dada was not confined to the visual and literary arts; its influence reached into sound and music. These movements exerted a pervasive influence on 20th-century music, especially on mid-century avant-garde composers based in New York—among them Edgard Varèse, Stefan Wolpe, John Cage, and Morton Feldman. Kurt Schwitters developed what he called "sound poems", while Francis Picabia and Georges Ribemont-Dessaignes composed Dada music performed at the Festival Dada in Paris on 26 May 1920. Other composers such as Erwin Schulhoff, Hans Heusser and Alberto Savinio all wrote "Dada music", while members of Les Six collaborated with members of the Dada movement and had their works performed at Dada gatherings. Erik Satie also dabbled with Dadaist ideas during his career. 

While broadly based, the movement was unstable. By 1924 in Paris, Dada was melding into Surrealism, and artists had gone on to other ideas and movements, including Surrealism, social realism and other forms of modernism. Some theorists argue that Dada was actually the beginning of postmodern art.

By the dawn of the Second World War, many of the European Dadaists had emigrated to the United States. Some (Otto Freundlich, Walter Serner) died in death camps under Adolf Hitler, who actively persecuted the kind of "degenerate art" that he considered Dada to represent. The movement became less active as post-war optimism led to the development of new movements in art and literature.

Dada is a named influence and reference of various anti-art and political and cultural movements, including the Situationist International and culture jamming groups like the Cacophony Society. Upon breaking up in July 2012, anarchist pop band Chumbawamba issued a statement which compared their own legacy with that of the Dada art movement.

At the same time that the Zürich Dadaists were making noise and spectacle at the Cabaret Voltaire, Lenin was planning his revolutionary plans for Russia in a nearby apartment. Tom Stoppard used this coincidence as a premise for his play "Travesties" (1974), which includes Tzara, Lenin, and James Joyce as characters. French writer Dominique Noguez imagined Lenin as a member of the Dada group in his tongue-in-cheek "Lénine Dada" (1989).

The former building of the Cabaret Voltaire fell into disrepair until it was occupied from January to March 2002, by a group proclaiming themselves Neo-Dadaists, led by Mark Divo. The group included Jan Thieler, Ingo Giezendanner, Aiana Calugar, Lennie Lee, and Dan Jones. After their eviction, the space was turned into a museum dedicated to the history of Dada. The work of Lee and Jones remained on the walls of the new museum.

Several notable retrospectives have examined the influence of Dada upon art and society. In 1967, a large Dada retrospective was held in Paris. In 2006, the Museum of Modern Art in New York City mounted a Dada exhibition in partnership with the National Gallery of Art in Washington, D.C., and the Centre Pompidou in Paris. The LTM label has released a large number of Dada-related sound recordings, including interviews with artists such as Tzara, Picabia, Schwitters, Arp, and Huelsenbeck, and musical repertoire including Satie, Ribemont-Dessaignes, Picabia, and Nelly van Doesburg.

Musician Frank Zappa was a self-proclaimed Dadaist after learning of the movement:In the early days, I didn't even know what to call the stuff my life was made of. You can imagine my delight when I discovered that someone in a distant land had the same idea—AND a nice, short name for it.David Bowie adapted William S. Burrough's cut-up technique for writing lyrics and Kurt Cobain also admittedly used this method for many of his Nirvana lyrics, including "In Bloom".

Dadaism also blurred the line between literary and visual arts:

Dada is the groundwork to abstract art and sound poetry, a starting point for performance art, a prelude to postmodernism, an influence on pop art, a celebration of antiart to be later embraced for anarcho-political uses in the 1960s and the movement that laid the foundation for Surrealism.

The Dadaists imitated the techniques developed during the cubist movement through the pasting of cut pieces of paper items, but extended their art to encompass items such as transportation tickets, maps, plastic wrappers, etc. to portray aspects of life, rather than representing objects viewed as still life. They also invented the “chance collage" technique, involving dropping torn scraps of paper onto a larger sheet and then pasting the pieces wherever they landed.

Cut-up technique is an extension of collage to words themselves, Tristan Tzara describes this in the Dada Manifesto:
<poem style="margin-left: 2em;">
TO MAKE A DADAIST POEM
Take a newspaper.
Take some scissors.
Choose from this paper an article of the length you want to make your poem.
Cut out the article.
Next carefully cut out each of the words that makes up this article and put them all in a bag.
Shake gently.
Next take out each cutting one after the other.
Copy conscientiously in the order in which they left the bag.
The poem will resemble you.
And there you are – an infinitely original author of charming sensibility, even though unappreciated by the vulgar herd.
</poem>

The Dadaists – the "monteurs" (mechanics) – used scissors and glue rather than paintbrushes and paints to express their views of modern life through images presented by the media. A variation on the collage technique, photomontage utilized actual or reproductions of real photographs printed in the press. In Cologne, Max Ernst used images from the First World War to illustrate messages of the destruction of war. Although the Berlin photomontages were assembled, like engines, the (non)relationships among the disparate elements were more rhetorical than real.

The assemblages were three-dimensional variations of the collage – the assembly of everyday objects to produce meaningful or meaningless (relative to the war) pieces of work including war objects and trash. Objects were nailed, screwed or fastened together in different fashions. Assemblages could be seen in the round or could be hung on a wall.

Marcel Duchamp began to view the manufactured objects of his collection as objects of art, which he called "readymades". He would add signatures and titles to some, converting them into artwork that he called "readymade aided" or "rectified readymades". Duchamp wrote: "One important characteristic was the short sentence which I occasionally inscribed on the 'readymade.' That sentence, instead of describing the object like a title, was meant to carry the mind of the spectator towards other regions more verbal. Sometimes I would add a graphic detail of presentation which in order to satisfy my craving for alliterations, would be called 'readymade aided. One such example of Duchamp's readymade works is the urinal that was turned onto its back, signed "R. Mutt", titled "Fountain", and submitted to the Society of Independent Artists exhibition that year, though it was not displayed.

Many young artists in America embraced the theories and ideas espoused by Duchamp. Robert Rauschenberg in particular was very influenced by Dadaism and tended to use found objects in his collages as a means of dissolving the boundary between high and low culture.


Sources


Manifestos

Debian

Debian (), also known as Debian GNU/Linux, is a Linux distribution composed of free and open-source software and optionally non-free firmware or software developed by the community-supported Debian Project, which was established by Ian Murdock on August 16, 1993. The first version of Debian (0.01) was released on September 15, 1993, and its first stable version (1.1) was released on June 17, 1996. The Debian Stable branch is the most popular edition for personal computers and servers.
Debian is also the basis for many other distributions that have different purposes, like Proxmox for servers, Ubuntu or Linux Mint for desktops, Kali for penetration testing, and Pardus and Astra for government use.

Debian is one of the oldest operating systems based on the Linux kernel and, as of September 2023, the second oldest Linux distribution still in active development, only behind Slackware. The project is coordinated over the Internet by a team of volunteers guided by the Debian Project Leader and three foundational documents: the Debian Social Contract, the Debian Constitution, and the Debian Free Software Guidelines. New distributions are updated continually, and the next candidate is released after a time-based freeze.

In general, Debian has been developed openly and distributed freely according to some of the principles of the GNU Project and Free Software. Because of this, the Free Software Foundation sponsored the project from November 1994 to November 1995. However, it is no longer endorsed by GNU and the FSF due to the distribution's long-term practice of hosting non-free software repositories and, since 2022, its inclusion of non-free firmware in its installation media by default. On June 16, 1997, the Debian Project founded the nonprofit organization Software in the Public Interest to continue financially supporting development.

Debian distribution codenames are based on the names of characters from the "Toy Story" films. Debian's "unstable" trunk is named after Sid, a character who regularly destroyed his toys.

First announced on August 16, 1993, Debian was founded by Ian Murdock, who initially called the system "the Debian Linux Release". The word "Debian" was formed as a portmanteau of the first name of his then-girlfriend (later ex-wife) Debra Lynn and his own first name. Before Debian's release, the Softlanding Linux System (SLS) had been a popular Linux distribution and the basis for Slackware. The perceived poor maintenance and prevalence of bugs in SLS motivated Murdock to launch a new distribution.

Debian 0.01, released on September 15, 1993, was the first of several internal releases. Version 0.90 was the first public release, providing support through mailing lists hosted at Pixar. The release included the Debian Linux Manifesto, outlining Murdock's view for the new operating system. In it he called for the creation of a distribution to be maintained "openly in the spirit of Linux and GNU."

The Debian project released the 0.9x versions in 1994 and 1995. During this time it was sponsored by the Free Software Foundation for one year. Ian Murdock delegated the base system, the core packages of Debian, to Bruce Perens and Murdock focused on the management of the growing project. The first ports to non-IA-32 architectures began in 1995, and Debian 1.1 was released in 1996. By that time and thanks to Ian Jackson, the dpkg package manager was already an essential part of Debian.

In 1996, Bruce Perens assumed the project leadership. Perens was a controversial leader, regarded as authoritarian and strongly attached to Debian. He drafted a social contract and edited suggestions from a month-long discussion into the Debian Social Contract and the Debian Free Software Guidelines. After the FSF withdrew their sponsorship in the midst of the free software vs. open source debate, Perens initiated the creation of the legal umbrella organization Software in the Public Interest instead of seeking renewed involvement with the FSF. He led the conversion of the project from a.out to ELF. He created the BusyBox program to make it possible to run a Debian installer on a single floppy disk, and wrote a new installer. By the time Debian 1.2 was released, the project had grown to nearly two hundred volunteers. Perens left the project in 1998.

Ian Jackson became the leader in 1998. Debian 2.0 introduced the second official port, m68k. During this time the first port to a non-Linux kernel, Debian GNU/Hurd, was started. On December 2, the first Debian Constitution was ratified.

From 1999, the project leader was elected yearly. The Advanced Packaging Tool was deployed with Debian 2.1. The number of applicants was overwhelming and the project established the new member process. The first Debian derivatives, namely Libranet, Corel Linux and Stormix's Storm Linux, were started in 1999. The 2.2 release in 2000 was dedicated to Joel Klecker, a developer who died of Duchenne muscular dystrophy.

In late 2000, the project reorganized the archive with new package "pools" and created the "Testing" distribution, made up of packages considered stable, to reduce the freeze for the next release. In the same year, developers began holding an annual conference called DebConf with talks and workshops for developers and technical users. In May 2001, Hewlett-Packard announced plans to base its Linux development on Debian.

In July 2002, the project released version 3.0, code-named Woody, the first release to include cryptographic software, a free licensed KDE and internationalization. During these last release cycles, the Debian project drew considerable criticism from the free software community because of the long time between stable releases.

Some events disturbed the project while working on Sarge, as Debian servers were attacked by fire and hackers. One of the most memorable was the Vancouver prospectus. After a meeting held in Vancouver, release manager Steve Langasek announced a plan to reduce the number of supported ports to four in order to shorten future release cycles. There was a large reaction because the proposal looked more like a decision and because such a drop would damage Debian's aim to be "the universal operating system".

The first version of the Debian-based Ubuntu, named "4.10 Warty Warthog", was released on October 20, 2004. Because it was distributed as a free download, it became one of the most popular and successful operating systems with more than "40 million users" according to Canonical Ltd. However, Murdock was critical of the differences between Ubuntu packages and Debian, stating that it leads to incompatibilities.

The 3.1 Sarge release was made in June 2005. This release updated 73% of the software and included over 9,000 new packages. A new installer with a modular design, Debian-Installer, allowed installations with RAID, XFS and LVM support, improved hardware detection, made installations easier for novice users, and was translated into almost forty languages. An installation manual and release notes were in ten and fifteen languages respectively. The efforts of Skolelinux, Debian-Med and Debian-Accessibility raised the number of packages that were educational, had a medical affiliation, and ones made for people with disabilities.
In 2006, as a result of a much-publicized dispute, Mozilla software was rebranded in Debian, with Firefox forked as Iceweasel and Thunderbird as Icedove. The Mozilla Corporation stated that software with unapproved modifications could not be distributed under the Firefox trademark. Two reasons that Debian modified the Firefox software were to change non-free artwork and to provide security patches. In February 2016, it was announced that Mozilla and Debian had reached an agreement and Iceweasel would revert to the name Firefox; similar agreement was anticipated for Icedove/Thunderbird.

A fund-raising experiment, Dunc-Tank, was created to solve the release cycle problem and release managers were paid to work full-time; in response, unpaid developers slowed down their work and the release was delayed.<br>
Debian 4.0 (Etch) was released in April 2007, featuring the x86-64 port and a graphical installer.<br>
Debian 5.0 (Lenny) was released in February 2009, supporting Marvell's Orion platform and netbooks such as the Asus Eee PC. The release was dedicated to Thiemo Seufer, a developer who died in a car crash.

In July 2009, the policy of time-based development freezes on a two-year cycle was announced. Time-based freezes are intended to blend the predictability of time based releases with Debian's policy of feature based releases, and to reduce overall freeze time. The Squeeze cycle was going to be especially short; however, this initial schedule was abandoned. In September 2010, the backports service became official, providing more recent versions of some software for the stable release.

Debian 6.0 (Squeeze) was released in February 2011, featuring Debian GNU/kFreeBSD as a technology preview, along with adding a dependency-based boot system, and moving problematic firmware to the non-free section.<br>
Debian 7.0 (Wheezy) was released in May 2013, featuring multiarch support.<br>
Debian 8.0 (Jessie) was released in April 2015, using systemd as the new init system.<br>
Debian 9.0 (Stretch) was released in June 2017, with nftables as a replacement for iptables, support for Flatpak apps, and MariaDB as the replacement for MySQL.<br>
Debian 10.0 (Buster) was released in July 2019, adding support for Secure Boot and enabling AppArmor by default.<br>
Debian 11.0 (Bullseye) was released in August 2021, enabling persistency in the system journal, adding support for driverless scanning, and containing kernel-level support for exFAT filesystems.<br>
Debian 12.0 (Bookworm) was released on June 10, 2023, including various improvements and features, increasing the supported Linux Kernel to version 6.1, and leveraging new "Emerald" artwork.<br>
Debian is still in development and new packages are uploaded to "unstable" every day.

Debian used to be released as a very large set of CDs for each architecture, but with the release of Debian 9 (Stretch) in 2017, many of the images have been dropped from the archive but remain buildable via jigdo.

Throughout Debian's lifetime, both the Debian distribution and its website have won various awards from different organizations, including "Server Distribution of the Year" 2011, "The best Linux distro of 2011", and a "Best of the Net" award for October 1998.

On December 2, 2015, Microsoft announced that they would offer Debian GNU/Linux as an endorsed distribution on the Azure cloud platform. Microsoft has also added a user environment to their Windows 10 desktop operating system called Windows Subsystem for Linux that offers a Debian subset.

Debian has access to online repositories that contain over 51,000 packages. Debian officially contains only free software, but non-free software can be downloaded and installed from the Debian repositories. Debian includes popular free programs such as LibreOffice, Firefox web browser, Evolution mail, K3b disc burner, VLC media player, GIMP image editor, and Evince document viewer. Debian is a popular choice for servers, for example as the operating system component of a LAMP stack.

Several flavors of the Linux kernel exist for each port. For example, the i386 port has flavors for IA-32 PCs supporting Physical Address Extension and real-time computing, for older PCs, and for x86-64 PCs. The Linux kernel does not officially contain firmware lacking source code, although such firmware is available in non-free packages and alternative installation media.

Debian offers CD and DVD images specifically built for Xfce, GNOME, KDE, MATE, Cinnamon, LXDE, and LXQt. MATE support was added in 2014, and Cinnamon support was added with Debian 8.0 Jessie. Less common window managers such as Enlightenment, Openbox, Fluxbox, IceWM, Window Maker and others are available.

The default desktop environment of version 7.0 Wheezy was temporarily switched to Xfce, because GNOME 3 did not fit on the first CD of the set. The default for the version 8.0 Jessie was changed again to Xfce in November 2013, and back to GNOME in September 2014.

Several parts of Debian are translated into languages other than American English, including package descriptions, configuration messages, documentation and the website. The level of software localization depends on the language, ranging from the highly supported German and French to the barely translated Creek and Samoan. The Debian 10 installer is available in 76 languages.

Multimedia support has been problematic in Debian regarding codecs threatened by possible patent infringements, lacking source code, or under too restrictive licenses. Even though packages with problems related to their distribution could go into the non-free area, software such as libdvdcss is not hosted at Debian .

A notable third party repository exists, formerly named Debian-multimedia.org, providing software not present in Debian such as Windows codecs, libdvdcss and the Adobe Flash Player. Even though this repository is maintained by Christian Marillat, a Debian developer, it is not part of the project and is not hosted on a Debian server. The repository provides packages already included in Debian, interfering with the official maintenance. Eventually, project leader Stefano Zacchiroli asked Marillat to either settle an agreement about the packaging or to stop using the "Debian" name. Marillat chose the latter and renamed the repository to deb-multimedia.org. The repository was so popular that the switchover was announced by the official blog of the Debian project.

Debian offers DVD and CD images for installation that can be downloaded using BitTorrent or jigdo. Physical discs can also be bought from retailers. The full sets are made up of several discs (the amd64 port consists of 13 DVDs or 84 CDs), but only the first disc is required for installation, as the installer can retrieve software not contained in the first disc image from online repositories.

Debian offers different network installation methods. A minimal install of Debian is available via the "netinst" CD, whereby Debian is installed with just a base and later added software can be downloaded from the Internet. Another option is to boot the installer from the network.

The default bootstrap loader is GNU GRUB version 2, though the package name is simply grub, while version 1 was renamed to grub-legacy. This conflicts with distros (e.g., Fedora Linux), where grub version 2 is named grub2.

The default desktop may be chosen from the DVD boot menu among GNOME, KDE Plasma, Xfce and LXDE, and from special disc 1 CDs.

Debian releases live install images for CDs, DVDs and USB thumb drives, for IA-32 and x86-64 architectures, and with a choice of desktop environments. These "Debian Live" images allow users to boot from removable media and run Debian without affecting the contents of their computer. A full install of Debian to the computer's hard drive can be initiated from the live image environment. Personalized images can be built with the live-build tool for discs, USB drives and for network booting purposes. Installation images are hybrid on some architectures and can be used to create a bootable USB drive (Live USB).

Package management operations can be performed with different tools available on Debian, from the lowest level command dpkg to graphical front-ends like Synaptic. The recommended standard for administering packages on a Debian system is the apt toolset.

dpkg provides the low-level infrastructure for package management. The dpkg database contains the list of installed software on the current system. The dpkg command tool does not know about repositories. The command can work with local .deb package files, and information from the dpkg database.

An Advanced Packaging Tool (APT) allows administering an installed Debian system to retrieve and resolve package dependencies from repositories. APT tools share dependency information and cached packages.

GDebi is an APT tool which can be used in command-line and on the GUI. GDebi can install a local .deb file via the command line like the dpkg command, but with access to repositories to resolve dependencies. Other graphical front-ends for APT include Software Center, Synaptic and Apper.

GNOME Software is a graphical front-end for PackageKit, which itself can work on top of various software packaging systems.

The Debian Free Software Guidelines (DFSG) define the distinctive meaning of the word "free" as in "free and open-source software". Packages that comply with these guidelines, usually under the GNU General Public License, Modified BSD License or Artistic License, are included inside the "main" area; otherwise, they are included inside the "non-free" and "contrib" areas. These last two areas are not distributed within the official installation media, but they can be adopted manually.

Non-free includes packages that do not comply with the DFSG, such as documentation with invariant sections and proprietary software, and legally questionable packages. Contrib includes packages which do comply with the DFSG but fail other requirements. For example, they may depend on packages which are in non-free or requires such for building them.

Richard Stallman and the Free Software Foundation have criticized the Debian project for hosting the non-free repository and because the contrib and non-free areas are easily accessible, an opinion echoed by some in Debian including the former project leader Wichert Akkerman. The internal dissent in the Debian project regarding the non-free section has persisted, but the last time it came to a vote in 2004, the majority decided to keep it.

The most popular optional Linux cross-distribution package manager are graphical (front-ends) package managers. They are available within the official Debian Repository but are not installed by default. They are widely popular with both Debian users and Debian software developers who are interested in installing the most recent versions of application or using the cross-distribution package manager built-in sandbox environment. While at the same time remaining in control of the security.

Four most popular cross-distribution package managers, sorted in alphabetical order:

Three branches of Debian (also called "releases", "distributions" or "suites") are regularly maintained:

Other branches in Debian:

The "snapshot" archive provides older versions of the branches. They may be used to install a specific older version of some software.

"Stable" and "oldstable" get minor updates, called "point releases"; , the "stable" release is version 11.7, released on , and the "oldstable" release is version 10.10.

The numbering scheme for the point releases up to Debian 4.0 was to include the letter "r" (for "revision") after the main version number and then the number of the point release; for example, the latest point release of version 4.0 is 4.0r9. This scheme was chosen because a new dotted version would make the old one look obsolete and vendors would have trouble selling their CDs.

From Debian 5.0, the numbering scheme of point releases was changed, conforming to the GNU version numbering standard; the first point release of Debian 5.0 was 5.0.1 instead of 5.0r1. The numbering scheme was once again changed for the first Debian 7 update, which was version 7.1. The "r" scheme is no longer in use, but point release announcements include a note about not throwing away old CDs.

Debian has two logos. The official logo (also known as <q>o"pen use logo"</q>) contains the well-known Debian <q>"swirl"</q> and best represents the visual identity of the Debian Project. A separate, restricted-use logo, also exists for use by the Debian Project and its members only. To refer to Debian, please prefer the open use logo.

The Debian "swirl" logo was designed by Raul Silva in 1999 as part of a contest to replace the semi-official logo that had been used. The winner of the contest received an @Debian.org email address, and a set of Debian 2.1 install CDs for the architecture of their choice. Initially, the swirl was magic smoke arising from an also included bottle of an Arabian-style genie presented in black profile, but shortly after was reduced to the red smoke swirl for situations where space or multiple colours were not an option, and before long the bottle version effectively was superseded. There has been no official statement from the Debian project on the logo's meaning, but at the time of the logo's selection, it was suggested that the logo represented the magic smoke ( or the genie ) that made computers work.

One theory about the origin of the Debian logo is that Buzz Lightyear, the chosen character for the first named Debian release, has a swirl in his chin. Stefano Zacchiroli also suggested that this swirl is the Debian one. Buzz Lightyear's swirl is a more likely candidate as the codenames for Debian are names of Toy Story characters. The former Debian project leader Bruce Perens used to work for Pixar and is credited as a studio tools engineer on "Toy Story 2" (1999).

Hardware requirements are at least those of the kernel and the GNU toolsets. Debian's recommended system requirements depend on the level of installation, which corresponds to increased numbers of installed components:

The real minimum memory requirements depend on the architecture and may be much less than the numbers listed in this table. It is possible to install Debian with 170 MB of RAM for x86-64; the installer will run in low memory mode and it is recommended to create a swap partition. The installer for z/Architecture requires about 20 MB of RAM, but relies on network hardware. Similarly, disk space requirements, which depend on the packages to be installed, can be reduced by manually selecting the packages needed. , no Pure Blend exists that would lower the hardware requirements easily.

It is possible to run graphical user interfaces on older or low-end systems, but the installation of window managers instead of desktop environments is recommended, as desktop environments are more resource intensive. Requirements for individual software vary widely and must be considered, with those of the base operating environment.

, the official ports are:

Unofficial ports are available as part of the "unstable" distribution:

Debian supports a variety of ARM-based NAS devices. The NSLU2 was supported by the installer in Debian 4.0 and 5.0, and Martin Michlmayr is providing installation tarballs since version 6.0. Other supported NAS devices are the Buffalo Kurobox Pro, GLAN Tank, Thecus N2100 and QNAP Turbo Stations.

Devices based on the Kirkwood system on a chip (SoC) are supported too, such as the SheevaPlug plug computer and OpenRD products. There are efforts to run Debian on mobile devices, but this is not a project goal yet since the Debian Linux kernel maintainers would not apply the needed patches. Nevertheless, there are packages for resource-limited systems.

There are efforts to support Debian on wireless access points. Debian is known to run on set-top boxes. Work is ongoing to support the AM335x processor, which is used in electronic point of service solutions. Debian may be customized to run on cash machines.

BeagleBoard, a low-power open-source hardware single-board computer (made by Texas Instruments) has switched to Debian Linux preloaded on its Beaglebone Black board's flash.

Roqos Core, manufactured by Roqos, is a x86-64 based IPS firewall router running Debian Linux.

Debian's policies and team efforts focus on collaborative software development and testing processes. As a result, a new major release tends to occur every two years with revision releases that fix security issues and important problems. The Debian project is a volunteer organization with three foundational documents:

Debian developers are organized in a web of trust. There are about one thousand active Debian developers, but it is possible to contribute to the project without being an official developer.

The project maintains official mailing lists and conferences for communication and coordination between developers. For issues with single packages and other tasks, a public bug tracking system is used by developers and end users. Internet Relay Chat is also used for communication among developers and to provide real time help.

Debian is supported by donations made to organizations authorized by the leader. The largest supporter is Software in the Public Interest, the owner of the Debian trademark, manager of the monetary donations and umbrella organization for various other community free software projects.

A Project Leader is elected once per year by the developers. The leader has special powers, but they are not absolute, and appoints delegates to perform specialized tasks. Delegates make decisions as they think is best, taking into account technical criteria and consensus. By way of a General Resolution, the developers may recall the leader, reverse a decision made by the leader or a delegate, amend foundational documents and make other binding decisions. The voting method is based on the Schulze method (Cloneproof Schwartz Sequential Dropping).

Project leadership is distributed occasionally. Branden Robinson was helped by the Project Scud, a team of developers that assisted the leader, but there were concerns that such leadership would split Debian into two developer classes. Anthony Towns created a supplemental position, Second In Charge (2IC), that shared some powers of the leader. Steve McIntyre was 2IC and had a 2IC himself.

One important role in Debian's leadership is that of a release manager. The release team sets goals for the next release, supervises the processes and decides when to release. The team is led by the next release managers and stable release managers. Release assistants were introduced in 2003.

The Debian Project has an influx of applicants wishing to become developers. These applicants must undergo a vetting process which establishes their identity, motivation, understanding of the project's principles, and technical competence. This process has become much harder throughout the years.

Debian developers join the project for many reasons. Some that have been cited include:

Debian developers may resign their positions at any time or, when deemed necessary, they can be expelled. Those who follow the retiring protocol are granted the "emeritus" status and they may regain their membership through a shortened new member process.

Each software package has a "maintainer" that may be either one person or a team of Debian developers and non-developer maintainers. The maintainer keeps track of upstream releases, and ensures that the package coheres with the rest of the distribution and meets the standards of quality of Debian. Packages may include modifications introduced by Debian to achieve compliance with Debian Policy, even to fix non-Debian specific bugs, although coordination with upstream developers is advised.

The maintainer releases a new version by uploading the package to the "incoming" system, which verifies the integrity of the packages and their digital signatures. If the package is found to be valid, it is installed in the package archive into an area called the "pool" and distributed every day to hundreds of mirrors worldwide. The upload must be signed using OpenPGP-compatible software. All Debian developers have individual cryptographic key pairs. Developers are responsible for any package they upload even if the packaging was prepared by another contributor.

Initially, an accepted package is only available in the "unstable" branch. For a package to become a candidate for the next release, it must migrate to the "Testing" branch by meeting the following:

Thus, a release-critical bug in a new version of a shared library on which many packages depend may prevent those packages from entering "Testing", because the updated library must meet the requirements too. From the branch viewpoint, the migration process happens twice per day, rendering "Testing" in perpetual beta.

Periodically, the release team publishes guidelines to the developers in order to ready the release. A new release occurs after a freeze, when all important software is reasonably up-to-date in the "Testing" branch and any other significant issues are solved. At that time, all packages in the "testing" branch become the new "stable" branch. Although freeze dates are time-based, release dates are not, which are announced by the release managers a couple of weeks beforehand.

A version of a package can belong to more than one branch, usually "testing" and "unstable". It is possible for a package to keep the same version between stable releases and be part of "oldstable", "stable", "testing" and "unstable" at the same time. Each branch can be seen as a collection of pointers into the package "pool" mentioned above.

One way to resolve the challenge of a release-critical bug in a new application version is the use of optional package managers. They allow software developers to use sandbox environments, while at the same time remaining in control of security. Another benefit of a cross-distribution package manager is that they allow application developers to directly provide updates to users without going through distributions, and without having to package and test the application separately for each distribution.

A new "stable" branch of Debian gets released approximately every 2 years. It will receive official support for about 3 years with update for major security or usability fixes. Point releases will be available every several months as determined by Stable Release Managers (SRM).

Debian also launched its Long Term Support (LTS) project since Debian 6 (Debian Squeeze). For each Debian release, it will receive two years of extra security updates provided by LTS Team after its End Of Life (EOL). However, no point releases will be made. Now each Debian release can receive 5 years of security support in total.

The Debian project handles security through public disclosure. Debian security advisories are compatible with the Common Vulnerabilities and Exposures dictionary, are usually coordinated with other free software vendors and are published the same day a vulnerability is made public. There used to be a security audit project that focused on packages in the stable release looking for security bugs; Steve Kemp, who started the project, retired in 2011 but resumed his activities and applied to rejoin in 2014.

The "stable" branch is supported by the Debian security team; "oldstable" is supported for one year. Although Squeeze is not officially supported, Debian is coordinating an effort to provide long-term support (LTS) until February 2016, five years after the initial release, but only for the IA-32 and x86-64 platforms. "Testing" is supported by the "testing" security team, but does not receive updates in as timely a manner as "stable". "Unstable"s security is left for the package maintainers.

The Debian project offers documentation and tools to harden a Debian installation both manually and automatically. AppArmor support is available and enabled by default since Buster. Debian provides an optional hardening wrapper, and does not harden all of its software by default using gcc features such as PIE and buffer overflow protection, unlike operating systems such as OpenBSD, but tries to build as many packages as possible with hardening flags.

In May 2008, a Debian developer discovered that the OpenSSL package distributed with Debian and derivatives such as Ubuntu made a variety of security keys vulnerable to a random number generator attack, since only 32,767 different keys were generated. The security weakness was caused by changes made in 2006 by another Debian developer in response to memory debugger warnings. The complete resolution procedure was cumbersome because patching the security hole was not enough; it involved regenerating all affected keys and certificates.

The cost of developing all of the packages included in Debian 5.0 Lenny (323 million lines of code) has been estimated to be about , using one method based on the COCOMO model. , Black Duck Open Hub estimates that the current codebase (74 million lines of code) would cost about to develop, using a different method based on the same model.

A large number of forks and derivatives have been built upon Debian over the years. Among the more notable are Ubuntu, developed by Canonical Ltd. and first released in 2004, which has surpassed Debian in popularity with desktop users; Knoppix, first released in the year 2000 and one of the first distributions optimized to boot from external storage; and Devuan, which gained attention in 2014 when it forked in disagreement over Debian's adoption of the systemd software suite, and has been mirroring Debian releases since 2017. The Linux Mint Debian Edition ("LMDE") uses Debian Stable as the software source base since 2014.

Debian is one of the most popular Linux distributions, and many other distributions have been created from the Debian codebase. , DistroWatch lists 121 active Debian derivatives. The Debian project provides its derivatives with guidelines for best practices and encourages derivatives to merge their work back into Debian.

Debian Pure Blends are subsets of a Debian release configured out-of-the-box for users with particular skills and interests. For example, Debian Jr. is made for children, while Debian Science is for researchers and scientists. The complete Debian distribution includes all available Debian Pure Blends. "Debian Blend" (without "Pure") is a term for a Debian-based distribution that strives to become part of mainstream Debian, and have its extra features included in future releases.

Debian GNU/Hurd is a flavor based on the Hurd operating system (which, in turn, runs on the GNU Mach microkernel), instead of Linux. Debian GNU/Hurd has been in development since 1998, and made a formal release in May 2013, with 78% of the software packaged for Debian GNU/Linux ported to the GNU Hurd. Hurd is not yet an official Debian release, and is maintained and developed as an unofficial port. Debian GNU/Hurd is distributed as an installer CD (running the official Debian installer) or ready-to-run virtual disk image (Live CD, Live USB). The CD uses the IA-32 architecture, making it compatible with IA-32 and x86-64 PCs. The current version of Debian GNU/Hurd is 2023, published in June 2023.

Debian GNU/kFreeBSD is a discontinued Debian flavor. It used the FreeBSD kernel and GNU userland. The majority of software in Debian GNU/kFreeBSD was built from the same sources as Debian, with some kernel packages from FreeBSD. The "k" in "kFreeBSD" is an abbreviation for "kernel", which refers to the FreeBSD kernel. Before discontinuing the project, Debian maintained i386 and amd64 ports. The last version of Debian kFreeBSD was Debian 8 (Jessie) RC3. Debian GNU/kFreeBSD was created in 2002. It was included in Debian 6.0 (Squeeze) as a technology preview, and in Debian 7.0 (Wheezy) as an official port. Debian GNU/kFreeBSD was discontinued as an officially supported platform as of Debian 8.0. Debian developers cited OSS, pf, jails, NDIS, and ZFS as reasons for being interested in the FreeBSD kernel. It has not been updated since Debian 8. From July 2019, the operating system continues to be maintained unofficially.As of July 2023, the development of Debian GNU/kFreeBSD has officially terminated due to the lack of interest and developers.




Doonesbury

Doonesbury is a comic strip by American cartoonist Garry Trudeau that chronicles the adventures and lives of an array of characters of various ages, professions, and backgrounds, from the President of the United States to the title character, Michael Doonesbury, who has progressed from a college student to a youthful senior citizen over the decades.

Created in "the throes of '60s and '70s counterculture", and frequently political in nature, "Doonesbury" features characters representing a range of affiliations, but the cartoon is noted for a liberal viewpoint. The name "Doonesbury" is a combination of the word "doone" (American prep school slang for someone who is clueless, inattentive, or careless) and the surname of Charles Pillsbury, Trudeau's roommate at Yale University.

"Doonesbury" is written and penciled by Garry Trudeau, then inked and lettered by an assistant, Don Carlton,
then Todd Pound. Sunday strips are colored by George Corsillo. "Doonesbury" was a daily strip through most of its existence, but since February 2014 it has run repeat strips Monday through Saturday, and new strips on Sunday.

"Doonesbury" began as a continuation of "Bull Tales", which appeared in the Yale University student newspaper, the "Yale Daily News", from 1968 to 1970. It focused on local campus events at Yale.

"Doonesbury" proper debuted as a daily strip in twenty-eight newspapers on October 26, 1970 (it being the first strip from Universal Press Syndicate). A Sunday strip began on March 21, 1971. Many of the early strips were reprints of the "Bull Tales" cartoons, with some changes to the drawings and plots. B. D.'s helmet changed from having a "Y" (for Yale) to a star (for the fictional Walden College). Mike and B. D. started "Doonesbury" as roommates; they were not roommates in "Bull Tales".

"Doonesbury" became known for its social and political commentary. By the 2010s, it was syndicated in approximately 1,400 newspapers worldwide.

In May 1975, "Doonesbury" became the first daily comic strip to win a Pulitzer Prize, taking the award for Editorial Cartooning. That year, U.S. President Gerald Ford told the Radio and Television Correspondents' Association at their annual dinner, "There are only three major vehicles to keep us informed as to what is going on in Washington: the electronic media, the print media, and "Doonesbury", not necessarily in that order."

Trudeau took a 22-month hiatus, from January 2, 1983, to September 30, 1984. Before the break in the strip, the characters were eternal college students, living in a commune together near Walden College, which was modeled after Trudeau's alma mater, Yale. During the break, Trudeau helped create a Broadway musical of the strip, showing the graduation of the main characters. The Broadway adaptation opened at the Biltmore Theatre on November 21, 1983, and played 104 performances. Elizabeth Swados composed the music for Trudeau's book and lyrics.

The strip resumed some time after the events in the musical, with further changes having taken place after the end of the musical's plot. Mike, Mark, Zonker, B.D., and Boopsie were all now graduates; B.D. and Boopsie were living in Malibu, California, where B.D. was a third-string quarterback for the Los Angeles Rams, and Boopsie was making a living from walk-on and cameo roles. Mark was living in Washington, D.C., working for National Public Radio. Michael and J.J. had gotten married, and Mike had dropped out of business school to start work in an advertising agency in New York City. Zonker, still not ready for the "real world", was living with Mike and J.J. until he was accepted as a medical student at his Uncle Duke's "Baby Doc College" in Haiti.

Prior to the hiatus, the strip's characters had aged only slightly. But when Trudeau returned to "Doonesbury", the characters began to age in something close to real time, as in "Gasoline Alley" and "For Better or for Worse", Since then, the main characters' ages and career developments have tracked that of standard media portrayals of baby boomers, with jobs in advertising, law enforcement, and the dot-com boom. Current events are mirrored through the original characters, their offspring (the "second generation"), and occasional new characters.

Garry Trudeau received the National Cartoonist Society Newspaper Comic Strip Award for 1994, and their Reuben Award for 1995 for his work on the strip.

"Doonesbury" syndicate, Universal Uclick, announced on May 29, 2013, that the comic strip would go on hiatus from June 10 to Labor Day of that year while Garry Trudeau worked on his streaming video comedy "Alpha House", which was picked up by Amazon Studios. "Doonesbury Flashbacks" were offered during those weeks, but due to the unusually long hiatus, some newspapers opted to run different comic strips instead. Sunday strips returned as scheduled, but the daily strip's hiatus was extended until November 2013.

After "Alpha House" was renewed for a second season in February 2014, Trudeau announced that he would now produce only Sunday strips for the foreseeable future. Since March 3, 2014, the strip has offered reruns starting from the very beginning of its history as opposed to the recent ones that re-run when Trudeau is on vacation. "Alpha House" was cancelled in 2016, but Trudeau did not return to drawing Monday-to-Saturday strips, and continued his Sunday-only schedule.

In a 2018 interview with "Rolling Stone", Trudeau said that while Donald Trump appears in only a limited number of strips, "for the last two years, he's been subtext in almost all of them."

In 1977, Trudeau wrote a script for a 26-minute animated special, "A Doonesbury Special", which was produced and directed by Trudeau along with John Hubley (who died during the storyboarding stage) and Faith Hubley. The special was first broadcast by NBC on November 27, 1977. It won a Special Jury Award at the Cannes International Film Festival for best short film, and received an Oscar nomination (for best animated short film), both in 1978. Voice actors for the special included Barbara Harris, William Sloane Coffin, Jr., Jack Gilford and Will Jordan. Also included were "Stop in the Middle" and "I Do Believe", two songs "sung" by the character Jimmy Thudpucker (actually actor/singer/songwriter/producer James Allen "Jimmy" Brewer), also part of the "Special". While the compositions and performances were credited to "Jimmy Thudpucker", they were in fact co-written and sung by Brewer, who also co-wrote and provided the vocals for "Ginny's Song", a 1976 single on the Warner Bros. label, and "Jimmy Thudpucker's Greatest Hits", an LP released by Windsong Records, John Denver's subsidiary of RCA Records.

With the exception of Walden College, Trudeau has frequently used real-life settings, based on real scenarios, but with fictional results. Because of lead times, real-world events have rendered some of Trudeau's comics unusable, such as a 1973 series featuring John Ehrlichman, a 1989 series set in Tiananmen Square in Beijing, China, a 1993 series involving Zoë Baird, and a 2005 series involving Harriet Miers. Trudeau has also displayed fluency in various forms of jargon, including those of real estate agents, flight attendants, computer scientists, journalists, presidential aides, and soldiers in Iraq.

The unnamed college attended by the main characters was later given the name "Walden College", revealed to be in Connecticut (the same state as Yale), and depicted as devolving into a third-rate institution under the weight of grade inflation, slipping academic standards, and the end of tenure, issues that Trudeau has consistently revisited since the original characters graduated. Some of the second generation of "Doonesbury" characters have attended Walden, a venue Trudeau uses to advance his concerns about academic standards in the United States.

President King, the leader of Walden College, was originally intended as a parody of Kingman Brewster, President of Yale, but all that remains of that is a certain physical resemblance.

Even though "Doonesbury" frequently features real-life U.S. politicians, they are rarely depicted with their real faces. Originally, strips featuring the President of the United States would show an external view of the White House, with dialogue emerging from inside. During the Gerald Ford administration, characters would be shown speaking to Ford at press conferences, and fictional dialogue supposedly spoken by Ford would be written as coming "off-panel". Similarly, while having several characters as students in a class taught by Henry Kissinger, the dialogue made up for Kissinger would also come from "off-panel" (although Kissinger had earlier appeared as a character with his face shown in a 1972 series of strips in which he met Mark Slackmeyer while the latter was on a trip to Washington). Sometimes hands, or in rare cases, the back of heads would also be seen.

Later, personal symbols reflecting some aspect of their character came into use. These included:


The long career of the series and continual use of real-life political figures, analysts note, have led to some uncanny cases of the cartoon foreshadowing a national shift in the politicians' political fortunes. Tina Gianoulis in "St. James Encyclopedia of Popular Culture" observes that "In 1971, well before the conservative Reagan years, a forward-looking B.D. called Ronald Reagan his 'hero'. In 1984, almost ten years before Congressman Newt Gingrich became Speaker of the House, another character worried that he would 'wake up someday in a country run by Newt Gingrich. In its 2003 series "John Kerry: A Candidate in the Making" on the 2004 presidential race, "The Boston Globe" reprinted and discussed 1971 "Doonesbury" cartoons of the young Kerry's Vietnam War protest speeches.

"Doonesbury" has a large group of recurring characters, with 24 currently listed at the strip's website. There, it notes that "readers new to "Doonesbury" sometimes experience a temporary bout of character shock", as the sheer number of characters (and the historical connections among them) can be overwhelming.

The main characters are a group who attended the fictional Walden College during the strip's first 12 years, and moved into a commune together in April 1972. Most of the other characters first appeared as family members, friends, or other acquaintances. The original Walden Commune residents were Mike Doonesbury, Zonker Harris, Mark Slackmeyer, Nichole, Bernie, and DiDi. In September 1972, Joanie Caucus joined the comic, meeting Mike and Mark in Colorado and eventually moving into the commune. They were later joined by B.D. and his girlfriend (later wife) Boopsie, upon B.D.'s return from Vietnam. Nichole, DiDi, and Bernie were mostly phased out in subsequent years, and Zonker's Uncle Duke was introduced as the most prominent character outside the Walden group, and the main link to many secondary characters.

The Walden students graduated in 1983, after which the strip began to progress in something closer to real time. Their spouses and developing families became more important after this: Joanie's daughter J.J. Caucus married Mike and they had a daughter, Alex Doonesbury. They divorced, Mike married Kim Rosenthal, a Vietnamese refugee (who had appeared in the strip as a baby adopted by a Jewish family just after the fall of Saigon; see Operation Babylift), and J.J. married Zeke Brenner, her former boyfriend and Uncle Duke's former groundskeeper. Joanie married Rick Redfern, and they had a son, Jeff. Uncle Duke and Roland Hedley have also appeared often, frequently in more topical settings unconnected to the main characters. In more recent years the second generation has taken prominence as they have grown to college age: Jeff Redfern, Alex Doonesbury, Zonker's nephew Zipper Harris, and Uncle Duke's son Earl.

"Doonesbury" has covered numerous political and social issues, some of which were pioneering and others that drew criticism:






Charles M. Schulz of "Peanuts" called Trudeau "unprofessional" for taking a long sabbatical. (See also, similar comments by Schulz about sabbaticals taken by Bill Watterson.) Nor was the return of the strip itself greeted with universal acclaim; in 1985, "Saturday Review" listed Trudeau as one of the country's "Most Overrated People in American Arts and Letters", commenting that the "most publicized return since MacArthur's has produced a strip that is predictable, mean-spirited, and not as funny as before."

"Doonesbury" has angered, irritated, or been rebuked by many of the political figures that have appeared or been referred to in the strip over the years. A 1984 series of strips showing Vice President George H. W. Bush placing his manhood in a blind trust—in parody of Bush's use of that financial instrument to fend off concerns that his governmental decisions would be influenced by his investment holdings—brought the politician to complain, ""Doonesbury" carrying water for the opposition. Trudeau is coming out of deep left field."

Some conservatives have intensely criticized "Doonesbury". Several examples are cited in the Milestones section of the strip's website. The strip has also met criticism from its readers almost since it began syndicated publication. For example, when Lacey Davenport's husband Dick, in the last moments before his death, calls on God, several conservative pundits called the strip blasphemous. The sequence of Dick Davenport's final bird-watching and fatal heart attack was run in November 1986.

Liberal politicians skewered by Trudeau in the strip have also complained, including Democrats such as former U.S. House Speaker Tip O'Neill and California Governor Jerry Brown.

Strips about post-World War II American wars have also generated controversy, including Vietnam, Grenada, Panama and both Gulf Wars.

After many letter-writing campaigns demanding the removal of the strip were unsuccessful, conservatives changed their tactics, and instead of writing to newspaper editors, they began writing to one of the printers who prints the color Sunday comics. In 2005, Continental Features refused to continue printing the Sunday "Doonesbury", causing it to disappear from the 38 Sunday papers that Continental Features printed. Of the 38, only one newspaper, "The Anniston Star" in Anniston, Alabama, continued to carry the Sunday "Doonesbury", though of necessity in black and white.

Some newspapers have dealt with the criticism by moving the strip from the comics page to the editorial page, because many people believe that a politically based comic strip like "Doonesbury" does not belong in a traditionally child-friendly comics section. The "Lincoln Journal" started the trend in 1973. In some papers (such as the "Tulsa World" and "Orlando Sentinel") "Doonesbury" appears on the opinions page alongside "Mallard Fillmore", a politically conservative comic strip.





Dice

Dice (: die or dice) are small, throwable objects with marked sides that can rest in multiple positions. They are used for generating random values, commonly as part of tabletop games, including dice games, board games, role-playing games, and games of chance.

A traditional die is a cube with each of its six faces marked with a different number of dots (pips) from one to six. When thrown or rolled, the die comes to rest showing a random integer from one to six on its upper surface, with each value being equally likely. Dice may also have polyhedral or irregular shapes, may have faces marked with numerals or symbols instead of pips and may have their numbers carved out from the material of the dice instead of marked on it. Loaded dice are specifically designed or modified to favor some results over others for cheating or entertainment.

Dice have been used since before recorded history, and it is uncertain where they originated. It is theorized that dice developed from the practice of fortune-telling with the talus of hoofed animals, colloquially known as knucklebones. The Egyptian game of senet (played before 3000 BCE and up to the 2nd century CE) was played with flat two-sided throwsticks which indicated the number of squares a player could move, and thus functioned as a form of dice. Perhaps the oldest known dice were excavated as part of a backgammon-like game set at the Burnt City, an archeological site in south-eastern Iran, estimated to be from between 2800 and 2500 BCE. Bone dice from Skara Brae, Scotland have been dated to 3100–2400 BCE. Excavations from graves at Mohenjo-daro, an Indus Valley civilization settlement, unearthed terracotta dice dating to 2500–1900 BCE, including at least one die whose opposite sides all add up to seven, as in modern dice.

Games involving dice are mentioned in the ancient Indian "Rigveda", "Atharvaveda," "Mahabharata" and Buddhist games list. There are several biblical references to "casting lots" ( "yappîlū ḡōrāl"), as in Psalm 22, indicating that dicing (or a related activity) was commonplace when the psalm was composed. Knucklebones was a game of skill played in ancient Greece; a derivative form had the four sides of bones receive different values like modern dice.

Although gambling was illegal, many Romans were passionate gamblers who enjoyed dicing, which was known as "aleam ludere" ("to play at dice"). There were two sizes of Roman dice. "Tali" were large dice inscribed with one, three, four, and six on four sides. "Tesserae" were smaller dice with sides numbered from one to six. Twenty-sided dice date back to the 2nd century CE and from Ptolemaic Egypt as early as the 2nd century BCE.

Dominoes and playing cards originated in China as developments from dice. The transition from dice to playing cards occurred in China around the Tang dynasty (618–907 CE), and coincides with the technological transition from rolls of manuscripts to block printed books. In Japan, dice were used to play a popular game called sugoroku. There are two types of sugoroku. "Ban-sugoroku" is similar to backgammon and dates to the Heian period (794–1185 CE), while "e-sugoroku" is a racing game.

Dice are thrown onto a surface either from the hand or from a container designed for this (such as a cup, tray, or tower). The face (or corner, in cases such as tetrahedral dice, or edge, for odd-numbered long dice) of the die that is uppermost when it comes to rest provides the value of the throw. 

The result of a die roll is determined by the way it is thrown, according to the laws of classical mechanics (although luck is often credited for the results of a roll). A die roll is made random by uncertainty in minor factors such as tiny movements in the thrower's hand; they are thus a crude form of hardware random number generator. 

One typical contemporary dice game is craps, where two dice are thrown simultaneously and wagers are made on the total value of the two dice. Dice are frequently used to introduce randomness into board games, where they are often used to decide the distance through which a piece will move along the board (as in backgammon and "Monopoly").

Common dice are small cubes, most often across, whose faces are numbered from one to six, usually by patterns of round dots called pips. (While the use of Arabic numerals is occasionally seen, such dice are less common.)

Opposite sides of a modern die traditionally add up to seven, requiring the 1, 2, and 3 faces to share a vertex. The faces of a die may be placed clockwise or counterclockwise about this vertex. If the 1, 2, and 3 faces run counterclockwise, the die is called "right-handed". If those faces run clockwise, the die is called "left-handed". Western dice are normally right-handed, and Chinese dice are normally left-handed.

The pips on standard six-sided dice are arranged in specific patterns as shown. Asian style dice bear similar patterns to Western ones, but the pips are closer to the center of the face; in addition, the pips are differently sized on Asian style dice, and the pips are colored red on the 1 and 4 sides. Red fours may be of Indian origin.

Non-precision dice are manufactured via the plastic injection molding process, often made of polymethyl methacrylate (PMMA). The pips or numbers on the die are a part of the mold. Different pigments can be added to the dice to make them opaque or transparent, or multiple pigments may be added to make the dice speckled or marbled.

The coloring for numbering is achieved by submerging the die entirely in paint, which is allowed to dry. The die is then polished via a tumble finishing process similar to rock polishing. The abrasive agent scrapes off all of the paint except for the indents of the numbering. A finer abrasive is then used to polish the die. This process also produces the smoother, rounded edges on the dice.

Precision casino dice may have a polished or sand finish, making them transparent or translucent respectively. Casino dice have their pips drilled, then filled flush with a paint of the same density as the material used for the dice, such that the center of gravity of the dice is as close to the geometric center as possible. This mitigates concerns that the pips will cause a small bias. All such dice are stamped with a serial number to prevent potential cheaters from substituting a die. Precision backgammon dice are made the same way; they tend to be slightly smaller and have rounded corners and edges, to allow better movement inside the dice cup and stop forceful rolls from damaging the playing surface.

The word die comes from Old French "dé"; from Latin "datum" "something which is given or played".

While the terms "ace", "deuce", "trey", "cater", "cinque" and "sice" are generally obsolete, with the names of the numbers preferred, they are still used by some professional gamblers to designate different sides of the dice. "Ace" is from the Latin "as", meaning "a unit"; the others are 2 to 6 in Old French.

When rolling two dice, certain combinations have slang names. The term "snake eyes" is a roll of one pip on each die. The "Online Etymology Dictionary" traces use of the term as far back as 1919.
The US term "boxcars", also known as "midnight", is a roll of six pips on each die. The pair of six pips resembles a pair of boxcars on a freight train. Many rolls have names in the game of craps.

Using Unicode characters, the faces can be shown in text using the range U+2680 to U+2685 or using decimal codice_1 to codice_2, and the emoji using U+1F3B2 or codice_3 from the Miscellaneous Symbols and Pictographs block.

A loaded, weighted, cheat, or crooked die is one that has been tampered with so that it will land with a specific side facing upwards more often or less often than a fair die would. There are several methods for making loaded dice, including rounded faces, off-square faces, and weights. Casinos and gambling halls frequently use transparent cellulose acetate dice as tampering is easier to detect than with opaque dice.

Various shapes like two-sided or four-sided dice are documented in archaeological findings; for example, from Ancient Egypt and the Middle East. While the cubical six-sided die became the most common type in many parts of the world, other shapes were always known, like 20-sided dice in Ptolemaic and Roman times. 

The modern tradition of using "sets" of polyhedral dice started around the end of the 1960s when non-cubical dice became popular among players of wargames, and since have been employed extensively in role-playing games and trading card games. Dice using both the numerals 6 and 9, which are reciprocally symmetric through rotation, typically distinguish them with a dot or underline.

Dice are often sold in sets, matching in color, of six different shapes. Five of the dice are shaped like the Platonic solids, whose faces are regular polygons. Aside from the cube, the other four Platonic solids have 4, 8, 12, and 20 faces, allowing for those number ranges to be generated. The only other common non-cubical die is the 10-sided die, a pentagonal trapezohedron die, whose faces are ten kites, each with two different edge lengths, three different angles, and two different kinds of vertices. Such sets frequently include a second 10-sided die either of contrasting color or numbered by tens, allowing the pair of 10-sided dice to be combined to generate numbers between 1 and 100.

Using these dice in various ways, games can closely approximate a variety of probability distributions. For instance, 10-sided dice can be rolled in pairs to produce a uniform distribution of random percentages, and summing the values of multiple dice will produce approximations to normal distributions. 

Unlike other common dice, a four-sided (tetrahedral) die does not have a side that faces upward when it is at rest on a surface, so it must be read in a different way. On some four-sided dice, each face features multiple numbers, with the same number printed near each vertex on all sides. In this case, the number around the vertex pointing up is used. Alternatively, the numbers on a tetrahedral die can be placed at the middles of the edges, in which case the numbers around the base are used.

Normally, the faces on a die will be placed so opposite faces will add up to one more than the number of faces. (This is not possible with 4-sided dice and dice with an odd number of faces.) Some dice, such as those with 10 sides, are usually numbered sequentially beginning with 0, in which case the opposite faces will add to one less than the number of faces.

Some twenty-sided dice have a different arrangement used for the purpose of keeping track of an integer that counts down, such as health points. These "spindown dice" are arranged such that adjacent integers appear on adjacent faces, allowing the user to easily find the next lower number. They are commonly used with collectible card games.

"Uniform fair dice" are dice where all faces have equal probability of outcome due to the symmetry of the die as it is face-transitive. In addition to the Platonic solids, these theoretically include:


Two other types of polyhedra are technically not face-transitive, but are still fair dice due to symmetry:


Long dice and teetotums can in principle be made with any number of faces, including odd numbers. Long dice are based on the infinite set of prisms. All the rectangular faces are mutually face-transitive, so they are equally probable. The two ends of the prism may be rounded or capped with a pyramid, designed so that the die cannot rest on those faces. 4-sided long dice are easier to roll than tetrahedra, and are used in the traditional board games dayakattai and daldøs.

The faces of most dice are labelled using sequences of whole numbers, usually starting at one, expressed with either pips or digits. However, there are some applications that require results other than numbers. Examples include letters for Boggle, directions for "Warhammer Fantasy Battle", Fudge dice, playing card symbols for poker dice, and instructions for sexual acts using sex dice.

Dice may have numbers that do not form a counting sequence starting at one. One variation on the standard die is known as the "average" die. These are six-sided dice with sides numbered codice_4, which have the same arithmetic mean as a standard die (3.5 for a single die, 7 for a pair of dice), but have a narrower range of possible values (2 through 5 for one, 4 through 10 for a pair). They are used in some table-top wargames, where a narrower range of numbers is required. Other numbered variations include Sicherman dice and intransitive dice.

A die can be constructed in the shape of a sphere, with the addition of an internal cavity in the shape of the dual polyhedron of the desired die shape and an internal weight. The weight will settle in one of the points of the internal cavity, causing it to settle with one of the numbers uppermost. For instance, a sphere with an octahedral cavity and a small internal weight will settle with one of the 6 points of the cavity held downwards by the weight.

Many board games use dice to randomize how far pieces move or to settle conflicts. Typically, this has meant that rolling higher numbers is better. Some games, such as "Axis & Allies", have inverted this system by making the lower values more potent. In the modern age, a few games and game designers have approached dice in a different way by making each side of the die similarly valuable. In "Castles of Burgundy", players spend their dice to take actions based on the die's value. In this game, a six is not better than a one, or vice versa. In "Quarriors" (and its descendant, "Dicemasters"), different sides of the dice can offer completely different abilities. Several sides often give resources while others grant the player useful actions.

Dice can be used for divination and using dice for such a purpose is called cleromancy. A pair of common dice is usual, though other forms of polyhedra can be used. Tibetan Buddhists sometimes use this method of divination. It is highly likely that the Pythagoreans used the Platonic solids as dice. They referred to such dice as "the dice of the gods" and they sought to understand the universe through an understanding of geometry in polyhedra.
Polyhedral dice are commonly used in role-playing games. The fantasy role-playing game "Dungeons & Dragons" (D&D) is largely credited with popularizing dice in such games. Some games use only one type, like "Exalted" which uses only ten-sided dice. Others use numerous types for different game purposes, such as D&D, which makes use of all common polyhedral dice. Dice are usually used to determine the outcome of events. Games typically determine results either as a total on one or more dice above or below a fixed number, or a certain number of rolls above a certain number on one or more dice. Due to circumstances or character skill, the initial roll may have a number added to or subtracted from the final result, or have the player roll extra or fewer dice. To keep track of rolls easily, dice notation is frequently used.

Astrological dice are a specialized set of three 12-sided dice for divination; the first die represents the planets, the Sun, the Moon, and the nodes of the Moon, the second die represents the 12 zodiac signs, and the third represents the 12 houses. A specialized icosahedron die provides the answers of the Magic 8 Ball, conventionally used to provide answers to yes-or-no questions.

Dice can be used to generate random numbers for use in passwords and cryptography applications. The Electronic Frontier Foundation describes a method by which dice can be used to generate passphrases. Diceware is a method recommended for generating secure but memorable passphrases, by repeatedly rolling five dice and picking the corresponding word from a pre-generated list.

In many gaming contexts, especially tabletop role-playing games, shorthand notations representing different dice rolls are used. A "d" or "D" is used to indicate a die with a specific number of sides; for example,codice_5denotes a four-sided die. If several dice of the same type are to be rolled, this is indicated by a leading number specifying the number of dice. Hence,codice_6 means the player should roll six eight-sided dice and add the results. Modifiers to a die roll can also be indicated as desired. For example, codice_7 instructs the player to roll three six-sided dice, calculate the total, and add four to it.




Dumpster diving

Dumpster diving (also totting, skipping, skip diving or skip salvage) is salvaging from large commercial, residential, industrial and construction containers for unused items discarded by their owners but deemed useful to the picker. It is not confined to dumpsters and skips specifically and may cover standard household waste containers, curb sides, landfills or small dumps.

Different terms are used to refer to different forms of this activity. For picking materials from the curbside trash collection, expressions such as curb shopping, trash picking or street scavenging are sometimes used. In the UK, if someone is primarily seeking recyclable metal, they are scrapping, and if they are picking the leftover food from farming left in the fields, they are gleaning.

People dumpster dive for items such as clothing, furniture, food, and similar items in good working condition. Some people do this out of necessity due to poverty; others do it for ideological reasons or professionally and systematically for profit.

The term "dumpster diving" emerged in the 1980s, combining "diving" with "dumpster", a large commercial trash bin. The term "Dumpster" itself comes from the Dempster Dumpster, a brand of bins manufactured by Dempster Brothers beginning in 1937. "Dumpster" became genericized by the 1970s. According to the "Oxford English Dictionary", the term "dumpster diving" is chiefly found in American English and first appeared in print in 1983, with the verb "dumpster-dive" appearing a few years later. In British English, the practice may be known as "skipping", from skip, another term for this type of container.

Alternative names for the practice include bin-diving, containering, D-mart, dumpstering, totting, and skipping. In Australia, garbage picking is called "skip dipping."

The term "binner" is often used to describe individuals who collect recyclable materials for their deposit value. For example, in Vancouver, British Columbia, binners, or bottle collectors, search garbage cans and dumpsters for recyclable materials that can be redeemed for their deposit value. On average, these binners earn about $40 a day for several garbage bags full of discarded containers. Some are scammers seeking for receipts to use in committing return fraud.

The karung guni, Zabbaleen, the rag and bone man, waste picker, junk man or bin hoker are terms for people who make their living by sorting and trading trash. A similar process known as gleaning was practised in rural areas and some ancient agricultural societies, where the residue from farmers' fields was collected.

Some dumpster divers, who self-identify as freegans, aim to reduce their ecological footprint by living from dumpster-dived-goods, sometimes exclusively.

The activity is performed by people out of necessity in the developing world. Some scavengers perform in organized groups, and some organize on various internet forums and social networking websites. By reusing, or repurposing, resources destined for the landfill, dumpster diving is sometimes considered to be an environmentalist endeavor, and is thus practiced by many pro-green communities. The wastefulness of consumer society and throw-away culture compels some individuals to rescue usable items (for example, computers or smartphones, which are frequently discarded due to the extensive use of planned obsolescence in the technology industry) from destruction and divert them to those who can make use of the items.

A wide variety of things may be disposed while still repairable or in working condition, making salvage of them a source of potentially free items for personal use, or to sell for profit. Irregular, blemished or damaged items that are still otherwise functional are regularly thrown away. Discarded food that might have slight imperfections, near its expiration date, or that is simply being replaced by newer stock is often tossed out despite being still edible. Many retailers are reluctant to sell this stock at reduced prices because of the risks that people will buy it instead of the higher-priced newer stock, that extra handling time is required, and that there are liability risks. In the United Kingdom, cookery books have been written on the cooking and consumption of such foods, which has contributed to the popularity of skipping. Artists often use discarded materials retrieved from trash receptacles to create works of found objects or assemblage.

Students have been known to partake in dumpster diving to obtain high tech items for technical projects, or simply to indulge their curiosity for unusual items. Dumpster diving can additionally be used in support of academic research. Garbage picking serves as the main tool for garbologists, who study the sociology and archeology of trash in modern life. Private and government investigators may pick through garbage to obtain information for their inquiries. Illegal cigarette consumption may be deduced from discarded packages.

Dumpster diving can be hazardous, due to potential exposure to biohazardous matter, broken glass, and overall unsanitary conditions that may exist in dumpsters.

Arguments against garbage picking often focus on the health and cleanliness implications of people rummaging in trash. This exposes the dumpster divers to potential health risks, and, especially if the dumpster diver does not return the non-usable items to their previous location, may leave trash scattered around. Divers can also be seriously injured or killed by garbage collection vehicles; in January 2012, in La Jolla, Swiss-American man Alfonso de Bourbon was killed by a truck while dumpster diving.

The unauthorized taking of materials from a dumpster or other waste disposal container is commonly referred to as "garbage theft". Dumpster diving is a different idiom. Due to the typical low value of the stolen goods, garbage theft is not typically recognized as a serious crime, with laws against it frequently focusing on combating identity theft instead. Depending on the state or nation's rules surrounding low-level crime, garbage theft may be considered a form of petty theft and subject to a penalty that often entails a brief period of incarceration, a modest fine, or both. As a privacy violation, discarded medical records as trash led to a $140,000 penalty against Massachusetts billing company Goldthwait Associates and a group of pathology offices in 2013 and a $400,000 settlement between Midwest Women's Healthcare Specialists and 1,532 clients in Kansas City in 2014.

Identity theft has historically been carried out through garbage theft, with thieves utilizing bank and credit card statements discovered in trash to assume the identity of a victim or access their credit. 

Criminals have been known to dumpster dive for cash receipts as part of a scheme to steal items and return them for cash, a form of return fraud known as "shoplisting." Police investigating shoplifting in Bellingham, Washington, found dozens of receipts from retailers such as The Home Depot, Rite Aid and Fred Meyer, along with a list of items on the receipts. Suspects believed to have taken receipts from trash receptacles near Walmart locations were arrested for return fraud in 2016 in Madison, Wisconsin.

Since dumpsters are usually located on private premises, divers may occasionally get in trouble for trespassing while dumpster diving, though the law is enforced with varying degrees of rigor. Some businesses may lock dumpsters to prevent pickers from congregating on their property, vandalism to their property, and to limit potential liability if a dumpster diver is injured while on their property.

Police searches of discarded waste as well as similar methods are also generally not considered violations; evidence seized in this manner has been permitted in many criminal trials. In the United States this has been affirmed by numerous courts including and up to the Supreme Court, in the decision "California v. Greenwood". The doctrine is not as well established in regard to civil litigation.

Companies run by private investigators specializing in such techniques have emerged as a result of the need for discreet, undetected retrieval of documents and evidence for civil and criminal trials. Private investigators have also written books on "P.I. technique" in which dumpster diving or its equivalent "wastebasket recovery" figures prominently.

In 2009, a Belgian dumpster diver and eco-activist nicknamed Ollie was detained for a month for removing food from a garbage can and was accused of theft and burglary. On February 25, 2009, he was arrested for removing food from a garbage can at an AD Delhaize supermarket in Bruges. Ollie's trial evoked protests in Belgium against restrictions from taking discarded food items.

In Ontario, Canada, the "Trespass to Property Act"—legislation dating back to the "British North America Act" of 1867—grants property owners and security guards the power to ban anyone from their premises, for any reason, permanently. This is done by issuing a notice to the intruder, who will only be breaking the law upon return. Similar laws exist in Prince Edward Island and Saskatchewan. A recent case in Canada, which involved a police officer who retrieved a discarded weapon from a trash receptacle as evidence, created some controversy. The judge ruled the policeman's actions as legal although there was no warrant present, which led some to speculate the event as validation for any Canadian citizen to raid garbage disposals.

Skipping in England and Wales may qualify as theft within the Theft Act 1968 or as common-law theft in Scotland, though there is very little enforcement in practice.

In Germany, dumpster diving is referred to as "containern", and a waste container's contents are regarded as the property of the container's owner. Therefore, taking items from such a container is viewed as theft. However, the police will routinely disregard the illegality of garbage picking since the items found are generally of low value. There has only been one known instance where people were prosecuted. In 2009 individuals were arrested on assumed burglary as they had surmounted a supermarket's fence which was then followed by a theft complaint by the owner; the case was suspended.

In the United States, the 1988 "California v. Greenwood" case in the U.S. Supreme Court held that there is no common law expectation of privacy for discarded materials. There are, however, limits to what can legally be taken from a company's refuse. In a 1983 Minnesota case involving the theft of customer lists from a garbage can, "Tennant Company v. Advance Machine Company" (355 N.W.2d 720), the owner of the discarded information was awarded $500,000 in damages.

Dumpster diving is practiced differently in developed countries than in developing countries.



In the 1960s, Jerry Schneider, using recovered instruction manuals from The Pacific Telephone & Telegraph Company, used the company's own procedures to acquire hundreds of thousands of dollars' worth of telephone equipment over several years until his arrest.

The "Castle Infinity" videogame, after its shutdown in 2005, was brought back from the dead by a fan rescuing its servers from the trash.

In October 2013, in North London, three men were arrested and charged under the 1824 Vagrancy Act when they were caught taking discarded food: tomatoes, mushrooms, cheese and cakes from bins behind an Iceland supermarket. The charges were dropped on 29 January 2014 after much public criticism as well as a request by Iceland's chief executive, Malcolm Walker.

In 1996, the source code for the Atari 7800 was discovered in the dumpster of the Atari office when the company closed.





Digital synthesizer

A digital synthesizer is a synthesizer that uses digital signal processing (DSP) techniques to make musical sounds. This in contrast to older analog synthesizers, which produce music using analog electronics, and samplers, which play back digital recordings of acoustic, electric, or electronic instruments. Some digital synthesizers emulate analog synthesizers; others include sampling capability in addition to digital synthesis.

The very earliest digital synthesis experiments were made with computers, as part of academic research into sound generation. 

In 1957, the first programming language for computer music, MUSIC, was developed by Max Mathews on an IBM 704 at Bell Labs in 1957. It generates digital audio waveforms through direct synthesis.

Circa 1969, EMS MUSYS 3 system was developed by Peter Grogono (software), David Cockerell (hardware and interfacing) and Peter Zinovieff (system design and operation) at their London (Putney) Studio. The system ran on two mini-computers, Digital Equipment PDP-8's. These had a pair of fast D/A and A/D converters, 12,000 (12k) bytes of core memory (RAM), backed up by a hard drive of 32k and by tape storage (DecTape). The earliest digital sampling was done on that system during 1971–1972 for Harrison Birtwistle's ""Chronometer"" released in 1975.

In 1972–1974, Dartmouth Digital Synthesizer was developed by Dartmouth College Professors Jon Appleton and Frederick J. Hooven, in association with NED co-founders Sydney A. Alonso and Cameron W. Jones.

In 1977, Bell Labs Digital Synthesizer was developed by Hal Ales at Bell Labs.

In 1977, New England Digital (NED) released the Synclavier, the first commercial synthesizer to use purely digital sound generation and also the world's first commercial FM synthesizer.

Early commercial digital synthesizers used simple hard-wired digital circuitry to implement techniques such as additive synthesis and FM synthesis. Two other early commercial digital synthesizers were the Fairlight CMI, introduced in 1979, and the New England Digital Synclavier II, introduced in 1979 as an upgrade to the original Synclavier. The Fairlight CMI was the one of the earlier sampling synthesizers, while the Synclavier originally used FM synthesis technology licensed from Yamaha, before adding the world's first 16-bit, real-time hard drive streaming sampler later in 1982. The Fairlight CMI and the Synclavier were both expensive systems, retailing for more than $20,000 in the early 1980s. The cost of digital synthesizers began falling rapidly in the early 1980s. E-mu Systems introduced the Emulator sampling synthesizer in 1982 at a retail price of $7,900. Although not as flexible or powerful as either the Fairlight CMI or the Synclavier, its lower cost and portability made it popular.
With the addition of sophisticated sequencers on board, now added to built-in effects and other features, the 'workstation' synthesizer had been born. These always include a multi-track sequencer, and can often record and play back samples, and in later years full audio tracks, to be used to record an entire song. These are usually also ROMplers, playing back samples, to give a wide variety of realistic instrument and other sounds such as drums, string instruments and wind instruments to sequence and compose songs, along with popular keyboard instrument sounds such as electric pianos and organs.

As there was still interest in analog synthesizers, and with the increase of computing power, over the 1990s another type of synthesizer arose: the analog modeling, or "virtual analog" synthesizer. These use computing power to simulate traditional analog waveforms and circuitry such as envelopes and filters, with the most popular examples of this type of instrument including the Nord Lead and Access Virus.

Digital synthesizers can now be completely emulated in software ("softsynth"), and run on conventional PC hardware. Such soft implementations require careful programming and a fast CPU to get the same latency response as their dedicated equivalents. To reduce latency, some professional sound card manufacturers have developed specialized Digital Signal Processing ([DSP]) hardware. Dedicated digital synthesizers have the advantage of a performance-friendly user interface (physical controls like buttons for selecting features and enabling functionality, and knobs for setting variable parameters). On the other hand, software synthesizers have the advantages afforded by a rich graphical display.

With focus on performance-oriented keyboards and digital computer technology, manufacturers of commercial electronic instruments created some of the earliest digital synthesizers for studio and experimental use with computers being able to handle built-in sound synthesis algorithms.

In 1973, the Japanese company Yamaha licensed the patent for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a commercial digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation, though it would take several years before Yamaha were to release their FM digital synthesizers. In the 1970s, Yamaha were granted a number of patents, under the company's former name "Nippon Gakki Seizo Kabushiki Kaisha", evolving Chowning's early work on FM synthesis technology. Yamaha built the first prototype digital synthesizer in 1974.

Released in 1979, the Casio VL-1 was the first low budget digital synthesizer, selling for $69.95. Yamaha eventually commercialized their FM synthesis technology and released the company's first FM digital synthesizer in 1980, the Yamaha GS-1, but at an expensive retail price of $16,000.

Introduced in 1983, the Yamaha DX7 was the breakthrough digital synthesizer to have a major impact, both innovative and affordable, and thus spelling the decline of analog synthesizers. It used FM synthesis and, although it was incapable of the sampling synthesis of the Fairlight CMI, its price was around $2,000, putting it within range of a much larger number of musicians. The DX-7 was also known for its "key scaling" method to avoid distortion and for its recognizably bright tonality that was partly due to its high sampling rate of 57 kHz. It became indispensable to many music artists of the 1980s, and would become one of the best-selling synthesizers of all time.

In 1987, Roland released its own influential synthesizer of the time, the D-50. This popular synth broke new ground in affordably combining short samples and digital oscillators, as well as the innovation of built-in digital effects (reverb., chorus, equalizer). Roland called this Linear Arithmetic (LA) synthesis. This instrument is responsible for some of the very recognisable preset synthesizer sounds of the late 1980s, such as the Pizzagogo sound used on Enya's "Orinoco Flow."

It gradually became feasible to include high quality samples of existing instruments as opposed to synthesizing them. In 1988, Korg introduced the last of the hugely popular trio of digital synthesizers of the 1980s after the DX7 and D50, the M1. This heralded both the increasing popularisation of digital sample-based synthesis, and the rise of 'workstation' synthesizers. After this time, many popular modern digital synthesizers have been described as not being full synthesizers in the most precise sense, as they play back samples stored in their memory. However, they still include options to shape the sounds through use of envelopes, LFOs, filters and effects such as reverb. The Yamaha Motif and Roland Fantom series of keyboards are typical examples of this type, described as 'ROMplers'; at the same time, they are also examples of "workstation" synthesizers.

As the cost of processing power and memory fell, new types of synthesizers emerged, offering a variety of novel sound synthesis options. The Korg Oasys was one such example, packaging multiple digital synthesizers into a single unit.

An analog synthesizer creates sound using electronic circuitry, such as voltage-controlled oscillators and voltage-controlled filters. In contrast, a digital synthesizer generates a stream of numbers, often using some form of digital signal processor, which are then converted to sound using a digital-to-analog converter (DAC).

A digital synthesizer is in essence a computer with (often) a piano or organ keyboard and an LCD as a user interface. Because computer technology is rapidly advancing, it is often possible to offer more features in a digital synthesizer than in an analog synthesizer at a given price. However, both technologies have their own merit. Some forms of synthesis, such as, for instance, sampling and additive synthesis are not feasible in analog synthesizers, while on the other hand, many musicians prefer the character of analog synthesizers over their digital equivalent.

The new wave era of the 1980s first brought the digital synthesizer to the public ear. Bands like Talking Heads and Duran Duran used the digitally made sounds on some of their most popular albums. Other more pop-inspired bands like Hall & Oates began incorporating the digital synthesizer into their sound in the 1980s. Through breakthroughs in technology in the 1990s many modern synthesizers use DSP.

Working in more or less the same way, every digital synthesizer appears similar to a computer. At a steady sample rate, digital synthesis produces a stream of numbers. Sound from speakers is then produced by a conversion to analog form. Direct digital synthesis is the typical architecture for digital synthesizers. Through signal generation, voice and instrument-level processing, a signal flow is created and controlled either by MIDI capabilities or voice and instrument-level controls.


Definition of music

A definition of music endeavors to give an accurate and concise explanation of music's basic attributes or essential nature and it involves a process of defining what is meant by the term "music". Many authorities have suggested definitions, but defining music turns out to be more difficult than might first be imagined, and there is ongoing debate. A number of explanations start with the notion of music as "organized sound," but they also highlight that this is perhaps too broad a definition and cite examples of organized sound that are not defined as music, such as human speech and sounds found in both natural and industrial environments . The problem of defining music is further complicated by the influence of culture in music cognition.

The "Concise Oxford Dictionary" defines music as "the art of combining vocal or instrumental sounds (or both) to produce beauty of form, harmony, and expression of emotion". However, some music genres, such as noise music and musique concrète, challenge these ideas by using sounds not widely considered as musical, beautiful or harmonious, like randomly produced electronic distortion, feedback, static, cacophony, and sounds produced using compositional processes which utilize indeterminacy.

An often-cited example of the dilemma in defining music is the work "4′33″" (1952) by the American composer John Cage (1912–1992). The written score has three movements and directs the performer(s) to appear on stage, indicate by gesture or other means when the piece begins, then make no sound throughout the duration of the piece, marking sections and the end by gesture. The audience hears only whatever ambient sounds may occur in the room. Some argue that "4′33″" is not music because, among other reasons, it contains no sounds that are conventionally considered "musical" and the composer and performer(s) exert no control over the organization of the sounds heard. Others argue it is music because the conventional definitions of musical sounds are unnecessarily and arbitrarily limited, and control over the organization of the sounds is achieved by the composer and performer(s) through their gestures that divide what is heard into specific sections and a comprehensible form.

Because of differing fundamental concepts of music, the languages of many cultures do not contain a word that can be accurately translated as "music" as that word is generally understood by Western cultures. Inuit and most North American Indian languages do not have a general term for music. Among the Aztecs, the ancient Mexican theory of rhetoric, poetry, dance, and instrumental music used the Nahuatl term "In xochitl-in kwikatl" to refer to a complex mix of music and other poetic verbal and non-verbal elements, and reserved the word "Kwikakayotl" (or cuicacayotl) only for the sung expressions. There is no term for music in Nigerian languages Tiv, Yoruba, Igbo, Efik, Birom, Hausa, Idoma, Eggon or Jarawa. Many other languages have terms which only partly cover what Western culture typically means by the term "music".() The Mapuche of Argentina do not have a word for "music", but they do have words for instrumental versus improvised forms ("kantun"), European and non-Mapuche music ("kantun winka"), ceremonial songs ("öl"), and "tayil".

While some languages in West Africa have no term for music, some West African languages accept the general concepts of music.() "Musiqi" is the Persian word for the science and art of music, "muzik" being the sound and performance of music,() though some things European-influenced listeners would include, such as Quran chanting, are excluded.

Ben Watson points out that Ludwig van Beethoven's "Große Fuge" (1825) "sounded like noise" to his audience at the time. Indeed, Beethoven's publishers persuaded him to remove it from its original setting as the last movement of a string quartet. He did so, replacing it with a sparkling "Allegro". They subsequently published it separately. Musicologist Jean-Jacques Nattiez considers the difference between noise and music nebulous, explaining that "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus ... By all accounts there is no "single" and "intercultural" universal concept defining what music might be".

An often-cited definition of music is that it is "organized sound", a term originally coined by modernist composer Edgard Varèse in reference to his own musical aesthetic. Varèse's concept of music as "organized sound" fits into his vision of "sound as living matter" and of "musical space as open rather than bounded". He conceived the elements of his music in terms of "sound-masses", likening their organization to the natural phenomenon of crystallization. Varèse thought that "to stubbornly conditioned ears, anything new in music has always been called noise", and he posed the question, "what is music but organized noises?"

The fifteenth edition of the "Encyclopædia Britannica" states that "while there are no sounds that can be described as inherently unmusical, musicians in each culture have tended to restrict the range of sounds they will admit." A human organizing element is often felt to be implicit in music (sounds produced by non-human agents, such as waterfalls or birds, are often described as "musical", but perhaps less often as "music"). The composer R. Murray states that the sound of classical music "has decays; it is granular; it has attacks; it fluctuates, swollen with impurities—and all this creates a musicality that comes before any 'cultural' musicality." However, in the view of semiologist Jean-Jacques Nattiez, "just as music is whatever people choose to recognize as such, noise is whatever is recognized as disturbing, unpleasant, or both". (See "music as social construct" below.)

Levi R. Bryant defines music not as a language, but as a marked-based, problem-solving method, comparable to mathematics.

Most definitions of music include a reference to sound and a list of universals of music can be generated by stating the elements (or aspects) of sound: pitch, timbre, loudness, duration, spatial location and texture.). However, in terms more specifically relating to music: following Wittgenstein, cognitive psychologist Eleanor Rosch proposes that categories are not clean cut but that something may be more or less a member of a category. As such the search for musical universals would fail and would not provide one with a valid definition. This is primarily because other cultures have different understandings in relation to the sounds that English-language writers refer to as music.

Many people do, however, share a general idea of music. The Websters definition of music is a typical example: "the science or art of ordering tones or sounds in succession, in combination, and in temporal relationships to produce a composition having unity and continuity" ("Webster's Collegiate Dictionary", online edition).

This approach to the definition focuses not on the "construction" but on the "experience" of music. An extreme statement of the position has been articulated by the Italian composer Luciano Berio: "Music is everything that one listens to with the intention of listening to music". This approach permits the boundary between music and noise to change over time as the conventions of musical interpretation evolve within a culture, to be different in different cultures at any given moment, and to vary from person to person according to their experience and proclivities. It is further consistent with the subjective reality that even what would commonly be considered music is experienced as non-music if the mind is concentrating on other matters and thus not perceiving the sound's "essence" "as music".

In his 1983 book, "Music as Heard", which sets out from the phenomenological position of Husserl, Merleau-Ponty, and Ricœur, Thomas Clifton defines music as "an ordered arrangement of sounds and silences whose meaning is presentative rather than denotative ... This definition distinguishes music, as an end in itself, from compositional technique, and from sounds as purely physical objects." More precisely, "music is the actualization of the possibility of any sound whatever to present to some human being a meaning which he experiences with his body—that is to say, with his mind, his feelings, his senses, his will, and his metabolism". It is therefore "a certain reciprocal relation established between a person, his behavior, and a sounding object".

Clifton accordingly differentiates music from non-music on the basis of the human behavior involved, rather than on either the nature of compositional technique or of sounds as purely physical objects. Consequently, the distinction becomes a question of what is meant by musical behavior: "a musically behaving person is one whose very being is absorbed in the significance of the sounds being experienced." However, "It is not altogether accurate to say that this person is listening "to" the sounds. First, the person is doing more than listening: he is perceiving, interpreting, judging, and feeling. Second, the preposition 'to' puts too much stress on the sounds as such. Thus, the musically behaving person experiences musical significance by means of, or through, the sounds".

In this framework, Clifton finds that there are two things that separate music from non-music: (1) musical meaning is presentative, and (2) music and non-music are distinguished in the idea of personal involvement. "It is the notion of personal involvement which lends significance to the word "ordered" in this definition of music". This is not to be understood, however, as a sanctification of extreme relativism, since "it is precisely the 'subjective' aspect of experience which lured many writers earlier in this century down the path of sheer opinion-mongering. Later on this trend was reversed by a renewed interest in 'objective,' scientific, or otherwise non-introspective musical analysis. But we have good reason to believe that a musical experience is not a purely private thing, like seeing pink elephants, and that reporting about such an experience need not be subjective in the sense of it being a mere matter of opinion".

Clifton's task, then, is to describe musical experience and the objects of this experience which, together, are called "phenomena", and the activity of describing phenomena is called "phenomenology". It is important to stress that this definition of music says nothing about aesthetic standards.
Music is not a fact or a thing in the world, but a meaning constituted by human beings. ... To talk about such experience in a meaningful way demands several things. First, we have to be willing to let the composition speak to us, to let it reveal its own order and significance. ... Second, we have to be willing to question our assumptions about the nature and role of musical materials. ... Last, and perhaps most important, we have to be ready to admit that describing a meaningful experience is itself meaningful.

"Music, often an art/entertainment, is a total social fact whose definitions vary according to era and culture", according to Jean. It is often contrasted with noise. According to musicologist Jean-Jacques Nattiez: "The border between music and noise is always culturally defined—which implies that, even within a single society, this border does not always pass through the same place; in short, there is rarely a consensus ... By all accounts there is no "single" and "intercultural" universal concept defining what music might be". Given the above demonstration that "there is no limit to the number or the genre of variables that might intervene in a definition of the musical", an organization of definitions and elements is necessary.

Nattiez (1990, 17) describes definitions according to a tripartite semiological scheme similar to the following:
There are three levels of description, the poietic, the neutral, and the esthesic:

Table describing types of definitions of music:
Because of this range of definitions, the study of music comes in a wide variety of forms. There is the study of sound and vibration or acoustics, the cognitive study of music, the study of music theory and performance practice or music theory and ethnomusicology and the study of the reception and history of music, generally called musicology.

Composer Iannis Xenakis in "Towards a Metamusic" (chapter 7 of "Formalized Music") defined music in the following way:


Sources


Dayton, Ohio

Dayton () is a city in and the county seat of Montgomery County, Ohio, United States. A small part of the city extends into Greene County. As of the 2020 census, the city proper had a population of 137,644, making it the sixth-most populous city in Ohio. Dayton anchors the state's fourth-largest metropolitan area, the Dayton metropolitan area, which had 814,049 residents. Dayton is located within Ohio's Miami Valley region, north of Cincinnati and west of Columbus. It is a principal city of the Dayton–Springfield–Sidney combined statistical area, home to a population of 1,086,512.

Dayton was founded in 1796 along the Great Miami River and named after Jonathan Dayton, a Founding Father who owned a significant amount of land in the area. It grew in the 19th century as a canal town and was home to many patents and inventors, most notably the Wright brothers, who developed the first successful motor-operated airplane. It later developed an industrialized economy and was home to the Dayton Project, a branch of the larger Manhattan Project, to develop polonium triggers used in early atomic bombs. With the decline of heavy manufacturing in the late 20th century, Dayton's businesses have diversified into a service economy.

Ohio's borders are within of roughly 60 percent of the country's population and manufacturing infrastructure, making Dayton a logistics hub. The city is home to Wright-Patterson Air Force Base, a significant contributor to research and development in the industrial, aeronautical, and astronautical engineering fields. Along with defense and aerospace, healthcare accounts for much of the Dayton area's economy. Significant institutions in Dayton include the Air Force Institute of Technology, Carillon Historical Park, Dayton Art Institute, Dayton Performing Arts Alliance, National Museum of the United States Air Force, and University of Dayton.

Dayton was founded on April 1, 1796, by 12 settlers known as the Thompson Party. They traveled in March from Cincinnati up the Great Miami River by pirogue and landed at what is now St. Clair Street, where they found two small camps of Native Americans. Among the Thompson Party was Benjamin Van Cleve, whose memoirs provide insights into the Ohio Valley's history. Two other groups traveling overland arrived several days later. The oldest surviving building is Newcom Tavern, which was used for various purposes, including housing Dayton's first church, which is still in existence.

In 1797, Daniel C. Cooper laid out Mad River Road, the first overland connection between Cincinnati and Dayton, opening the "Mad River Country" to settlement. Ohio was admitted into the Union in 1803, and the village of Dayton was incorporated in 1805 and chartered as a city in 1841. The city was named after Jonathan Dayton, a captain in the American Revolutionary War who signed the U.S. Constitution and owned a significant amount of land in the area. In 1827, construction on the Dayton–Cincinnati canal began, which provided a better way to transport goods from Dayton to Cincinnati and contributed significantly to Dayton's economic growth during the 1800s.

Innovation led to business growth in the region. In 1884, John Henry Patterson acquired James Ritty's National Manufacturing Company along with his cash register patents and formed the National Cash Register Company (NCR). The company manufactured the first mechanical cash registers and played a crucial role in the shaping of Dayton's reputation as an epicenter for manufacturing in the early 1900s. In 1906, Charles F. Kettering, a leading engineer at the company, helped develop the first electric cash register, which propelled NCR into the national spotlight. NCR also helped develop the US Navy Bombe, a code-breaking machine that helped crack the Enigma machine cipher during World War II.

Dayton has been the home for many patents and inventions since the 1870s. According to the National Park Service, citing information from the U.S. Patent Office, Dayton had granted more patents per capita than any other U.S. city in 1890 and ranked fifth in the nation as early as 1870. The Wright brothers, inventors of the airplane, and Charles F. Kettering, world-renowned for his numerous inventions, hailed from Dayton. The city was also home to James Ritty's Incorruptible Cashier, the first mechanical cash register, and Arthur E. Morgan's hydraulic jump, a flood prevention mechanism that helped pioneer hydraulic engineering. Paul Laurence Dunbar, an African-American poet and novelist, penned his most famous works in the late 19th century and became an integral part of the city's history.

Powered aviation began in Dayton. Orville and Wilbur Wright were the first to construct and demonstrate powered flight. Although the first flight was in Kitty Hawk, North Carolina, their Wright Flyer was built in and returned to Dayton for improvements and further flights at Huffman Field, a cow pasture northeast of Dayton, near the current Wright-Patterson Air Force Base.

When the government tried to move development to Langley field in southern Virginia, six Dayton businessmen including Edward A. Deeds, formed the Dayton-Wright Airplane Company in Moraine and established a flying field. Deeds also opened a field to the north in the flood plain of the Great Miami River between the confluences of that river, the Stillwater River, and the Mad River, near downtown Dayton. Later named McCook Field for Alexander McDowell McCook, an American Civil War general, this became the Army Signal Corps' primary aviation research and training location. Wilbur Wright also purchased land near Huffman prairie to continue their research.

During World War I, the Army purchased 40 acres adjacent to Huffman Prairie for the Fairfield Aviation General Supply Depot. As airplanes developed more capability, they needed more runway space than McCook could offer, and a new location was sought. The Patterson family formed the Dayton Air Service Committee, Inc which held a campaign that raised $425,000 in two days and purchased northeast of Dayton, including Wilbur Wright Field and the Huffman Prairie Flying Field. Wright Field was "formally dedicated" on October 12, 1927. After World War II, Wright Field and the adjacent Patterson Field, Dayton Army Air Field, and Clinton Army Air Field were merged as the Headquarters, Air Force Technical Base. On January 13, 1948, the facility was renamed Wright-Patterson Air Force Base.

A catastrophic flood in March 1913, known as the Great Dayton Flood, led to the creation of the Miami Conservancy District, a series of dams as well as hydraulic pumps installed around Dayton, in 1914.

Like other cities across the country, Dayton was heavily involved in the war effort during World War II. Several locations around the city hosted the Dayton Project, a branch of the larger Manhattan Project, to develop polonium triggers used in early atomic bombs. The war efforts led to a manufacturing boom throughout the city, including high demand for housing and other services. At one point, emergency housing was put into place due to a housing shortage in the region, much of which is still in use today.

Alan Turing is widely considered to be the father of theoretical computer science and artificial intelligence. He visited the National Cash Register (NCR) company in Dayton in December 1942. He was able to show that it was not necessary to build 336 Bombes, so the initial order was scaled down to 96 machines to decipher German Enigma-machine-encrypted secret messages during World War II.

Between the 1940s and the 1970s, the city saw significant growth in suburban areas from population migration. Veterans were returning from military service in large numbers seeking industrial and manufacturing jobs, a part of the local industry that was expanding rapidly. Advancements in architecture also contributed to the suburban boom. New, modernized shopping centers and the Interstate Highway System allowed workers to commute greater distances and families to live further from the downtown area. More than 127,000 homes were built in Montgomery County during the 1950s.

During this time, the city was the site of several race riots, including one in 1955 following the murder of Emmett Till, the 1966 Dayton race riot, two in 1967 (following a speech by civil rights activist H. Rap Brown and another following the police killing of an African American man), and one in 1968 as part of the nationwide King assassination riots.

Since the 1980s, however, Dayton's population has declined, mainly due to the loss of manufacturing jobs and decentralization of metropolitan areas, as well as the national housing crisis that began in 2008. While much of the state has suffered for similar reasons, the impact on Dayton has been greater than most. Dayton had the third-greatest percentage loss of population in the state since the 1980s, behind Cleveland and Youngstown. Despite this, Dayton has begun diversifying its workforce from manufacturing into other growing sectors such as healthcare and education.

In 1995, the Dayton Agreement, a peace accord between the parties to the hostilities of the conflict in Bosnia-Herzegovina and the former Yugoslavia, was negotiated at Wright-Patterson Air Force Base, near Fairborn, Ohio, from November 1 to 21.

Richard Holbrooke wrote about these events in his memoirs:

Downtown expansion that began in the 2000s has helped revitalize the city and encourage growth. Day Air Ballpark, home of the Dayton Dragons, was built in 2000. The highly successful minor league baseball team has been an integral part of Dayton's culture. In 2001, the city's public park system, Five Rivers MetroParks, built RiverScape MetroPark, an outdoor entertainment venue that attracts more than 400,000 visitors each year. A new performance arts theater, the Schuster Center, opened in 2003. A large health network in the region, Premier Health Partners, expanded its Miami Valley Hospital with a 12-story tower addition.

In 2010, the Downtown Dayton Partnership, in cooperation with the City of Dayton and community leaders, introduced the Greater Downtown Dayton Plan. It focuses on job creation and retention, infrastructure improvements, housing, recreation, and collaboration. The plan is to be implemented through the year 2020.

Dayton is known as the "Gem City". The nickname's origin is uncertain, but several theories exist. In the early 19th century, a well-known racehorse named Gem hailed from Dayton. In 1845, an article published in the "Cincinnati Daily Chronicle" by an author known as T stated:

In the late 1840s, Major William D. Bickham of the "Dayton Journal" began a campaign to nickname Dayton the "Gem City." The name was adopted by the city's Board of Trade several years later. Paul Laurence Dunbar referred to the nickname in his poem, "Toast to Dayton", as noted in the following excerpt:

<poem>
She shall ever claim our duty,
For she shines—the brightest gem
That has ever decked with beauty
</poem>

Dayton also plays a role in a nickname given to the state of Ohio, "Birthplace of Aviation." Dayton is the hometown of the Wright brothers, aviation pioneers who are credited with inventing and building the first practical airplane in history. After their first manned flights in Kitty Hawk, North Carolina, which they had chosen due to its ideal weather and climate conditions, the Wrights returned to Dayton and continued testing at nearby Huffman Prairie.

Additionally, Dayton is colloquially referred to as "Little Detroit". This nickname comes from Dayton's prominence as a Midwestern manufacturing center.

According to the United States Census Bureau, the city has a total area of , of which is land and is water.

Dayton's climate features warm, muggy summers and cold, dry winters, and is classified as a humid continental climate (Köppen "Dfa"). Unless otherwise noted, all normal figures quoted within the text below are from the official climatology station, Dayton International Airport, at an elevation of about to the north of downtown Dayton, which lies within the valley of the Miami River; thus temperatures there are typically cooler than in downtown.

At the airport, monthly mean temperatures range from in January to in July. The highest temperature ever recorded in Dayton was on July 22, 1901, and the coldest was on February 13 during the Great Blizzard of 1899. On average, there are 14 days of + highs and 4.5 nights of sub- lows annually. Snow is moderate, with a normal seasonal accumulation of , usually occurring from November to March, occasionally April, and rarely October. Precipitation averages annually, with total rainfall peaking in May.

Dayton is subject to severe weather typical of the Midwestern United States. Tornadoes are possible from the spring to the fall. Floods, blizzards, and severe thunderstorms can also occur.

On Memorial Day of 2019, Dayton suffered extensive property damage and one death during a tornado outbreak, in which a total of 15 tornadoes touched down in the Dayton area. Although some of the tornadoes were only EF0 and remained on the ground for less than a mile, one was an EF4 measuring a half-mile-wide (805 meters), which tore through the communities of Brookville, Trotwood, Dayton, Northridge, and Riverside. Several streets were closed, including portions of I-75 and North Dixie Drive in Northridge. 64,000 residents lost power and much of the region's water supply was cut off.

The Dayton Audubon Society is the National Audubon Society's local chapter. The Dayton chapter manages local activities contributing to the annual, hemisphere-wide Christmas Bird Count. The Chapter began participation in the National Count in 1924. The local Count was initially coordinated by Ben Blincoe, who was succeeded by Jim Hill in 1970. In the mid-1960s, the freezing of Lake Erie and associated marshlands led species of waterfowl to appear in the Dayton-area, where surface waters remained unfrozen. Nine varieties of birds have been observed every year in the Dayton area: downy woodpecker, Carolina chickadee, tufted titmouse, brown creeper, cardinal, junco, tree sparrow, song sparrow and crow.

Unlike many Midwestern cities its age, Dayton has very broad and straight downtown streets (generally two or three full lanes in each direction) that improved access to the downtown even after the automobile became popular. The main reason for the broad streets was that Dayton was a marketing and shipping center from its beginning; streets were broad to enable wagons drawn by teams of three to four pairs of oxen to turn around. Also, some of today's streets were once barge canals flanked by draw-paths.
A courthouse building was built in downtown Dayton in 1888 to supplement Dayton's original Neoclassical courthouse, which still stands. This second, "new" courthouse has since been replaced with new facilities as well as a park. The Old Court House has been a favored political campaign stop. On September 17, 1859, Abraham Lincoln delivered an address on its steps. Eight other presidents have visited the courthouse, either as presidents or during presidential campaigns: Andrew Johnson, James Garfield, John F. Kennedy, Lyndon B. Johnson, Richard Nixon, Gerald Ford, Ronald Reagan, and Bill Clinton.

The Dayton Arcade, which opened on March 3, 1904, was built in the hopes of replacing open-air markets throughout the city. Throughout the decades, the Arcade has gone through many transformations but has retained its charm. Some of its main features include a Flemish facade at the Third Street entrance, a glass dome above the Arcade rotunda, and a chateau roof line above the Third Street facade. The Dayton Arcade is currently under renovations with no official completion date set.

In 2009, the CareSource Management Group finished construction of a $55 million corporate headquarters in downtown Dayton. The , 10-story building was downtown's first new office tower in more than a decade.

Dayton's two tallest buildings are the Kettering Tower at and the KeyBank Tower at . Kettering Tower was originally Winters Tower, the headquarters of Winters Bank. The building was renamed after Virginia Kettering when Winters was merged into Bank One. KeyBank Tower was known as the MeadWestvaco Tower before KeyBank gained naming rights to the building in 2008.

Ted Rall said in 2015 that over the last five decades Dayton has been demolishing some of its architecturally significant buildings to reduce the city's rental vacancy rate and thus increase the occupancy rate.

Dayton's ten historic neighborhoods—Oregon District, Wright Dunbar, Dayton View, Grafton Hill, McPherson Town, Webster Station, Huffman, Kenilworth, St. Anne's Hill, and South Park—feature mostly single-family houses and mansions in the Neoclassical, Jacobethan, Tudor Revival, English Gothic, Chateauesque, Craftsman, Queen Anne, Georgian Revival, Colonial Revival, Renaissance Revival Architecture, Shingle Style Architecture, Prairie, Mission Revival, Eastlake/Italianate, American Foursquare, and Federal styles. Downtown Dayton is also a large area that encompasses several neighborhoods itself and has seen a recent uplift and revival.

Dayton's suburbs with a population of 10,000 or more include Beavercreek, Centerville, Clayton, Englewood, Fairborn, Harrison Township, Huber Heights, Kettering, Miami Township, Miamisburg, Oakwood, Riverside, Springboro, Trotwood, Vandalia, Washington Township, West Carrollton, and Xenia.

In the federal government's National Urban Policy and New Community Development Act of 1970, funding was provided for thirteen "new towns" or planned cities throughout the country. One location was set to become a suburb of Dayton and was known variously as Brookwood or Newfields. The goal was to have an entirely new suburb that would eventually house about 35,000 residents. The new town was to be located between Trotwood and Brookville, and modeled on the ideas of Ian McHarg. The project was abandoned in 1978 and most of the land became Sycamore State Park.

Dayton's city proper population declined significantly from a peak of 262,332 residents in 1960 to 137,644 residents in 2020. This was in part due to the slowdown of the region's manufacturing sector. The metropolitan area as a whole has experienced both population growth and decreases since 1960, with the overall trend leaning towards growth for the metro area. The city's most populous ethnic group, white, declined from 78.1% in 1960 to 51.7% by 2010.

As of the census of 2020, there were 137,644 people living in the city, for a population density of 2,466.47 people per square mile (952.31/km). There were 68,899 housing units. The racial makeup of the city was 47.6% White, 40.7% Black or African American, 0.4% Native American, 1.4% Asian, 0.1% Pacific Islander, 3.3% from some other race, and 6.6% from two or more races. 5.7% of the population were Hispanic or Latino of any race.

There were 63,308 households, out of which 22.1% had children under the age of 18 living with them, 24.5% were married couples living together, 30.9% had a male householder with no spouse present, and 38.2% had a female householder with no spouse present. 47.4% of all households were made up of individuals, and 14.6% were someone living alone who was 65 years of age or older. The average household size was 1.95, and the average family size was 2.83.

18.9% of the city's population were under the age of 18, 65.0% were 18 to 64, and 16.1% were 65 years of age or older. The median age was 38.4. For every 100 females, there were 101.5 males.

According to the U.S. Census American Community Survey, for the period 2016-2020 the estimated median annual income for a household in the city was $43,780, and the median income for a family was $60,408. About 25.4% of the population were living below the poverty line, including 39.5% of those under age 18 and 21.5% of those age 65 or over. About 53.6% of the population were employed, and 24.4% had a bachelor's degree or higher.

As of the 2010 census, there were 141,759 people, 58,404 households, and 31,064 families residing in the city. The population density was . There were 74,065 housing units at an average density of . The racial makeup of the city was 51.7% White, 42.9% African American, 0.3% Native American, 0.9% Asian, 1.3% from other races, and 2.9% from two or more races. Hispanic or Latino of any race were 3.0% of the population.

There were 58,404 households, of which 28.3% had children under the age of 18 living with them, 25.9% were married couples living together, 21.4% had a female householder with no husband present, 5.9% had a male householder with no wife present, and 46.8% were non-families. 38.8% of all households were made up of individuals, and 11.2% had someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 3.03.

The median age in the city was 34.4 years. 22.9% of residents were under the age of 18; 14.2% were between the ages of 18 and 24; 25.3% were from 25 to 44; 25.8% were from 45 to 64, and 11.8% were 65 years of age or older. The gender makeup of the city was 48.7% male and 51.3% female.

Dayton's crime declined between 2003 and 2008 in key categories according to FBI Uniform Crime Reports and Dayton Police Department data. In 2009, crime continued to fall in the city of Dayton. Crime in the categories of forcible rape, aggravated assault, property crime, motor vehicle theft, robbery, burglary, theft and arson all showed declines for 2009. Overall, crime in Dayton dropped 40% over the previous year. The Dayton Police Department reported a total of 39 murders in 2016, which marked a 39.3% increase in homicides from 2015.

John Dillinger, a bank robber during the early 1930s, was captured and arrested by Dayton city police while visiting his girlfriend at a high-class boarding house in downtown Dayton.

On August 4, 2019, a mass shooting took place in Dayton. Ten people were killed, including the perpetrator; and twenty-seven were injured.

Dayton's economy is relatively diversified and vital to the overall economy of the state of Ohio. In 2008 and 2009, "Site Selection" magazine ranked Dayton the #1 medium-sized metropolitan area in the U.S. for economic development. Dayton is also among the top 100 metropolitan areas in both exports and export-related jobs, ranked 16 and 14 respectively by the Brookings Institution. The 2010 report placed the value of exports at $4.7 billion and the number of export-related jobs at 44,133. The Dayton Metropolitan Statistical Area ranks 4th in Ohio's Gross Domestic Product with a 2008 industry total of $33.78 billion. Additionally, Dayton ranks third among 11 major metropolitan areas in Ohio for exports to foreign countries. The Dayton Development Coalition is attempting to leverage the region's large water capacity, estimated to be 1.5 trillion gallons of renewable water aquifers, to attract new businesses. Moody's Investment Services revised Dayton's bond rating from A1 to the stronger rating of Aa2 as part of its global recalibration process. Standard & Poor's upgraded Dayton's rating from A+ to AA− in the summer of 2009.

"Bloomberg Businessweek" ranked Dayton in 2010 as one of the best places in the U.S. for college graduates looking for a job. Companies such as Reynolds and Reynolds, Stratacache, CareSource, DP&L (soon AES inc), LexisNexis, Kettering Health Network, Premier Health Partners, and Standard Register have their headquarters in Dayton. It is also the former home of the Speedwell Motor Car Company, MeadWestvaco (formerly known as the Mead Paper Company), and NCR. NCR was headquartered in Dayton for over 125 years and was a major innovator in computer technology.

The Dayton region gave birth to aviation and is known for its high concentration of aerospace and aviation technology. In 2009, Governor Ted Strickland designated Dayton as Ohio's aerospace innovation hub, the state's first such technology hub. Two major United States research and development organizations have leveraged Dayton's historical leadership in aviation and maintain their headquarters in the area: The National Air and Space Intelligence Center (NASIC) and the Air Force Research Laboratory (AFRL). Both have their headquarters at Wright-Patterson Air Force Base.

Several research organizations support NASIC, AFRL, and the Dayton community. The Advanced Technical Intelligence Center is a confederation of government, academic, and industry partners. The University of Dayton Research Institute (UDRI) is led by the University of Dayton. The Cognitive Technologies Division (CTD) of Applied Research Associates, Inc., which carries out human-centered research and design, is headquartered in the Dayton suburb of Fairborn. The city of Dayton has started Tech Town, a development project to attract technology-based firms and revitalize the downtown area. Tech Town is home to the world's first RFID business incubator. The University of Dayton–led Institute for Development & Commercialization of Sensor Technologies (IDCAST) at TechTown is a center for remote sensing and sensing technology. It is one of Dayton's technology business incubators housed in The Entrepreneurs Center building.

The Kettering Health Network and Premier Health Partners have a major role on the Dayton area's economy. Hospitals in the Greater Dayton area have an estimated combined employment of nearly 32,000 and a yearly economic impact of $6.8 billion. In addition, several Dayton area hospitals consistently earn top national ranking and recognition including the "U.S. News & World Report"s list of "America's Best Hospitals" as well as many of HealthGrades top ratings. The most notable hospitals are Miami Valley Hospital and Kettering Medical Center.

The Dayton region has several key institutes and centers for health care. The Center for Tissue Regeneration and Engineering at Dayton focuses on the science and development of human tissue regeneration. The National Center for Medical Readiness (NCMR) is also in the Dayton area. The center includes Calamityville, which is a disaster training facility. Over five years, Calamityville is estimated to have a regional economic impact of $374 million. Also, the Neurological Institute at Miami Valley Hospital is an institute focused on the diagnosis, treatment, and research of neurological disorders.

According to the city's 2019 Comprehensive Annual Financial Report, the top employers in the city proper are:

The Dayton Region ranked within the top 10% in the nation in arts and culture. In a 2012 readers' poll by "American Style" magazine, Dayton ranked #2 in the country among mid-size cities as an arts destination, ranking higher than larger cities such as Atlanta, St. Louis, and Cincinnati. Dayton is the home of the Dayton Art Institute.

The Benjamin and Marian Schuster Performing Arts Center in downtown Dayton is a world-class performing arts center and the home venue of the Dayton Philharmonic Orchestra, Dayton Opera, and the Dayton Ballet. In addition to philharmonic and opera performances, the Schuster Center hosts concerts, lectures, and traveling Broadway shows, and is a popular spot for weddings and other events. The historic Victoria Theatre in downtown Dayton hosts concerts, traveling Broadway shows, ballet, a summertime classic film series, and more. The Loft Theatre, also downtown, is the home of the Human Race Theatre Company. The Dayton Playhouse, in West Dayton, is the site of numerous plays and theatrical productions. Between 1957 and 1995, the Kenley Players presented live theater productions in Dayton. In 2013, John Kenley was inducted into the Dayton Theatre Hall of Fame. Dayton is also home to the Winter Guard International world finals, hosting finals for winter guard, indoor percussion, and indoor winds. 

Dayton is the home to several ballet companies including:

Front Street, the largest artists' collective in Dayton, is housed in three industrial buildings on East Second Street.

The Vectren Dayton Air Show is an annual air show that takes place at the Dayton International Airport. The Vectren Dayton Airshow is one of the largest air shows in the United States.

The Dayton area is served by Five Rivers MetroParks, encompassing over 23 facilities for year-round recreation, education, and conservation. In cooperation with the Miami Conservancy District, the MetroParks maintains over of paved, multi-use scenic trails that connect Montgomery County with Greene, Miami, Warren, and Butler counties.

Dayton was home to a thriving funk music scene from the 1970s to the early 1980s, that included bands such as Ohio Players, Roger Troutman & Zapp, Lakeside, Sun, Dayton, Heatwave, and Slave.

Dayton was also the birthplace to several influential indie and punk bands such as The Breeders, Guided by Voices, and Brainiac. 

From 1996 to 1998, Dayton hosted the National Folk Festival. Since then, the annual Cityfolk Festival has continued to bring folk, ethnic, and world music and arts to Dayton. The Five Rivers MetroParks also owns and operates the PNC Second Street Market near downtown Dayton.

The Dayton area hosts several arenas and venues. South of Dayton in Kettering is the Fraze Pavilion, whose notable performances have included the Backstreet Boys, Boston, and Steve Miller Band. South of downtown, on the banks of the Great Miami River, is the University of Dayton Arena, home venue for the University of Dayton Flyers basketball teams and the location of various other events and concerts. It also hosts the Winter Guard International championships, at which hundreds of percussion and color guard ensembles from around the world compete. In addition, the Dayton Amateur Radio Association hosts the annual Dayton Hamvention, North America's largest hamfest, at the Greene County Fairgrounds in nearby Xenia. The Nutter Center, which is just east of Dayton in the suburb of Fairborn, is the home arena for athletics of Wright State University and the former Dayton Bombers hockey team. This venue is used for many concerts, community events, and various national traveling shows and performances.

The Oregon District is a historic residential and commercial district in southeast downtown Dayton. The district is populated with art galleries, specialty shops, pubs, nightclubs, and coffee houses.

The city of Dayton is also host to yearly festivals, such as the Dayton Celtic Festival, the Dayton Blues Festival, Dayton Music Fest, Urban Nights, Women in Jazz, the African American and Cultural Festival, and the Dayton Reggae Fest.

The city's fine dining restaurants include The Pine Club, a nationally known steakhouse.

Dayton is home to a variety of pizza chains that have become woven into local culture, the most notable of which are Cassano's and Marion's Piazza, both of which produce Dayton-style pizza, which has a thin, crisp, salty crust dusted on the bottom with cornmeal and topped with a thin layer of thick unsweetened sauce. Cheese and other topping ingredients are heavily distributed and spread edge-to-edge with no outer rim of crust, and the finished pizza is cut into bite-size squares.

Notable Dayton-based restaurant chains include Hot Head Burritos.

In addition to restaurants, the city is also home to Esther Price Candies, a candy and chocolate company, and Mike-sells, the oldest potato chip company in the United States.

The city began developing a reputation for its number of breweries and craft beer venues by the late 2010s.

Many major religions are represented in Dayton. Christianity is represented in Dayton by dozens of denominations and their respective churches. Notable Dayton churches include the First Lutheran Church, Sacred Heart Church, and Ginghamsburg Church. Dayton's Muslim community is largely represented by the Islamic Society of Greater Dayton (ISGD), a Muslim community that includes a mosque on Josie Street. Dayton is also home to the United Theological Seminary, one of 13 seminaries affiliated with the United Methodist Church. Judaism is represented by Temple Israel. Hinduism is represented by the Hindu Temple of Dayton. Old North Dayton also has a number of Catholic churches built by immigrants from Lithuania, Poland, Hungary, and Germany.

Tourism also accounts for one out of every 14 private sector jobs in the county. Tourism in the Dayton region is led by the National Museum of the United States Air Force at Wright-Patterson Air Force Base, the largest and oldest military aviation museum in the world. The museum draws over 1.3 million visitors per year and is one of the most-visited tourist attractions in Ohio. The museum houses the National Aviation Hall of Fame.

Other museums also play significant roles in the tourism and economy of the Dayton area. The Dayton Art Institute, a museum of fine arts, owns collections containing more than 20,000 objects spanning 5,000 years of art and archaeological history. The Dayton Art Institute was rated one of the top 10 best art museums in the United States for children. The Boonshoft Museum of Discovery is a children's museum of science with numerous exhibits, one of which includes an indoor zoo with nearly 100 different animals.

There are also some notable historical museums in the region. The Dayton Aviation Heritage National Historical Park, operated by the National Park Service, commemorates the lives and achievements of Dayton natives Orville and Wilbur Wright and Paul Laurence Dunbar. The Wright brothers' famous Wright Flyer III aircraft is housed in a museum at Carillon Historical Park. Dayton is also home to America's Packard Museum, which contains many restored historical Packard vehicles. SunWatch Indian Village/Archaeological Park, a partially reconstructed 12th-century prehistoric American Indian village, is on the south end of Dayton; it is organized around a central plaza dominated by wood posts forming an astronomical calendar. The park includes a museum where visitors can learn about the Indian history of the Miami Valley.

Dayton was named National Geographic's outdoor adventure capital of the Midwest in 2019 due in large part to the metropolitan area's revitalized Five Rivers MetroPark, extensive bicycle and jogging trail system, urban green spaces, lakes and camping areas.
In cooperation with the Miami Conservancy District, Five Rivers MetroParks hosts 340 miles of paved trails, the largest network of paved off-street trails in the United States. The regional trail system represents over 35% of the 900 miles in Ohio's off-street trail network. In 2010, the city of Troy was named "bike friendly" by the League of American Bicyclists, which gave the city the organization's bronze designation. The honorable mention made Dayton one of two cities in Ohio to receive the award, the other being Columbus, and one of 15 cities nationwide.

The Dayton area is home to several minor league and semi pro teams, as well as NCAA Division I sports programs.
The Dayton Dragons professional baseball team is a Class A minor league affiliate for the Cincinnati Reds. The Dayton Dragons are the first (and only) team in minor league baseball history to sell out an entire season before it began and was voted as one of the top 10 hottest tickets to get in all of professional sports by Sports Illustrated. The Dayton Dragons 815 consecutive sellouts surpassed the NBA's Portland Trail Blazers for the longest sellout streak across all professional sports in the U.S.
The University of Dayton and Wright State University both host NCAA basketball. The University of Dayton Arena has hosted more games in the NCAA men's basketball tournament over its history than any other venue. UD Arena is also the site of the First Round games of the NCAA Tournament. In 2012, eight teams competed for the final four spots in the NCAA basketball tournament. Wright State University's NCAA men's basketball is the Wright State Raiders and the University of Dayton's NCAA men's basketball team is the Dayton Flyers.

The Dayton Gems were a minor league ice hockey team in the International Hockey League from 1964 to 1977, 1979 to 1980, and most recently 2009 to 2012. The Dayton Bombers were an ECHL ice hockey team from 1991 to 2009. They most recently played the North Division of the ECHL's American Conference. In June 2009, it was announced the Bombers would turn in their membership back to the league.

Despite the folding of the Bombers, hockey remained in Dayton as the Dayton Gems of the International Hockey League were formed in the fall of 2009 at Hara Arena. The Gems folded after the 2011–12 season. Shortly after the Gems folded, it was announced a new team, the Dayton Demonz, would begin play in 2012 in the Federal Hockey League (FHL). The Demonz folded in 2015 and were immediately replaced by the Dayton Demolition, also in the FHL. However, the Demolition would cease operations after only one season when Hara Arena decided to close due to financial difficulties.

Dayton hosted the first American Professional Football Association game (precursor to the NFL). The game was played at Triangle Park between the Dayton Triangles and the Columbus Panhandles on October 3, 1920, and is considered one of the first professional football games ever played. Football teams in the Dayton area include the Dayton Flyers and the Dayton Sharks.

The Dayton region is also known for the many golf courses and clubs that it hosts. The Miami Valley Golf Club, Moraine Country Club, NCR Country Club, and the Pipestone Golf Course are some of the more notable courses. Also, several PGA Championships have been held at area golf courses. The Miami Valley Golf Club hosted the 1957 PGA Championship, the Moraine Country Club hosted the 1945 PGA Championship, and the NCR Country club hosted the 1969 PGA Championship. Additionally, NCR CC hosted the 1986 U.S. Women's Open, the 2005 U.S. Senior Open, the 2013 State Team Championships and most recently the 2022 Senior Women's Open. Other notable courses include the Yankee Trace Golf Club, the Beavercreek Golf Club, Dayton Meadowbrook Country Club, Sycamore Creek Country Club, Heatherwoode Golf Club, Community Golf Course, and Kitty Hawk Golf Course.

The city of Dayton is the home to the Dayton Area Rugby Club which hosts their home games at the Dayton Rugby Grounds. As of 2018, the club fields two men's and one women's side for Rugby Union and several Rugby Sevens sides. The club also hosts the annual Gem City 7's tournament.

The Dayton City Commission is composed of the mayor and four city commissioners. Each city commission member is elected at-large on a non-partisan basis for four-year, overlapping terms. All policy items are decided by the city commission, which is empowered by the City Charter to pass ordinances and resolutions, adopt regulations, and appoint the city manager. The city manager is responsible for budgeting and implementing policies and initiatives. Dayton was the first large American city to adopt the city manager (Henry Matson Waite (engineer)), form of municipal government, in 1913.

Dayton Public Schools operates 34 schools that serve 16,855 students, including:

The city of Dayton has more than 35 private schools within the city, including:

Dayton has 33 charter schools. Three of the top five charter schools named in 2011 are K–8 schools managed by National Heritage Academies. Notable charter schools include:

The Dayton area was ranked tenth for higher education among metropolitan areas in the United States by "Forbes" in 2009. The city is home to two major universities. The University of Dayton is a private, Catholic institution founded in 1850 by the Marianist order. It has the only American Bar Association (ABA)-approved law school in the Dayton area. The University of Dayton is Ohio's largest private university and is also home to the University of Dayton Research Institute, which ranks third in the nation for sponsored materials research, and the Center for Tissue Regeneration and Engineering at Dayton, which focuses on human tissue regeneration.

The public Wright State University became a state university in 1967. Wright State University established the National Center for Medical Readiness, a national training program for disaster preparedness and relief. Wright State's Boonshoft School of Medicine is the Dayton area's only medical school and is a leader in biomedical research.

Dayton is also home to Sinclair Community College, the largest community college at a single location in Ohio and one of the nation's largest community colleges. Sinclair is acclaimed as one of the country's best community colleges. Sinclair was founded as the YMCA college in 1887.

Other schools just outside Dayton that shape the educational landscape are Antioch College and Antioch University, both in Yellow Springs, Central State University in Wilberforce, Kettering College of Medical Arts and School of Advertising Art in Kettering, DeVry University in Beavercreek, Cedarville University, Clark State Community College and Wittenberg University in Springfield. The Air Force Institute of Technology, which was founded in 1919 and serves as a graduate school for the United States Air Force, is at the nearby Wright-Patterson Air Force Base.


Dayton is served in print by "The Dayton Daily News", the city's sole remaining daily newspaper. The "Dayton Daily News" is owned by Cox Enterprises. The Dayton region's main business newspaper is the "Dayton Business Journal". The "Dayton City Paper," a community paper focused on music, art, and independent thought ceased operation in 2018. "The Dayton Weekly News" has been published since 1993, providing news and information to Dayton's African-American community.

There are numerous magazines produced in and for the Dayton region. "The Dayton Magazine" provides insight into arts, food, and events. "Focus on Business" is published by the Chamber of Commerce to provide awareness of companies and initiatives affecting the regional economy

Nielsen Media Research ranked the 11-county Dayton television market as the No. 62 market in the United States. The market is served by stations affiliated with major American networks including: WDTN, channel 2 – NBC, operated by Nexstar Media Group; WHIO-TV, channel 7 – CBS, operated by Cox Media Group; WPTD, channel 16 – PBS, operated by ThinkTV, which also operates WPTO, assigned to Oxford; WKEF, channel 22 – ABC/Fox, operated by Sinclair Broadcasting; WBDT, channel 26 – The CW, operated by Vaughan Media (a shell corporation of Nexstar), assigned to Springfield; WKOI-TV, channel 43 – Ion Television, assigned to Richmond, Indiana; and WRGT-TV, channel 45 – My Network TV, operated under a local marketing agreement by Sinclair Broadcasting. The nationally syndicated morning talk show "The Daily Buzz" originated from WBDT, the former ACME Communications property in Miamisburg, before moving to its current home in Florida.

Dayton is also served by 42 AM and FM radio stations directly, and numerous other stations are heard from elsewhere in southwest Ohio, which serve outlying suburbs and adjoining counties.

The Greater Dayton Regional Transit Authority (RTA) operates public bus routes in the Dayton metro area. In addition to routes covered by traditional diesel-powered buses, RTA has several electric trolley bus routes. The Dayton trolleybus system is the second longest-running of the five remaining trolleybus systems in the U.S., having entered service in 1933. It is the present manifestation of an electric transit service that has operated continuously in Dayton since 1888.

Dayton operates a Greyhound Station which provides inter-city bus transportation to and from Dayton. The hub is in the Greater Dayton Regional Transit Authority North-West hub in Trotwood.

Dayton International Airport lies in a northern exclave of the city and offers service to 21 markets through 10 airlines. In 2008, it served 2.9 million passengers. The Dayton International Airport is also a significant regional air freight hub hosting FedEx Express, UPS Airlines, United States Postal Service, and major commercial freight carriers.

The Dayton area also has several regional airports. The Dayton–Wright Brothers Airport is a general aviation airport owned by the City of Dayton south of the central business district of Dayton on Springboro Pike in Miami Township. It serves as the reliever airport for Dayton International Airport. The airport primarily serves corporate and personal aircraft users. The Dahio Trotwood Airport, also known as Dayton-New Lebanon Airport, is a privately owned, public-use airport west of the central business district of Dayton. The Moraine Airpark is a privately owned, public-use airport southwest of the city of Dayton.

The Dayton region is primarily served by three interstates:

Other major routes for the region include:

From 2010 through 2017, the Ohio Department of Transportation (ODOT) performed a $533 million construction project to modify, reconstruct and widen I-75 through downtown Dayton, from Edwin C Moses Blvd. to Stanley Avenue.

Dayton hosts several inter-modal freight railroad terminals. Two Class I railroads, CSX and Norfolk Southern Railway, operate switching yards in the city.

Formerly the Baltimore and Ohio Railroad, New York Central Railroad and the Pennsylvania Railroad, and afterward, Amtrak made long-distance passenger train stops at Dayton Union Station on S. Sixth Street. The last train leaving there was the "National Limited" in October 1979.

Dayton's sister cities are:




Diode

A diode is a two-terminal electronic component that conducts current primarily in one direction (asymmetric conductance). It has low (ideally zero) resistance in one direction and high (ideally infinite) resistance in the other.

A semiconductor diode, the most commonly used type today, is a crystalline piece of semiconductor material with a p–n junction connected to two electrical terminals. It has an exponential current–voltage characteristic. Semiconductor diodes were the first semiconductor electronic devices. The discovery of asymmetric electrical conduction across the contact between a crystalline mineral and a metal was made by German physicist Ferdinand Braun in 1874. Today, most diodes are made of silicon, but other semiconducting materials such as gallium arsenide and germanium are also used.

The obsolete thermionic diode is a vacuum tube with two electrodes, a heated cathode and a plate, in which electrons can flow in only one direction, from cathode to plate.

Among many uses, diodes are found in rectifiers to convert alternating current (AC) power to direct current (DC), demodulation in radio receivers, and can even be used for logic or as temperature sensors. A common variant of a diode is a light-emitting diode, which is used as electric lighting and status indicators on electronic devices.

The most common function of a diode is to allow an electric current to pass in one direction (called the diode's "forward" direction), while blocking it in the opposite direction (the "reverse" direction). Its hydraulic analogy is a check valve. This unidirectional behavior can convert alternating current (AC) to direct current (DC), a process called rectification. As rectifiers, diodes can be used for such tasks as extracting modulation from radio signals in radio receivers.

A semiconductor diode's exponential current–voltage characteristic results in more complicated behavior than a simple on–off action. Since exponential functions can be viewed as having a "knee" voltage, for simplicity, a diode is commonly said to have a "forward threshold voltage", above which there is significant current and below which there is almost no current. However, this is only an approximation as the forward characteristic is gradual in its current–voltage curve.

Since a diode's forward-direction voltage drop varies only a little with the current, and is more so a function of temperature, this effect can be used as a temperature sensor or as a somewhat imprecise voltage reference.

A diode's high resistance to current flowing in the reverse direction suddenly drops to a low resistance when the reverse voltage across the diode reaches a value called the breakdown voltage. This effect is used to used to regulate voltage (Zener diodes) or to protect circuits from high voltage surges (avalanche diodes).

A semiconductor diode's current–voltage characteristic can be tailored by selecting the semiconductor materials and the doping impurities introduced into the materials during manufacture. These techniques are used to create special-purpose diodes that perform many different functions. For example, to electronically tune radio and TV receivers (varactor diodes), to generate radio-frequency oscillations (tunnel diodes, Gunn diodes, IMPATT diodes), and to produce light (light-emitting diodes). Tunnel, Gunn and IMPATT diodes exhibit negative resistance, which is useful in microwave and switching circuits.

Diodes, both vacuum and semiconductor, can be used as shot-noise generators.

Thermionic (vacuum-tube) diodes and solid-state (semiconductor) diodes were developed separately, at approximately the same time, in the early 1900s, as radio receiver detectors. Until the 1950s, vacuum diodes were used more frequently in radios because the early point-contact semiconductor diodes were less stable. In addition, most receiving sets had vacuum tubes for amplification that could easily have the thermionic diodes included in the tube (for example the 12SQ7 double diode triode), and vacuum-tube rectifiers and gas-filled rectifiers were capable of handling some high-voltage/high-current rectification tasks better than the semiconductor diodes (such as selenium rectifiers) that were available at that time.

In 1873, Frederick Guthrie observed that a grounded, white-hot metal ball brought in close proximity to an electroscope would discharge a positively charged electroscope, but not a negatively charged electroscope. In 1880, Thomas Edison observed unidirectional current between heated and unheated elements in a bulb, later called Edison effect, and was granted a patent on application of the phenomenon for use in a DC voltmeter. About 20 years later, John Ambrose Fleming (scientific adviser to the Marconi Company and former Edison employee) realized that the Edison effect could be used as a radio detector. Fleming patented the first true thermionic diode, the Fleming valve, in Britain on 16 November 1904 (followed by in November 1905). Throughout the vacuum tube era, valve diodes were used in almost all electronics such as radios, televisions, sound systems, and instrumentation. They slowly lost market share beginning in the late 1940s due to selenium rectifier technology and then to semiconductor diodes during the 1960s. Today they are still used in a few high power applications where their ability to withstand transient voltages and their robustness gives them an advantage over semiconductor devices, and in musical instrument and audiophile applications.

In 1874, German scientist Karl Ferdinand Braun discovered the "unilateral conduction" across a contact between a metal and a mineral. Indian scientist Jagadish Chandra Bose was the first to use a crystal for detecting radio waves in 1894. The crystal detector was developed into a practical device for wireless telegraphy by Greenleaf Whittier Pickard, who invented a silicon crystal detector in 1903 and received a patent for it on 20 November 1906. Other experimenters tried a variety of other minerals as detectors. Semiconductor principles were unknown to the developers of these early rectifiers. During the 1930s understanding of physics advanced and in the mid-1930s researchers at Bell Telephone Laboratories recognized the potential of the crystal detector for application in microwave technology. Researchers at Bell Labs, Western Electric, MIT, Purdue and in the UK intensively developed point-contact diodes ("crystal rectifiers" or "crystal diodes") during World War II for application in radar. After World War II, AT&T used these in its microwave towers that criss-crossed the United States, and many radar sets use them even in the 21st century. In 1946, Sylvania began offering the 1N34 crystal diode. During the early 1950s, junction diodes were developed.

In 2022, the first 
superconducting diode effect without an external magnetic field was realized.

At the time of their invention, asymmetrical conduction devices were known as rectifiers. In 1919, the year tetrodes were invented, William Henry Eccles coined the term "diode" from the Greek roots "di" (from "δί"), meaning 'two', and "ode" (from "οδός"), meaning 'path'. The word "diode" however was already in use, as were "triode, tetrode, pentode, hexode", as terms of multiplex telegraphy.

Although all diodes "rectify", ""rectifier"" usually applies to diodes used for power supply, to differentiate them from diodes intended for small signal circuits.

A thermionic diode is a thermionic-valve device consisting of a sealed, evacuated glass or metal envelope containing two electrodes: a cathode and a plate. The cathode is either "indirectly heated" or "directly heated". If indirect heating is employed, a heater is included in the envelope.

In operation, the cathode is heated to red heat, around . A directly heated cathode is made of tungsten wire and is heated by a current passed through it from an external voltage source. An indirectly heated cathode is heated by infrared radiation from a nearby heater that is formed of Nichrome wire and supplied with current provided by an external voltage source.
The operating temperature of the cathode causes it to release electrons into the vacuum, a process called thermionic emission. The cathode is coated with oxides of alkaline earth metals, such as barium and strontium oxides. These have a low work function, meaning that they more readily emit electrons than would the uncoated cathode.

The plate, not being heated, does not emit electrons; but is able to absorb them.

The alternating voltage to be rectified is applied between the cathode and the plate. When the plate voltage is positive with respect to the cathode, the plate electrostatically attracts the electrons from the cathode, so a current of electrons flows through the tube from cathode to plate. When the plate voltage is negative with respect to the cathode, no electrons are emitted by the plate, so no current can pass from the plate to the cathode.

Point-contact diodes were developed starting in the 1930s, out of the early crystal detector technology, and are now generally used in the 3 to 30 gigahertz range. Point-contact diodes use a small diameter metal wire in contact with a semiconductor crystal, and are of either "non-welded" contact type or "welded contact" type. Non-welded contact construction utilizes the Schottky barrier principle. The metal side is the pointed end of a small diameter wire that is in contact with the semiconductor crystal. In the welded contact type, a small P region is formed in the otherwise N-type crystal around the metal point during manufacture by momentarily passing a relatively large current through the device. Point contact diodes generally exhibit lower capacitance, higher forward resistance and greater reverse leakage than junction diodes.

A p–n junction diode is made of a crystal of semiconductor, usually silicon, but germanium and gallium arsenide are also used. Impurities are added to it to create a region on one side that contains negative charge carriers (electrons), called an n-type semiconductor, and a region on the other side that contains positive charge carriers (holes), called a p-type semiconductor. When the n-type and p-type materials are attached together, a momentary flow of electrons occurs from the n to the p side resulting in a third region between the two where no charge carriers are present. This region is called the depletion region because there are no charge carriers (neither electrons nor holes) in it. The diode's terminals are attached to the n-type and p-type regions. The boundary between these two regions, called a p–n junction, is where the action of the diode takes place. When a sufficiently higher electrical potential is applied to the P side (the anode) than to the N side (the cathode), it allows electrons to flow through the depletion region from the N-type side to the P-type side. The junction does not allow the flow of electrons in the opposite direction when the potential is applied in reverse, creating, in a sense, an electrical check valve.

Another type of junction diode, the Schottky diode, is formed from a metal–semiconductor junction rather than a p–n junction, which reduces capacitance and increases switching speed.

A semiconductor diode's behavior in a circuit is given by its current–voltage characteristic. The shape of the curve is determined by the transport of charge carriers through the so-called "depletion layer" or "depletion region" that exists at the p–n junction between differing semiconductors. When a p–n junction is first created, conduction-band (mobile) electrons from the N-doped region diffuse into the P-doped region where there is a large population of holes (vacant places for electrons) with which the electrons "recombine". When a mobile electron recombines with a hole, both hole and electron vanish, leaving behind an immobile positively charged donor (dopant) on the N side and negatively charged acceptor (dopant) on the P side. The region around the p–n junction becomes depleted of charge carriers and thus behaves as an insulator.

However, the width of the depletion region (called the depletion width) cannot grow without limit. For each electron–hole pair recombination made, a positively charged dopant ion is left behind in the N-doped region, and a negatively charged dopant ion is created in the P-doped region. As recombination proceeds and more ions are created, an increasing electric field develops through the depletion zone that acts to slow and then finally stop recombination. At this point, there is a "built-in" potential across the depletion zone.

If an external voltage is placed across the diode with the same polarity as the built-in potential, the depletion zone continues to act as an insulator, preventing any significant electric current flow (unless electron–hole pairs are actively being created in the junction by, for instance, light; see photodiode). This is called the "reverse bias" phenomenon.

However, if the polarity of the external voltage opposes the built-in potential, recombination can once again proceed, resulting in a substantial electric current through the p–n junction (i.e. substantial numbers of electrons and holes recombine at the junction). Thus, if an external voltage greater than and opposite to the built-in voltage is applied, a current will flow and the diode is said to be "turned on" as it has been given an external "forward bias".

At higher currents, the forward voltage drop of the diode increases. A drop of 1 V to 1.5 V is typical at full rated current for power diodes. (See also: )

A diode's current–voltage characteristic can be approximated by four operating regions. From lower to higher bias voltages, these are:


The "Shockley ideal diode equation" or the "diode law" (named after the bipolar junction transistor co-inventor William Bradford Shockley) models the exponential current–voltage (I–V) relationship of diodes in moderate forward or reverse bias. The article Shockley diode equation provides details.

At forward voltages less than the saturation voltage, the voltage versus current characteristic curve of most diodes is not a straight line. The current can be approximated by formula_1 as explained in the Shockley diode equation article.

In detector and mixer applications, the current can be estimated by a Taylor's series. The odd terms can be omitted because they produce frequency components that are outside the pass band of the mixer or detector. Even terms beyond the second derivative usually need not be included because they are small compared to the second order term. The desired current component is approximately proportional to the square of the input voltage, so the response is called "square law" in this region.

Following the end of forwarding conduction in a p–n type diode, a reverse current can flow for a short time. The device does not attain its blocking capability until the mobile charge in the junction is depleted.

The effect can be significant when switching large currents very quickly. A certain amount of "reverse recovery time" (on the order of tens of nanoseconds to a few microseconds) may be required to remove the reverse recovery charge from the diode. During this recovery time, the diode can actually conduct in the reverse direction. This might give rise to a large current in the reverse direction for a short time while the diode is reverse biased. The magnitude of such a reverse current is determined by the operating circuit (i.e., the series resistance) and the diode is said to be in the storage-phase. In certain real-world cases it is important to consider the losses that are incurred by this non-ideal diode effect. However, when the slew rate of the current is not so severe (e.g. Line frequency) the effect can be safely ignored. For most applications, the effect is also negligible for Schottky diodes.

The reverse current ceases abruptly when the stored charge is depleted; this abrupt stop is exploited in step recovery diodes for the generation of extremely short pulses.

Normal (p–n) diodes, which operate as described above, are usually made of doped silicon or germanium. Before the development of silicon power rectifier diodes, cuprous oxide and later selenium was used. Their low efficiency required a much higher forward voltage to be applied (typically 1.4 to 1.7 V per "cell", with multiple cells stacked so as to increase the peak inverse voltage rating for application in high voltage rectifiers), and required a large heat sink (often an extension of the diode's metal substrate), much larger than the later silicon diode of the same current ratings would require. The vast majority of all diodes are the p–n diodes found in CMOS integrated circuits, which include two diodes per pin and many other internal diodes.


The symbol used to represent a particular type of diode in a circuit diagram conveys the general electrical function to the reader. There are alternative symbols for some types of diodes, though the differences are minor. The triangle in the symbols points to the forward direction, i.e. in the direction of conventional current flow.

There are a number of common, standard and manufacturer-driven numbering and coding schemes for diodes; the two most common being the EIA/JEDEC standard and the European Pro Electron standard:

The standardized 1N-series numbering "EIA370" system was introduced in the US by EIA/JEDEC (Joint Electron Device Engineering Council) about 1960. Most diodes have a 1-prefix designation (e.g., 1N4003). Among the most popular in this series were: 1N34A/1N270 (germanium signal), 1N914/1N4148 (silicon signal), 1N400x (silicon 1A power rectifier), and 1N580x (silicon 3A power rectifier).

The JIS semiconductor designation system has all semiconductor diode designations starting with "1S".

The European Pro Electron coding system for active components was introduced in 1966 and comprises two letters followed by the part code. The first letter represents the semiconductor material used for the component (A = germanium and B = silicon) and the second letter represents the general function of the part (for diodes, A = low-power/signal, B = variable capacitance, X = multiplier, Y = rectifier and Z = voltage reference); for example:

Other common numbering/coding systems (generally manufacturer-driven) include:

In optics, an equivalent device for the diode but with laser light would be the optical isolator, also known as an optical diode, that allows light to only pass in one direction. It uses a Faraday rotator as the main component.

The first use for the diode was the demodulation of amplitude modulated (AM) radio broadcasts. The history of this discovery is treated in depth in the crystal detector article. In summary, an AM signal consists of alternating positive and negative peaks of a radio carrier wave, whose amplitude or envelope is proportional to the original audio signal. The diode rectifies the AM radio frequency signal, leaving only the positive peaks of the carrier wave. The audio is then extracted from the rectified carrier wave using a simple filter and fed into an audio amplifier or transducer, which generates sound waves via audio speaker.

In microwave and millimeter wave technology, beginning in the 1930s, researchers improved and miniaturized the crystal detector. Point contact diodes ("crystal diodes") and Schottky diodes are used in radar, microwave and millimeter wave detectors.

Rectifiers are constructed from diodes, where they are used to convert alternating current (AC) electricity into direct current (DC). Automotive alternators are a common example, where the diode, which rectifies the AC into DC, provides better performance than the commutator or earlier, dynamo. Similarly, diodes are also used in "Cockcroft–Walton voltage multipliers" to convert AC into higher DC voltages.

Since most electronic circuits can be damaged when the polarity of their power supply inputs are reversed, a series diode is sometimes used to protect against such situations. This concept is known by multiple naming variations that mean the same thing: reverse voltage protection, reverse polarity protection, and reverse battery protection.

Diodes are frequently used to conduct damaging high voltages away from sensitive electronic devices. They are usually reverse-biased (non-conducting) under normal circumstances. When the voltage rises above the normal range, the diodes become forward-biased (conducting). For example, diodes are used in (stepper motor and H-bridge) motor controller and relay circuits to de-energize coils rapidly without the damaging voltage spikes that would otherwise occur. (A diode used in such an application is called a flyback diode). Many integrated circuits also incorporate diodes on the connection pins to prevent external voltages from damaging their sensitive transistors. Specialized diodes are used to protect from over-voltages at higher power (see Diode types above).

Diode-resistor logic constructs AND and OR logic gates. Functional completeness can be achieved by adding an active device to provide inversion (as done with diode-transistor logic).

In addition to light, mentioned above, semiconductor diodes are sensitive to more energetic radiation. In electronics, cosmic rays and other sources of ionizing radiation cause noise pulses and single and multiple bit errors.
This effect is sometimes exploited by particle detectors to detect radiation. A single particle of radiation, with thousands or millions of electron volt, s of energy, generates many charge carrier pairs, as its energy is deposited in the semiconductor material. If the depletion layer is large enough to catch the whole shower or to stop a heavy particle, a fairly accurate measurement of the particle's energy can be made, simply by measuring the charge conducted and without the complexity of a magnetic spectrometer, etc.
These semiconductor radiation detectors need efficient and uniform charge collection and low leakage current. They are often cooled by liquid nitrogen. For longer-range (about a centimeter) particles, they need a very large depletion depth and large area. For short-range particles, they need any contact or un-depleted semiconductor on at least one surface to be very thin. The back-bias voltages are near breakdown (around a thousand volts per centimeter). Germanium and silicon are common materials. Some of these detectors sense position as well as energy.
They have a finite life, especially when detecting heavy particles, because of radiation damage. Silicon and germanium are quite different in their ability to convert gamma rays to electron showers.

Semiconductor detectors for high-energy particles are used in large numbers. Because of energy loss fluctuations, accurate measurement of the energy deposited is of less use.

A diode can be used as a temperature measuring device, since the forward voltage drop across the diode depends on temperature, as in a silicon bandgap temperature sensor. From the Shockley ideal diode equation given above, it might "appear" that the voltage has a "positive" temperature coefficient (at a constant current), but usually the variation of the reverse saturation current term is more significant than the variation in the thermal voltage term. Most diodes therefore have a "negative" temperature coefficient, typically −2 mV/°C for silicon diodes. The temperature coefficient is approximately constant for temperatures above about 20 kelvin. Some graphs are given for 1N400x series, and CY7 cryogenic temperature sensor.

Diodes will prevent currents in unintended directions. To supply power to an electrical circuit during a power failure, the circuit can draw current from a battery. An uninterruptible power supply may use diodes in this way to ensure that the current is only drawn from the battery when necessary. Likewise, small boats typically have two circuits each with their own battery/batteries: one used for engine starting; one used for domestics. Normally, both are charged from a single alternator, and a heavy-duty split-charge diode is used to prevent the higher-charge battery (typically the engine battery) from discharging through the lower-charge battery when the alternator is not running.

Diodes are also used in electronic musical keyboards. To reduce the amount of wiring needed in electronic musical keyboards, these instruments often use keyboard matrix circuits. The keyboard controller scans the rows and columns to determine which note the player has pressed. The problem with matrix circuits is that, when several notes are pressed at once, the current can flow backward through the circuit and trigger "phantom keys" that cause "ghost" notes to play. To avoid triggering unwanted notes, most keyboard matrix circuits have diodes soldered with the switch under each key of the musical keyboard. The same principle is also used for the switch matrix in solid-state pinball machines.

Diodes can be used to limit the positive or negative excursion of a signal to a prescribed voltage.

A diode clamp circuit can take a periodic alternating current signal that oscillates between positive and negative values, and vertically displace it such that either the positive or the negative peaks occur at a prescribed level. The clamper does not restrict the peak-to-peak excursion of the signal, it moves the whole signal up or down so as to place the peaks at the reference level.

The diode's exponential current–voltage relationship is exploited to evaluate exponentiation and its inverse function the logarithm using analog voltage signals (see ).

Diodes are usually referred to as "D" for diode on PCBs. Sometimes the abbreviation "CR" for "crystal rectifier" is used.





Drexel University

Drexel University is a private research university with its main campus in Philadelphia, Pennsylvania. Drexel's undergraduate school was founded in 1891 by Anthony J. Drexel, a financier and philanthropist. Founded as Drexel Institute of Art, Science and Industry, it was renamed Drexel Institute of Technology in 1936, before assuming its current name in 1970., more than 24,000 students were enrolled in over 70 undergraduate programs and more than 100 master's, doctoral, and professional programs at the university. 

Drexel University was founded in 1891 as the Drexel Institute of Art, Science and Industry by Anthony J. Drexel, a Philadelphia financier and philanthropist. The original mission of the institution was to provide educational opportunities in the "practical arts and sciences" for women and men of all backgrounds.

Drexel can now trace its roots to 1812 as in 2011 Drexel took over the management of what is now named Academy of Natural Sciences of Drexel University which Academy was formed in the winter of 1812. On 25 April 1817, the legislature of Commonwealth of Pennsylvania incorporated the organization under the name "Academy of Natural Sciences of Philadelphia".

In 1936, the institution was renamed as the Drexel Institute of Technology; in 1970, it was renamed again as the Drexel Institute of Technology. It gained university status, and was finally named Drexel University.

Despite changes during its first century, the university has remained a privately controlled, non-sectarian, coeducational center of higher learning committed to practical education and hands-on experience in an occupational setting. The central aspect of Drexel University's focus on career preparation, in the form of its cooperative education program, was introduced in 1919. Participating students alternate periods of classroom-based study with periods of full-time, practical work experience related to one's academic major and career interests.

Between 1995 and 2009, Drexel University underwent a period of significant change to its programs, enrollment, and facilities under the leadership of Dr. Constantine Papadakis, the university's president during that time. Papadakis oversaw Drexel's largest expansion in its history, with a 471 percent increase in its endowment and a 102 percent increase in student enrollment. He oversaw improved performance in collegiate rankings, the implementation of a more selective approach to admissions, and a more rigorous academic program at all levels. In 2007, Drexel was the host of the 2008 Democratic presidential candidate debate in Philadelphia, televised by MSNBC.

In 2002, Drexel University acquired and assumed management of the former MCP Hahnemann University, creating the Drexel University College of Medicine. In 2006, the university established the Thomas R. Kline School of Law, and in 2011 the School of Law achieved full accreditation by the American Bar Association.

Constantine Papadakis died of pneumonia in April 2009 while still employed as the university's president. His successor, John Anderson Fry, was previously the president of Franklin & Marshall College and executive vice president of the University of Pennsylvania. 

Under Fry's leadership, Drexel has continued its expansion, including the July 2011 acquisition of The Academy of Natural Sciences.

The College of Arts and Sciences was formed in 1990 when Drexel merged the two existing College of Sciences and College of Humanities together.

The College of Media Arts and Design "fosters the study, exploration and management of the arts: media, design, the performing and visual". The college offers 18 undergraduate programs and nine graduate programs, in modern art and design fields that range from architecture, graphic design and dance to fashion design and television management. Its wide range of programs has helped the college earn full accreditation from the National Association of Schools of Art and Design, the National Architectural Accrediting Board, and the Council for Interior Design Accreditation.

The Bennett S. LeBow College of Business history dates to the founding in 1891 of the Drexel Institute, that later became Drexel University, and of its Business Department in 1896. Today LeBow offers thirteen undergraduate majors, eight graduate programs, and two doctoral programs; 22 percent of Drexel University's undergraduate students are enrolled in a LeBow College of Business program. 

The LeBow College of Business has been ranked as the 38th best private business school in the nation. Its online MBA program is ranked 14th in the world by the "Financial Times"; the publication also ranks the undergraduate business program at LeBow as 19th in the United States. The part-time MBA program ranks 1st in academic quality in the 2015 edition of "Business Insider's" rankings. Undergraduate and graduate entrepreneurship programs are ranked 19th in the country by the "Princeton Review".

Economics programs at the LeBow College of Business are housed within the School of Economics. In addition to the undergraduate program in economics, the school is home to an M.S. in Economics program as well as a PhD program in economics. Faculty members in the School of Economics have been published in the "American Economic Review", "RAND Journal of Economics", and "Review of Economics and Statistics." The school has been ranked among the best in the world for its extensive research into matters of international trade.

Drexel's College of Engineering is one of its oldest and largest academic colleges and served as the original focus of the career-oriented school upon its founding in 1891. The College of Engineering is home to several notable alumni, including two astronauts; financier Bennett S. LeBow, for whom the university's College of Business is named; and Paul Baran, inventor of the packet-switched network. Today, Drexel University's College of Engineering, which is home to 19 percent of the undergraduate student body, is known for creating the world's first engineering degree in appropriate technology. The college is also one of only 17 U.S. universities to offer a bachelor's degree in architectural engineering, and only one of five private institutions to do so.
The engineering curriculum used by the school was originally called E4 (Enhanced Educational Experience for Engineers) which was established in 1986 and funded in part by the Engineering Directorate of the National Science Foundation. 

In 1988, the program evolved into tDEC (the Drexel Engineering Curriculum) which is composed of two full years of rigorous core engineering courses which encompass the freshman and sophomore years of the engineering student. The College of Engineering hasn't used the tDEC curriculum since approximately 2005.

The College of Computing and Informatics is a recent addition to Drexel University, though its programs have been offered to students for many years. The college was formed by the consolidation of the former College of Information Science & Technology (often called the "iSchool"), the Department of Computer Science, and the Computing and Security Technology program. Undergraduate and graduate programs in computer science, software engineering, data science, information systems, and computer security are offered by the college.

The Drexel University College of Medicine was added to the colleges and schools of the university in 2002, having been formed upon the acquisition of MCP Hahnemann University. In addition to its M.D. program, the College of Medicine offers more than 40 graduate programs in its Graduate School of Biomedical Sciences and Professional Studies.

The Graduate School of Biomedical Sciences and Professional studies offers both Master of Science and Doctor of Philosophy degree programs in fields like biochemistry, biotechnology, clinical research, and forensic science. The school also serves as the center for biomedical research at Drexel University.

Founded in 1961 as the United States' first Biomedical Engineering and Science Institute, the School of Biomedical Engineering, Science and Health Systems focuses on the emerging field of biomedical science at the undergraduate, graduate, and doctoral levels. Primary research areas within the school include bioinformatics, biomechanics, biomaterials, neuroengineering, and cardiovascular engineering.

Formed in 2002 along with the College of Medicine, Drexel's College of Nursing and Health Professions offers more than 25 programs to undergraduate and graduate students in the fields of nursing, nutrition, health sciences, health services, and radiologic technology. The college's research into matters of nutrition and rehabilitation have garnered approximately $2.9 million in external research funding on an annual basis. The physician assistant program at Drexel's College of Nursing and Health Professions is ranked in the top 15 such programs in the United States; its anesthesia programs and physical therapy programs are, respectively, ranked as top-50 programs nationwide.

Established in 1892, the department now known as the College of Professional Studies has focused exclusively on educational programs and pursuits for nontraditional adult learners. Today, the Goodwin College of Professional Studies offers several options designed for adult learners at all stages of career and educational development. Bachelor of Science degree completion programs are offered in part-time evening or weekend formats; graduate programs and doctoral programs are offered at the graduate level, as are self-paced "continuing education" courses and nearly a dozen self-paced certification programs.

The Pennoni Honors College, named for Drexel alumnus and trustee Dr. C.R. "Chuck" Pennoni '63, '66, Hon. '92, and his wife Annette, recognizes and promotes excellence among Drexel students. Students admitted to the Honors College live together and take many of the same classes; the college provides these students with access to unique cultural and social activities and a unique guest speaker series. Students are also involved in the university's Honors Student Advisory Committee and have the opportunity to take part in Drexel's "Alternative Spring Break", an international study tour held each spring.

Upon its founding in 2006, the Thomas R. Kline School of Law, originally known as the Earle Mack School of Law, was the first law school founded in Philadelphia in more than three decades. The School of Law offers L.L.M. and Master of Legal Studies degrees, in addition to the flagship Juris Doctor program, and uniquely offers cooperative education as part of its curriculum across all programs. In 2015, "Bloomberg Business" ranked the Kline School of Law as the second most underrated law school in the United States.

One of the oldest schools within Drexel University, the modern School of Education dates back to the 1891 founding of the school. Originally, the Department of Education offered teacher training to women as one of its original, career-focused degree programs. Today, the School of Education offers a coeducational approach to teacher training at the elementary and secondary levels for undergraduates. Other undergraduate programs include those focused on the intersection between learning and technology, teacher certification for non-education majors, and a minor in education for students with an interest in instruction. Graduate degrees offered by the School of Education include those in administration and leadership, special education, higher education, mathematics education, international education, and educational creativity and innovation. Doctoral degrees are offered in educational leadership and learning technologies. 

The School of Public Health states that its mission is to "provide education, conduct research, and partner with communities and organizations to improve the health of populations". To that end, the school offers both a B.S. and a minor in public health for undergraduate students as well as several options for students pursuing graduate and doctoral degrees in the field. At the graduate level, the Dornsife School offers both a Master of Public Health and an Executive Master of Public Health, as well as an M.S. in biostatistics and an M.S. in epidemiology. Two Doctor of Public Health degrees are also offered, as isa Doctor of Philosophy in epidemiology. The school's graduate and doctoral students are heavily invested in the research activities of the Dornsife School of Public Health, which has helped the school attract annual funding for its four research centers.

The Center for Hospitality and Sport Management was formed in 2013, in an effort to house and consolidate academic programs in hospitality, tourism management, the culinary arts, and sport management. Academic programs combine the unique skills required of the sports and hospitality industries with the principles and curriculum espoused by the management programs within Drexel's LeBow College of Business.

Focusing specifically on the skills required to successfully start and launch a business, The Charles D. Close School of Entrepreneurship is the first and only freestanding school of entrepreneurship in the United States. Undergraduate students take part in a B.A. program in entrepreneurship and innovation, while graduate students a combined Master of Science degree in biomedicine and entrepreneurship. Minors in entrepreneurship are also offered to undergraduate students.

Housed within the Close School is the Baiada Institute for Entrepreneurship. The institute serves as an incubator for Drexel student startups, providing resources and mentorships to students and some post-graduates who are starting their own business while enrolled in one of the Close School's degree programs or academic minors.

Drexel University launched its first Internet-based education program, a master's degree in Library & Information Science, in 1996. In 2001, Drexel created its wholly owned, for-profit online education subsidiary, Drexel e-Learning, Inc., better known as Drexel University Online. It was announced in October 2013 that Drexel University Online would no longer be a for-profit venture, but rather become an internal division within the university to better serve its online student population. Although headquartered in Philadelphia, Drexel announced a new Washington, D.C., location in December 2012 to serve as both an academic and outreach center, catering to the online student population.

Drexel University Online founded the National Distance Learning Week, in conjunction with the United States Distance Learning Association, in 2007. In September 2010, Drexel University Online received the Sloan-C award for institution-wide excellence in online education indicating that it had exceptional programs of "demonstrably high quality" at the regional and national levels and across disciplines. Drexel University Online won the 2008 United States Distance Learning Association's Best Practices Awards for Distance Learning Programming. In 2007, the online education subsidiary had a revenue of $40 million. In March 2013, Drexel Online had more than 7,000 unique students from all 50 states and more than 20 countries pursuing a bachelor's, master's, or certificate. , Drexel University Online offers more than 100 fully accredited master's degrees, bachelor's degrees and certificate programs.

Drexel's longstanding cooperative education, or "co-op" program is one of the largest and oldest in the United States. Drexel has a fully internet-based job database, where students can submit résumés and request interviews with any of the thousands of companies that offer positions. Students also have the option of obtaining a co-op via independent search. A student graduating from Drexel's 5-year degree program typically has a total of 18 months of co-op with up to three different companies. The majority of co-ops are paid, averaging $18,720 per 6-month period, however this figure changes with major. About one third of Drexel graduates are offered full-time positions by their co-op employers right after graduation.

Drexel is classified among ". The university was ranked 51st in the 2018 edition of the "Top 100 Worldwide Universities Granted U.S. Utility Patents" list released by the National Academy of Inventors and the Intellectual Property Owners Association.

In its 2024 rankings, "U.S. News & World Report" ranked Drexel tied for 98th among national universities in the United States, tied for 18th in the "Most Innovative Schools" category, 95th in "Best Value Schools", and tied for 273rd in "Top Performers on Social Mobility. "The Wall Street Journal" ranked Drexel 54th among 400 institutions in the United States. 

In its 2018 rankings, "Times Higher Education World University Rankings" and "The Wall Street Journal" ranked Drexel 74th among national universities and 351st-400th among international universities.

In its 2018 rankings, "Forbes" ranked Drexel 24th among STEM universities. In 2019, it also ranked Drexel 226th among 650 national universities, liberal arts colleges and service academies, 120th among research universities, 154th among private universities, and 96th among universities in the Northeast.

In 2016, "Bloomberg Businessweek" ranked the undergraduate business program 78th in the country. In 2014, Business Insider ranked Drexel's graduate business school 19th in the country for networking.

Drexel University's programs are divided across three Philadelphia-area campuses: the University City Campus, the Center City Campus and the Queen Lane College of Medicine Campus.
The University City Main Campus of Drexel University is located just west of the Schuylkill River in the University City district of Philadelphia. It is Drexel's largest and oldest campus; the campus contains the university's administrative offices and serves as the main academic center for students. The northern, residential portion of the main campus is located in the Powelton Village section of West Philadelphia. The two prominent performing stages at Drexel University are the Mandell Theater and the Main Auditorium. The Main Auditorium dates back to the founding of Drexel and construction of its main hall. It features over 1000 seats, and a pipe organ installed in 1928. The organ was purchased by Saturday Evening Post publisher Cyrus H. K. Curtis after he had donated a similar organ, the Curtis Organ, to nearby University of Pennsylvania and it was suggested that he do the same for Drexel. The 424-seat Mandell Theater was built in 1973 and features a more performance-oriented stage, including a full fly system, modern stage lighting facilities, stadium seating, and accommodations for wheelchairs. It is used for the semiannual spring musical, as well as various plays and many events.

The Queen Lane Campus was purchased by Drexel University as part of its acquisition of MCP Hahnemann University. It is located in the East Falls neighborhood of northwest Philadelphia and is primarily utilized by first- and second-year medical students, and researchers. A free shuttle is available, connecting the Queen Lane Campus to the Center City Hahnemann and University City Main campuses.

The Center City Campus is in the middle of Philadelphia, straddling the Vine Street Expressway between Broad and 15th Streets. Shuttle service is offered between the Center City Campus and both the University City and Queen Lane campuses of the university.

In 2011, The Academy of Natural Sciences entered into an agreement to become a subsidiary of Drexel University. Founded in 1812, the Academy of Natural Sciences is America's oldest natural history museum and is a world leader in biodiversity and environmental research.

On January 5, 2009, Drexel University opened the Center for Graduate Studies in Sacramento, California. Eventually renamed Drexel University Sacramento upon the addition of an undergraduate program in business administration, the campus also offered an Ed.D. program in Educational Leadership and Management and master's degree programs in Business Administration, Finance, Higher Education, Human Resource Development, Public Health, and Interdepartmental Medical Science. On March 5, 2015, Drexel University announced the closure of the Sacramento campus, with an 18-month "phase out" period designed to allow current students to complete their degrees.

The Graduate Student Association "advocates the interests and addresses concerns of graduate students at Drexel; strives to enhance graduate student life at the University in all aspects, from academic to campus security; and provides a formal means of communication between graduate students and the University community".

Drexel has an approximate Jewish population of 5% and has both a Chabad House and a Hillel. Both provide services to Jewish and non-Jewish students at Drexel. Due to an increase in the number of Orthodox Jewish students the Hillel has hot kosher food Monday through Thursday . There is also an eruv which is jointly managed by Jewish students from Drexel and the University of Pennsylvania.

DUTV is Drexel's Philadelphia cable television station. The student operated station is part of the Paul F. Harron Studios at Drexel University. The purpose of DUTV is to provide "the people of Philadelphia with quality educational television, and providing Drexel students the opportunity to gain experience in television management and production". The Programing includes an eclectic variety of shows from a bi-monthly news show, DNews, to old films, talk shows dealing with important current issues and music appreciation shows. Over 75 percent of DUTV's programming is student produced.

"The Triangle" has been the university's newspaper since 1926 and currently publishes on a biweekly basis every Friday of the academic term. 
"The Triangle" has won several Mark of Excellence Awards which honor the best in Student Journalism from the Society of Professional Journalists. First place in Editorial Writing (2000), General Column Writing (2000), Second place in Editorial Writing (2001), and third place in Sports Column Writing (2001). In 2004, it won two National Pacemaker Awards for excellence in college newspapers. In December 2019 "The Triangle" announced the creation of their podcasting division, "Tri-Pod,", which debuted on January 10, 2020. Tri-Pod had two podcasts, "Last Call". and "Mark and Jair Explain Sports". 

The school yearbook was first published in 1911 and named the Lexerd in 1913. Prior to the publishing of a campus-wide yearbook in 1911 "The Hanseatic" and "The Eccentric" were both published in 1896 as class books.

Drexel requires all non-commuting first- and second-year students to live in one of its ten residence halls or in "university approved housing".

Second-year students have the option of living in a residence hall designated for upperclassmen, or "university approved housing". The residence halls for upperclassmen are North and Caneris Halls. North Hall operates under the For Students By Students Residential Experience Engagement Model, developed by the Residential Living Office. There are many apartments that are university approved that second-year students can choose to live in. Three of the largest apartment buildings that fit this description are Chestnut Square, University Crossings, and The Summit, all owned by American Campus Communities. Many other students live in smaller apartment buildings or individual townhouse-style apartments in Powelton Village. A second-year student can choose one of the already listed university approved housing options or petition the university to add a new property to the approved list. 

Drexel University recognizes over 250 student organizations in the following categories:

Approximately 12 percent of Drexel's undergraduate population are members of a social Greek-letter organization. There are currently fourteen Interfraternity Council (IFC) chapters, seven Panhellenic Council (PHC) chapters and thirteen Multi-cultural Greek Council (MGC) chapters.

Drexel's school mascot is a dragon known as "Mario the Magnificent", named in honor of alumnus and Board of Trustees member Mario V. Mascioli. The Dragon has been the mascot of the school since around the mid-1920s; the first written reference to the Dragons occurred in 1928, when the football team was called "The Dragons in The Triangle". Before becoming known as the Dragons, the athletic teams had been known by such names as the Blue & Gold, the Engineers, and the Drexelites. The school's sports teams, now known as the Drexel Dragons, participate in the NCAA's Division I as a member of the Coastal Athletic Association. They do not currently field a varsity football team.

In addition to its NCAA Division I teams, Drexel University is home to 33 active club teams including men's ice hockey, lacrosse, water polo, squash, triathlon, and cycling. Other club teams include soccer, baseball, rugby, field hockey, and roller hockey. The club teams operate under the direction of the Club Sports Council and the Recreational Sports Office.

Since its founding the university has graduated over 100,000 alumni. Certificate-earning alumni such as artist Violet Oakley and illustrator Frank Schoonover reflect the early emphasis on art as part of the university's curriculum. With World War II, the university's technical programs swelled, and as a result Drexel graduated alumni such as Paul Baran, one of the founding fathers of the Internet and one of the inventors of the packet switching network, and Norman Joseph Woodland, the inventor of barcode technology. In addition to its emphasis on technology Drexel has graduated several notable athletes such as National Basketball Association (NBA) basketball players Michael Anderson, Damion Lee, and Malik Rose, and several notable business people such as Raj Gupta, former president and Chief executive officer (CEO) of Rohm and Haas, and Kenneth C. Dahlberg, former CEO of Science Applications International Corporation (SAIC). Alassane Dramane Ouattara President of the Republic of Ivory Coast. In 2018, Tirthak Saha -a 2016 graduate of the ECE school - was named to the Forbes 30 Under 30 list for achievements in the Energy field.

In 1991, the university's centennial anniversary, Drexel created an association called the Drexel 100, for alumni who have demonstrated excellence work, philanthropy, or public service. After the creation of the association 100 alumni were inducted in 1992 and since then the induction process has been on a biennial basis. In 2006 164 total alumni had been inducted into the association.

Drexel University created the annual $100,000 Anthony J. Drexel Exceptional Achievement Award to recognize a faculty member from a U.S. institution whose work transforms both research and the society it serves. The first recipient was bioengineer James J. Collins of Boston University (now at MIT) and the Howard Hughes Medical Institute.

In 2004, in conjunction with BAYADA Home Health Care, Drexel University's College of Nursing and Health Professions created the BAYADA Award for Technological Innovation in Nursing Education and Practice. The award honors nursing educators and practicing nurses whose innovation leads to improved patient care or improved nursing education.


Daedalus

In Greek mythology, Daedalus (, ; Greek: Δαίδαλος; Latin: "Daedalus"; Etruscan: "Taitale") was a skillful architect and craftsman, seen as a symbol of wisdom, knowledge and power. He is the father of Icarus, the uncle of Perdix, and possibly also the father of Iapyx. Among his most famous creations are the wooden cow for Pasiphaë, the Labyrinth for King Minos of Crete which imprisoned the Minotaur, and wings that he and his son Icarus used to attempt to escape Crete. It was during this escape that Icarus did not heed his father's warnings and flew too close to the sun; the wax holding his wings together melted and Icarus fell to his death.

The name "Daidalos" seems to be attested in Linear B, a writing system used to record Mycenaean Greek. The name appears in the form "da-da-re-jo-de", possibly referring to a sanctuary.

Daedalus's parentage was supplied as a later addition, with various authors attributing different parents to him. His father is claimed to be either Eupalamus, Metion, or Palamaon. Similarly, his mother was either Alcippe, Iphinoe, Phrasmede or Merope, daughter of King Erechtheus. Daedalus had two sons: Icarus and Iapyx, along with a nephew named either Talos, Calos, or Perdix.

The Athenians rewrote the Cretan-born Daedalus as an Athenian himself, the grandson of the ancient king Erechtheus who only fled to Crete after killing his nephew.

A mythical craftsman named Daedalus is first mentioned in roughly 1400 BC on the Knossian Linear B tablets. He is later mentioned by Homer as the creator of a dancing floor for Ariadne, similar to that which Hephaestus placed on the Shield of Achilles. It is clear that this Daedalus was not an original character of Homer's. Rather, Homer was referencing mythology that his audience was already familiar with.

Daedalus is not mentioned again in literature until the fifth century BC, but he is widely praised as an inventor, artist, and architect, though classical sources disagree on which inventions exactly are attributable to him. In Pliny's Natural History (7.198) he is credited with inventing carpentry, including tools like the axe, saw, glue, and more. Supposedly, he first invented masts and sails for ships for the navy of King Minos. He is also said to have carved statues so spirited they appeared to be living and moving. Pausanias, in traveling around Greece, attributed to Daedalus numerous archaic wooden cult figures (see "xoana") that impressed him. In fact, so many other statues and artworks are attributed to Daedalus by Pausanias and various other sources that likely many of them were never made by him.

Plato cited Daedalus's handiwork as a metaphor for genuine understanding of truth, as opposed to belief that coincidentally happens to be true, in a Socratic dialogue with Meno. Socrates argues that while truth, like one of Daedalus's "moving" statues, is inherently valuable, their animacy would mean they are worthless if the owner cannot shackle them in place to stop them from wandering off.

Daedalus gave his name, eponymously, to many Greek craftsmen and many Greek contraptions and inventions that represented dextrous skill. A specific sort of early Greek sculptures are named Daedalic sculpture in his honor. In Boeotia there was a festival, the Daedala, in which a temporary wooden altar was fashioned and an effigy was made from an oak-tree and dressed in bridal attire. It was carried in a cart with a woman who acted as bridesmaid. The image was called daedala"." Some sources claim that the daedala did not receive their name from Daedalus, but the opposite. Pausanias claims that Daedalus was not the name given to the inventor at birth, but that he was named so later after the daedala.

Some of the functions of Daedalus overlapped with those of Aristaeus (Aristaeos), another famous Greek inventor god. But Aristaeos mostly concerned himself with the rural and agricultural arts.

Daedalus was so proud of his achievements that he could not bear the idea of a rival. His sister had placed her son under his charge to be taught the mechanical arts as an apprentice. His nephew is named variously as Perdix, Talos, or Calos, although some sources say that Perdix was the name of Daedalus' sister. The nephew showed striking evidence of ingenuity. Finding the spine of a fish on the seashore, he took a piece of iron and notched it on the edge, and thus invented the saw. He put two pieces of iron together, connecting them at one end with a rivet, and sharpening the other ends, and made a pair of compasses. Daedalus was so envious of his nephew's accomplishments that he attempted to murder him by throwing him down from the Acropolis in Athens. Athena saved his nephew and turned him into a partridge. Tried and convicted for this murder attempt, Daedalus left Athens and fled to Crete.

Daedalus created the Labyrinth on Crete, in which the Minotaur was kept. 

Poseidon had given a white bull to King Minos to use it as a sacrifice. Instead, the king kept the bull for himself and sacrificed another. As revenge, Poseidon, with the help of Aphrodite, made King Minos's wife, Pasiphaë, lust for the bull. Pasiphaë asked Daedalus to help her. Daedalus built a hollow, wooden cow, covered in real cow hide for Pasiphaë, so she could mate with the bull. As a result, Pasiphaë gave birth to the Minotaur, a creature with the body of a man, but the head and tail of a bull. King Minos ordered the Minotaur to be imprisoned and guarded in the Labyrinth built by Daedalus for that purpose.

In the story of the Labyrinth as told by the Hellenes, the Athenian hero Theseus is challenged to kill the Minotaur, finding his way back out with the help of Ariadne's thread. It is Daedalus himself who gives Ariadne the clue as to how to escape the labyrinth.

Ignoring Homer, later writers envisaged the Labyrinth as an edifice rather than a single dancing path to the center and out again, and gave it numerous winding passages and turns that opened into one another, seeming to have neither beginning nor end. Ovid, in his "Metamorphoses", suggests that Daedalus constructed the Labyrinth so cunningly that he himself could barely escape it after he built it.

The most familiar literary telling explaining Daedalus' wings is a late one by Ovid in his "Metamorphoses".After Theseus and Ariadne eloped together, Daedalus and his son Icarus were imprisoned by King Minos in the labyrinth that he had built. He could not leave Crete by sea, as King Minos kept a strict watch on all vessels, permitting none to sail without being carefully searched. Since Minos controlled the land routes as well, Daedalus set to work to make wings for himself and his son Icarus. Using bird feathers of various sizes, thread, and beeswax, he shaped them to resemble a bird's wings. When both were prepared for flight, Daedalus warned Icarus not to fly too high, because the heat of the sun would melt the beeswax (holding his feathers together) and the wings would break, nor too low, because the sea foam would soak the feathers and make them heavy and he would fall. After Daedalus and Icarus had passed Samos, Delos, and Lebynthos, Icarus disobeyed his father and began to soar upward toward the sun. He flew too close to the sun. Without any warning, the sun melted the wax (which held the feathers together) and they fell off. Icarus kept flapping his "wings". But he realized he had no feathers left. He was only flapping his featherless arms. The feathers—one by one—fell like snowflakes, and down, down, and down he went into the sea (where he sank to the bottom and drowned). Seeing Icarus' wings floating in the sea, Daedalus wept, cursed his art, and (after finding Icarus's dead body on an island shore) buried Icarus's body on the island shore. Then he named the island Icaria in the memory of his child. The southeast end of the Aegean Sea where Icarus fell into the water was also called "Mare Icarium" or the Icarian Sea. In a twist of fate, a partridge, presumably the nephew Daedalus murdered, mocked Daedalus as he buried his son. The fall and death of Icarus is seemingly portrayed as punishment for Daedalus's murder of his nephew.

After burying Icarus, Daedalus traveled to Camicus in Sicily, where he stayed as a guest under the protection of King Cocalus. There Daedalus built a temple to Apollo, and hung up his wings as an offering to the god. In an invention of Virgil (Aeneid VI), Daedalus flies to Cumae and founds his temple there, rather than in Sicily.

Minos, meanwhile, searched for Daedalus by traveling from city to city asking a riddle. He presented a spiral seashell and asked for a string to be run through it. When he reached Camicus, King Cocalus, knowing Daedalus would be able to solve the riddle, accepted the shell and gave it to Daedalus. Daedalus tied the string to an ant which, lured by a drop of honey at one end, walked through the seashell stringing it all the way through. With the riddle solved, Minos realized that Daedalus was in the court of King Cocalus and insisted he be handed over. Cocalus agreed to do so, but convinced Minos to take a bath first. In the bath, Cocalus' daughters killed Minos, possibly by pouring boiling water over his body. In some versions, it is Cocalus that kills Minos in the bath. Other variants say that Daedalus himself poured the boiling water, or that he had built the pipes that could supply hot water to the bath and this was used to instead pour "boiling" water on him.

At least two locations are associated with the death of Daedalus. One version of the story says he retired to the Cretan colony of Telmessos, ruled by Minos's estranged brother Sarpedon, and while wandering outside the city, he was bitten by a snake and died. A town on this site, Daidala, is said to be named after him, and is mentioned in Roman sources. Another version of the story places his death on a small island in the Nile river, where he was later worshipped.

The anecdotes are literary and late. However, in the founding tales of the Greek colony of Gela, founded in the 680s BC on the southwest coast of Sicily, a tradition was preserved that the Greeks had seized cult images wrought by Daedalus from their local predecessors, the Sicani.

Daedalus and the myths associated with him are often depicted in paintings, sculptures, and more by later artists. The myth about his flight and the fall of Icarus is especially popular in depictions. A few noteworthy pieces are included below.

There are also a number of adaptations of the myth of Daedalus and Icarus in modern literature and film, including a poem by Edward Field.



Deception Pass

Deception Pass (; ) is a strait separating Whidbey Island from Fidalgo Island, in the northwest part of the U.S. state of Washington. It connects Skagit Bay, part of Puget Sound, with the Strait of Juan de Fuca. A pair of bridges known collectively as Deception Pass Bridge cross Deception Pass. The bridges were added to the National Register of Historic Places in 1982.

The Deception Pass area has been home to various Coast Salish tribes for thousands of years. The first Europeans to see Deception Pass were members of the 1790 expedition of Manuel Quimper on the "Princesa Real". The Spanish gave it the name "Boca de Flon".

A group of sailors led by Joseph Whidbey, part of the Vancouver Expedition, found and mapped Deception Pass on June 7, 1792. George Vancouver gave it the name "Deception" because it had misled him into thinking Whidbey Island was a peninsula. The "deception" was heightened due to Whidbey's failure to find the strait at first. In May 1792, Vancouver was anchored near the southern end of Whidbey Island. He sent Joseph Whidbey to explore the waters east of Whidbey Island, now known as Saratoga Passage, using small boats. Whidbey reached the northern end of Saratoga Passage and explored eastward into Skagit Bay, which is shallow and difficult to navigate. He returned south to rejoin Vancouver without having found Deception Pass. It appeared that Skagit Bay was a dead-end and that Whidbey Island and Fidalgo Island were a long peninsula attached to the mainland. In June, the expedition sailed north along the west coast of Whidbey Island. Vancouver sent Joseph Whidbey to explore inlets leading to the east. The first inlet turned out to be a "very narrow and intricate channel, which...abounded with rocks above and beneath the surface of the water". This channel led to Skagit Bay, thus separating Whidbey Island from the mainland. Vancouver apparently felt he and Joseph Whidbey had been deceived by the tricky strait. Vancouver wrote of Whidbey's efforts: "This determined [the shore they had been exploring] to be an island, which, in consequence of Mr. Whidbey’s circumnavigation, I distinguished by the name of Whidbey’s Island: and this northern pass, leading into [Skagit Bay], Deception Passage".

In the waters of Deception Pass, just east of the present-day Deception Pass Bridge, is a small island known as Ben Ure Island. The island became infamous for its activity of human smuggling of migrant Chinese people for local labor. Ben Ure and his partner Lawrence "Pirate" Kelly were quite profitable at their human smuggling business and played hide-and-seek with the United States Customs Department for years. Ure's own operation at Deception Pass in the late 1880s consisted of Ure and his Native-American wife. Local tradition has it that his wife would camp on the nearby Strawberry Island (which was visible from the open sea) and signal him with a fire on the island's summit to alert him to whether or not it was safe to attempt to bring the human cargo he illegally transported ashore. For transport, Ure would tie the people up in burlap bags so that if customs agents approached he could toss the bagged people overboard. The tidal currents carried the entrapped drowned migrants' bodies to San Juan Island to the north and west of the pass; many ended up in Dead Man's Bay.

Between 1910 and 1914, a prison rock quarry was operated on the Fidalgo Island side of the pass. Nearby barracks housed some 40 prisoners, members of an honors program out of Walla Walla State Penitentiary and the prison population was made up of several types of prisoners, including those convicted of murder. Guards stood watch at the quarry as prisoners cut the rock into gravel and loaded it onto barges at the base of the cliff atop the pass's waters. The quarried rock was then barged to the Seattle waterfront. The camp was dismantled in 1924 and although abandoned as a quarry, the remains of the camp can still be found. The location is hazardous; over the years there have been several fatal accidents when visitors have ventured onto the steep cliffs.

Upon completion on July 31, 1935, the span Deception Pass Bridge connected Whidbey Island to the tiny Pass Island, and Pass Island to Fidalgo Island. Prior to the bridge, travelers used an inter-island ferry to commute between Fidalgo and Whidbey islands.

Deception Pass is a dramatic seascape where the tidal flow and whirlpools beneath the twin bridges connecting Fidalgo Island to Whidbey Island move quickly. During ebb and flood tide current speed reaches about , flowing in opposite directions between ebb and flood. This swift current can lead to standing waves, large whirlpools, and roiling eddies. This swift current phenomenon can be viewed from the twin bridges' pedestrian walkways or from the trail leading below the larger south bridge from the parking lot on the Whidbey Island side. Boats can be seen waiting on either side of the pass for the current to stop or change direction before going through. Thrill-seeking kayakers go there during large tide changes to surf the standing waves and brave the class 2 and 3 rapid conditions.

Diving Deception Pass is dangerous and only for the most competent and prepared divers. There are a few times each year that the tides are right for a drift dive from the cove, under the bridge, and back to the cove as the tide changes. These must be planned well in advance by divers who know how to read currents and are aware of the dangerous conditions. However, because of the large tidal exchange, Deception Pass hosts some of the most spectacular colors and life in the Pacific Northwest. The walls and bottom are covered in colorful invertebrates, lingcod, greenlings, and barnacles everywhere.

Deception Pass is surrounded by Deception Pass State Park, one of the most visited Washington state parks with over two million annual visitors. 
The park was officially established in 1923, when the original of a military reserve was transferred to Washington State Parks. The park's facilities were greatly enhanced in the 1930s when the Civilian Conservation Corps (CCC) built roads, trails, and buildings in order to develop the park. The road to West Beach was created in 1950, opening up a stretch of beach to hordes of vehicles. The former fish hatchery at Bowman Bay became a part of the park in the early 1970s. The old entrance to the park was closed in 1997 when a new entrance was created at the intersection of Highway 20 and Cornet Bay road, improving access into and out of the park.

The park's recreational facilities include campgrounds, hiking trails, beaches, and tidepools. Several miles of the Pacific Northwest Trail are within the park, most notably including the section that crosses Deception Pass on the Highway 20 bridge. In addition, the Cornet Bay Retreat Center provides cabins and dining and recreation facilities. Cornet Bay offers boat launches and fishing opportunities, while Bowman Bay has an interpretive center that explains the story of the Civilian Conservation Corps throughout Washington state. Near the center is a CCC honor statue, which can be found in 30 different states in the country. Fishing is popular in Pass Lake, on the north side of the bridge. Boat rentals and guided tours of the park are also offered.
Included in the park are ten islands: Northwest Island, Deception Island, Pass Island, Strawberry, Ben Ure, Kiket, Skagit, Hope, and Big and Little Deadman Islands. Ben Ure Island is partially privately owned. The island is not open to the public except for a small rentable cabin available via the state park, which is only accessible by rowboat.

Jonathan Raban's 1999 travel memoir "Passage to Juneau" describes the history of the pass and Raban's passage through it in a 30 ft yacht. The 2002 horror movie "The Ring" was in part filmed near the pass. The bridge is fictionalized as a toll bridge named "Desolation Bridge" in season one of The Killing. Seattle shoegaze act The Sight Below filmed the 2008 video for their track "Further Away" at Deception Pass, with Deception Island's scenic imagery prominently featured. Seattle grunge band Mudhoney named a song on their 1993 EP Five Dollar Bob's Mock Cooter Stew "Deception Pass." Seattle progressive rock band Queensrÿche filmed scenes of their video "Anybody Listening" near Deception Pass and Deception Island.



Dominoes

Dominoes is a family of tile-based games played with gaming pieces. Each domino is a rectangular tile, usually with a line dividing its face into two square "ends". Each end is marked with a number of spots (also called "pips" or "dots") or is blank. The backs of the tiles in a set are indistinguishable, either blank or having some common design. The gaming pieces make up a domino set, sometimes called a "deck" or "pack". The traditional European domino set consists of 28 tiles, also known as pieces, bones, rocks, stones, men, cards or just dominoes, featuring all combinations of spot counts between zero and six. A domino set is a generic gaming device, similar to playing cards or dice, in that a variety of games can be played with a set. Another form of entertainment using domino pieces is the practice of domino toppling.
The earliest mention of dominoes is from Song dynasty China found in the text "Former Events in Wulin" by Zhou Mi (1232–1298). Modern dominoes first appeared in Italy during the 18th century, but they differ from Chinese dominoes in a number of respects, and there is no confirmed link between the two. European dominoes may have developed independently, or Italian missionaries in China may have brought the game to Europe.

The name "domino" is probably derived from the resemblance to a kind of carnival costume worn during the Venetian Carnival, often consisting of a black-hooded robe and a white mask. Despite the coinage of the word "polyomino" as a generalization, there is no connection between the word "domino" and the number 2 in any language.
The most commonly played domino games are Domino Whist, Matador, and Muggins (All Fives). Other popular forms include Texas 42, Chicken Foot, Concentration, Double Fives, and Mexican Train. In Britain, the most popular league and pub game is Fives and Threes.

Dominoes have sometimes been used for divination, such as bone throwing in Chinese culture and in the African diaspora.

European-style dominoes are traditionally made of bone, silver lip ocean pearl oyster shell (mother of pearl), ivory, or a dark hardwood such as ebony, with contrasting black or white pips (inlaid or painted). Some sets feature the top half thickness in MOP, ivory, or bone, with the lower half in ebony. Alternatively, domino sets have been made from many different natural materials: stone (e.g., marble, granite or soapstone); other woods (e.g., ash, oak, redwood, and cedar); metals (e.g., brass or pewter); ceramic clay, or even frosted glass or crystal. These sets have a more novel look, and the often heavier weight makes them feel more substantial; also, such materials and the resulting products are usually much more expensive than polymer materials. 

Modern commercial domino sets are usually made of synthetic materials, such as ABS or polystyrene plastics, or Bakelite and other phenolic resins; many sets approximate the look and feel of ivory while others use colored or even translucent plastics to achieve a more contemporary look. Modern sets also commonly use a different color for the dots of each different end value (one-spots might have black pips while two-spots might be green, three red, etc.) to facilitate finding matching ends. Occasionally, one may find a domino set made of card stock like that for playing cards. Such sets are lightweight, compact, and inexpensive, and like cards are more susceptible to minor disturbances such as a sudden breeze. Sometimes, the tiles have a metal pin (called a spinner or pivot) in the middle.

The traditional domino set contains one unique piece for each possible combination of two ends with zero to six spots, and is known as a double-six set because the highest-value piece has six pips on each end (the "double six"). The spots from one to six are generally arranged as they are on six-sided dice, but because blank ends having no spots are used, seven faces are possible, allowing 28 unique pieces in a double-six set.

However, this is a relatively small number especially when playing with more than four people, so many domino sets are "extended" by introducing ends with greater numbers of spots, which increases the number of unique combinations of ends and thus of pieces. Each progressively larger set increases the maximum number of pips on an end by three; so the common extended sets are double-nine (55 tiles), double-12 (91 tiles), double-15 (136 tiles), and double-18 (190 tiles), which is the maximum in practice. Larger sets such as double-21 (253 tiles) could theoretically exist, but they seem to be extremely rare if not nonexistent, as that would be far more than is normally necessary for most domino games, even with eight players. As the set becomes larger, identifying the number of pips on each domino becomes more difficult, so some large domino sets use more readable Arabic numerals instead of pips.

In China, early "domino" tiles were functionally identical to playing cards. An identifiable version of Chinese dominoes developed in the 12th or 13th century.

The oldest written mention of domino tiles in China dates to the 13th century and comes from Hangzhou where "pupai" (gambling plaques or tiles) and dice are listed as items sold by peddlers during the reign of Emperor Xiaozong of Song (r. 1162–1189). It is not entirely clear that "pupai" means dominoes, but the same term is used two centuries later by the Ming author Lu Rong (1436–1494) in a context that clearly describes domino tiles. The earliest known manual on dominoes is the "Manual of the Xuanhe Period" which purports to be written by Qu You (1341–1427), but some scholars believe it is a later forgery.

The traditional 32-piece Chinese domino set, made to represent each possible face of two thrown dice and thus have no blank faces, differs from the 28-piece domino set found in the West during the mid 18th century, although Chinese dominoes with blank faces were known during the 17th century.
Each domino originally represented one of the 21 results of throwing two six-sided dice (2d6). One half of each domino is set with the pips from one die and the other half contains the pips from the second die. Chinese sets also introduce duplicates of some throws and divide the tiles into two suits: military and civil. Chinese dominoes are also longer than typical European ones.

Modern dominoes first appeared in Italy during the 18th century, but they differ from Chinese dominoes in a number of respects, and there is no confirmed link between the two. European dominoes may have developed independently, or Italian missionaries in China may have brought the game to Europe. Having been established in Italy, the game of dominoes spread rapidly to Austria, southern Germany and France. 
The game became fashionable in France in the mid-18th century. The name "domino" does not appear before that time, being first recorded in 1771, in the "Dictionnaire de Trévoux".
There are two earlier recorded meanings for the French word "domino", one referring to the masquerades of the period, derived from the term for the hooded garment of a priest, the other referred to crude and brightly colored woodcuts on paper formerly popular among French peasants. The way by which this word became the name of the game of domino remains unclear. The earliest game rules in Europe describe a simple block game for two or four players. Later French rules add the variant of "Domino à la Pêche" ("Fishing Domino"), an early draw game as well as a three-hand game with a pool.

From France, the game was introduced to England by the late 1700s, purportedly brought in by French prisoners-of-war. The early forms of the game in England were the "Block Game" and "Draw Game". The rules for these games were reprinted, largely unchanged, for over half a century. In 1863, a new game variously described as "All Fives, Fives" or "Cribbage Dominoes" appeared for the first time in both English and American sources; this was the first scoring game and it borrowed the counting and scoring features of cribbage, but 5 domino spots instead of 15 card points became the basic scoring unit, worth 1 game point. The game was played to 31 and employed a cribbage board to keep score.
In 1864, "The American Hoyle" describes three new variants: Muggins, Bergen and Rounce; alongside the Block Dominoes and Draw Dominoes. In Muggins, the cribbage board was dropped, 5 spots scored 5 points, and game was now 200 for two players and 150 for three or four. Despite the name, there was no 'muggins rule' as in Cribbage to challenge a player who fails to declare his scoring combinations. This omission was rectified in the 1868 edition of "The Modern Pocket Hoyle", but reprints of both rule sets continued to be produced in parallel for around twenty years before the version with the muggins rule prevailed. From around 1871, however, the names of All Fives and Muggins, became conflated and many publications issued rules for 'Muggins or All Fives' or 'Muggins or Fives' without making any distinction between the two. This confusion continues to the present day with some publications equating the names and others describing All Fives as a separate game.

In 1889, dominoes was described as having spread worldwide, "but nowhere is it more popular than in the cafés of France and Belgium. From the outset, the European game was different from the Chinese one. European domino sets contain neither the military-civilian suit distinctions of Chinese dominoes nor the duplicates that went with them. Moreover, according to Michael Dummett, in the Chinese games it is only the identity of the tile that matters; there is no concept of matching. Instead, the basic set of 28 unique tiles contains seven additional pieces, six of them representing the values that result from throwing a single die with the other half of the tile left blank, and the seventh domino representing the blank-blank (0–0) combination. Subsequently 45-piece (double eight) sets appeared in Austria and, in recent times, 55-piece (double nine) and 91-piece (double twelve) sets have been produced.

All the early games are still played today alongside games that have sprung up in the last 60 years such as Five Up, Mexican Train and Chicken Foot, the last two taking advantage of the larger domino sets available.

Some modern descriptions of All Fives are quite different from the original, having lost much of their cribbage character and incorporating a single spinner, making it identical, or closely related, to Sniff. Most published rule sets for Muggins include the rule that gives the game its name, but some modern publications omit it even though the muggins rule has been described as the unique feature of this game.

Dominoes is now played internationally. It is recognized as an "ingrained cultural activity within the Caribbean" but is also popular with the Windrush generation (who have Caribbean heritage) in the UK.

In the U.S. state of Alabama, it was illegal to play dominoes on Sunday within the state until the relevant section of the Alabama Criminal Code was repealed, effective April 21, 2015.

Dominoes (also known as bones, cards, men, pieces or tiles), are normally twice as long as they are wide, which makes it easier to re-stack pieces after use. A domino usually features a line in the middle to divide it visually into two squares, called ends. The value of either side is the number of spots or pips. In the most common variant (double-six), the values range from six pips down to none or blank. The sum of the two values, i.e. the total number of pips, may be referred to as the rank or weight of a tile; a tile may be described as "heavier" than a "lighter" one that has fewer (or no) pips.

Tiles are generally named after their two values. For instance, the following are descriptions of the tile bearing the values two and five:
A tile that has the same pips-value on each end is called a double or doublet, and is typically referred to as double-zero , double-one , and so on. Conversely, a tile bearing different values is called a single.

Every tile which features a given number is a member of the suit of that number. A single tile is a member of two suits: for example, belongs both to the suit of threes and the suit of blanks, or 0 suit.

In some versions the doubles can be treated as an additional suit of doubles. In these versions, the belongs both to the suit of sixes and the suit of doubles. However, the dominant approach is that each double belongs to only one suit.

The most common domino sets commercially available are double six (with 28 tiles) and double nine (with 55 tiles). Larger sets exist and are popular for games involving several players or for players looking for long domino games.

The number of tiles in a double-n set obeys the following formula:

This formula can be simplified a little bit when formula_2 is made equal to the "total number of doubles in the domino set":

formula_3

The total number of pips in a double-n set is found by:
formula_4 i.e. the number of tiles multiplied by the maximum pip-count (n)

e.g. a 6-6 set has (7 × 8) / 2 = 56/2 = 28 tiles, the average number of pips per tile is 6 (range is from 0 to 12), giving a total pip count of 6 × 28 = 168

The most popular type of play are layout games, which fall into two main categories, blocking games and scoring games.


The most basic domino variant is for two players and requires a double-six set. The 28 tiles are shuffled face down and form the "stock" or "boneyard". Each player draws seven tiles from the stock. Once the players begin drawing tiles, they are typically placed on-edge in front of the players, so players can see their own tiles, but not the value of their opponents' tiles. Players can thus see how many tiles remain in their opponents' hands at all times.

One player begins by downing (playing the first tile) one of their tiles. This tile starts the line of play, in which values of adjacent pairs of tile ends must match. The players alternately extend the line of play with one tile at one of its two ends; if a player is unable to place a valid tile, they must continue drawing tiles from the stock until they are able to place a tile. The game ends when one player wins by playing their last tile, or when the game is blocked because neither player can play. If that occurs, whoever caused the block receives all of the remaining player points not counting their own.

Players accrue points during game play for certain configurations, moves, or emptying one's hand. Most scoring games use variations of the draw game. If a player does not call "domino" before the tile is laid on the table, and another player says domino after the tile is laid, the first player must pick up an extra domino.

In a draw game (blocking or scoring), players are additionally allowed to draw as many tiles as desired from the stock before playing a tile, and they are not allowed to pass before the stock is (nearly) empty. The score of a game is the number of pips in the losing player's hand plus the number of pips in the stock. Most rules prescribe that two tiles need to remain in the stock. The draw game is often referred to as simply "dominoes".

Adaptations of both games can accommodate more than two players, who may play individually or in teams.

The line of play is the configuration of played tiles on the table. It starts with a single tile and typically grows in two opposite directions when players add matching tiles. In practice, players often play tiles at right angles when the line of play gets too close to the edge of the table.

The rules for the line of play often differ from one variant to another. In many rules, the doubles serve as spinners, i.e., they can be played on all four sides, causing the line of play to branch. Sometimes, the first tile is required to be a double, which serves as the only spinner. In some games such as Chicken Foot, all sides of a spinner must be occupied before anybody is allowed to play elsewhere. Matador has unusual rules for matching. Bendomino uses curved tiles, so one side of the line of play (or both) may be blocked for geometrical reasons.

In Mexican Train and other train games, the game starts with a spinner from which various trains branch off. Most trains are owned by a player and in most situations players are allowed to extend only their own train.

In blocking games, scoring happens at the end of the game. After a player has emptied their hand, thereby winning the game for the team, the score consists of the total pip count of the losing team's hands. In some rules, the pip count of the remaining stock is added. If a game is blocked because no player can move, the winner is often determined by adding the pips in players' hands.

In scoring games, each individual can potentially add to the score. For example, in Bergen, players score two points whenever they cause a configuration in which both open ends have the same value and three points if additionally one open end is formed by a double. In Muggins, players score by ensuring the total pip count of the open ends is a multiple of a certain number. In variants of Muggins, the line of play may branch due to spinners. In the common U.S. variant known as Fives players score by making the open ends a multiple of five.

In British public houses and social clubs, a scoring version of "5s-and-3s" is used. The game is normally played in pairs (two against two) and is played as a series of "ends". In each "end", the objective is for players to attach a domino from their hand to one end of those already played so that the sum of the end tiles is divisible by five or three. One point is scored for each time five or three can be divided into the sum of the two tiles, i.e. four at one end and five at the other makes nine, which is divisible by three three times, resulting in three points. Double five at one end and five at the other makes 15, which is divisible by three five times (five points) and divisible by five three times (three points) for a total of eight points.

An "end" stops when one of the players is out, i.e., has played all of their tiles. In the event no player is able to empty their hand, then the player with the lowest domino left in hand is deemed to be out and scores one point. A game consists of any number of ends with points scored in the ends accumulating towards a total. The game ends when one of the pair's total score exceeds a set number of points. A running total score is often kept on a cribbage board. 5s-and-3s is played in a number of competitive leagues in the British Isles.

Apart from the usual blocking and scoring games, also domino games of a very different character are played, such as solitaire or trick-taking games. Most of these are adaptations of card games and were once popular in certain areas to circumvent religious proscriptions against playing cards.
A very simple example is a Concentration variant played with a double-six set; two tiles are considered to match if their total pip count is 12.

A popular domino game in Texas is 42. The game is similar to the card game spades. It is played with four players paired into teams. Each player draws seven tiles, and the tiles are played into tricks. Each trick counts as one point, and any domino with a multiple of five dots counts toward the total of the hand. These 35 points of "five count" and seven tricks equals 42 points, hence the name.

Dominoes is played at a professional level, similar to poker. Numerous organisations and clubs of amateur domino players exist around the world. Some organizations organize international competitions. Examples include the Anglo Caribbean Dominoes League (ACDL) in the UK which includes over 40 clubs including the Brixton Immortals.
Since April 2008, the character encoding standard Unicode includes characters that represent the double-six domino tiles. While a complete domino set has only 28 tiles, the Unicode set has "reversed" versions of the 21 tiles with different numbers on each end, a "back" image, and everything duplicated as horizontal and vertical orientations, for a total of 100 glyphs. Few fonts are known to support these glyphs. 



Dissociation constant

In chemistry, biochemistry, and pharmacology, a dissociation constant ("K") is a specific type of equilibrium constant that measures the propensity of a larger object to separate (dissociate) reversibly into smaller components, as when a complex falls apart into its component molecules, or when a salt splits up into its component ions. The dissociation constant is the inverse of the association constant. In the special case of salts, the dissociation constant can also be called an ionization constant.

For a general reaction:

</chem>

in which a complex formula_1 breaks down into "x" A subunits and "y" B subunits, the dissociation constant is defined as

where [A], [B], and [A B] are the equilibrium concentrations of A, B, and the complex A B, respectively.

One reason for the popularity of the dissociation constant in biochemistry and pharmacology is that in the frequently encountered case where "x" = "y" = 1, "K" has a simple physical interpretation: when [A] = "K", then [B] = [AB] or, equivalently, formula_3

The dissociation constant is commonly used to describe the affinity between a ligand <chem>L</chem> (such as a drug) and a protein <chem>P</chem>; i.e., how tightly a ligand binds to a particular protein. Ligand–protein affinities are influenced by non-covalent intermolecular interactions between the two molecules such as hydrogen bonding, electrostatic interactions, hydrophobic and van der Waals forces. Affinities can also be affected by high concentrations of other macromolecules, which causes macromolecular crowding.

The formation of a ligand–protein complex <chem>LP</chem> can be described by a two-state process

</chem>

the corresponding dissociation constant is defined

where <chem>[P], [L]</chem>, and <chem>[LP]</chem> represent molar concentrations of the protein, ligand, and protein–ligand complex, respectively.

The dissociation constant has molar units (M) and corresponds to the ligand concentration <chem>[L]</chem> at which half of the proteins are occupied at equilibrium, i.e., the concentration of ligand at which the concentration of protein with ligand bound <chem>[LP]</chem> equals the concentration of protein with no ligand bound <chem>[P]</chem>. The smaller the dissociation constant, the more tightly bound the ligand is, or the higher the affinity between ligand and protein. For example, a ligand with a nanomolar (nM) dissociation constant binds more tightly to a particular protein than a ligand with a micromolar (μM) dissociation constant.

Sub-picomolar dissociation constants as a result of non-covalent binding interactions between two molecules are rare. Nevertheless, there are some important exceptions. Biotin and avidin bind with a dissociation constant of roughly 10 M = 1 fM = 0.000001 nM.
Ribonuclease inhibitor proteins may also bind to ribonuclease with a similar 10 M affinity. 

The dissociation constant for a particular ligand–protein interaction can change with solution conditions (e.g., temperature, pH and salt concentration). The effect of different solution conditions is to effectively modify the strength of any intermolecular interactions holding a particular ligand–protein complex together.

Drugs can produce harmful side effects through interactions with proteins for which they were not meant to or designed to interact. Therefore, much pharmaceutical research is aimed at designing drugs that bind to only their target proteins (negative design) with high affinity (typically 0.1–10 nM) or at improving the affinity between a particular drug and its "in vivo" protein target (positive design).

In the specific case of antibodies (Ab) binding to antigen (Ag), usually the term affinity constant refers to the association constant.
</chem>

This chemical equilibrium is also the ratio of the on-rate ("k" or "k") and off-rate ("k" or "k") constants. Two antibodies can have the same affinity, but one may have both a high on- and off-rate constant, while the other may have both a low on- and off-rate constant.

For the deprotonation of acids, "K" is known as "K", the acid dissociation constant. Strong acids, such as sulfuric or phosphoric acid, have large dissociation constants; weak acids, such as acetic acid, have small dissociation constants.

The symbol "K", used for the acid dissociation constant, can lead to confusion with the association constant, and it may be necessary to see the reaction or the equilibrium expression to know which is meant.

Acid dissociation constants are sometimes expressed by p"K", which is defined by

This formula_8 notation is seen in other contexts as well; it is mainly used for covalent dissociations (i.e., reactions in which chemical bonds are made or broken) since such dissociation constants can vary greatly.

A molecule can have several acid dissociation constants. In this regard, that is depending on the number of the protons they can give up, we define "monoprotic", "diprotic" and "triprotic" acids. The first (e.g., acetic acid or ammonium) have only one dissociable group, the second (e.g., carbonic acid, bicarbonate, glycine) have two dissociable groups and the third (e.g., phosphoric acid) have three dissociable groups. In the case of multiple p"K" values they are designated by indices: p"K", p"K", p"K" and so on. For amino acids, the p"K" constant refers to its carboxyl (–COOH) group, p"K" refers to its amino (–NH) group and the p"K" is the p"K" value of its side chain.

The dissociation constant of water is denoted "K":

The concentration of water, [HO], is omitted by convention, which means that the value of "K" differs from the value of "K" that would be computed using that concentration.

The value of "K" varies with temperature, as shown in the table below. This variation must be taken into account when making precise measurements of quantities such as pH.


Dimensional analysis

In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric current) and units of measurement (such as metres and grams) and tracking these dimensions as calculations or comparisons are performed. The term dimensional analysis is also used to refer to conversion of units from one dimensional unit to another, which can be used to evaluate scientific formulae. 
Commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are expressed in differing units of measurement; e.g., metres and feet, grams and pounds, seconds and years. "Incommensurable" physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are expressed in, e.g. metres and grams, seconds and grams, metres and seconds. For example, asking whether a gram is larger than an hour is meaningless.

Any physically meaningful equation, or inequality, "must" have the same dimensions on its left and right sides, a property known as "dimensional homogeneity". Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.

The concept of physical dimension, and of dimensional analysis, was introduced by Joseph Fourier in 1822.

The Buckingham π theorem describes how every physically meaningful equation involving variables can be equivalently rewritten as an equation of dimensionless parameters, where "m" is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.

A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or physical constants of nature. This may give insight into the fundamental properties of the system, as illustrated in the examples below.

The dimension of a physical quantity can be expressed as a product of the base physical dimensions such as length, mass and time, each raised to an integer (and occasionally rational) power. The "dimension" of a physical quantity is more fundamental than some "scale" or unit used to express the amount of that physical quantity. For example, "mass" is a dimension, while the kilogram is a particular reference quantity chosen to express a quantity of mass. The choice of unit is arbitrary, and its choice is often based on historical precedent. Natural units, being based on only universal constants, may be thought of as being "less arbitrary".

There are many possible choices of base physical dimensions. The SI standard selects the following dimensions and corresponding dimension symbols: 
The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity is given by 
where , , , , , , are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis – for instance, one could replace the dimension (I) of electric current of the SI basis with a dimension (Q) of electric charge, since .

A quantity that has only (with all other exponents zero) is known as a geometric quantity. A quantity that has only both and is known as a kinematic quantity. A quantity that has only all of , , and is known as a dynamic quantity.
A quantity that has all exponents null is said to have dimension one.

The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, ; in this case 2.54 cm/in is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.

There are also physicists who have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.

As examples, the dimension of the physical quantity speed is

The dimension of the physical quantity acceleration is

The dimension of the physical quantity force is

The dimension of the physical quantity pressure is 

The dimension of the physical quantity energy is

The dimension of the physical quantity power is

The dimension of the physical quantity electric charge is

The dimension of the physical quantity electric potential difference is

The dimension of the physical quantity capacitance is

In dimensional analysis, Rayleigh's method is a conceptual tool used in physics, chemistry, and engineering. It expresses a functional relationship of some variables in the form of an exponential equation. It was named after Lord Rayleigh.

The method involves the following steps:

As a drawback, Rayleigh's method does not provide any information regarding number of dimensionless groups to be obtained as a result of dimensional analysis.

Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with "per" are expressed with division, e.g. 60 km/h. Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m for square metres), or combinations thereof.

A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m), thus they are considered derived or compound units.

Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which may be expressed as the product of mass (with unit kg) and acceleration (with unit m⋅s). The newton is defined as .

Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as "hundredths", since .

Taking a derivative with respect to a quantity divides the dimension by the dimension of the variable that is differentiated with respect to. Thus:
Likewise, taking an integral adds the dimension of the variable one is integrating with respect to, but in the numerator.

In economics, one distinguishes between stocks and flows: a stock has a unit (say, widgets or dollars), while a flow is a derivative of a stock, and has a unit of the form of this unit divided by one of time (say, dollars/year).

In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus debt-to-GDP should have the unit year, which indicates that debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.

The most basic rule of dimensional analysis is that of dimensional homogeneity. 

However, the dimensions form an abelian group under multiplication, so:
For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes sense to ask whether 1 mile is more, the same, or less than 1 kilometre, being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.

The rule implies that in a physically meaningful "expression" only quantities of the same dimension can be added, subtracted, or compared. For example, if , and denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression is meaningful, but the heterogeneous expression is meaningless. However, is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.

Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension , they are fundamentally different physical quantities.

To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same unit. For example, to compare 32 metres with 35 yards, use to convert 35 yards to 32.004 m.

A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that a conversion factor between a unit that measures the same dimension must take: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a "conversion factor". For example, kPa and bar are both units of pressure, and . The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to . Since any quantity can be multiplied by 1 without changing it, the expression "" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including the unit. For example, because , and bar/bar cancels out, so .

Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.

A simple application of dimensional analysis to mathematics is in computing the form of the volume of an -ball (the solid ball in "n" dimensions), or the area of its surface, the -sphere: being an -dimensional figure, the volume scales as , while the surface area, being -dimensional, scales as . Thus the volume of the -ball in terms of the radius is , for some constant . Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.

In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

In fluid mechanics, dimensional analysis is performed to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:

The origins of dimensional analysis have been disputed by historians. The first written application of dimensional analysis has been credited to François Daviet, a student of Lagrange, in a 1799 article at the Turin Academy of Science.

This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p. 39). In the second edition of 1833, Poisson explicitly introduces the term "dimension" instead of the Daviet "homogeneity".

In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like should be independent of the units employed to measure the physical variables.

James Clerk Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be "the three fundamental units", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant is taken as unity, thereby defining . By assuming a form of Coulomb's law in which the Coulomb constant "k" is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were , which, after substituting his equation for mass, results in charge having the same dimensions as mass, viz. .

Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize. It was used for the first time in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue. Rayleigh first published the technique in his 1877 book "The Theory of Sound".

The original meaning of the word "dimension", in Fourier's "Theorie de la Chaleur", was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are TL, instead of just the exponents.

What is the period of oscillation of a mass attached to an ideal linear spring with spring constant suspended in gravity of strength ? That period is the solution for of some dimensionless equation in the variables , , , and .
The four quantities have the following dimensions: [T]; [M]; [M/T]; and [L/T]. From these we can form only one dimensionless product of powers of our chosen variables, , and putting for some dimensionless constant gives the dimensionless equation sought. The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term "group" means "collection" rather than mathematical group. They are often called dimensionless numbers as well.

The variable does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines with , , and , because is the only quantity that involves the dimension L. This implies that in this problem the is irrelevant. Dimensional analysis can sometimes yield strong statements about the "irrelevance" of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of : it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way: , for some dimensionless constant (equal to formula_15 from the original dimensionless equation).

When faced with a case where dimensional analysis rejects a variable (, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.

When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be "complete" – although it still may involve unknown dimensionless constants, such as .

Consider the case of a vibrating wire of length (L) vibrating with an amplitude (L). The wire has a linear density (M/L) and is under tension (LM/T), and we want to know the energy (LM/T) in the wire. Let and be two dimensionless products of powers of the variables chosen, given by

The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation
where is some unknown function, or, equivalently as
where is some other unknown function. Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension. Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function . But our experiments are simpler than in the absence of dimensional analysis. We'd perform none to verify that the energy is proportional to the tension. Or perhaps we might guess that the energy is proportional to , and so infer that . The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.

The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex. Consider, for example, a small pebble sitting on the bed of a river. If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water. At what critical velocity will this occur? Sorting out the guessed variables is not so easy as before. But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness (L) and radius (L). The disc has a density (M/L), rotates at an angular velocity (T) and this leads to a stress (TLM) in the material. There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid. As the disc becomes thicker relative to the radius then the plane stress solution breaks down. If the disc is restrained axially on its free faces then a state of plane strain will occur. However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case. An engineer might, therefore, be interested in establishing a relationship between the five variables. Dimensional analysis for this case leads to the following () non-dimensional groups:

Through the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure. As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs.

The dimensions that can be formed from a given collection of basic physical dimensions, such as T, L, and M, form an abelian group: The identity is written as 1; , and the inverse of L is 1/L or L. L raised to any integer power is a member of the group, having an inverse of L or 1/L. The operation of the group is multiplication, having the usual rules for handling exponents (). Physically, 1/L can be interpreted as reciprocal length, and 1/T as reciprocal time (see reciprocal second).

An abelian group is equivalent to a module over the integers, with the dimensional symbol corresponding to the tuple . When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the module. When measurable quantities are raised to an integer power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the module.

A basis for such a module of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any module, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).

The group identity, the dimension of dimensionless quantities, corresponds to the origin in this module, .

In certain cases, one can define fractional dimensions, specifically by formally defining fractional powers of one-dimensional vector spaces, like . However, it is not possible to take arbitrary fractional powers of units, due to representation-theoretic obstructions.

One can work with vector spaces with given dimensions without needing to use units (corresponding to coordinate systems of the vector spaces). For example, given dimensions and , one has the vector spaces and , and can define as the tensor product. Similarly, the dual space can be interpreted as having "negative" dimensions. This corresponds to the fact that under the natural pairing between a vector space and its dual, the dimensions cancel, leaving a dimensionless scalar.

The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., ) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, . (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same unit as some derived quantity can be expressed in the general form

Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

Knowing this restriction can be a powerful tool for obtaining new insight into the system.

The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions T, L, and M – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.

For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to T, L, M: the former can be expressed as [F = LM/T], L, M, while the latter can be expressed as [T = (LM/F)], L, M.

On the other hand, length, velocity and time (T, L, V) do not form a set of base dimensions for mechanics, for two reasons:

Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of T, L, M and Q, where Q represents the dimension of electric charge. In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ. In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ ) is also defined as a base dimension, N.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.

Bridgman’s theorem restricts the type of function that can be used to define a physical quantity from general (dimensionally compounded) quantities to only products of powers of the quantities, unless some of the independent quantities are algebraically combined to yield dimensionless groups, whose functions are grouped together in the dimensionless numeric multiplying factor. This excludes polynomials of more than one term or transcendental functions not of that form.

Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity , where the logarithm is taken in any base, holds for dimensionless numbers and , but it does "not" hold if and are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.

Similarly, while one can evaluate monomials () of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for , the expression makes sense (as an area), while for , the expression does not make sense.

However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless. For example,

This is the height to which an object rises in time  if the acceleration of gravity is 9.8 and the initial upward speed is 500 . It is not necessary for to be in "seconds". For example, suppose  = 0.01 minutes. Then the first term would be

The value of a dimensional physical quantity is written as the product of a unit [] within the dimension and a dimensionless numerical value or numerical factor, .

When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in the same unit so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 metre added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

The factor 0.3048 m/ft is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to the same unit so that their numerical values can be added or subtracted.

Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.

A quantity equation, also sometimes called a complete equation, is an equation that remains valid independently of the unit of measurement used when expressing the physical quantities.

In contrast, in a "numerical-value equation", just the numerical values of the quantities occur, without units. Therefore, it is only valid when each numerical values is referenced to a specific unit.

For example, a quantity equation for displacement as speed multiplied by time difference would be:
for = 5 m/s, where and may be expressed in any units, converted if necessary.
In contrast, a corresponding numerical-value equation would be:
where is the numeric value of when expressed in seconds and is the numeric value of when expressed in metres.

Generally, the use of numerical-value equations is discouraged.

The dimensionless constants that arise in the results obtained, such as the in the Poiseuille's Law problem and the in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation. Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity. This observation can allow one to sometimes make "back of the envelope" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.

Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on "dimensional grounds" that the non-analytical part of the free energy per lattice site should be , where is the dimension of the lattice.

It has been argued by some physicists, e.g., Michael J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: , , and , in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.

Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants , , and (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit , and . In problems involving a gravitational field the latter limit should be taken such that the field stays finite.

Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.

Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML, and later in F#. There are implementations for Haskell, OCaml, and Rust, Python, and a code checker for Fortran.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.
McBride and Nordvall-Forsberg show how to use dependent types to extend type systems for units of measure.

Mathematica 13.2 has a function for transformations with quantities named NondimensionalizationTransform that applies a nondimensionalization transform to an equation. Mathematica also has a function to find the dimensions of a unit such as 1 J named UnitDimensions. Mathematica also has a function that will find dimensionally equivalent combinations of a subset of physical quantities named DimensionalCombations. Mathematica can also factor out certain dimension with UnitDimensions by specifying an argument to the function UnityDimensions. For example, you can use UnityDimensions to factor out angles. In addition to UnitDimensions, Mathematica can find the dimensions of a QuantityVariable with the function QuantityVariableDimensions.

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).

Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:
This illustrates the subtle distinction between "affine" quantities (ones modeled by an affine space, such as position) and "vector" quantities (ones modeled by a vector space, such as displacement).

Properly then, positions have dimension of "affine" length, while displacements have dimension of "vector" length. To assign a number to an "affine" unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a "vector" unit only requires a unit of measurement.

Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.

This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,
where the symbol ≘ means "corresponds to", since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.

For temperature differences,
(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.

Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a "direction". (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.

This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.

Huntley has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank formula_26 of the dimensional matrix.

He introduced two approaches:

As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component formula_27 and a horizontal velocity component , assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then , the distance travelled, with dimension L, , , both dimensioned as TL, and the downward acceleration of gravity, with dimension TL.

With these four quantities, we may conclude that the equation for the range may be written:

Or dimensionally
from which we may deduce that formula_30 and , which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions T and L, and four parameters, with one equation.

However, if we use directed length dimensions, then formula_31 will be dimensioned as TL, formula_32 as TL, as L and as TL. The dimensional equation becomes:
and we may solve completely as , and . The increase in deductive power gained by the use of directed length dimensions is apparent.

Huntley's concept of directed length dimensions however has some serious limitations:

It also is often quite difficult to assign the L, L, L, L, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the "symmetry" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of "symmetry" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?

Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.

In Huntley's second approach, he holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia ("inertial mass"), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity only to inertial mass, while not implicating inertial properties. No further restrictions are added to its definition.

For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass, we may choose as the relevant variables:
There are three fundamental variables, so the above five equations will yield two independent dimensionless variables:

If we distinguish between inertial mass with dimension formula_36 and quantity of matter with dimension formula_37, then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

where now only is an undetermined constant (found to be equal to formula_39 by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.

Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.

Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin at a speed and angle above the "x"-axis, with the force of gravity directed along the negative "y"-axis. It is desired to find the range , at which point the mass returns to the "x"-axis. Conventional analysis will yield the dimensionless variable , but offers no insight into the relationship between and .

Siano has suggested that the directed dimensions of Huntley be replaced by using "orientational symbols" to denote vector directions, and an orientationless symbol 1. Thus, Huntley's L becomes L1 with L specifying the dimension of length, and specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own. Along with the requirement that , the following multiplication table for the orientation symbols results:

The orientational symbols form a group (the Klein four-group or "Viergruppe"). In this system, scalars always have the same orientation as the identity element, independent of the "symmetry of the problem". Physical quantities that are vectors have the orientation expected: a force or a velocity in the z-direction has the orientation of . For angles, consider an angle that lies in the z-plane. Form a right triangle in the z-plane with being one of the acute angles. The side of the right triangle adjacent to the angle then has an orientation and the side opposite has an orientation . Since (using to indicate orientational equivalence) we conclude that an angle in the xy-plane must have an orientation , which is not unreasonable. Analogous reasoning forces the conclusion that has orientation while has orientation 1. These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form , where and are real scalars. An expression such as formula_40 is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:
which for formula_42 and formula_43 yields . Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is .

The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive more information about acceptable solutions of physical problems. In this approach, one solves the dimensional equation as far as one can. If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral, putting it into normal form. The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols. The solution is then more complete than the one that dimensional analysis alone gives. Often, the added information is that one of the powers of a certain variable is even or odd.

As an example, for the projectile problem, using orientational symbols, , being in the xy-plane will thus have dimension and the range of the projectile will be of the form:

Dimensional homogeneity will now correctly yield and , and orientational homogeneity requires that . In other words, that must be an odd integer. In fact, the required function of theta will be which is a series consisting of odd powers of .

It is seen that the Taylor series of and are orientationally homogeneous using the above multiplication table, while expressions like and are not, and are (correctly) deemed unphysical.

Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.





Digital television

Digital television (DTV) is the transmission of television signals using digital encoding, in contrast to the earlier analog television technology which used analog signals. At the time of its development it was considered an innovative advancement and represented the first significant evolution in television technology since color television in the 1950s. Modern digital television is transmitted in high-definition television (HDTV) with greater resolution than analog TV. It typically uses a widescreen aspect ratio (commonly ) in contrast to the narrower format () of analog TV. It makes more economical use of scarce radio spectrum space; it can transmit up to seven channels in the same bandwidth as a single analog channel, and provides many new features that analog television cannot. A transition from analog to digital broadcasting began around 2000. Different digital television broadcasting standards have been adopted in different parts of the world; below are the more widely used standards:

Digital television's roots are tied to the availability of inexpensive, high performance computers. It was not until the 1990s that digital TV became a real possibility. Digital television was previously not practically feasible due to the impractically high bandwidth requirements of uncompressed video, requiring around 200Mbit/s for a standard-definition television (SDTV) signal, and over 1Gbit/s for high-definition television (HDTV).

In the mid-1980s, Toshiba released a television set with digital capabilities, using integrated circuit chips such as a microprocessor to convert analog television broadcast signals to digital video signals, enabling features such as freezing pictures and showing two channels at once. In 1986, Sony and NEC Home Electronics announced their own similar TV sets with digital video capabilities. However, they still relied on analog TV broadcast signals, with true digital TV broadcasts not yet being available at the time.

A digital TV broadcast service was proposed in 1986 by Nippon Telegraph and Telephone (NTT) and the Ministry of Posts and Telecommunication (MPT) in Japan, where there were plans to develop an "Integrated Network System" service. However, it was not possible to practically implement such a digital TV service until the adoption of motion-compensated DCT video compression formats such as MPEG made it possible in the early 1990s.

In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, and as the MUSE analog format was proposed by Japan's public broadcaster NHK as a worldwide standard. Japanese advancements were seen as pacesetters that threatened to eclipse US electronics companies. Until June 1990, the Japanese MUSE standard—based on an analog system—was the front-runner among the more than 23 different technical concepts under consideration.

Between 1988 and 1991, several European organizations were working on DCT-based digital video coding standards for both SDTV and HDTV. The EU 256 project by the CMTT and ETSI, along with research by Italian broadcaster RAI, developed a DCT video codec that broadcast SDTV at 34Mbit/s and near-studio-quality HDTV at about 70140Mbit/s. RAI demonstrated this with a 1990 FIFA World Cup broadcast in March 1990. An American company, General Instrument, also demonstrated the feasibility of a digital television signal in 1990. This led to the FCC being persuaded to delay its decision on an advanced television (ATV) standard until a digitally based standard could be developed.

In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new TV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images. Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being simulcast on different channels. The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.

The final standard adopted by the FCC did not produce a universal standard for scanning formats, aspect ratios, or lines of resolution. This outcome resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—is superior. Interlaced scanning, which is used in televisions worldwide, scans even-numbered lines first, then odd-numbered ones. Progressive scanning, which is the format used in computers, scans lines in sequences, from top to bottom. The computer industry argued that progressive scanning is superior because it does not flicker in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format.

DirecTV in the US launched the first commercial digital satellite platform in May 1994, using the Digital Satellite System (DSS) standard. Digital cable broadcasts were tested and launched in the US in 1996 by TCI and Time Warner. The first digital terrestrial platform was launched in November 1998 as ONdigital in the UK, using the DVB-T standard.

Digital television supports many different picture formats defined by the broadcast television systems which are a combination of size and aspect ratio (width to height ratio).

With digital terrestrial television (DTT) broadcasting, the range of formats can be broadly divided into two categories: high-definition television (HDTV) for the transmission of high-definition video and standard-definition television (SDTV). These terms by themselves are not very precise, and many subtle intermediate cases exist.

One of several different HDTV formats that can be transmitted over DTV is: pixels in progressive scan mode (abbreviated "720p") or pixels in interlaced video mode ("1080i"). Each of these uses a aspect ratio. HDTV cannot be transmitted over analog television channels because of channel capacity issues.

SDTV, by comparison, may use one of several different formats taking the form of various aspect ratios depending on the technology used in the country of broadcast. NTSC can deliver a resolution in 4:3 and in , while PAL can give in and in . However, broadcasters may choose to reduce these resolutions to reduce bit rate (e.g., many DVB-T channels in the UK use a horizontal resolution of 544 or 704 pixels per line).

Each commercial broadcasting terrestrial television DTV channel in North America is allocated enough bandwidth to broadcast up to 19 megabits per second. However, the broadcaster does not need to use this entire bandwidth for just one broadcast channel. Instead, the broadcast can use Program and System Information Protocol and subdivide across several video subchannels (a.k.a. feeds) of varying quality and compression rates, including non-video datacasting services.

A broadcaster may opt to use a standard-definition (SDTV) digital signal instead of an HDTV signal, because current convention allows the bandwidth of a DTV channel (or "multiplex") to be subdivided into multiple digital subchannels, (similar to what most FM radio stations offer with HD Radio), providing multiple feeds of entirely different television programming on the same channel. This ability to provide either a single HDTV feed or multiple lower-resolution feeds is often referred to as distributing one's bit budget or multicasting. This can sometimes be arranged automatically, using a statistical multiplexer. With some implementations, image resolution may be less directly limited by bandwidth; for example in DVB-T, broadcasters can choose from several different modulation schemes, giving them the option to reduce the transmission bit rate and make reception easier for more distant or mobile viewers.

There are several different ways to receive digital television. One of the oldest means of receiving DTV (and TV in general) is from terrestrial transmitters using an antenna (known as an "aerial" in some countries). This delivery method is known as digital terrestrial television (DTT). With DTT, viewers are limited to channels that have a terrestrial transmitter in range of their antenna.

Other delivery methods include digital cable and digital satellite. In some countries where transmissions of TV signals are normally achieved by microwaves, digital multichannel multipoint distribution service is used. Other standards, such as digital multimedia broadcasting (DMB) and digital video broadcasting - handheld (DVB-H), have been devised to allow handheld devices such as mobile phones to receive TV signals. Another way is Internet Protocol television (IPTV), which is the delivery of TV over a computer network. Finally, an alternative way is to receive digital TV signals via the open Internet (Internet television), whether from a central streaming service or a P2P (peer-to-peer) system.

Some signals are protected by encryption and backed up with the force of law under the WIPO Copyright Treaty and national legislation implementing it, such as the US Digital Millennium Copyright Act. Access to encrypted channels can be controlled by a removable card, for example via the Common Interface or CableCard.

Digital television signals must not interfere with each other, and they must also coexist with analog television until it is phased out. The following table gives allowable signal-to-noise and signal-to-interference ratios for various interference scenarios. This table is a crucial regulatory tool for controlling the placement and power levels of stations. Digital TV is more tolerant of interference than analog TV.

People can interact with a DTV system in various ways. One can, for example, browse the electronic program guide. Modern DTV systems sometimes use a return path providing feedback from the end user to the broadcaster. This is possible over cable TV or through an Internet connection but is not possible with a standard antenna alone.

Some of these systems support video on demand using a communication channel localized to a neighborhood rather than a city (terrestrial) or an even larger area (satellite).

1seg (1-segment) is a special form of ISDB. Each channel is further divided into 13 segments. Twelve are allocated for HDTV and the other for narrow-band receivers such as mobile televisions and cell phones.

DTV has several advantages over analog television, the most significant being that digital channels take up less bandwidth, and the bandwidth allocations are flexible depending on the level of compression and resolution of the transmitted image. This means that digital broadcasters can provide more digital channels in the same space, provide high-definition television service, or provide other non-television services such as multimedia or interactivity. DTV also permits special services such as multiplexing (more than one program on the same channel), electronic program guides and additional languages (spoken or subtitled). The sale of non-television services may provide an additional revenue source to broadcasters.

Digital and analog signals react to interference differently. For example, common problems with analog television include ghosting of images, noise from weak signals, and other problems that degrade the quality of the image and sound, although the program material may still be watchable. With digital television, because of the cliff effect, reception of the digital signal must be very nearly complete; otherwise, neither audio nor video will be usable.

Analog TV began with monophonic sound and later developed multichannel television sound with two independent audio signal channels. DTV allows up to 5 audio signal channels plus a subwoofer bass channel, producing broadcasts similar in quality to movie theaters and DVDs.

Digital TV signals require less transmission power than analog TV signals to be broadcast and received satisfactorily.

DTV images have some picture defects that are not present on analog television or motion picture cinema, because of present-day limitations of bit rate and compression algorithms such as MPEG-2. This defect is sometimes referred to as mosquito noise.

Because of the way the human visual system works, defects in an image that are localized to particular features of the image or that come and go are more perceptible than defects that are uniform and constant. However, the DTV system is designed to take advantage of other limitations of the human visual system to help mask these flaws, e.g. by allowing more compression artifacts during fast motion where the eye cannot track and resolve them as easily and, conversely, minimizing artifacts in still backgrounds that, because time allows, may be closely examined in a scene.

Broadcast, cable, satellite, and Internet DTV operators control the picture quality of television signal encoders using sophisticated, neuroscience-based algorithms, such as the structural similarity index measure (SSIM) video quality measurement tool. Another tool called visual information fidelity (VIF), is used in the Netflix VMAF video quality monitoring system.

Changes in signal reception from factors such as degrading antenna connections or changing weather conditions may gradually reduce the quality of analog TV. The nature of digital TV results in a perfectly decodable video initially, until the receiving equipment starts picking up interference that overpowers the desired signal or if the signal is too weak to decode. Some equipment will show a garbled picture with significant damage, while other devices may go directly from perfectly decodable video to no video at all or lock up. This phenomenon is known as the digital cliff effect.

Block errors may occur when transmission is done with compressed images. A block error in a single frame often results in black boxes in several subsequent frames, making viewing difficult.

For remote locations, distant channels that, as analog signals, were previously usable in a snowy and degraded state may, as digital signals, be perfectly decodable or may become completely unavailable. The use of higher frequencies add to these problems, especially in cases where a clear line-of-sight from the receiving antenna to the transmitter is not available, because usually higher frequency signals can't pass through obstacles as easily.

Television sets with only analog tuners cannot decode digital transmissions. When analog broadcasting over the air ceases, users of sets with analog-only tuners may use other sources of programming (e.g. cable, recorded media) or may purchase set-top converter boxes to tune in the digital signals. In the United States, a government-sponsored coupon was available to offset the cost of an external converter box.

The digital television transition began as early as the late 1990s and has been completed on a country-by-country basis in most parts of the world.

Prior to the conversion to digital TV, analog television broadcast audio for TV channels on a separate FM carrier signal from the video signal. This FM audio signal could be heard using standard radios equipped with the appropriate tuning circuits.

However, after the digital television transition, no portable radio manufacturer has yet developed an alternative method for portable radios to play just the audio signal of digital TV channels; DTV radio is not the same thing.

The adoption of a broadcast standard incompatible with existing analog receivers has created the problem of large numbers of analog receivers being discarded during digital television transition. One superintendent of public works was quoted in 2009 saying; "some of the studies I’ve read in the trade magazines say up to a quarter of American households could be throwing a TV out in the next two years following the regulation change". In 2009, an estimated 99 million analog TV receivers were sitting unused in homes in the US alone and, while some obsolete receivers are being retrofitted with converters, many more are simply dumped in landfills where they represent a source of toxic metals such as lead as well as lesser amounts of materials such as barium, cadmium and chromium.

According to one campaign group, a CRT computer monitor or TV contains an average of of lead. According to another source, the lead in glass of a CRT varies from 1.08 lb to 11.28 lb, depending on screen size and type, but the lead is in the form of "stable and immobile" lead oxide mixed into the glass. It is claimed that the lead can have long-term negative effects on the environment if dumped as landfill. However, the glass envelope can be recycled at suitably equipped facilities. Other portions of the receiver may be subject to disposal as hazardous material.

Local restrictions on disposal of these materials vary widely; in some cases second-hand stores have refused to accept working color television receivers for resale due to the increasing costs of disposing of unsold TVs. Those thrift stores which are still accepting donated TVs have reported significant increases in good-condition working used television receivers abandoned by viewers who often expect them not to work after digital transition.

In Michigan in 2009, one recycler estimated that as many as one household in four would dispose of or recycle a TV set in the following year. The digital television transition, migration to high-definition television receivers and the replacement of CRTs with flatscreens are all factors in the increasing number of discarded analog CRT-based television receivers.




Declaration of Arbroath

The Declaration of Arbroath (; ; ) is the name usually given to a letter, dated 6 April 1320 at Arbroath, written by Scottish barons and addressed to Pope John XXII. It constituted King Robert I's response to his excommunication for disobeying the pope's demand in 1317 for a truce in the First War of Scottish Independence. The letter asserted the antiquity of the independence of the Kingdom of Scotland, denouncing English attempts to subjugate it.

Generally believed to have been written in Arbroath Abbey by Bernard of Kilwinning (or of Linton), then Chancellor of Scotland and Abbot of Arbroath, and sealed by fifty-one magnates and nobles, the letter is the sole survivor of three created at the time. The others were a letter from the King of Scots, Robert I, and a letter from four Scottish bishops which all made similar points. The "Declaration" was intended to assert Scotland's status as an independent, sovereign state and defend Scotland's right to use military action when unjustly attacked.

Submitted in Latin, the "Declaration" was little known until the late 17th century, and is unmentioned by any of Scotland's major 16th-century historians. In the 1680s, the Latin text was printed for the first time and translated into English in the wake of the Glorious Revolution, after which time it was sometimes described as a declaration of independence.

The "Declaration" was part of a broader diplomatic campaign, which sought to assert Scotland's position as an independent kingdom, rather than its being a feudal land controlled by England's Norman kings, as well as to lift the excommunication of Robert the Bruce. The pope had recognised Edward I of England's claim to overlordship of Scotland in 1305 and Bruce was excommunicated by the Pope for murdering John Comyn before the altar at Greyfriars Church in Dumfries in 1306. This excommunication was lifted in 1308; subsequently the pope threatened Robert with excommunication again if Avignon's demands in 1317 for peace with England were ignored. Warfare continued, and in 1320 John XXII again excommunicated Robert I. In reply, the "Declaration" was composed and signed and, in response, the papacy rescinded King Robert Bruce's excommunication and thereafter addressed him using his royal title.

The wars of Scottish independence began as a result of the deaths of King Alexander III of Scotland in 1286 and his heir the "Maid of Norway" in 1290, which left the throne of Scotland vacant and the subsequent succession crisis of 1290–1296 ignited a struggle among the Competitors for the Crown of Scotland, chiefly between the House of Comyn, the House of Balliol, and the House of Bruce who all claimed the crown. After July 1296's deposition of King John Balliol by Edward of England and then February 1306's killing of John Comyn III, Robert Bruce's rivals to the throne of Scotland were gone, and Robert was crowned king at Scone that year. Edward I, the "Hammer of Scots", died in 1307; his son and successor Edward II did not renew his father's campaigns in Scotland. In 1309 a parliament held at St Andrews acknowledged Robert's right to rule, received emissaries from the Kingdom of France recognising the Bruce's title, and proclaimed the independence of the kingdom from England.

By 1314 only Edinburgh, Berwick-upon-Tweed, Roxburgh, and Stirling remained in English hands. In June 1314 the Battle of Bannockburn had secured Robert Bruce's position as King of Scots; Stirling, the Central Belt, and much of Lothian came under Robert's control while the defeated Edward II's power on escaping to England via Berwick weakened under the sway of his cousin Henry, Earl of Lancaster. King Robert was thus able to consolidate his power, and sent his brother Edward Bruce to claim the Kingdom of Ireland in 1315 with an army landed in Ulster the previous year with the help of Gaelic lords from the Isles. Edward Bruce died in 1318 without achieving success, but the Scots campaigns in Ireland and in northern England were intended to press for the recognition of Robert's crown by King Edward. At the same time, it undermined the House of Plantagenet's claims to overlordship of the British Isles and halted the Plantagenets' effort to absorb Scotland as had been done in Ireland and Wales. Thus were the Scots nobles confident in their letters to Pope John of the distinct and independent nature of Scotland's kingdom; the "Declaration of Arbroath" was one such. According to historian David Crouch, "The two nations were mutually hostile kingdoms and peoples, and the ancient idea of Britain as an informal empire of peoples under the English king's presidency was entirely dead."

The text describes the ancient history of Scotland, in particular the "Scoti", the Gaelic forebears of the Scots who the "Declaration" claims have origins in "Scythia Major" prior to migrating via Spain to Great Britain "1,200 years from the Israelite people's crossing of the Red Sea". The "Declaration" describes how the Scots had "thrown out the Britons and completely destroyed the Picts", resisted the invasions of "the Norse, the Danes and the English", and "held itself ever since, free from all slavery". It then claims that in the Kingdom of Scotland, "one hundred and thirteen kings have reigned of their own Blood Royal, without interruption by foreigners". The text compares Robert Bruce with the Biblical warriors Judah Maccabee and Joshua.

The "Declaration" made a number of points: that Edward I of England had unjustly attacked Scotland and perpetrated atrocities; that Robert the Bruce had delivered the Scottish nation from this peril; and, most controversially, that the independence of Scotland was the prerogative of the Scottish people, rather than the King of Scots.

Some have interpreted this last point as an early expression of popular sovereignty – that government is contractual and that kings can be chosen by the community rather than by God alone. It has been considered to be the first statement of the contractual theory of monarchy underlying modern constitutionalism. 

It has also been argued that the "Declaration" was not a statement of popular sovereignty (and that its signatories would have had no such concept) but a statement of royal propaganda supporting Bruce's faction. A justification had to be given for the rejection of King John Balliol in whose name William Wallace and Andrew de Moray had rebelled in 1297. The reason given in the "Declaration" is that Bruce was able to defend Scotland from English aggression whereas King John could not.

Whatever the true motive, the idea of a contract between King and people was advanced to the Pope as a justification for Bruce's coronation whilst John de Balliol, who had abdicated the Scottish throne, still lived as a Papal prisoner.

There is also recent scholarship that suggests that the Declaration was substantially derived from the 1317 Irish Remonstrance, also sent in protest of English actions. There are substantial similarities in content between the 1317 Irish Remonstrance and the Declaration of Arbroath, produced three years later. It is also clear that the drafters of the Declaration of Arbroath would have access to the 1317 Irish Remonstrance, it having been circulated to Scotland in addition to the Pope. It has been suggested therefore that the 1317 Remonstrance was a "prototype" for the Declaration of Arbroath, suggesting Irish-Scottish cooperation in attempts to protest against English interference.

For the full text in Latin and a translation in English, See on WikiSource.

There are 39 names—eight earls and thirty-one barons—at the start of the document, all of whom may have had their seals appended, probably over the space of some time, possibly weeks, with nobles sending in their seals to be used. The folded foot of the document shows that at least eleven additional barons and freeholders (who were not noble) who were not listed on the head were associated with the letter. On the extant copy of the "Declaration" there are only 19 seals, and of those 19 people only 12 are named within the document. It is thought likely that at least 11 more seals than the original 39 might have been appended. The "Declaration" was then taken to the papal court at Avignon by Sir Adam Gordon, Sir Odard de Maubuisson, and Bishop Kininmund who was not yet a bishop and probably included for his scholarship.
The Pope heeded the arguments contained in the "Declaration", influenced by the offer of support from the Scots for his long-desired crusade if they no longer had to fear English invasion. He exhorted Edward II in a letter to make peace with the Scots. However, it did not lead to his recognising Robert as King of Scots, and the following year was again persuaded by the English to take their side and issued six bulls to that effect.

Eight years later, on 1 March 1328, the new English king, Edward III, signed a peace treaty between Scotland and England, the Treaty of Edinburgh–Northampton. In this treaty, which was in effect until 1333, Edward renounced all English claims to Scotland. In October 1328, the interdict on Scotland, and the excommunication of its king, were removed by the Pope.

The original copy of the "Declaration" that was sent to Avignon is lost. The only existing manuscript copy of the "Declaration" survives among Scotland's state papers, measuring 540mm wide by 675mm long (including the seals), it is held by the National Archives of Scotland in Edinburgh, a part of the National Records of Scotland. 

The most widely known English language translation was made by Sir James Fergusson, formerly Keeper of the Records of Scotland, from text that he reconstructed using this extant copy and early copies of the original draft. 

G. W. S. Barrow has shown that one passage in particular, often quoted from the Fergusson translation, was carefully written using different parts of "The Conspiracy of Catiline" by the Roman author, Sallust (86–35 BC) as the direct source:

Listed below are the signatories of the Declaration of Arbroath in 1320.

The letter itself is written in Latin. It uses the Latin versions of the signatories' titles, and in some cases, the spelling of names has changed over the years. This list generally uses the titles of the signatories' Wikipedia biographies.


In addition, the names of the following do not appear in the document's text, but their names are written on seal tags and their seals are present:


In 1998 former majority leader Trent Lott succeeded in instituting an annual "National Tartan Day" on 6 April by resolution of the United States Senate. US Senate Resolution 155 of 10 November 1997 states that "the Declaration of Arbroath, the Scottish Declaration of Independence, was signed on April 6, 1320 and the American Declaration of Independence was modeled [sic] on that inspirational document". However, although this influence is accepted by some historians, it is disputed by others. 

In 2016 the Declaration of Arbroath was placed on the UK Memory of the World Register, part of UNESCO's Memory of the World Programme.

2020 was the 700th anniversary of the Declaration of Arbroath's composition; an "Arbroath 2020" festival was arranged but postponed due to the COVID-19 pandemic. The National Museum of Scotland in Edinburgh planned to display the document to the public for the first time in fifteen years.




Digital data

Digital data, in information theory and information systems, is information represented as a string of discrete symbols, each of which can take on one of only a finite number of values from some alphabet, such as letters or digits. An example is a text document, which consists of a string of alphanumeric characters. The most common form of digital data in modern information systems is "binary data", which is represented by a string of binary digits (bits) each of which can have one of two values, either 0 or 1.

Digital data can be contrasted with "analog data", which is represented by a value from a continuous range of real numbers. Analog data is transmitted by an analog signal, which not only takes on continuous values but can vary continuously with time, a continuous real-valued function of time. An example is the air pressure variation in a sound wave. 

The word "digital" comes from the same source as the words digit and "digitus" (the Latin word for "finger"), as fingers are often used for counting. Mathematician George Stibitz of Bell Telephone Laboratories used the word "digital" in reference to the fast electric pulses emitted by a device designed to aim and fire anti-aircraft guns in 1942. The term is most commonly used in computing and electronics, especially where real-world information is converted to binary numeric form as in digital audio and digital photography.

Since symbols (for example, alphanumeric characters) are not continuous, representing symbols digitally is rather simpler than conversion of continuous or analog information to digital. Instead of sampling and quantization as in analog-to-digital conversion, such techniques as polling and encoding are used.

A symbol input device usually consists of a group of switches that are polled at regular intervals to see which switches are switched. Data will be lost if, within a single polling interval, two switches are pressed, or a switch is pressed, released, and pressed again. This polling can be done by a specialized processor in the device to prevent burdening the main CPU. When a new symbol has been entered, the device typically sends an interrupt, in a specialized format, so that the CPU can read it.

For devices with only a few switches (such as the buttons on a joystick), the status of each can be encoded as bits (usually 0 for released and 1 for pressed) in a single word. This is useful when combinations of key presses are meaningful, and is sometimes used for passing the status of modifier keys on a keyboard (such as shift and control). But it does not scale to support more keys than the number of bits in a single byte or word.

Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded or converted into a number based on the status of modifier keys and the desired character encoding.

A custom encoding can be used for a specific application with no loss of data. However, using a standard encoding such as ASCII is problematic if a symbol such as 'ß' needs to be converted but is not in the standard.

It is estimated that in the year 1986, less than 1% of the world's technological capacity to store information was digital and in 2007 it was already 94%. The year 2002 is assumed to be the year when humankind was able to store more information in digital than in analog format (the "beginning of the digital age").

Digital data come in these three states: data at rest, data in transit, and data in use. The confidentiality, integrity, and availability have to be managed during the entire lifecycle from 'birth' to the destruction of the data.

All digital information possesses common properties that distinguish it from analog data with respect to communications:

Even though digital signals are generally associated with the binary electronic digital systems used in modern electronics and computing, digital systems are actually ancient, and need not be binary or electronic.


Deduction

Deduction may refer to:




Demon

A demon is a malevolent supernatural entity. Historically, belief in demons, or stories about demons, occurs in religion, occultism, literature, fiction, mythology, and folklore; as well as in media such as comics, video games, movies, and television series.

Belief in demons probably goes back to the Paleolithic age, stemming from humanity's fear of the unknown, the strange and the horrific. In ancient Near Eastern religions and in the Abrahamic religions, including early Judaism and ancient-medieval Christian demonology, a demon is considered a harmful spiritual entity which may cause demonic possession, calling for an exorcism. Large portions of Jewish demonology, a key influence on Christianity and Islam, originated from a later form of Zoroastrianism, and was transferred to Judaism during the Persian era. 

Demons may or may not also be considered to be devils: minions of the Devil. In many traditions, demons are independent operators, with different demons causing different types of evils (destructive natural phenomena, specific diseases, etc.). In religions featuring a principal Devil (e.g. Satan) locked in an eternal struggle with God, demons are often also thought to be subordinates of the principal Devil. As lesser spirits doing the Devil's work, they have additional duties— causing humans to have sinful thoughts and tempting humans to commit sinful actions. 

The original Ancient Greek word ' () did not carry negative connotations, as it denotes a spirit or divine power. The Greek conception of a ' notably appears in the philosophical works of Plato, where it describes the divine inspiration of Socrates. 

In Christianity, morally ambivalent "" were replaced by demons, forces of evil only striving for corruption. Such demons are not the Greek intermediary spirits, but hostile entities, already known in Iranian beliefs. In Western esotericism and Renaissance magic, which grew out of an amalgamation of Greco-Roman magic, Jewish Aggadah and Christian demonology, a demon is believed to be a spiritual entity that may be conjured and controlled.

Belief in demons remains an important part of many modern religions and occultist traditions. Demons are still feared largely due to their alleged power to possess living creatures. In the contemporary Western occultist tradition (perhaps epitomized by the work of Aleister Crowley), a demon (such as Choronzon, which is Crowley's interpretation of the so-called "Demon of the Abyss") is a useful metaphor for certain inner psychological processes (inner demons), though some may also regard it as an objectively real phenomenon.

The Ancient Greek word (') denotes a spirit or divine power, much like the Latin ' or '. "Daimōn" most likely came from the Greek verb ' ("to divide" or "distribute"). The Greek conception of a "daimōn" notably appears in the philosophical works of Plato, where it describes the divine inspiration of Socrates. The original Greek word ' does not carry the negative connotation initially understood by implementation of the Koine ('), and later ascribed to any cognate words sharing the root.

The Greek terms do not have any connotations of evil or malevolence. In fact, ("", which literally translates as "good-spiritedness") means happiness. By the early centuries of the Roman Empire, cult statues were seen, by Pagans and their Christian neighbors alike, as inhabited by the numinous presence of the Greco-Roman gods: "Like pagans, Christians still sensed and saw the gods and their power, and as something, they had to assume, lay behind it, by an easy traditional shift of opinion they turned these pagan "daimones" into malevolent 'demons', the troupe of Satan. Far into the Byzantine period, Christians eyed their cities' old pagan statuary as a seat of the demons' presence. It was no longer beautiful, it was infested." The term had first acquired its negative connotations in the Septuagint translation of the Hebrew Bible into Greek, which drew on the mythology of ancient Semitic religions. This was then inherited by the Koine text of the New Testament. 

The English use of "demon" as synonym for devils goes back at least as far as about 825. The German word ('), however, is different from devil (') and demons as evil spirits, and akin to the original meaning of "daimon". The Western Modern era conception of a "demon", as in the "Ars Goetia", derives seamlessly from the ambient popular culture of Late Antiquity.

The exact definition of "demon" in Egyptology posed a major problem for modern scholarship, since the borders between a deity and a demon are sometimes blurred and the ancient Egyptian language lacks a term for the modern English "demon". Both deities and demons can act as intermediaries to deliver messages to humans. By that, they share some resemblance to the Greek daimon. However, magical writings indicate that ancient Egyptians acknowledged the existence of malevolent demons by highlighting the demon names with red ink. Demons in this culture appeared to be subordinative and related to a specific deity, yet they may have occasionally acted independently of the divine will. The existence of demons can be related to the realm of chaos, beyond the created world. But even this negative connotation cannot be denied in light of the magical texts. The role of demons in relation to the human world remains ambivalent and largely depends on context.

Ancient Egyptian demons can be divided into two classes: "guardians" and "wanderers".> "Guardians" are tied to a specific place; their demonic activity is topographically defined and their function can be benevolent towards those who have the secret knowledge to face them. Demons protecting the underworld may prevent human souls from entering paradise. Only by knowing the right charms is the deceased able to enter the "Halls of Osiris". Here, the aggressive nature of the guardian demons is motivated by the need to protect their abodes and not by their evil essence. Accordingly, demons guarded sacred places or the gates to the netherworld. During the Ptolemaic and Roman period, the guardians shifted towards the role of genius loci and they were the focus of local and private cults.

The "wanderers" are associated with possession, mental illness, death and plagues. Many of them serve as executioners for the major deities, such as Ra or Osiris, when ordered to punish humans on earth or in the netherworld. Wanderers can also be agents of chaos, arising from the world beyond creation to bring about misfortune and suffering without any divine instructions, led only by evil motivations. The influences of the wanderers can be warded off and kept at the borders of the human world by the use of magic, but they can never be destroyed. A sub-category of "wanderers" are nightmare demons, which were believed to cause nightmares by entering a human body.

The ancient Mesopotamians believed that the underworld was home to many demons, which are sometimes referred to as "offspring of "arali"". These demons could sometimes leave the underworld and terrorize mortals on earth. One class of demons that were believed to reside in the underworld were known as "galla"; their primary purpose appears to have been to drag unfortunate mortals back to Kur. They are frequently referenced in magical texts, and some texts describe them as being seven in number. Several extant poems describe the "galla" dragging the god Dumuzid into the underworld. Like other demons, however, "galla" could also be benevolent and, in a hymn from King Gudea of Lagash ( 2144 – 2124 BCE), a minor god named Ig-alima is described as "the great "galla" of Girsu".

Lamashtu was a demonic goddess with the "head of a lion, the teeth of a donkey, naked breasts, a hairy body, hands stained (with blood?), long fingers and fingernails, and the feet of Anzû". She was believed to feed on the blood of human infants and was widely blamed as the cause of miscarriages and cot deaths. Although Lamashtu has traditionally been identified as a demoness, the fact that she could cause evil on her own without the permission of other deities strongly indicates that she was seen as a goddess in her own right. Mesopotamian peoples protected against her using amulets and talismans. She was believed to ride in her boat on the river of the underworld and she was associated with donkeys. She was believed to be the daughter of An.

Pazuzu is a demonic god who was well known to the Babylonians and Assyrians throughout the first millennium BCE. He is shown with "a rather canine face with abnormally bulging eyes, a scaly body, a snake-headed penis, the talons of a bird and usually wings". He was believed to be the son of the god Hanbi. He was usually regarded as evil, but he could also sometimes be a beneficent entity who protected against winds bearing pestilence and he was thought to be able to force Lamashtu back to the underworld. Amulets bearing his image were positioned in dwellings to protect infants from Lamashtu and pregnant women frequently wore amulets with his head on them as protection from her.

Šul-pa-e's name means "youthful brilliance", but he was not envisioned as youthful god. According to one tradition, he was the consort of Ninhursag, a tradition which contradicts the usual portrayal of Enki as Ninhursag's consort. In one Sumerian poem, offerings made to Šhul-pa-e in the underworld and, in later mythology, he was one of the demons of the underworld.

According to "The Jewish Encyclopedia", originally published in 12 volumes from 1901 to 1906, "In Chaldean mythology the seven evil deities were known as "shedu", storm-demons, represented in ox-like form." They were represented as winged bulls, derived from the colossal bulls used as protective jinn of royal palaces.

There are differing opinions in Judaism about the existence or non-existence of demons ("shedim" or "se'irim"). Some Rabbinic scholars assert that demons have existed in Talmudic times, but do not exist regularly in present. When prophecy, divine presence, and divine inspiration gradually decreased, the demonic powers of impurity have become correspondingly weak, too.

The Hebrew Bible mentions two classes of demonic spirits, the "se'irim" and the "shedim". The word "shedim" (sing "shed" or "sheyd") appears in two places in the Hebrew Bible. The "se'irim" (sing. "sa'ir", "male goat") are mentioned once in Leviticus 17:7, probably a recollection of Assyrian demons in the shape of goats. They might be a metaphorical symbol for life-threatening animals, such as hyenas, ostrichs, and jackals. The "shedim", however, are not pagan demigods, but the foreign gods themselves. Both entities appear in a scriptural context of animal or child sacrifice to non-existent false gods.

Various diseases and ailments were ascribed to demons, particularly those affecting the brain and those of internal nature. Examples include catalepsy, headache, epilepsy and nightmares. There also existed a demon of blindness, "Shabriri" (lit. "dazzling glare") who rested on uncovered water at night and blinded those who drank from it.

Demons supposedly entered the body and caused the disease while overwhelming or "seizing" the victim. To cure such diseases, it was necessary to draw out the evil demons by certain incantations and talismanic performances, at which the Essenes excelled. Josephus, who spoke of demons as "spirits of the wicked which enter into men that are alive and kill them", but which could be driven out by a certain root, witnessed such a performance in the presence of the Emperor Vespasian and ascribed its origin to King Solomon. In mythology, there were few defences against Babylonian demons. The mythical mace Sharur had the power to slay demons such as Asag, a legendary "gallu" or "edimmu" of hideous strength.

In the Jerusalem Talmud, notions of "shedim" ("demons" or "spirits") are almost unknown or occur only very rarely, whereas in the Babylonian Talmud there are many references to "shedim" and magical incantations. The existence of "shedim" in general was not questioned by most of the Babylonian Talmudists. As a consequence of the rise of influence of the Babylonian Talmud over that of the Jerusalem Talmud, late rabbis, in general, took as fact the existence of "shedim", nor did most of the medieval thinkers question their reality. However, rationalists like Maimonides and Saadia Gaon and others explicitly denied their existence, and completely rejected concepts of demons, evil spirits, negative spiritual influences, attaching and possessing spirits. They thought the essential teaching about "shedim" and similar spirits is, that they should not be an object of worship, not a reality to be acknowledged or feared. Their point of view eventually became mainstream Jewish understanding.

The opinion of some authors is not clear. Abraham ibn Ezra states that insane people can see the image of "se'irim", when they go astray and ascribe to them powers independent from God. It is not clear from his work, if he considered these images of "se'irim" as manifestations of actual spirits or merely delusions. Despite academic consensus, Rabbis disputed that Maimonides denied the existence of demons entirely. He would only dispute the existence of demons in his own life time, but not that demons had existed once.

Occasionally an angel is called "satan" in the Babylon Talmud. But "satans" do not refer to demons as they remain at the service of God: "Stand not in the way of an ox when coming from the pasture, for Satan dances between his horns".

Aggadic tales from the Persian tradition describe the "shedim", the" mazziḳim" ("harmers"), and the" ruḥin" ("spirits"). There were also "lilin" ("night spirits"), "ṭelane" ("shade", or "evening spirits"), "ṭiharire" ("midday spirits"), and" ẓafrire" ("morning spirits"), as well as the "demons that bring famine" and "such as cause storm and earthquake". According to some aggadic stories, demons were under the dominion of a king or chief, usually "Asmodai".

In Kabbalah, demons are regarded as a necessary part of the divine emanation in the material world and a byproduct of human sin (Qlippoth). After they are created, they assume an existence on their own. Demons would attach themselves to the sinner and start to multiply as an act of self-preservation. Medieval Kabbalists characterize such demons as punishing angels of destruction. They are subject to the divine will, and do not act independently.

Other demonic entities, such as the "shedim", might be considered benevolent. The Zohar classifies them as those who are like humans and submit to the Torah, and those who have no fear of God and are like animals.

The sources of demonic influence were thought to originate from the Watchers or Nephilim, who are first mentioned in Genesis 6 and are the focus of 1 Enoch Chapters 1–16, and also in Jubilees 10. The Nephilim were seen as the source of the sin and evil on Earth because they are referenced in Genesis 6:4 before the story of the Flood. In Genesis 6:5, God sees evil in the hearts of men. Ethiopic Enoch refers to Genesis 6:4–5, and provides further description of the story connecting the Nephilim to the corruption of humans. According to the Book of Enoch, sin originates when angels descend from heaven and fornicate with women, birthing giants. The Book of Enoch shows that these fallen angels can lead humans to sin through direct interaction or through providing forbidden knowledge. Most scholars understand the text, that demons originate from the evil spirits of the deceased giants, cursed by God to wander the Earth. Dale Martin disagrees with this interpretation, arguing that the ghosts of the Nephilim are distinct. The evil spirits would make the people sacrifice to the demons, but they were not demons themselves. The spirits are stated in Enoch to "corrupt, fall, be excited, and fall upon the earth, and cause sorrow".

The Book of Jubilees conveys that sin occurs when Cainan accidentally transcribes astrological knowledge used by the Watchers. This differs from Enoch in that it does not place blame on the angels. However, in Jubilees 10:4 the evil spirits of the Watchers are discussed as evil and still remain on Earth to corrupt humans. God binds only 90% of the Watchers and destroys them, leaving 10% to be ruled by Mastema. Because the evil in humans is great, only 10% would be needed to corrupt and lead humans astray. These spirits of the giants are also referred to as "the bastards" in the apotropaic prayer Songs of the Sage, which lists the names of demons the narrator hopes to expel.

To the Qumran community during the Second Temple period, this apotropaic prayer was assigned, stating: "And, I the Sage, declare the grandeur of his radiance in order to frighten and terri[fy] all the spirits of the ravaging angels and the bastard spirits, demons, Liliths, owls" ("Dead Sea Scrolls", "Songs of the Sage", Lines 4–5).

In the Veda, gods ("deva") and anti-gods ("asura") share both the upper world. It is only by the time of the Brahmanas that they are said to inhabit the underworld. The identification of "asura" with "demons" stems from the description of asura as "formerly gods" ("pūrvadeva"). The gods are said to have claimed heaven for themselves and tricked the demons, ending on earth. During the Vedic period, gods aid humans against demons. By that, gods secure their own place in heaven, using humans as tools to defeat their cosmic enemies.

"Asura", in the earliest hymns of the Rigveda, originally meant any supernatural spirit, either good or bad. Since the /s/ of the Indic linguistic branch is cognate with the /h/ of the Early Iranian languages, the word "asura", representing a category of celestial beings, is a cognate with Old Persian "Ahura". Ancient Hinduism tells that Devas (also called "suras") and Asuras are half-brothers, sons of the same father Kashyapa; although some of the Devas, such as Varuna, are also called Asuras. Later, during Puranic age, Asura and Rakshasa came to exclusively mean any of a race of anthropomorphic, powerful, possibly evil beings. Daitya (lit. sons of the mother "Diti"), Danava (lit. sons of the mother "Danu"), Maya Danava, Rakshasa (lit. from "harm to be guarded against"), and asura are incorrectly translated into English as "demon".

With increase in asceticism during the post-Vedic period, withdrawal of sacrificial rituals was considered a threat to the gods. Ascetic humans or ascetic demons were supposed to be more powerful than gods. Pious, highly enlightened Asuras and Rakshasas, such as Prahlada and Vibhishana, are not uncommon. The Asura are not fundamentally against the gods, nor do they tempt humans to fall. Many people metaphorically interpret the Asura as manifestations of the ignoble passions in the human mind and as symbolic devices. There were also cases of power-hungry asuras challenging various aspects of the gods, but only to be defeated eventually and seek forgiveness.

Hinduism advocates the reincarnation and transmigration of souls according to one's karma. Souls (Atman) of the dead are adjudged by the Yama and are accorded various purging punishments before being reborn. Humans that have committed extraordinary wrongs are condemned to roam as lonely, often mischief mongers, spirits for a length of time before being reborn. Many kinds of such spirits (Vetalas and Pishachas) are recognized in the later Hindu texts. According to Hinduism, demons are not inherently evil beings, but good by following their "dharma" what is being evil and deceitful. However, nothing is purely evil or good, and a demon could eventually abandon his demonic nature.

Belief in demons does not constitute an essential feature in Buddhism. However, since belief in demons were common during the rise of Buddhism, they are integrated into the "cycle of Saṃsāra". Accordingly, their malevolent condition is due to their bad "karma" from their previous lives. When Buddhism spread, it accommodated itself with indigenous popular ideas about demons.

The Zorastrian belief in demons ("Daeva", later "div") had strong influence on the Abrahamic religions, especially Christianity and Islam.
In hell, demons continue to torment the damned.

Book 3 of the Denkard describes demons as the opposite of the creative power of God. As such, they cannot create, but only corrupt, and thus, evil is merely the corruption of the good. Since demons can only destroy, they will ultimately destroy themselves. Chapter 30 questions the reality of demons, since their existence seem to rely on their destruction of good. Therefore, Ahriman and his demons would miss any substance and exist only as absence of good.

Manichaeism was a major religion founded in the third century AD by the Parthian prophet Mani (), in the Sasanian Empire. One of its key concepts is the doctrine of "Two Principles and Three Moments". Accordingly, the world could be described as resulting from a past moment, in which two principles (good and evil) were separate, a contemporary moment in which both principles are mixed due to an assault of the world of darkness on the realm of light, and a future moment when both principles are distinct forever. Thus, evil and demons played a significant role within Manichaean teachings.

There are numberless designations for various groups of demonic entities in Manichaean cosmology. The general term used for the beings of the world of darkness is "dyw" ("dev"). Before the demons attacked the realm of light, they have been in constant battle and intercourse against each other. It is only in the realm of darkness demons are described in their physical form. After their assault on the world above, they have been overcome by the Living Spirit, and imprisoned in the structure of the world. From that point onwards, they impact human's ethical life, and appear as personified ethical qualities, mostly greed, envy, grief, and wrath (desire for destruction).

Ibn al-Jawzi, in his work "Talbīs Iblīs" (devils' delusion), credits the Manichaeans with believing that each "Light" and "Darkness" (God and the Devil) consist of four bodies and one spirit. The bodies of "Light" (God) were referred to as angels, while the bodies of "Darkness" (Devil) were referred to as "ifrits". "Light" and "Darkness" would multiple by angels and demons respectively.

In "The Book of Giants", one of the canonical seven treatises also known from Jewish intertestamental literature, the Grigori ("egrēgoroi") beget giant half-demon offspring with human woman. In the Middle Persian version of the "Book of Giants" they are referred to as "kʾw", while in the Coptic "Kephalaia" as "gigas". In accordance with some interpretations of Genesis 6:1–4, the giant offspring became the ancient tyrannic rulers over mankind, until overthrown by the angels of punishment. Nonetheless, these demons are still active in the microcosm, such as "Āz" and "Āwarzōg". Views on stars ("abāxtarān") are mixed. On one hand, they are regarded as light particles of the world soul fixed in the sky. On the other hand, stars are identified with powers hindering the soul from leaving the material world. The Third Messenger (Jesus) is said to have chained up demons in the sky. Their offspring, the "nephilim" ("nĕf īlīm") or "asrestar" ("āsarēštārān"), "Ašqalūn" and "Nebrō’ēl" in particular, play instrumental roles in the creation of Adam and Eve. According to Manichaeism, the watchers, known as angels in Jewish lore, are not considered angels, but demons.

The poem begins with the kings of the Pishdadian dynasty. They defeat and subjugate the demonic divs. Tahmuras commanded the divs and became known as "dīvband" (binder of demons). Jamshid, the fourth king of the world, ruled over both angels and divs, and served as a high priest of Ahura Mazda (Hormozd). Like his father, he slayed many divs, however, spared some under the condition they teach him new valuable arts, such as writing in different languages. After a just reign over hundreds of years, Jamshid grew haughty and claimed, because of his wealth and power, divinity for himself. Whereupon God withdraws his blessings from him, and his people get unsatisfied with their king. With the ceasing influence of God, the devil gains power and aids Zahhak to usurp the throne. Jamshid dies sawn in two by two demons. Tricked by Ahriman (or Iblis), Zahhak grew two snakes on his shoulders and becomes the demonic serpent-king. The King Kay Kāvus fails to conquer the legendary Mazandaran, the land of divs and gets captured. To save his king, Rustam takes a journey and fights through seven trials. Divs are among the common enemies Rustam faces, the last one the Div-e Sepid, the demonic king of Mazandaran.

The div in the Shahnameh might include both demonic supernatural beings as well as evil humans.

Rustam's battle against the demonic may also have a symbolic meaning: Rustam represents wisdom and rationality, fights the demon, embodiment of passion and instinct. Rustam's victory over the White Div is also a triumph over men's lower drives, and killing the demon is a way to purge the human soul from such evil inclinations. The killing of the White Div is an inevitable act to restore the human king's eyesight. Eliminating the divs is an act of self-preservation to safeguard the good in oneself's, and the part acceptable in a regulated society.

The Algonquian people traditionally believe in a spirit called a wendigo. The spirit is believed to possess people who then become cannibals. In Athabaskan folklore, there is a belief in wechuge, a similar cannibal spirit.

The existence of demons as inherently malicious spirits within Old Testamental texts is absent. Though there are evil spirits sent by YHWH, they can hardly be called "demons", since they serve and do not oppose the governing deity. First then the Hebrew Bible was translated into Greek, the "gods of other nations" were merged into a single category of demons (daimones) with implied negativity.

The Greek Daimons were associated with demi-divine entities, deities, illnesses and fortune-telling. The Jewish translators rendered them all as demons, depicting their power as nullified comparable to the description of shedim in the Tanakh. Although all these supernatural powers were translated, none were angels, despite sharing a similar function to that of the Greek Daimon. This established a dualism between the angels on God's side and negatively evaluated demons of pagan origin. Their relationship to the God-head became the main difference between angels and demons, not their degree of benevolence. Both angels and demons might be fierce and terrifying. However, the angels act always at service of the high god of the Israelites, differing from the pagan demons, who represent the powers of foreign deities. The Septuagint refers to evil spirits as demons (daimon).

Through the New Testament, demons appear 55 times, and 46 times in reference to demonic possession or exorcisms. Some old English Bible translations such as King James Version do not have the word 'demon' in their vocabulary and translate it as 'devil'. As adversaries of Jesus, demons are not morally ambivalent spirits, but evil; causes of misery, suffering, and death. They are not tempters, but the cause of pain, suffering, and maladies, both physical and mental. Temptation is reserved for the devil only. Unlike spirits in pagan beliefs, demons are not intermediary spirits who must be sacrificed for the appeasement of a deity. Possession also shows no trace of positivity, contrary to some pagan depictions of spirit possession. They are explicitly said to be ruled by the Devil or Beelzebub. Their origin is unclear, the texts take the existence of demons for granted. Many early Christians, like Irenaeus, Justin Martyr, Clement of Alexandria, and Lactantius assumed demons were ghosts of the Nephilim, known from Intertestamental writings. Because of references to Satan as the lord of demons and evil angels of Satan throughout the New Testament, other scholars identified fallen angels with demons. Demons as entirely evil entities, who have been born evil, may not fit the proposed origin of evil in free will, taught in alternate or opposing theologies.

Demons are included in biblical interpretation. In the story of Passover, the Bible tells the story as "the Lord struck down all the firstborn in Egypt" (). In the Book of Jubilees, which is considered canonical only by the Ethiopian Orthodox Church, this same event is told slightly differently: "All the powers of [the demon] Mastema had been let loose to slay all the first-born in the land of Egypt. And the powers of the Lord did everything according as the Lord commanded them." (Jubilees 49:2–4)

In the Genesis flood narrative, the author explains how God was noticing "how corrupt the earth had become, for all the people on earth had corrupted their ways" (). In Jubilees, the sins of man are attributed to "the unclean demons [who] began to lead astray the children of the sons of Noah, and to make to err and destroy them" (Jubilees 10:1). In Jubilees, Mastema questions the loyalty of Abraham and tells God to "bid him offer him as a burnt offering on the altar, and Thou wilt see if he will do this command" (Jubilees 17:16). The discrepancy between the story in Jubilees and the story in Genesis 22 exists with the presence of Mastema. In Genesis, God tests the will of Abraham merely to determine whether he is a true follower, however; in Jubilees, Mastema has an agenda behind promoting the sacrifice of Abraham's son, "an even more demonic act than that of Satan in Job". In Jubilees, where Mastema, an angel tasked with tempting mortals into sin and iniquity, requests that God give him a tenth of the spirits of the children of the watchers, demons, in order to aid the process (Jubilees 10:7–9). These demons are passed into Mastema's authority, where once again, an angel is in charge of demonic spirits.

In the Testament of Solomon, written sometime in the first three centuries C.E., the demon Asmodeus explains that he is the son of an angel and a human mother. Another demon describes himself as having died in the "massacre in the age of giants". "Beelzeboul", the prince of demons, appears as a fallen angel, not as a demon, but makes people worship demons as their gods.

Since Early Christianity, demonology has developed from a simple acceptance of demons to a complex study that has grown from the original ideas taken from Jewish demonology and Christian scriptures. Christian demonology is studied in depth within the Roman Catholic Church, although many other Christian churches affirm and discuss the existence of demons.

Building upon the few references to "daimon" in the New Testament, especially the poetry of the Book of Revelation, Christian writers of apocrypha from the second century onwards created a more complicated tapestry of beliefs about "demons" that was largely independent of Christian scripture.

While daimons were considered as both potentially benevolent or malevolent, Origen argued against Celsus that daimons are exclusively evil entities, supporting the later idea of (evil) demons. According to Origen's cosmology, increasing corruption and evil within the soul, the more estranged the soul gets from God. Therefore, Origen opined that the most evil demons are located underground. Besides the fallen angels known from Christian scriptures, Origen talks about Greek daemons, like nature spirits and giants. These creatures were thought to inhabit nature or air and nourish from pagan sacrifices roaming the earth. However, there is no functional difference between the spirits of the underworld and of earth, since both have fallen from perfection into the material world. Origen sums them up as fallen angels and thus equal to demons.

Many ascetics, like Origen and Anthony the Great, described demons as psychological powers, tempting to evil, in contrast to benevolent angels advising good. According to "Life of Anthony", written in Greek around 360 by Athanasius of Alexandria, most of the time, the demons were expressed as an internal struggle, inclinations, and temptations. But after Anthony successfully resisted the demons, they would appear in human form to tempt and threaten him even more intensely.

Pseudo-Dionysius the Areopagite described "evil" as "defiancy" and does not give "evil" an ontological existence. He explains demons are deficient creatures, who willingly turn themselves towards the unreal and non-existence. Their dangerous nature results not from the power of their nature, but from their tendency to drag others into the "void" and the unreal, away from God.

Michael Psellos proposed the existence of several types of demons, deeply influenced by the material nature of the regions they dwell. The highest and most powerful demons attack the mind of people using their "imaginative action" (phantastikos) to produce illusions in the mind. The lowest demons, on the other hand, are almost mindless, gross, and grunting spirits, which try to possess people instinctively, simply attracted by the warmth and life of humans. These cause diseases, fatal accidents and animalistic behavior in their victims. They are unable to speak, while other lower types of demons might give out false oracles. The demons are divided into:
Invocation of Saints, holy men and women, especially ascetics, reading the Gospel, holy oil or water is said to drive them out. However, Psellos' schemes have been too inconsistent to answer questions about the hierarchy of fallen angels. The devil's position is impossible to assign in this scheme and it does not respond to living perceptions of felt experience and was considered rather impractical to have a lasting effect or impact on Christian demonology.

The contemporary Roman Catholic Church unequivocally teaches that angels and demons are real beings rather than just symbolic devices. The Catholic Church has a cadre of officially sanctioned exorcists which perform many exorcisms each year. The exorcists of the Catholic Church teach that demons attack humans continually but that afflicted persons can be effectively healed and protected either by the formal rite of exorcism, authorized to be performed only by bishops and those they designate, or by prayers of deliverance, which any Christian can offer for themselves or others.

At various times in Christian history, attempts have been made to classify demons according to various proposed demonic hierarchies.

In recent times, scholars doubted that independent demons exist, and rather considers them, aking to Jewish "satan", to be servants of God. According to S. N. Chiu, God is shown sending a demon against Saul in 1 Samuel 16 and 18 in order to punish him for the failure to follow God's instructions, showing God as having the power to use demons for his own purposes, putting the demon under his divine authority. According to the "Britannica Concise Encyclopedia", demons, despite being typically associated with evil, are often shown to be under divine control, and not acting of their own devices.

In Mandaeism, the World of Darkness (), also referred to as Sheol, is the underworld located below Tibil (Earth). It is ruled by its king Ur (Leviathan) and its queen Ruha, mother of the seven planets and twelve constellations. The great dark Ocean of Sup (or Suf) lies in the World of Darkness. The great dividing river of Hitfun, analogous to the river Styx in Greek mythology, separates the World of Darkness from the World of Light.
Prominent infernal beings found in the World of Darkness include "lilith", "nalai" (vampire), "niuli" (hobgoblin), "gadalta" (ghost), "satani" (Satan) and various other demons and evil spirits.

Gnosticism largely relies on Greek and Persian dualism, especially on Platonism. In accordance with Platonism, they regarded the "idea" as good while considering the material and conscious world to be inherently evil. The demonized star-deities of late Persian religion became associated with a demon, thus identifying the seven observable planets with an Archon (demonic ruler). These demons rule over the earth and the realm of planets, representing different desires and passions. According to Origen, the Ophites depicted the world as surrounded by the demonic Leviathan.

Like in Christianity, the term "daimons" was used for demons and refers to both the Archons as well as to their demonic assistants. Judas Iscariot is, in the Gospel of Judas, portrayed as the "thirtheenth daimon" for betraying Jesus and a supporter of the Archons.

Examples of Gnostic portrayals of demons can be found in the "Apocryphon of John" in which they are said to have helped construct the physical Adam and in "Pistis Sophia" which states they are ruled over by Hekate and punish corrupt souls.

In Islamic beliefs, demons are called "Shayāṭīn" (or "Daeva" in Persian language). In Islam demons try to lead humans astray from God, by tempting them to sin, teaching them sorcery and cause mischief among humans. Occult practises may include conjuring demons, albeit not forbidden per se, it requires acts against God's laws and are therefore forbidden. Such acts may include illicit blood-sacrifices, abandoning prayer, and rejecting fasting. Based on the Islamic view on Solomon, who is widely believed to have been a ruler over genies and demons, Islam has a rich tradition about conjuring demons. Among the demons are the devils ("shayatin") and the fiends ("div"). Both are believed to have worked for Solomon as slaves. While the devils usually appear within a Judeo-Christian background, the "div" frequently feature in beliefs of Persian and Indian origin. But it is to be noted that in Islam both angels and demons are considered to be the creatures of God and so God has ultimate power over all of them. God sends devils as companions to those who want to disbelief in this world. Even the malicious "div" are created by God and thus have a place in the world's order.

According to exegesis of the Quran the devils are the offspring of Iblis (Satan). They are said to live until the world ceases to exist, always shadow in humans (and jinn) whispering onto their hearts to lead them astray. Prayers are used to ward off their attacks, dissolving them temporarily. As the counterpart of the angels, they are created rebellious and their abode in Hell is pre-destined. They lack free will and are bound to evil. The "ifrit" and "marid" are considered to be two more powerful classes of devils.

According to Abu Ali Bal'ami's work on the history of the world, Wahb ibn Munabbih explained that the "divs" were the first beings created by God. Some argue the devils were created good, but turned evil by Iblis' act of arrogance, the "div" were created as vicious creatures and embodiment of evil. When Iblis was still among the angels, he led an army against the spirits on the earth. Among them were the "div", who formed two orders; one of which sided with the jinn and were banished with them, condemned to roam the earth. The other, treacherous "div" joined Iblis in battle, and was exiled to Hell with him. The "div" are often depicted as sorcerers whose misdeeds are not bound to temptation only. They could cause sickness, mental illnesses, or even turn humans to stone by touching. While the devils frequently appear to ordinary humans to tempt them into everything disapproved by society, the "div" usually appear to specific heroes.

In the Bahá'í Faith, demons are not regarded as independent evil spirits as they are in some faiths. Rather, evil spirits described in various faiths' traditions, such as Satan, fallen angels, demons and jinn, are metaphors for the base character traits a human being may acquire and manifest when he turns away from God and follows his lower nature. Belief in the existence of ghosts and earthbound spirits is rejected and considered to be the product of superstition.

In Occult practises demons are often seen as beneficial and useful, lacking an inherent negative connotation. While some people fear demons, or attempt to exorcise them, others willfully attempt to summon them for knowledge, assistance, or power. William of Conches () understands 'demon' closer to the Greek 'daimon', reserving the concept of the "devil" only for the "demons of the lower regions":You think, as I infer from your words, that a demon is the same as a devil, which is not the case. For a demon is said to be any invisible being using reason, as if knowing. Of these the two high orders are called calodemons, that is, 'good knowing ones', the lower order is called cacodemon, that is, 'evil knowing one', for calos means 'good', cacos 'bad'.

The ceremonial magician usually consults a grimoire, which gives the names and abilities of demons as well as detailed instructions for conjuring and controlling them. Grimoires are not limited to demons – some give instructions for the invocation of deity, a process called theurgy. The use of ceremonial magic to call demons is also known as goetia, the name taken from a section in the famous grimoire known as the "Lesser Key of Solomon".

The Age of Enlightenment, conceptualizes humans as autonomous individuals, mostly independent from outer invisible forces, such as demons or gods ruling over human fate. Previously, the world was understood to be inhabited by various spirits and demons. With the raise of the rationalistic school of thought, the existence of foreign unknown forces was increasingly rejected. 
Demons were explained as non-existent. Visions of demons and ghosts were explained as the products of one's own mind. By labelling local deities and demons as superstition, local religious ideas were banished, supporting the promotion of nationwide gods and religions. Because of that, demons became increasingly associated with delusions. Wilkinson Duran states that people who believe in demons are often marginalized in the United States. The notion of possession was at odds with Western philosophy, such as the American Dream and capitalism, which implies the belief that everyone is responsible for their own fate and not at the mercy of external forces.

A minority of Muslim scholars in the Medieval Age, often associated with the Muʿtazila and the Jahmītes, denied that demons (jinn, devils, divs etc.) have physicality and asserted, they could only affect the mind by "waswās" (, 'demonic whisperings in the mind'). Some scholars, like ibn Sina, rejected the reality of jinn altogether. Al-Jāḥiẓ and al-Masʿūdī, explained jinn and demons as a merely psychological phenomena. In his "Kitāb al-Hayawān", al-Jāḥiẓ states that jinn and demons are the product of loneliness. Such a state induces people to mind-games, causing "waswās". Al-Masʿūdī is similarly critical regarding the reality of demons. He states that alleged demonic encounters are the result of fear and "wrong thinking". Alleged encounters are then told to other generations in bedtime stories and poems. When they grow up, they remember such stories in a state of fear or loneliness. This encourages their imaginations, resulting in another alleged demonic encounter.

Psychologist Wilhelm Wundt remarked that "among the activities attributed by myths all over the world to demons, the harmful predominate, so that in popular belief bad demons are clearly older than good ones." Sigmund Freud developed this idea and claimed that the concept of demons was derived from the important relation of the living to the dead: "The fact that demons are always regarded as the spirits of those who have died "recently" shows better than anything the influence of mourning on the origin of the belief in demons."

M. Scott Peck, an American psychiatrist, wrote two books on the subject, "People of the Lie: The Hope For Healing Human Evil" and "Glimpses of the Devil: A Psychiatrist's Personal Accounts of Possession, Exorcism, and Redemption". Peck describes in some detail several cases involving his patients. In "People of the Lie" he provides identifying characteristics of an evil person, whom he classified as having a character disorder. In "Glimpses of the Devil" Peck goes into significant detail describing how he became interested in exorcism in order to debunk the "myth" of possession by evil spirits – only to be convinced otherwise after encountering two cases which did not fit into any category known to psychology or psychiatry. Peck came to the conclusion that possession was a rare phenomenon related to evil and that possessed people are not actually evil; rather, they are doing battle with the forces of evil.

Although Peck's earlier work was met with widespread popular acceptance, his work on the topics of evil and possession has generated significant debate and derision. Much was made of his association with (and admiration for) the controversial Malachi Martin, a Roman Catholic priest and a former Jesuit, despite the fact that Peck consistently called Martin a liar and a manipulator.










Domino effect

A domino effect is the cumulative effect produced when one event sets off a series of similar or related events, a form of chain reaction. The term is an analogy to a falling row of dominoes. It typically refers to a linked sequence of events where the time between successive events is relatively short. The term can be used literally (about a series of actual collisions) or metaphorically (about causal linkages within systems such as global finance or politics). 

The literal, mechanical domino effect is exploited in Rube Goldberg machines. In chemistry, the principle applies to a domino reaction, in which one chemical reaction sets up the conditions necessary for a subsequent one that soon follows. In the realm of process safety, a domino-effect accident is an initial undesirable event triggering additional ones in related equipment or facilities, leading to a total incident effect more severe than the primary accident alone.

The metaphorical usage implies that an outcome is inevitable or highly likely (as it has already started to happen) – a form of slippery slope argument. When this outcome is actually unlikely (the argument is fallacious), it has also been called the "domino fallacy".


Diffusion pump

Diffusion pumps use a high speed jet of vapor to direct gas molecules in the pump throat down into the bottom of the pump and out the exhaust. They were the first type of high vacuum pumps operating in the regime of free molecular flow, where the movement of the gas molecules can be better understood as diffusion than by conventional fluid dynamics. Invented in 1915 by Wolfgang Gaede, he named it a "diffusion pump" since his design was based on the finding that gas cannot diffuse against the vapor stream, but will be carried with it to the exhaust. However, the principle of operation might be more precisely described as gas-jet pump, since diffusion also plays a role in other types of high vacuum pumps. In modern textbooks, the diffusion pump is categorized as a momentum transfer pump.

The diffusion pump is widely used in both industrial and research applications. Most modern diffusion pumps use silicone oil or polyphenyl ethers as the working fluid.

In the late 19th century, most vacuums were created using a Sprengel pump, which had the advantage of being very simple to operate, and capable of achieving quite good vacuum given enough time. Compared to later pumps, however, the pumping speed was very slow and the vapor pressure of the liquid mercury limited the ultimate vacuum.

Following his invention of the molecular pump, Wolfgang Gaede invented the diffusion pump in 1915, and originally used elemental mercury as the working fluid. After its invention, the design was quickly commercialized by Leybold. It was then improved by Irving Langmuir and W. Crawford. Cecil Reginald Burch discovered the possibility of using silicone oil in 1928.

An oil diffusion pump is used to achieve higher vacuum (lower pressure) than is possible by use of positive displacement pumps alone. Although its use has been mainly associated within the high-vacuum range, down to
, diffusion pumps today can produce pressures approaching

The oil diffusion pump is operated with an oil of low vapor pressure. The high speed jet is generated by boiling the fluid and directing the vapor through a jet assembly. Note that the oil is gaseous when entering the nozzles. Within the nozzles, the flow changes from laminar to supersonic and molecular. Often, several jets are used in series to enhance the pumping action. The outside of the diffusion pump is cooled using either air flow, water lines or a water-filled jacket. As the vapor jet hits the outer cooled shell of the diffusion pump, the working fluid condenses and is recovered and directed back to the boiler. The pumped gases continue flowing to the base of the pump at increased pressure, flowing out through the diffusion pump outlet, where they are compressed to ambient pressure by the secondary mechanical forepump and exhausted.

Unlike turbomolecular pumps and cryopumps, diffusion pumps have no moving parts and as a result are quite durable and reliable. They can function over pressure ranges of
. They are driven only by convection and thus have a very low energy efficiency.

One major disadvantage of diffusion pumps is the tendency to backstream oil into the vacuum chamber. This oil can contaminate surfaces inside the chamber or upon contact with hot filaments or electrical discharges may result in carbonaceous or siliceous deposits. Due to backstreaming, oil diffusion pumps are not suitable for use with highly sensitive analytical equipment or other applications which require an extremely clean vacuum environment, but mercury diffusion pumps may be in the case of ultra high vacuum chambers used for metal deposition. Often cold traps and baffles are used to minimize backstreaming, although this results in some loss of pumping speed.

The oil of a diffusion pump cannot be exposed to the atmosphere when hot. If this occurs, the oil will oxidise and has to be replaced. If a fire occurs, the smoke and residue may contaminate other parts of the system.

The least expensive diffusion pump oils are based on hydrocarbons which have been purified by double-distillation. Compared with the other fluids, they have higher vapor pressure, so are usually limited to a pressure of
. They are also the most likely to burn or explode if exposed to oxidizers.

The most common silicone oils used in diffusion pumps are trisiloxanes, which contain the chemical group Si-O-Si-O-Si, to which various phenyl groups or methyl groups are attached. These are available as the so called 702 and 703 blends, which were formerly manufactured by Dow Corning. These can be further separated into 704 and 705 oils, which are made up of the isomers of tetraphenyl tetramethyl trisiloxane and pentaphenyl trimethyl trisiloxane respectively.

For pumping reactive species, usually a polyphenyl ether based oil is used. These oils are the most chemical and heat resistant type of diffusion pump oil.

 
The steam ejector is a popular form of pump for vacuum distillation and freeze-drying. A jet of steam entrains the vapour that must be removed from the vacuum chamber. Steam ejectors can have single or multiple stages, with and without condensers in between the stages. While both steam ejectors and diffusion pumps use jets of vapor to entrain gas, they work on fundamentally different principles - steam ejectors rely on viscous flow and mixing to pump gas, whereas diffusion pumps use molecular diffusion. This has several consequences. In diffusion pumps, the inlet pressure can be much lower than the static pressure of jet, whereas in steam ejectors the two pressures are about the same. Also, diffusion pumps are capable of much higher compression ratios, and cannot discharge directly to atmosphere.



Domenico Alberti

Domenico Alberti (c. 1710 – 14 October 1746 (according to other sources: 1740)) was an Italian singer, harpsichordist, and composer.

Alberti was born in Venice and studied music with Antonio Lotti. He wrote operas, songs, and sonatas for keyboard instruments, for which he is best known today. His sonatas frequently employ arpeggiated accompaniment in the left hand in one of several patterns that are now collectively known as "Alberti bass". Alberti was one of the earliest composers to use those patterns, but was not the first or only one. The most well-known of these patterns consists of regular broken chords, with the lowest note sounding first, then the highest, then the middle and then the highest again, with the pattern repeated.

Today, Alberti is regarded as a minor composer, and his works are played or recorded only irregularly. However, the Alberti bass was used by many later composers, and it became an important element in much keyboard music of the classical music era.

An example of Alberti bass (Mozart's "Piano Sonata, K 545"):

In his own lifetime, Alberti was known as a singer, and often used to accompany himself on the harpsichord. In 1736, he served as a page for Pietro Andrea Cappello, the Venetian ambassador to Spain. While at the Spanish court, the famous castrato singer Farinelli heard him sing. Farinelli was said to have been impressed, although Alberti was an amateur.

Alberti's best known pieces are his keyboard sonatas, although even they are very rarely performed. It is thought he wrote around 36 sonatas, of which 14 have survived. They all have two movements, each in binary form.

It is probable that Mozart's first violin sonatas, written at the age of seven, were modeled on Alberti's work.

Doris Day

Doris Day (born Doris Mary Kappelhoff; April 3, 1922 – May 13, 2019) was an American actress and singer. She began her career as a big band singer in 1939, achieving commercial success in 1945 with two No. 1 recordings, "Sentimental Journey" and "My Dreams Are Getting Better All the Time" with Les Brown and His Band of Renown. She left Brown to embark on a solo career and recorded more than 650 songs from 1947 to 1967.

Day was one of the leading Hollywood film stars of the 1950s and 1960s. Her film career began with "Romance on the High Seas" (1948). She starred in films of many genres, including musicals, comedies, dramas and thrillers. She played the title role in "Calamity Jane" (1953) and starred in Alfred Hitchcock's "The Man Who Knew Too Much" (1956) with James Stewart. She costarred with Rock Hudson in three successful comedies including "Pillow Talk" (1959), for which she was nominated for the Academy Award for Best Actress. She also worked with James Garner on both "Move Over, Darling" (1963) and "The Thrill of It All" (1963) and starred alongside Clark Gable, Cary Grant, James Cagney, David Niven, Ginger Rogers, Jack Lemmon, Frank Sinatra, Kirk Douglas, Lauren Bacall, and Rod Taylor in various films. After ending her film career in 1968, only briefly removed from the height of her popularity, she starred in her own television sitcom "The Doris Day Show" (1968–1973).

In 1989, Day was awarded the Golden Globe and the Cecil B. DeMille Award for Lifetime Achievement in Motion Pictures. In 2004, she was awarded the Presidential Medal of Freedom. In 2008, she received the Grammy Lifetime Achievement Award as well as a Legend Award from the Society of Singers. In 2011, she was awarded the Los Angeles Film Critics Association's Career Achievement Award. In 2011, Day released her 29th studio album, "My Heart", which contained new material and became a UK Top 10 album. , she was one of eight recording artists to have been the top box-office earner in the United States four times.

Day was born Doris Mary Kappelhoff on April 3, 1922, in Cincinnati, Ohio, the daughter of German-American parents Alma Sophia ("née" Welz; 1895–1976) and William Joseph Kappelhoff (1892–1967). She was named after actress Doris Kenyon. Her mother was a homemaker, and her father was a music teacher and choirmaster. Her paternal grandfather Franz Joseph Wilhelm Kappelhoff immigrated to the United States in 1875 and settled within the large German community in Cincinnati. For most of her life, Day stated that she was born in 1924, but on the occasion of her 95th birthday, the Associated Press found her birth certificate that showed a 1922 date of birth.

Day had two older brothers: Richard (1917–1919), who died before her birth, and Paul (1919–1957). Her father's infidelity caused her parents to separate in 1932 when she was 10. She developed an early interest in dance, and in the mid-1930s formed a dance duo with Jerry Doherty that performed in nationwide competitions. On October 13, 1937, while Day was riding with friends, their car collided with a freight train, and she broke her right leg, curtailing her prospects as a professional dancer.

While recovering from her car accident, Day sang along with the radio and discovered her singing talent. She later said: "During this long, boring period, I used to while away a lot of time listening to the radio, sometimes singing along with the likes of Benny Goodman, Duke Ellington, Tommy Dorsey, and Glenn Miller. But the one radio voice I listened to above others belonged to Ella Fitzgerald. There was a quality to her voice that fascinated me, and I'd sing along with her, trying to catch the subtle ways she shaded her voice, the casual yet clean way she sang the words."

Day's mother Alma arranged for Doris to receive singing lessons from Grace Raine. After three lessons, Raine told Alma that Day had "tremendous potential" and gave her three lessons per week for the price of one. Years later, Day said that Raine had a greater effect on her singing style and career than had anyone else.
During the eight months when she was receiving singing lessons, Day secured her first professional jobs as a vocalist on the WLW radio program "Carlin's Carnival" and in a local restaurant, Charlie Yee's Shanghai Inn. During her radio performances, she first caught the attention of Barney Rapp, who was seeking a female vocalist and asked her to audition for the job. According to Rapp, he had auditioned about 200 other singers.

In 1939, Rapp suggested the stage name Doris Day because the Kappelhoff surname was too long for marquees and he admired her rendition of the song "Day After Day". After working with Rapp, Day worked with bandleaders Jimmy James, Bob Crosby and Les Brown. In 1941, Day appeared as a singer in three Soundies with the Les Brown band.

While working with Brown, Day recorded her first hit recording, "Sentimental Journey", released in early 1945. It soon became an anthem for World War II servicemen. The song continues to be associated with Day, and she rerecorded it on several occasions, including a version for her 1971 television special. During 1945–46, Day (as vocalist with the Les Brown Band) had six other top ten hits on the "Billboard" chart: "My Dreams Are Getting Better All the Time", Tain't Me", "Till the End of Time", "You Won't Be Satisfied (Until You Break My Heart)", "The Whole World Is Singing My Song" and "I Got the Sun in the Mornin. Les Brown said, "As a singer Doris belongs in the company of Bing Crosby and Frank Sinatra."

While singing with the Les Brown band and for nearly two years on Bob Hope's weekly radio program, Day toured extensively across the United States.

Her performance of the song "Embraceable You" impressed songwriter Jule Styne and his partner Sammy Cahn, and they recommended her for a role in "Romance on the High Seas" (1948). Day was cast for the role after auditioning for director Michael Curtiz. She was shocked to receive the offer and admitted to Curtiz that she was a singer without acting experience. but he appreciated her honesty and felt that "her freckles made her look like the All-American Girl."

The film provided her with a No. 2 hit recording as a soloist, "It's Magic", which occurred two months after her first No. 1 hit "Love Somebody", a duet with Buddy Clark. Day recorded "Someone Like You" before the film "My Dream Is Yours" (1949), which featured the song. In 1950, she collaborated as a singer with the polka musician Frankie Yankovic, and the U.S. servicemen in Korea voted her their favorite star.

Day continued to appear in light musicals such as "On Moonlight Bay" (1951), "By the Light of the Silvery Moon" (1953) and "Tea For Two" (1950) for Warner Bros.

Her most commercially successful film for Warner Bros. was "I'll See You in My Dreams" (1951), a musical biography of lyricist Gus Kahn that broke box-office records of 20 years. It was Day's fourth film directed by Curtiz. She appeared as the title character in the comedic western-themed musical "Calamity Jane" (1953). A song from the film, "Secret Love", won the Academy Award for Best Original Song and became Day's fourth No. 1 hit single in the United States.

Between 1950 and 1953, the albums from six of her film musicals charted in the Top 10, including three that reached No. 1. After filming "Lucky Me" (1954) with Bob Cummings and "Young at Heart" (1955) with Frank Sinatra, Day elected to not renew her contract with Warner Brothers.

During this period, Day also had her own radio program, "The Doris Day Show". It was broadcast on CBS in 1952–1953.

Primarily recognized as a musical-comedy actress, Day began to accept more dramatic roles in order to broaden her range. Her dramatic star turn as singer Ruth Etting in "Love Me or Leave Me" (1955), with top billing above James Cagney, received critical and commercial success, becoming Day's greatest film success to that point. Cagney said that she had "the ability to project the simple, direct statement of a simple, direct idea without cluttering it," comparing her performance to that of Laurette Taylor in the Broadway production "The Glass Menagerie" (1945). Day felt that it was her best film performance. The film's producer Joe Pasternak said, "I was stunned that Doris did not get an Oscar nomination." The film's soundtrack album became a No. 1 hit.

Day starred in Alfred Hitchcock's suspense film "The Man Who Knew Too Much (1956 film)" opposite James Stewart. She sang two songs in the film, "Que Sera, Sera (Whatever Will Be, Will Be)", which won an Academy Award for Best Original Song, and "We'll Love Again". The film was Day's 10th to reach the top 10 at the box office. She played the title role in the film noir thriller "Julie" (1956) with Louis Jourdan.

After three successive dramatic films, Day returned to her musical/comedic roots in "The Pajama Game" (1957) with John Raitt, based on the Broadway play of the same name. She appeared in the Paramount comedy "Teacher's Pet" (1958) alongside Clark Gable and Gig Young. She costarred with Richard Widmark and Gig Young in the romantic comedy film "The Tunnel of Love" (1958) and with Jack Lemmon in "It Happened to Jane" (1959).

"Billboard" annual nationwide poll of disc jockeys had ranked Day as the No. 1 female vocalist nine times in ten years (1949 through 1958), but her success and popularity as a singer was now being overshadowed by her box-office appeal.

In 1959, Day entered her most successful phase as a film actress with a series of romantic comedies beginning with "Pillow Talk" (1959), costarring Rock Hudson, who became a lifelong friend, and Tony Randall. Day received a nomination for an Academy Award for Best Actress, her only career Oscar nomination. Day, Hudson and Randall appeared in two more films together, "Lover Come Back" (1961) and "Send Me No Flowers" (1964).

Along with David Niven and Janis Paige, Day starred in "Please Don't Eat the Daisies" (1960) and with Cary Grant in the comedy "That Touch of Mink" (1962). During 1960 and the 1962-1964 period, she ranked No. 1 at the box office, the second woman to be No. 1 four times, an accomplishment equaled by no other actress except Shirley Temple. She set a record that has yet to be matched by receiving seven consecutive Laurel Awards as the top female box-office star.

Day teamed with James Garner starting with "The Thrill of It All", followed by "Move Over, Darling" (both 1963). The film's theme song "Move Over Darling", cowritten by her son, reached No. 8 in the UK. Between these comedic film appearances, Day costarred with Rex Harrison in the thriller "Midnight Lace" (1960), an update of the stage thriller "Gaslight".

Day's next film "Do Not Disturb" (1965) was popular with audiences, but her popularity soon waned. By the late 1960s, in the period of the emerging sexual revolution, some critics and comics dubbed Day "The World's Oldest Virgin," and she slipped from the list of top box-office stars, last appearing in the top ten with the hit film "The Glass Bottom Boat" (1966). Among the roles that she declined was that of Mrs. Robinson in "The Graduate", a role that eventually went to Anne Bancroft. In her memoirs, Day said that she had rejected the part on moral grounds, finding the script "vulgar and offensive."

Day starred in the Western film "The Ballad of Josie" in 1967. That same year, Day recorded "The Love Album", although it was not released until 1994. In 1968, she starred in the comedy film "Where Were You When the Lights Went Out?" about the Northeast blackout of November 9, 1965. Her final feature, the comedy "With Six You Get Eggroll", was released in 1968.

From 1959 to 1970, Day received nine Laurel Award nominations (and won four times) for best female performance in eight comedies and one drama. From 1959 through 1969, she received six Golden Globe nominations for best female performance in three comedies, one drama ("Midnight Lace"), one musical ("Jumbo") and her television series.

After her third husband Martin Melcher died on April 20, 1968, Day was shocked to discover that Melcher and his business partner and advisor Jerome Bernard Rosenthal had squandered her earnings, leaving her deeply in debt. Rosenthal had been her attorney since 1949 when he had represented her in her uncontested divorce action against her second husband, saxophonist George W. Weidler. Day filed suit against Rosenthal in February 1969 and won a successful decision in 1974, but she did not receive compensation until a settlement was reached in 1979.

Day also learned to her displeasure that Melcher had committed her to a television series that became "The Doris Day Show".

Day hated the idea of performing on television but felt obligated to forge ahead with the series. The first episode of "The Doris Day Show" aired on September 24, 1968, and featured a rerecorded version of "Que Sera, Sera" as its theme song. Day persevered with the show, needing to work to repay her debts, but only after CBS ceded creative control to her and her son. The show enjoyed a successful five-year run, although it may be best remembered for its abrupt season-to-season changes in casting and premise.

After the end of the television show's run in 1973, Day largely retired from acting but completed two television specials, "The Doris Mary Anne Kappelhoff Special" (1971) and "Doris Day Today" (1975), and she was a guest on various shows in the 1970s.

In the 1985–86 season, Day hosted her own television talk show, "Doris Day's Best Friends", on the Christian Broadcasting Network (CBN). The network canceled the show after 26 episodes despite the worldwide publicity that it had received. One episode featured Rock Hudson, who was showing the first public symptoms of AIDS, including severe weight loss and fatigue. He died from the disease later that year. Day later said, "He was very sick. But I just brushed that off and I came out and put my arms around him and said, 'Am I glad to see you'."

In October 1985, the Supreme Court of California rejected Rosenthal's appeal of the multimillion-dollar judgment awarded to Day in her suit against him for legal malpractice and upheld the conclusions of a trial court and an appeals court that Rosenthal had acted improperly. In April 1986, the U.S. Supreme Court refused to review the lower court's judgment. In June 1987, Rosenthal filed a $30 million lawsuit against lawyers whom he claimed had cheated him out of millions of dollars in real-estate investments. He named Day as a codefendant, describing her as an "unwilling, involuntary plaintiff whose consent cannot be obtained." Rosenthal claimed that much of the money that Day had lost was the result of the unwise advice of other attorneys who had suggested that she sell three hotels at a loss, as well as some oil leases in Kentucky and Ohio. He claimed that he had made the investments under a long-term plan and did not intend to sell them until they appreciated in value. Two of the hotels sold in 1970 for about $7 million, and their estimated worth in 1986 was $50 million.

Terry Melcher stated that his father's premature death saved Day from financial ruin. It was not known whether Martin Melcher had himself been duped by Rosenthal, and Day stated publicly that she believed him to be innocent of any deliberate wrongdoing, stating that he "simply trusted the wrong person." Author David Kaufman asserts that Day's former costar Louis Jourdan, maintained that Day disliked her husband, but Day's public statements regarding Melcher appear to contradict that assertion.

Day was scheduled to present, along with Patrick Swayze and Marvin Hamlisch, the award for Best Original Score Oscar at the 61st Academy Awards in March 1989, but she suffered a deep leg cut from a sprinkler and was unable to attend.

Day was inducted into the Ohio Women's Hall of Fame in 1981 and received the Cecil B. DeMille Award for career achievement in 1989. In 1994, Day's "Greatest Hits" album entered the British charts. Her cover of "Perhaps, Perhaps, Perhaps" was included in the soundtrack of the Australian film "Strictly Ballroom."

Day participated in celebrations of her birthday with an annual Doris Day music marathon.

She declined tribute offers from the American Film Institute and the Kennedy Center Honors because they both require that recipients attend in person. In 2004, she was awarded the Presidential Medal of Freedom by President George W. Bush for her achievements in the entertainment industry and for her work on behalf of animals. President Bush stated:

Columnist Liz Smith and film critic Rex Reed mounted vigorous campaigns to gather support for an Academy Honorary Award for Day. According to "The Hollywood Reporter", the academy had offered her the honorary Oscar multiple times, but she declined as she saw the film industry as a part of her past life. Day received a Grammy for Lifetime Achievement in Music in 2008, albeit again in absentia.

Day received Grammy Hall of Fame Awards in 1998, 1999 and 2012 for her recordings of "Sentimental Journey", "Secret Love" and "Que Sera, Sera", respectively. She was inducted into the Hit Parade Hall of Fame in 2007, and in 2010 received the first Legend Award presented by the Society of Singers.

At the age of 89, Day released "My Heart" in the United Kingdom on September 5, 2011, her first new album since the 1994 release of "The Love Album", which had been recorded in 1967. The album is a compilation of previously unreleased recordings produced by Day's son Terry Melcher. Tracks include the 1970s Joe Cocker hit "You Are So Beautiful", the Beach Boys' "Disney Girls" and jazz standards such as "My Buddy", which Day originally sang in the film "I'll See You in My Dreams" (1951).

In the U.S., the album reached No. 12 on Amazon's bestseller list and helped raise funds for the Doris Day Animal League. Day became the oldest artist to score a UK Top 10 with an album featuring new material.

In January 2012, the Los Angeles Film Critics Association presented Day with a Lifetime Achievement Award.

In April 2014, Day made an unexpected public appearance to attend the annual Doris Day Animal Foundation benefit.

Clint Eastwood offered Day a role in a film that he was planning to direct in 2015, but she eventually declined.

Day granted ABC a telephone interview on her birthday in 2016 that was accompanied by photos of her life and career.

During the filming of "The Man Who Knew Too Much", Day observed the mistreatment of animals in a marketplace scene and was inspired to act against animal abuse. She was so appalled at the conditions with which the animals used in filming were kept that she refused to work unless they received sufficient food and proper care. The production company erected feeding stations for the animals and fed them every day before Day would agree to return to work.

In 1971, she cofounded Actors and Others for Animals and appeared in a series of newspaper advertisements denouncing the wearing of fur along with Mary Tyler Moore, Angie Dickinson and Jayne Meadows.

In 1978, Day founded the Doris Day Pet Foundation, now the Doris Day Animal Foundation (DDAF). An independent nonprofit 501(c)(3) grant-giving public charity, DDAF funds other nonprofit causes that promote animal welfare.

To complement the Doris Day Animal Foundation, Day formed the Doris Day Animal League (DDAL) in 1987, a national nonprofit citizens' lobbying organization on behalf of animals. Day actively lobbied the United States Congress in support of legislation designed to safeguard animal welfare on a number of occasions, and in 1995 she originated the annual World Spay Day. The DDAL merged into the Humane Society of the United States (HSUS) in 2006.

The Doris Day Horse Rescue and Adoption Center, which helps abused and neglected horses, opened in 2011 in Murchison, Texas on the grounds of an animal sanctuary started by Day's late friend, author Cleveland Amory. Day contributed $250,000 toward the founding of the center.

A posthumous auction of 1,100 of Day's possessions in April 2020 generated $3 million for the Doris Day Animal Foundation.

Doris Day actively engaged in HIV/AIDS awareness for many years. Her commitment was primarily focused on raising awareness and fundraising for HIV/AIDS research. She co-organized several fundraising events for HIV/AIDS-related charities and provided financial contributions to research and support programs for individuals affected by the disease. In 2011, the Canadian magazine Gay Globe paid tribute to Doris Day by featuring her on the cover of their #79 edition.

Day's only child was music producer and songwriter Terry Melcher, who had a hit in the 1960s with "Hey Little Cobra" under the name the Rip Chords before becoming a successful producer whose acts included the Byrds, Paul Revere & the Raiders and the Beach Boys. In the late 1960s, Melcher became acquainted with Charles Manson and nearly signed him to a record deal. In August 1969, the Tate murders, orchestrated by Manson, were committed at the Benedict Canyon house that Melcher had formerly occupied. Melcher died of melanoma in November 2004.

Since the 1980s, Day owned a hotel in Carmel-by-the-Sea called the Cypress Inn, an early pet–friendly hotel that was featured in "Architectural Digest" in 1999.

Day was married four times. From April 1941 to February 1943, she was married to trombonist Al Jorden (1917–1967), whom she met in Barney Rapp's band. Jorden, a violent schizophrenic, committed suicide. When Day became pregnant and refused to have an abortion, he beat her in an attempt to force a miscarriage. Their son was born Terrence Paul Jorden in 1942, and he adopted the surname of Melcher when he was adopted by Day's third husband.

Her second marriage was to George William Weidler (1926–1989), a saxophonist and brother of actress Virginia Weidler, from March 30, 1946, to May 31, 1949. Weidler and Day met again several years later during a brief reconciliation and he introduced her to Christian Science.

Day married American film producer Martin Melcher (1915–1968), who produced many of her films, on April 3, 1951, her 29th birthday, and the marriage lasted until he died in April 1968. Melcher adopted Day's son Terry. As Day and Melcher were both Christian Scientists, she refused to visit a doctor for some time after experiencing symptoms that might have suggested cancer. Following Melcher's death, Day separated from the Church of Christ, Scientist and grew close to charismatic Protestants such as Kathryn Kuhlman, although she never lost interest in Christian Science teaching and practice.

Day's fourth marriage was to Barry Comden (1935–2009) from April 14, 1976, until April 2, 1982. He was the "maître d'hôtel" at one of Day's favorite restaurants. He knew of her great love of dogs and endeared himself to her by giving her a bag of meat scraps and bones as she left the restaurant. He later complained that Day cared more for her "animal friends" than for him.

After her retirement from films, Day lived in Carmel-by-the-Sea, California. She had many pets and adopted stray animals. She was a lifelong Republican.

In a rare interview with "The Hollywood Reporter" on April 4, 2019, the day after her 97th birthday, Day talked about her work on the Doris Day Animal Foundation, founded in 1978. Asked to name the favorite of her films, she answered with "Calamity Jane": "I was such a tomboy growing up, and she was such a fun character to play. Of course, the music was wonderful, too—'Secret Love,' especially, is such a beautiful song."

To commemorate her birthday, Day's fans gathered in late March each year for a three-day party in Carmel-by-the-Sea, California. The event was also a fundraiser for her animal foundation. During the 2019 event, there was a special screening of her film "Pillow Talk" (1959) to celebrate its 60th anniversary. Speaking about the film, Day stated that she "had such fun working with my pal, Rock. We laughed our way through three films we made together and remained great friends. I miss him."

Day died of pneumonia at her home in Carmel Valley, California, on May 13, 2019, at the age of 97. Her death was announced by the Doris Day Animal Foundation. As requested by Day, the foundation announced that there would be no funeral services, grave marker or other public memorials.



"Source"




Distillation

Distillation, also classical distillation, is the process of separating the component substances of a liquid mixture of two or more chemically discrete substances; the separation process is realized by way of the selective boiling of the mixture and the condensation of the vapors in a still.

Dry distillation (thermolysis and pyrolysis) is the heating of solid materials to produce gases that condense either into fluid products or into solid products. The term "dry distillation" includes the separation processes of destructive distillation and of chemical cracking, breaking down large hydrocarbon molecules into smaller hydrocarbon molecules. Moreover, a partial distillation results in partial separations of the mixture's components, which process yields nearly-pure components; partial distillation also realizes partial separations of the mixture to increase the concentrations of selected components. In either method, the separation process of distillation exploits the differences in the relative volatility of the component substances of the heated mixture.

In the industrial applications of classical distillation, the term "distillation" is used as a unit of operation that identifies and denotes a process of physical separation, not a chemical reaction; thus an industrial installation that produces distilled beverages, is a distillery of alcohol. These are some applications of the chemical separation process that is distillation:


Early evidence of distillation was found on Akkadian tablets dated describing perfumery operations. The tablets provided textual evidence that an early, primitive form of distillation was known to the Babylonians of ancient Mesopotamia.

Aristotle knew that water condensing from evaporating seawater is fresh:

Letting seawater evaporate and condense into freshwater is not distillation, for distillation involves boiling, but the experiment may have been an important step towards distillation.

Distillation was practiced in the ancient Indian subcontinent, which is evident from baked clay retorts and receivers found at Taxila, Shaikhan Dheri, and Charsadda in Pakistan and Rang Mahal in India dating to the early centuries of the Common Era. Frank Raymond Allchin says these terracotta distill tubes were "made to imitate bamboo". These "Gandhara stills" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat.

Distillation in China may have begun during the Eastern Han dynasty (1st–2nd century CE)

Early evidence of distillation has been found related to alchemists working in Alexandria in Roman Egypt in the 1st century CE.

Distilled water has been in use since at least , when Alexander of Aphrodisias described the process. Work on distilling other liquids continued in early Byzantine Egypt under Zosimus of Panopolis in the 3rd century.

Medieval Muslim chemists such as Jābir ibn Ḥayyān (Latin: Geber, ninth century) and Abū Bakr al-Rāzī (Latin: Rhazes, ) experimented extensively with the distillation of various substances.

The distillation of wine is attested in Arabic works attributed to al-Kindī () and to al-Fārābī (), and in the 28th book of al-Zahrāwī's (Latin: Abulcasis, 936–1013) ' (later translated into Latin as '). In the twelfth century, recipes for the production of " ("burning water", i.e., ethanol) by distilling wine with salt started to appear in a number of Latin works, and by the end of the thirteenth century it had become a widely known substance among Western European chemists. The works of Taddeo Alderotti (1223–1296) describe a method for concentrating alcohol involving repeated distillation through a water-cooled still, by which an alcohol purity of 90% could be obtained.

The fractional distillation of organic substances plays an important role in the works attributed to Jābir, such as in the ('The Book of Seventy'), translated into Latin by Gerard of Cremona () under the title . The Jabirian experiments with fractional distillation of animal and vegetable substances, and to a lesser degree also of mineral substances, is the main topic of the , an originally Arabic work falsely attributed to Avicenna that was translated into Latin and would go on to form the most important alchemical source for Roger Bacon ().

The distillation of beverages began in the Southern Song (10th–13th century) and Jin (12th–13th century) dynasties, according to archaeological evidence. A still was found in an archaeological site in Qinglong, Hebei province, China, dating back to the 12th century. Distilled beverages were common during the Yuan dynasty (13th–14th century).

In 1500, German alchemist Hieronymus Brunschwig published " ("The Book of the Art of Distillation out of Simple Ingredients"), the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published "The Art of Distillation", the first major English compendium on the practice, but it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.

As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle to act as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water, for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols, such as cognac, Scotch whisky, Irish whiskey, tequila, rum, cachaça, and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for use in the domestic production of flower water or essential oils.

Early forms of distillation involved batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists reportedly carried out as many as 500 to 600 distillations in order to obtain a pure compound.

In the early 19th century, the basics of modern techniques, including pre-heating and reflux, were developed. In 1822, Anthony Perrier developed one of the first continuous stills, and then, in 1826, Robert Stein improved that design to make his patent still. In 1830, Aeneas Coffey got a patent for improving the design even further. Coffey's continuous still may be regarded as the archetype of modern petrochemical units. The French engineer Armand Savalle developed his steam regulator around 1846. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation, and the same and subsequent years saw developments in this theme for oils and spirits.

With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods, such as the McCabe–Thiele method by Ernest Thiele and the Fenske equation. The first industrial plant in the United States to use distillation as a means of ocean desalination opened in Freeport, Texas in 1961 with the hope of bringing water security to the region.
The availability of powerful computers has allowed direct computer simulations of distillation columns.

The application of distillation can roughly be divided into four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate in the processing of beverages and herbs.

The main difference between laboratory scale distillation and industrial distillation are that laboratory scale distillation is often performed on a batch basis, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds, and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions, which are collected sequentially from most volatile to less volatile, with the bottoms – remaining least or non-volatile fraction – removed at the end. The still can then be recharged and the process repeated.

In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.

The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.

It is a misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure, allowing the vapors of each component to collect separately and purely. However, this does not occur, even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law and assume that vapor–liquid equilibria are attained.

Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up, a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.

Dalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. A mixture with a given composition has one boiling point at a given pressure when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.

An implication of one boiling point is that lighter components never cleanly "boil first". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and, thus, are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily vaporize also, albeit at a lower concentration in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch vaporizes, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling point, although this is because its composition changes: each intermediate mixture has its own, singular boiling point.

The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is when the vapor phase and liquid phase contain the same composition. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.

It is not possible to completely purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is vaporized and the other component, e.g., a salt, has zero partial pressure for practical purposes, the process is simpler.

Heating an ideal mixture of two volatile substances, A and B, with A having the higher volatility, or lower boiling point, in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid that contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid. The ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This, in turn, means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than in the starting liquid).

The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio of A : B in the distillate.

If the difference in vapour pressure between the two components A and B is large – generally expressed as the difference in boiling points – the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.

Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.

Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.

Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.

There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include:

Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a "still", consists at a minimum of a reboiler or "pot" in which the source material is heated, a condenser in which the heated vapor is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also ).

A completely sealed distillation apparatus could experience extreme and rapidly varying internal pressure, which could cause it to burst open at the joints. Therefore, some path is usually left open (for instance, at the receiving flask) to allow the internal pressure to equalize with atmospheric pressure. Alternatively, a vacuum pump may be used to keep the apparatus at a lower than atmospheric pressure. If the substances involved are air- or moisture-sensitive, the connection to the atmosphere can be made through one or more drying tubes packed with materials that scavenge the undesired air components, or through bubblers that provide a movable liquid barrier. Finally, the entry of undesired air components can be prevented by pumping a low but steady flow of suitable inert gas, like nitrogen, into the apparatus.

In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.

As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.

A cutaway schematic of a simple distillation operation is shown at right. The starting liquid 15 in the boiling flask 2 is heated by a combined hotplate and magnetic stirrer 13 via a silicone oil bath (orange, 14). The vapor flows through a short Vigreux column 3, then through a Liebig condenser 5, is cooled by water (blue) that circulates through ports 6 and 7. The condensed liquid drips into the receiving flask 8, sitting in a cooling bath (blue, 16). The adapter 10 has a connection 9 that may be fitted to a vacuum pump. The components are connected by ground glass joints.

For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.

As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors is determined once again by Raoult's law. Each vaporization-condensation cycle (called a "theoretical plate") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; "theoretical plate" is thus a concept rather than an accurate description.

More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.

Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.

Steam distillation of various aromatic herbs and flowers can result in two products: an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.

Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.

This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.

Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e., the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.

Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr apparatus is a kind of short path distillation method which often contains multiple chambers to collect distillate fractions.

Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.

The Perkin triangle has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.

Zone distillation is a distillation process in a long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with the number of iterations.
Zone distillation is the distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization – with the replacement of the distribution co-efficient k of crystallization - for the separation factor α of distillation.

Non-condensable gas can be expelled from the apparatus by the vapor of relatively volatile co-solvent, which spontaneously evaporates during initial pumping, and this can be achieved with regular oil or diaphragm pump.


The unit process of evaporation may also be called "distillation":

Other uses:

Interactions between the components of the solution create properties unique to the solution, as most processes entail non-ideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not result in separation. For example, 95.6% ethanol (by mass) in water forms an azeotrope at 78.1 °C.

If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a more pure distillate. These techniques are known as azeotropic distillation. Some techniques achieve this by "jumping" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.

Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.

High boiling azeotropes, such as a 20 percent by weight mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.

The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapor pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to remain identical further along the pressure axis to either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.

This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.

Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapors being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.

Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.

A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.

Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.

This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.

One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.

Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.

To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.

Industrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about and heights ranging from about or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different "fractions" or products having different boiling points or boiling ranges. The "lightest" products (those with the lowest boiling point) exit from the top of the columns and the "heaviest" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.
Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.

Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.
Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as "plates" or "trays") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.

In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.

This packing material can either be random or dumped packing ( wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of "theoretical stages" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both these factors affect packing performance.

Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work has been done on this topic by Fractionation Research, Inc. (commonly known as FRI).

The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m of water recovered figure and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m:

There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.

Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.




David Hilbert

David Hilbert (; ; 23 January 1862 – 14 February 1943) was a German mathematician and one of the most influential mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas including invariant theory, the calculus of variations, commutative algebra, algebraic number theory, the foundations of geometry, spectral theory of operators and its application to integral equations, mathematical physics, and the foundations of mathematics (particularly proof theory).

Hilbert adopted and defended Georg Cantor's set theory and transfinite numbers. In 1900, he presented a collection of problems that set a course for mathematical research of the 20th century.

Hilbert and his students contributed to establishing rigor and developed important tools used in modern mathematical physics. Hilbert was one of the founders of proof theory and mathematical logic.

Hilbert, the first of two children and only son of Otto, a county judge, and Maria Therese Hilbert (née Erdtmann), the daughter of a merchant, was born in the Province of Prussia, Kingdom of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth. His paternal grandfather was David Hilbert, a judge and "Geheimrat". His mother Maria had an interest in philosophy, astronomy and prime numbers, while his father Otto taught him Prussian virtues. After his father became a city judge, the family moved to Königsberg. David's sister, Elise, was born when he was six. He began his schooling aged eight, two years later than the usual starting age.

In late 1872, Hilbert entered the Friedrichskolleg Gymnasium ("Collegium fridericianum", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the "Albertina". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but had gone to Berlin for three semesters), returned to Königsberg and entered the university. Hilbert developed a lifelong friendship with the shy, gifted Minkowski.

In 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled "Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen" ("On the invariant properties of special binary forms, in particular the spherical harmonic functions").

Hilbert remained at the University of Königsberg as a "Privatdozent" (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen. During the Klein and Hilbert years, Göttingen became the preeminent institution in the mathematical world. He remained there for the rest of his life.

Among Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.

Among his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the "Mathematische Annalen", the leading mathematical journal of the time. He was elected an International Member of the United States National Academy of Sciences in 1907.

In 1892, Hilbert married Käthe Jerosch (1864–1945), who was the daughter of a Königsberg merchant, "an outspoken young lady with an independence of mind that matched [Hilbert's]." While at Königsberg they had their one child, (1893–1969).
Franz suffered throughout his life from mental illness, and after he was admitted into a psychiatric clinic, Hilbert said, "From now on, I must consider myself as not having a son." His attitude toward Franz brought Käthe considerable sorrow.

Hilbert considered the mathematician Hermann Minkowski to be his "best and truest friend".

Hilbert was baptized and raised a Calvinist in the Prussian Evangelical Church. He later left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other "a priori" assumptions. When Galileo Galilei was criticized for failing to stand up for his convictions on the Heliocentric theory, Hilbert objected: "But [Galileo] was not an idiot. Only an idiot could believe that scientific truth needs martyrdom; that may be necessary in religion, but scientific results prove themselves in due time."

Like Albert Einstein, Hilbert had closest contacts with the Berlin Group whose leading founders had studied under Hilbert in Göttingen (Kurt Grelling, Hans Reichenbach and Walter Dubislav).

Around 1925, Hilbert developed pernicious anemia, a then-untreatable vitamin deficiency whose primary symptom is exhaustion; his assistant Eugene Wigner described him as subject to "enormous fatigue" and how he "seemed quite old," and that even after eventually being diagnosed and treated, he "was hardly a scientist after 1925, and certainly not a Hilbert."

Hilbert was elected to the American Philosophical Society in 1932.

Hilbert lived to see the Nazis purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book "Grundlagen der Mathematik" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert–Ackermann book "Principles of Mathematical Logic" from 1928. Hermann Weyl's successor was Helmut Hasse.

About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether "the Mathematical Institute really suffered so much because of the departure of the Jews." Hilbert replied, "Suffered? It doesn't exist any longer, does it?"

By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world several months after he died.

The epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: ""Ignoramus et ignorabimus"" or "We do not know and we shall not know":

The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a round table discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem. Gödel's incompleteness theorems show that even elementary axiomatic systems such as Peano arithmetic are either self-contradicting or contain logical propositions that are impossible to prove or disprove within that system.

Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous "finiteness theorem". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. To solve what had become known in some circles as "Gordan's Problem", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated "Hilbert's basis theorem", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof—it did not display "an object"—but rather, it was an existence proof and relied on use of the law of excluded middle in an infinite extension.

Hilbert sent his results to the "Mathematische Annalen". Gordan, the house expert on the theory of invariants for the "Mathematische Annalen", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:

Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the "Annalen". After having read the manuscript, Klein wrote to him, saying:

Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:

For all his successes, the nature of his proof created more trouble than Hilbert could have imagined. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that "many different constructions are subsumed under one fundamental idea"—in other words (to quote Reid): "Through a proof of existence, Hilbert had been able to obtain a construction"; "the proof" (i.e. the symbols on the page) "was" "the object". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist "school", much to Hilbert's torment in his later years. Indeed, Hilbert would lose his "gifted pupil" Weyl to intuitionism—"Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert responded:

The text "Grundlagen der Geometrie" (tr.: "Foundations of Geometry") published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the "Grundlagen" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.

Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat "things", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.

Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.

Hilbert put forth a highly influential list consisting of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned as the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.

After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later "foundationalist" Russell–Whitehead or "encyclopedist" Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could engage in problems of which he had identified as crucial aspects of important areas of mathematics.

The problem set was launched as a talk "The Problems of Mathematics" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:

He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. See also Hilbert's twenty-fourth problem. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.

Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some continue to remain challenges.

The following are the headers for Hilbert's 23 problems as they appeared in the 1902 translation in the Bulletin of the American Mathematical Society.

In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.

In 1920, Hilbert proposed a research project in metamathematics that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done by showing that:


He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the ignorabimus, still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.

This program is still recognizable in the most popular philosophy of mathematics, where it is usually called "formalism". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.

Hilbert wrote in 1919:

Hilbert published his views on the foundations of mathematics in the 2-volume work, Grundlagen der Mathematik.

Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.

Gödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.

Nevertheless, the subsequent achievements of proof theory at the very least "clarified" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in the work of Alonzo Church and Alan Turing, also grew directly out of this "debate".

Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction. Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.

Until 1912, Hilbert was almost exclusively a pure mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar on the subject in 1905.

In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a "physics tutor" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.

By 1907, Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years to put the theory into its final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer, Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915, Einstein published several papers culminating in "The Field Equations of Gravitation" (see Einstein field equations). Nearly simultaneously, Hilbert published "The Foundations of Physics", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.

Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation, and his namesake Hilbert space plays an important part in quantum theory. In 1926, von Neumann showed that, if quantum states were understood as vectors in Hilbert space, they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.

Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher mathematics, physicists tended to be "sloppy" with it. To a pure mathematician like Hilbert, this was both ugly, and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found – most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic "Methoden der mathematischen Physik" ("Methods of Mathematical Physics") including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said "Physics is too hard for physicists", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.

Hilbert unified the field of algebraic number theory with his 1897 treatise "Zahlbericht" (literally "report on numbers"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.

He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.

Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.

His collected works ("Gesammelte Abhandlungen") have been published several times. The original versions of his papers contained "many technical errors of varying degree"; when the collection was first published, the errors were corrected and it was found that this could be done without major changes in the statements of the theorems, with one exception—a claimed proof of the continuum hypothesis. The errors were nonetheless so numerous and significant that it took Olga Taussky-Todd three years to make the corrections.






Down syndrome

Down syndrome or Down's syndrome, also known as trisomy 21, is a genetic disorder caused by the presence of all or part of a third copy of chromosome 21. It is usually associated with developmental delays, mild to moderate intellectual disability, and characteristic physical features. There are three types of Down syndrome, all with the same features: Trisomy 21, the most common type; Mosaic Down syndrome, and Translocation Down syndrome.
The parents of the affected individual are usually genetically normal. The probability increases from less than 0.1% in 20-year-old mothers to 3% in those of age 45. The extra chromosome is provided at conception as the egg and sperm combine. A very small percentage of 1-2% gets the additional chromosome in the embryo stage and it only impacts some of the cells in the body; this is known as Mosaic Down syndrome. Usually, babies get 23 chromosomes from each parent for a total of 46, whereas in Down syndrome, a third 21st chromosome is attached. It is believed to occur by chance, with no known behavioral activity or environmental factor that changes the probability. Down syndrome can be identified during pregnancy by prenatal screening, followed by diagnostic testing, or after birth by direct observation and genetic testing. Since the introduction of screening, Down syndrome pregnancies are often aborted (rates varying from 50 to 85% depending on maternal age, gestational age, and maternal race/ethnicity).
There is no cure for Down syndrome. Education and proper care have been shown to provide good quality of life. Some children with Down syndrome are educated in typical school classes, while others require more specialized education. Some individuals with Down syndrome graduate from high school, and a few attend post-secondary education. In adulthood, about 20% in the United States do paid work in some capacity, with many requiring a sheltered work environment. Support in financial and legal matters is often needed. Life expectancy is around 50 to 60 years in the developed world, with proper health care. Regular screening for health issues common in Down syndrome is recommended throughout the person's life.
Down syndrome is the most common chromosomal abnormality. It occurs in about 1 in 1,000 babies born each year. In the US this figure is given as one in 700 births. In 2015, Down syndrome was present in 5.4 million individuals globally and resulted in 27,000 deaths, down from 43,000 deaths in 1990. It is named after British physician John Langdon Down, who fully described the syndrome in 1866. Some aspects of the condition were described earlier by French psychiatrist Jean-Étienne Dominique Esquirol in 1838 and French physician Édouard Séguin in 1844. The genetic cause of Down syndrome was discovered in 1959.

English physician John Langdon Down first described Down syndrome in 1862, recognizing it as a distinct type of mental disability, and again in a more widely published report in 1866. Édouard Séguin described it as separate from cretinism in 1844. By the 20th century, Down syndrome had become the most recognizable form of mental disability.

Due to his perception that children with Down syndrome shared facial similarities with those of Blumenbach's Mongolian race, John Langdon Down used the term "mongoloid". He felt that the existence of Down syndrome confirmed that all peoples were genetically related. In the 1950s with discovery of the underlying cause as being related to chromosomes, concerns about the race-based nature of the name increased.

In 1961, a group of nineteen scientists suggested that "mongolism" had "misleading connotations" and had become "an embarrassing term". The World Health Organization (WHO) dropped the term in 1965 after a request by the delegation from the Mongolian People's Republic. While this racist terminology continued to be used until the late twentieth century, it is now considered unacceptable and is no longer in common use.

In antiquity, many infants with disabilities were either killed or abandoned. 
In June 2020, the earliest incidence of Down syndrome was found in genomic evidence from an infant that was buried before 3200 BC at Poulnabrone dolmen in Ireland.
Researchers believe that a number of historical pieces of art portray Down syndrome, including pottery from the pre-Columbian Tumaco-La Tolita culture in present-day Colombia and Ecuador, and the 16th-century painting "The Adoration of the Christ Child".

In the 20th century, many individuals with Down syndrome were institutionalized, few of the associated medical problems were treated, and most people died in infancy or early adulthood. With the rise of the eugenics movement, 33 of the then 48 U.S. states and several countries began programs of forced sterilization of individuals with Down syndrome and comparable degrees of disability. Action T4 in Nazi Germany saw the systematic murder of people with Down syndrome made public policy.

With the discovery of karyotype techniques in the 1950s it became possible to identify abnormalities of chromosomal number or shape. In 1959 Jérôme Lejeune reported the discovery that Down syndrome resulted from an extra chromosome. However, Lejeune's claim to the discovery has been disputed, and in 2014 the Scientific Council of the French Federation of Human Genetics unanimously awarded its Grand Prize to his colleague Marthe Gautier for her role in this discovery. The discovery took place in the laboratory of Raymond Turpin at the Hôpital Trousseau in Paris, France. Jérôme Lejeune and Marthe Gautier were both his students.

As a result of this discovery, the condition became known as trisomy 21. Even before the discovery of its cause, the presence of the syndrome in all races, its association with older maternal age, and its rarity of recurrence had been noticed. Medical texts had assumed it was caused by a combination of inheritable factors that had not been identified. Other theories had focused on injuries sustained during birth.

People with Down syndrome may have these physical characteristics: a small chin, epicanthic folds, low muscle tone, a flat nasal bridge, a single crease of the palm, and a protruding tongue. A protruding tongue is caused by low tone and weak facial muscles, and often corrected with myofunctional exercises. Some characteristic airway features can lead to obstructive sleep apnea in around half of those with Down syndrome. Other common features include: excessive joint flexibility, extra space between big toe and second toe, single palm lines and short fingers.

Instability of the atlantoaxial joint occurs in about 1-2%. Atlantoaxial instability may cause myelopathy due to cervical spinal cord compression later in life, this often manifests as new onset weakness, problems with coordination, bowel or bladder incontinence, and gait dysfunction. Serial imaging cannot reliably predict future cervical cord compression, but changes can be seen on neurological exam. The condition is surgically corrected with spine surgery.

Growth in height is slower, resulting in adults who tend to have short stature—the average height for men is 154 cm (5 ft 1 in), and for women is 142 cm (4 ft 8 in). Individuals with Down syndrome are at increased risk for obesity as they age due to hypothyroidism, other medical issues and lifestyle. Growth charts have been developed specifically for children with Down syndrome.

This syndrome causes about a third of cases of intellectual disability. Many developmental milestones are delayed with the ability to crawl typically occurring around 8–22 months rather than 6–12 months, and the ability to walk independently typically occurring around 1–4 years rather than 9–18 months. Walking is acquired in 50% of children after 24 months.

Most individuals with Down syndrome have mild (IQ: 50–69) or moderate (IQ: 35–50) intellectual disability with some cases having severe (IQ: 20–35) difficulties. Those with mosaic Down syndrome typically have IQ scores 10–30 points higher than that. As they age, the gap tends to widen between people with Down syndrome and their same-age peers.

Commonly, individuals with Down syndrome have better language understanding than ability to speak. Babbling typically emerges around 15 months on average. 10-45% of those with Down syndrome have either a stutter or rapid and irregular speech, making it difficult to understand them. After reaching 30 years of age, some may lose their ability to speak.

They typically do fairly well with social skills. Behavior problems are not generally as great an issue as in other syndromes associated with intellectual disability. In children with Down syndrome, mental illness occurs in nearly 30% with autism occurring in 5–10%. People with Down syndrome experience a wide range of emotions. While people with Down syndrome are generally happy, symptoms of depression and anxiety may develop in early adulthood.

Children and adults with Down syndrome are at increased risk of epileptic seizures, which occur in 5–10% of children and up to 50% of adults. This includes an increased risk of a specific type of seizure called infantile spasms. Many (15%) who live 40 years or longer develop Alzheimer's disease. In those who reach 60 years of age, 50–70% have the disease.

Hearing and vision disorders occur in more than half of people with Down syndrome. 

Vision problems occur in 38-80% of cases. Between 20 and 50% have strabismus, in which the two eyes do not move together. Cataracts (cloudiness of the lens of the eye) occur in 15%, and may be present at birth. Keratoconus (a thin, cone-shaped cornea) and glaucoma (increased eye pressure) are also more common, as are refractive errors requiring glasses or contacts. Brushfield spots (small white or grayish/brown spots on the outer part of the iris) are present in 38 to 85% of individuals.
Hearing problems are found in 50–90% of children with Down syndrome. This is often the result of otitis media with effusion which occurs in 50–70% and chronic ear infections which occur in 40-60%. Ear infections often begin in the first year of life and are partly due to poor eustachian tube function. Excessive ear wax can also cause hearing loss due to obstruction of the outer ear canal. Even a mild degree of hearing loss can have negative consequences for speech, language understanding, and academics. It is important to rule out hearing loss as a factor in social and cognitive deterioration. Age-related hearing loss of the sensorineural type occurs at a much earlier age and affects 10–70% of people with Down syndrome.

The rate of congenital heart disease in newborns with Down syndrome is around 40%. Of those with heart disease, about 80% have an atrial septal defect or ventricular septal defect with the former being more common. Congenital heart disease can also put individuals at a higher risk of pulmonary hypertension, where arteries in the lungs narrow and cause inadequate blood oxygenation. Some of the genetic contributions to pulmonary hypertension in individuals with Down Syndrome are abnormal lung development, endothelial dysfunction, and proinflammatory genes. Mitral valve problems become common as people age, even in those without heart problems at birth. Other problems that may occur include tetralogy of Fallot and patent ductus arteriosus. People with Down syndrome have a lower risk of hardening of the arteries.

Although the overall risk of cancer in Down syndrome is not changed, the risk of testicular cancer and certain blood cancers, including acute lymphoblastic leukemia (ALL) and acute megakaryoblastic leukemia (AMKL) is increased while the risk of other non-blood cancers is decreased. People with Down syndrome are believed to have an increased risk of developing cancers derived from germ cells whether these cancers are blood- or non-blood-related. In 2008, the World Health Organization (WHO) introduced a distinct classification for myeloid proliferation in individuals with Down syndrome.

Leukemia is 10 to 15 times more common in children with Down syndrome. In particular, acute lymphoblastic leukemia is 20 times more common and the megakaryoblastic form of acute myeloid leukemia (acute megakaryoblastic leukemia), is 500 times more common. Acute megakaryoblastic leukemia (AMKL) is a leukemia of megakaryoblasts, the precursors cells to megakaryocytes which form blood platelets. Acute lymphoblastic leukemia in Down syndrome accounts for 1–3% of all childhood cases of ALL. It occurs most often in those older than nine years or having a white blood cell count greater than 50,000 per microliter and is rare in those younger than one year old. ALL in Down syndrome tends to have poorer outcomes than other cases of ALL in people without Down syndrome. In short, the likelihood of developing Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL) is higher in children with Down syndrome compared to those without Down syndrome.

Myeloid leukemia typically precedes Down syndrome and is accompanied by a condition known as Transient Abnormal Myelopoiesis (TAM), which generally disrupts the differentiation of megakaryocytes and erythrocytes. In Down syndrome, AMKL is typically preceded by transient myeloproliferative disease (TMD), a disorder of blood cell production in which non-cancerous megakaryoblasts with a mutation in the "GATA1" gene rapidly divide during the later period of pregnancy. GATA1 mutations combined with Trisomy 21 contribute to a predisposition to Transient Abnormal Myelopoiesis (TAM). In Trisomy 21, the process of leukemogenesis starts in early fetal life, with genetic factors, including GATA1 mutations, contributing to the development of Transient Abnormal Myelopoiesis (TAM) on the preleukemic pathway. The condition affects 3–10% of babies with Down. While it often spontaneously resolves within three months of birth, it can cause serious blood, liver, or other complications. In about 10% of cases, TMD progresses to AMKL during the three months to five years following its resolution.

People with Down syndrome have a lower risk of all major solid cancers, including those of lung, breast, and cervix, with the lowest relative rates occurring in those aged 50 years or older. This low risk is thought to be due to an increase in the expression of tumor suppressor genes present on chromosome 21. One exception is testicular germ cell cancer which occurs at a higher rate in Down syndrome.

Problems of the thyroid gland occur in 20–50% of individuals with Down syndrome. Low thyroid is the most common form, occurring in almost half of all individuals. Thyroid problems can be due to a poorly or nonfunctioning thyroid at birth (known as congenital hypothyroidism) which occurs in 1% or can develop later due to an attack on the thyroid by the immune system resulting in Graves' disease or autoimmune hypothyroidism. Type 1 diabetes mellitus is also more common.

Constipation occurs in nearly half of people with Down syndrome and may result in changes in behavior. One potential cause is Hirschsprung's disease, occurring in 2–15%, which is due to a lack of nerve cells controlling the colon. Other congenital problems can include duodenal atresia, imperforate anus and gastroesophageal reflux disease. Celiac disease affects about 7–20%

People with Down syndrome tend to be more susceptible to gingivitis as well as early, severe periodontal disease, necrotising ulcerative gingivitis, and early tooth loss, especially in the lower front teeth. While plaque and poor oral hygiene are contributing factors, the severity of these periodontal diseases cannot be explained solely by external factors. Research suggests that the severity is likely a result of a weakened immune system. The weakened immune system also contributes to increased incidence of yeast infections in the mouth (from "Candida albicans").

People with Down syndrome also tend to have a more alkaline saliva resulting in a greater resistance to tooth decay, despite decreased quantities of saliva, less effective oral hygiene habits, and higher plaque indexes.

Higher rates of tooth wear and bruxism are also common. Other common oral manifestations of Down syndrome include enlarged hypotonic tongue, crusted and hypotonic lips, mouth breathing, narrow palate with crowded teeth, class III malocclusion with an underdeveloped maxilla and posterior crossbite, delayed exfoliation of baby teeth and delayed eruption of adult teeth, shorter roots on teeth, and often missing and malformed (usually smaller) teeth. Less common manifestations include cleft lip and palate and enamel hypocalcification (20% prevalence).

Taurodontism, an elongation of the pulp chamber, has a high prevalence in people with DS.

Males with Down syndrome usually do not father children, while females have lower rates of fertility relative to those who are unaffected. Fertility is estimated to be present in 30–50% of females. Menopause usually occurs at an earlier age. The poor fertility in males is thought to be due to problems with sperm development; however, it may also be related to not being sexually active. As of 2006, three instances of males with Down syndrome fathering children and 26 cases of females having children have been reported. Without assisted reproductive technologies, around half of the children of someone with Down syndrome will also have the syndrome.

Those with Down syndrome nearly always have physical and intellectual disabilities. As adults, their mental abilities are typically similar to those of an 8- or 9-year-old. At the same time, their emotional and social awareness is very high. They can have poor immune function and generally reach developmental milestones at a later age. They have an increased risk of a number of health concerns, such as congenital heart defect, epilepsy, leukemia, and thyroid diseases.

The cause of the extra full or partial chromosome is still unknown. Most of the time, Down syndrome is caused by a random mistake in cell division during early development of the fetus, but not inherited, and there is no scientific research which shows that environmental factors or the parents' activities contribute to Down syndrome. The only factor that has been linked to the increased chance of having a baby with Down syndrome is advanced parental age. This is mostly associated with advanced maternal age but about 10 per cent of cases are associated with advanced paternal age. 
Down syndrome is caused by having three copies of the genes on chromosome 21, rather than the usual two. The parents of the affected individual are typically genetically normal. Those who have one child with Down syndrome have about a 1% possibility of having a second child with the syndrome, if both parents are found to have normal karyotypes.

The extra chromosome content can arise through several different ways. The most common cause (about 92–95% of cases) is a complete extra copy of chromosome 21, resulting in trisomy 21. In 1–2.5% of cases, some of the cells in the body are normal and others have trisomy 21, known as mosaic Down syndrome. The other common mechanisms that can give rise to Down syndrome include: a Robertsonian translocation, isochromosome, or ring chromosome. These contain additional material from chromosome 21 and occur in about 2.5% of cases. An isochromosome results when the two long arms of a chromosome separate together rather than the long and short arm separating together during egg or sperm development.

Down syndrome (also known by the karyotype 47,XX,+21 for females and 47,XY,+21 for males) is mostly caused by a failure of the 21st chromosome to separate during egg or sperm development, known as nondisjunction. As a result, a sperm or egg cell is produced with an extra copy of chromosome 21; this cell thus has 24 chromosomes. When combined with a normal cell from the other parent, the baby has 47 chromosomes, with three copies of chromosome 21. About 88% of cases of trisomy 21 result from nonseparation of the chromosomes in the mother, 8% from nonseparation in the father, and 3% after the egg and sperm have merged.

Mosaic Down syndrome is diagnosed when there is a mixture of two types of cells: some cells have three copies of chromosome 21 but some cells have the typical two copies of chromosome 21. This type is the least common form of Down syndrome and accounts for only about 1% of all cases. Children with mosaic Down syndrome may have the same features as other children with Down syndrome. However, they may have fewer characteristics of the condition due to the presence of some (or many) cells with a typical number of chromosomes.

The extra chromosome 21 material may also occur due to a Robertsonian translocation in 2–4% of cases. In this translocation Down syndrome, the long arm of chromosome 21 is attached to another chromosome, often chromosome 14. In a male affected with Down syndrome, it results in a karyotype of 46XY,t(14q21q). This may be a new mutation or previously present in one of the parents. The parent with such a translocation is usually normal physically and mentally; however, during production of egg or sperm cells, a higher chance of creating reproductive cells with extra chromosome 21 material exists. This results in a 15% chance of having a child with Down syndrome when the mother is affected and a less than 5% probability if the father is affected. The probability of this type of Down syndrome is not related to the mother's age. Some children without Down syndrome may inherit the translocation and have a higher probability of having children of their own with Down syndrome. In this case it is sometimes known as "familial" Down syndrome.

The extra genetic material present in Down syndrome results in overexpression of a portion of the 310 genes located on chromosome 21. This overexpression has been estimated at 50%, due to the third copy of the chromosome present. Some research has suggested the Down syndrome critical region is located at bands 21q22.1–q22.3, with this area including genes for the amyloid precursor protein, superoxide dismutase, and likely the ETS2 proto oncogene. Other research, however, has not confirmed these findings. MicroRNAs are also proposed to be involved.

The dementia that occurs in Down syndrome is due to an excess of amyloid beta peptide produced in the brain and is similar to Alzheimer's disease, which also involves amyloid beta build-up. Amyloid beta is processed from amyloid precursor protein, the gene for which is located on chromosome 21. Senile plaques and neurofibrillary tangles are present in nearly all by 35 years of age, though dementia may not be present. It is hypothesized that those with Down syndrome lack a normal number of lymphocytes and produce less antibodies which is said to present an increased risk of infection.

Down syndrome is associated with an increased risk of some chronic diseases that are typically associated with older age such as Alzheimer's disease. It is believed that accelerated aging occurs and increases the biological age of tissues, but molecular evidence for this hypothesis is sparse. According to a biomarker of tissue age known as epigenetic clock, it is hypothesized that trisomy 21 increases the age of blood and brain tissue (on average by 6.6 years).

Guidelines recommend screening for Down syndrome to be offered to all pregnant women, regardless of age. A number of tests are used, with varying levels of accuracy. They are typically used in combination to increase the detection rate. None can be definitive, thus if screening is positive, either amniocentesis or chorionic villus sampling is required to confirm the diagnosis.

When screening tests predict a high possibility of Down syndrome, a more invasive diagnostic test (amniocentesis or chorionic villus sampling) is needed to confirm the diagnosis.

Prenatal ultrasound can be used to screen for Down syndrome. Findings that indicate increased chances when seen at 14 to 24 weeks of gestation include a small or no nasal bone, large ventricles, nuchal fold thickness, and an abnormal right subclavian artery, among others. The presence or absence of many markers is more accurate. Increased fetal nuchal translucency (NT) indicates an increased possibility of Down syndrome picking up 75–80% of cases and being falsely positive in 6%.

Several blood markers can be measured to predict the chances of Down syndrome during the first or second trimester. Testing in both trimesters is sometimes recommended and test results are often combined with ultrasound results. In the second trimester, often two or three tests are used in combination with two or three of: α-fetoprotein, unconjugated estriol, total hCG, and free βhCG detecting about 60–70% of cases.

Testing of the mother's blood for fetal DNA is being studied and appears promising in the first trimester. The International Society for Prenatal Diagnosis considers it a reasonable screening option for those women whose pregnancies are at a high likelihood of trisomy 21. Accuracy has been reported at 98.6% in the first trimester of pregnancy. Confirmatory testing by invasive techniques (amniocentesis, CVS) is still required to confirm the screening result.

For combinations of ultrasonography and non-genetic blood tests, screening in both the first and second trimesters is better than just screening in the first trimester. The different screening techniques in use are able to pick up 90–95% of cases, with a false-positive rate of 2–5%. If Down syndrome occurs in one in 500 pregnancies with a 90% detection rate and the test used has a 5% false-positive rate, this means, of 20 women who test positive on screening, only one will not have a fetus with Down syndrome confirmed. If the screening test has a 2% false-positive rate, this means, of 50 women who test positive on screening, one will not have a fetus with Down syndrome.

Amniocentesis and chorionic villus sampling are more reliable tests, but they increase the risk of miscarriage by between 0.5-1%. The risk of limb problems may be increased in the offspring if chorionic villus sampling is performed before 10 weeks. 

The risk from the procedure is greater the earlier it is performed, thus amniocentesis is not recommended before 15 weeks gestational age and chorionic villus sampling before 10 weeks gestational age.

About 92% of pregnancies in Europe with a diagnosis of Down syndrome are terminated. As a result, there is almost no one with Down syndrome in Iceland and Denmark, where screening is commonplace. In the United States, the termination rate after diagnosis is around 75%, but varies from 61 to 93%, depending on the population surveyed. Rates are lower among women who are younger and have decreased over time. When asked if they would have a termination if their fetus tested positive, 23–33% said yes, when high-risk pregnant women were asked, 46–86% said yes, and when women who screened positive are asked, 89–97% say yes.

The diagnosis can often be suspected based on the child's physical appearance at birth. Informing the parents of a diagnosis needs to be made as soon as possible, with care and sensitivity. Even an uncertain diagnosis needs to be informed of in the same way. This allows for a longer time for processing the information.

An analysis of the child's chromosomes is needed to confirm the diagnosis, and to determine if a translocation is present, as this may help determine the chances of the child's parents having further children with Down syndrome.

Efforts such as early childhood intervention, therapies, screening for common medical issues, a good family environment, and work-related training can improve the development of children with Down syndrome and provide good quality of life. Common therapies utilized include physical therapy, occupational therapy and speech therapy. Education and proper care can provide a positive quality of life. Typical childhood vaccinations are recommended.

A number of health organizations have issued recommendations for screening those with Down syndrome for particular diseases. This is recommended to be done systematically.

At birth, all children should get an electrocardiogram and ultrasound of the heart. Surgical repair of heart problems may be required as early as three months of age. Heart valve problems may occur in young adults, and further ultrasound evaluation may be needed in adolescents and in early adulthood. Due to the elevated risk of testicular cancer, some recommend checking the person's testicles yearly.

Some people with Down syndrome experience hearing loss. In this instance, hearing aids or other amplification devices can be useful for language learning. Speech therapy may be useful and is recommended to be started around nine months of age. As those with Down syndrome typically have good hand-eye coordination, learning sign language is a helpful communication tool. Augmentative and alternative communication methods, such as pointing, body language, objects, or pictures, are often used to help with communication. Behavioral issues and mental illness are typically managed with counseling or medications.

Education programs before reaching school age may be useful. School-age children with Down syndrome may benefit from inclusive education (whereby students of differing abilities are placed in classes with their peers of the same age), provided some adjustments are made to the curriculum. In the United States, the Individuals with Disabilities Education Act of 1975 requires public schools generally to allow attendance by students with Down syndrome.
Individuals with Down syndrome may learn better visually. Drawing may help with language, speech, and reading skills. Children with Down syndrome still often have difficulty with sentence structure and grammar, as well as developing the ability to speak clearly. Several types of early intervention can help with cognitive development. Efforts to develop motor skills include physical therapy, speech and language therapy, and occupational therapy. Physical therapy focuses specifically on motor development and teaching children to interact with their environment. Speech and language therapy can help prepare for later language. Lastly, occupational therapy can help with skills needed for later independence.

Tympanostomy tubes are often needed and often more than one set during the person's childhood. Tonsillectomy is also often done to help with sleep apnea and throat infections. Surgery does not correct every instance of sleep apnea and a continuous positive airway pressure (CPAP) machine may be useful in those cases.

Efforts to prevent respiratory syncytial virus (RSV) infection with human monoclonal antibodies should be considered, especially in those with heart problems. In those who develop dementia there is no evidence for memantine, donepezil, rivastigmine, or galantamine.

Between 5-15% of children with Down syndrome in Sweden attend regular school. Some graduate from high school; however, most do not. Of those with intellectual disability in the United States who attended high school about 40% graduated. Many learn to read and write and some are able to do paid work. In adulthood about 20% in the United States do paid work in some capacity. In Sweden, however, less than 1% have regular jobs. Many are able to live semi-independently, but they often require help with financial, medical, and legal matters. Those with mosaic Down syndrome usually have better outcomes.

Individuals with Down syndrome have a higher risk of early death than the general population. This is most often from heart problems or infections. Following improved medical care, particularly for heart and gastrointestinal problems, the life expectancy has increased. This increase has been from 12 years in 1912, to 25 years in the 1980s, to 50 to 60 years in the developed world in the 2000s. Data collected between the 1985–2003 showed between 4-12% infants with Down syndrome die in the first year of life. The probability of long-term survival is partly determined by the presence of heart problems. From research at the turn of the century, it tracked those with congenital heart problems, showing 60% survived to at least 10 years and 50% survived to at least 30 years of age. The research failed to track further aging beyond 30 years. In those without heart problems, 85% studied survived to at least 10 years and 80% survived to at least 30 years of age. It is estimated that 10% lived to 70 years of age in the early 2000s. Much of this data is outdated and life expectancy has drastically improved with more equitable healthcare and continuous advancement of surgical practice. The National Down Syndrome Society provides information regarding raising a child with Down syndrome.

Down syndrome is the most common chromosomal abnormality in humans. Globally, , Down syndrome occurs in about 1 per 1,000 births and results in about 17,000 deaths. More children are born with Down syndrome in countries where abortion is not allowed and in countries where pregnancy more commonly occurs at a later age. About 1.4 per 1,000 live births in the United States and 1.1 per 1,000 live births in Norway are affected. In the 1950s, in the United States, it occurred in 2 per 1,000 live births with the decrease since then due to prenatal screening and abortions. The number of pregnancies with Down syndrome is more than two times greater with many spontaneously aborting. It is the cause of 8% of all congenital disorders.

Maternal age affects the chances of having a pregnancy with Down syndrome. At age 20, the chance is 1 in 1,441; at age 30, it is 1 in 959; at age 40, it is 1 in 84; and at age 50 it is 1 in 44. Although the probability increases with maternal age, 70% of children with Down syndrome are born to women 35 years of age and younger, because younger people have more children. The father's older age is also a risk factor in women older than 35, but not in women younger than 35, and may partly explain the increase in risk as women age.

Down syndrome is named after John Langdon Down. He was the first person to provide an accurate description of the syndrome. His research that was published in 1866 earned him the recognition as the Father of the syndrome. While others had previously recognized components of the condition, John Langdon Down described the syndrome as a distinct, unique medical condition.

In 1975, the United States National Institutes of Health (NIH) convened a conference to standardize the naming and recommended replacing the possessive form, "Down's syndrome" with "Down syndrome". However, both the possessive and nonpossessive forms remain in use by the general population. The term "trisomy 21" is also commonly used.

Obstetricians routinely offer antenatal screenings for various conditions, including Down syndrome. As a medically reasonable procedure, requiring informed consent, people should be given information about it. It will then be the woman's choice, based on her personal beliefs, how much or how little screening she wishes. When results from testing become available, it is considered an ethical requirement to share the results with the patient. 

Some bioethicists deem it reasonable for parents to select a child who would have the highest well-being. One criticism of this reasoning is that it often values those with disabilities less. Some parents argue that Down syndrome should not be prevented or cured and that eliminating Down syndrome amounts to genocide. The disability rights movement does not have a position on screening, although some members consider testing and abortion discriminatory. Some in the United States who are anti-abortion support abortion if the fetus is disabled, while others do not. Of a group of 40 mothers in the United States who have had one child with Down syndrome, half agreed to screening in the next pregnancy.

Within the US, some Protestant denominations see abortion as acceptable when a fetus has Down syndrome while Orthodox Christianity and Roman Catholicism do not. Some of those against screening refer to it as a form of eugenics. Disagreement exists within Islam regarding the acceptability of abortion in those carrying a fetus with Down syndrome. Some Islamic countries allow abortion, while others do not. Parents may be stigmatized whichever decision they make.

Advocacy groups for individuals with Down syndrome began to be formed after the Second World War. These were organizations advocating for the inclusion of people with Down syndrome into the general school system and for a greater understanding of the condition among the general population, as well as groups providing support for families with children living with Down syndrome. Before this individuals with Down syndrome were often placed in mental hospitals or asylums. Organizations included the Royal Society for Handicapped Children and Adults founded in the UK in 1946 by Judy Fryd, Kobato Kai founded in Japan in 1964, the National Down Syndrome Congress founded in the United States in 1973 by Kathryn McGee and others, and the National Down Syndrome Society founded in 1979 in the United States. The first Roman Catholic order of nuns for women with Down Syndrome, Little Sisters Disciples of the Lamb, was founded in 1985 in France.

The first World Down Syndrome Day was held on 21 March 2006. The day and month were chosen to correspond with 21 and trisomy, respectively. It was recognized by the United Nations General Assembly in 2011.

Efforts are underway to determine how the extra chromosome 21 material causes Down syndrome, as currently this is unknown, and to develop treatments to improve intelligence in those with the syndrome. Two efforts being studied are the use stem cells and gene therapy. Other methods being studied include the use of antioxidants, gamma secretase inhibition, adrenergic agonists, and memantine. Research is often carried out on an animal model, the Ts65Dn mouse.

Down syndrome may also occur in hominids other than humans. In great apes chromosome 22 corresponds to the human chromosome 21 and thus trisomy 22 causes Down syndrome in apes. The condition was observed in a common chimpanzee in 1969 and a Bornean orangutan in 1979, but neither lived very long. The common chimpanzee Kanako (born around 1993, in Japan) has become the longest-lived known example of this condition. Kanako has some of the same symptoms that are common in human Down syndrome. It is unknown how common this condition is in chimps, but it is plausible it could be roughly as common as Down syndrome is in humans.

Individuals

Television and film




Dyslexia

Dyslexia, previously known as word blindness, is a learning disability ('learning difficulty' in the UK) that affects either reading or writing. Different people are affected to different degrees. Problems may include difficulties in spelling words, reading quickly, writing words, "sounding out" words in the head, pronouncing words when reading aloud and understanding what one reads. Often these difficulties are first noticed at school. The difficulties are involuntary, and people with this disorder have a normal desire to learn. People with dyslexia have higher rates of attention deficit hyperactivity disorder (ADHD), developmental language disorders, and difficulties with numbers.
Dyslexia is believed to be caused by the interaction of genetic and environmental factors. Some cases run in families. Dyslexia that develops due to a traumatic brain injury, stroke, or dementia is sometimes called "acquired dyslexia" or alexia. The underlying mechanisms of dyslexia result from differences within the brain's language processing. Dyslexia is diagnosed through a series of tests of memory, vision, spelling, and reading skills. Dyslexia is separate from reading difficulties caused by hearing or vision problems or by insufficient teaching or opportunity to learn.
Treatment involves adjusting teaching methods to meet the person's needs. While not curing the underlying problem, it may decrease the degree or impact of symptoms. Treatments targeting vision are not effective. Dyslexia is the most common learning disability and occurs in all areas of the world. It affects 3–7% of the population; however, up to 20% of the general population may have some degree of symptoms. While dyslexia is more often diagnosed in boys, this is partly explained by a self-fulfilling referral bias among teachers and professionals. It has even been suggested that the condition affects men and women equally. Some believe that dyslexia is best considered as a different way of learning, with both benefits and downsides.

Dyslexia is divided into developmental and acquired forms. Acquired dyslexia occurs subsequent to neurological insult, such as traumatic brain injury or stroke. People with acquired dyslexia exhibit some of the signs or symptoms of the developmental disorder, but require different assessment strategies and treatment approaches. "Pure alexia", also known as "agnosic alexia" or "pure word blindness", is one form of alexia which makes up "the peripheral dyslexia" group.

In early childhood, symptoms that correlate with a later diagnosis of dyslexia include delayed onset of speech and a lack of phonological awareness. A common myth closely associates dyslexia with mirror writing and reading letters or words backwards. These behaviors are seen in many children as they learn to read and write, and are not considered to be defining characteristics of dyslexia.

School-age children with dyslexia may exhibit signs of difficulty in identifying or generating rhyming words, or counting the number of syllables in words—both of which depend on phonological awareness. They may also show difficulty in segmenting words into individual sounds (such as sounding out the three sounds of "k", "a", and "t" in "cat") or may struggle to blend sounds, indicating reduced phonemic awareness.

Difficulties with word retrieval or naming things is also associated with dyslexia. People with dyslexia are commonly poor spellers, a feature sometimes called "dysorthographia" or "dysgraphia", which depends on the skill of orthographic coding.

Problems persist into adolescence and adulthood and may include difficulties with summarizing stories, memorization, reading aloud, or learning foreign languages. Adults with dyslexia can often read with good comprehension, though they tend to read more slowly than others without a learning difficulty and perform worse in spelling tests or when reading nonsense words—a measure of phonological awareness.

Dyslexia often co-occurs with other learning disorders, but the reasons for this comorbidity have not been clearly identified. These associated disabilities include:


Researchers have been trying to find the neurobiological basis of dyslexia since the condition was first identified in 1881. For example, some have tried to associate the common problem among people with dyslexia of not being able to see letters clearly to abnormal development of their visual nerve cells.

Neuroimaging techniques, such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), have shown a correlation between both functional and structural differences in the brains of children with reading difficulties. Some people with dyslexia show less activation in parts of the left hemisphere of the brain involved with reading, such as the inferior frontal gyrus, inferior parietal lobule, and the middle and ventral temporal cortex. Over the past decade, brain activation studies using PET to study language have produced a breakthrough in the understanding of the neural basis of language. Neural bases for the visual lexicon and for auditory verbal short-term memory components have been proposed, with some implication that the observed neural manifestation of developmental dyslexia is task-specific (i.e., functional rather than structural). fMRIs of people with dyslexia indicate an interactive role of the cerebellum and cerebral cortex as well as other brain structures in reading.

The cerebellar theory of dyslexia proposes that impairment of cerebellum-controlled muscle movement affects the formation of words by the tongue and facial muscles, resulting in the fluency problems that some people with dyslexia experience. The cerebellum is also involved in the automatization of some tasks, such as reading. The fact that some children with dyslexia have motor task and balance impairments could be consistent with a cerebellar role in their reading difficulties. However, the cerebellar theory has not been supported by controlled research studies.

Research into potential genetic causes of dyslexia has its roots in post-autopsy examination of the brains of people with dyslexia. Observed anatomical differences in the language centers of such brains include microscopic cortical malformations known as ectopias, and more rarely, vascular micro-malformations, and microgyrus—a smaller than usual size for the gyrus. The previously cited studies and others suggest that abnormal cortical development, presumed to occur before or during the sixth month of fetal brain development, may have caused the abnormalities. Abnormal cell formations in people with dyslexia have also been reported in non-language cerebral and subcortical brain structures. Several genes have been associated with dyslexia, including DCDC2 and KIAA0319 on chromosome 6, and DYX1C1 on chromosome 15.

The contribution of gene–environment interaction to reading disability has been intensely studied using twin studies, which estimate the proportion of variance associated with a person's environment and the proportion associated with their genes. Both environmental and genetic factors appear to contribute to reading development. Studies examining the influence of environmental factors such as parental education and teaching quality have determined that genetics have greater influence in supportive, rather than less optimal, environments. However, more optimal conditions may just allow those genetic risk factors to account for more of the variance in outcome because the environmental risk factors have been minimized.

As environment plays a large role in learning and memory, it is likely that epigenetic modifications play an important role in reading ability. Measures of gene expression, histone modifications, and methylation in the human periphery are used to study epigenetic processes; however, all of these have limitations in the extrapolation of results for application to the human brain.

The orthographic complexity of a language directly affects how difficult it is to learn to read it. English and French have comparatively "deep" phonemic orthographies within the Latin alphabet writing system, with complex structures employing spelling patterns on several levels: letter-sound correspondence, syllables, and morphemes. Languages such as Spanish, Italian and Finnish primarily employ letter-sound correspondence—so-called "shallow" orthographies—which makes them easier to learn for people with dyslexia. Logographic writing systems, such as Chinese characters, have extensive symbol use; and these also pose problems for dyslexic learners.

For most people who are right-hand dominant, the left hemisphere of their brain is more specialized for language processing. With regard to the mechanism of dyslexia, fMRI studies suggest that this specialization is less pronounced or absent in people with dyslexia. In other studies, dyslexia is correlated with anatomical differences in the corpus callosum, the bundle of nerve fibers that connects the left and right hemispheres.

Data via diffusion tensor MRI indicate changes in connectivity or in gray matter density in areas related to reading and language. Finally, the left inferior frontal gyrus has shown differences in phonological processing in people with dyslexia. Neurophysiological and imaging procedures are being used to ascertain phenotypic characteristics in people with dyslexia, thus identifying the effects of dyslexia-related genes.

The dual-route theory of reading aloud was first described in the early 1970s. This theory suggests that two separate mental mechanisms, or cognitive routes, are involved in reading aloud. One mechanism is the lexical route, which is the process whereby skilled readers can recognize known words by sight alone, through a "dictionary" lookup procedure. The other mechanism is the nonlexical or sublexical route, which is the process whereby the reader can "sound out" a written word. This is done by identifying the word's constituent parts (letters, phonemes, graphemes) and applying knowledge of how these parts are associated with each other, for example, how a string of neighboring letters sound together. The dual-route system could explain the different rates of dyslexia occurrence between different languages (e.g., the consistency of phonological rules in the Spanish language could account for the fact that Spanish-speaking children show a higher level of performance in non-word reading, when compared to English-speakers).

Dyslexia is a heterogeneous, dimensional learning disorder that impairs accurate and fluent word reading and spelling. Typical—but not universal—features include difficulties with phonological awareness; inefficient and often inaccurate processing of sounds in oral language ("phonological processing"); and verbal working memory deficits.

Dyslexia is a neurodevelopmental disorder, subcategorized in diagnostic guides as a "learning disorder with impairment in reading" (ICD-11 prefixes "developmental" to "learning disorder"; DSM-5 uses "specific"). Dyslexia is not a problem with intelligence. Emotional problems often arise secondary to learning difficulties. The National Institute of Neurological Disorders and Stroke describes dyslexia as "difficulty with phonological processing (the manipulation of sounds), spelling, and/or rapid visual-verbal responding".

The British Dyslexia Association defines dyslexia as "a learning difficulty that primarily affects the skills involved in accurate and fluent word reading and spelling" and is characterized by "difficulties in phonological awareness, verbal memory and verbal processing speed". "Phonological awareness" enables one to identify, discriminate, remember (working memory), and mentally manipulate the sound structures of language—phonemes, onsite-rime segments, syllables, and words.

The following can be done to assess for dyslexia:

Apply a multidisciplinary team approach involving the child's parent(s) and teacher(s), school psychologist, pediatrician, and, as appropriate, speech and language pathologist (speech therapist), and occupational therapist.

Gain familiarity with typical ages children reach various general developmental milestones, and domain-specific milestones, such as phonological awareness (recognizing rhyming words; identifying the initial sounds in words).

Do not rely on tests exclusively. Careful observation of the child in the school and home environments, and sensitive, comprehensive parental interviews are just as important as tests.

Look at the empirically supported response to intervention (RTI) approach, which "... involves monitoring the progress of a group of children through a programme of intervention rather than undertaking a static assessment of their current skills. Children with the most need are those who fail to respond to effective teaching, and they are readily identified using this approach."

There is a wide range of tests that are used in clinical and educational settings to evaluate the possibility of dyslexia. If initial testing suggests that a person might have dyslexia, such tests are often followed up with a full diagnostic assessment to determine the extent and nature of the disorder. Some tests can be administered by a teacher or computer; others require specialized training and are given by psychologists. Some test results indicate how to carry out teaching strategies. Because a variety of different cognitive, behavioral, emotional, and environmental factors all could contribute to difficultly learning to read, a comprehensive evaluation should consider these different possibilities. These tests and observations can include:

Screening procedures seek to identify children who show signs of possible dyslexia. In the preschool years, a family history of dyslexia, particularly in biological parents and siblings, predicts an eventual dyslexia diagnosis better than any test. In primary school (ages 5–7), the ideal screening procedure consists of training primary school teachers to carefully observe and record their pupils' progress through the phonics curriculum, and thereby identify children progressing slowly. When teachers identify such students they can supplement their observations with screening tests such as the "Phonics screening check" used by United Kingdom schools during Year one.

In the medical setting, child and adolescent psychiatrist M. S. Thambirajah emphasizes that "[g]iven the high prevalence of developmental disorders in school-aged children, all children seen in clinics should be systematically screened for developmental disorders irrespective of the presenting problem/s." Thambirajah recommends screening for developmental disorders, including dyslexia, by conducting a brief developmental history, a preliminary psychosocial developmental examination, and obtaining a school report regarding academic and social functioning.

Through the use of compensation strategies, therapy and educational support, individuals with dyslexia can learn to read and write. There are techniques and technical aids that help to manage or conceal symptoms of the disorder. Reducing stress and anxiety can sometimes improve written comprehension. For dyslexia intervention with alphabet-writing systems, the fundamental aim is to increase a child's awareness of correspondences between graphemes (letters) and phonemes (sounds), and to relate these to reading and spelling by teaching how sounds blend into words. Reinforced collateral training focused on reading and spelling may yield longer-lasting gains than oral phonological training alone. Early intervention can be successful in reducing reading failure.

Research does not suggest that specially-tailored fonts (such as Dyslexie and OpenDyslexic) help with reading. Children with dyslexia read text set in a regular font such as Times New Roman and Arial just as quickly, and they show a preference for regular fonts over specially-tailored fonts. Some research has pointed to increased letter-spacing being beneficial.

There is currently no evidence showing that music education significantly improves the reading skills of adolescents with dyslexia.

There is some evidence from an RCT that atomoxetine might be helpful for dyslexic with or without ADHD.

Dyslexic children require special instruction for word analysis and spelling from an early age. The prognosis, generally speaking, is positive for individuals who are identified in childhood and receive support from friends and family. The New York educational system (NYED) indicates "a daily uninterrupted 90-minute block of instruction in reading" and "instruction in phonemic awareness, phonics, vocabulary development, reading fluency" so as to improve the individual's reading ability.

The prevalence of dyslexia is unknown, but it has been estimated to be as low as 5% and as high as 17% of the population. Dyslexia is diagnosed more often in males.

There are different definitions of dyslexia used throughout the world. Further, differences in writing systems may affect development of written language ability due to the interplay between auditory and written representations of phonemes. Dyslexia is not limited to difficulty in converting letters to sounds, and Chinese people with dyslexia may have difficulty converting Chinese characters into their meanings. The Chinese vocabulary uses logographic, monographic, non-alphabet writing where one character can represent an individual phoneme.

The phonological-processing hypothesis attempts to explain why dyslexia occurs in a wide variety of languages. Furthermore, the relationship between phonological capacity and reading appears to be influenced by orthography.

Dyslexia was clinically described by Oswald Berkhan in 1881, but the term "dyslexia" was coined in 1883 by Rudolf Berlin, an ophthalmologist in Stuttgart. He used the term to refer to the case of a young boy who had severe difficulty learning to read and write, despite showing typical intelligence and physical abilities in all other respects. In 1896, W. Pringle Morgan, a British physician from Seaford, East Sussex, published a description of a reading-specific learning disorder in a report to the "British Medical Journal" titled "Congenital Word Blindness". The distinction between phonological versus surface types of dyslexia is only descriptive, and without any etiological assumption as to the underlying brain mechanisms. However, studies have alluded to potential differences due to variation in performance. Over time, we have changed from the intelligence-based model to the age-based model, in terms of those with Dyslexia.

As is the case with any disorder, society often makes an assessment based on incomplete information. Before the 1980s, dyslexia was thought to be a consequence of education, rather than a neurological disability. As a result, society often misjudges those with the disorder. There is also sometimes a workplace stigma and negative attitude towards those with dyslexia. If the instructors of a person with dyslexia lack the necessary training to support a child with the condition, there is often a negative effect on the student's learning participation.

Since at least the 1960s in the UK, the children diagnosed with developmental dyslexia have consistently been from privileged families. Although half of prisoners in the UK have significant reading difficulties, very few have ever been evaluated for dyslexia. Access to some special educational resources and funding is contingent upon having a diagnosis of dyslexia. As a result, when Staffordshire and Warwickshire proposed in 2018 to teach reading to all children with reading difficulties, using techniques proven to be successful for most children with a diagnosis of dyslexia, without first requiring the families to obtain an official diagnosis, dyslexia advocates and parents of children with dyslexia were fearful that they were losing a privileged status.

Due to the various cognitive processes that dyslexia affects and the overwhelming societal stigma around the disability, individuals with dyslexia often employ behaviors of self-stigma and perfectionistic self-presentation in order to cope with their disability. The perfectionist self-presentation is when an individual attempts to present themselves as the perfect ideal image and hides any imperfections. This behavior presents serious risk as it often results in mental health issues and refusal to seek help for their disability.

Most dyslexia research relates to alphabetic writing systems, and especially to European languages. However, substantial research is also available regarding people with dyslexia who speak Arabic, Chinese, Hebrew, or other languages. The outward expression of individuals with reading disability, and regular poor readers, is the same in some respects.



Delft

Delft () is a city and municipality in the province of South Holland, Netherlands. It is located between Rotterdam, to the southeast, and The Hague, to the northwest. Together with them, it is a part of both the Rotterdam–The Hague metropolitan area and the Randstad.

Delft is a popular tourist destination in the Netherlands, famous for its historical connections with the reigning House of Orange-Nassau, for its blue pottery, for being home to the painter Jan Vermeer, and for hosting Delft University of Technology (TU Delft). Historically, Delft played a highly influential role in the Dutch Golden Age. In terms of science and technology, thanks to the pioneering contributions of Antonie van Leeuwenhoek and Martinus Beijerinck, Delft can be considered to be the birthplace of microbiology.

The city of Delft came into being beside a canal, the 'Delf', which comes from the word "delven", meaning to delve or dig, and this led to the name Delft. At the elevated place where this 'Delf' crossed the creek wall of the silted up river Gantel, a Count established his manor, probably around 1075. Partly because of this, Delft became an important market town, the evidence for which can be seen in the size of its central market square.

Having been a rural village in the early Middle Ages, Delft developed into a city, and on 15 April 1246, Count Willem II granted Delft its city charter. Trade and industry flourished. In 1389 the Delfshavensche Schie canal was dug through to the river Maas, where the port of Delfshaven was built, connecting Delft to the sea.

Until the 17th century, Delft was one of the major cities of the then county (and later province) of Holland. In 1400, for example, the city had 6,500 inhabitants, making it the third largest city after Dordrecht (8,000) and Haarlem (7,000). In 1560, Amsterdam, with 28,000 inhabitants, had become the largest city, followed by Delft, Leiden and Haarlem, which each had around 14,000 inhabitants.

In 1536, a large part of the city was destroyed by the great fire of Delft.

The town's association with the House of Orange started when William of Orange (Willem van Oranje), nicknamed William the Silent (Willem de Zwijger), took up residence in 1572 in the former Saint-Agatha convent (subsequently called the Prinsenhof). At the time he was the leader of growing national Dutch resistance against Spanish occupation, known as the Eighty Years' War. By then Delft was one of the leading cities of Holland and was equipped with the necessary city walls to serve as a headquarters. In October 1573, an attack by Spanish forces was repelled in the Battle of Delft.

After the Act of Abjuration was proclaimed in 1581, Delft became the "de facto" capital of the newly independent Netherlands, as the seat of the Prince of Orange.

When William was shot dead on 10 July 1584 by Balthazar Gerards in the hall of the Prinsenhof (now the Prinsenhof Museum), the family's traditional burial place in Breda was still in the hands of the Spanish. Therefore, he was buried in the Delft Nieuwe Kerk (New Church), starting a tradition for the House of Orange that has continued to the present day.

Around this time, Delft also occupied a prominent position in the field of printing.

A number of Italian glazed earthenware makers settled in the city and introduced a new style. The tapestry industry also flourished when famous manufacturer François Spierincx moved to the city. In the 17th century, Delft experienced a new heyday, thanks to the presence of an office of the Dutch East India Company (VOC) (opened in 1602) and the manufacture of Delft Blue china.

A number of notable artists based themselves in the city, including Leonard Bramer, Carel Fabritius, Pieter de Hoogh, Gerard Houckgeest, Emanuel de Witte, Jan Steen, and Johannes Vermeer.
Reinier de Graaf and Antonie van Leeuwenhoek received international attention for their scientific research.

The Delft Explosion, also known in history as the Delft Thunderclap, occurred on 12 October 1654 when a gunpowder store exploded, destroying much of the city. Over a hundred people were killed and thousands were injured.

About of gunpowder were stored in barrels in a magazine in a former Clarist convent in the Doelenkwartier district, where the Paardenmarkt is now located. Cornelis Soetens, the keeper of the magazine, opened the store to check a sample of the powder and a huge explosion followed. Luckily, many citizens were away, visiting a market in Schiedam or a fair in The Hague.

Today, the explosion is primarily remembered for killing Rembrandt's most promising pupil, Carel Fabritius, and destroying almost all of his works.

Delft artist Egbert van der Poel painted several pictures of Delft showing the devastation.

The gunpowder store (Dutch: Kruithuis) was subsequently re-housed, a 'cannonball's distance away', outside the city, in a new building designed by architect Pieter Post.

The city centre retains a large number of monumental buildings, while in many streets there are canals of which the banks are connected by typical bridges, altogether making this city a notable tourist destination.

Historical buildings and other sights of interest include:

Delft is well known for the Delft pottery ceramic products which were styled on the imported Chinese porcelain of the 17th century. The city had an early start in this area since it was a home port of the Dutch East India Company. It can still be seen at the pottery factories De Koninklijke Porceleyne Fles (or Royal Delft) and De Delftse Pauw, while new ceramics and ceramic art can be found at the Gallery Terra Delft.

The painter Johannes Vermeer (1632–1675) was born in Delft. Vermeer used Delft streets and home interiors as the subject or background in his paintings.
Several other famous painters lived and worked in Delft at that time, such as Pieter de Hoogh, Carel Fabritius, Nicolaes Maes, Gerard Houckgeest and Hendrick Cornelisz. van Vliet. They were all members of the Delft School. The Delft School is known for its images of domestic life and views of households, church interiors, courtyards, squares and the streets of Delft. The painters also produced pictures showing historic events, flowers, portraits for patrons and the court as well as decorative pieces of art.

Delft supports creative arts' companies. From 2001 the , a building that had been disused since 1951, began to house small companies in the creative arts sector. Its demolition started in December 2009, making way for the new railway tunnel in Delft. The occupants of the building, as well as the name 'Bacinol', moved to another building in the city. The name Bacinol relates to Dutch penicillin research during WWII.

Delft University of Technology (TU Delft) is one of four universities of technology in the Netherlands. It was founded as an academy for civil engineering in 1842 by King William II. Today, well over 21,000 students are enrolled.

The UNESCO-IHE Institute for Water Education, providing postgraduate education for people from developing countries, draws on the strong tradition in water management and hydraulic engineering of the Delft university.

The Hague University of Applied Sciences has a building on the Delft University of Technology campus. It opened in 2009 and offers several bachelor degrees for the Faculty of Technology, Innovation & Society.

Inholland University of Applied Sciences also has a building on the Delft University of Technology campus. Several bachelor degrees for the Agri, Food & Life Sciences faculty and the Engineering, Design and Computing faculty are being taught at the Delft campus.

In the local economic field, essential elements are:

East of Delft lies a relatively large nature and recreation area called the "Delftse Hout" ("Delft Wood"). Through the forest lie bike, horse-riding and footpaths. It also includes a vast lake (suitable for swimming and windsurfing), narrow beaches, a restaurant, and community gardens, plus camping ground and other recreational and sports facilities. (There is also a facility for renting bikes from the station.)

Inside the city, apart from a central park, there are several smaller town parks, including "Nieuwe Plantage", "Agnetapark", "Kalverbos".
There is also the Botanical Garden of the TU and an arboretum in Delftse Hout.

Delft is the birthplace of:







Delft is twinned with:
Trains stopping at these stations connect Delft with, among others, the nearby cities of Rotterdam and The Hague, as often as every five minutes, for most of the day.

There are several bus routes from Delft to similar destinations. Trams frequently travel between Delft and The Hague via special double tracks crossing the city.

The whole city center and adjacent areas are a paid on-street parking area. In 2018, with the day parking fee of 29.5 Euro, it was the most expensive on-street parking area in the Netherlands, with the city centers of Deventer and Dordrecht being second and third, respectively.




Duesberg hypothesis

The Duesberg hypothesis is the claim that AIDS is not caused by HIV, but instead that AIDS is caused by noninfectious factors such as recreational and pharmaceutical drug use and that HIV is merely a harmless passenger virus. The hypothesis was popularized by Peter Duesberg, a professor of biology at University of California, Berkeley, from whom the hypothesis gets its name. The scientific consensus is that the Duesberg hypothesis is incorrect and that HIV is the cause of AIDS. The most prominent supporters of the hypothesis are Duesberg himself, biochemist and vitamin proponent David Rasnick, and journalist Celia Farber. The scientific community generally contends that Duesberg's arguments in favor of the hypothesis are the result of cherry-picking predominantly outdated scientific data and selectively ignoring evidence that demonstrates HIV's role in causing AIDS.

Duesberg argues that there is a statistical correlation between trends in recreational drug use and trends in AIDS cases. He argues that the epidemic of AIDS cases in the 1980s corresponds to a supposed epidemic of recreational drug use in the United States and Europe during the same time frame.

These claims are not supported by epidemiologic data. The average yearly increase in opioid-related deaths from 1990 to 2002 was nearly three times the yearly increase from 1979 to 1990, with the greatest increase in 2000–2002, yet AIDS cases and deaths fell dramatically during the mid-to-late-1990s. Duesberg's claim that recreational drug use, rather than HIV, was the cause of AIDS has been specifically examined and found to be false. Cohort studies have found that only HIV-positive drug users develop opportunistic infections; HIV-negative drug users do not develop such infections, indicating that HIV rather than drug use is the cause of AIDS.

Duesberg has also argued that nitrite inhalants were the cause of the epidemic of Kaposi sarcoma (KS) in gay men. However, this argument has been described as an example of the fallacy of a statistical confounding effect; it is now known that a herpesvirus, potentiated by HIV, is responsible for AIDS-associated KS.

Moreover, in addition to recreational drugs, Duesberg argues that anti-HIV drugs such as zidovudine (AZT) can cause AIDS. Duesberg's claim that antiviral medication causes AIDS is regarded as disproven within the scientific community. Placebo-controlled studies have found that AZT as a single agent produces modest and short-lived improvements in survival and delays the development of opportunistic infections; it certainly did not cause AIDS, which develops in both treated and untreated study patients. With the subsequent development of protease inhibitors and highly active antiretroviral therapy, numerous studies have documented the fact that anti-HIV drugs prevent the development of AIDS and substantially prolong survival, further disproving the claim that these drugs "cause" AIDS.

Several studies have specifically addressed Duesberg's claim that recreational drug abuse or sexual promiscuity were responsible for the manifestations of AIDS. An early study of his claims, published in "Nature" in 1993, found Duesberg's drug abuse-AIDS hypothesis to have "no basis in fact."

A large prospective study followed a group of 715 homosexual men in the Vancouver, Canada, area; approximately half were HIV-seropositive or became so during the follow-up period, and the remainder were HIV-seronegative. After more than eight years of follow-up, despite similar rates of drug use, sexual contact, and other supposed risk factors in both groups, only the HIV-positive group suffered from opportunistic infections. Similarly, CD4 counts dropped in the patients who were HIV-infected, but remained stable in the HIV-negative patients, despite similar rates of risk behavior. The authors concluded that "the risk-AIDS hypothesis ... is clearly rejected by our data," and that "the evidence supports the hypothesis that HIV-1 has an integral role in the CD4 depletion and progressive immune dysfunction that characterise AIDS."

Similarly, the Multicenter AIDS Cohort Study (MACS) and the Women's Interagency HIV Study (WIHS)—which between them observed more than 8,000 Americans—demonstrated that "the presence of HIV infection is the only factor that is strongly and consistently associated with the conditions that define AIDS." A 2008 study found that recreational drug use (including cannabis, cocaine, poppers, and amphetamines) had no effect on CD4 or CD8 T-cell counts, providing further evidence against a role of recreational drugs as a cause of AIDS.

Duesberg argued in 1989 that a significant number of AIDS victims had died without proof of HIV infection. However, with the use of modern culture techniques and polymerase chain reaction testing, HIV can be demonstrated in virtually all patients with AIDS. Since AIDS is now defined partially by the presence of HIV, Duesberg claims it is impossible by definition to offer evidence that AIDS does not require HIV. However, the first definitions of AIDS mentioned no cause and the first AIDS diagnoses were made before HIV was discovered. The addition of HIV positivity to surveillance criteria as an absolutely necessary condition for case reporting occurred only in 1993, after a scientific consensus was established that HIV caused AIDS.

According to the Duesberg hypothesis, AIDS is not found in Africa. What Duesberg calls "the myth of an African AIDS epidemic," among people" exists for several reasons, including:

Duesberg states that African AIDS cases are "a collection of long-established, indigenous diseases, such as chronic fevers, weight loss, alias "slim disease," diarrhea, and tuberculosis" that result from malnutrition and poor sanitation. African AIDS cases, though, have increased in the last three decades as HIV's prevalence has increased but as malnutrition percentages and poor sanitation have declined in many African regions. In addition, while HIV and AIDS are more prevalent in urban than in rural settings in Africa, malnutrition and poor sanitation are found more commonly in rural than in urban settings.

According to Duesberg, common diseases are easily misdiagnosed as AIDS in Africa because "the diagnosis of African AIDS is arbitrary" and does not include HIV testing. A definition of AIDS agreed upon in 1985 by the World Health Organization in Bangui did not require a positive HIV test, but since 1985, many African countries have added positive HIV tests to the Bangui criteria for AIDS or changed their definitions to match those of the U.S. Centers for Disease Control. One of the reasons for using more HIV tests despite their expense is that, rather than overestimating AIDS as Duesberg suggests, the Bangui definition alone excluded nearly half of African AIDS patients."

Duesberg notes that diseases associated with AIDS differ between African and Western populations, concluding that the causes of immunodeficiency must be different. Tuberculosis is much more commonly diagnosed among AIDS patients in Africa than in Western countries, while PCP conforms to the opposite pattern. Tuberculosis, though, had higher prevalence in Africa than in the West before the spread of HIV. In Africa and the United States, HIV has spurred a similar percentage increase in tuberculosis cases. PCP may be underestimated in Africa: since machinery "required for accurate testing is relatively rare in many resource-poor areas, including large parts of Africa, PCP is likely to be underdiagnosed in Africa. Consistent with this hypothesis, studies that report the highest rates of PCP in Africa are those that use the most advanced diagnostic methods" Duesberg also claims that Kaposi's sarcoma is "exclusively diagnosed in male homosexual risk groups using nitrite inhalants and other psychoactive drugs as aphrodisiacs", but the cancer is fairly common among heterosexuals in some parts of Africa, and is found in heterosexuals in the United States as well.

Because reported AIDS cases in Africa and other parts of the developing world include a larger proportion of people who do not belong to Duesberg's preferred risk groups of drug addicts and male homosexuals, Duesberg writes on his website that "There are no risk groups in Africa, like drug addicts and homosexuals." However, many studies have addressed the issue of risk groups in Africa and concluded that the risk of AIDS is not equally distributed. In addition, AIDS in Africa largely kills sexually active working-age adults.

South African president Thabo Mbeki accepted Duesberg's hypothesis and, through the mid-2000s, rejected offers of medical assistance to fight HIV infection, a policy of inaction that cost over 300,000 lives.

Duesberg argues that retroviruses like HIV must be harmless to survive: they do not kill cells and they do not cause cancer, he maintains. Duesberg writes, "retroviruses do not kill cells because they depend on viable cells for the replication of their RNA from viral DNA integrated into cellular DNA." Duesberg elsewhere states that "the typical virus reproduces by entering a living cell and commandeering the cell's resources in order to make new virus particles, a process that ends with the disintegration of the dead cell."

Duesberg also rejects the involvement of retroviruses and other viruses in cancer. To him, virus-associated cancers are "freak accidents of nature" that do not warrant research programs such as the war on cancer. Duesberg rejects a role in cancer for numerous viruses, including leukemia viruses, Epstein–Barr virus, human papilloma virus, hepatitis B, feline leukemia virus, and human T-lymphotropic virus.

Duesberg claims that the supposedly innocuous nature of all retroviruses is supported by what he considers to be their normal mode of proliferation: infection from mother to child "in utero". Duesberg does not suggest that HIV is an endogenous retrovirus, a virus integrated into the germline and genetically heritable:

The consensus in the scientific community is that the Duesberg hypothesis has been refuted by a large and growing mass of evidence showing that HIV causes AIDS, that the amount of virus in the blood correlates with disease progression, that a plausible mechanism for HIV's action has been proposed, and that anti-HIV medication decreases mortality and opportunistic infection in people with AIDS.

In issue of "Science" (Vol. 266, No. 5191), Duesberg's methods and claims were evaluated in a group of articles. The authors concluded that

The vast majority of people with AIDS have never received antiretroviral drugs, including those in developed countries prior to the licensure of AZT (zidovudine) in 1987, and people in developing countries today where very few individuals have access to these medications.

The NIAID reports that 

Duesberg claims as support for his idea that many drug-free HIV-positive people have not yet developed AIDS; HIV/AIDS scientists note that many drug-free HIV-positive people have developed AIDS, and that, in the absence of medical treatment or rare genetic factors postulated to delay disease progression, it is very likely that nearly all HIV-positive people will eventually develop AIDS. Scientists also note that HIV-negative drug users do not suffer from immune system collapse.



DSL (disambiguation)

DSL or digital subscriber line is a family of technologies that provide digital data transmission over the wires of a local telephone network.

DSL may also refer to:

Dinosaur

Dinosaurs are a diverse group of reptiles of the clade Dinosauria. They first appeared during the Triassic period, between 243 and 233.23 million years ago (mya), although the exact origin and timing of the evolution of dinosaurs is a subject of active research. They became the dominant terrestrial vertebrates after the Triassic–Jurassic extinction event 201.3 mya and their dominance continued throughout the Jurassic and Cretaceous periods. The fossil record shows that birds are feathered dinosaurs, having evolved from earlier theropods during the Late Jurassic epoch, and are the only dinosaur lineage known to have survived the Cretaceous–Paleogene extinction event approximately 66 mya. Dinosaurs can therefore be divided into avian dinosaurs—birds—and the extinct non-avian dinosaurs, which are all dinosaurs other than birds.

Dinosaurs are varied from taxonomic, morphological and ecological standpoints. Birds, at over 11,000 living species, are among the most diverse groups of vertebrates. Using fossil evidence, paleontologists have identified over 900 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species (birds) and fossil remains. Through the first half of the 20th century, before birds were recognized as dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction. Some were herbivorous, others carnivorous. Evidence suggests that all dinosaurs were egg-laying, and that nest-building was a trait shared by many dinosaurs, both avian and non-avian.

While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. While the dinosaurs' modern-day surviving avian lineage (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs (non-avian and avian) were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of and heights of and were the largest land animals of all time. The misconception that non-avian dinosaurs were uniformly gigantic is based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small, some measuring about in length.

The first dinosaur fossils were recognized in the early 19th century, with the name "dinosaur" (meaning "terrible lizard") being coined by Sir Richard Owen in 1842 to refer to these "great fossil lizards". Since then, mounted fossil dinosaur skeletons have been major attractions at museums worldwide, and dinosaurs have become an enduring part of popular culture. The large sizes of some dinosaurs, as well as their seemingly monstrous and fantastic nature, have ensured their regular appearance in best-selling books and films, such as "Jurassic Park". Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.

Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of the most recent common ancestor (MRCA) of "Triceratops" and modern birds (Neornithes), and all its descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of "Megalosaurus" and "Iguanodon", because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions cover the same known genera: Dinosauria = Ornithischia + Saurischia. This includes major groups such as ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (bipedal or quadrupedal herbivores with neck frills), pachycephalosaurians (bipedal herbivores with thick skulls), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), theropods (mostly bipedal carnivores and birds), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).

Birds are the sole surviving dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, most contemporary paleontologists reject the traditional style of classification based on anatomical similarity, in favor of phylogenetic taxonomy based on deduced ancestry, in which each group is defined as all descendants of a given founding genus. Birds belong to the dinosaur subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians.

Research by Matthew G. Baron, David B. Norman, and Paul M. Barrett in 2017 suggested a radical revision of dinosaurian systematics. Phylogenetic analysis by Baron "et al." recovered the Ornithischia as being closer to the Theropoda than the Sauropodomorpha, as opposed to the traditional union of theropods with sauropodomorphs. This would cause sauropods and kin to fall outside traditional dinosaurs, so they re-defined Dinosauria as the last common ancestor of "Triceratops horridus", "Passer domesticus" and "Diplodocus carnegii", and all of its descendants, to ensure that sauropods and kin remain included as dinosaurs. They also resurrected the clade Ornithoscelida to refer to the group containing Ornithischia and Theropoda.

Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Other prehistoric animals, including pterosaurs, mosasaurs, ichthyosaurs, plesiosaurs, and "Dimetrodon", while often popularly conceived of as dinosaurs, are not taxonomically classified as dinosaurs. Pterosaurs are distantly related to dinosaurs, being members of the clade Ornithodira. The other groups mentioned are, like dinosaurs and pterosaurs, members of Sauropsida (the reptile and bird clade), except "Dimetrodon" (which is a synapsid). None of them had the erect hind limb posture characteristic of true dinosaurs.

Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic Era, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a domestic cat, and were generally rodent-sized carnivores of small prey. Dinosaurs have always been recognized as an extremely varied group: over 900 non-avian dinosaur genera have been confidently identified (2018) with 1124 species (2016). Estimates put the total number of dinosaur genera preserved in the fossil record at 1850, nearly 75% still undiscovered, and the number that ever existed (in or out of the fossil record) at 3,400. A 2016 estimate put the number of dinosaur species living in the Mesozoic at 1,543–2,468, compared to the number of modern-day birds (avian dinosaurs) at 10,806 species. 

Extinct dinosaurs, as well as modern birds, include genera which are herbivorous and others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some evolved into quadrupeds, and others, such as "Anchisaurus" and "Iguanodon", could walk as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although the best-known genera are remarkable for their large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by the Early Jurassic epoch at latest. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as "Microraptor") could fly or at least glide, and others, such as spinosaurids, had semiaquatic habits.

While recent discoveries have made it more difficult to present a universally agreed-upon list of their distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clearly descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the most recent common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.
A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known:

Nesbitt found a number of further potential synapomorphies and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others.

A variety of other skeletal features are shared by dinosaurs. However, because they either are common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of Infratemporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in "Herrerasaurus"); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in "Saturnalia tupiniquim", for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the Late Triassic epoch are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature.

Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar-erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.

Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese considered them to be dragon bones and documented them as such. For example, "Huayang Guo Zhi" (), a gazetteer compiled by Chang Qu () during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.

Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a "Megalosaurus", was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his "The Natural History of Oxford-shire" (1677). He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He, therefore, concluded it to be the femur of a huge human, perhaps a Titan or another type of giant featured in legends. Edward Lhuyd, a friend of Sir Isaac Newton, published "Lithophylacii Britannici ichnographia" (1699), the first scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, "Rutellum impicatum", that had been found in Caswell, near Witney, Oxfordshire.

Between 1815 and 1824, the Rev William Buckland, the first Reader of Geology at the University of Oxford, collected more fossilized bones of "Megalosaurus" and became the first person to describe a non-avian dinosaur in a scientific journal. The second non-avian dinosaur genus to be identified, "Iguanodon", was according to legend discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell who in fact had required remains years earlier. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.

The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Sir Richard Owen coined the term "dinosaur", using it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived . Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it also to evoke their size and majesty. Owen recognized that the remains that had been found so far, "Iguanodon", "Megalosaurus" and "Hylaeosaurus", shared distinctive features, and so decided to present them as a distinct taxonomic group. As clarified by British geologist and historian Hugh Torrens, Owen had given a presentation about fossil reptiles to the British Association for the Advancement of Science in 1841, but reports of the time show that Owen did not mention the word "dinosaur", nor recognize dinosaurs as a distinct group of reptiles in his address. He introduced the Dinosauria only in the revised text version of his talk published in April 1842. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.

In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named "Hadrosaurus foulkii". It was an extremely important find: "Hadrosaurus" was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of interests in dinosaurs in the United States, known as dinosaur mania.

Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. This fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones. Modern paleontologists would find such methods crude and unacceptable, since blasting easily destroys fossil and stratigraphic evidence. Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History in New York City, while Marsh's is at the Peabody Museum of Natural History at Yale University.

World War II caused a pause in palaeontological research; after the war, research attention was also diverted increasingly to fossil mammals rather than dinosaurs, which were seen as sluggish and cold-blooded. At the end of the 1960s, however, the field of dinosaur research experienced a surge in activity that remains ongoing. Several seminal studies led to this activity. First, John Ostrom discovered the bird-like dromaeosaurid theropod "Deinonychus" and described it in 1969. Its anatomy indicated that it was an active predator that was likely warm-blooded, in marked contrast to the then-prevailing image of dinosaurs. Concurrently, Robert T. Bakker published a series of studies that likewise argued for active lifestyles in dinosaurs based on anatomical and ecological evidence (see ), which were subsequently summarized in his 1986 book "The Dinosaur Heresies".
New revelations were supported by an increase in dinosaur discoveries. Major new dinosaur discoveries have been made by paleontologists working in previously unexplored regions, including India, South America, Madagascar, Antarctica, and most significantly China. Across theropods, sauropodomorphs, and ornithischians, the number of named genera began to increase exponentially in the 1990s. over 30 new species of dinosaurs were named each year. At least sauropodomorphs experienced a further increase in the number of named species in the 2010s, with an average of 9.3 new species having been named each year between 2009 and 2020. As a consequence, more sauropodomorphs were named between 1990 and 2020 than in all previous years combined. These new localities also led to improvements in overall specimen quality, with new species being increasingly named not on scrappy fossils but on more complete skeletons, sometimes from multiple individuals. Better specimens also led to new species being invalidated less frequently. Asian localities have produced the most complete theropod specimens, while North American localities have produced the most complete sauropodomorph specimens.

Prior to the dinosaur renaissance, dinosaurs were mostly classified using the traditional rank-based system of Linnaean taxonomy. The renaissance was also accompanied by the increasingly widespread application of cladistics, a more objective method of classification based on ancestry and shared traits, which has proved tremendously useful in the study of dinosaur systematics and evolution. Cladistic analysis, among other techniques, helps to compensate for an often incomplete and fragmentary fossil record. Reference books summarizing the state of dinosaur research, such as David B. Weishampel and colleagues' "The Dinosauria", made knowledge more accessible and spurred further interest in dinosaur research. The release of the first and second editions of "The Dinosauria" in 1990 and 2004, and of a review paper by Paul Sereno in 1998, were accompanied by increases in the number of published phylogenetic trees for dinosaurs.

Dinosaur fossils are not limited to bones, but also include imprints or mineralized remains of skin coverings, organs, and other tissues. Of these, skin coverings based on keratin proteins are most easily preserved because of their cross-linked, hydrophobic molecular structure. Fossils of keratin-based skin coverings or bony skin coverings are known from most major groups of dinosaurs. Dinosaur fossils with scaly skin impressions have been found since the 19th century. Samuel Beckles discovered a sauropod forelimb with preserved skin in 1852 that was incorrectly attributed to a crocodile; it was correctly attributed by Marsh in 1888 and subject to further study by Reginald Hooley in 1917. Among ornithischians, in 1884 Jacob Wortman found skin impressions on the first known specimen of "Edmontosaurus annectens", which were largely destroyed during the specimen's excavation. Owen and Hooley subsequently described skin impressions of "Hypsilophodon" and "Iguanodon" in 1885 and 1917. Since then, scale impressions have been most frequently found among hadrosaurids, where the impressions are known from nearly the entire body across multiple specimens.
Starting from the 1990s, major discoveries of exceptionally preserved fossils in deposits known as conservation Lagerstätten contributed to research on dinosaur soft tissues. Chiefly among these were the rocks that produced the Jehol (Early Cretaceous) and Yanliao (Mid-to-Late Jurassic) biotas of northeastern China, from which hundreds of dinosaur specimens bearing impressions of feather-like structures (both closely related to birds and otherwise, see ) have been described by Xing Xu and colleagues. In living reptiles and mammals, pigment-storing cellular structures known as melanosomes are partially responsible for producing colouration. Both chemical traces of melanin and characteristically shaped melanosomes have been reported from feathers and scales of Jehol and Yanliao dinosaurs, including both theropods and ornithischians. This has enabled multiple full-body reconstructions of dinosaur colouration, such as for "Sinosauropteryx" and "Psittacosaurus" by Jakob Vinther and colleagues, and similar techniques have also been extended to dinosaur fossils from other localities. (However, some researchers have also suggested that fossilized melanosomes represent bacterial remains.) Stomach contents in some Jehol and Yanliao dinosaurs closely related to birds have also provided indirect indications of diet and digestive system anatomy (e.g., crops). More concrete evidence of internal anatomy has been reported in "Scipionyx" from the Pietraroja Plattenkalk of Italy. It preserves portions of the intestines, colon, liver, muscles, and windpipe.
Concurrently, a line of work led by Mary Higby Schweitzer, Jack Horner, and colleagues reported various occurrences of preserved soft tissues and proteins within dinosaur bone fossils. Various mineralized structures that likely represented red blood cells and collagen fibres had been found by Schweitzer and others in tyrannosaurid bones as early as 1991. However, in 2005, Schweitzer and colleagues reported that a femur of "Tyrannosaurus" preserved soft, flexible tissue within, including blood vessels, bone matrix, and connective tissue (bone fibers) that had retained their microscopic structure. This discovery suggested that original soft tissues could be preserved over geological time, with multiple mechanisms having been proposed. Later, in 2009, Schweitzer and colleagues reported that a "Brachylophosaurus" femur preserved similar microstructures, and immunohistochemical techniques (based on antibody binding) demonstrated the presence of proteins such as collagen, elastin, and laminin. Both specimens yielded collagen protein sequences that were viable for molecular phylogenetic analyses, which grouped them with birds as would be expected. The extraction of fragmentary DNA has also been reported for both of these fossils, along with a specimen of "Hypacrosaurus". In 2015, Sergio Bertazzo and colleagues reported the preservation of collagen fibres and red blood cells in eight Cretaceous dinosaur specimens that did not show any signs of exceptional preservation, indicating that soft tissue may be preserved more commonly than previously thought. Suggestions that these structures represent bacterial biofilms have been rejected, but cross-contamination remains a possibility that is difficult to detect.

Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic epochs, roughly 20 million years after the devastating Permian–Triassic extinction event wiped out an estimated 96% of all marine species and 70% of terrestrial vertebrate species approximately 252 million years ago. The oldest dinosaur fossils known from substantial remains date to the Carnian epoch of the Triassic period and have been found primarily in the Ischigualasto and Santa Maria Formations of Argentina and Brazil, and the Pebbly Arkose Formation of Zimbabwe.

The Ischigualasto Formation (radiometrically dated at 231-230 million years old) has produced the early saurischian "Eoraptor", originally considered a member of the Herrerasauridae but now considered to be an early sauropodomorph, along with the herrerasaurids "Herrerasaurus" and "Sanjuansaurus", and the sauropodomorphs "Chromogisaurus", "Eodromaeus", and "Panphagia". "Eoraptor" likely resemblance to the common ancestor of all dinosaurs suggests that the first dinosaurs would have been small, bipedal predators. The Santa Maria Formation (radiometrically dated to be older, at 233.23 million years old) has produced the herrerasaurids "Gnathovorax" and "Staurikosaurus", along with the sauropodomorphs "Bagualosaurus", "Buriolestes", "Guaibasaurus", "Macrocollum", "Nhandumirim", "Pampadromaeus", "Saturnalia", and "Unaysaurus". The Pebbly Arkose Formation, which is of uncertain age but was likely comparable to the other two, has produced the sauropodomorph "Mbiresaurus", along with an unnamed herrerasaurid.

Less well-preserved remains of the sauropodomorphs "Jaklapallisaurus" and "Nambalia", along with the early saurischian "Alwalkeria", are known from the Upper Maleri and Lower Maleri Formations of India. The Carnian-aged Chañares Formation of Argentina preserves primitive, dinosaur-like ornithodirans such as "Lagosuchus" and "Lagerpeton" in Argentina, making it another important site for understanding dinosaur evolution. These ornithodirans support the model of early dinosaurs as small, bipedal predators. Dinosaurs may have appeared as early as the Anisian epoch of the Triassic, approximately 243 million years ago, which is the age of "Nyasasaurus" from the Manda Formation of Tanzania. However, its known fossils are too fragmentary to identify it as a dinosaur or only a close relative. The referral of the Manda Formation to the Anisian is also uncertain. Regardless, dinosaurs existed alongside non-dinosaurian ornithodirans for a period of time, with estimates ranging from 5–10 million years to 21 million years.

When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchians, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 201 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early –mid Norian and late Norian or earliest Rhaetian stages, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct. Also notably, there was a heightened rate of extinction during the Carnian pluvial event.

Dinosaur evolution after the Triassic followed changes in vegetation and the location of continents. In the Late Triassic and Early Jurassic, the continents were connected as the single landmass Pangaea, and there was a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the Late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the Middle and Late Jurassic, where most localities had predators consisting of ceratosaurians, megalosauroids, and allosauroids, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized metriacanthosaurid theropods and unusual, long-necked sauropods like "Mamenchisaurus". Ankylosaurians and ornithopods were also becoming more common, but primitive sauropodomorphs had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like earlier sauropodomorphs, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians.

By the Early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like "Psittacosaurus" became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late Early Cretaceous or early Late Cretaceous. A major change in the Early Cretaceous, which would be amplified in the Late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with dental batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid "Nigersaurus".

There were three general dinosaur faunas in the Late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting supercontinent Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became very diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.

The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, including crocodilians, dyrosaurs, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.

The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of "Gastornis", eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often stated that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic Era. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, becoming extinct only very recently after over 20 million years of co-existence with many mammal groups.

Dinosaurs belong to a group known as archosaurs, which also includes modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.

Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with "Triceratops" than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek ' () meaning "lizard" and ' () meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups ("Herrerasaurus", therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).

By contrast, ornithischians—"bird-hipped", from the Greek "ornitheios" (ὀρνίθειος) meaning "of a bird" and "ischion" (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubic bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species that were primarily herbivores.

Despite the terms "bird hip" (Ornithischia) and "lizard hip" (Saurischia), birds are not part of Ornithischia. Birds instead belong to Saurischia, the "lizard-hipped" dinosaurs—birds evolved from earlier dinosaurs with "lizard hips".

The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and those of the main dinosaur groups Theropoda, Sauropodomorpha and Ornithischia, compiled by Justin Tweet. Further details and other hypotheses of classification may be found on individual articles.


Timeline of major dinosaur groups per .

Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and other soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the Earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.

Current evidence suggests that dinosaur average size varied through the Triassic, Early Jurassic, Late Jurassic and Cretaceous. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the category. The mode of Mesozoic dinosaur body masses is between . This contrasts sharply with the average size of Cenozoic mammals, estimated by the National Museum of Natural History as about .

The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest was an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as "Paraceratherium" (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.

Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals were ever fossilized and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.

The tallest and heaviest dinosaur known from good skeletons is "Giraffatitan brancai" (previously classified as a species of "Brachiosaurus"). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde in Berlin; this mount is tall and long, and would have belonged to an animal that weighed between and  kilograms ( and  lb). The longest complete dinosaur is the long "Diplodocus", which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Museum of Natural History in 1907. The longest dinosaur known from good fossil material is "Patagotitan": the skeleton mount in the American Museum of Natural History in New York is long. The Museo Municipal Carmen Funes in Plaza Huincul, Argentina, has an "Argentinosaurus" reconstructed skeleton mount that is long.
There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were discovered in the 1970s or later, and include the massive "Argentinosaurus", which may have weighed and reached lengths of ; some of the longest were the long "Diplodocus hallorum" (formerly "Seismosaurus"), the long "Supersaurus", and long "Patagotitan"; and the tallest, the tall "Sauroposeidon", which could have reached a sixth-floor window. There were a few dinosaurs that was considered either the heaviest and longest. The most famous one include "Amphicoelias fragillimus", known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been long and weighed . However, recent research have placed "Amphicoelias" from the long, gracile diplodocid to the shorter but much stockier rebbachisaurid. Now renamed as "Maraapunisaurus", this sauropod now stands as much as long and weigh as much as . Another contender of this title includes "Bruhathkayosaurus matleyi", an incredibly controversial taxon that was recently confirmed to exist after archived photos were uncovered. "Bruhathkayosaurus" was a titanosaur and would have most likely weighed more than even "Marrapunisaurus". Recent size estimates in 2023 have placed this sauropod reaching lengths of up to long and a colossal weight range of around , if these upper estimates up true, "Bruhathkayosaurus" would have rivaled the "blue whale" and "Perucetus colossus" as one of the largest animals to have ever existed.

The largest carnivorous dinosaur was "Spinosaurus", reaching a length of , and weighing . Other large carnivorous theropods included "Giganotosaurus", "Carcharodontosaurus" and "Tyrannosaurus". "Therizinosaurus" and "Deinocheirus" were among the tallest of the theropods. The largest ornithischian dinosaur was probably the hadrosaurid "Shantungosaurus giganteus" which measured . The largest individuals may have weighed as much as .
The smallest dinosaur known is the bee hummingbird, with a length of only and mass of around . The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, "Anchiornis huxleyi" is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of and a total skeletal length of . The smallest herbivorous non-avialan dinosaurs included "Microceratus" and "Wannanosaurus", at about long each.

Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (closest living relatives of birds), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.

The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 "Iguanodon", ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-billed (hadrosaurids) may have moved in great herds, like the American bison or the African springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded remains of over 20 "Sinornithomimus", from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as "Deinonychus" and "Allosaurus" can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators.
The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.

From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a "Velociraptor" attacking a "Protoceratops", providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an "Edmontosaurus", a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod "Majungasaurus".

Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, "Juravenator", and "Megapnosaurus" were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian "Agilisaurus" was inferred to be diurnal.

Based on fossil evidence from dinosaurs such as "Oryctodromeus", some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids) such as "Microraptor") most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, pioneered by Robert McNeill Alexander, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.

Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups, such as horns, frills, crests, sails, and feathers, suggests that visual communication has always been important in dinosaur biology. Reconstruction of the plumage color of "Anchiornis", suggest the importance of color in visual communication in non-avian dinosaurs. Vocalization in non-avian dinosaurs is less certain. In birds, the larynx plays no role in sound production. Instead they vocalize with a novel organ called the syrinx, located further down the trachea. The earliest remains of a syrinx was found in a specimen of the duck-like "Vegavis iaai" dated 69 –66 million years ago, and this organ is unlikely to have existed in non-avian dinosaurs. 
Paleontologist Phil Senter has suggested that since non-avian dinosaurs did not have a syrinx, and their next closest living relatives, crocodilians, use the larynx, they could not vocalize as the common ancestor would have been mute. He states that they mostly on visual displays and possibly non-vocal acoustic sounds like hissing, jaw grinding or clapping, splashing and wing beating (possible in winged maniraptoran dinosaurs). Other researchers have countered that vocalizations also exist in turtles, the closest relatives of archosaurs, suggesting that the trait is ancestral to their lineage. In addition, vocal communication in dinosaurs is indicated by the development of advanced hearing in nearly all major groups. Hence the syrinx may have supplemented and then replaced the larynx as a vocal organ rather than there being a "silent period" in bird evolution.

In 2023, a fossilized larynx was described from a specimen of the ankylosaurid "Pinacosaurus". The structure was composed of cricoid and arytenoid cartilages, similar to those of non-avian reptiles. However, the mobile cricoid-arytenoid joint and long arytenoid cartilages would have allowed for air-flow control similar to that of birds, and thus could have made bird-like vocalizations. In addition, the cartilages were ossified, implying that laryngeal ossification is a feature of some non-avian dinosaurs. A 2016 study concludes that some dinosaurs may have produced closed mouth vocalizations like cooing, hooting and booming. These occur in both reptiles and birds and involve inflating the esophagus or tracheal pouches. Such vocalizations evolved independently in extant archosaurs numerous times, following increases in body size. The crests of some hadrosaurids and the nasal chambers of ankylosaurids have been suggested to have functioned in acoustic resonance.

All dinosaurs laid amniotic eggs. Dinosaur eggs were usually laid in a nest. Most species create somewhat elaborate nests which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as "Troodon", exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.

When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a "Tyrannosaurus" skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur "Allosaurus" and the ornithopod "Tenontosaurus". Because the line of dinosaurs that includes "Allosaurus" and "Tyrannosaurus" diverged from the line that led to "Tenontosaurus" very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs.
Another widespread trait among modern birds (but see below in regards to fossil groups and extant megapodes) is parental care for young after hatching. Jack Horner's 1978 discovery of a "Maiasaura" ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods. A specimen of the oviraptorid "Citipati osmolskae" was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. An embryo of the basal sauropodomorph "Massospondylus" was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland.

However, there is ample evidence of precociality or superprecociality among many dinosaur species, particularly theropods. For instance, non-ornithuromorph birds have been abundantly demonstrated to have had slow growth rates, megapode-like egg burying behavior and the ability to fly soon after birth. Both "Tyrannosaurus" and "Troodon" had juveniles with clear superprecociality and likely occupying different ecological niches than the adults. Superprecociality has been inferred for sauropods.

Genital structures are unlikely to fossilize as they lack scales that may allow preservation via pigmentation or residual calcium phosphate salts. In 2021, the best preserved specimen of a dinosaur's cloacal vent exterior was described for "Psittacosaurus", demonstrating lateral swellings similar to crocodylian musk glands used in social displays by both sexes and pigmented regions which could also reflect a signalling function. However, this specimen on its own does not offer enough information to determine whether this dinosaur had sexual signalling functions; it only supports the possibility. Cloacal visual signalling can occur in either males or females in living birds, making it unlikely to be useful to determine sex for extinct dinosaurs.

Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are endothermic ("warm-blooded"), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extended. Various researchers have supported dinosaurs as being endothermic, ectothermic ("cold-blooded"), or somewhere in between. An emerging consensus among researchers is that, while different lineages of dinosaurs would have had different metabolisms, most of them had higher metabolic rates than other reptiles but lower than living birds and mammals, which is termed mesothermy by some. Evidence from crocodiles and their extinct relatives suggests that such elevated metabolisms could have developed in the earliest archosaurs, which were the common ancestors of dinosaurs and crocodiles.
After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic remained a prevalent view until Robert T. Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968. Bakker specifically used anatomical and ecological evidence to argue that sauropods, which had hitherto been depicted as sprawling aquatic animals with their tails dragging on the ground, were endotherms that lived vigorous, terrestrial lives. In 1972, Bakker expanded on his arguments based on energy requirements and predator-prey ratios. This was one of the seminal results that led to the dinosaur renaissance.

One of the greatest contributions to the modern understanding of dinosaur physiology has been paleohistology, the study of microscopic tissue structure in dinosaurs. From the 1960s forward, Armand de Ricqlès suggested that the presence of fibrolamellar bone—bony tissue with an irregular, fibrous texture and filled with blood vessels—was indicative of consistently fast growth and therefore endothermy. Fibrolamellar bone was common in both dinosaurs and pterosaurs, though not universally present. This has led to a significant body of work in reconstructing growth curves and modeling the evolution of growth rates across various dinosaur lineages, which has suggested overall that dinosaurs grew faster than living reptiles. Other lines of evidence suggesting endothermy include the presence of feathers and other types of body coverings in many lineages (see ); more consistent ratios of the isotope oxygen-18 in bony tissue compared to ectotherms, particularly as latitude and thus air temperature varied, which suggests stable internal temperatures (although these ratios can be altered during fossilization); and the discovery of polar dinosaurs, which lived in Australia, Antarctica, and Alaska when these places would have had cool, temperate climates.
In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Such respiratory systems, which may have appeared in the earliest saurischians, would have provided them with more oxygen compared to a mammal of similar size, while also having a larger resting tidal volume and requiring a lower breathing frequency, which would have allowed them to sustain higher activity levels. The rapid airflow would also have been an effective cooling mechanism, which in conjunction with a lower metabolic rate would have prevented large sauropods from overheating. These traits may have enabled sauropods to grow quickly to gigantic sizes. Sauropods may also have benefitted from their size—their small surface area to volume ratio meant that they would have been able to thermoregulate more easily, a phenomenon termed gigantothermy.

Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. This would have helped them to conserve water. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets are known as early as the Jurassic from "Anchiornis".

The size and shape of the brain can be partly reconstructed based on the surrounding bones. In 1896, Marsh calculated ratios between brain weight and body weight of seven species of dinosaurs, showing that the brain of dinosaurs was proportionally smaller than in today's crocodiles, and that the brain of "Stegosaurus" was smaller than in any living land vertebrate. This contributed to the widespread public notion of dinosaurs as being sluggish and extraordinarily stupid. Harry Jerison, in 1973, showed that proportionally smaller brains are expected at larger body sizes, and that brain size in dinosaurs was not smaller than expected when compared to living reptiles. Later research showed that relative brain size progressively increased during the evolution of theropods, with the highest intelligence – comparable to that of modern birds – calculated for the troodontid "Troodon".

The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of them being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in "Oviraptor", but misidentified as an interclavicle. In the 1970s, Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Jehol Biota, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives. They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.

Feathers are one of the most recognizable characteristics of modern birds, and a trait that was also shared by several non-avian dinosaurs. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians, and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs.

However, researchers do not agree regarding whether these structures share a common origin between lineages (i.e., they are homologous), or if they were the result of widespread experimentation with skin coverings among ornithodirans. If the former is the case, filaments may have been common in the ornithodiran lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching. The description of feathered dinosaurs has not been without controversy in general; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.

"Archaeopteryx" was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Charles Darwin's seminal "On the Origin of Species" (1859), its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for the small theropod "Compsognathus". Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Many of these specimens were unearthed in the lagerstätten of the Jehol Biota. If feather-like structures were indeed widely present among non-avian dinosaurs, the lack of abundant fossil evidence for them may be due to the fact that delicate features like skin and feathers are seldom preserved by fossilization and thus often absent from the fossil record.

Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.

Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described "Aerosteon riocoloradensis", the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT scanning of "Aerosteon"'s fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.

Fossils of the troodonts "Mei" and "Sinornithoides" demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.

Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.

All non-avian dinosaurs and most lineages of birds became extinct in a mass extinction event, called the Cretaceous–Paleogene (K-Pg) extinction event, at the end of the Cretaceous period. Above the Cretaceous–Paleogene boundary, which has been dated to 66.038 ± 0.025 million years ago, fossils of non-avian dinosaurs disappear abruptly; the absence of dinosaur fossils was historically used to assign rocks to the ensuing Cenozoic. The nature of the event that caused this mass extinction has been extensively studied since the 1970s, leading to the development of two mechanisms that are thought to have played major roles: an extraterrestrial impact event in the Yucatán Peninsula, along with flood basalt volcanism in India. However, the specific mechanisms of the extinction event and the extent of its effects on dinosaurs are still areas of ongoing research. Alongside dinosaurs, many other groups of animals became extinct: pterosaurs, marine reptiles such as mosasaurs and plesiosaurs, several groups of mammals, ammonites (nautilus-like mollusks), rudists (reef-building bivalves), and various groups of marine plankton. In all, approximately 47% of genera and 76% of species on Earth became extinct during the K-Pg extinction event. The relatively large size of most dinosaurs and the low diversity of small-bodied dinosaur species at the end of the Cretaceous may have contributed to their extinction; the extinction of the bird lineages that did not survive may also have been caused by a dependence on forest habitats or a lack of adaptations to eating seeds for survival.

Just before the K-Pg extinction event, the number of non-avian dinosaur species that existed globally has been estimated at between 628 and 1078. It remains uncertain whether the diversity of dinosaurs was in gradual decline before the K-Pg extinction event, or whether dinosaurs were actually thriving prior to the extinction. Rock formations from the Maastrichtian epoch, which directly preceded the extinction, have been found to have lower diversity than the preceding Campanian epoch, which led to the prevailing view of a long-term decline in diversity. However, these comparisons did not account either for varying preservation potential between rock units or for different extents of exploration and excavation. In 1984, Dale Russell carried out an analysis to account for these biases, and found no evidence of a decline; another analysis by David Fastovsky and colleagues in 2004 even showed that dinosaur diversity continually increased until the extinction, but this analysis has been rebutted. Since then, different approaches based on statistics and mathematical models have variously supported either a sudden extinction or a gradual decline. End-Cretaceous trends in diversity may have varied between dinosaur lineages: it has been suggested that sauropods were not in decline, while ornithischians and theropods were in decline.

The bolide impact hypothesis, first brought to wide attention in 1980 by Walter Alvarez, Luis Alvarez, and colleagues, attributes the K-Pg extinction event to a bolide (extraterrestrial projectile) impact. Alvarez and colleagues proposed that a sudden increase in iridium levels, recorded around the world in rock deposits at the Cretaceous–Paleogene boundary, was direct evidence of the impact. Shocked quartz, indicative of a strong shockwave emanating from an impact, was also found worldwide. The actual impact site remained elusive until a crater measuring wide was discovered in the Yucatán Peninsula of southeastern Mexico, and was publicized in a 1991 paper by Alan Hildebrand and colleagues. Now, the bulk of the evidence suggests that a bolide wide impacted the Yucatán Peninsula 66 million years ago, forming this crater and creating a "kill mechanism" that triggered the extinction event.

Within hours, the Chicxulub impact would have created immediate effects such as earthquakes, tsunamis, and a global firestorm that likely killed unsheltered animals and started wildfires. However, it would also have had longer-term consequences for the environment. Within days, sulfate aerosols released from rocks at the impact site would have contributed to acid rain and ocean acidification. Soot aerosols are thought to have spread around the world over the ensuing months and years; they would have cooled the surface of the Earth by reflecting thermal radiation, and greatly slowed photosynthesis by blocking out sunlight, thus creating an impact winter. (This role was ascribed to sulfate aerosols until experiments demonstrated otherwise.) The cessation of photosynthesis would have led to the collapse of food webs depending on leafy plants, which included all dinosaurs save for grain-eating birds.

At the time of the K-Pg extinction, the Deccan Traps flood basalts of India were actively erupting. The eruptions can be separated into three phases around the K-Pg boundary, two prior to the boundary and one after. The second phase, which occurred very close to the boundary, would have extruded 70 to 80% of the volume of these eruptions in intermittent pulses that occurred around 100,000 years apart. Greenhouse gases such as carbon dioxide and sulfur dioxide would have been released by this volcanic activity, resulting in climate change through temperature perturbations of roughly but possibly as high as . Like the Chicxulub impact, the eruptions may also have released sulfate aerosols, which would have caused acid rain and global cooling. However, due to large error margins in the dating of the eruptions, the role of the Deccan Traps in the K-Pg extinction remains unclear.

Before 2000, arguments that the Deccan Traps eruptions—as opposed to the Chicxulub impact—caused the extinction were usually linked to the view that the extinction was gradual. Prior to the discovery of the Chicxulub crater, the Deccan Traps were used to explain the global iridium layer; even after the crater's discovery, the impact was still thought to only have had a regional, not global, effect on the extinction event. In response, Luis Alvarez rejected volcanic activity as an explanation for the iridium layer and the extinction as a whole. Since then, however, most researchers have adopted a more moderate position, which identifies the Chicxulub impact as the primary progenitor of the extinction while also recognizing that the Deccan Traps may also have played a role. Walter Alvarez himself has acknowledged that the Deccan Traps and other ecological factors may have contributed to the extinctions in addition to the Chicxulub impact. Some estimates have placed the start of the second phase in the Deccan Traps eruptions within 50,000 years after the Chicxulub impact. Combined with mathematical modelling of the seismic waves that would have been generated by the impact, this has led to the suggestion that the Chicxulub impact may have triggered these eruptions by increasing the permeability of the mantle plume underlying the Deccan Traps.

Whether the Deccan Traps were a major cause of the extinction, on par with the Chicxulub impact, remains uncertain. Proponents consider the climatic impact of the sulfur dioxide released to have been on par with the Chicxulub impact, and also note the role of flood basalt volcanism in other mass extinctions like the Permian-Triassic extinction event. They consider the Chicxulub impact to have worsened the ongoing climate change caused by the eruptions. Meanwhile, detractors point out the sudden nature of the extinction and that other pulses in Deccan Traps activity of comparable magnitude did not appear to have caused extinctions. They also contend that the causes of different mass extinctions should be assessed separately. In 2020, Alfio Chiarenza and colleagues suggested that the Deccan Traps may even have had the opposite effect: they suggested that the long-term warming caused by its carbon dioxide emissions may have dampened the impact winter from the Chicxulub impact.

Non-avian dinosaur remains have occasionally been found above the K-Pg boundary. In 2000, Spencer Lucas and colleagues reported the discovery of a single hadrosaur right femur in the San Juan Basin of New Mexico, and described it as evidence of Paleocene dinosaurs. The rock unit in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.8 million years ago. If the bone was not re-deposited by weathering action, it would provide evidence that some dinosaur populations survived at least half a million years into the Cenozoic. Other evidence includes the presence of dinosaur remains in the Hell Creek Formation up to above the Cretaceous–Paleogene boundary, representing 40,000 years of elapsed time. This has been used to support the view that the K-Pg extinction was gradual. However, these supposed Paleocene dinosaurs are considered by many other researchers to be reworked, that is, washed out of their original locations and then re-buried in younger sediments. The age estimates have also been considered unreliable.

By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. The entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.

Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures was unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. The enduring popularity of dinosaurs, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.

The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens "Bleak House", dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel "Journey to the Center of the Earth", Sir Arthur Conan Doyle's 1912 book "The Lost World", the 1914 animated film "Gertie the Dinosaur" (featuring the first animated dinosaur), the iconic 1933 film "King Kong", the 1954 "Godzilla" and its many sequels, the best-selling 1990 novel "Jurassic Park" by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.


Diamagnetism

Diamagnetism is the property of materials that are repelled by a magnetic field; an applied magnetic field creates an induced magnetic field in them in the opposite direction, causing a repulsive force. In contrast, paramagnetic and ferromagnetic materials are attracted by a magnetic field. Diamagnetism is a quantum mechanical effect that occurs in all materials; when it is the only contribution to the magnetism, the material is called diamagnetic. In paramagnetic and ferromagnetic substances, the weak diamagnetic force is overcome by the attractive force of magnetic dipoles in the material. The magnetic permeability of diamagnetic materials is less than the permeability of vacuum, "μ". In most materials, diamagnetism is a weak effect which can be detected only by sensitive laboratory instruments, but a superconductor acts as a strong diamagnet because it entirely expels any magnetic field from its interior (the Meissner effect).

Diamagnetism was first discovered when Anton Brugmans observed in 1778 that bismuth was repelled by magnetic fields. In 1845, Michael Faraday demonstrated that it was a property of matter and concluded that every material responded (in either a diamagnetic or paramagnetic way) to an applied magnetic field. On a suggestion by William Whewell, Faraday first referred to the phenomenon as "diamagnetic" (the prefix "dia-" meaning "through" or "across"), then later changed it to "diamagnetism".

A simple rule of thumb is used in chemistry to determine whether a particle (atom, ion, or molecule) is paramagnetic or diamagnetic: If all electrons in the particle are paired, then the substance made of this particle is diamagnetic; If it has unpaired electrons, then the substance is paramagnetic.

Diamagnetism is a property of all materials, and always makes a weak contribution to the material's response to a magnetic field. However, other forms of magnetism (such as ferromagnetism or paramagnetism) are so much stronger such that, when different forms of magnetism are present in a material, the diamagnetic contribution is usually negligible. Substances where the diamagnetic behaviour is the strongest effect are termed diamagnetic materials, or diamagnets. Diamagnetic materials are those that some people generally think of as "non-magnetic", and include water, wood, most organic compounds such as petroleum and some plastics, and many metals including copper, particularly the heavy ones with many core electrons, such as mercury, gold and bismuth. The magnetic susceptibility values of various molecular fragments are called Pascal's constants (named after ).

Diamagnetic materials, like water, or water-based materials, have a relative magnetic permeability that is less than or equal to 1, and therefore a magnetic susceptibility less than or equal to 0, since susceptibility is defined as . This means that diamagnetic materials are repelled by magnetic fields. However, since diamagnetism is such a weak property, its effects are not observable in everyday life. For example, the magnetic susceptibility of diamagnets such as water is . The most strongly diamagnetic material is bismuth, , although pyrolytic carbon may have a susceptibility of in one plane. Nevertheless, these values are orders of magnitude smaller than the magnetism exhibited by paramagnets and ferromagnets. Because "χ" is derived from the ratio of the internal magnetic field to the applied field, it is a dimensionless value.

In rare cases, the diamagnetic contribution can be stronger than paramagnetic contribution. This is the case for gold, which has a magnetic susceptibility less than 0 (and is thus by definition a diamagnetic material), but when measured carefully with X-ray magnetic circular dichroism, has an extremely weak paramagnetic contribution that is overcome by a stronger diamagnetic contribution.

Superconductors may be considered perfect diamagnets (), because they expel all magnetic fields (except in a thin surface layer) due to the Meissner effect.

If a powerful magnet (such as a supermagnet) is covered with a layer of water (that is thin compared to the diameter of the magnet) then the field of the magnet significantly repels the water. This causes a slight dimple in the water's surface that may be seen by a reflection in its surface.

Diamagnets may be levitated in stable equilibrium in a magnetic field, with no power consumption. Earnshaw's theorem seems to preclude the possibility of static magnetic levitation. However, Earnshaw's theorem applies only to objects with positive susceptibilities, such as ferromagnets (which have a permanent positive moment) and paramagnets (which induce a positive moment). These are attracted to field maxima, which do not exist in free space. Diamagnets (which induce a negative moment) are attracted to field minima, and there can be a field minimum in free space.

A thin slice of pyrolytic graphite, which is an unusually strongly diamagnetic material, can be stably floated in a magnetic field, such as that from rare earth permanent magnets. This can be done with all components at room temperature, making a visually effective and relatively convenient demonstration of diamagnetism.

The Radboud University Nijmegen, the Netherlands, has conducted experiments where water and other substances were successfully levitated. Most spectacularly, a live frog (see figure) was levitated.

In September 2009, NASA's Jet Propulsion Laboratory (JPL) in Pasadena, California announced it had successfully levitated mice using a superconducting magnet, an important step forward since mice are closer biologically to humans than frogs. JPL said it hopes to perform experiments regarding the effects of microgravity on bone and muscle mass.

Recent experiments studying the growth of protein crystals have led to a technique using powerful magnets to allow growth in ways that counteract Earth's gravity.

A simple homemade device for demonstration can be constructed out of bismuth plates and a few permanent magnets that levitate a permanent magnet.

The electrons in a material generally settle in orbitals, with effectively zero resistance and act like current loops. Thus it might be imagined that diamagnetism effects in general would be common, since any applied magnetic field would generate currents in these loops that would oppose the change, in a similar way to superconductors, which are essentially perfect diamagnets. However, since the electrons are rigidly held in orbitals by the charge of the protons and are further constrained by the Pauli exclusion principle, many materials exhibit diamagnetism, but typically respond very little to the applied field.

The Bohr–Van Leeuwen theorem proves that there cannot be any diamagnetism or paramagnetism in a purely classical system. However, the classical theory of Langevin for diamagnetism gives the same prediction as the quantum theory. The classical theory is given below.

Paul Langevin's theory of diamagnetism (1905) applies to materials containing atoms with closed shells (see dielectrics). A field with intensity , applied to an electron with charge and mass , gives rise to Larmor precession with frequency . The number of revolutions per unit time is , so the current for an atom with electrons is (in SI units)

The magnetic moment of a current loop is equal to the current times the area of the loop. Suppose the field is aligned with the axis. The average loop area can be given as formula_2, where formula_3 is the mean square distance of the electrons perpendicular to the axis. The magnetic moment is therefore

If the distribution of charge is spherically symmetric, we can suppose that the distribution of coordinates are independent and identically distributed. Then formula_5, where formula_6 is the mean square distance of the electrons from the nucleus. Therefore, formula_7. If formula_8 is the number of atoms per unit volume, the volume diamagnetic susceptibility in SI units is

In atoms, Langevin susceptibility is of the same order of magnitude as Van Vleck paramagnetic susceptibility.

The Langevin theory is not the full picture for metals because there are also non-localized electrons. The theory that describes diamagnetism in a free electron gas is called Landau diamagnetism, named after Lev Landau, and instead considers the weak counteracting field that forms when the electrons' trajectories are curved due to the Lorentz force. Landau diamagnetism, however, should be contrasted with Pauli paramagnetism, an effect associated with the polarization of delocalized electrons' spins. For the bulk case of a 3D system and low magnetic fields, the (volume) diamagnetic susceptibility can be calculated using Landau quantization, which in SI units is

where formula_11 is the Fermi energy. This is equivalent to formula_12, exactly formula_13 times Pauli paramagnetic susceptibility, where formula_14 is the Bohr magneton and formula_15 is the density of states (number of states per energy per volume). This formula takes into account the spin degeneracy of the carriers (spin ½ electrons).

In doped semiconductors the ratio between Landau and Pauli susceptibilities may change due to the effective mass of the charge carriers differing from the electron mass in vacuum, increasing the diamagnetic contribution. The formula presented here only applies for the bulk; in confined systems like quantum dots, the description is altered due to quantum confinement. Additionally, for strong magnetic fields, the susceptibility of delocalized electrons oscillates as a function of the field strength, a phenomenon known as the De Haas–Van Alphen effect, also first described theoretically by Landau.



Duke of Marlborough (title)

Duke of Marlborough ( ) is a title in the Peerage of England. It was created by Queen Anne in 1702 for John Churchill, 1st Earl of Marlborough (1650–1722), the noted military leader. In historical texts, unqualified use of the title typically refers to the 1st Duke. The name of the dukedom refers to Marlborough in Wiltshire.

The earldom of Marlborough was held by the family of Ley from its creation in 1626 until its extinction with the death of the 4th earl in 1679. The title was recreated 10 years later for John Churchill (in 1689).

Churchill had been made "Lord Churchill of Eyemouth" (1682) in the Peerage of Scotland, and "Baron Churchill" of Sandridge (1685) and "Earl of Marlborough" (1689) in the Peerage of England. Shortly after her accession to the throne in 1702, Queen Anne made Churchill the first "Duke of Marlborough" and granted him the subsidiary title "Marquess of Blandford".

In 1678, Churchill married Sarah Jennings (1660–1744), a courtier and influential favourite of the queen. They had seven children, of whom four daughters married into some of the most important families in Great Britain; one daughter and one son died in infancy. He was pre-deceased by his son, John Churchill, Marquess of Blandford, in 1703; so, to prevent the extinction of the titles, a special Act of Parliament was passed. When the 1st Duke of Marlborough died in 1722 his title as "Lord Churchill of Eyemouth" in the Peerage of Scotland became extinct and the Marlborough titles passed, according to the Act, to his eldest daughter Henrietta (1681–1733), the 2nd Duchess of Marlborough. She was married to Francis Godolphin, 2nd Earl of Godolphin and had a son who predeceased her.

When Henrietta died in 1733, the Marlborough titles passed to her nephew Charles Spencer (1706–1758), the third son of her late sister Anne (1683–1716), who had married the 3rd Earl of Sunderland in 1699. After his older brother's death in 1729, Charles Spencer had already inherited the Spencer family estates and the titles of "Earl of Sunderland" (1643) and "Baron Spencer" of Wormleighton (1603), all in the Peerage of England. Upon his maternal aunt Henrietta's death in 1733, Charles Spencer succeeded to the Marlborough family estates and titles and became the 3rd Duke. When he died in 1758, his titles passed to his eldest son George (1739–1817), who was succeeded by his eldest son George, the 5th Duke (1766–1840). In 1815, Francis Spencer (the younger son of the 4th Duke) was created "Baron Churchill" in the Peerage of the United Kingdom. In 1902, his grandson, the 3rd Baron Churchill, was created Viscount Churchill.

In 1817, the 5th Duke obtained permission to assume and bear the surname of Churchill in addition to his surname of Spencer, to perpetuate the name of his illustrious great-great-grandfather. At the same time he received Royal Licence to quarter the coat of arms of Churchill with his paternal arms of Spencer. The modern Dukes thus originally bore the surname "Spencer": the double-barrelled surname of "Spencer-Churchill" as used since 1817 remains in the family, although many members have preferred to style themselves simply as "Churchill".

The 7th Duke was the paternal grandfather of the British Prime Minister Sir Winston Churchill, born at Blenheim Palace on 30 November 1874.

The 11th Duke, John Spencer-Churchill died in 2014, having assumed the title in 1972. The 12th and present Duke is Charles James Spencer-Churchill.

The family seat is Blenheim Palace in Woodstock, Oxfordshire.

After his leadership in the victory against the French in the Battle of Blenheim on 13 August 1704, the 1st Duke was honoured by Queen Anne granting him the royal manor of Woodstock, and building him a house at her expense to be called Blenheim. Construction started in 1705 and the house was completed in 1722, the year of the 1st Duke's death. Blenheim Palace has since remained in the Churchill and Spencer-Churchill family.

With the exception of the 10th Duke and his first wife, the dukes and duchesses of Marlborough are buried in Blenheim Palace's chapel. Most other members of the Spencer-Churchill family are interred in St. Martin's parish churchyard at Bladon, a short distance from the palace.

The dukedom can theoretically pass through a female line. However, unlike the remainder to heirs general found in most other peerages that allow male-preference primogeniture, the grant does not allow for abeyance and follows a more restrictive Semi-Salic formula designed to keep succession wherever possible in the male line.

Succession to the title under the first and second contingencies has lapsed; holders of the title from the 3rd Duke trace their status from the third contingency.

It is now very unlikely that the dukedom will be passed to a woman or through a woman, since all the male-line descendants of the 1st Duke's second daughter Anne Spencer, Countess of Sunderland—including the lines of the Viscounts Churchill and Barons Churchill of Wychwood and of the Earl Spencer and of the entire Spencer-Churchill and Spencer family—would have to become extinct.

If that were to happen, the Churchill titles would pass to the Earl of Jersey, the heir-male of the 1st Duke's granddaughter Anne Villiers (born Egerton), Countess of Jersey, daughter of Elizabeth Egerton, Duchess of Bridgewater, the third daughter of the first Duke.


The Duke holds subsidiary titles: "Marquess of Blandford" (created in 1702 for John Churchill), "Earl of Sunderland" (created in 1643 for the Spencer family), "Earl of Marlborough" (created in 1689 for John Churchill), "Baron Spencer" of Wormleighton (created in 1603 for the Spencer family), and "Baron Churchill" of Sandridge (created in 1685 for John Churchill), all in the Peerage of England.

The title "Marquess of Blandford" is used as the courtesy title for the Duke's eldest son and heir. The Duke's eldest son's eldest son can use the courtesy title "Earl of Sunderland", and the duke's eldest son's eldest son's eldest son (not necessarily the eldest great-grandson) the title "Lord Spencer of Wormleighton" (not to be confused with Earl Spencer).

The title of "Earl of Marlborough", created for John Churchill in 1689, had previously been created for James Ley, in 1626, becoming extinct in 1679.

The 1st Duke was honoured with land and titles in the Holy Roman Empire: Emperor Leopold I created him a Prince in 1704, and in 1705, his successor Emperor Joseph I gave him the principality of Mindelheim (once the lordship of the noted soldier Georg von Frundsberg). He was obliged to surrender Mindelheim in 1714 by the Treaty of Utrecht, which returned it to Bavaria. He tried to obtain Nellenburg in Austria in exchange, which at that time was only a county ('Landgrafschaft'), but this failed, partially because Austrian law did not allow for Nellenburg to be converted into a sovereign principality. The 1st Duke's princely title of Mindelheim became extinct either on the return of the land to Bavaria or on his death, as the Empire operated Salic Law, which prevented female succession.

The original arms of Sir Winston Churchill (1620–1688), father of the 1st Duke of Marlborough, were simple and in use by his own father in 1619. The shield was Sable a lion rampant Argent, debruised by a bendlet Gules. The addition of a canton of Saint George (see below) rendered the distinguishing mark of the bendlet unnecessary.

The Churchill crest is blazoned as a lion couchant guardant Argent, supporting with its dexter forepaw a banner Gules, charged with a dexter hand appaumée of the first, staff Or.

In recognition of Sir Winston's services to King Charles I as Captain of the Horse, and his loyalty to King Charles II as a Member of Parliament, he was awarded an augmentation of honour to his arms around 1662. This rare mark of royal favour took the form of a canton of Saint George. At the same time, he was authorised to omit the bendlet, which had served the purpose of distinguishing this branch of the Churchill family from others which bore an undifferenced lion.

Sir Winston's shield and crest were inherited by his son John Churchill, 1st Duke of Marlborough. Minor modifications reflected the bearer's social rise: the helm was now shown in profile and had a closed grille to signify the bearer's rank as a peer, and there were now supporters placed on either side of the shield. They were the mythical Griffin (part lion, part eagle) and Wyvern (a dragon without hind legs). The supporters were derived from the arms of the family of the 1st Duke's mother, Drake of Ash (Argent, a wyvern gules; these arms can be seen on the monument in Musbury Church to Sir Bernard Drake, d.1586).

The motto was "Fiel pero desdichado" (Spanish for "Faithful but unfortunate"). The 1st Duke was also entitled to a coronet indicating his rank.

When the 1st Duke was made a Prince of the Holy Roman Empire in 1705, two unusual features were added: the Imperial Eagle and a Princely Coronet. His estates in Germany, such as Mindelheim, were represented in his arms by additional quarterings.

In 1817, the 5th Duke received Royal Licence to place the quarter of Churchill ahead of his paternal arms of Spencer. The shield of the Spencer family arms is: quarterly Argent and Gules, in the second and third quarters a fret Or, over all on a bend Sable three escallops of the first. The Spencer crest is: out of a ducal coronet Or, a griffin's head between two wings expanded Argent, gorged with a collar gemel and armed Gules. Paul Courtenay observes that "It would be normal in these circumstances for the paternal arms (Spencer) to take precedence over the maternal (Churchill), but because the Marlborough dukedom was senior to the Sunderland earldom, the procedure was reversed in this case."

Also in 1817, a further augmentation of honour was added to his armorial achievement. This incorporated the bearings from the standard of the Manor of Woodstock and was borne on an escutcheon, displayed over all in the centre chief point, as follows: Argent a cross of Saint George surmounted by an inescutcheon Azure, charged with three fleurs-de-lys Or, two over one. This inescutcheon represents the royal arms of France.

These quartered arms, incorporating the two augmentations of honour, have been the arms of all subsequent Dukes of Marlborough.

The motto "Fiel pero desdichado" is Spanish for 'Faithful but unfortunate'. "Desdichado" means without happiness or without joy, alluding to the first Duke's father, Winston, who was a royalist and faithful supporter of the king during the English Civil War but was not compensated for his losses after the restoration. Charles II knighted Winston Churchill and other Civil War royalists but did not compensate them for their wartime losses, thereby inducing Winston to adopt the motto. It is unusual for the motto of an Englishman of the era to be in Spanish rather than Latin, and it is not known why this is the case.

The earldom of Marlborough was held by the family of Ley from 1626 to 1679. James Ley, the 1st Earl (c. 1550 – 1629), was lord chief justice of the King’s Bench in Ireland and then in England; he was an English member of parliament and was lord high treasurer from 1624 to 1628. In 1624 he was created Baron Ley and in 1626 Earl of Marlborough. The 3rd earl was his grandson James (1618–1665), a naval officer who was killed in action with the Dutch. James was succeeded by his uncle William, a younger son of the 1st earl, on whose death in 1679 the earldom became extinct.




The heir apparent to the dukedom is George John Godolphin Spencer-Churchill, Marquess of Blandford (b. 1992), eldest son of the 12th Duke.

<section begin=FamilyTree />

<section end="FamilyTree" />


Difference engine

A difference engine is an automatic mechanical calculator designed to tabulate polynomial functions. It was designed in the 1820s, and was first created by Charles Babbage. The name "difference engine" is derived from the method of divided differences, a way to interpolate or tabulate functions by using a small set of polynomial co-efficients. Some of the most common mathematical functions used in engineering, science and navigation are built from logarithmic and trigonometric functions, which can be approximated by polynomials, so a difference engine can compute many useful tables.

The notion of a mechanical calculator for mathematical functions can be traced back to the Antikythera mechanism of the 2nd century BC, while early modern examples are attributed to Pascal and Leibniz in the 17th century.

In 1784 J. H. Müller, an engineer in the Hessian army, devised and built an adding machine and described the basic principles of a difference machine in a book published in 1786 (the first written reference to a difference machine is dated to 1784), but he was unable to obtain funding to progress with the idea.

Charles Babbage began to construct a small difference engine in c. 1819 and had completed it by 1822 (Difference Engine 0). He announced his invention on 14 June 1822, in a paper to the Royal Astronomical Society, entitled "Note on the application of machinery to the computation of astronomical and mathematical tables". This machine used the decimal number system and was powered by cranking a handle. The British government was interested, since producing tables was time-consuming and expensive and they hoped the difference engine would make the task more economical.

In 1823, the British government gave Babbage £1700 to start work on the project. Although Babbage's design was feasible, the metalworking techniques of the era could not economically make parts in the precision and quantity required. Thus the implementation proved to be much more expensive and doubtful of success than the government's initial estimate. According to the 1830 design for Difference Engine No. 1, it would have about 25,000 parts, weigh 4 tons, and operate on 20-digit numbers by sixth-order differences. In 1832, Babbage and Joseph Clement produced a small working model (one-seventh of the plan), which operated on 6-digit numbers by second-order differences. Lady Byron described seeing the working prototype in 1833: "We both went to see the thinking machine (or so it seems) last Monday. It raised several Nos. to the 2nd and 3rd powers, and extracted the root of a Quadratic equation." Work on the larger engine was suspended in 1833.

By the time the government abandoned the project in 1842, Babbage had received and spent over £17,000 on development, which still fell short of achieving a working engine. The government valued only the machine's output (economically produced tables), not the development (at unpredictable cost) of the machine itself. Babbage refused to recognize that predicament. Meanwhile, Babbage's attention had moved on to developing an analytical engine, further undermining the government's confidence in the eventual success of the difference engine. By improving the concept as an analytical engine, Babbage had made the difference engine concept obsolete, and the project to implement it an utter failure in the view of the government.

The incomplete Difference Engine No. 1 was put on display to the public at the 1862 International Exhibition in South Kensington, London.

Babbage went on to design his much more general analytical engine, but later produced an improved "Difference Engine No. 2" design (31-digit numbers and seventh-order differences), between 1846 and 1849. Babbage was able to take advantage of ideas developed for the analytical engine to make the new difference engine calculate more quickly while using fewer parts.

Inspired by Babbage's difference engine in 1834, Per Georg Scheutz built several experimental models. In 1837 his son Edward proposed to construct a working model in metal, and in 1840 finished the calculating part, capable of calculating series with 5-digit numbers and first-order differences, which was later extended to third-order (1842). In 1843, after adding the printing part, the model was completed.

In 1851, funded by the government, construction of the larger and improved (15-digit numbers and fourth-order differences) machine began, and finished in 1853. The machine was demonstrated at the World's Fair in Paris, 1855 and then sold in 1856 to the Dudley Observatory in Albany, New York. Delivered in 1857, it was the first printing calculator sold. In 1857 the British government ordered the next Scheutz's difference machine, which was built in 1859. It had the same basic construction as the previous one, weighing about .

Martin Wiberg improved Scheutz's construction (c. 1859, his machine has the same capacity as Scheutz's: 15-digit and fourth-order) but used his device only for producing and publishing printed tables (interest tables in 1860, and logarithmic tables in 1875).

Alfred Deacon of London in c. 1862 produced a small difference engine (20-digit numbers and third-order differences).

American George B. Grant started working on his calculating machine in 1869, unaware of the works of Babbage and Scheutz (Schentz). One year later (1870) he learned about difference engines and proceeded to design one himself, describing his construction in 1871. In 1874 the Boston Thursday Club raised a subscription for the construction of a large-scale model, which was built in 1876. It could be expanded to enhance precision and weighed about .

Christel Hamann built one machine (16-digit numbers and second-order differences) in 1909 for the "Tables of Bauschinger and Peters" ("Logarithmic-Trigonometrical Tables with eight decimal places"), which was first published in Leipzig in 1910. It weighed about .

Burroughs Corporation in about 1912 built a machine for the Nautical Almanac Office which was used as a difference engine of second-order. It was later replaced in 1929 by a Burroughs Class 11 (13-digit numbers and second-order differences, or 11-digit numbers and <nowiki>[at least up to]</nowiki> fifth-order differences).

Alexander John Thompson about 1927 built "integrating and differencing machine" (13-digit numbers and fifth-order differences) for his table of logarithms "Logarithmetica britannica". This machine was composed of four modified Triumphator calculators.

Leslie Comrie in 1928 described how to use the Brunsviga-Dupla calculating machine as a difference engine of second-order (15-digit numbers). He also noted in 1931 that National Accounting Machine Class 3000 could be used as a difference engine of sixth-order.

During the 1980s, Allan G. Bromley, an associate professor at the University of Sydney, Australia, studied Babbage's original drawings for the Difference and Analytical Engines at the Science Museum library in London. This work led the Science Museum to construct a working calculating section of difference engine No. 2 from 1985 to 1991, under Doron Swade, the then Curator of Computing. This was to celebrate the 200th anniversary of Babbage's birth in 1991. In 2002, the printer which Babbage originally designed for the difference engine was also completed. The conversion of the original design drawings into drawings suitable for engineering manufacturers' use revealed some minor errors in Babbage's design (possibly introduced as a protection in case the plans were stolen), which had to be corrected. The difference engine and printer were constructed to tolerances achievable with 19th-century technology, resolving a long-standing debate as to whether Babbage's design could have worked using Georgian-era engineering methods. The machine contains 8,000 parts and weighs about 5 tons.

The printer's primary purpose is to produce stereotype plates for use in printing presses, which it does by pressing type into soft plaster to create a flong. Babbage intended that the Engine's results be conveyed directly to mass printing, having recognized that many errors in previous tables were not the result of human calculating mistakes but from slips in the manual typesetting process. The printer's paper output is mainly a means of checking the engine's performance.

In addition to funding the construction of the output mechanism for the Science Museum's difference engine, Nathan Myhrvold commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the Computer History Museum in Mountain View, California from May 2008 to January 2016. It has since been transferred to Intellectual Ventures in Seattle where it is on display just outside the main lobby.

The difference engine consists of a number of columns, numbered from 1 to N. The machine is able to store one decimal number in each column. The machine can only add the value of a column "n" + 1 to column "n" to produce the new value of "n". Column "N" can only store a constant, column 1 displays (and possibly prints) the value of the calculation on the current iteration.

The engine is programmed by setting initial values to the columns. Column 1 is set to the value of the polynomial at the start of computation. Column 2 is set to a value derived from the first and higher derivatives of the polynomial at the same value of X. Each of the columns from 3 to "N" is set to a value derived from the formula_1 first and higher derivatives of the polynomial.

In the Babbage design, one iteration (i.e. one full set of addition and carry operations) happens for each rotation of the main shaft. Odd and even columns alternately perform an addition in one cycle. The sequence of operations for column formula_2 is thus:

Steps 1,2,3,4 occur for every odd column, while steps 3,4,1,2 occur for every even column.

While Babbage's original design placed the crank directly on the main shaft, it was later realized that the force required to crank the machine would have been too great for a human to handle comfortably. Therefore, the two models that were built incorporate a 4:1 reduction gear at the crank, and four revolutions of the crank are required to perform one full cycle.

Each iteration creates a new result, and is accomplished in four steps corresponding to four complete turns of the handle shown at the far right in the picture below. The four steps are:

The engine represents negative numbers as ten's complements. Subtraction amounts to addition of a negative number. This works in the same manner that modern computers perform subtraction, known as two's complement.

The principle of a difference engine is Newton's method of divided differences. If the initial value of a polynomial (and of its finite differences) is calculated by some means for some value of X, the difference engine can calculate any number of nearby values, using the method generally known as the method of finite differences. For example, consider the quadratic polynomial

with the goal of tabulating the values "p"(0), "p"(1), "p"(2), "p"(3), "p"(4), and so forth. The table below is constructed as follows: the second column contains the values of the polynomial, the third column contains the differences of the two left neighbors in the second column, and the fourth column contains the differences of the two neighbors in the third column:

The numbers in the third values-column are constant. In fact, by starting with any polynomial of degree "n", the column number "n" + 1 will always be constant. This is the crucial fact behind the success of the method.

This table was built from left to right, but it is possible to continue building it from right to left down a diagonal in order to compute more values. To calculate "p"(5) use the values from the lowest diagonal. Start with the fourth column constant value of 4 and copy it down the column. Then continue the third column by adding 4 to 11 to get 15. Next continue the second column by taking its previous value, 22 and adding the 15 from the third column. Thus "p"(5) is 22 + 15 = 37. In order to compute "p"(6), we iterate the same algorithm on the "p"(5) values: take 4 from the fourth column, add that to the third column's value 15 to get 19, then add that to the second column's value 37 to get 56, which is "p"(6). This process may be continued "ad infinitum". The values of the polynomial are produced without ever having to multiply. A difference engine only needs to be able to add. From one loop to the next, it needs to store 2 numbers—in this example (the last elements in the first and second columns). To tabulate polynomials of degree "n", one needs sufficient storage to hold "n" numbers.

Babbage's difference engine No. 2, finally built in 1991, can hold 8 numbers of 31 decimal digits each and can thus tabulate 7th degree polynomials to that precision. The best machines from Scheutz could store 4 numbers with 15 digits each.

The initial values of columns can be calculated by first manually calculating N consecutive values of the function and by backtracking (i.e. calculating the required differences).

Col formula_6 gets the value of the function at the start of computation formula_7. Col formula_8 is the difference between formula_9 and formula_7...

If the function to be calculated is a polynomial function, expressed as
the initial values can be calculated directly from the constant coefficients "a", "a","a", ..., "a" without calculating any data points. The initial values are thus:


Many commonly used functions are analytic functions, which can be expressed as power series, for example as a Taylor series. The initial values can be calculated to any degree of accuracy; if done correctly the engine will give exact results for first N steps. After that, the engine will only give an approximation of the function.

The Taylor series expresses the function as a sum obtained from its derivatives at one point. For many functions the higher derivatives are trivial to obtain; for instance, the sine function at 0 has values of 0 or formula_19 for all derivatives. Setting 0 as the start of computation we get the simplified Maclaurin series

The same method of calculating the initial values from the coefficients can be used as for polynomial functions. The polynomial constant coefficients will now have the value

The problem with the methods described above is that errors will accumulate and the series will tend to diverge from the true function. A solution which guarantees a constant maximum error is to use curve fitting. A minimum of "N" values are calculated evenly spaced along the range of the desired calculations. Using a curve fitting technique like Gaussian reduction an "N"−1th degree polynomial interpolation of the function is found. With the optimized polynomial, the initial values can be calculated as above.


Draupnir

In Norse mythology, Draupnir (Old Norse: , "the dripper") is a gold ring possessed by the god Odin with the ability to multiply itself: Every ninth night, eight new rings 'drip' from Draupnir, each one of the same size and weight as the original.

Draupnir was forged by the dwarven brothers Brokkr and Eitri (or Sindri). Brokkr and Eitri made this ring as one of a set of three gifts which included Mjöllnir and Gullinbursti. They made these gifts in accordance with a bet Loki made saying that Brokkr and Eitri could not make better gifts than the three made by the Sons of Ivaldi. In the end, Mjöllnir, Thor's hammer, won the contest for Brokkr and Eitri. Loki used a loophole to get out of the wager for his head (the wager was for Loki's head only, but he argued that, to remove his head, they would have to injure his neck, which was not in the bargain) and Brokkr punished him by sealing his lips shut with wire.

The ring was placed by Odin on the funeral pyre of his son Baldr:

Odin laid upon the pyre the gold ring called Draupnir; this quality attended it: that every ninth night there fell from it eight gold rings of equal weight. (from the "Gylfaginning").
The ring was subsequently retrieved by Hermóðr. It was offered as a gift by Freyr's servant Skírnir in the wooing of Gerðr, which is described in the poem "Skírnismál".

The ring appears in the 2022 video game "God of War Ragnarök", where it is used as a base for creating the new weapon of Kratos. The ability of replicating itself is instead used to create spears that can be thrown and explode on command.


Divergence

In vector calculus, divergence is a vector operator that operates on a vector field, producing a scalar field giving the quantity of the vector field's source at each point. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.

As an example, consider air as it is heated or cooled. The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region. The divergence of the velocity field in that region would thus have a positive value. While the air is cooled and thus contracting, the divergence of the velocity has a negative value.

In physical terms, the divergence of a vector field is the extent to which the vector field flux behaves like a source at a given point. It is a local measure of its "outgoingness" – the extent to which there are more of the field vectors exiting from an infinitesimal region of space than entering it. A point at which the flux is outgoing has positive divergence, and is often called a "source" of the field. A point at which the flux is directed inward has negative divergence, and is often called a "sink" of the field. The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. A point at which there is zero flux through an enclosing surface has zero divergence.

The divergence of a vector field is often illustrated using the simple example of the velocity field of a fluid, a liquid or gas. A moving gas has a velocity, a speed and direction at each point, which can be represented by a vector, so the velocity of the gas forms a vector field. If a gas is heated, it will expand. This will cause a net motion of gas particles outward in all directions. Any closed surface in the gas will enclose gas which is expanding, so there will be an outward flux of gas through the surface. So the velocity field will have positive divergence everywhere. Similarly, if the gas is cooled, it will contract. There will be more room for gas particles in any volume, so the external pressure of the fluid will cause a net flow of gas volume inward through any closed surface. Therefore the velocity field has negative divergence everywhere. In contrast, in a gas at a constant temperature and pressure, the net flux of gas out of any closed surface is zero. The gas may be moving, but the volume rate of gas flowing into any closed surface must equal the volume rate flowing out, so the "net" flux is zero. Thus the gas velocity has zero divergence everywhere. A field which has zero divergence everywhere is called solenoidal.

If the gas is heated only at one point or small region, or a small tube is introduced which supplies a source of additional gas at one point, the gas there will expand, pushing fluid particles around it outward in all directions. This will cause an outward velocity field throughout the gas, centered on the heated point. Any closed surface enclosing the heated point will have a flux of gas particles passing out of it, so there is positive divergence at that point. However any closed surface "not" enclosing the point will have a constant density of gas inside, so just as many fluid particles are entering as leaving the volume, thus the net flux out of the volume is zero. Therefore the divergence at any other point is zero.

[[Image:Definition of divergence.svg|thumb|The divergence at a point is the limit of the ratio of the flux formula_1 through the surface "(red arrows)" to the volume formula_2 for any sequence of closed regions enclosing that approaches zero volume: formula_3, where is the [[metric tensor]]. The [[determinant]] appears because it provides the appropriate invariant definition of the volume, given a set of vectors. Since the determinant is a scalar quantity which doesn't depend on the indices, these can be suppressed, writing formula_4. The absolute value is taken in order to handle the general case where the determinant might be negative, such as in pseudo-Riemannian spaces. The reason for the square-root is a bit subtle: it effectively avoids double-counting as one goes from curved to Cartesian coordinates, and back. The volume (the determinant) can also be understood as the [[Jacobian matrix and determinant|Jacobian]] of the transformation from Cartesian to curvilinear coordinates, which for gives 

Some conventions expect all local basis elements to be normalized to unit length, as was done in the previous sections. If we write formula_5 for the normalized basis, and formula_6 for the components of with respect to it, we have that 
using one of the properties of the metric tensor. By dotting both sides of the last equality with the contravariant element formula_8, we can conclude that formula_9. After substituting, the formula becomes:

See "" for further discussion.

The following properties can all be derived from the ordinary differentiation rules of [[calculus]]. Most importantly, the divergence is a [[linear operator]], i.e.,

for all vector fields and and all [[real number]]s and .

There is a [[product rule]] of the following type: if is a scalar-valued function and is a vector field, then

or in more suggestive notation

Another product rule for the [[cross product]] of two vector fields and in three dimensions involves the [[Curl (mathematics)|curl]] and reads as follows:

or

The [[Laplacian]] of a [[scalar field]] is the divergence of the field's [[gradient]]:

The divergence of the [[Curl (mathematics)|curl]] of any vector field (in three dimensions) is equal to zero: 

If a vector field with zero divergence is defined on a ball in , then there exists some vector field on the ball with . For regions in more topologically complicated than this, the latter statement might be false (see [[Poincaré lemma]]). The degree of "failure" of the truth of the statement, measured by the [[homology (mathematics)|homology]] of the [[chain complex]]

serves as a nice quantification of the complicatedness of the underlying region . These are the beginnings and main motivations of [[de Rham cohomology]].

It can be shown that any stationary flux that is twice continuously differentiable in and vanishes sufficiently fast for can be decomposed uniquely into an "irrotational part" and a "source-free part" . Moreover, these parts are explicitly determined by the respective "source densities" (see above) and "circulation densities" (see the article [[Curl (mathematics)|Curl]]):

For the irrotational part one has

with

The source-free part, , can be similarly written: one only has to replace the "scalar potential" by a "vector potential" and the terms by , and the source density 
by the circulation density .

This "decomposition theorem" is a by-product of the stationary case of [[electrodynamics]]. It is a special case of the more general [[Helmholtz decomposition]], which works in dimensions greater than three as well.

The divergence of a vector field can be defined in any finite number formula_21 of dimensions. If 

in a Euclidean coordinate system with coordinates , define

In the 1D case, reduces to a regular function, and the divergence reduces to the derivative.

For any , the divergence is a linear operator, and it satisfies the "product rule"

for any scalar-valued function .

One can express the divergence as a particular case of the [[exterior derivative]], which takes a [[2-form]] to a 3-form in . Define the current two-form as
It measures the amount of "stuff" flowing through a surface per unit time in a "stuff fluid" of density moving with local velocity . Its exterior derivative is then given by
where formula_27 is the [[wedge product]].

Thus, the divergence of the vector field can be expressed as:
Here the superscript is one of the two [[musical isomorphism]]s, and is the [[Hodge star operator]]. When the divergence is written in this way, the operator formula_29 is referred to as the [[codifferential]]. Working with the current two-form and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.

The appropriate expression is more complicated in [[Curvilinear coordinates#Grad, curl, div, Laplacian|curvilinear coordinates]]. The divergence of a vector field extends naturally to any [[differentiable manifold]] of dimension that has a [[volume form]] (or [[density on a manifold|density]]) , e.g. a [[Riemannian manifold|Riemannian]] or [[Lorentzian manifold]]. Generalising the construction of a [[two-form]] for a vector field on , on such a manifold a vector field defines an -form obtained by contracting with . The divergence is then the function defined by

The divergence can be defined in terms of the [[Lie derivative]] as

This means that the divergence measures the rate of expansion of a unit of volume (a [[volume element]]) as it flows with the vector field.

On a [[pseudo-Riemannian manifold]], the divergence with respect to the volume can be expressed in terms of the [[Levi-Civita connection]] :

where the second expression is the contraction of the vector field valued 1-form with itself and the last expression is the traditional coordinate expression from [[Ricci calculus]].

An equivalent expression without using a connection is

where is the [[metric tensor|metric]] and formula_34 denotes the partial derivative with respect to coordinate . The square-root of the (absolute value of the [[determinant]] of the) metric appears because the divergence must be written with the correct conception of the [[volume]]. In curvilinear coordinates, the basis vectors are no longer orthonormal; the determinant encodes the correct idea of volume in this case. It appears twice, here, once, so that the formula_35 can be transformed into "flat space" (where coordinates are actually orthonormal), and once again so that formula_34 is also transformed into "flat space", so that finally, the "ordinary" divergence can be written with the "ordinary" concept of volume in flat space ("i.e." unit volume, "i.e." one, "i.e." not written down). The square-root appears in the denominator, because the derivative transforms in the opposite way ([[Covariance and contravariance of vectors|contravariantly]]) to the vector (which is [[Covariance and contravariance of vectors|covariant]]). This idea of getting to a "flat coordinate system" where local computations can be done in a conventional way is called a [[vielbein]]. A different way to see this is to note that the divergence is the [[codifferential]] in disguise. That is, the divergence corresponds to the expression formula_37 with formula_38 the [[differential of a function|differential]] and formula_39 the [[Hodge star]]. The Hodge star, by its construction, causes the [[volume form]] to appear in all of the right places.

Divergence can also be generalised to [[tensor]]s. In [[Einstein notation]], the divergence of a [[contravariant vector]] is given by

where denotes the [[covariant derivative]]. In this general setting, the correct formulation of the divergence is to recognize that it is a [[codifferential]]; the appropriate properties follow from there.

Equivalently, some authors define the divergence of a [[mixed tensor]] by using the [[musical isomorphism]] : if is a -[[tensor]] ( for the contravariant vector and for the covariant one), then we define the "divergence of " to be the -tensor

that is, we take the trace over the "first two" covariant indices of the covariant derivative.
The formula_42 symbol refers to the [[musical isomorphism]].




[[Category:Differential operators]]
[[Category:Linear operators in calculus]]
[[Category:Vector calculus]]
Decision problem

In computability theory and computational complexity theory, a decision problem is a computational problem that can be posed as a yes–no question of the input values. An example of a decision problem is deciding by means of an algorithm whether a given natural number is prime. Another is the problem "given two numbers "x" and "y", does "x" evenly divide "y"?". The answer is either 'yes' or 'no' depending upon the values of "x" and "y". A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem "given two numbers "x" and "y", does "x" evenly divide "y"?" would give the steps for determining whether "x" evenly divides "y". One such algorithm is long division. If the remainder is zero the answer is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm is called "decidable".

Decision problems typically appear in mathematical questions of decidability, that is, the question of the existence of an effective method to determine the existence of some object or its membership in a set; some of the most important problems in mathematics are undecidable.

The field of computational complexity categorizes "decidable" decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes "undecidable" decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution.

A "decision problem" is a yes-or-no question on an infinite set of inputs. It is traditional to define the decision problem as the set of possible inputs together with the set of inputs for which the answer is "yes".

These inputs can be natural numbers, but can also be values of some other kind, like binary strings or strings over some other alphabet. The subset of strings for which the problem returns "yes" is a formal language, and often decision problems are defined as formal languages.

Using an encoding such as Gödel numbering, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers. Therefore, the algorithm of a decision problem is to compute the characteristic function of a subset of the natural numbers.

A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.

A decision problem is "decidable" or "effectively solvable" if the set of inputs (or natural numbers) for which the answer is yes is a recursive set. A problem is "partially decidable", "semidecidable", "solvable", or "provable" if the set of inputs (or natural numbers) for which the answer is yes is a recursively enumerable set. Problems that are not decidable are "undecidable". For those it is not possible to create an algorithm, efficient or otherwise, that solves them.

The halting problem is an important undecidable decision problem; for more examples, see list of undecidable problems.

Decision problems can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions. A decision problem "P" is said to be "complete" for a set of decision problems "S" if "P" is a member of "S" and every problem in "S" can be reduced to "P". Complete decision problems are used in computational complexity theory to characterize complexity classes of decision problems. For example, the Boolean satisfiability problem is complete for the class NP of decision problems under polynomial-time reducibility.

Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'. A corresponding function problem is "given two numbers "x" and "y", what is "x" divided by "y"?".

A function problem consists of a partial function "f"; the informal "problem" is to compute the values of "f" on the inputs for which it is defined.

Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function. (The graph of a function "f" is the set of pairs ("x","y") such that "f"("x") = "y".) If this decision problem were effectively solvable then the function problem would be as well. This reduction does not respect computational complexity, however. For example, it is possible for the graph of a function to be decidable in polynomial time (in which case running time is computed as a function of the pair ("x","y")) when the function is not computable in polynomial time (in which case running time is computed as a function of "x" alone). The function "f"("x") = 2 has this property.

Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem. If this function is computable then the associated decision problem is decidable. However, this reduction is more liberal than the standard reduction used in computational complexity (sometimes called polynomial-time many-one reduction); for example, the complexity of the characteristic functions of an NP-complete problem and its co-NP-complete complement is exactly the same even though the underlying decision problems may not be considered equivalent in some typical models of computation.

Unlike decision problems, for which there is only one correct answer for each input, optimization problems are concerned with finding the "best" answer to a particular input. Optimization problems arise naturally in many applications, such as the traveling salesman problem and many questions in linear programming.

Function and optimization problems are often transformed into decision problems by considering the question of whether the output is "equal to" or "less than or equal to" a given value. This allows the complexity of the corresponding decision problem to be studied; and in many cases the original function or optimization problem can be solved by solving its corresponding decision problem. For example, in the traveling salesman problem, the optimization problem is to produce a tour with minimal weight. The associated decision problem is: for each "N", to decide whether the graph has any tour with weight less than "N". By repeatedly answering the decision problem, it is possible to find the minimal weight of a tour.

Because the theory of decision problems is very well developed, research in complexity theory has typically focused on decision problems. Optimization problems themselves are still of interest in computability theory, as well as in fields such as operations research.



Domain Name System

The Domain Name System (DNS) is a hierarchical and distributed naming system for computers, services, and other resources in the Internet or other Internet Protocol (IP) networks. It associates various information with "domain names" (identification strings) assigned to each of the associated entities. Most prominently, it translates readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. The Domain Name System has been an essential component of the functionality of the Internet since 1985.

The Domain Name System delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Network administrators may delegate authority over subdomains of their allocated name space to other name servers. This mechanism provides distributed and fault-tolerant service and was designed to avoid a single large central database. In addition, the DNS specifies the technical functionality of the database service that is at its core. It defines the DNS protocol, a detailed specification of the data structures and data communication exchanges used in the DNS, as part of the Internet protocol suite.

The Internet maintains two principal namespaces, the domain name hierarchy and the IP address spaces. The Domain Name System maintains the domain name hierarchy and provides translation services between it and the address spaces. Internet name servers and a communication protocol implement the Domain Name System. A DNS name server is a server that stores the DNS records for a domain; a DNS name server responds with answers to queries against its database.

The most common types of records stored in the DNS database are for start of authority (SOA), IP addresses (A and AAAA), SMTP mail exchangers (MX), name servers (NS), pointers for reverse DNS lookups (PTR), and domain name aliases (CNAME). Although not intended to be a general purpose database, DNS has been expanded over time to store records for other types of data for either automatic lookups, such as DNSSEC records, or for human queries such as "responsible person" (RP) records. As a general purpose database, the DNS has also been used in combating unsolicited email (spam) by storing a real-time blackhole list (RBL). The DNS database is traditionally stored in a structured text file, the zone file, but other database systems are common.

The Domain Name System originally used the User Datagram Protocol (UDP) as transport over IP. Reliability, security, and privacy concerns spawned the use of the Transmission Control Protocol (TCP) as well as numerous other protocol developments.

An often-used analogy to explain the DNS is that it serves as the phone book for the Internet by translating human-friendly computer hostnames into IP addresses. For example, the hostname codice_1 within the domain name example.com translates to the addresses (IPv4) and (IPv6). The DNS can be quickly and transparently updated, allowing a service's location on the network to change without affecting the end users, who continue to use the same hostname. Users take advantage of this when they use meaningful Uniform Resource Locators (URLs) and e-mail addresses without having to know how the computer actually locates the services.

An important and ubiquitous function of the DNS is its central role in distributed Internet services such as cloud services and content delivery networks. When a user accesses a distributed Internet service using a URL, the domain name of the URL is translated to the IP address of a server that is proximal to the user. The key functionality of the DNS exploited here is that different users can "simultaneously" receive different translations for the "same" domain name, a key point of divergence from a traditional phone-book view of the DNS. This process of using the DNS to assign proximal servers to users is key to providing faster and more reliable responses on the Internet and is widely used by most major Internet services.

The DNS reflects the structure of administrative responsibility on the Internet. Each subdomain is a zone of administrative autonomy delegated to a manager. For zones operated by a registry, administrative information is often complemented by the registry's RDAP and WHOIS services. That data can be used to gain insight on, and track responsibility for, a given host on the Internet.

Using a simpler, more memorable name in place of a host's numerical address dates back to the ARPANET era. The Stanford Research Institute (now SRI International) maintained a text file named HOSTS.TXT that mapped host names to the numerical addresses of computers on the ARPANET. Elizabeth Feinler developed and maintained the first ARPANET directory. Maintenance of numerical addresses, called the Assigned Numbers List, was handled by Jon Postel at the University of Southern California's Information Sciences Institute (ISI), whose team worked closely with SRI.

Addresses were assigned manually. Computers, including their hostnames and addresses, were added to the primary file by contacting the SRI Network Information Center (NIC), directed by Feinler, via telephone during business hours. Later, Feinler set up a WHOIS directory on a server in the NIC for retrieval of information about resources, contacts, and entities. She and her team developed the concept of domains. Feinler suggested that domains should be based on the location of the physical address of the computer. Computers at educational institutions would have the domain "edu", for example. She and her team managed the Host Naming Registry from 1972 to 1989.

By the early 1980s, maintaining a single, centralized host table had become slow and unwieldy and the emerging network required an automated naming system to address technical and personnel issues. Postel directed the task of forging a compromise between five competing proposals of solutions to Paul Mockapetris. Mockapetris instead created the Domain Name System in 1983 while at the University of Southern California.

The Internet Engineering Task Force published the original specifications in RFC 882 and RFC 883 in November 1983. These were updated in RFC 973 in January 1986.

In 1984, four UC Berkeley students, Douglas Terry, Mark Painter, David Riggle, and Songnian Zhou, wrote the first Unix name server implementation for the Berkeley Internet Name Domain, commonly referred to as BIND. In 1985, Kevin Dunlap of DEC substantially revised the DNS implementation. Mike Karels, Phil Almquist, and Paul Vixie then took over BIND maintenance. Internet Systems Consortium was founded in 1994 by Rick Adams, Paul Vixie, and Carl Malamud, expressly to provide a home for BIND development and maintenance. BIND versions from 4.9.3 onward were developed and maintained by ISC, with support provided by ISC's sponsors. As co-architects/programmers, Bob Halley and Paul Vixie released the first production-ready version of BIND version 8 in May 1997. Since 2000, over 43 different core developers have worked on BIND.

In November 1987, RFC 1034 and RFC 1035 superseded the 1983 DNS specifications. Several additional Request for Comments have proposed extensions to the core DNS protocols.

The domain name space consists of a tree data structure. Each node or leaf in the tree has a "label" and zero or more "resource records" (RR), which hold information associated with the domain name. The domain name itself consists of the label, concatenated with the name of its parent node on the right, separated by a dot.

The tree sub-divides into "zones" beginning at the root zone. A DNS zone may consist of as many domains and sub domains as the zone manager chooses. DNS can also be partitioned according to "class" where the separate classes can be thought of as an array of parallel namespace trees.

Administrative responsibility for any zone may be divided by creating additional zones. Authority over the new zone is said to be "delegated" to a designated name server. The parent zone ceases to be authoritative for the new zone.

The definitive descriptions of the rules for forming domain names appear in RFC 1035, RFC 1123, RFC 2181, and RFC 5892. A domain name consists of one or more parts, technically called "labels", that are conventionally concatenated, and delimited by dots, such as example.com.

The right-most label conveys the top-level domain; for example, the domain name www.example.com belongs to the top-level domain "com".

The hierarchy of domains descends from right to left; each label to the left specifies a subdivision, or subdomain of the domain to the right. For example, the label "example" specifies a subdomain of the "com" domain, and "www" is a subdomain of example.com. This tree of subdivisions may have up to 127 levels.

A label may contain zero to 63 characters. The null label, of length zero, is reserved for the root zone. The full domain name may not exceed the length of 253 characters in its textual representation. In the internal binary representation of the DNS the maximum length requires 255 octets of storage, as it also stores the length of the name.

Although no technical limitation exists to prevent domain name labels using any character which is representable by an octet, hostnames use a preferred format and character set. The characters allowed in labels are a subset of the ASCII character set, consisting of characters "a" through "z", "A" through "Z", digits "0" through "9", and hyphen. This rule is known as the "LDH rule" (letters, digits, hyphen). Domain names are interpreted in case-independent manner. Labels may not start or end with a hyphen. An additional rule requires that top-level domain names should not be all-numeric.

The limited set of ASCII characters permitted in the DNS prevented the representation of names and words of many languages in their native alphabets or scripts. To make this possible, ICANN approved the Internationalizing Domain Names in Applications (IDNA) system, by which user applications, such as web browsers, map Unicode strings into the valid DNS character set using Punycode. In 2009 ICANN approved the installation of internationalized domain name country code top-level domains ("ccTLD"s). In addition, many registries of the existing top-level domain names ("TLD"s) have adopted the IDNA system, guided by RFC 5890, RFC 5891, RFC 5892, RFC 5893.

The Domain Name System is maintained by a distributed database system, which uses the client–server model. The nodes of this database are the name servers. Each domain has at least one authoritative DNS server that publishes information about that domain and the name servers of any domains subordinate to it. The top of the hierarchy is served by the root name servers, the servers to query when looking up ("resolving") a TLD.

An "authoritative" name server is a name server that only gives answers to DNS queries from data that have been configured by an original source, for example, the domain administrator or by dynamic DNS methods, in contrast to answers obtained via a query to another name server that only maintains a cache of data.

An authoritative name server can either be a "primary" server or a "secondary" server. Historically the terms "master/slave" and "primary/secondary" were sometimes used interchangeably but the current practice is to use the latter form. A primary server is a server that stores the original copies of all zone records. A secondary server uses a special automatic updating mechanism in the DNS protocol in communication with its primary to maintain an identical copy of the primary records.

Every DNS zone must be assigned a set of authoritative name servers. This set of servers is stored in the parent domain zone with name server (NS) records.

An authoritative server indicates its status of supplying definitive answers, deemed "authoritative", by setting a protocol flag, called the ""Authoritative Answer"" ("AA") bit in its responses. This flag is usually reproduced prominently in the output of DNS administration query tools, such as dig, to indicate "that the responding name server is an authority for the domain name in question."

When a name server is designated as the authoritative server for a domain name for which it does not have authoritative data, it presents a type of error called a "lame delegation" or "lame response".

Domain name resolvers determine the domain name servers responsible for the domain name in question by a sequence of queries starting with the right-most (top-level) domain label.

For proper operation of its domain name resolver, a network host is configured with an initial cache ("hints") of the known addresses of the root name servers. The hints are updated periodically by an administrator by retrieving a dataset from a reliable source.

Assuming the resolver has no cached records to accelerate the process, the resolution process starts with a query to one of the root servers. In typical operation, the root servers do not answer directly, but respond with a referral to more authoritative servers, e.g., a query for "www.wikipedia.org" is referred to the "org" servers. The resolver now queries the servers referred to, and iteratively repeats this process until it receives an authoritative answer. The diagram illustrates this process for the host that is named by the fully qualified domain name "www.wikipedia.org".

This mechanism would place a large traffic burden on the root servers, if every resolution on the Internet required starting at the root. In practice caching is used in DNS servers to off-load the root servers, and as a result, root name servers actually are involved in only a relatively small fraction of all requests.

In theory, authoritative name servers are sufficient for the operation of the Internet. However, with only authoritative name servers operating, every DNS query must start with recursive queries at the root zone of the Domain Name System and each user system would have to implement resolver software capable of recursive operation.

To improve efficiency, reduce DNS traffic across the Internet, and increase performance in end-user applications, the Domain Name System supports DNS cache servers which store DNS query results for a period of time determined in the configuration ("time-to-live") of the domain name record in question.
Typically, such caching DNS servers also implement the recursive algorithm necessary to resolve a given name starting with the DNS root through to the authoritative name servers of the queried domain. With this function implemented in the name server, user applications gain efficiency in design and operation.

The combination of DNS caching and recursive functions in a name server is not mandatory; the functions can be implemented independently in servers for special purposes.

Internet service providers typically provide recursive and caching name servers for their customers. In addition, many home networking routers implement DNS caches and recursion to improve efficiency in the local network.

The client side of the DNS is called a DNS resolver. A resolver is responsible for initiating and sequencing the queries that ultimately lead to a full resolution (translation) of the resource sought, e.g., translation of a domain name into an IP address. DNS resolvers are classified by a variety of query methods, such as "recursive", "non-recursive", and "iterative". A resolution process may use a combination of these methods.

In a "non-recursive query", a DNS resolver queries a DNS server that provides a record either for which the server is authoritative, or it provides a partial result without querying other servers. In case of a caching DNS resolver, the non-recursive query of its local DNS cache delivers a result and reduces the load on upstream DNS servers by caching DNS resource records for a period of time after an initial response from upstream DNS servers.

In a "recursive query", a DNS resolver queries a single DNS server, which may in turn query other DNS servers on behalf of the requester. For example, a simple stub resolver running on a home router typically makes a recursive query to the DNS server run by the user's ISP. A recursive query is one for which the DNS server answers the query completely by querying other name servers as needed. In typical operation, a client issues a recursive query to a caching recursive DNS server, which subsequently issues non-recursive queries to determine the answer and send a single answer back to the client. The resolver, or another DNS server acting recursively on behalf of the resolver, negotiates use of recursive service using bits in the query headers. DNS servers are not required to support recursive queries.

The "iterative query" procedure is a process in which a DNS resolver queries a chain of one or more DNS servers. Each server refers the client to the next server in the chain, until the current server can fully resolve the request. For example, a possible resolution of www.example.com would query a global root server, then a "com" server, and finally an "example.com" server.

Name servers in delegations are identified by name, rather than by IP address. This means that a resolving name server must issue another DNS request to find out the IP address of the server to which it has been referred. If the name given in the delegation is a subdomain of the domain for which the delegation is being provided, there is a circular dependency.

In this case, the name server providing the delegation must also provide one or more IP addresses for the authoritative name server mentioned in the delegation. This information is called "glue". The delegating name server provides this glue in the form of records in the "additional section" of the DNS response, and provides the delegation in the "authority section" of the response. A glue record is a combination of the name server and IP address.

For example, if the authoritative name server for example.org is ns1.example.org, a computer trying to resolve www.example.org first resolves ns1.example.org. As ns1 is contained in example.org, this requires resolving example.org first, which presents a circular dependency. To break the dependency, the name server for the top level domain org includes glue along with the delegation for example.org. The glue records are address records that provide IP addresses for ns1.example.org. The resolver uses one or more of these IP addresses to query one of the domain's authoritative servers, which allows it to complete the DNS query.

A standard practice in implementing name resolution in applications is to reduce the load on the Domain Name System servers by caching results locally, or in intermediate resolver hosts. Results obtained from a DNS request are always associated with the time to live (TTL), an expiration time after which the results must be discarded or refreshed. The TTL is set by the administrator of the authoritative DNS server. The period of validity may vary from a few seconds to days or even weeks.

As a result of this distributed caching architecture, changes to DNS records do not propagate throughout the network immediately, but require all caches to expire and to be refreshed after the TTL. RFC 1912 conveys basic rules for determining appropriate TTL values.

Some resolvers may override TTL values, as the protocol supports caching for up to sixty-eight years or no caching at all. Negative caching, i.e. the caching of the fact of non-existence of a record, is determined by name servers authoritative for a zone which must include the Start of Authority (SOA) record when reporting no data of the requested type exists. The value of the "minimum" field of the SOA record and the TTL of the SOA itself is used to establish the TTL for the negative answer.

A reverse DNS lookup is a query of the DNS for domain names when the IP address is known. Multiple domain names may be associated with an IP address. The DNS stores IP addresses in the form of domain names as specially formatted names in pointer (PTR) records within the infrastructure top-level domain arpa. For IPv4, the domain is in-addr.arpa. For IPv6, the reverse lookup domain is ip6.arpa. The IP address is represented as a name in reverse-ordered octet representation for IPv4, and reverse-ordered nibble representation for IPv6.

When performing a reverse lookup, the DNS client converts the address into these formats before querying the name for a PTR record following the delegation chain as for any DNS query. For example, assuming the IPv4 address 208.80.152.2 is assigned to Wikimedia, it is represented as a DNS name in reverse order: 2.152.80.208.in-addr.arpa. When the DNS resolver gets a pointer (PTR) request, it begins by querying the root servers, which point to the servers of American Registry for Internet Numbers (ARIN) for the 208.in-addr.arpa zone. ARIN's servers delegate 152.80.208.in-addr.arpa to Wikimedia to which the resolver sends another query for 2.152.80.208.in-addr.arpa, which results in an authoritative response.

Users generally do not communicate directly with a DNS resolver. Instead DNS resolution takes place transparently in applications such as web browsers, e-mail clients, and other Internet applications. When an application makes a request that requires a domain name lookup, such programs send a resolution request to the DNS resolver in the local operating system, which in turn handles the communications required.

The DNS resolver will almost invariably have a cache (see above) containing recent lookups. If the cache can provide the answer to the request, the resolver will return the value in the cache to the program that made the request. If the cache does not contain the answer, the resolver will send the request to one or more designated DNS servers. In the case of most home users, the Internet service provider to which the machine connects will usually supply this DNS server: such a user will either have configured that server's address manually or allowed DHCP to set it; however, where systems administrators have configured systems to use their own DNS servers, their DNS resolvers point to separately maintained name servers of the organization. In any event, the name server thus queried will follow the process outlined above, until it either successfully finds a result or does not. It then returns its results to the DNS resolver; assuming it has found a result, the resolver duly caches that result for future use, and hands the result back to the software which initiated the request.

Some large ISPs have configured their DNS servers to violate rules, such as by disobeying TTLs, or by indicating that a domain name does not exist just because one of its name servers does not respond.

Some applications such as web browsers maintain an internal DNS cache to avoid repeated lookups via the network. This practice can add extra difficulty when debugging DNS issues as it obscures the history of such data. These caches typically use very short caching times on the order of one minute.

Internet Explorer represents a notable exception: versions up to IE 3.x cache DNS records for 24 hours by default. Internet Explorer 4.x and later versions (up to IE 8) decrease the default timeout value to half an hour, which may be changed by modifying the default configuration.

When Google Chrome detects issues with the DNS server it displays a specific error message.

The Domain Name System includes several other functions and features.

Hostnames and IP addresses are not required to match in a one-to-one relationship. Multiple hostnames may correspond to a single IP address, which is useful in virtual hosting, in which many web sites are served from a single host. Alternatively, a single hostname may resolve to many IP addresses to facilitate fault tolerance and load distribution to multiple server instances across an enterprise or the global Internet.

DNS serves other purposes in addition to translating names to IP addresses. For instance, mail transfer agents use DNS to find the best mail server to deliver e-mail: An MX record provides a mapping between a domain and a mail exchanger; this can provide an additional layer of fault tolerance and load distribution.

The DNS is used for efficient storage and distribution of IP addresses of blacklisted email hosts. A common method is to place the IP address of the subject host into the sub-domain of a higher level domain name, and to resolve that name to a record that indicates a positive or a negative indication.

For example:
E-mail servers can query blacklist.example to find out if a specific host connecting to them is in the blacklist. Many of such blacklists, either subscription-based or free of cost, are available for use by email administrators and anti-spam software.

To provide resilience in the event of computer or network failure, multiple DNS servers are usually provided for coverage of each domain. At the top level of global DNS, thirteen groups of root name servers exist, with additional "copies" of them distributed worldwide via anycast addressing.

Dynamic DNS (DDNS) updates a DNS server with a client IP address on-the-fly, for example, when moving between ISPs or mobile hot spots, or when the IP address changes administratively.

The DNS protocol uses two types of DNS messages, queries and responses; both have the same format. Each message consists of a header and four sections: question, answer, authority, and an additional space. A header field ("flags") controls the content of these four sections.

The header section consists of the following fields: "Identification", "Flags", "Number of questions", "Number of answers", "Number of authority resource records" (RRs), and "Number of additional RRs". Each field is 16 bits long, and appears in the order given. The identification field is used to match responses with queries. The flag field consists of sub-fields as follows:

After the flag, the header ends with four 16-bit integers which contain the number of records in each of the sections that follow, in the same order.

The question section has a simpler format than the resource record format used in the other sections. Each question record (there is usually just one in the section) contains the following fields:

The domain name is broken into discrete labels which are concatenated; each label is prefixed by the length of that label.

The Domain Name System specifies a database of information elements for network resources. The types of information elements are categorized and organized with a list of DNS record types, the resource records (RRs). Each record has a type (name and number), an expiration time (time to live), a class, and type-specific data. Resource records of the same type are described as a "resource record set" (RRset), having no special ordering. DNS resolvers return the entire set upon query, but servers may implement round-robin ordering to achieve load balancing. In contrast, the Domain Name System Security Extensions (DNSSEC) work on the complete set of resource record in canonical order.

When sent over an Internet Protocol network, all records use the common format specified in RFC 1035:

"NAME" is the fully qualified domain name of the node in the tree . On the wire, the name may be shortened using label compression where ends of domain names mentioned earlier in the packet can be substituted for the end of the current domain name.

"TYPE" is the record type. It indicates the format of the data and it gives a hint of its intended use. For example, the "A" record is used to translate from a domain name to an IPv4 address, the "NS" record lists which name servers can answer lookups on a DNS zone, and the "MX" record specifies the mail server used to handle mail for a domain specified in an e-mail address.

"RDATA" is data of type-specific relevance, such as the IP address for address records, or the priority and hostname for MX records. Well known record types may use label compression in the RDATA field, but "unknown" record types must not (RFC 3597).

The "CLASS" of a record is set to IN (for "Internet") for common DNS records involving Internet hostnames, servers, or IP addresses. In addition, the classes Chaos (CH) and Hesiod (HS) exist. Each class is an independent name space with potentially different delegations of DNS zones.

In addition to resource records defined in a zone file, the domain name system also defines several request types that are used only in communication with other DNS nodes ("on the wire"), such as when performing zone transfers (AXFR/IXFR) or for EDNS (OPT).

The domain name system supports wildcard DNS records which specify names that start with the "asterisk label", '*', e.g., *.example. DNS records belonging to wildcard domain names specify rules for generating resource records within a single DNS zone by substituting whole labels with matching components of the query name, including any specified descendants. For example, in the following configuration, the DNS zone "x.example" specifies that all subdomains, including subdomains of subdomains, of "x.example" use the mail exchanger (MX) "a.x.example". The A record for "a.x.example" is needed to specify the mail exchanger IP address. As this has the result of excluding this domain name and its subdomains from the wildcard matches, an additional MX record for the subdomain "a.x.example", as well as a wildcarded MX record for all of its subdomains, must also be defined in the DNS zone.
x.example. MX 10 a.x.example.
a.x.example. MX 10 a.x.example.
a.x.example. AAAA 2001:db8::1
The role of wildcard records was refined in , because the original definition in was incomplete and resulted in misinterpretations by implementers.

The original DNS protocol had limited provisions for extension with new features. In 1999, Paul Vixie published in RFC 2671 (superseded by RFC 6891) an extension mechanism, called Extension Mechanisms for DNS (EDNS) that introduced optional protocol elements without increasing overhead when not in use. This was accomplished through the OPT pseudo-resource record that only exists in wire transmissions of the protocol, but not in any zone files. Initial extensions were also suggested (EDNS0), such as increasing the DNS message size in UDP datagrams.

Dynamic DNS updates use the UPDATE DNS opcode to add or remove resource records dynamically from a zone database maintained on an authoritative DNS server. This facility is useful to register network clients into the DNS when they boot or become otherwise available on the network. As a booting client may be assigned a different IP address each time from a DHCP server, it is not possible to provide static DNS assignments for such clients.

From the time of its origin in 1983 the DNS has used the User Datagram Protocol (UDP) for transport over IP. Its limitations have motivated numerous protocol developments for reliability, security, privacy, and other criteria, in the following decades.

UDP reserves port number 53 for servers listening to queries. Such queries consist of a clear-text request sent in a single UDP packet from the client, responded to with a clear-text reply sent in a single UDP packet from the server. When the length of the answer exceeds 512 bytes and both client and server support Extension Mechanisms for DNS (EDNS), larger UDP packets may be used. Use of DNS over UDP is limited by, among other things, its lack of transport-layer encryption, authentication, reliable delivery, and message length. In 1989, RFC 1123 specified optional Transmission Control Protocol (TCP) transport for DNS queries, replies and, particularly, zone transfers. Via fragmentation of long replies, TCP allows longer responses, reliable delivery, and re-use of long-lived connections between clients and servers. For larger responses, the server refers the client to TCP transport.

DNS over TLS emerged as an IETF standard for encrypted DNS in 2016, utilizing Transport Layer Security (TLS) to protect the entire connection, rather than just the DNS payload. DoT servers listen on TCP port 853. specifies that opportunistic encryption and authenticated encryption may be supported, but did not make either server or client authentication mandatory.

DNS over HTTPS was developed as a competing standard for DNS query transport in 2018, tunneling DNS query data over HTTPS, which transports HTTP over TLS. DoH was promoted as a more web-friendly alternative to DNS since, like DNSCrypt, it uses TCP port 443, and thus looks similar to web traffic, though they are easily differentiable in practice without proper padding.

RFC 9250, published in 2022 by the Internet Engineering Task Force, describes DNS over QUIC. It has "privacy properties similar to DNS over TLS (DoT) [...], and latency characteristics similar to classic DNS over UDP". This method is not the same as DNS over HTTP/3.

Oblivious DNS (ODNS) was invented and implemented by researchers at Princeton University and the University of Chicago as an extension to unencrypted DNS, before DoH was standardized and widely deployed. Apple and Cloudflare subsequently deployed the technology in the context of DoH, as Oblivious DoH (ODoH). ODoH combines ingress/egress separation (invented in ODNS) with DoH's HTTPS tunneling and TLS transport-layer encryption in a single protocol.

DNS may be run over virtual private networks (VPNs) and tunneling protocols. A use which has become common since 2019 to warrant its own frequently used acronym is DNS over Tor. The privacy gains of Oblivious DNS can be garnered through the use of the preexisting Tor network of ingress and egress nodes, paired with the transport-layer encryption provided by TLS.

The DNSCrypt protocol, which was developed in 2011 outside the IETF standards framework, introduced DNS encryption on the downstream side of recursive resolvers, wherein clients encrypt query payloads using servers' public keys, which are published in the DNS (rather than relying upon third-party certificate authorities) and which may in turn be protected by DNSSEC signatures. DNSCrypt uses either TCP or UDP port 443, the same port as HTTPS encrypted web traffic. This introduced not only privacy regarding the content of the query, but also a significant measure of firewall-traversal capability. In 2019, DNSCrypt was further extended to support an "anonymized" mode, similar to the proposed "Oblivious DNS", in which an ingress node receives a query which has been encrypted with the public key of a different server, and relays it to that server, which acts as an egress node, performing the recursive resolution. Privacy of user/query pairs is created, since the ingress node does not know the content of the query, while the egress nodes does not know the identity of the client. DNSCrypt was first implemented in production by OpenDNS in December 2011. There are several free and open source software implementations that additionally integrate ODoH. It is available for a variety of operating systems, including Unix, Apple iOS, Linux, Android, and MS Windows.

Originally, security concerns were not major design considerations for DNS software or any software for deployment on the early Internet, as the network was not open for participation by the general public. However, the expansion of the Internet into the commercial sector in the 1990s changed the requirements for security measures to protect data integrity and user authentication.

Several vulnerability issues were discovered and exploited by malicious users. One such issue is DNS cache poisoning, in which data is distributed to caching resolvers under the pretense of being an authoritative origin server, thereby polluting the data store with potentially false information and long expiration times (time-to-live). Subsequently, legitimate application requests may be redirected to network hosts operated with malicious intent.

DNS responses traditionally do not have a cryptographic signature, leading to many attack possibilities; the Domain Name System Security Extensions (DNSSEC) modify DNS to add support for cryptographically signed responses. DNSCurve has been proposed as an alternative to DNSSEC. Other extensions, such as TSIG, add support for cryptographic authentication between trusted peers and are commonly used to authorize zone transfer or dynamic update operations.

Some domain names may be used to achieve spoofing effects. For example, and are different names, yet users may be unable to distinguish them in a graphical user interface depending on the user's chosen typeface. In many fonts the letter "l" and the numeral "1" look very similar or even identical. This problem, known as the IDN homograph attack, is acute in systems that support internationalized domain names, as many character codes in ISO 10646 may appear identical on typical computer screens. This vulnerability is occasionally exploited in phishing.

Techniques such as forward-confirmed reverse DNS can also be used to help validate DNS results.

DNS can also "leak" from otherwise secure or private connections, if attention is not paid to their configuration, and at times DNS has been used to bypass firewalls by malicious persons, and exfiltrate data, since it is often seen as innocuous.

Originally designed as a public, hierarchical, distributed and heavily cached database, DNS protocol has no confidentiality controls. User queries and nameserver responses are being sent unencrypted which enables network packet sniffing, DNS hijacking, DNS cache poisoning and man-in-the-middle attacks. This deficiency is commonly used by cybercriminals and network operators for marketing purposes, user authentication on captive portals and censorship.

User privacy is further exposed by proposals for increasing the level of client IP information in DNS queries (RFC 7871) for the benefit of Content Delivery Networks.

The main approaches that are in use to counter privacy issues with DNS:

Solutions preventing DNS inspection by local network operator are criticized for thwarting corporate network security policies and Internet censorship. They are also criticized from a privacy point of view, as giving away the DNS resolution to the hands of a small number of companies known for monetizing user traffic and for centralizing DNS name resolution, which is generally perceived as harmful for the Internet.

The right to use a domain name is delegated by domain name registrars which are accredited by the Internet Corporation for Assigned Names and Numbers (ICANN) or other organizations such as OpenNIC, that are charged with overseeing the name and number systems of the Internet. In addition to ICANN, each top-level domain (TLD) is maintained and serviced technically by an administrative organization, operating a registry. A "registry" is responsible for operating the database of names within its authoritative zone, although the term is most often used for TLDs. A "registrant" is a person or organization who asked for domain registration. The registry receives registration information from each domain name "registrar", which is authorized (accredited) to assign names in the corresponding zone and publishes the information using the WHOIS protocol. As of 2015, usage of RDAP is being considered.

ICANN publishes the complete list of TLDs, TLD registries, and domain name registrars. Registrant information associated with domain names is maintained in an online database accessible with the WHOIS service. For most of the more than 290 country code top-level domains (ccTLDs), the domain registries maintain the WHOIS (Registrant, name servers, expiration dates, etc.) information. For instance, DENIC, Germany NIC, holds the DE domain data. From about 2001, most Generic top-level domain (gTLD) registries have adopted this so-called "thick" registry approach, i.e. keeping the WHOIS data in central registries instead of registrar databases.

For top-level domains on COM and NET, a "thin" registry model is used. The domain registry (e.g., GoDaddy, BigRock and PDR, VeriSign, etc., etc.) holds basic WHOIS data (i.e., registrar and name servers, etc.). Organizations, or registrants using ORG on the other hand, are on the Public Interest Registry exclusively.

Some domain name registries, often called "network information centers" (NIC), also function as registrars to end-users, in addition to providing access to the WHOIS datasets. The top-level domain registries, such as for the domains COM, NET, and ORG use a registry-registrar model consisting of many domain name registrars. In this method of management, the registry only manages the domain name database and the relationship with the registrars. The "registrants" (users of a domain name) are customers of the registrar, in some cases through additional subcontracting of resellers.

The Domain Name System is defined by Request for Comments (RFC) documents published by the Internet Engineering Task Force (Internet standards). The following is a list of RFCs that define the DNS protocol.





These RFCs are advisory in nature, but may provide useful information despite defining neither a standard or BCP. (RFC 1796)

These RFCs have an official status of Unknown, but due to their age are not clearly labeled as such.



David Letterman

David Michael Letterman (born April 12, 1947) is an American television host, comedian, writer and producer. He hosted late-night television talk shows for 33 years, beginning with the February 1, 1982, debut of "Late Night with David Letterman" on NBC and ending with the May 20, 2015, broadcast of "Late Show with David Letterman" on CBS. In total, Letterman hosted 6,080 episodes of "Late Night" and "Late Show", surpassing his friend and mentor Johnny Carson as the longest-serving late-night talk show host in American television history.

He is also a television and film producer. His company, Worldwide Pants, produced his shows as well as "The Late Late Show" and several primetime comedies, the most successful of which was the CBS sitcom "Everybody Loves Raymond". Several late-night hosts have cited Letterman's influence, including Conan O'Brien, Jimmy Fallon, Seth Meyers (each of whom succeeded Letterman on "Late Night"), Stephen Colbert (his successor on "The Late Show"), Jimmy Kimmel, and Jon Stewart. Since 2018, he has hosted the Netflix series "My Next Guest Needs No Introduction with David Letterman".

Letterman was born in Indianapolis, Indiana, in 1947, and has two sisters, one older and one younger. His father, Harry Joseph Letterman (April 15, 1915 – February 13, 1973), was a florist. His mother, Dorothy Marie Letterman Mengering (née Hofert; July 18, 1921 – April 11, 2017), a church secretary for the Second Presbyterian Church of Indianapolis, was an occasional figure on Letterman's show, usually at holidays and birthdays.

Letterman grew up on the north side of Indianapolis, in the Broad Ripple area, about from the Indianapolis Motor Speedway. He enjoyed collecting model cars, including racers. In 2000, he told an interviewer for "Esquire" that, while growing up, he admired his father's ability to tell jokes and be the life of the party. Harry Joseph Letterman survived a heart attack at the age of 36 when David was a young boy. The fear of losing his father was constantly with Letterman as he grew up. The elder Letterman died of a second heart attack in 1973 at the age of 57.

Letterman attended his hometown's Broad Ripple High School and worked as a stock boy at the local Atlas Supermarket. According to the "Ball State Daily News", he originally wanted to attend Indiana University, but his grades were not good enough, so he instead attended Ball State University in Muncie, Indiana. He is a member of the Sigma Chi fraternity, and graduated in 1969 from what was then the Department of Radio and Television. A self-described average student, Letterman later endowed a scholarship for what he called "C students" at Ball State. Though he registered for the draft and passed his physical after graduating from college, he was not drafted for service in Vietnam because he received a draft lottery number of 346 (out of 366).

Letterman began his broadcasting career as an announcer and newscaster at the college's student-run radio station—WBST—a 10-watt campus station that is now part of Indiana Public Radio. He was fired for treating classical music with irreverence. He then became involved with the founding of another campus station—WAGO-AM 570 (now WCRD, 91.3).

He credits Paul Dixon, host of the "Paul Dixon Show", a Cincinnati-based talk show also shown in Indianapolis while he was growing up, for inspiring his choice of career:
I was just out of college [in 1969], and I really didn't know what I wanted to do. And then all of a sudden I saw him doing it [on TV]. And I thought: That's really what I want to do!

Soon after graduating from Ball State in 1969, Letterman began his career as a radio talk show host on WNTS (AM) and on Indianapolis television station WLWI (which changed its call sign to WTHR in 1976) as an anchor and weatherman. He received some attention for his unpredictable on-air behavior, which included congratulating a tropical storm for being upgraded to a hurricane and predicting hailstones "the size of canned hams." He also occasionally reported the weather and the day's very high and low temps for fictitious cities ("Eight inches of snow in Bingree and surrounding areas"), on another occasion saying that the state border between Indiana and Ohio had been erased when a satellite map accidentally omitted it, attributing it to dirty political dealings. ("The higher-ups have removed the border between Indiana and Ohio, making it one giant state. Personally, I'm against it. I don't know what to do about it.") He also starred in a local kiddie show, made wisecracks as host of a late-night TV show called "Freeze-Dried Movies" (he once acted out a scene from "Godzilla" using plastic dinosaurs), and hosted a talk show that aired early on Saturday mornings called "Clover Power", in which he interviewed 4-H members about their projects.

In 1971, Letterman appeared as a pit road reporter for ABC Sports' tape-delayed coverage of the Indianapolis 500, which was his first nationally telecast appearance (WLWI was the local ABC affiliate at the time). He was initially introduced as Chris Economaki, but this was corrected at the end of the interview (Jim McKay announced his name as Dave Letterman). Letterman interviewed Mario Andretti, who had just crashed out of the race.

In 1975, encouraged by his then-wife Michelle and several of his Sigma Chi fraternity brothers, Letterman moved to Los Angeles, California, with the hope of becoming a comedy writer. He and Michelle packed their belongings in his pickup truck and headed west. As of 2012, he still owned the truck. In Los Angeles, he began performing comedy at The Comedy Store. Jimmie Walker saw him on stage; with an endorsement from George Miller, Letterman joined a group of comedians whom Walker hired to write jokes for his stand-up act, a group that at various times also included Jay Leno, Paul Mooney, Robert Schimmel, Richard Jeni, Louie Anderson, Elayne Boosler, Byron Allen, Jack Handey, and Steve Oedekerk.

By the summer of 1977, Letterman was a writer and regular on the six-week summer series "The Starland Vocal Band Show", broadcast on CBS. He hosted a 1977 pilot for a game show called "The Riddlers" (which was never picked up), and co-starred in the Barry Levinson-produced comedy special "Peeping Times", which aired in January 1978. Later that year, Letterman was a cast member on Mary Tyler Moore's variety show, "Mary". He made a guest appearance on "Mork & Mindy" (as a parody of EST leader Werner Erhard) and appearances on game shows such as "The $20,000 Pyramid", "The Gong Show", "Hollywood Squares", "Password Plus", and "Liar's Club", as well as the Canadian cooking show "Celebrity Cooks" (November 1977), talk shows such as "90 Minutes Live" (February 24 and April 14, 1978), and "The Mike Douglas Show" (April 3, 1979 and February 7, 1980). He was also screen tested for the lead role in the 1980 film "Airplane!", a role that eventually went to Robert Hays.

Letterman's brand of dry, sarcastic humor caught the attention of scouts for "The Tonight Show Starring Johnny Carson", and he was soon a regular guest on the show. He became a favorite of Carson and was a regular guest host for the show beginning in 1978. Letterman credits Carson as the person who influenced his career the most.

On June 23, 1980, Letterman was given his own morning comedy show on NBC, "The David Letterman Show". It was originally 90 minutes long but was shortened to 60 minutes in August 1980. The show was a critical success, winning two Emmy Awards, but was a ratings disappointment and was canceled, the last show airing October 24, 1980.

NBC kept Letterman on its payroll to try him in a different time slot. "Late Night with David Letterman" debuted February 1, 1982; the first guest was Bill Murray. Murray went on to become one of Letterman's most recurrent guests, guesting on his later CBS show's celebration of his 30th anniversary in late-night television, which aired January 31, 2012, and on the final CBS show, which aired May 20, 2015. The show ran Monday through Thursday nights at 12:30 a.m. Eastern Time, immediately following "The Tonight Show Starring Johnny Carson" (a Friday night broadcast was added in June 1987). It was seen as edgy and unpredictable, and soon developed a cult following (particularly among college students). Letterman's reputation as an acerbic interviewer was borne out in verbal sparring matches with Cher (who even called him an "asshole" on the show), Shirley MacLaine, Charles Grodin, and Madonna. The show also featured comedy segments and running characters, in a style heavily influenced by the 1950s and 1960s programs of Steve Allen.

The show often featured quirky, genre-mocking regular features, including "Stupid Pet Tricks" (which had its origins on Letterman's morning show), Stupid Human Tricks, dropping various objects off the roof of a five-story building, demonstrations of unorthodox clothing (such as suits made of Alka-Seltzer, Velcro and suet), a recurring Top 10 list, the Monkey-Cam (and the Audience Cam), a facetious letter-answering segment, several "Film[s] by My Dog Bob" in which a camera was mounted on Letterman's own dog (often with comic results) and Small Town News, all of which moved with Letterman to CBS.

Other episodes included Letterman using a bullhorn to interrupt a live interview on "The Today Show" on August 19, 1985, announcing that he was the NBC News president Lawrence K. Grossman and that he was not wearing any pants; walking across the hall to Studio 6B, at the time the news studio for WNBC-TV, and interrupting Al Roker's weather segments during "Live at Five"; and staging "elevator races", complete with commentary by NBC Sports' Bob Costas. In one appearance, in 1982, Andy Kaufman (who was wearing a neck brace) appeared with professional wrestler Jerry Lawler, who slapped and knocked the comedian to the ground (Lawler and Kaufman's friend Bob Zmuda later revealed that the incident was staged).

In 1992, Johnny Carson retired, and many fans, and Carson himself, believed that Letterman would become the new host of "The Tonight Show". When NBC instead gave the job to Jay Leno, Letterman departed NBC to host his own late-night show on CBS, opposite "The Tonight Show" at 11:30 p.m., called the "Late Show with David Letterman". The new show debuted on August 30, 1993, and was taped at the historic Ed Sullivan Theater, where Ed Sullivan broadcast his eponymous variety series from 1948 to 1971. For Letterman's arrival, CBS spent $8 million in renovations. CBS also signed Letterman to a three-year, $14 million/year contract, doubling his "Late Night" salary.

But while the expectation was that Letterman would retain his unique style and sense of humor with the move, "Late Show" was not an exact replica of his old NBC program. The monologue was lengthened. Paul Shaffer and the World's Most Dangerous Band followed Letterman to CBS, but they added a brass section and were rebranded the CBS Orchestra (at Shaffer's request); a small band had been mandated by Carson while Letterman occupied the 12:30 slot. Additionally, because of intellectual property disagreements, Letterman was unable to import many of his "Late Night" segments verbatim, but he sidestepped this problem by simply renaming them (the "Top Ten List" became the "Late Show Top Ten", "Viewer Mail" became the "CBS Mailbag", etc.). "Time" magazine wrote, "Letterman's innovation ... gained power from its rigorous formalism"; as his biographer Jason Zinoman puts it, he was "a fascinatingly disgruntled eccentric trapped inside a more traditional talk show."

The "Late Show"'s main competitor was NBC's "The Tonight Show", which Jay Leno hosted for 22 years from 1992 to 2014, except from June 1, 2009, to January 22, 2010, when Conan O'Brien hosted. In 1993 and 1994, the "Late Show" consistently gained higher ratings than "The Tonight Show". But in 1995, ratings dipped and Leno's show consistently beat Letterman's in the ratings from the time that Hugh Grant came on Leno's show after Grant's arrest for soliciting a prostitute.

Leno typically attracted about five million nightly viewers between 1999 and 2009. The "Late Show" lost nearly half its audience during its competition with Leno, attracting 7.1 million viewers nightly in its 1993–94 season and about 3.8 million per night as of Leno's departure in 2009. In the final months of his first stint as host of "The Tonight Show", Leno beat Letterman in the ratings by a 1.3 million-viewer margin (5.2 million to 3.9 million), and "Nightline" and the "Late Show" were virtually tied. Once O'Brien took over "Tonight", Letterman closed the gap in the ratings. O'Brien initially drove the median age of "Tonight Show" viewers from 55 to 45, with most older viewers opting to watch the "Late Show" instead. After Leno returned to "The Tonight Show", Leno regained his lead.

Letterman's shows have garnered both critical and industry praise, receiving 67 Emmy Award nominations, winning 12 times in his first 20 years in late night television. From 1993 to 2009, Letterman ranked higher than Leno in the annual Harris Poll of "Nation's Favorite TV Personality" 12 times. For example, in 2003 and 2004 Letterman ranked second in that poll, behind only Oprah Winfrey, a year that Leno was ranked fifth. Leno was higher than Letterman on that poll three times during the same period, in 1998, 2007, and 2008.

On March 27, 1995, Letterman hosted the 67th Academy Awards ceremony. Critics blasted what they deemed his poor performance, noting that his irreverent style undermined the traditional importance and glamor of the event. In a joke about their unusual names (inspired by a celebrated comic essay in "The New Yorker", "Yma Dream" by Thomas Meehan), he started off by introducing Uma Thurman to Oprah Winfrey, and then both of them to Keanu Reeves: "Oprah...Uma. Uma...Oprah," "Have you kids met Keanu?" This and many of his other jokes fell flat. Although Letterman attracted the highest ratings to the annual telecast since 1983, many felt that the bad publicity he generated caused a decline in the "Late Show" ratings.

Letterman recycled the apparent debacle into a long-running gag. On his first show after the Oscars, he joked, "Looking back, I had no idea that thing was being televised." He lampooned his stint two years later, during Billy Crystal's opening Oscar skit, which also parodied the plane-crashing scenes from that year's chief nominated film, "The English Patient".

For years afterward, Letterman recounted his hosting the Oscars, although the Academy of Motion Picture Arts and Sciences continued to hold Letterman in high regard and invited him to host the Oscars again. On September 7, 2010, he made an appearance on the premiere of the 14th season of "The View", and confirmed that he had been considered for hosting again.

On January 14, 2000, a routine check-up revealed that an artery in Letterman's heart was severely obstructed. He was rushed to emergency surgery for a quintuple bypass at New York Presbyterian Hospital. During the initial weeks of his recovery, reruns of the "Late Show" were shown and introduced by friends of Letterman including Norm Macdonald, Drew Barrymore, Ray Romano, Robin Williams, Bonnie Hunt, Megan Mullally, Bill Murray, Regis Philbin, Charles Grodin, Nathan Lane, Julia Roberts, Bruce Willis, Jerry Seinfeld, Martin Short, Steven Seagal, Hillary Clinton, Danny DeVito, Steve Martin, and Sarah Jessica Parker.

Subsequently, while still recovering from surgery, Letterman revived the late-night talk show tradition of "guest hosts" that had virtually disappeared on network television during the 1990s, allowing Bill Cosby, Kathie Lee Gifford, Dana Carvey, Janeane Garofalo, and others to host new episodes of the "Late Show". Upon his return to the show on February 21, 2000, Letterman brought all but one of the doctors and nurses on stage who had participated in his surgery and recovery (with extra teasing of a nurse who had given him bed baths—"This woman gave me a bath!"), including Dr. O. Wayne Isom and physician Louis Aronne, who frequently appeared on the show.

For a number of episodes, Letterman continued to crack jokes about his bypass, including saying, "Bypass surgery: it's when doctors surgically create new blood flow to your heart. A bypass is what happened to me when I didn't get "The Tonight Show!" It's a whole different thing." In a later running gag, he lobbied Indiana to rename the freeway circling Indianapolis (I-465) "The David Letterman Bypass". He also featured a montage of faux news coverage of his bypass surgery, which included a clip of Letterman's heart for sale on the Home Shopping Network. Letterman became friends with his doctors and nurses. In 2008, a "Rolling Stone" interview stated he hosted a doctor and nurse who'd helped perform the emergency quintuple-bypass heart surgery that saved his life in 2000. 'These are people who were complete strangers when they opened my chest,' he says. 'And now, eight years later, they're among my best friends.' Additionally, Letterman invited the band Foo Fighters to play "Everlong", introducing them as "my favorite band, playing my favorite song." During Letterman's last show, on which Foo Fighters appeared, Letterman said that Foo Fighters had been in the middle of a South American tour which they canceled to come play on his comeback episode.

Letterman again handed over the reins of the show to several guest hosts (including Bill Cosby, Brad Garrett, Whoopi Goldberg, Elvis Costello, John McEnroe, Vince Vaughn, Will Ferrell, Bonnie Hunt, Luke Wilson, and bandleader Paul Shaffer) in February 2003, when he was diagnosed with a severe case of shingles. Later that year, Letterman made regular use of guest hosts—including Tom Arnold and Kelsey Grammer—for new shows broadcast on Fridays. In March 2007, Adam Sandler, who had been scheduled to be the lead guest, served as a guest host while Letterman was ill with a stomach virus.

In March 2002, as Letterman's contract with CBS neared expiration, ABC offered him the time slot for long-running news program "Nightline" with Ted Koppel. Letterman was interested, as he believed he could never match Leno's ratings at CBS due to Letterman's complaint of weaker lead-ins from the network's late local news programs, but was reluctant to replace Koppel. He addressed his decision to re-sign on the air, stating that he was content at CBS and that he had great respect for Koppel.

On December 4, 2006, CBS revealed that Letterman signed a new contract to host "Late Show with David Letterman" through the fall of 2010. "I'm thrilled to be continuing on at CBS," said Letterman. "At my age you really don't want to have to learn a new commute." Letterman further joked about the subject by pulling up his right pants leg, revealing a tattoo, presumably temporary, of the ABC logo.

"Thirteen years ago, David Letterman put CBS late night on the map and in the process became one of the defining icons of our network," said Leslie Moonves, president and CEO of CBS Corporation. "His presence on our air is an ongoing source of pride, and the creativity and imagination that the "Late Show" puts forth every night is an ongoing display of the highest quality entertainment. We are truly honored that one of the most revered and talented entertainers of our time will continue to call CBS 'home.'"

According to a 2007 article in "Forbes" magazine, Letterman earned $40 million a year. A 2009 article in "The New York Times", however, said his salary was estimated at $32 million. In June 2009, Letterman's Worldwide Pants and CBS reached an agreement to continue the "Late Show" until at least August 2012. The previous contract had been set to expire in 2010, and the two-year extension was shorter than the typical three-year contract period negotiated in the past. Worldwide Pants agreed to lower its fee for the show, though it had remained a "solid moneymaker for CBS" under the previous contract.

On the February 3, 2011 edition of the "Late Show", during an interview with Howard Stern, Letterman said he would continue to do his talk show for "maybe two years, I think." In April 2012, CBS announced it had extended its contract with Letterman through 2014. His contract was subsequently extended to 2015.

During the taping of his show on April 3, 2014, Letterman announced that he had informed CBS president Leslie Moonves that he would retire from hosting "Late Show" by May 20, 2015. Later in his retirement Letterman occasionally stated, in jest, that he had been fired. It was announced soon after that comedian and political satirist Stephen Colbert would succeed Letterman. Letterman's last episode aired on May 20, 2015, and opened with a presidential sendoff featuring four of the five living American presidents, George H. W. Bush, Bill Clinton, George W. Bush, and Barack Obama, each mimicking the late president Gerald Ford's statement "Our long national nightmare is over." It also featured cameos from "The Simpsons" and "Wheel of Fortune" (the latter with a puzzle saying "Good riddance to David Letterman"), a Top Ten List of "things I wish I could have said to David Letterman" performed by regular guests including Alec Baldwin, Barbara Walters, Steve Martin, Jerry Seinfeld, Jim Carrey, Chris Rock, Julia Louis-Dreyfus, Peyton Manning, Tina Fey, and Bill Murray, and closed with a montage of scenes from both his CBS and NBC series set to a live performance of "Everlong" by Foo Fighters.

The final episode of "Late Show with David Letterman" was watched by 13.76 million viewers in the United States with an audience share of 9.3/24, earning the show its highest ratings since following the 1994 Winter Olympics on February 25, 1994, and the show's highest demo numbers (4.1 in adults 25–54 and 3.1 in adults 18–49) since Oprah Winfrey's first "Late Show" appearance following the ending of her feud with Letterman on December 1, 2005. Bill Murray, who had been his first guest on "Late Night", was his final guest on "Late Show". In a rarity for a late-night show, it was also the highest-rated program on network television that night, beating out all prime-time shows. In total, Letterman hosted 6,080 episodes of "Late Night" and "Late Show", surpassing friend and mentor Johnny Carson as the longest-serving late-night talk show host in U.S. television history.

In the months following the end of "Late Show", Letterman was seen occasionally at sports events such as the Indianapolis 500, during which he submitted to an interview with a local publication. He made a surprise appearance on stage in San Antonio, Texas when he was invited up for an extended segment during Steve Martin's and Martin Short's "A Very Stupid Conversation" show, saying "I retired, and...I have no regrets," Letterman told the crowd after walking on stage. "I was happy. I'll make actual friends. I was complacent. I was satisfied. I was content, and then a couple of days ago Donald Trump said he was running for president. I have made the biggest mistake of my life, ladies and gentlemen" and then delivering a Top Ten List roasting Trump's presidential campaign followed by an onstage conversation with Martin and Short. Cellphone recordings of the appearance were posted on YouTube by audience members and widely reported in the media.

In 2016, Letterman joined the climate change documentary show "Years of Living Dangerously" as one of its celebrity correspondents. In season two's premiere episode, Letterman traveled to India to investigate the country's efforts to expand its inadequate energy grid, power its booming economy, and bring electricity to 300 million citizens for the first time. He also interviewed Indian Prime Minister Narendra Modi and traveled to rural villages where power is a scarce luxury and explored the United States' role in India's energy future.

On April 7, 2017, Letterman gave the induction speech for the band Pearl Jam into the Rock & Roll Hall Of Fame at a ceremony held at the Barclays Center in Brooklyn, New York City. Also in 2017, Letterman and Alec Baldwin co-hosted "The Essentials" on Turner Classic Movies. Letterman and Baldwin introduced seven films for the series.

In 2018, Letterman began hosting a six-episode monthly series of hour-long programs on Netflix consisting of long-form interviews and field segments. The show, "My Next Guest Needs No Introduction with David Letterman", premiered January 12, 2018 with Barack Obama as its first guest. The second season premiered on May 31, 2019. Season 3 premiered on October 21, 2020, and includes Kim Kardashian West, Robert Downey Jr., Dave Chappelle and Lizzo as guests. Season 4 premiered on May 20, 2022, with Billie Eilish as the first guest. In October 2022, Letterman traveled to Kyiv, Ukraine, to film a special standalone episode of "My Next Guest Needs No Introduction with David Letterman", interviewing Ukrainian president Volodymyr Zelenskyy.

In spite of Johnny Carson's clear intention to pass his title to Letterman, NBC selected Jay Leno to host "The Tonight Show" after Carson's departure. Letterman maintained a close relationship with Carson through his break with NBC. Three years after he left for CBS, HBO produced a made-for-television movie called "The Late Shift", based on a book by "The New York Times" reporter Bill Carter, chronicling the battle between Letterman and Leno for the "Tonight Show" hosting spot.

Carson later made a few cameo appearances as a guest on Letterman's show. Carson's final television appearance was on May 13, 1994, on a "Late Show" episode taped in Los Angeles, when he made a surprise appearance during a Top 10 list segment. In early 2005, it was revealed that Carson occasionally sent jokes to Letterman, who used them in his monologue; according to CBS senior vice president Peter Lassally (a onetime producer for both men), Carson got "a big kick out of it." Letterman would do a characteristic Carson golf swing after delivering one of his jokes. In a tribute to Carson, all the opening monologue jokes during the first show after Carson's death were by Carson.

Lassally also claimed that Carson had always believed Letterman, not Leno, to be his "rightful successor". During the early years of the "Late Show"s run, Letterman occasionally used some of Carson's trademark bits, including "Carnac the Magnificent" (with Paul Shaffer as Carnac), "Stump the Band", and the "Week in Review".

Oprah Winfrey appeared on Letterman's show when he was hosting NBC's "Late Night" on May 2, 1989. After that appearance, the two had a 16-year feud that arose, as Winfrey explained to Letterman after it had been resolved, as a result of the acerbic tone of their 1989 interview, of which she said that it "felt so uncomfortable to me that I didn't want to have that experience again". The feud apparently ended on December 2, 2005, when Winfrey appeared on CBS's "Late Show with David Letterman" in an event Letterman jokingly called "the Super Bowl of Love".

Winfrey and Letterman also appeared together in a "Late Show" promo aired during CBS's coverage of Super Bowl XLI in February 2007, with the two sitting next to each other on a couch watching the game. Since the game was played between the Indianapolis Colts and Chicago Bears, the Indianapolis-born Letterman wore a Peyton Manning jersey, while Winfrey, whose show was taped in Chicago, wore a Brian Urlacher jersey. On September 10, 2007, Letterman made his first appearance on "The Oprah Winfrey Show" at Madison Square Garden in New York City.

Three years later, during CBS's coverage of Super Bowl XLIV between the Colts and the New Orleans Saints, the two appeared again in a "Late Show" promo, this time with Winfrey sitting on a couch between Letterman and Leno. Letterman wore the retired 70 jersey of Art Donovan, a member of the Colts' Hall of Fame and a regular Letterman guest. The appearance was Letterman's idea: Leno flew to New York City on an NBC corporate jet, sneaking into the Ed Sullivan Theater during the "Late Show"s February 4 taping wearing a disguise and meeting Winfrey and Letterman at a living room set created in the theater's balcony, where they taped their promo.

Winfrey interviewed Letterman in January 2013 on "Oprah's Next Chapter". They discussed their feud and Winfrey revealed that she had had a "terrible experience" while appearing on Letterman's show years earlier. Letterman could not recall the incident but apologized.

"Late Show" went off air for eight weeks in 2007 during November and December because of the Writers Guild of America strike. Letterman's production company, Worldwide Pants, was the first company to make an individual agreement with the WGA, allowing his show to come back on the air on January 2, 2008. In his first episode back, he surprised the audience with a newly grown beard, which signified solidarity with the strike. His beard was shaved off during the show on January 7, 2008.

On June 8 and 9, 2009, Letterman told two sexually themed jokes about a daughter (never named) of Sarah Palin on his TV show. These included a statutory rape joke about Palin's then 14-year-old daughter, Willow, and MLB player Alex Rodriguez. Palin was in New York City at the time with Willow, and none of her other children were at the game.

In a statement posted on the Internet, Palin said, "I doubt [Letterman would] ever dare make such comments about anyone else's daughter" and that "laughter incited by sexually perverted comments made by a 62-year-old male celebrity aimed at a 14-year-old girl is disgusting." On his June 10 show, Letterman responded to the controversy, saying the jokes were meant to be about Palin's 18-year-old daughter, Bristol, whose pregnancy as an unmarried teenager had caused some controversy during the United States presidential election of 2008. "These are not jokes made about [Palin's] 14-year-old daughter ... I would never, never make jokes about raping or having sex of any description with a 14-year-old girl."

His remarks did not end public criticism. The National Organization for Women (NOW) released a statement supporting Palin, noting that Letterman had made "[only] something of an apology." When the controversy failed to subside, Letterman addressed the issue again on his June 15 show, faulting himself for the error and apologizing "especially to the two daughters involved, Bristol and Willow, and also to the governor and her family and everybody else who was outraged by the joke." Rodriguez demanded an apology for implying that he was a child molester. Letterman never specifically apologized to Rodriguez.

On August 17, 2011, it was reported that an Islamist militant had posted a death threat against Letterman on a website frequented by Al-Qaeda supporters, calling on American Muslims to kill him for making a joke about the death of Ilyas Kashmiri, an Al-Qaeda leader who was killed in a June 2011 drone strike in Pakistan. In his August 22 show, Letterman joked about the threat, saying "State Department authorities are looking into this. They're not taking this lightly. They're looking into it. They're questioning, they're interrogating, there's an electronic trail—but everybody knows it's Leno."

Letterman appeared in the pilot episode of the short-lived 1986 series "Coach Toast", and appears with a bag over his head as a guest on Bonnie Hunt's 1990s sitcom "The Building". He appeared in "The Simpsons" as himself in a couch gag when the Simpsons find themselves (and the couch) in "Late Night with David Letterman". He had a cameo in the feature film "Cabin Boy", with Chris Elliott, who worked as a writer for Letterman. In this and other appearances, Letterman is listed in the credits as "Earl Hofert", the name of Letterman's maternal grandfather. He also appeared as himself in the Howard Stern biographical film "Private Parts" and the 1999 Andy Kaufman biopic "Man on the Moon", in a few episodes of Garry Shandling's 1990s TV series "The Larry Sanders Show", and in "The Abstinence", a 1996 episode of the sitcom "Seinfeld".

Letterman provided vocals for the Warren Zevon song "Hit Somebody" from "My Ride's Here", and provided the voice for Butt-head's father in the 1996 animated film "Beavis and Butt-Head Do America", again credited as Earl Hofert.

Letterman was the focus of "The Avengers on "Late Night with David Letterman"", issue 239 (January 1984) of the Marvel comic book series "The Avengers", in which the title characters (specifically Hawkeye, Wonder Man, Black Widow, Beast, and Black Panther) are guests on "Late Night". A parody of Letterman named David Endochrine is gassed to death along with his bandleader, Paul, and their audience in Frank Miller's "The Dark Knight Returns". In "", Letterman was parodied as "David Litterbin". Letterman appears in issues 13–14 and 18 of Harvey Pekar's autobiographical comic book "American Splendor". Those issues show Pekar's accounts of appearances on "Late Night".

In 2010, a documentary directed by Joke Fincioen and Biagio Messina, "Dying to do Letterman", was released, featuring Steve Mazan, a standup comic, who has cancer and wants to appear on Letterman's show. The film won best documentary and jury awards at the Cinequest Film Festival. Mazan published a book of the same name (full title "Dying to Do Letterman: Turning Someday into Today") about his own saga.

Letterman appeared as a guest on CNN's "Piers Morgan Tonight" on May 29, 2012, when he was interviewed by Regis Philbin, the guest host and Letterman's longtime friend. Philbin again interviewed Letterman (and Shaffer) while guest-hosting CBS's "The Late Late Show" (between the tenures of Craig Ferguson and James Corden) on January 27, 2015. In June 2013, Letterman appeared in the second episode of season two of "Comedians in Cars Getting Coffee". On November 5, 2013, he and Bruce McCall published a fiction satire book, "This Land Was Made for You and Me (But Mostly Me)", .

In Week 13 of the 2021 NFL season, Letterman joined Peyton and Eli Manning on their Manningcast feed of the "Monday Night Football" game between the New England Patriots and Buffalo Bills. Letterman mocked Bill Belichick after he was caught on camera wiping his nose with his shirt and was in the middle of recalling being with Roger Goodell when Goodell was booed at the unveiling of Peyton Manning's statue in Indianapolis when ESPN suddenly cut to commercials.

On February 1, 2022, Letterman was the guest on "Late Night with Seth Meyers", marking the 40th anniversary of the franchise's debut.

On November 20, 2023, Letterman returned to the Ed Sullivan theater on "The Late Show with Stephen Colbert".

Letterman started his production company, Worldwide Pants Incorporated, which produced his show and several others, in 1991. The company also produces feature films and documentaries and founded its own record label, Clear Entertainment. Worldwide Pants received significant attention in December 2007 after it was announced that it had independently negotiated its own contract with the Writers Guild of America, East, thus allowing Letterman, Craig Ferguson, and their writers to return to work, while the union continued its strike against production companies, networks, and studios with whom it had not yet reached agreements.

Letterman, Bobby Rahal, and Mike Lanigan co-own Rahal Letterman Lanigan Racing, an auto racing team competing in the WeatherTech SportsCar Championship and NTT IndyCar series. The team has twice won the Indianapolis 500: in 2004 with driver Buddy Rice, and in 2020 with Takuma Sato.

The Letterman Foundation for Courtesy and Grooming is a private foundation through which Letterman has donated millions of dollars to charities and other nonprofit organizations in Indiana and Montana, celebrity-affiliated organizations such as Paul Newman's Hole in the Wall Gang Camp, Ball State University, the American Cancer Society, the Salvation Army, and "Médecins Sans Frontières".

Letterman's biggest influence and mentor was Johnny Carson. Other comedians who influenced Letterman were Paul Dixon, Steve Allen, Jonathan Winters, Garry Moore, Jack Paar, Don Rickles, and David Brenner. Although Ernie Kovacs has also been mentioned as an influence, Letterman has denied this.

Comedians influenced by Letterman include Conan O'Brien, Jon Stewart, Stephen Colbert, Ray Romano, Jimmy Kimmel, Jay Leno, Arsenio Hall, Larry Wilmore, Seth Meyers, Norm Macdonald, Jimmy Fallon, John Oliver, and James Corden.

Letterman has tinnitus, a symptom of hearing loss. On the "Late Show" in 1996, he talked about his experience with tinnitus during an interview with William Shatner, who has severe tinnitus caused by an on-set explosion. Letterman has said that he was initially unable to pinpoint the noise inside his head and that he hears a constant ringing in his ears.

Letterman no longer drinks alcohol. On more than one occasion, he said that he had once been a "horrible alcoholic" and had begun drinking around the age of 13 and continued until 1981 when he was 34. He has said that in 1981, "I was drunk 80% of the time ... I loved it. I was one of those guys, I looked around, and everyone else had stopped drinking and I couldn't understand why." When he was shown drinking what appears to be alcohol on "Late Night" or the "Late Show", it was actually apple juice.

In 2015, Letterman said of his anxiety: "For years and years and years—30, 40 years—I was anxious and hypochondriacal and an alcoholic, and many, many other things that made me different from other people." He became calmer through a combination of Transcendental Meditation and low doses of medication. Letterman is a Presbyterian, a religious tradition he was originally brought up in by his mother, though he once said he was motivated by "Lutheran, Midwestern guilt".

In August 2021, Letterman was hospitalized in Providence, Rhode Island, after hitting his head on the sidewalk and falling unconscious. He favorably recalled the care he received at Rhode Island Hospital in a video released by the hospital's owner.

On July 2, 1968, Letterman married his college sweetheart, Michelle Cook, in Muncie, Indiana; they divorced by October 1977. He also had a long-term cohabiting relationship with the former head writer and producer on "Late Night", Merrill Markoe, from 1978 to 1988. Markoe created several "Late Night" staples, such as "Stupid Pet/Human Tricks". "Time" magazine wrote that theirs was the defining relationship of Letterman's career, with Markoe also acting as his writing partner. She "put the surrealism in Letterman's comedy."

Letterman and Regina Lasko started dating in February 1986, while he was still living with Markoe. Lasko gave birth to their son, Harry Joseph Letterman, on November 3, 2003. Harry is named after Letterman's father. In 2005, police discovered a plot to kidnap Letterman's son and demand a $5 million ransom. Kelly Frank, a house painter who had worked for Letterman, was charged in the conspiracy.

Letterman and Lasko wed on March 19, 2009, in a quiet courthouse civil ceremony in Choteau, Montana, where he had purchased a ranch in 1999. Letterman announced the marriage during the taping of his show of March 23, shortly after congratulating Bruce Willis on his marriage the week before. Letterman told the audience he nearly missed the ceremony because his truck became stuck in mud two miles from their house. The family resides in North Salem, New York, on a estate.

On October 1, 2009, Letterman announced on his show that he had been the victim of a blackmail attempt by a person threatening to reveal his sexual relationships with several of his female employees—a fact Letterman immediately thereafter confirmed. He said that someone had left a package in his car with material he said he would write into a screenplay and a book if Letterman did not pay him $2 million. Letterman said that he contacted the Manhattan District Attorney's office and partook in a sting operation that involved the handover of a fake check to the extortionist.

Joe Halderman, a producer of the CBS news magazine television series "48 Hours", was arrested around noon (EST) on October 1, 2009, after trying to deposit the check. He was indicted by a Manhattan grand jury following testimony from Letterman and pleaded not guilty to a charge of attempted grand larceny on October 2, 2009. Halderman pleaded guilty in March 2010 and was sentenced to six months in prison, followed by probation and community service.

A central figure in the case and one of the women with whom Letterman had had a sexual relationship was his longtime personal assistant Stephanie Birkitt, who often appeared on the show. She had also worked for "48 Hours". Until a month before the revelations, she had shared a residence with Halderman, who allegedly had copied her personal diary and used it, along with private emails, in the blackmail package.

In the days following the initial announcement of the affairs and the arrest, several prominent women, including Kathie Lee Gifford, co-host of NBC's "Today Show", and NBC news anchor Ann Curry, questioned whether Letterman's affairs with subordinates created an unfair working environment. A spokesman for Worldwide Pants said that the company's sexual harassment policy did not prohibit sexual relationships between managers and employees. According to business news reporter Eve Tahmincioglu, "CBS suppliers are supposed to follow the company's business conduct policies" and the CBS 2008 Business Conduct Statement states that "If a consenting romantic or sexual relationship between a supervisor and a direct or indirect subordinate should develop, CBS requires the supervisor to disclose this information to his or her Company's Human Resources Department".

On October 3, 2009, TMZ reported that a former CBS employee, Holly Hester, had had a yearlong secret affair with Letterman in the early 1990s while she was his intern and a student at New York University. On October 5, 2009, Letterman devoted a segment of his show to a public apology to his wife and staff. Three days later, Worldwide Pants announced that Birkitt had been placed on a "paid leave of absence" from the "Late Show".

Beginning in May 1988, Letterman was stalked by Margaret Mary Ray, a woman with schizophrenia. She stole his Porsche, camped out on his tennis court, and repeatedly broke into his house. Her exploits drew national attention, with Letterman occasionally joking about her on his show, though he never named her. After she died by suicide at age 46 in October 1998, Letterman told "The New York Times" that he had great compassion for her. A spokesperson for Letterman said: "This is a sad ending to a confused life."

In 2005, a woman was able to obtain a restraining order from a New Mexico judge, prohibiting Letterman from contacting her. She claimed he had sent her coded messages via his television program, causing her bankruptcy and emotional distress. Law professor Eugene Volokh called the case "patently frivolous".

Letterman is a car enthusiast and owns an extensive collection. In 2012, it was reported that the collection consisted of ten Ferraris, eight Porsches, four Austin-Healeys, two Honda motorcycles, a Chevy pickup, and one car each from automakers Mercedes-Benz, Jaguar, MG, Volvo, and Pontiac.

In his 2013 appearance on "Comedians in Cars Getting Coffee", part of Jerry Seinfeld's conversation with Letterman was filmed in Letterman's 1995 Volvo 960 station wagon, which is powered by a 380-horsepower racing engine. Paul Newman had the car built for Letterman.

Letterman shares a close relationship with the rock band Foo Fighters since its appearance on his first show upon his return from heart surgery. The band appeared many times on the "Late Show", including a week-long stint in October 2014. While introducing the band's performance of "Miracle" on the show of October 17, 2014, Letterman told the story of how a souvenir video of himself and his four-year-old son learning to ski used the song as background music, unbeknownst to Letterman until he saw it. He stated: "This is the second song of theirs that will always have great, great meaning for me for the rest of my life". This was the first time the band had heard this story. Worldwide Pants co-produced Dave Grohl's "" TV series. "Letterman was the first person to get behind this project," Grohl said.

David Letterman Communication and Media Building
In 1996, Letterman was ranked 45th on "TV Guide"s 50 Greatest TV Stars of All Time. In 2002, "The Late Show with David Letterman" was ranked seventh on TV Guide's 50 Greatest TV Shows of All Time.

On September 7, 2007, Letterman visited his "alma mater", Ball State University in Muncie, Indiana, for the dedication of a communications facility named in his honor for his dedication to the university. The $21 million, David Letterman Communication and Media Building opened for the 2007 fall semester. Thousands of Ball State students, faculty, and local residents welcomed Letterman back to Indiana. Letterman's emotional speech touched on his struggles as a college student and his late father, and also included the "top ten good things about having your name on a building", finishing with "if reasonable people can put my name on a $21 million building, anything is possible." Over many years Letterman "has provided substantial assistance to [Ball State's] Department of Telecommunications, including an annual scholarship that bears his name."

At the same time, Indiana Governor Mitch Daniels gave Letterman a Sagamore of the Wabash award, which recognizes distinguished service to the state of Indiana.

Awards and nominations

In his capacities as a performer, producer, or as part of a writing team, Letterman is among the most nominated people in the history of the Emmy Awards, with 52 nominations, winning two Daytime Emmys and ten Primetime Emmys since 1981. He won four American Comedy Awards and in 2011 became the first recipient of the Johnny Carson Award for Comedic Excellence at The Comedy Awards.

Letterman was a recipient of the 2012 Kennedy Center Honors, where he was called "one of the most influential personalities in the history of television, entertaining an entire generation of late-night viewers with his unconventional wit and charm." On May 16, 2017, Letterman was named the next recipient of the Mark Twain Prize for American Humor, the award granted annually by the John F. Kennedy Center for the Performing Arts. He received the prize in a ceremony on October 22, 2017.



Delroy Lindo

Delroy George Lindo (born 18 November 1952) is an English-American actor. He is the recipient of such accolades as a NAACP Image Award, a Satellite Award, and nominations for a Drama Desk Award, a Helen Hayes Award, a Tony Award, two Critics' Choice Television Awards, and three Screen Actors Guild Awards.

He moved with his mother to San Francisco in the United States when he was 16, after they had left London and lived in Canada for a few years. Here he completed his education and entered acting. Lindo has played prominent roles in four Spike Lee films: West Indian Archie in "Malcolm X" (1992), Woody Carmichael in "Crooklyn" (1994), Rodney Little in "Clockers" (1995), and Paul in "Da 5 Bloods" (2020). He received universal acclaim for his performance in "Da 5 Bloods" as a Vietnam War veteran, winning the New York Film Critics Circle Award for Best Actor and the National Society of Film Critics Award for Best Actor. 

Lindo also played Bo Catlett in "Get Shorty" (1995), Arthur Rose in "The Cider House Rules" (1999), and Detective Castlebeck in "Gone in 60 Seconds" (2000). Lindo starred as Alderman Ronin Gibbons in the TV series "The Chicago Code" (2011), as Winter on the series "Believe" (2014), and as Adrian Boseman in "The Good Fight" (2017–2021).

Delroy Lindo was born in 1952 in Lewisham, south-east London, the son of Jamaican parents. His mother had immigrated to the UK in 1951 to work as a nurse, and his father worked in various jobs. Lindo grew up in nearby Eltham and attended Woolwich Polytechnic School for Boys. He became interested in acting as a child when he appeared in a nativity play at school. 

When he was a teenager, Lindo moved with his mother to Toronto, Ontario. When he was 16, they moved to San Francisco. At the age of 24, Lindo began his studies in acting at the American Conservatory Theater, graduating in 1979.

Lindo made his film debut in 1976 with the Canadian John Candy comedy "Find the Lady". He played an army sergeant in "More American Graffiti" (1979).

For a decade from the early 1980s, Lindo's career was more focused on theatre acting than film, although he has said this was not a conscious decision. In 1982 he debuted on Broadway in ""Master Harold"...and the Boys," directed by the play's South African author Athol Fugard. By 1988, Lindo had earned a Tony nomination for his portrayal of Herald Loomis in August Wilson's "Joe Turner's Come and Gone".

Lindo returned to film in the science fiction film "Salute of the Jugger" (1990), which has become a cult classic. Although he had turned down Spike Lee for a role in "Do the Right Thing", Lee cast him as Woody Carmichael in the drama "Crooklyn" (1994), which brought Lindo notice. His other roles with Lee include West Indian Archie, a psychotic gangster, in "Malcolm X", and a starring role as a neighborhood drug dealer in "Clockers".

Other films in which he has starring roles are Barry Sonnenfeld's "Get Shorty" (1995), Ron Howard's "Ransom" (1996), and "Soul of the Game" (1996), as the baseball player Satchel Paige.

In 1998 Lindo co-starred as African-American explorer Matthew Henson, in the TV film "Glory & Honor", directed by Kevin Hooks. It portrayed Henson's nearly 20-year partnership with Commander Robert Peary in Arctic exploration, and their effort to find the Geographic North Pole in 1909. Lindo received a Satellite Award for best actor for his portrayal of Henson. Lindo has continued to work in television, and in 2006 was seen on the short-lived NBC drama "Kidnapped".

Lindo had a small role in the 1995 film "Congo", playing the corrupt Captain Wanta. Lindo was not credited for the role. He played an angel in the comedy film "A Life Less Ordinary" (1997).

He guest-starred on "The Simpsons" in the episode "Brawl in the Family", playing a character named Gabriel.

In the British film "Wondrous Oblivion" (2003), directed by Paul Morrison, Lindo starred as Dennis Samuels, the father of a Jamaican immigrant family in London in the 1950s. Lindo said he made the film in honor of his parents, who had similarly moved to London in those years.

In 2007, Lindo began an association with Berkeley Repertory Theatre in Berkeley, California, when he directed Tanya Barfield's play "The Blue Door". In the autumn of 2008, Lindo revisited August Wilson's play "Joe Turner's Come and Gone", directing a production at the Berkeley Rep. In 2010, he played the role of elderly seer Bynum in David Lan's production of "Joe Turner" at the Young Vic Theatre in London.

Lindo was in the main cast of the Fox crime drama "The Chicago Code" (2011), the NBC fantasy series "Believe", and the ABC soap "Blood & Oil" (2015). In 2017, Lindo began playing Adrian Boseman in the CBS legal drama "The Good Fight", a role he would star in for the series' first four seasons and reprise as a guest star in its fifth season. Lindo was cast as the lead in an ABC drama pilot "Harlem's Kitchen" in March 2020.

In 2015, Lindo was expected to play Marcus Garvey in a biopic of the black nationalist historical figure that had been in pre-production for several years. Lindo appeared in the action film "Point Break" (2015), the horror film "Malicious" (2018), the drama "Battlecreek", "Da 5 Bloods" (2020), another collaboration with Spike Lee, and "The Harder They Fall" (2021) as Bass Reeves. 

For his role in "Da 5 Bloods", Lindo received critical acclaim and a number of accolades.

"Entertainment Weekly" said of Hulu's comedy series "Unprisoned" (2023), "Delroy Lindo is so good it should be illegal."

It was announced in July 2021 that Lindo would star as Mr Nancy in the British Amazon Prime miniseries adaptation of Neil Gaiman's "Anansi Boys" alongside Malachi Kirby. In November, Lindo officially joined the cast of the upcoming Marvel Studios film "Blade" in an undisclosed role.

As of 2020, Lindo was developing a screenplay for his directorial debut about the Windrush generation.

Lindo married his wife Nashormeh in 1990. They settled in Oakland, California, in 1996, having moved from New York City and preferring the San Francisco Bay Area to Los Angeles. Their son Damiri was born in 2001. 

Lindo is a football fan and supports Manchester United.

Upon learning more about the Windrush generation, both through his mother's account and his own role as a Jamaican immigrant in "Wondrous Oblivion", Lindo became inspired to study the subject and history further. In 2014, he completed a master's thesis from New York University's Gallatin School. 

He was awarded an honorary doctorate in Arts and Humanities from Virginia Union University.


David Janssen

David Janssen (born David Harold Meyer; March 27, 1931February 13, 1980) was an American film and television actor who is best known for his starring role as Richard Kimble in the television series "The Fugitive" (1963–1967). Janssen also had the title roles in three other series: "Richard Diamond, Private Detective"; "O'Hara, U.S. Treasury" and "Harry O".

In 1996 "TV Guide" ranked him number 36 on its "50 Greatest TV Stars of All Time" list.

David Janssen was born on March 27, 1931, in Naponee, a village in Franklin County in southern Nebraska, to Harold Edward Meyer, a banker, and Berniece Graf, a former Miss Nebraska and Ziegfeld girl. Following his parents' divorce in 1935, his mother moved with five-year-old David to Los Angeles, and married Eugene Janssen in 1940. Young David used his stepfather's name after he entered show business as a child. 

He attended Fairfax High School, where he excelled on the basketball court, setting a school scoring record that lasted over 20 years. His first film part was at the age of thirteen, and by the age of twenty-five he had appeared in twenty films and served two years as an enlisted man in the United States Army. During his Army days, Janssen became a friend of fellow enlistees Martin Milner and Clint Eastwood while posted at Fort Ord, California.

Janssen starred in four television series of his own:

At the time of its airing in August 1967, the final episode of "The Fugitive" held the record for the greatest number of American homes with television sets to watch a series finale – 72 percent. In 1996 "TV Guide" ranked "The Fugitive" number 36 on its "50 Greatest Shows of All Time" list.

His films include: "To Hell and Back", the biography of Audie Murphy, who was the most decorated American soldier of World War II; "Hell to Eternity", a 1960 American World War II biopic starring Jeffrey Hunter as a Hispanic boy who fought in the Battle of Saipan and who was raised by Japanese-American foster parents; John Wayne's Vietnam war film "The Green Berets"; opposite Gregory Peck, in the space story "Marooned", in which Janssen played an astronaut sent to rescue three stranded men in space; and "The Shoes of the Fisherman", as a television journalist in Rome reporting on the election of a new Pope (Anthony Quinn). 

He also played pilot Harry Walker in the 1973 action movie "Birds of Prey". He starred as a Los Angeles police detective trying to clear himself in the killing of an apparently innocent doctor in the 1967 film "Warning Shot", which was shot during a break in the spring and summer of 1966 between the third and fourth seasons of "The Fugitive."

Janssen played an alcoholic in the 1977 TV movie "A Sensitive, Passionate Man", which co-starred Angie Dickinson, and played an engineer who devises an unbeatable system for blackjack in the 1978 made-for-TV movie "Nowhere to Run", co-starring Stefanie Powers and Linda Evans. Janssen's impressively husky voice was used to good effect as the narrator for the TV mini-series "Centennial" (1978–79); he also appeared in the final episode. And in 1979 he starred in the made-for-TV mini series "S.O.S. Titanic" as John Jacob Astor, playing opposite Beverly Ross as his wife, Madeleine.

Though Janssen's scenes were cut from the final release, he also appeared as a journalist in the film "Inchon", which he accepted to work with Laurence Olivier, who played General Douglas MacArthur. At the time of his death, Janssen had just begun filming a television movie playing the part of Father Damien, the priest who dedicated himself to the leper colony on the island of Molokai, Hawaii. The part was eventually reassigned to actor Ken Howard of the CBS series "The White Shadow".

Janssen was married twice. His first marriage was to model and interior decorator Ellie Graham, whom he married in Las Vegas on August 25, 1958. They divorced in 1968. In 1975, he married actress and model Dani Crayne Greco. They remained married until Janssen's death.

Janssen was a heavy drinker, and a chain smoker who smoked up to four packs of cigarettes a day. He died from a sudden heart attack in the early morning of February 13, 1980, at his beachfront home in Malibu, California, at the age of 48. At the time of his death, Janssen was filming the television movie "Father Damien". Janssen was buried at the Hillside Memorial Park Cemetery in Culver City, California. A non-denominational funeral was held at the Jewish chapel of the cemetery on February 17. Suzanne Pleshette delivered the eulogy at the request of Janssen's widow. Milton Berle, Johnny Carson, Tommy Gallagher, Richard Harris, Stan Herman, Rod Stewart and Gregory Peck were among Janssen's pallbearers. Honorary pallbearers included Jack Lemmon, George Peppard, James Stewart and Danny Thomas.

For his contribution to the television industry, David Janssen has a star on the Hollywood Walk of Fame located on the 7700 block of Hollywood Boulevard.



Docetism

In the history of Christianity, docetism (from the "dokeĩn" "to seem", "dókēsis" "apparition, phantom") is the heterodox doctrine that the phenomenon of Jesus, his historical and bodily existence, and above all the human form of Jesus, was mere semblance without any true reality. Broadly it is taken as the belief that Jesus only seemed to be human, and that his human form was an illusion.

The word "Dokētaí" ("Illusionists") referring to early groups who denied Jesus's humanity, first occurred in a letter by Bishop Serapion of Antioch (197–203), who discovered the doctrine in the Gospel of Peter, during a pastoral visit to a Christian community using it in Rhosus, and later condemned it as a forgery. It appears to have arisen over theological contentions concerning the meaning, figurative or literal, of a sentence from the Gospel of John: "the Word was made Flesh".

Docetism was unequivocally rejected at the First Council of Nicaea in 325 and is regarded as heretical by the Catholic Church, Eastern Orthodox Church, Coptic Orthodox Church of Alexandria, Armenian Apostolic Church, Ethiopian Orthodox Tewahedo Church, and many Protestant denominations that accept and hold to the statements of these early church councils, such as Reformed Baptists, Reformed Christians, and all Trinitarian Christians.

Docetism is broadly defined as any teaching that claims that Jesus' body was either absent or illusory. The term 'docetic' is rather nebulous. Two varieties were widely known. In one version, as in Marcionism, Christ was so divine that he could not have been human, since God lacked a material body, which therefore could not physically suffer. Jesus only "appeared" to be a flesh-and-blood man; his body was a phantasm. Other groups who were accused of docetism held that Jesus was a man in the flesh, but Christ was a separate entity who entered Jesus' body in the form of a dove at his baptism, empowered him to perform miracles, and abandoned him upon his death on the cross.

Docetism's origin within Christianity is obscure. Ernst Käsemann controversially defined the Christology of the Gospel of John as "naïve docetism" in 1968. The ensuing debate reached an impasse as awareness grew that the very term "docetism", like "gnosticism", was difficult to define within the religio-historical framework of the debate. It has occasionally been argued that its origins were in heterodox Judaism or Oriental and Grecian philosophies. The alleged connection with Jewish Christianity would have reflected Jewish Christian concerns with the inviolability of (Jewish) monotheism. Docetic opinions seem to have circulated from very early times, 1 John 4:2 appearing explicitly to reject them. Some 1stcentury Christian groups developed docetic interpretations partly as a way to make Christian teachings more acceptable to pagan ways of thinking about divinity.

In his critique of the theology of Clement of Alexandria, Photius in his Myriobiblon held that Clement's views reflected a quasi-docetic view of the nature of Christ, writing that "[Clement] hallucinates that the Word was not incarnate but "only seems to be"." (ὀνειροπολεῖ καὶ μὴ σαρκωθῆναι τὸν λόγον ἀλλὰ "δόξαι".) In Clement's time, some disputes contended over whether Christ assumed the "psychic" flesh of mankind as heirs to Adam, or the "spiritual" flesh of the resurrection. Docetism largely died out during the first millennium AD.

The opponents against whom Ignatius of Antioch inveighs are often taken to be Monophysite docetists. In his letter to the Smyrnaeans, 7:1, written around 110AD, he writes:
While these characteristics fit a Monophysite framework, a slight majority of scholars consider that Ignatius was waging a polemic on two distinct fronts, one Jewish, the other docetic; a minority holds that he was concerned with a group that commingled Judaism and docetism. Others, however, doubt that there was actual docetism threatening the churches, arguing that he was merely criticizing Christians who lived Jewishly or that his critical remarks were directed at an Ebionite or Cerinthian possessionist Christology, according to which Christ was a heavenly spirit that temporarily possessed Jesus.

Some commentators have attempted to make a connection between Islam and docetism using the following Quranic verse:

Some scholars theorise that Islam was influenced by Manichaeism (Docetism) in this view. However the general consensus is that Manichaeism was not prevalent in Mecca in the 6th and 7th centuries, when Islam developed, and the influence can therefore not be proven.

Since Arthur Drews published his "The Christ Myth" ("Die Christusmythe") in 1909, occasional connections have been drawn between docetist theories and the modern idea that Christ was a myth. Shailer Mathews called Drews' theory a "modern docetism". Frederick Cornwallis Conybeare thought any connection to be based on a misunderstanding of docetism. The idea recurred in classicist Michael Grant's 1977 review of the evidence for Jesus, who compared modern scepticism about a historical Jesus to the ancient docetic idea that Jesus only "seemed" to come into the world "in the flesh". Modern supporters of the theory did away with "seeming".





Drachma

Drachma may refer to:



Denarius

The denarius (; : dēnāriī, ) was the standard Roman silver coin from its introduction in the Second Punic War to the reign of Gordian III (AD 238–244), when it was gradually replaced by the "antoninianus". It continued to be minted in very small quantities, likely for ceremonial purposes, until and through the Tetrarchy (293–313).

The word "dēnārius" is derived from the Latin "dēnī" "containing ten", as its value was originally of 10 "assēs". The word for "money" descends from it in Italian ("denaro"), Slovene ("denar"), Portuguese ("dinheiro"), and Spanish ("dinero"). Its name also survives in the dinar currency.

Its symbol is represented in Unicode as 𐆖 (U+10196), a numeral monogram that appeared on the obverse in the Republican period, denoting the 10 "asses" ("X") to 1 "denarius" ("I") conversion rate. However it can also be represented as X̶ (capital letter X with combining long stroke overlay).

A predecessor of the "denarius" was first struck in 269 or 268 BC, five years before the First Punic War, with an average weight of 6.81 grams, or of a Roman pound. Contact with the Greeks had prompted a need for silver coinage in addition to the bronze currency that the Romans were using at that time. This predecessor of the "denarius" was a Greek-styled silver coin of "didrachm" weight, which was struck in Neapolis and other Greek cities in southern Italy. These coins were inscribed with a legend that indicated that they were struck for Rome, but in style they closely resembled their Greek counterparts. They were rarely seen at Rome, to judge from finds and hoards, and were probably used either to buy supplies or to pay soldiers.

The first distinctively Roman silver coin appeared around 226 BC. Classical historians have sometimes called these coins "heavy "denarii"", but they are classified by modern numismatists as "quadrigati", a term which survives in one or two ancient texts and is derived from the "quadriga", or four-horse chariot, on the reverse. This, with a two-horse chariot or "biga" which was used as a reverse type for some early "denarii", was the prototype for the most common designs used on Roman silver coins for a number of years.

Rome overhauled its coinage shortly before 211 BC, and introduced the "denarius" alongside a short-lived denomination called the "victoriatus". The "denarius" contained an average 4.5 grams, or of a Roman pound, of silver, and was at first tariffed at ten "asses", hence its name, which means 'tenner'. It formed the backbone of Roman currency throughout the Roman Republic and the early Empire.

The "denarius" began to undergo slow debasement toward the end of the republican period. Under the rule of Augustus (27 BC – 14 AD) its weight fell to 3.9 grams (a theoretical weight of of a Roman pound). It remained at nearly this weight until the time of Nero (AD 37–68), when it was reduced to of a pound, or 3.4 grams. Debasement of the coin's silver content continued after Nero. Later Roman emperors also reduced its weight to 3 grams around the late 3rd century.

The value at its introduction was 10 "asses", giving the "denarius" its name, which translates as "containing ten". In about 141 BC, it was re-tariffed at 16 "asses", to reflect the decrease in weight of the "as". The "denarius" continued to be the main coin of the Roman Empire until it was replaced by the antoninianus in the early 3rd century AD. The coin was last issued, in bronze, under Aurelian between 270 and 275 AD, and in the first years of the reign of Diocletian.

1 gold "aureus" = 2 gold "quinarii" = 25 silver "denarii" = 50 silver "quinarii" = 100 bronze "sestertii" = 200 bronze dupondii = 400 copper "asses" = 800 copper "semisses" = 1,600 copper "quadrantes"

It is difficult to give even rough comparative values for money from before the 20th century, as the range of products and services available for purchase was so different. During the republic (509 BC – 27 BC), a legionary earned 112.5 "denarii" per year (0.3 "denarii" per day). Under Julius Caesar, this was doubled to 225 "denarii"/yr, with soldiers having to pay for their own food and arms, while in the reign of Augustus a Centurion received at least 3,750 "denarii" per year, and for the highest rank, 15,000 "denarii".

By the late Roman Republic and early Roman Empire (), a common soldier or unskilled laborer would be paid 1 "denarius"/day (with no tax deductions), around 300% inflation compared to the early period. Using the cost of bread as a baseline, this pay equates to around US$20 in 2013 terms. Expressed in terms of the price of silver, and assuming 0.999 purity, a troy ounce "denarius" had a precious metal value of around US$2.60 in 2021.

At the height of the Roman Empire a "sextarius" (546 ml or about 2 American cups) of ordinary wine cost roughly one "dupondius" ( of a "denarius"); after Diocletian's Edict on Maximum Prices was issued in 301 AD, the same item cost 8 debased common "denarii" – 6300% inflation.

Silver content plummeted across the lifespan of the "denarius". Under the Roman Empire (after Nero) the "denarius" contained approximately 50 grains, 3.24 grams, or 0.105 ozt (about troy ounce). The fineness of the silver content varied with political and economic circumstances. From a purity of greater than 90% silver in the 1st century AD, the "denarius" fell to under 60% purity by 200 AD, and plummeted to 5% purity by 300 AD. By the reign of Gallienus, the "antoninianus" was a copper coin with a thin silver wash.

In the final years of the 1st century BC Tincomarus, a local ruler in southern Britain, started issuing coins that appear to have been made from melted down "denarii". The coins of Eppillus, issued around Calleva Atrebatum around the same time, appear to have derived design elements from various "denarii", such as those of Augustus and M. Volteius.

Even after the "denarius" was no longer regularly issued, it continued to be used as a unit of account, and the name was applied to later Roman coins in a way that is not understood. The Arabs who conquered large parts of the land that once belonged to the Eastern Roman Empire issued their own gold dinar. The lasting legacy of the "denarius" can be seen in the use of "d" as the abbreviation for the British penny until 1971. It also survived in France as the name of a coin, the denier. The "denarius" also survives in the common Arabic name for a currency unit, the "dinar" used from pre-Islamic times, and still used in several modern Arab nations. The major currency unit in former Principality of Serbia, Kingdom of Serbia and former Yugoslavia was "dinar", and it is still used in present-day Serbia. The Macedonian currency "denar" is also derived from the Roman "denarius". The Italian word "denaro", the Spanish word "dinero", the Portuguese word "dinheiro", and the Slovene word "", all meaning money, are also derived from Latin "denarius". The pre-decimal currency of the United Kingdom until 1970 of pounds, shillings and pence was abbreviated as lsd, with "d" referring to "denarius" and standing for penny.

In the New Testament, the gospels refer to the "denarius" as a day's wage for a common laborer (Matthew 20:2, John 12:5). In the Book of Revelation, during the Third Seal: Black Horse, a choinix ("quart") of wheat and three quarts of barley were each valued at one "denarius". Bible scholar Robert H. Mounce says the price of the wheat and barley as described in the vision appears to be ten to twelve times their normal cost in ancient times. Revelation thus describes a condition where basic goods are sold at greatly inflated prices. Thus, the black horse rider depicts times of deep scarcity or famine, but not of starvation. Apparently, a choinix of wheat was the daily ration of one adult. Thus, in the conditions pictured by Revelation 6, the normal income for a working-class family would buy enough food for only one person. The less costly barley would feed three people for one day's wages.

The "denarius" is also mentioned in the Parable of the Unforgiving Servant & in Parable of the Good Samaritan (Luke 10:25–37). The Render unto Caesar passage in Matthew 22:15–22 and Mark 12:13–17 uses the word (δηνάριον) to describe the coin held up by Jesus, translated in the King James Bible as "tribute penny". It is commonly thought to be a "denarius" with the head of Tiberius.



Della Rovere

The House of Della Rovere (; literally "of the oak tree") was a powerful Italian noble family. It had humble origins in Savona, in Liguria, and acquired power and influence through nepotism and ambitious marriages arranged by two Della Rovere popes: Francesco Della Rovere, who ruled as Sixtus IV from 1471 to 1484) and his nephew Giuliano, who became Julius II in 1503. Sixtus IV built the Sistine Chapel, which was named after him. Julius II was patron to Michelangelo, Raphael and many other Renaissance artists and started the modern rebuilt of St. Peter's Basilica. Also the Basilica of San Pietro in Vincoli in Rome was the family church of the Della Rovere. Members of the family were influential in the Church of Rome, and as dukes of Urbino, dukes of Sora and lords of Senigallia; the title of Urbino was extinguished with the death of Francesco Maria II in 1631, and the family died out with the death of his granddaughter Vittoria, Grand Duchess of Tuscany.

Francesco Della Rovere was born into a poor family in Liguria in north-west Italy in 1414, the son of Leonardo della Rovere of Savona. A Franciscan who became Minister General of his order, then cardinal, he had a reptation for unworldliness until he was elected pope in 1471. As Sixtus IV he was both wealthy and powerful, and at once set about giving power and wealth to his nephews of the Della Rovere and Riario families. Within months of his election, he had made Giuliano della Rovere (the future pope Julius II) and Pietro Riario both cardinals and bishops; four other nephews were also made cardinals. He made Giovanni Della Rovere, who was not a priest, prefect of Rome, and arranged for him to marry into the da Montefeltro family, dukes of Urbino. Sixtus claimed descent from a noble Della Rovere family, the counts of Vinovo in Piemonte, and adopted their coat-of-arms.

Guidobaldo da Montefeltro adopted Francesco Maria I della Rovere, his sister's child and nephew of Pope Julius II. Guidobaldo I, who was heirless, called Francesco Maria at his court, and named him as heir of the Duchy of Urbino in 1504, this through the intercession of Julius II. In 1508, Francesco Maria inherited the duchy thereby starting the line of Rovere Dukes of Urbino. That dynasty ended in 1626 when Pope Urban VIII incorporated Urbino into the papal dominions. As compensation to the last sovereign duke, the title only could be continued by Francesco Maria II, and after his death by his heir, Federico Ubaldo.

Vittoria, last descendant of the della Rovere family (she was the only child of Federico Ubaldo), married Ferdinando II de' Medici, Grand Duke of Tuscany. They had two children: Cosimo III, Tuscany's longest reigning monarch, and Francesco Maria de' Medici, a prince of the Church.


Among the many people who did not belong to this family, but bore the same name, are:
and various artists, including:


David Mamet

David Alan Mamet (; born November 30, 1947) is an American playwright, filmmaker, and author. He won a Pulitzer Prize and received Tony nominations for his plays "Glengarry Glen Ross" (1984) and "Speed-the-Plow" (1988). He first gained critical acclaim for a trio of off-Broadway 1970s plays: "The Duck Variations", "Sexual Perversity in Chicago", and "American Buffalo". His plays "Race" and "The Penitent", respectively, opened on Broadway in 2009 and previewed off-Broadway in 2017.

Feature films that Mamet both wrote and directed include "House of Games" (1987), "Homicide" (1991), "The Spanish Prisoner" (1997), and his biggest commercial success, "Heist" (2001). His screenwriting credits include "The Postman Always Rings Twice" (1981), "The Verdict" (1982), "The Untouchables" (1987), "Hoffa" (1992), "Wag the Dog" (1997), and "Hannibal" (2001). Mamet himself wrote the screenplay for the 1992 adaptation of "Glengarry Glen Ross", and wrote and directed the 1994 adaptation of his play "Oleanna" (1992). He created and produced the CBS series "The Unit" (2006–2009).

Mamet's books include: "On Directing Film" (1991), a commentary and dialogue about film-making; "The Old Religion" (1997), a novel about the lynching of Leo Frank; "Five Cities of Refuge: Weekly Reflections on Genesis, Exodus, Leviticus, Numbers and Deuteronomy" (2004), a Torah commentary with Rabbi Lawrence Kushner; "The Wicked Son" (2006), a study of Jewish self-hatred and antisemitism; "Bambi vs. Godzilla", a commentary on the movie business; "The Secret Knowledge: On the Dismantling of American Culture" (2011), a commentary on cultural and political issues; "Three War Stories" (2013), a trio of novellas about the physical and psychological effects of war; and "Everywhere an Oink Oink: An Embittered, Dyspeptic, and Accurate Report of Forty Years in Hollywood" (2023), an autobiographical account of his experiences in Hollywood.

Mamet was born in 1947 in Chicago to Lenore June (née Silver), a teacher, and Bernard Morris Mamet, a labor attorney. He is Jewish. His paternal grandparents were Polish Jews. Mamet has said his parents were communists and described himself as a red diaper baby. One of his earliest jobs was as a busboy at Chicago's London House and The Second City. He also worked as an actor, editor for "Oui" magazine and as a cab-driver. He was educated at the progressive Francis W. Parker School and at Goddard College in Plainfield, Vermont. At the Chicago Public Library Foundation 20th anniversary fundraiser in 2006, though, Mamet announced "My alma mater is the Chicago Public Library. I got what little educational foundation I got in the third-floor reading room, under the tutelage of a Coca-Cola sign".

After a move to Chicago's North Side, Mamet encountered theater director Robert Sickinger, and began to work occasionally at Sickinger's Hull House Theatre. This represented the beginning of Mamet's lifelong involvement with the theater.

Mamet is a founding member of the Atlantic Theater Company; he first gained acclaim for a trio of off-Broadway plays in 1976, "The Duck Variations," "Sexual Perversity in Chicago," and "American Buffalo." He was awarded the Pulitzer Prize in 1984 for "Glengarry Glen Ross," which received its first Broadway revival in the summer of 2005. His play "Race", which opened on Broadway on December 6, 2009, and featured James Spader, David Alan Grier, Kerry Washington, and Richard Thomas in the cast, received mixed reviews. His play "The Anarchist", starring Patti LuPone and Debra Winger, in her Broadway debut, opened on Broadway on November 13, 2012, in previews and was scheduled to close on December 16, 2012. His 2017 play "The Penitent" previewed off-Broadway on February 8, 2017.

In 2002, Mamet was inducted into the American Theater Hall of Fame. Mamet later received the PEN/Laura Pels Theater Award for Grand Master of American Theater in 2010.

In 2017, Mamet released an online class for writers entitled "David Mamet teaches dramatic writing".

In 2019 Mamet returned to the London West End with a new play, "Bitter Wheat", at the Garrick Theatre, starring John Malkovich.

In 2023 it was announced Mamet was writing a new play entitled, "Henry Johnson". The production is announced to debut in Los Angeles starring Shia LaBeouf.

Mamet's first film work was as a screenwriter, later directing his own scripts.

According to Joe Mantegna, Mamet worked as a script doctor for the 1978 film "Towing".

Mamet's first produced screenplay was the 1981 production of "The Postman Always Rings Twice", based on James M. Cain's novel. He received an Academy Award nomination one year later for the 1982 legal drama, "The Verdict". He also wrote the screenplays for "The Untouchables" (1987), "Hoffa" (1992), "The Edge" (1997), "Wag the Dog" (1997), "Ronin" (1998), and "Hannibal" (2001). He received a second Academy Award nomination for "Wag the Dog".

In 1987, Mamet made his film directing debut with his screenplay "House of Games", which won Best Screenplay awards at the 1987 Venice Film Festival and the Film of the Year in 1989 from the London Film Critics' Circle Awards. The film starred his then-wife, Lindsay Crouse, and many longtime stage associates and friends, including fellow Goddard College graduates. Mamet was quoted as saying, "It was my first film as a director and I needed support, so I stacked the deck." After "House of Games", Mamet later wrote and directed two more films focusing on the world of con artists, "The Spanish Prisoner" (1997) and "Heist" (2001). Among those films, "Heist" enjoyed the biggest commercial success.

Other films that Mamet both wrote and directed include: "Things Change" (1988), "Homicide" (1991) (nominated for the Palme d'Or at 1991 Cannes Film Festival and won a "Screenwriter of the Year" award for Mamet from the London Film Critics' Circle Awards), "Oleanna" (1994), "The Winslow Boy" (1999), "State and Main" (2000), "Spartan" (2004), "Redbelt" (2008), and the 2013 bio-pic TV movie "Phil Spector".

A feature-length film, a thriller titled "Blackbird", was intended for release in 2015, but is still in development.
When Mamet adapted his play for the 1992 film "Glengarry Glen Ross", he wrote an additional part (including the monologue "Coffee's for closers") for Alec Baldwin.

Mamet continues to work with an informal repertory company for his films, including Crouse, William H. Macy, Joe Mantegna, and Rebecca Pidgeon, as well as the aforementioned school friends.

Mamet rewrote the script for "Ronin" under the pseudonym "Richard Weisz" and turned in an early version of a script for "Malcolm X" which was rejected by director Spike Lee. Mamet also wrote an unproduced biopic script about Roscoe Arbuckle with Chris Farley intended to portray him. In 2000, Mamet directed a film version of "Catastrophe," a one-act play by Samuel Beckett featuring Harold Pinter and John Gielgud (in his final screen performance). In 2008, he wrote and directed the mixed martial arts movie "Redbelt," about a martial arts instructor tricked into fighting in a professional bout.

In "On Directing Film", Mamet advocates for a method of storytelling based on Eisenstein's montage theory, stating that the story should be told through the juxtaposition of uninflected images. This method relies heavily on the cut between scenes, and Mamet urges directors to eliminate as much narration as possible. Mamet asserts that directors should focus on getting the point of a scene across, rather than simply following a protagonist, or adding visually beautiful or intriguing shots. Films should create order from disorder in search of the objective.

In 2023, reports emerged that Mamet would direct and co-write a new film titled "Assassination", his first film since 2008. The film will center around the Chicago Mob ordering the assassination of John F. Kennedy, and will star Viggo Mortensen, Shia LaBeouf, Courtney Love, Al Pacino, and John Travolta. The film's production is scheduled to start in September 2023.

Mamet published the essay collection "Writing in Restaurants" in 1986, followed by the poetry collection "The Hero Pony" in 1990. He has also published a series of short plays, monologues and four novels, "The Village" (1994), "The Old Religion" (1997), "Wilson: A Consideration of the Sources" (2000), and "Chicago" (2018). He has written several non-fiction texts, and children's stories, including "True and False: Heresy and Common Sense for the Actor "(1997). In 2004 he published a lauded version of the classical Faust story, "Faustus", however, when the play was staged in San Francisco during the spring of 2004, it was not well received by critics. On May 1, 2010, Mamet released a graphic novel "The Trials of Roderick Spode (The Human Ant)".

Mamet detailed his conversion from modern liberalism to "a reformed liberal" in "The Secret Knowledge: On the Dismantling of American Culture" in 2011. Mamet published "Three War Stories", a collection of novellas, in 2013 ; the novel "The Diary of a Porn Star by Priscilla Wriston-Ranger: As Told to David Mamet With an Afterword by Mr. Mamet" in 2019; and the political commentary "Recessional: The Death of Free Speech and the Cost of a Free Lunch" in 2022. In 2023 Mamet recounted his experiences in Hollywood and the movie-making industry in "Everywhere an Oink Oink: An Embittered, Dyspeptic, and Accurate Report of Forty Years in Hollywood."

Mamet wrote one episode of "Hill Street Blues", "A Wasted Weekend", that aired in 1987. His then-wife, Lindsay Crouse, appeared in numerous episodes (including that one) as Officer McBride. Mamet is also the creator, producer and frequent writer of the television series "The Unit", where he wrote a well-circulated memo to the writing staff. He directed a third-season episode of "The Shield" with Shawn Ryan. In 2007, Mamet directed two television commercials for Ford Motor Company. The two 30-second ads featured the Ford Edge and were filmed in Mamet's signature style of fast-paced dialogue and clear, simple imagery. Mamet's sister, Lynn, is a producer and writer for television shows, such as "The Unit" and "Law & Order".

Mamet has contributed several dramas to BBC Radio through Jarvis & Ayres Productions, including an adaptation of "Glengarry Glen Ross" for BBC Radio 3 and new dramas for BBC Radio 4. The comedy "Keep Your Pantheon (or On the Whole I'd Rather Be in Mesopotamia)" was aired in 2007. "The Christopher Boy's Communion" was another Jarvis & Ayres production, first broadcast on BBC Radio 4 on March 8, 2021.

The papers of David Mamet were sold to the Harry Ransom Center at the University of Texas at Austin in 2007 and first opened for research in 2009. The growing collection consists mainly of manuscripts and related production materials for most of his plays, films, and other writings, but also includes his personal journals from 1966 to 2005. In 2015, the Ransom Center secured a second major addition to Mamet's papers, including more recent works. Additional materials relating to Mamet and his career can be found in the Ransom Center's collections of Robert De Niro, Mel Gussow, Tom Stoppard, Sam Shepard, Paul Schrader, Don DeLillo, and John Russell Brown.

Mamet's style of writing dialogue, marked by a cynical, street-smart edge, has come to be called "Mamet speak." Mamet himself has criticized his (and other writers') tendency to write "pretty" at the expense of sound, logical plots. When asked how he developed his style for writing dialogue, Mamet said, "In my family, in the days prior to television, we liked to while away the evenings by making ourselves miserable, based solely on our ability to speak the language viciously. That's probably where my ability was honed."

Mamet's plays have frequently sparked debate and controversy. Following a 1992 staging of "Oleanna", a play in which a college student accuses her professor of trying to rape her, a critic reported that the play divided the audience by gender and recounted that "couples emerged screaming at each other".

In his 2014 book "David Mamet and Male Friendship", Arthur Holmberg examined Mamet's portrayal of male friendships, especially focusing on the contradictions and ambiguities of male bonding as dramatized in Mamet's plays and films.

Mamet and actress Lindsay Crouse married in 1977 and divorced in 1990. The couple have two children. Mamet has been married to actress and singer-songwriter Rebecca Pidgeon since 1991, and they have two children. Mamet and Pidgeon live in Santa Monica, California.

Mamet is a Reform Jew and strongly pro-Israel.

In 2005, Mamet became a contributing blogger for "The Huffington Post", drawing satirical cartoons with themes including political strife in Israel. In a 2008 essay at "The Village Voice" titled "Why I Am No Longer a 'Brain-Dead Liberal he discussed how his political views had shifted from liberalism to conservatism. In interviews, Mamet has highlighted his agreement with free market theorists such as Friedrich Hayek, the historian Paul Johnson, and economist Thomas Sowell, whom Mamet called "one of our greatest minds". In 2022, Mamet declined to explicitly label himself a Republican, but described himself as a conservative who "would like to conserve those things I grew up with: the love of family, the love of the country, love of service, love of God, love of community".

During promotion of a book, Mamet said British people had "a taint of anti-semitism," claiming they "want to give [Israel] away to some people whose claim is rather dubious." In the same interview, Mamet went on to say that "there are famous dramatists and novelists [in the UK] whose works are full of anti-Semitic filth." He refused to give examples because of British libel laws (the interview was conducted in New York City for the "Financial Times"). He is known for his pro-Israel positions; in his book "The Secret Knowledge" he claimed that "Israelis would like to live in peace within their borders; the Arabs would like to kill them all."

Mamet endorsed Republican Mitt Romney for president in 2012, and wrote an article for "The Jewish Journal of Greater Los Angeles" imploring fellow Jewish Americans to vote for Romney.

In an essay for "Newsweek", published on January 29, 2013, Mamet argued against gun control laws: "It was intended to guard us against this inevitable decay of government that the Constitution was written. Its purpose was and is not to enthrone a Government superior to an imperfect and confused electorate, but to protect us from such a government."

Mamet has described the NFL anthem protests as "absolutely fucking despicable". In a 2020 interview, he described Donald Trump as a "great president" and supported his re-election. After Trump lost the election, Mamet appeared to endorse claims that the election had been illegitimate in his 2022 book "Recessional: The Death of Free Speech and the Cost of a Free Lunch", though shortly after its publication, he said he "misspoke" on the subject.

In 2022, Mamet made comments in support of Florida's Parental Rights in Education Act, called the "Don't Say Gay" bill by its critics, which restricts what public school teachers in Florida can discuss with children in kindergarten through third grade about sexual orientation and gender identity. In an interview with Fox News, Mamet claimed that the law was necessary because teachers "are abusing [children] mentally and using sex to do so", further alleging that "teachers are inclined, particularly men because men are predators, to pedophilia".

Films

Short films

Television

Acting roles

Unrealized projects




Definable real number

Informally, a definable real number is a real number that can be uniquely specified by its description. The description may be expressed as a construction or as a formula of a formal language. For example, the positive square root of 2, formula_1, can be defined as the unique positive solution to the equation formula_2, and it can be constructed with a compass and straightedge.

Different choices of a formal language or its interpretation give rise to different notions of definability. Specific varieties of definable numbers include the constructible numbers of geometry, the algebraic numbers, and the computable numbers. Because formal languages can have only countably many formulas, every notion of definable numbers has at most countably many definable real numbers. However, by Cantor's diagonal argument, there are uncountably many real numbers, so almost every real number is undefinable.

One way of specifying a real number uses geometric techniques. A real number formula_3 is a constructible number if there is a method to construct a line segment of length formula_3 using a compass and straightedge, beginning with a fixed line segment of length 1.

Each positive integer, and each positive rational number, is constructible. The positive square root of 2 is constructible. However, the cube root of 2 is not constructible; this is related to the impossibility of doubling the cube.

A real number formula_3 is called a real algebraic number if there is a polynomial formula_6, with only integer coefficients, so that formula_3 is a root of formula_8, that is, formula_9. 
Each real algebraic number can be defined individually using the order relation on the reals. For example, if a polynomial formula_10 has 5 real roots, the third one can be defined as the unique formula_3 such that formula_12 and such that there are two distinct numbers less than formula_3 at which formula_14 is zero.

All rational numbers are constructible, and all constructible numbers are algebraic. There are numbers such as the cube root of 2 which are algebraic but not constructible.

The real algebraic numbers form a subfield of the real numbers. This means that 0 and 1 are algebraic numbers and, moreover, if formula_15 and formula_16 are algebraic numbers, then so are formula_17, formula_18, formula_19 and, if formula_16 is nonzero, formula_21.

The real algebraic numbers also have the property, which goes beyond being a subfield of the reals, that for each positive integer formula_22 and each real algebraic number formula_15, all of the formula_22th roots of formula_15 that are real numbers are also algebraic.

There are only countably many algebraic numbers, but there are uncountably many real numbers, so in the sense of cardinality most real numbers are not algebraic. This nonconstructive proof that not all real numbers are algebraic was first published by
Georg Cantor in his 1874 paper "On a Property of the Collection of All Real Algebraic Numbers".

Non-algebraic numbers are called transcendental numbers. The best known transcendental numbers are and .

A real number is a computable number if there is an algorithm that, given a natural number formula_22, produces a decimal expansion for the number accurate to formula_22 decimal places. This notion was introduced by Alan Turing in 1936.

The computable numbers include the algebraic numbers along with many transcendental numbers including formula_28 Like the algebraic numbers, the computable numbers also form a subfield of the real numbers, and the positive computable numbers are closed under taking formula_22th roots for each 

Not all real numbers are computable. Specific examples of noncomputable real numbers include the limits of Specker sequences, and algorithmically random real numbers such as Chaitin's Ω numbers.

Another notion of definability comes from the formal theories of arithmetic, such as Peano arithmetic. The language of arithmetic has symbols for 0, 1, the successor operation, addition, and multiplication, intended to be interpreted in the usual way over the natural numbers. Because no variables of this language range over the real numbers, a different sort of definability is needed to refer to real numbers. A real number formula_15 is "definable in the language of arithmetic" (or "arithmetical") if its Dedekind cut can be defined as a predicate in that language; that is, if there is a first-order formula formula_31 in the language of arithmetic, with three free variables, such that
formula_32 of the arithmetical hierarchy, one of the lowest levels. Similarly, the reals with arithmetical Dedekind cuts form the lowest level of the analytical hierarchy.

A real number formula_15 is first-order definable in the language of set theory, without parameters, if there is a formula formula_31 in the language of set theory, with one free variable, such that formula_15 is the unique real number such that formula_36 holds. This notion cannot be expressed as a formula in the language of set theory.

All analytical numbers, and in particular all computable numbers, are definable in the language of set theory. Thus the real numbers definable in the language of set theory include all familiar real numbers such as 0, 1, formula_28, formula_38, et cetera, along with all algebraic numbers. Assuming that they form a set in the model, the real numbers definable in the language of set theory over a particular model of ZFC form a field. 
Each set model formula_39 of ZFC set theory that contains uncountably many real numbers must contain real numbers that are not definable within formula_39 (without parameters). This follows from the fact that there are only countably many formulas, and so only countably many elements of formula_39 can be definable over formula_39. Thus, if formula_39 has uncountably many real numbers, one can prove from "outside" formula_39 that not every real number of formula_39 is definable over formula_39. 
This argument becomes more problematic if it is applied to class models of ZFC, such as the von Neumann universe. The assertion "the real number formula_47 is definable over the "class" model formula_48" cannot be expressed as a formula of ZFC. Similarly, the question of whether the von Neumann universe contains real numbers that it cannot define cannot be expressed as a sentence in the language of ZFC. Moreover, there are countable models of ZFC in which all real numbers, all sets of real numbers, functions on the reals, etc. are definable.


Diego de Almagro

Diego de Almagro (; – July 8, 1538), also known as El Adelantado and El Viejo, was a Spanish conquistador known for his exploits in western South America. He participated with Francisco Pizarro in the Spanish conquest of Peru. While subduing the Inca Empire he laid the foundation for Quito and Trujillo as Spanish cities in present-day Ecuador and Peru respectively. From Peru, Almagro led the first Spanish military expedition to central Chile. Back in Peru, a longstanding conflict with Pizarro over the control of the former Inca capital of Cuzco erupted into a civil war between the two bands of conquistadores. In the battle of Las Salinas in 1538, Almagro was defeated by the Pizarro brothers and months later he was executed.

The origins of Diego de Almagro were humble. He was born in 1475 in the village of Almagro or in Malagón, in Ciudad Real, where he was given the name of the village for his surname as he was the illegitimate son of Juan de Montenegro and Elvira Gutiérrez. In order to preserve the honor of his mother, her relatives took the infant Diego and took him to the nearby town of Bolaños de Calatrava, where he was raised by Sancha López del Peral, later moving to Aldea del Rey.

At the age of four he returned to Almagro, and was placed under the tutelage of an uncle named Hernán Gutiérrez. At age fifteen he ran away from home because of his uncle's harshness. He went to the home of his mother, who was now living with her new husband, to tell her what had happened and that he was going to travel the world, and asked for some bread. His mother, anguished, gave him a piece of bread and some coins and said: ""Take, son, and do not give me more trouble, and go, and God help you in your adventure.""

He went to Seville and after probably stealing to survive, Almagro became a "criado" or servant of Don Luis Gonzalez de Polanco, one of the four "Alcaldes de la Casa y Corte de Su Majestad" and later a Counselor of the Catholic Monarchs. While living in Seville, Almagro stabbed another servant in an argument, inflicting serious enough injuries that he was to be tried in court.

Don Luis, using his influence, prevailed upon Don Pedro Arias Dávila to allow Almagro to embark in one of the ships going to the New World from the port of Sanlucar de Barrameda. The Casa de Contratacion (royal agency for the Spanish Empire) required that the men who crossed the Atlantic provide their own weapons, clothes, and farming tools, which Don Polanco provided to his servant.

Diego de Almagro, now in his late thirties, arrived in the New World on June 30, 1514, with the expedition that Ferdinand II of Aragon had sent under the leadership of Dávila. The expedition arrived at the city of Santa María la Antigua del Darién, Panama, where many other future conquistadors were already assembled, among them Francisco Pizarro.

There are not many details of Almagro's activities during this period, but it is known that he accompanied various sailors who departed from Darien between 1514 and 1515. He eventually returned and settled in Darien, where he was granted an encomienda, building a house and making a living from agriculture.

Almagro undertook his first independent conquest on November 1515, commanding 260 men as he founded Villa del Acla, named after the Indian place. Due to illness he had to hand over command to Gaspar de Espinosa.

Espinosa decided to undertake a new expedition, which departed in December 1515 with 200 men, including Almagro and Francisco Pizarro, who for the first time was designated as a captain. During this expedition, which lasted 14 months, Almagro, Pizarro and Hernando de Luque became close friends.

Also during this time Almagro established a friendship with Vasco Núñez de Balboa, who was in charge of Acla. Almagro wanted to have a ship built with the remaining materials of the Espinosa expedition, to be finished on the coast of the "Great South Sea", as the Pacific Ocean was first called by the Spanish. Current historians do not believe that Almagro was expected to participate in Balboa's expedition and probably returned to Darien.

Almagro took part in the various expeditions that took place in the Gulf of Panama, including those of Espinosa, which were supported by Balboa's ships. Almagro was recorded as a witness on the lists of natives whom Espinosa ordered to be carried. He remained as an early settler in the newly founded city of Panama, staying there for four years, managing his properties and those of Pizarro. He took Ana Martínez, an indigenous woman, as a common-law wife. In this period, his first son, "El Mozo", was born to them.

By 1524 an association of conquest regarding South America was formalized among De Almagro, Pizarro and Luque. By the beginning of August 1524, they had received the requisite permission to discover and conquer lands further south. In the first expedition, De Almagro lost his eye to an arrow shot at the Battle of Punta Quemada. He subsequently remained in Panama to recruit men and gather supplies for the expeditions led by Pizarro.

After several expeditions to South America, Pizarro secured his stay in Peru with the "Capitulation" on 6 July 1529. During Pizarro's continued exploration of Incan territory, he and his men succeeded in defeating the Inca army under Emperor Atahualpa during the Battle of Cajamarca in 1532. De Almagro joined Pizarro soon afterward, bringing more men and arms.

After Peru fell to the Spanish, both Pizarro and De Almagro initially worked together in the founding of new cities to consolidate their dominions. As such, Pizarro dispatched De Almagro to pursue Quizquiz, fleeing to the Inca Empire's northern city of Quito. Their fellow conquistador Sebastián de Belalcázar, who had gone forth without Pizarro's approval, had already reached Quito and witnessed the destruction of the city by Inca general Rumiñawi. The Inca warrior had ordered the city to be burned and its gold to be buried at an undisclosed location where the Spanish could never find it. The arrival of Pedro de Alvarado from Guatemala, in search of Inca gold further complicated the situation for Almagro and Belalcázar. Alvarado's presence, however, did not last long as he left South America in exchange for monetary compensation from Pizarro.

In an attempt to claim Quito ahead of Belalcázar, in August 1534 De Almagro founded a city on the shores of Laguna de Colta (Colta Lake) in the foothills of Chimborazo, some south of present-day Quito, and named it "Santiago de Quito." Four months later would come the foundation of the Peruvian city of Trujillo, which Almagro named as "Villa Trujillo de Nueva Castilla" (the Village of Trujillo in New Castille) in honor of Francisco Pizarro's birthplace, Trujillo in Extremadura, Spain. These events were the height of the Pizarro-Almagro friendship, which historians describe as one of the last events in which their friendship soon faded and entered a period of turmoil for the control of the Incan capital of Cuzco.

After splitting the treasure of Inca emperor Atahualpa, both Pizarro and Almagro left towards Cuzco and took the city in 1533. However, De Almagro's friendship with Pizarro showed signs of deterioration in 1526 when Pizarro, in the name of the rest of the conquistadors, called forth the "Capitulacion de Toledo" law in which King Charles I of Spain had laid out his authorization for the conquest of Peru and the awards every conquistador would receive from it. Long before, however, each conquistador had promised to equally split the benefits. Pizarro managed to have a larger stake and awards for himself. Despite this, De Almagro still obtained an important fortune for his services, and the King awarded him in November 1532 the noble title of "Don" and he was assigned a personal coat of arms.

Although by this time Diego de Almagro had already acquired sufficient wealth in the conquest of Peru and was living a luxurious life in Cuzco, the prospect of conquering the lands further south was very attractive to him. Given that the dispute with Pizarro over Cuzco had kept intensifying, Almagro spent a great deal of time and money equipping a company of 500 men for a new exploration south of Peru.

By 1534 the Spanish crown had determined to split the region in two parallel lines, forming the governorship of "Nueva Castilla" (from the 1° to the 14° latitude, close to Pisco), and that of "Nueva Toledo" (from the 14° to the 25° latitude, in Taltal, Chile), assigning the first to Francisco Pizarro and the second to Diego de Almagro. The crown had previously assigned Almagro the governorship of Cuzco, and as such De Almagro was heading there when Charles V divided the territory between Nueva Castilla and Nuevo Toledo. This might have been the reason why Almagro did not immediately confront Pizarro for Cuzco, and promptly decided to embark on his new quest for the discovery of the riches of Chile.

Charles V had given Diego a grant extending two hundred leagues south of Francisco Pizarro's. Francisco and Diego concluded a new contract on 12 June 1535, in which they agreed to share future discoveries equally. Diego raised an expedition for Chile, expecting it "would lead to even greater riches than they had found in Peru." Almagro prepared the way by sending ahead three of his Spanish soldiers, the religious chief of the Inca empire, Willaq Umu, and Paullo Topa, brother of Manco Inca Yupanqui. De Almagro sent Juan de Saavedra forward with one hundred and fifty men, and soon followed them with additional forces. Saavedra established on January 23, 1535, the first Spanish settlement in Bolivia near the Inca regional capital of Paria.

Almagro left Cuzco on July 3, 1535, with his supporters and stopped at Moina until the 20th of that month. Meanwhile, Francisco Pizarro's brother, Juan Pizarro, had arrested Inca Manco Inca Yupanqui, further complicating De Almagro's plans as it heavily increased the dissatisfaction of the Indians submitted to Spanish rule. Not having formally been appointed governor of any territories in the Capitulation of Toledo in 1528, however, forcing him to declare himself "adelantado" (governor) of Nueva Toledo, or southern Peru and present-day Chile. Some sources suggest Almagro received such a requirement in 1534 by the Spanish king and was officially declared governor of New Toledo.

Once he left Moina, De Almagro followed the Inca trail followed by 750 Spaniards deciding to join him in quest for the gold lost in the ransom of Atahualpa, which had mainly benefited the Pizarro brothers and their supporters. After crossing the Bolivian mountain range and traveling past Lake Titicaca, Almagro arrived on the shores of the Desaguadero River and finally set up camp in Tupiza. From there, the expedition stopped at Chicoana and then turned to the southeast to cross the Andes mountains.

The expedition turned out to be a difficult and exhausting endeavor. The hardest phase was the crossing of the Andean cordilleras: the cold, hunger and tiredness meant the death of various Spanish and natives, but mainly slaves who were not accustomed to such rigorous climate.

Upon this point, De Almagro determined everything was a failure. He ordered a small group under Rodrigo Orgóñez on a reconnaissance of the country to the south.

By luck, these men found the Valley of Copiapó, where Gonzalo Calvo Barrientos, a Spanish soldier whom Pizarro had expelled from Peru for stealing objects the Inca had offered for his ransom, had already established a friendship with the local natives. There, in the valley of the river Copiapó, Almagro took official possession of Chile and claimed it in the name of King Charles V.

De Almagro promptly initiated the exploration of the new territory, starting up the valley the Aconcagua River, where he was well received by the natives. However, the intrigues of his interpreter, Felipillo, who had previously helped Pizarro in dealing with "Atahualpa", almost thwarted De Almagro's efforts. Felipillo had secretly urged the local natives to attack the Spanish, but they desisted, not understanding the dangers that they posed. De Almagro directed Gómez de Alvarado along with 100 horsemen and 100 foot to continue the exploration, which ended in the confluence of the Ñuble and Itata rivers. The Battle of Reinohuelén between the Spanish and Mapuche indigenous peoples forced the explorers to return to the north.

De Almagro's own reconnaissance of the land and the bad news of Gómez de Alvarado's encounter with the fierce Mapuche, along with the bitter cold winter that settled ferociously upon them, only served to confirm that everything had failed. He never found gold or the cities which Incan scouts had told him lay ahead, only communities of the indigenous population who lived from subsistence agriculture. Local tribes put up fierce resistance to the Spanish forces. The exploration of the territories of Nueva Toledo, which lasted 2 years, was marked by a complete failure for De Almagro. Despite this, at first he thought staying and founding a city would serve well for his honor. The initial optimism that led Almagro to bring his son he had with the indigenous Panamanian Ana Martínez to Chile had faded.

Some historians have suggested that, but for the urging of his senior explorers, De Almagro would probably have stayed permanently in Chile. He was urged to return to Peru and this time take definitive possession of Cuzco, so as to consolidate an inheritance for his son. Dismayed with his experience in the south, Almagro made plans of return to Peru. He never officially founded a city in the territory of what is now Chile.

The withdrawal of the Spanish from valleys of Chile was violent: Almagro authorized his soldiers to ransack the natives' properties, leaving their soil desolate. In addition, the Spanish soldiers took natives captive to serve as slaves. The locals were captured, tied together, and forced to carry the heavy loads belonging to the conquistadors.

After the exhausting crossing of the Atacama Desert, mainly due to the harsh weather conditions, Almagro finally reached Cuzco, Peru, in 1537. According to some authors, it was during this time that the Spanish term ""roto"" (torn), used by Peruvians to refer to Chileans, was first coined. De Almagro's disappointed troops returned to Cuzco with their "torn clothes" due to the extensive and laborious passage on foot by the Atacama Desert.

After his return, De Almagro was surprised to learn of the Inca Manco's rebellion. Diego de Almagro sent an embassy to the Inca, but they mistrusted all of the Spaniards by this time. Hernando Pizarro's men formed an uneasy truce with De Almagro's men, surveying to determine the boundaries of their leaders' royal grants. They needed to determine in which portion the city of Cuzco was located. However, De Almagro's troops quickly took the city and imprisoned the Pizarro brothers, Hernando and Gonzalo, on the night of 8 April 1537.

After occupying Cuzco, De Almagro confronted an army sent by Francisco Pizarro to liberate his brothers. Alonso de Alvarado commanded it and was defeated during the Battle of Abancay on July 12, 1537. He and some of his men were imprisoned. Later, Gonzalo Pizarro and De Alvarado escaped prison. Subsequent negotiations between Francisco Pizarro and De Almagro concluded with the liberation of Hernando, the third Pizarro brother, in return for conceding control and administration of Cuzco to De Almagro. Pizarro never intended to give up the city permanently, but was buying time to organize an army strong enough to defeat Almagro's troops.

During this time De Almagro fell ill, and Pizarro and his brothers grabbed the opportunity to defeat him and his followers. The Almagristas were defeated at Las Salinas in April 1538, with Orgóñez being killed on the field of battle. De Almagro fled to Cuzco, still in the hands of his loyal supporters, but found only temporary refuge; the forces of the Pizarro brothers entered the city without resistance. Once captured, Almagro was humiliated by Hernando Pizarro and his requests to appeal to the King were ignored.

When Diego de Almagro begged for his life, Hernando responded:

"-he was surprised to see Almagro demean himself in a manner so unbecoming a brave cavalier, that his fate was no worse than had befallen many a soldier before him; and that, since God had given him the grace to be a Christian, he should employ his remaining moments in making up his account with Heaven!"

Almagro was condemned to death and executed by "garrote" in his dungeon, and then decapitated, on July 8, 1538. His corpse was taken to the public Plaza Mayor of Cuzco, where a herald proclaimed his crimes. Hernán Ponce de León took his body and buried him in the church of Our Lady of Mercy in Cuzco.

Diego de Almagro II (1520–1542), known as "El Mozo" (The Lad), son of Diego de Almagro I, whose mother was an Indian girl of Panama, became the foil of the conspirators who had put Pizarro to the sword. Pizarro was murdered on June 26, 1541; the conspirators promptly proclaimed the lad De Almagro Governor of Peru. From various causes, all of the conspirators either died or were killed except for one, who was executed after the lad Almagro gave an order. The lad De Almagro fought the desperate battle of Chupas on September 16, 1542, escaped to Cuzco, but was arrested, immediately condemned to death, and executed in the great square of the city.




Divinity

Divinity or the divine are things that are either related to, devoted to, or proceeding from a deity. What is or is not divine may be loosely defined, as it is used by different belief systems. Under monotheism and polytheism this is clearly delineated. However, in pantheism and animism this becomes synonymous with concepts of sacredness and transcendence.

The root of the word "divinity" is the Latin "divus" meaning of or belonging to a God ("deus"). The word entered English from Medieval Latin in the 14th century.

Divinity as a quality has two distinct usages:

Overlap occurs between these usages because deities or godly entities are often identical with or identified by the powers and forces that are credited to them — in many cases, a deity is merely a power or force personified — and these powers and forces may then be extended or granted to mortal individuals. For instance, Jehovah is closely associated with storms and thunder throughout much of the Old Testament. He is said to speak in thunder, and thunder is seen as a token of his anger. This power was then extended to prophets like Moses and Samuel, who caused thunderous storms to rain down on their enemies. Divinity always carries connotations of goodness, beauty, beneficence, justice, and other positive, pro-social attributes. In monotheistic faiths there is an equivalent cohort of malefic supernatural beings and powers, such as demons, devils, afreet, etc., which are not conventionally referred to as divine; "demonic" is often used instead. Polytheistic and animistic systems of belief make no such distinction; gods and other beings of transcendent power often have complex, ignoble, or even incomprehensible motivations for their acts. Note that while the terms "demon" and "demonic" are used in monotheistic faiths as antonyms to "divine", they are in fact derived from the Greek word "daimón" (δαίμων), which itself translates as "divinity".

There are three distinct usages of "divinity" and "divine" in religious discourse:

In monotheistic faiths, the word "divinity" is often used to refer to the singular God central to that faith. Often the word takes the definite article and is capitalized — ""the Divinity"" — as though it were a proper name or definitive honorific. 
"Divine" — capitalized — may be used as an adjective to refer to the manifestations of such a Divinity or its powers: e.g. "basking in the Divine presence..."

The terms "divinity" and "divine" — uncapitalized, and lacking the definite article — are sometimes used to denote 'god(s) or certain other beings and entities which fall short of absolute Godhood but lie outside the human realm.

As previously noted, divinities are closely related to the transcendent force(s) or power(s) credited to them, so much so that in some cases the powers or forces may themselves be invoked independently. This leads to the second usage of the word "divine" (and less common usage of "divinity"): to refer to the operation of transcendent power in the world.

In its most direct form, the operation of transcendent power implies some form of divine intervention. For monotheistic and polytheistic faiths this usually implies the direct action of one god or another on the course of human events. In Greek legend, for instance, it was Poseidon (god of the sea) who raised the storms that blew Odysseus's craft off course on his return journey, and Japanese tradition holds that a god-sent wind saved them from Mongol invasion. Prayers or propitiations are often offered to specific gods to garner favorable interventions in particular enterprises: e.g. safe journeys, success in war, or a season of bountiful crops. Many faiths around the world — from Japanese Shinto and Chinese traditional religion, to certain African practices and the faiths derived from those in the Caribbean, to Native American beliefs — hold that ancestral or household deities offer daily protection and blessings. In monotheistic religions, divine intervention may take very direct forms: miracles, visions, or intercessions by blessed figures.

Transcendent force or power may also operate through more subtle and indirect paths. Monotheistic faiths generally support some version of divine providence, which acknowledges that the divinity of the faith has a profound but unknowable plan always unfolding in the world. Unforeseeable, overwhelming, or seemingly unjust events are often thrown on 'the will of the Divine', in deferences like the Muslim "inshallah" ('as God wills it') and Christian 'God works in mysterious ways'. Often such faiths hold out the possibility of divine retribution as well, where the divinity will unexpectedly bring evil-doers to justice through the conventional workings of the world; from the subtle redressing of minor personal wrongs to such large-scale havoc as the destruction of Sodom and Gomorrah or the biblical Great Flood. Other faiths are even more subtle: the doctrine of "karma" shared by Buddhism and Hinduism is a divine law similar to divine retribution but without the connotation of punishment: our acts, good or bad, intentional or unintentional, reflect back on us as part of the natural working of the universe. Philosophical Taoism also proposes a transcendent operant principle — transliterated in English as "tao" or "dao", meaning 'the way' — which is neither an entity nor a being per se, but reflects the natural ongoing process of the world. Modern western mysticism and new age philosophy often use the term 'the Divine' as a noun in this latter sense: a non-specific principle or being that gives rise to the world, and acts as the source or wellspring of life. In these latter cases, the faiths do not promote deference, as happens in monotheisms; rather each suggests a path of action that will bring the practitioner into conformance with the divine law: "ahimsa" — 'no harm' — for Buddhist and Hindu faiths; "de" or "te" — 'virtuous action' — in Taoism; and any of numerous practices of peace and love in new age thinking.

In the third usage, extensions of divinity and divine power are credited to living, mortal individuals. Political leaders are known to have claimed actual divinity in certain early societies — the ancient Egyptian Pharaohs being the premier case — taking a role as objects of worship and being credited with superhuman status and powers. More commonly, and more pertinent to recent history, leaders merely claim some form of divine mandate, suggesting that their rule is in accordance with the will of God. The doctrine of the divine right of kings was introduced as late as the 17th century, proposing that kings rule by divine decree; Japanese Emperors ruled by divine mandate until the inception of the Japanese constitution after World War II.

Less politically, most faiths have any number of people that are believed to have been touched by divine forces: saints, prophets, heroes, oracles, martyrs, and enlightened beings, among others. Saint Francis of Assisi, in Catholicism, is said to have received instruction directly from God and it is believed that he grants plenary indulgence to all who confess their sins and visit his chapel on the appropriate day. In Greek mythology, Achilles' mother bathed him in the river Styx to give him immortality, and Hercules — as the son of Zeus — inherited near-godly powers. In religious Taoism, Laozi is venerated as a saint with his own powers. Various individuals in the Buddhist faith, beginning with Siddhartha, are considered to be enlightened, and in religious forms of Buddhism they are credited with divine powers. Christ in the Bible is said to be God's Son and is said to have performed divine miracles.

In general, mortals with divine qualities are carefully distinguished from the deity or deities in their religion's main pantheon. Even the Christian faith, which generally holds Christ to be identical to God, distinguishes between God the Father and Christ the begotten Son. There are, however, certain esoteric and mystical schools of thought, present in many faiths — Sufis in Islam, Gnostics in Christianity, Advaitan Hindus, Zen Buddhists, as well as several non-specific perspectives developed in new age philosophy — which hold that all humans are in essence divine, or unified with the Divine in a non-trivial way. Such divinity, in these faiths, would express itself naturally if it were not obscured by the social and physical worlds we live in; it needs to be brought to the fore through appropriate spiritual practices.

In the New Testament the Greek word θεῖον ("theion") in the Douay Version, is translated as "divinity". Examples are below:

The word translated as either "deity", "Godhead", or "divinity" in the Greek New Testament is also the Greek word θεότητος ("theotētos"), and the one verse that contains it is this:
Colossians 2:9

The word "divine" in the New Testament is the Greek word θείας ("theias"), and is the adjective form of "divinity". Biblical examples from the King James Bible are below:

The most prominent conception of divine entities in the Church of Jesus Christ of Latter-day Saints (LDS Church) is the Godhead, a divine council of three distinct beings: Elohim (the Father), Jehovah (the Son, or Jesus), and the Holy Spirit. Joseph Smith described a nontrinitarian Godhead, with God the Father and Jesus Christ each having individual physical bodies, and the Holy Spirit as a distinct personage with a spirit body. Smith also introduced the existence of a Heavenly Mother in the King Follett Discourse, but very little is acknowledged or known beyond her existence.

Mormons hold a belief in the divine potential of humanity; Smith taught a form of divinization where mortal men and women can become like god through salvation and exaltation. Lorenzo Snow succinctly summarized this using a couplet, which is often repeated within the LDS Church: "As man now is, God once was: As God now is, man may be."

Wiccan views of divinity are generally theistic, and revolve around a Goddess and a Horned God, thereby being generally dualistic. In traditional Wicca, as expressed in the writings of Gerald Gardner and Doreen Valiente, the emphasis is on the theme of divine gender polarity, and the God and Goddess are regarded as equal and opposite divine cosmic forces. In some newer forms of Wicca, such as feminist or Dianic Wicca, the Goddess is given primacy or even exclusivity. In some forms of traditional witchcraft that share a similar duotheistic theology, the Horned God is given precedence over the Goddess.

Epicurean philosophy admits the existence of gods, but since it does not accept the supernatural and teaches that all things are material, posits a theology where the Epicurean gods are physical beings whose bodies are made of atoms and who live in the region between the words (intermundia). Needless to say, these gods do not need our worship, are not creators or maintainers of the cosmos, nor do they answer prayers. Therefore, Epicurean theology belongs properly in the realm of speculation about super-evolved, intelligent extraterrestrial life.

However, Epicurus of Samos (the founder of the School) recognized the utility of religiosity and its central, unifying symbols. He was adamant in his requirement that his disciples be pious, and established two taboos concerning their conception of the gods: they had to believe that their gods were immortal (that is, indestructible and fully self-sufficient) and blessed (happy, or blissful). Outside of that, Epicureans are free to speculate concerning the nature of the highest life forms in the cosmos.


Depth of field

The depth of field (DOF) is the distance between the nearest and the furthest objects that are in acceptably sharp focus in an image captured with a camera.

For cameras that can only focus on one object distance at a time, depth of field is the distance between the nearest and the farthest objects that are in acceptably sharp focus in the image. "Acceptably sharp focus" is defined using a property called the "circle of confusion".

The depth of field can be determined by focal length, distance to subject (object to be imaged), the acceptable circle of confusion size, and aperture. Limitations of depth of field can sometimes be overcome with various techniques and equipment. The approximate depth of field can be given by:

formula_1

for a given maximum acceptable circle of confusion , focal length , f-number , and distance to subject .

As distance or the size of the acceptable circle of confusion increases, the depth of field increases; however, increasing the size of the aperture (i.e., reducing ) or increasing the focal length reduces the depth of field. Depth of field changes linearly with and circle of confusion, but changes in proportion to the square of the distance to the subject and inversely in proportion to the square of the focal length. As a result, photos taken at extremely close range (i.e., so small ) have a proportionally much smaller depth of field.

Rearranging the equation shows that it is the ratio between distance and focal length that affects ;

formula_2

Note that formula_3 is the transverse magnification which is the ratio of the lateral image size to the lateral subject size.

Image sensor size affects in counterintuitive ways. Because the circle of confusion is directly tied to the sensor size, decreasing the size of the sensor while holding focal length and aperture constant will the depth of field (by the crop factor). The resulting image however will have a different field of view. If the focal length is altered to maintain the field of view, the change in focal length will counter the decrease of from the smaller sensor and the depth of field (also by the crop factor).

For a given subject framing and camera position, the is controlled by the lens aperture diameter, which is usually specified as the f-number (the ratio of lens focal length to aperture diameter). Reducing the aperture diameter (increasing the ) increases the because only the light travelling at shallower angles passes through the aperture so only cones of rays with shallower angles reach the image plane. In other words, the circles of confusion are reduced or increasing the .

For a given size of the subject's image in the focal plane, the same on any focal length lens will give the same depth of field. This is evident from the above equation by noting that the ratio is constant for constant image size. For example, if the focal length is doubled, the subject distance is also doubled to keep the subject image size the same. This observation contrasts with the common notion that "focal length is twice as important to defocus as f/stop", which applies to a constant subject distance, as opposed to constant image size.

Motion pictures make limited use of aperture control; to produce a consistent image quality from shot to shot, cinematographers usually choose a single aperture setting for interiors (e.g., scenes inside a building) and another for exteriors (e.g., scenes in an area outside a building), and adjust exposure through the use of camera filters or light levels. Aperture settings are adjusted more frequently in still photography, where variations in depth of field are used to produce a variety of special effects.

Precise focus is only possible at an exact distance from a lens; at that distance, a point object will produce a small spot image. Otherwise, a point object will produce a larger or blur spot image that is typically and approximately a circle. When this circular spot is sufficiently small, it is visually indistinguishable from a point, and appears to be in focus. The diameter of the largest circle that is indistinguishable from a point is known as the acceptable circle of confusion, or informally, simply as the circle of confusion.

The acceptable circle of confusion depends on how the final image will be used. The circle of confusion as 0.25 mm for an image viewed from 25 cm away is generally accepted.

For 35mm motion pictures, the image area on the film is roughly 22 mm by 16 mm. The limit of tolerable error was traditionally set at diameter, while for 16 mm film, where the size is about half as large, the tolerance is stricter, . More modern practice for 35 mm productions set the circle of confusion limit at .

The term "camera movements" refers to swivel (swing and tilt, in modern terminology) and shift adjustments of the lens holder and the film holder. These features have been in use since the 1800s and are still in use today on view cameras, technical cameras, cameras with tilt/shift or perspective control lenses, etc. Swiveling the lens or sensor causes the plane of focus (POF) to swivel, and also causes the field of acceptable focus to swivel with the ; and depending on the criteria, to also change the shape of the field of acceptable focus. While calculations for of cameras with swivel set to zero have been discussed, formulated, and documented since before the 1940s, documenting calculations for cameras with non-zero swivel seem to have begun in 1990.

More so than in the case of the zero swivel camera, there are various methods to form criteria and set up calculations for when swivel is non-zero. There is a gradual reduction of clarity in objects as they move away from the , and at some virtual flat or curved surface the reduced clarity becomes unacceptable. Some photographers do calculations or use tables, some use markings on their equipment, some judge by previewing the image.

When the is rotated, the near and far limits of may be thought of as wedge-shaped, with the apex of the wedge nearest the camera; or they may be thought of as parallel to the .

Traditional depth-of-field formulas can be hard to use in practice. As an alternative, the same effective calculation can be done without regard to the focal length and . Moritz von Rohr and later Merklinger observe that the effective absolute aperture diameter can be used for similar formula in certain circumstances.

Moreover, traditional depth-of-field formulas assume equal acceptable circles of confusion for near and far objects. Merklinger suggested that distant objects often need to be much sharper to be clearly recognizable, whereas closer objects, being larger on the film, do not need to be so sharp. The loss of detail in distant objects may be particularly noticeable with extreme enlargements. Achieving this additional sharpness in distant objects usually requires focusing beyond the hyperfocal distance, sometimes almost at infinity. For example, if photographing a cityscape with a traffic bollard in the foreground, this approach, termed the "object field method" by Merklinger, would recommend focusing very close to infinity, and stopping down to make the bollard sharp enough. With this approach, foreground objects cannot always be made perfectly sharp, but the loss of sharpness in near objects may be acceptable if recognizability of distant objects is paramount.

Other authors such as Ansel Adams have taken the opposite position, maintaining that slight unsharpness in foreground objects is usually more disturbing than slight unsharpness in distant parts of a scene.

Some methods and equipment allow altering the apparent , and some even allow the to be determined after the image is made. These are based or supported by computational imaging processes. For example, focus stacking combines multiple images focused on different planes, resulting in an image with a greater (or less, if so desired) apparent depth of field than any of the individual source images. Similarly, in order to reconstruct the 3-dimensional shape of an object, a depth map can be generated from multiple photographs with different depths of field. Xiong and Shafer concluded, in part, "...the improvements on precisions of focus ranging and defocus ranging can lead to efficient shape recovery methods."

Another approach is focus sweep. The focal plane is swept across the entire relevant range during a single exposure. This creates a blurred image, but with a convolution kernel that is nearly independent of object depth, so that the blur is almost entirely removed after computational deconvolution. This has the added benefit of dramatically reducing motion blur.

Other technologies use a combination of lens design and post-processing: Wavefront coding is a method by which controlled aberrations are added to the optical system so that the focus and depth of field can be improved later in the process.

The lens design can be changed even more: in colour apodization the lens is modified such that each colour channel has a different lens aperture. For example, the red channel may be , green may be , whilst the blue channel may be . Therefore, the blue channel will have a greater depth of field than the other colours. The image processing identifies blurred regions in the red and green channels and in these regions copies the sharper edge data from the blue channel. The result is an image that combines the best features from the different .

At the extreme, a plenoptic camera captures 4D light field information about a scene, so the focus and depth of field can be altered after the photo is taken.

Diffraction causes images to lose sharpness at high (i.e., narrow aperture stop opening sizes), and hence limits the potential depth of field. (This effect is not considered in the above formula giving approximate values.) In general photography this is rarely an issue; because large typically require long exposure times to acquire acceptable image brightness, motion blur may cause greater loss of sharpness than the loss from diffraction. However, diffraction is a greater issue in close-up photography, and the overall image sharpness can be degraded as photographers are trying to maximize depth of field with very small apertures.

Hansma and Peterson have discussed determining the combined effects of defocus and diffraction using a root-square combination of the individual blur spots. Hansma's approach determines the that will give the maximum possible sharpness; Peterson's approach determines the minimum that will give the desired sharpness in the final image and yields a maximum depth of field for which the desired sharpness can be achieved. In combination, the two methods can be regarded as giving a maximum and minimum for a given situation, with the photographer free to choose any value within the range, as conditions (e.g., potential motion blur) permit. Gibson gives a similar discussion, additionally considering blurring effects of camera lens aberrations, enlarging lens diffraction and aberrations, the negative emulsion, and the printing paper. Couzin gave a formula essentially the same as Hansma's for optimal , but did not discuss its derivation.

Hopkins, Stokseth, and Williams and Becklund have discussed the combined effects using the modulation transfer function.

Many lenses include scales that indicate the for a given focus distance and ; the 35 mm lens in the image is typical. That lens includes distance scales in feet and meters; when a marked distance is set opposite the large white index mark, the focus is set to that distance. The scale below the distance scales includes markings on either side of the index that correspond to . When the lens is set to a given , the extends between the distances that align with the markings.

Photographers can use the lens scales to work backwards from the desired depth of field to find the necessary focus distance and aperture. For the 35 mm lens shown, if it were desired for the to extend from 1 m to 2 m, focus would be set so that index mark was centered between the marks for those distances, and the aperture would be set to .

On a view camera, the focus and can be obtained by measuring the depth of field and performing simple calculations. Some view cameras include calculators that indicate focus and without the need for any calculations by the photographer.

The beyond the subject is always greater than the in front of the subject. When the subject is at the hyperfocal distance or beyond, the far is infinite, so the ratio is 1:∞; as the subject distance decreases, near:far ratio increases, approaching unity at high magnification. For large apertures at typical portrait distances, the ratio is still close to 1:1.

This section covers some additional formula for evaluating depth of field; however they are all subject to significant simplifying assumptions: for example, they assume the paraxial approximation of Gaussian optics. They are suitable for practical photography, lens designers would use significantly more complex ones.

For given near and far limits and , the required f-number is smallest when focus is set to

formula_4

the harmonic mean of the near and far distances. In practice, this is equivalent to the arithmetic mean for shallow depths of field. Sometimes, view camera users refer to the difference as the "focus spread".

If a subject is at distance and the foreground or background is at distance , let the distance between the subject and the foreground or background be indicated by

formula_5

The blur disk diameter of a detail at distance from the subject can be expressed as a function of the subject magnification , focal length , f-number , or alternatively the aperture , according to

formula_6

The minus sign applies to a foreground object, and the plus sign applies to a background object.

The blur increases with the distance from the subject; when is less than the circle of confusion, the detail is within the depth of field.




Dumnonii

The Dumnonii or Dumnones were a British tribe who inhabited Dumnonia, the area now known as Cornwall and Devon (and some areas of present-day Dorset and Somerset) in the further parts of the South West peninsula of Britain, from at least the Iron Age up to the early Saxon period. They were bordered to the east by the Durotriges tribe.

William Camden, in his 1607 edition of "Britannia", describes Cornwall and Devon as being two parts of the same 'country' which:

Camden had learnt some Welsh during the course of his studies and it would appear that he is the origin of the interpretation of Dumnonii as "deep valley dwellers" from his understanding of the Welsh of his time. The modern Welsh term is "Dyfnaint". John Rhŷs later theorized that the tribal name was derived from the name of a goddess, "Domnu", probably meaning "the goddess of the deep". The proto-Celtic root *dubno- or *dumno- meaning "the deep" or "the earth" (or alternatively meaning "dark" or "gloomy") appears in personal names such as Dumnorix and Dubnovellaunus. Another group with a similar name but with no known links were the Fir Domnann of Connacht.

The Roman name of the town of Exeter, "Isca Dumnoniorum" ("Isca of the Dumnonii"), contains the root "*iska-" "water" for "Water of the Dumnonii". The Latin name suggests that the city was already an "oppidum", or walled town, on the banks on the River Exe before the foundation of the Roman city, in about AD 50. The Dumnonii gave their name to the English county of Devon, and their name is represented in Britain's two extant Brythonic languages as "Dewnans" in Cornish and "Dyfnaint" in Welsh. Amédée Thierry ("Histoire des Gaulois", 1828), one of the inventors of the "historic race" of Gauls, could confidently equate them with the Cornish ("les Cornouailles").

Victorian historians often referred to the tribe as the Damnonii, which is also the name of another people from lowland Scotland, although there are no known links between the two populations.

The people of Dumnonia spoke a Southwestern Brythonic dialect of Celtic similar to the forerunner of more recent Cornish and Breton. Irish immigrants, the Déisi, are evidenced by the Ogham-inscribed stones they have left behind, confirmed and supplemented by toponymical studies. The stones are sometimes inscribed in Latin, sometimes in both scripts. Tristram Risdon suggested the continuance of a Brythonic dialect in the South Hams, Devon, as late as the 14th century, in addition to its use in Cornwall.

Ptolemy's 2nd century "Geography" places the Dumnonii to the west of the Durotriges. The name "purocoronavium" that appears in the Ravenna Cosmography implies the existence of a sub-tribe called the Cornavii or Cornovii, perhaps the ancestors of the Cornish people.

Gaius Iulius Solinus, probably in the 3rd century, remarks: "This turbid strait also divides the island Silura from the shore which is held by the Dumnonii, a British tribe. The men of this island even now preserve an old custom: they do not use coins. They give and accept, obtaining the necessities of life by exchange rather than by money. They reverence gods, and the men and women equally declare knowledge of the future."

In the sub-Roman period a Brythonic kingdom called Dumnonia emerged, covering the entire peninsula, although it is believed by some to have effectively been a collection of sub-kingdoms.

A kingdom of Domnonée (and of Cornouaille alongside) was established in the province of Armorica directly across the English Channel, and has apparent links with the British population, suggesting an ancient connection of peoples along the western Atlantic seaboard which is also borne out by the modern genetics of Devon & Cornish populations

The Latin name for Exeter is Isca Dumnoniorum ("Water of the Dumnonii"). This oppidum (a Latin term meaning an important town) on the banks of River Exe certainly existed prior to the foundation of the Roman city in about AD 50. "Isca" is derived from the Brythonic word for flowing water, which was given to the River Exe. The Gaelic term for water is "uisce/uisge". This is reflected in the Welsh name for Exeter: "Caerwysg" meaning "fortified settlement on the river Uisc".

Isca Dumnoniorum originated with a settlement that developed around the Roman fortress of the Legio II Augusta and is one of the four "poleis" (cities) attributed to the tribe by Ptolemy. It is also listed in two routes of the late 2nd century Antonine Itinerary.

A legionary bath-house was built inside the fortress sometime between 55 and 60 and underwent renovation shortly afterwards (c. 60-65) but by c. 68 (perhaps even 66) the legion had transferred to a newer fortress at Gloucester. This saw the dismantling of the Isca fortress, and the site was then abandoned. Around AD 75, work on the "civitas forum" and "basilica" had commenced on the site of the former "principia" and by the late 2nd century the "civitas" walls had been completed. They were 3 metres thick and 6 metres high and enclosed exactly the same area as the earlier fortress. However, by the late 4th century the "civitas" was in decline.

As well as Isca Dumnoniorum, Ptolemy's 2nd century "Geography" names three other towns:

The Ravenna Cosmography includes the last two names (in slightly different forms, as "Tamaris" and "Uxelis"), and adds several more names which may be settlements in the territory. These include:

Other Romano-British sites in Dumnonia include:

New settlements continued to be built throughout the Roman period, including sites at Chysauster and Trevelgue Head. The style is native in form with no Romanised features. Near Padstow, a site of some importance that was inhabited from the late Bronze/early Iron Age to the mid 6th century now lies buried under the sands on the opposite side of the Camel estuary near St. Enodoc's Church, and may have been a western coastal equivalent of a Saxon Shore Fort. Byzantine and African pottery has been discovered at the site. At Magor Farm in Illogan, near Camborne, an archaeological site has been identified as being a villa.

The Dumnonii are thought to have occupied relatively isolated territory in Cornwall, Devon, Somerset and possibly part of Dorset. Their cultural connections, as expressed in their ceramics, were with the peninsula of Armorica across the Channel, rather than with the southeast of Britain. They do not seem to have been politically centralised: coins are relatively rare, none of them locally minted, and the structure, distribution and construction of Bronze Age and Iron Age hill forts, "Cornish rounds", and defensible farmsteads in the south west point to a number of smaller tribal groups living alongside each other.

Dumnonia is noteworthy for its many settlements that have survived from the Romano-British period, but also for its lack of a villa system. Local archaeology has revealed instead the isolated enclosed farmsteads known locally as "rounds". These seem to have survived the Roman abandonment of Britain, but were subsequently replaced, in the 6th and 7th centuries, by the unenclosed farms taking the Brythonic toponymic "tre-".

As in most other Brythonic areas, Iron Age hill forts, such as Hembury Castle, were refortified for the use of chieftains or kings. Other high-status settlements such as Tintagel seem to have been reconstructed during this period. Post-Roman imported pottery has been excavated from many sites across the region, and the apparent surge in late 5th century Mediterranean and/or Byzantine imports is yet to be explained satisfactorily.

Apart from fishing and agriculture, the main economic resource of the Dumnonii was tin mining. The area of Dumnonia had been mined since ancient times, and the tin was exported from the ancient trading port of Ictis (St Michael's Mount). Tin extraction (mainly by streaming) had existed here from the early Bronze Age around the 22nd century BC. West Cornwall, around Mount's Bay, was traditionally thought to have been visited by metal traders from the eastern Mediterranean

During the first millennium BC trade became more organised, first with the Phoenicians, who settled Gades (Cadiz) around 1100 BC, and later with the Greeks, who had settled Massilia (Marseilles) and Narbo (Narbonne) around 600 BC. Smelted Cornish tin was collected at Ictis whence it was conveyed across the Bay of Biscay to the mouth of the Loire and then to Gades via the Loire and Rhone valleys. It went then through the Mediterranean Sea in ships to Gades.

During the period c. 500-450 BC, the tin deposits seem to have become more important, and fortified settlements appear such as at Chun Castle and Kenidjack Castle, to protect both the tin smelters and mines.

The earliest account of Cornish tin mining was written by Pytheas of Massilia late in the 4th century BC after his circumnavigation of the British Isles. Underground mining was described in this account, although it cannot be determined when it had started. Pytheas's account was noted later by other writers including Pliny the Elder and Diodorus Siculus.

It is likely that tin trade with the Mediterranean was later on under the control of the Veneti. Britain was one of the places proposed for the "Cassiterides", that is Tin Islands. Tin working continued throughout Roman occupation although it appears that output declined because of new supplies brought in from the deposits discovered in Iberia (Spain and Portugal). However, when these supplies diminished, production in Dumnonia increased and appears to have reached a peak during the 3rd century AD.

The Sub-Roman or Post-Roman history of Dumnonia comes from a variety of sources and is considered exceedingly difficult to interpret given that historical fact, legend and confused pseudo-history are compounded by a variety of sources in Middle Welsh and Latin. The main sources available for discussion of this period include Gildas's "De Excidio Britanniae" and Nennius's "Historia Brittonum", the "Annales Cambriae", "Anglo-Saxon Chronicle", William of Malmesbury's "Gesta Regum Anglorum" and "De Antiquitate Glastoniensis Ecclesiae", along with texts from the "Black Book of Carmarthen" and the "Red Book of Hergest", and Bede's "Historia ecclesiastica gentis Anglorum" as well as "The Descent of the Men of the North" ("Bonedd Gwŷr y Gogledd", in Peniarth MS 45 and elsewhere) and the "Book of Baglan".





Declaration of independence

A declaration of independence, declaration of statehood or proclamation of independence is an assertion by a polity in a defined territory that it is independent and constitutes a state. Such places are usually declared from part or all of the of another state or failed state, or are breakaway territories from within the larger state. In 2010, the UN's International Court of Justice ruled in an advisory opinion in Kosovo that "International law contains no prohibition on declarations of independence", though the state from which the territory wishes to secede may regard the declaration as rebellion, which may lead to a war of independence or a constitutional settlement to resolve the crisis.


Drag racing

Drag racing is a type of motor racing in which automobiles or motorcycles compete, usually two at a time, to be first to cross a set finish line. The race follows a short, straight course from a standing start over a measured distance, most commonly , with a shorter, distance becoming increasingly popular, as it has become the standard for Top Fuel dragsters and Funny Cars, where some major bracket races and other sanctioning bodies have adopted it as the standard. The is also popular in some circles. Electronic timing and speed sensing systems have been used to record race results since the 1960s.

The history of automobiles and motorcycles being used for drag racing is nearly as long as the history of motorized vehicles themselves, and has taken the form of both illegal street racing and as a regulated motorsport.

Drag racing started in the 1940s. World War II veterans were prominently involved, and some early drag races were done at decommissioned aircraft bases with landing strips that made them an ideal place for the sport. In 1951, Wally Parks formed the National Hot Rod Association (NHRA). The organization banned the use of nitromethane in 1957, calling it unsafe, in part through the efforts of C. J. Hart; the ban would be lifted in 1963.

Several other racing organizations were created over the past several decades, such as the Professional Drag Racers Association (PDRA) founded in 2014.

Thanks, in part, to the Discovery series Street Outlaws, fandom for drag racing has seen a resurgence in the past decade.

Push starts to get engines running were necessary until the National Hot Rod Association (NHRA) mandated self-starters in 1976. After burnouts, cars would be pushed back by crews; this persisted until NHRA required reversing systems in 1980. Don Garlits was the first to do burnouts across the starting line, which is now standard practice. Each driver then backs up to and stages at the starting line.

Before each race (commonly known as a pass), each driver is allowed to perform a burnout, which heats the driving tires and lays rubber down at the beginning of the track, improving traction. The cars run through a "water box" (formerly a "bleach box", before bleach was replaced by flammable traction compound, which produced spectacular, and dangerous, flame burnouts; the hazard led NHRA to mandate use of water in the 1970s).

Modern races are started electronically by a system known as a "Christmas tree", which consists of a column of lights for each driver/lane, and two light beam sensors per lane on the track at the starting line. Current NHRA trees, for example, feature one blue light (split into halves), then three amber, one green, and one red. When the first light beam is broken by a vehicle's front tire(s), the vehicle is "pre-staged" (approximately from the starting line), and the pre-stage indicator on the tree is lit. When the second light beam is broken, the vehicle is "staged", and the stage indicator on the tree is lit. Vehicles may then leave the pre-stage beam, but must remain in the stage beam until the race starts.

Once one competitor is staged, their opponent has a set amount of time to stage or they will be instantly disqualified, indicated by a red light on the tree. Otherwise, once both drivers are staged, the system chooses a short delay at random (to prevent a driver being able to anticipate the start), then starts the race. The light sequence at this point varies slightly. For example, in NHRA Professional classes, three amber lights on the tree flash simultaneously, followed 0.4 seconds later by a green light (this is also known as a "pro tree"). In NHRA Sportsman classes, the amber lights illuminate in sequence from top to bottom, 0.5 seconds apart, followed 0.5 seconds later by the green light (this is also known as a "sportsman tree" or "full tree"). If a vehicle leaves the starting line before the green light illuminates, the red light for that lane illuminates instead, and the driver is disqualified (also known as "red lighting"). In a handicap start, the green light automatically lights up for the first driver, and the red light is only lit in the proper lane after both cars have launched if one driver leaves early, or if both drivers left early, the driver whose reaction time is worse (if one lane has a -.015 and the other lane has a -.022, the lane of the driver who committed a 0.022 is given the red light after both cars have left), as a red light infraction is only assessed to the driver with the worse infraction, if both drivers leave early. Even if both drivers leave early, the green light is automatically lit for the driver that left last, and they still may win the pass (as in the 2014 NHRA Auto Club Pro Stock final, Erica Enders-Stevens and Jason Line both committed red light infractions; only Line was assessed with a red light, as he was -.011 versus Enders-Stevens' -.002).

Several measurements are taken for each race: reaction time, elapsed time, and speed. Reaction time is the period from the green light illuminating to the vehicle leaving the staging beams or breaking the guard beam. Elapsed time is the period from the vehicle leaving the starting line to crossing the finish line. Speed is measured through a speed trap covering the final to the finish line, indicating average speed of the vehicle in that distance.

Except where a breakout rule is in place, the winner is the first vehicle to cross the finish line, and therefore the driver with the lowest combined reaction time and elapsed time. Because these times are measured separately, a driver with a slower elapsed time can actually win if that driver's advantage in reaction time exceeds the elapsed time difference. In heads-up racing, this is known as a "holeshot win". In categories where a breakout rule is in effect (for example, NHRA Junior Dragster, Super Comp, Super Gas, Super Stock, and Stock classes, as well as some dial-in classes), if a competitor is faster than his or her predetermined time (a "breakout"), that competitor loses. If both competitors are faster than their predetermined times, the competitor who breaks out by less time wins. Regardless, a red light foul is worse than a breakout, except in Junior Dragster where exceeding the absolute limit is a cause for disqualification.

Most race events use a traditional bracket system, where the losing car and driver are eliminated from the event while the winner advances to the next round, until a champion is crowned. Events can range from 16 to over 100 car brackets. Drivers are typically seeded by elapsed times in qualifying. In bracket racing without a breakout (such as NHRA Competition Eliminator), pairings are based on times compared to their index (faster than index for class is better). In bracket racing with a breakout (Stock, Super Stock, but also the NHRA's Super classes), the closest to the index is favorable.

A popular alternative to the standard eliminations format is the Chicago Style format (also called the Three Round format in Australia), named for the US 30 Dragstrip near Merrillville, Indiana where a midweek meet featured this format. All entered cars participate in one qualifying round, and then are paired for the elimination round. The two fastest times among winners from this round participate in the championship round. Depending on the organization, the next two fastest times may play for third, then fifth, and so forth, in consolation rounds. Currently, the National Drag Racing Championship in Australia uses the format for major categories.

The standard distance of a drag race is 1,320 feet, 402 m, or 1/4 mile (±0.2% FIA & NHRA rules). However, due to safety concerns, certain sanctioning bodies (notably the NHRA for its Top Fuel and Funny Car classes) have shortened races to 1,000 feet. Some drag strips are even shorter and run 660 feet, 201 m, or 1/8 mile. The 1,000 foot distance is now also popular with bracket racing, especially in meets where there are 1/8 mile cars and 1/4 mile cars racing together, and is used by the revived American Drag Racing League for its primary classes (not Jr Dragster). Some organizations that deal with Pro Modified and "Mountain Motor" Pro Stock cars (Professional Drag Racers Association) use the 1/8 mile distance, even if the tracks are 1/4 mile tracks.

The National Hot Rod Association (NHRA) oversees the majority of drag racing events in North America. The next largest organization is the International Hot Rod Association (IHRA). One of the major sanctioning bodies in drag racing. 

The World Drag Racing Alliance (WDRA) WDRA is the first fully functioning sanctioning body created (one that actually sanctions facilities) since 1970.

Besides NHRA, IHRA, and WDRA, there are several other racing organizations were created over the past several decades. The Professional Drag Racers Association (PDRA), founded in 2014, races 1/8 mile with events throughout the US. The National Mustang Racers Association (NMRA), started in 1999, is considered the leader in Ford motorsports events. The National Muscle Car Association (NMCA), is the longest running major street-legal drag racing association. The NMCA provides competitive and organized national event competitions from grassroots drag racers all the way to the powerful and fast VP Racing Fuels Xtreme Pro Mod racers.

There are also niche organizations for muscle cars and nostalgia vehicles. The Nostalgia Drag Racing League (NDRL) based in Brownsburg, IN, runs a series of 1/4 mile (402 m) drag races in the Midwest for 1979 and older nostalgic appearing cars, with four classes of competition running in an index system. Pro 7.0 and Pro 7.50 run heads up 200 mile per hour (320 kilometre per hour) passes, while Pro Comp and Pro Gas run 8.0 to 10.0 indices. NDRL competition vehicles typically include Front Engine Dragsters, Altereds, Funny Cars, early Pro Stock clones, Super Stocks and Gassers. 

The National Electric Drag Racing Association (NEDRA) races electric vehicles against high performance gasoline-powered vehicles such as Dodge Vipers or classic muscle cars in 1/4 and 1/8 mile (402 m & 201 m) races. The current electric drag racing record is 6.940 seconds at 201.37 mph (324.0736 km/h) for a quarter mile (402 m). Another niche organization is the VWDRC which run a VW-only championship with vehicles running under 7 seconds.

Prior to the founding of the NHRA and IHRA, smaller organizations sanctioned drag racing in the early years, which included the competing AHRA in the United States from 1955 to 2005.

The first Australian Nationals event was run in 1965 at Riverside raceway, near Melbourne. The Australian National Drag Racing Association (ANDRA) was established in 1973, and today they claim they are the "best in the world outside the United States". ANDRA sanctions races throughout Australia and throughout the year at all levels, from Junior Dragster to Top Fuel.

The ANDRA Drag Racing Series is for professional drivers and riders and includes Top Fuel, Top Alcohol, Top Doorslammer (similar to the USA Pro Modified class), Pro Stock (using 400 cubic inch engines (6.5 litres)), Top Bike and Pro Stock Motorcycle.

The Summit Sportsman Series is for ANDRA sportsman drivers and riders and includes Competition, Super Stock, Super Compact, Competition Bike, Supercharged Outlaws, Top Sportsman, Modified, Super Sedan, Modified Bike, Super Street and Junior Dragster.

In 2015, after a dispute with ANDRA, Sydney Dragway, Willowbank Raceway and the Perth Motorplex invited the International Hot Rod Association (IHRA) to sanction events at their tracks. Shortly thereafter the Perth Motorplex reverted to ANDRA sanction. Although greatly assisted by ANDRA prior
to its construction, Springmount Raceway opted for IHRA sanction. The 400 Thunder Series targets professional racers to its races. Intended to be the premier Drag racing series in Australia it has never been able
to run a truly National series and has been on a steady decline since its inception. Most recently Top Fuel Australia (the organization that represents the Top Fuel owners) recently extracted itself from the 400 Thunder series. ANDRA recently launched a new National series that will initially cater for Top Doorslammer and Top Fue Motorcycle. This series will provide a greater National coverage than the 400 Thunder Series did and will soon add other Professional categories.
In 2021 Heathcote Park Raceway in Victoria was sold to new ownership and has since been sanctioned by IHRA for small regional events.

Drag racing was imported to Europe by American NATO troops during the Cold War. Races were held in West Germany beginning in the 1960s at the airbases at Ramstein and Sembach and in the UK at various airstrips and racing circuits before the opening of Europe's first permanent drag strip at Santa Pod Raceway in 1966.

The FIA organises a Europe-wide four wheeled championship for the Top Fuel, Top Methanol Dragster, Top Methanol Funny Car, Pro Modified and Pro Stock classes. FIM Europe organises a similar championship for bike classes. In addition, championships are run for sportsman classes in many countries throughout Europe by the various national motorsport governing bodies.

Drag racing in New Zealand started in the 1960s. The New Zealand Hot Rod Association (NZHRA) sanctioned what is believed to have been the first drag meeting at an open cut coal mine at Kopuku, south of Auckland, sometime in 1966. In 1973, the first and only purpose built drag strip opened in Meremere by the Pukekohe Hot Rod Club. In April 1993 the governance of drag racing was separated from the NZHRA and the New Zealand Drag Racing Association (NZDRA) was formed. In 2014, New Zealand's second purpose built drag strip – Masterton Motorplex – opened.

The first New Zealand Drag Racing Nationals was held in the 1966/67 season at Kopuku, near Auckland.

There are now two governing bodies operating drag racing in New Zealand with the Florida-based International Hot Rod Association sanctioning both of New Zealands major tracks at Ruapuna (Pegasus Bay Drag Racing Association ) in the South Island and Meremere Dragway Inc in the North Island which is now become the best drag strip in NZ. However, the official ASN of the sport, per FIA regulations, is the New Zealand Drag Racing Association.

Many countries in South America race 200 meters, unlike in the United States and Australia, where the quarter-mile, or, 400 meters, respectively, is typical.

Organized drag racing in Colombia is the responsibility of Club G3, a private organization. The events take place at Autódromo de Tocancipá.

Local "muscles" like Chevrolet Opala and Chevrolet Chevette (both models brought from Opel with increased motors from Detroit, US) are the show makers for brazilian dragstrips.

Curaçao

On the island of Curaçao, organization of drag racing events is handled by the Curaçao Autosport Foundation (FAC)
All racing events, including street legal competitions, happen at the Curaçao International Raceway.

On the island of Aruba, all racing events, including street legal competitions, happen at Palomarga International Raceway.

Barbados

On the island of Barbados, organization of drag racing events is done by the Barbados Association of Dragsters and Drifters. Currently the drag racing is done at Bushy Park racing circuit over 1/8 mile, while "acceleration tests" of 1/4 mile are done at the Paragon military base.

Saint Lucia

On the Island of Saint Lucia, organization of drag racing events is done by no-one. All local groups are tie ups. Currently races are held at the US Old military base also known as the "Ca Ca Beff", "The Base" near the Hewanorra International Airport in Vieux Fort.

Dominican Republic

On Santo Domingo, organization of drag racing events is done by Autodromo Sunix and they happen at the Autodromo Sunix, close to the Airport SDQ.

Organized drag racing is rapidly growing in India. The country's first drag race meet was organized by "Autocar India" in Mumbai in 2002. Since then there have been many drag racing events in India. The most popular event is Elite Octanes' Valley Run which is held at Ambey Valley air strip in Loanavla every year.

The biggest drag series event was organized by India Speed Week with three different locations around India. After the series two riders were chosen to represent the country 2017 initiative to bring 11 times world drag racing champion Rickey Gadson to India. The initiative was executed during the Valley Run 2017 event, which gave the participants a platform to perform at the highest level globally. Rickey Gadson, as an extension of the initiative invited two of the top performing drag racers to visit USA to train and get an opportunity to represent India at the World Finals of drag racing held on 16-18 November 2018 in Valdosta GA, USA. As a result the two riders performed in their maiden event outside India. Also during the event, Amit Sharma, the fastest drag racer in Indian drag racing history, produced a time slip of 8.87 sec's – the fastest ever by any Indian.

Drag racing is also gaining popularity in Pakistan, with private organizations sponsoring such events. The Bahria Town housing project recently organized a drag racing event in Rawalpindi with the help of some of the country's best drivers.

Sri Lanka has seen an immense growth in drag racing due to legal meets held by the Ceylon Motor Sports Club, an FIA sanctioned body. In recent years, exotic cars and Japanese power houses have been taking part in these popular events.

Drag racing is an established sport in South Africa, with a number of strips around the country including Tarlton International Raceway and ODI Raceway. Drag racing is controlled by Motorsport South Africa and all drivers are required to hold a valid Motorsport South Africa license. Drivers can compete in a number of categories including Top Eliminator, Senior Eliminator, Super Competition Eliminator, Competition Eliminator, Pro Street Bikes, Superbike Eliminator, Supersport Shootout (motorcycle), Street Modified, and Factory Stock.

Drag racing in Russia started in 2004 in Moscow when the Russian Automotive Federation (RAF) sanctioned it as an official motorsport. Drag Racing became popular in Russia after "The Fast and the Furious" film in 2001, but competitions were illegal before 2004. The most outstanding drag racing event of the early years was "DRAG BITVA" (Drag Battle) which took place in Krasnoyarsk, Siberia from 2005 to 2008. Krasnoyarsk is located in the middle of Russia, so it was the best place to bring all the fastest cars from all over the country. Due to the financial situation "DRAG BITVA" was canceled in 2009 and never came back. It was difficult times for drag racing in Russia from 2009 to 2014, but it was supported by enthusiasts in every region. There were a lot of competitions but it was not as big as "DRAG BITVA". In 2014 Dragtimes company in partnership with SMP Racing became the Russian Drag Racing Championship (SMP RDRC) promoters, since then Drag Racing in Russia became more professional. From the very beginning to 2014 only streetcars were allowed to compete in Russia. Now it's also allowed to run promods and dragsters in SMP RDRC. Thanks to the efforts of SMP RDRC promoters in 2019 the first professional dragstrip in Russia "RDRC Racepark" was built. It's located near Moscow in 40 kilometers of downtown at the former airfield Bykovo. It gave many opportunities to test the cars and make new records. Before the track was built, competitions took place on straight parts of circuits, so it was not allowed to prepare the whole 1/4 mile, only 1/8 and the tracks were available for drag racers except racing weekends of local or national events. From the very beginning one of the main ideas of the promoters was to increase the quality and reach of live broadcasts, so SMP RDRC became the first racing series with its video production and remains so to this day.

Russian Championship has four classes:

Regional Series also have four classes divided by ET:

The national record belongs to 4-time national champion Dmitry Samorukov: 6.325 seconds at . It was set in a special record run in 2016 on Dodge Viper Doorslammer in Grozny, Chechen Republic at "Fort Grozny" racetrack.

Dmitry Samorukov was the first Russian participant of the FIA European Championship on a newly built Chevrolet Camaro in the most competitive Promod class in 2019. After six stages of the competition, he took 10th of 38 places overall.

Russian driver Dmitry Kapustin on Nissan Skyline GT-R R32 is holding the European record of AWD streetcars: 7.182 seconds at . The record was set in a qualifying run in Grozny, Chechen Republic at "Fort Grozny" racetrack in 2018.

1/2 mile races are also popular in Russia. "Unlim 500+" is the main 1/2 mile race in Russia. It's a supercar and sportscar festival where only 500+ hp cars are allowed (e. g. Nissan GT-R, McLaren 720S, Lamborghini Aventador, Porsche 911, Ferrari 488, etc.). The national record on 1/2 mile distance also belongs to Dmitry Samorukov on Nissan GT-R R 35: 13.305 seconds at . The record was set on a test and tune day at the "RDRC Racepark" track in 2020.

There are hundreds of classes in drag racing, each with different requirements and restrictions on things such as weight, engine size, body style, modifications, and many others. NHRA and IHRA share some of these classes, but many are solely used by one sanctioning body or the other. The NHRA boasts over 200 classes, while the IHRA has fewer. Some IHRA classes have multiple sub-classes in them to differentiate by engine components and other features. There is even a class for aspiring youngsters, Junior Dragster, which typically uses an eighth-mile track, also favored by VW racers.

In 1997, the FIA (cars) and UEM (bikes) began sanctioning drag racing in Europe with a fully established European Drag Racing Championship, in cooperation (and rules compliance) with NHRA. The major European drag strips include Santa Pod Raceway in Podington, England; Alastaro Circuit, Finland; Mantorp Park, Sweden; Gardermoen Raceway, Norway and the Hockenheimring in Germany.

There is a somewhat arbitrary definition of what constitutes a "professional" class. The NHRA includes 5 pro classes; Top Fuel, Funny Car, Pro Stock, Pro Modified and Pro Stock Motorcycle. The FIA features a different set of 5 pro classes; Top Fuel, Top Methanol Dragster, Top Methanol Funny Car, Pro Modified and Pro Stock. Other sanctioning bodies have similarly different definitions. A partial list of classes includes:



A complete listing of all classes can be found on the respective NHRA and IHRA official websites.

The UEM also has a different structure of professional categories with Top Fuel Bike, Super Twin Top Fuel Bike, and Pro Stock Bike contested, leaving the entire European series with a total of 8 professional categories.

To allow different cars to compete against each other, some competitions are raced on a handicap basis, with faster cars delayed on the starting line enough to theoretically even things up with the slower car. This may be based on rule differences between the cars in stock, super stock, and modified classes, or on a competitor's chosen "dial-in" in bracket racing.

For a list of drag racing world records in each class, see Dragstrip#Quarter mile times.

A 'dial-in' is a time the driver estimates it will take their car to cross the finish line, and is generally displayed on one or more windows so the starter can adjust the starting lights on the tree accordingly. The slower car will then get a head start equal to the difference in the two dial-ins, so if both cars perform perfectly, they would cross the finish line dead even. If either car goes faster than its dial-in (called breaking out), it is disqualified regardless of who has the lower elapsed time; if both cars break out, the one who breaks out by the smallest amount wins. However, if a driver had jump-started (red light) or crossed a boundary line, both violations override any break out (except in some classes with an absolute break out rule such as Junior classes).

The effect of the bracket racing rules is to place a premium on consistency of performance of the driver and car rather than on raw speed, in that victory goes to the driver able to precisely predict elapsed time, whether it is fast or slow. This in turn makes victory much less dependent on budget, and more dependent on skill, making it popular with casual weekend racers.





Draugr

The draugr or draug (, plural ; modern , , and Danish, Swedish, and ) is an undead creature from the Scandinavian saga literature and folktales.

Commentators extend the term "draugr" to the undead in medieval literature, even if it is never explicitly referred to as such in the text, and designated them rather as a ("barrow-dweller") or an , literally "again-walker" ().

Draugar live in their graves or royal palaces, often guarding treasure buried with them in their burial mound. They are revenants, or animated corpses with a corporeal body, rather than ghosts which possess intangible spiritual bodies.

Old Norse "" is defined as "a ghost, spirit, esp. the dead inhabitant of a cairn". Often the "draugr" is regarded not so much as a ghost but a revenant, i.e., the reanimated corpse of the deceased inside the burial mound (as in the example of Kárr inn gamli in "Grettis saga").

The "draugr" was referred to as "barrow-wight" in the 1869 translation of "Grettis saga", long before J. R. R. Tolkien employed this term in his novels, though "barrow-wight" is actually a rendering of (literally the ‘howe-dweller’), otherwise translated as "barrow-dweller".

In Old Norse, the term "draugr" also meant a tree trunk or dry dead wood, or in poetry could refer to a man or warrior, since Old Norse poetry often used terms for trees to represent humans, especially in kennings, referencing the myth that the god Odin and his brothers created the first humans Ask and Embla from trees. There was thus a connection between the idea of a felled tree's trunk and that of a dead man's corpse.

In Swedish, "draug" is a modern loanword from West Norse, as the native Swedish form "drög" has acquired the meaning of "a pale, ineffectual, and slow-minded person that drags himself along". 

The word is hypothetically traced to Proto-Indo European stem "*" "phantom", from "*" "deceive" (see also Avestan "druj").

Beings in British folklore such as "shag-boys" and "hogboons" derive their names from .

Unlike Kárr inn gamli (Kar the Old) in "Grettis saga", who is specifically called a "draugr", Glámr the ghost in the same saga is never explicitly called a "draugr" in the text, though called a "troll" in it. Yet Glámr is still routinely referred to as a "draugr" by modern scholars.

Beings not specifically called , but actually only referred to as (‘revenants’, pl. of ) and (‘haunting’) in these medieval sagas are still commonly discussed as a in various scholarly works, or the "draugar" and the haugbúar are lumped into one.

A further caveat is that the application of the term "draugr" may not necessarily follow what the term might have meant in the strict sense during medieval times, but rather follow a modern definition or notion of "draugr", specifically such ghostly beings (by whatever names they are called) that occur in Icelandic folktales categorized as "Draugasögur" in Jón Árnason's collection, based on the classification groundwork laid by Konrad Maurer.

The draugr is a "corporeal ghost" with a physical tangible body and not an "imago", and in tales it is often delivered a "second death" by destruction of the enlivened corpse.

The draugr has also been conceived of as a type of "vampire" by folktale anthologist Andrew Lang in late 1897, with the idea further pursued by more modern commentators. The focus here is not on blood-sucking, which is not attested for the "draugr", but rather, contagiousness or transmittable nature of vampirism, that is to say, how a vampire begets another by turning his or her attack victim into one of his own kind. Sometimes the chain of contagion becomes an outbreak, e.g., the case of Þórólfr bægifótr (Thorolf Lame-foot or Twist-Foot), and even called an "epidemic" regarding Þórgunna (Thorgunna).

A more speculative case of vampirism is that of Glámr, who was asked to tend sheep for a haunted farmstead and was subsequently found dead with his neck and every bone in his body broken. It has been surmised by commentators that Glámr by "contamination" was turned into an undead ("draugr") by whatever being was haunting the farm.

Draugar usually possessed superhuman strength, and were "generally hideous to look at", bearing a necrotic black or blue color, and were associated with a "reek of decay" or more precisely inhabited haunts that often issued foul stench. 

The draugar were said to be either "hel-blár" ("death-blue") or "nár-fölr" ("corpse-pale"). Glámr when found dead was described as ""blár sem Hel en digr sem naut" (black as hell and bloated to the size of a bull)". Þórólfr Lame-foot, when lying dormant, looked "uncorrupted" and also "was black as death [i.e., bruised black and blue] and swollen to the size of an ox". The close similarity of these descriptions have been noted. "Laxdæla saga" describes how bones were dug up belonging to a dead sorceress who had appeared in dreams, and they were "blue and evil looking".

Þráinn (Thrain) the berserker of Valland "turned himself into a troll" in "Hrómundar saga Gripssonar" was a fiend ("dólgr") which was "black and huge.. roaring loudly and blowing fire", and moreover, possessed long scratching claws, and the claws stuck in the neck, prompting the hero Hrómundr to refer to the "dragur" as a sort of cat ().

Draugrs often give off a morbid stench, not unlike the smell of a decaying body. The mound where Kárr the Old was entombed reeked horribly. In "Harðar saga" Hörðr Grímkelsson’s two underlings die even before entering Sóti the Viking's mound, due to the "gust and stink ()" wafting out of it. When enraged Þráinn filled the barrow with an "evil reek."

Draugar are noted for having numerous magical abilities (referred to as "trollskap") resembling those of living witches and wizards, such as shape-shifting, controlling the weather, and seeing into the future.

The undead Víga-Hrappr Sumarliðason (Killer-Hrapp) of "Laxdaela saga", unlike the typical guardian of a treasure hoard, does not stay put in his burial place but roams around his farmstead of Hrappstaðir, menacing the living. Víga-Hrappr's ghost, it has been suggested, was capable of transforming into the seal with human-like eyes which appeared before Þorsteinn svarti/surt (Thorsteinn the Black) sailing by ship, and was responsible for the sinking of the ship to prevent the family from reaching Hrappstaðir. The ability to shape-shift has been ascribed to Icelandic ghosts generally, particularly into the shape of a seal.

A draugr in Icelandic folktales collected in the modern age can also change into a great flayed bull, a grey horse with a broken back but no ears or tail, and a cat that would sit upon a sleeper's chest and grow steadily heavier until their victim suffocated.

Draugar have the ability to enter into the dreams of the living, and they will frequently leave a gift behind so that "the living person may be assured of the tangible nature of the visit". Draugar also have the ability to curse a victim, as shown in the Grettis saga, where Grettir is cursed to be unable to become any stronger. Draugar also brought disease to a village and could create temporary darkness in daylight hours. They preferred to be active during the night, although they did not appear to be vulnerable to sunlight like some other revenants. Draugr can also kill people with bad luck.

A draugr's presence might be shown by a great light that glowed from the mound like foxfire. This fire would form a barrier between the land of the living and the land of the dead. 

The undead Víga-Hrappr exhibited the ability to sink into the ground to escape from Óláfr Hǫskuldsson the Peacock.

Some draugar are immune to weapons, and only a hero has the strength and courage needed to stand up to so formidable an opponent. In legends, the hero would often have to wrestle the draugr back to his grave, thereby defeating him, since weapons would do no good. A good example of this is found in "Hrómundar saga Gripssonar". Iron could injure a draugr, as is the case with many supernatural creatures, although it would not be sufficient to stop it. Sometimes the hero is required to dispose of the body in unconventional ways. The preferred method is to cut off the draugr's head, burn the body, and dump the ashes in the sea—the emphasis being on making absolutely sure that the draugr was dead and gone.

Any mean, nasty, or greedy person can become a draugr. As Ármann Jakobsson notes, "most medieval Icelandic ghosts are evil or marginal people. If not dissatisfied or evil, they are unpopular".

The draugr's motivation was primarily envy and greed. Greed causes it to viciously attack any would-be grave robbers, but the draugr also expresses an innate envy of the living stemming from a longing for the things of life which it once had. They also exhibit an immense and nearly insatiable appetite, as shown in the encounter of Aran and Asmund, sword brothers who made an oath that, if one should die, the other would sit vigil with him for three days inside the burial mound. When Aran died, Asmund brought his own possessions into the barrow—banners, armor, hawk, hound, and horse—then set himself to wait the three days:

The draugr's victims were not limited to trespassers in its home. The roaming undead devastated livestock by running the animals to death either by riding them or pursuing them in some hideous, half-flayed form. Shepherds' duties kept them outdoors at night, and they were particular targets for the hunger and hatred of the undead:

Animals feeding near the grave of a draugr might be driven mad by the creature's influence. They may also die from being driven mad. Thorolf, for example, caused birds to drop dead when they flew over his bowl barrow.

The main indication that a deceased person will become a draugr is that the corpse is not in a horizontal position but is found standing upright (Víga-Hrappr), or in a sitting position (Þórólfr), indicating that the dead might return. Ármann Jakobsson suggests further that breaking the draugr's posture is a necessary or helpful step in destroying the "draugr", but this is fraught with the risk of being inflicted with the evil eye, whether this is explicitly told in the case of Grettir who receives the curse from Glámr, or only implied in the case of Þórólfr, whose son warns the others to beware while they unbend Þórólfr's seated posture. 

The revenant "draugr" needing to be decapitated in order to incapacitate them from further hauntings is a common theme in the family sagas.

Traditionally, a pair of open iron scissors was placed on the chest of the recently deceased, and straws or twigs might be hidden among their clothes. The big toes were tied together or needles were driven through the soles of the feet in order to keep the dead from being able to walk. Tradition also held that the coffin should be lifted and lowered in three different directions as it was carried from the house to confuse a possible draugr's sense of direction.

The most effective means of preventing the return of the dead was believed to be a corpse door, a special door through which the corpse was carried feet-first with people surrounding it so that the corpse couldn't see where it was going. The door was then bricked up to prevent a return. It is speculated that this belief began in Denmark and spread throughout the Norse culture, founded on the idea that the dead could only leave through the way they entered.

In "Eyrbyggja saga", draugar are driven off by holding a "door-doom". One by one, they are summoned to the door-doom and given judgment and forced out of the home by this legal method. The home was then purified with holy water to ensure that they never came back.

A variation of the draugr is the "haugbui" (from Old Norse "haugr"' "howe, barrow, tumulus") which was a mound-dweller, the dead body living on within its tomb. The notable difference between the two was that the haugbui is unable to leave its grave site and only attacks those who trespass upon their territory.

The haugbui was rarely found far from its burial place and is a type of undead commonly found in Norse sagas. The creature is said to either swim alongside boats or sail around them in a partially submerged vessel, always on their own. In some accounts, witnesses portray them as shapeshifters who take on the appearance of seaweed or moss-covered stones on the shoreline.

One of the best-known draugar is "Glámr", who is defeated by the hero in "Grettis saga". After Glámr dies on Christmas Eve, "people became aware that Glámr was not resting in peace. He wrought such havoc that some people fainted at the sight of him, while others went out of their minds". After a battle, Grettir eventually gets Glámr on his back. Just before Grettir kills him, Glámr curses Grettir because "Glámr was endowed with more evil force than most other ghosts", and thus he was able to speak and leave Grettir with his curse after his death.

A somewhat ambivalent, alternative view of the draugr is presented by the example of Gunnar Hámundarson in "Njáls saga": "It seemed as though the howe was agape, and that Gunnar had turned within the howe to look upwards at the moon. They thought that they saw four lights within the howe, but not a shadow to be seen. Then they saw that Gunnar was merry, with a joyful face."

In the "Eyrbyggja saga", a shepherd is assaulted by a blue-black draugr. The shepherd's neck is broken during the ensuing scuffle. The shepherd rises the next night as a draugr.

In more recent Scandinavian folklore, the draug (the modern spelling used in Denmark, Norway, and Sweden) is a supernatural being that occurs in legends along the coast of Norway. Draugen was originally a dead person who either lived in the mound (in Norse called haugbúi) or went out to haunt the living. In later folklore, it became common to limit the figure to a ghost of a dead fisherman who had drifted at sea, and who was not buried in Christian soil. It was said that he wore a leather jacket or was dressed in oilskin, but had a seaweed vase for his head. He sailed in a half-boat with blocked sails (the Norwegian municipality of Bø, Nordland has the half-boat in its coat-of-arms) and announced death for those who saw him or even wanted to pull them down. This trait is common in the northernmost part of Norway, where life and culture was based on fishing more than anywhere else. The reason for this may be that the fishermen often drowned in great numbers, and the stories of restless dead coming in from sea were more common in the north than any other region of the country.

A recorded legend from Trøndelag tells how a cadaver lying on a beach became the object of a quarrel between the two types of draug (headless and seaweed-headed). A similar source even tells of a third type, the "gleip", known to hitch themselves to sailors walking ashore and make them slip on the wet rocks.

But, though the draug usually presages death, there is an amusing account in Northern Norway of a northerner who managed to outwit him:

The modern and popular connection between the draug and the sea can be traced back to authors like Jonas Lie and Regine Nordmann, whose works include several books of fairy tales, as well as the drawings of Theodor Kittelsen, who spent some years living in Svolvær. Up north, the tradition of sea-draugs is especially vivid.

Arne Garborg describes land-draugs coming fresh from the graveyards, and the term "draug" is even used of vampires. The notion of draugs who live in the mountains is present in the poetic works of Henrik Ibsen ("Peer Gynt"), and Aasmund Olavsson Vinje. The Nynorsk translation of "The Lord of the Rings" used the term for both Nazgûl and the dead men of Dunharrow. Tolkien's barrow-wights bear obvious similarity to, and were inspired by the haugbúi.

In "The Elder Scrolls" video game series, draugr are the undead mummified corpses of fallen warriors that inhabit the ancient burial sites of a Nordic-inspired race of man. These draugr behave more like haugbúi than traditional draugr. They first appeared in the Bloodmoon expansion to "The Elder Scrolls III: Morrowind", and would later go on to appear all throughout "The Elder Scrolls V: Skyrim". Draugr are a common enemy, the first encountered by the player, in the 2018 video game "God of War", with a variety of different powers and abilities. In 2019, a spaceship named "Draugur" was added to the game "Eve Online", as the command destroyer of the Triglavian faction. Draugr appear as an enemies in the 2021 early access game "Valheim", where they take the more recent, seaweed version of the Draug.

In "Draug (film)", a group of Viking warriors encounter the draugr while searching for a missing person inside a huge forest. The draugr are depicted as blue-black animated corpses wielding many magical abilities.

In the movie "The Northman", Amleth enters a burial mound, in search of a magical sword named "Draugr". Inside the grave chamber Amleth encounters an undead Mound Dweller (draugr), who he has to fight in order to obtain the blade.

Season two episode two of "Hilda", entitled "The Draugen," involved draugen as the ghosts of sailors who died at sea. While their form was ghostly, the captain was able to wear a coat, and had a shock of seaweed for hair.

The exoplanet PSR B1257+12 A has been named "Draugr".




Day

A day is the time period of a full rotation of the Earth with respect to the Sun. On average, this is 24 hours (86,400 seconds). As a day passes at a given location it experiences morning, noon, afternoon, evening, and night. This daily cycle drives circadian rhythms in many organisms, which are vital to many life processes.

A collection of sequential days is organized into calendars as dates, almost always into weeks, months and years. A solar calendar organizes dates based on the Sun's annual cycle, giving consistent start dates for the four seasons from year to year. A lunar calendar organizes dates based on the Moon's lunar phase. 

In common usage, a day starts at midnight, written as 00:00 or 12:00 am in 24- or 12-hour clocks, respectively. Because the time of midnight varies between locations, time zones are set up to facilitate the use of a uniform standard time. Other conventions are sometimes used, for example the Jewish religious calendar counts days from sunset to sunset, so the Jewish Sabbath begins at sundown on Friday. In astronomy, a day begins at noon so that observations throughout a single night are recorded as happening on the same day.

In specific applications, the definition of a day is slightly modified, such as in the SI day (exactly 86,400 seconds) used for computers and standards keeping, local mean time accounting of the Earth's natural fluctuation of a solar day, and stellar day and sidereal day (using the celestial sphere) used for astronomy. In most countries outside of the tropics, daylight saving time is practiced, and each year there will be one 23-hour civil day and one 25-hour civil day. Due to slight variations in the rotation of the Earth, there are rare times when a leap second will get inserted at the end of a UTC day, and so while almost all days have a duration of 86,400 seconds, there are these exceptional cases of a day with 86,401 seconds (in the half-century spanning 1972 through 2022, there have been a total of 27 leap seconds that have been inserted, so roughly once every other year).

The term comes from the Old English term "dæġ" (), with its cognates such as "dagur" in Icelandic, "Tag" in German, and "dag" in Norwegian, Danish, Swedish and Dutch – all stemming from a Proto-Germanic root "*dagaz". , "day" is the 205th most common word in American English, and the 210th most common in English English.

Several definitions of this universal human concept are used according to context, need, and convenience. Besides the day of 24 hours (86,400 seconds), the word "day" is used for several different spans of time based on the rotation of the Earth around its axis. An important one is the "solar day", the time it takes for the Sun to return to its culmination point (its highest point in the sky). Due to an orbit's eccentricity, the Sun resides in one of the orbit's foci instead of the middle. Consequently, due to Kepler's second law, the planet travels at different speeds at various positions in its orbit, and thus a solar day is not the same length of time throughout the orbital year. Because the Earth moves along an eccentric orbit around the Sun while the Earth spins on an inclined axis, this period can be up to 7.9 seconds more than (or less than) 24 hours. In recent decades, the average length of a solar day on Earth has been about 86,400.002 seconds (24.000 000 6 hours). There are currently about 365.2421875 solar days in one mean tropical year.

Ancient custom has a new day starting at either the rising or setting of the Sun on the local horizon (Italian reckoning, for example, being 24 hours from sunset, old style). The exact moment of, and the interval between, two sunrises or sunsets depends on the geographical position (longitude and latitude, as well as altitude), and the time of year (as indicated by ancient hemispherical sundials).

A more constant day can be defined by the Sun passing through the local meridian, which happens at local noon (upper culmination) or midnight (lower culmination). The exact moment is dependent on the geographical longitude, and to a lesser extent on the time of the year. The length of such a day is nearly constant (24 hours ± 30 seconds). This is the time as indicated by modern sundials.

A further improvement defines a fictitious mean Sun that moves with constant speed along the celestial equator; the speed is the same as the average speed of the real Sun, but this removes the variation over a year as the Earth moves along its orbit around the Sun (due to both its velocity and its axial tilt).

In terms of Earth's rotation, the average day length is about 360.9856°. A day lasts for more than 360° of rotation because of the Earth's revolution around the Sun. With a full year being slightly more than 360 days, the Earth's daily orbit around the Sun is slightly less than 1°, so the day is slightly less than 361° of rotation.

Elsewhere in the Solar System or other parts of the universe, a day is a full rotation of other large astronomical objects with respect to its star.

For civil purposes, a common clock time is typically defined for an entire region based on the local mean solar time at a central meridian. Such " time zones" began to be adopted about the middle of the 19th century when railroads with regularly occurring schedules came into use, with most major countries having adopted them by 1929. As of 2015, throughout the world, 40 such zones are now in use: the central zone, from which all others are defined as offsets, is known as , which uses Coordinated Universal Time (UTC).

The most common convention starts the civil day at midnight: this is near the time of the lower culmination of the Sun on the central meridian of the time zone. Such a day may be called a calendar day.

A day is commonly divided into 24 hours, with each hour being made up of 60 minutes, and each minute composed of 60 seconds.

A sidereal day or stellar day is the span of time it takes for the Earth to make one entire rotation with respect to the celestial background or a distant star (assumed to be fixed). Measuring a day as such is used in astronomy. A sidereal day is about 4 minutes less than a solar day of 24 hours (23 hours 56 minutes and 4.09 seconds), or 0.99726968 of a solar day of 24 hours. There are about 366.2422 stellar days in one mean tropical year (one stellar day more than the number of solar days).

Besides a stellar day on Earth, other bodies in the Solar System have day times, the durations of these being:
In the International System of Units (SI), a day not an official unit, but is accepted for use with SI. A day, with symbol d, is defined using SI units as 86,400 seconds; the second is the base unit of time in SI units. In 1967–68, during the 13th CGPM (Resolution 1), the International Bureau of Weights and Measures (BIPM) redefined a second as "... the duration of 9,192,631,770 periods of the radiation corresponding to the transition between two hyperfine levels of the ground state of the caesium 133 atom." This makes the SI-based day last exactly 794,243,384,928,000 of those periods.

Various decimal or metric time proposals have been made, but do not redefine the day, and use the day or sidereal day as a base unit. Metric time uses metric prefixes to keep time. It uses the day as the base unit, and smaller units being fractions of a day: a metric hour ("deci") is of a day; a metric minute ("milli") is of a day; etc. Similarly, in decimal time, the length of a day is static to normal time. A day is also split into 10 hours, and 10 days comprise a "décade –" the equivalent of a week. 3 "décades" make a month. Various decimal time proposals which do not redefine the day: Henri de Sarrauton's proposal kept days, and subdivided hours into 100 minutes; in Mendizábal y Tamborel's proposal, the sidereal day was the basic unit, with subdivisions made upon it; and Rey-Pailhade's proposal divided the day 100 "cés."

The word refers to various similarly defined ideas, such as:



Mainly due to tidal deceleration – the Moon's gravitational pull slowing down the Earth's rotation – the Earth's rotational period is slowing. Because of the way the second is defined, the mean length of a solar day is now about 86,400.002 seconds, and is increasing by about 2 milliseconds per century.

Since the rotation rate of the Earth is slowing, the length of a second fell out of sync with a second derived from the rotational period. This arose the need for leap seconds, which insert extra seconds into Coordinated Universal Time (UTC). Although typically 86,400 seconds in duration, a civil day can be either 86,401 or 86,399 SI seconds long on such a day. Other than the two-millisecond variation from tidal deceleration, other factors minutely affect the day's length, which creates an irregularity in the placement of leap seconds. Leap seconds are announced in advance by the International Earth Rotation and Reference Systems Service (IERS), which measures the Earth's rotation and determines whether a leap second is necessary.

Discovered by paleontologist John W. Wells, the day lengths of geological periods have been estimated by measuring sedimentation rings in coral fossils, due to some biological systems being affected by the tide. The length of a day at the Earth's formation is estimated at 6 hours. Arbab I. Arbab plotted day lengths over time and found a curved line. Arbab attributed this to the change of water volume present affecting Earth's rotation.

For most diurnal animals, the day naturally begins at dawn and ends at sunset. Humans, with their cultural norms and scientific knowledge, have employed several different conceptions of the day's boundaries. 
In the Hebrew Bible, Genesis 1:5 defines a day in terms of "evening" and "morning" before recounting the creation of the Sun to illuminate it: "And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day."
The Jewish day begins at either sunset or nightfall (when three second-magnitude stars appear).
Medieval Europe also followed this tradition, known as Florentine reckoning: In this system, a reference like "two hours into the day" meant "two hours after sunset" and thus times during the evening need to be shifted back one calendar day in modern reckoning. 
Days such as Christmas Eve, Halloween (“All Hallows’ Eve”), and the Eve of Saint Agnes are remnants of the older pattern when holidays began during the prior evening. 
The common convention among the ancient Romans, ancient Chinese and in modern times is for the civil day to begin at midnight, i.e. 00:00, and to last a full 24 hours until 24:00, i.e. 00:00 of the next day.
In ancient Egypt the day was reckoned from sunrise to sunrise. 

Prior to 1926, Turkey had two time systems: "Turkish", counting the hours from sunset, and "French", counting the hours from midnight.

Humans have divided the day in rough periods, which can have cultural implications, and other effects on humans' biological processes. The parts of the day do not have set times; they can vary by lifestyle or hours of daylight in a given place.

Daytime is the part of the day during which sunlight directly reaches the ground, assuming that there are no obstacles. The length of daytime averages slightly more than half of the 24-hour day. Two effects make daytime on average longer than night. The Sun is not a point but has an apparent size of about 32 minutes of arc. Additionally, the atmosphere refracts sunlight in such a way that some of it reaches the ground even when the Sun is below the horizon by about 34 minutes of arc. So the first light reaches the ground when the centre of the Sun is still below the horizon by about 50 minutes of arc. Thus, daytime is on average around 7 minutes longer than 12 hours.

Daytime is further divided into morning, afternoon, and evening. Morning occurs between sunrise and noon. Afternoon occurs between noon and sunset, or between noon and the start of evening. This period of time sees human's highest body temperature, an increase of traffic collisions, and a decrease of productivity. Evening begins around 5 or 6 pm, or when the sun sets, and ends when one goes to bed.

Twilight is the period before sunrise and after sunset in which there is natural light but no direct sunlight. The morning twilight begins at dawn and ends at sunrise, while the evening twilight begins at sunset and ends at dusk. Both periods of twilight can be divided into civil twilight, nautical twilight, and astronomical twilight. Civil twilight is when the sun is up to 6 degrees below the horizon; nautical when it is up to 12 degrees below, and astronomical when it is up to 18 degrees below.

Night is the period in which the sky is dark; the period between dusk and dawn where no light from the sun is visible. Light pollution during night can impact human and animal life, for example by disrupting sleep.


Database

In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.

Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.

Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.

Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.

Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.

Outside the world of professional information technology, the term "database" is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.

Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:


Both a database and its DBMS conform to the principles of a particular database model. "Database system" refers collectively to the database model, database management system, and database.

Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.

Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.

Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.

The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.

The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another.

The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.

Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object–relational databases.

The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.

The introduction of the term "database" coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.

As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the "CODASYL approach", and soon a number of commercial products based on this approach entered the market.

The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:

Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.

IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation "The Programmer as Navigator". IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use .

Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking "A Relational Model of Data for Large Shared Data Banks".

In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or "relations") aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called "views" could present the data in different ways for different users, but views could not be directly updated.

Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.

In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a "repeating group" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.

For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be "normalized" into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.

As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.

Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.

IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called "relational" are actually SQL DBMSs.

In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.

In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.

Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).

IBM started working on a prototype system loosely based on Codd's concepts as "System R" in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as "SQL/DS", and, later, "Database 2" (IBM Db2).

Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.

Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).

In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.

Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.

The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation." dBASE was one of the top selling software titles in the 1980s and early 1990s.

The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields. The term "object–relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.

XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.

NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.

In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.

NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.

Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).

Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.

One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.




Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database". Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.

The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.

The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:


It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.

Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.

The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.

Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.

A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.

External interaction with the database will be via an application program that interfaces with the DBMS. This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.

A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.

Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:


Database languages are specific to a particular data model. Notable examples include:


A database language may also incorporate features like:


Database storage is the container of the physical materialization of a database. It comprises the "internal" (physical) "level" in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the "conceptual level" and "external level" from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).

Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.

Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.

Often storage redundancy is employed to increase performance. A common example is storing "materialized views", which consist of frequently needed "external views" or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.

Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.

With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.

Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).

Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.

This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.

Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).

Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.

Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).

The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.

A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.

After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).

When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.

After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.

Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.

Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.

Other DBMS features might include:


Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".

The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.

Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.

Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms "data model" and "database model" are often used interchangeably, but in this article we use "data model" for the design of a specific database, and "database model" for the modeling notation used to express that design).

The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.

The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called "physical database design", and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.

Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.

A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.

Common logical data models for databases include:

An object–relational database combines the two related structures.

Physical data models include:

Other models include:

Specialized models are optimized for particular types of data:

A database management system provides three views of the database data:


While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different "views" of the company's database.

The three-level database architecture relates to the concept of "data independence" which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.

The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.

Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.

The database research area has several dedicated academic journals (for example, "ACM Transactions on Database Systems"-TODS, "Data and Knowledge Engineering"-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).



Dipole

In physics, a dipole () is an electromagnetic phenomenon which occurs in two ways:

Dipoles, whether electric or magnetic, can be characterized by their dipole moment, a vector quantity. For the simple electric dipole, the electric dipole moment points from the negative charge towards the positive charge, and has a magnitude equal to the strength of each charge times the separation between the charges. (To be precise: for the definition of the dipole moment, one should always consider the "dipole limit", where, for example, the distance of the generating charges should "converge" to 0 while simultaneously, the charge strength should "diverge" to infinity in such a way that the product remains a positive constant.)

For the magnetic (dipole) current loop, the magnetic dipole moment points through the loop (according to the right hand grip rule), with a magnitude equal to the current in the loop times the area of the loop.

Similar to magnetic current loops, the electron particle and some other fundamental particles have magnetic dipole moments, as an electron generates a magnetic field identical to that generated by a very small current loop. However, an electron's magnetic dipole moment is not due to a current loop, but to an intrinsic property of the electron. The electron may also have an "electric" dipole moment though such has yet to be observed (see electron electric dipole moment).
A permanent magnet, such as a bar magnet, owes its magnetism to the intrinsic magnetic dipole moment of the electron. The two ends of a bar magnet are referred to as poles (not to be confused with monopoles, see Classification below) and may be labeled "north" and "south". In terms of the Earth's magnetic field, they are respectively "north-seeking" and "south-seeking" poles: if the magnet were freely suspended in the Earth's magnetic field, the north-seeking pole would point towards the north and the south-seeking pole would point towards the south. The dipole moment of the bar magnet points from its magnetic south to its magnetic north pole. In a magnetic compass, the north pole of a bar magnet points north. However, that means that Earth's geomagnetic north pole is the "south" pole (south-seeking pole) of its dipole moment and vice versa.

The only known mechanisms for the creation of magnetic dipoles are by current loops or quantum-mechanical spin since the existence of magnetic monopoles has never been experimentally demonstrated.

A "physical dipole" consists of two equal and opposite point charges: in the literal sense, two poles. Its field at large distances (i.e., distances large in comparison to the separation of the poles) depends almost entirely on the dipole moment as defined above. A "point (electric) dipole" is the limit obtained by letting the separation tend to 0 while keeping the dipole moment fixed. The field of a point dipole has a particularly simple form, and the order-1 term in the multipole expansion is precisely the point dipole field.

Although there are no known magnetic monopoles in nature, there are magnetic dipoles in the form of the quantum-mechanical spin associated with particles such as electrons (although the accurate description of such effects falls outside of classical electromagnetism). A theoretical magnetic "point dipole" has a magnetic field of exactly the same form as the electric field of an electric point dipole. A very small current-carrying loop is approximately a magnetic point dipole; the magnetic dipole moment of such a loop is the product of the current flowing in the loop and the (vector) area of the loop.

Any configuration of charges or currents has a 'dipole moment', which describes the dipole whose field is the best approximation, at large distances, to that of the given configuration. This is simply one term in the multipole expansion when the total charge ("monopole moment") is 0—as it "always" is for the magnetic case, since there are no magnetic monopoles. The dipole term is the dominant one at large distances: Its field falls off in proportion to , as compared to for the next (quadrupole) term and higher powers of for higher terms, or for the monopole term.

Many molecules have such dipole moments due to non-uniform distributions of positive and negative charges on the various atoms. Such is the case with polar compounds like hydrogen fluoride (HF), where electron density is shared unequally between atoms. Therefore, a molecule's dipole is an electric dipole with an inherent electric field that should not be confused with a magnetic dipole, which generates a magnetic field.

The physical chemist Peter J. W. Debye was the first scientist to study molecular dipoles extensively, and, as a consequence, dipole moments are measured in the non-SI unit named "debye" in his honor.

For molecules there are three types of dipoles:

More generally, an induced dipole of "any" polarizable charge distribution "ρ" (remember that a molecule has a charge distribution) is caused by an electric field external to "ρ". This field may, for instance, originate from an ion or polar molecule in the vicinity of "ρ" or may be macroscopic (e.g., a molecule between the plates of a charged capacitor). The size of the induced dipole moment is equal to the product of the strength of the external field and the dipole polarizability of "ρ".

Dipole moment values can be obtained from measurement of the dielectric constant. Some typical gas phase values in debye units are:

Potassium bromide (KBr) has one of the highest dipole moments because it is an ionic compound that exists as a molecule in the gas phase.
The overall dipole moment of a molecule may be approximated as a vector sum of bond dipole moments. As a vector sum it depends on the relative orientation of the bonds, so that from the dipole moment information can be deduced about the molecular geometry.

For example, the zero dipole of CO implies that the two C=O bond dipole moments cancel so that the molecule must be linear. For HO the O−H bond moments do not cancel because the molecule is bent. For ozone (O) which is also a bent molecule, the bond dipole moments are not zero even though the O−O bonds are between similar atoms. This agrees with the Lewis structures for the resonance forms of ozone which show a positive charge on the central oxygen atom.

An example in organic chemistry of the role of geometry in determining dipole moment is the "cis" and "trans" isomers of 1,2-dichloroethene. In the "cis" isomer the two polar C−Cl bonds are on the same side of the C=C double bond and the molecular dipole moment is 1.90 D. In the "trans" isomer, the dipole moment is zero because the two C−Cl bonds are on opposite sides of the C=C and cancel (and the two bond moments for the much less polar C−H bonds also cancel).

Another example of the role of molecular geometry is boron trifluoride, which has three polar bonds with a difference in electronegativity greater than the traditionally cited threshold of 1.7 for ionic bonding. However, due to the equilateral triangular distribution of the fluoride ions centered on and in the same plane as the boron cation, the symmetry of the molecule results in its dipole moment being zero.

Consider a collection of "N" particles with charges "q" and position vectors r. For instance, this collection may be a molecule consisting of electrons, all with charge −"e", and nuclei with charge "eZ", where "Z" is the atomic number of the "i" th nucleus.
The dipole observable (physical quantity) has the quantum mechanical dipole operator:

Notice that this definition is valid only for neutral atoms or molecules, i.e. total charge equal to zero. In the ionized case, we have
where formula_3 is the center of mass of the molecule/group of particles.

A non-degenerate ("S"-state) atom can have only a zero permanent dipole. This fact follows quantum mechanically from the inversion symmetry of atoms. All 3 components of the dipole operator are antisymmetric under inversion with respect to the nucleus,

where formula_5 is the dipole operator and formula_6 is the inversion operator.

The permanent dipole moment of an atom in a non-degenerate state (see degenerate energy level) is given as the expectation (average) value of the dipole operator,

where formula_8 is an "S"-state, non-degenerate, wavefunction, which is symmetric or antisymmetric under inversion: formula_9. Since the product of the wavefunction (in the ket) and its complex conjugate (in the bra) is always symmetric under inversion and its inverse,

it follows that the expectation value changes sign under inversion. We used here the fact that formula_11, being a symmetry operator, is unitary: formula_12 and by definition the Hermitian adjoint formula_13 may be moved from bra to ket and then becomes formula_14. Since the only quantity that is equal to minus itself is the zero, the expectation value vanishes,

In the case of open-shell atoms with degenerate energy levels, one could define a dipole moment by the aid of the first-order Stark effect. This gives a non-vanishing dipole (by definition proportional to a non-vanishing first-order Stark shift) only if some of the wavefunctions belonging to the degenerate energies have opposite parity; i.e., have different behavior under inversion. This is a rare occurrence, but happens for the excited H-atom, where 2s and 2p states are "accidentally" degenerate (see article Laplace–Runge–Lenz vector for the origin of this degeneracy) and have opposite parity (2s is even and 2p is odd).

The far-field strength, "B", of a dipole magnetic field is given by

where

Conversion to cylindrical coordinates is achieved using and

where "ρ" is the perpendicular distance from the "z"-axis. Then,

The field itself is a vector quantity:

where

This is "exactly" the field of a point dipole, "exactly" the dipole term in the multipole expansion of an arbitrary field, and "approximately" the field of any dipole-like configuration at large distances.

The vector potential A of a magnetic dipole is

with the same definitions as above.

The electrostatic potential at position r due to an electric dipole at the origin is given by:

where p is the (vector) dipole moment, and "є" is the permittivity of free space.

This term appears as the second term in the multipole expansion of an arbitrary electrostatic potential Φ(r). If the source of Φ(r) is a dipole, as it is assumed here, this term is the only non-vanishing term in the multipole expansion of Φ(r). The electric field from a dipole can be found from the gradient of this potential:

This is of the same form of the expression for the magnetic field of a point magnetic dipole, ignoring the delta function. 
In a real electric dipole, however, the charges are physically separate and the electric field diverges or converges at the point charges. 
This is different to the magnetic field of a real magnetic dipole which is continuous everywhere. The delta function represents the strong field pointing in the opposite direction between the point charges, which is often omitted since one is rarely interested in the field at the dipole's position. 
For further discussions about the internal field of dipoles, see or Magnetic moment#Internal magnetic field of a dipole.

Since the direction of an electric field is defined as the direction of the force on a positive charge, electric field lines point away from a positive charge and toward a negative charge.

When placed in a homogeneous electric or magnetic field, equal but opposite forces arise on each side of the dipole creating a torque }:

for an electric dipole moment p (in coulomb-meters), or

for a magnetic dipole moment m (in ampere-square meters).

The resulting torque will tend to align the dipole with the applied field, which in the case of an electric dipole, yields a potential energy of

The energy of a magnetic dipole is similarly

In addition to dipoles in electrostatics, it is also common to consider an electric or magnetic dipole that is oscillating in time. It is an extension, or a more physical next-step, to spherical wave radiation.

In particular, consider a harmonically oscillating electric dipole, with angular frequency "ω" and a dipole moment "p" along the ẑ direction of the form

In vacuum, the exact field produced by this oscillating dipole can be derived using the retarded potential formulation as:

For  ≫ 1, the far-field takes the simpler form of a radiating "spherical" wave, but with angular dependence embedded in the cross-product:

The time-averaged Poynting vector

is not distributed isotropically, but concentrated around the directions lying perpendicular to the dipole moment, as a result of the non-spherical electric and magnetic waves. In fact, the spherical harmonic function (sin "θ") responsible for such toroidal angular distribution is precisely the "l" = 1 "p" wave.

The total time-average power radiated by the field can then be derived from the Poynting vector as

Notice that the dependence of the power on the fourth power of the frequency of the radiation is in accordance with the Rayleigh scattering, and the underlying effects why the sky consists of mainly blue colour.

A circular polarized dipole is described as a superposition of two linear dipoles.



Dynamics

Dynamics (from Greek δυναμικός "dynamikos" "powerful", from δύναμις "dynamis" "power") or dynamic may refer to:




Internet


Mathematics





Draught beer

Draught beer, also spelt draft, is beer served from a cask or keg rather than from a bottle or can. Draught beer served from a pressurised keg is also known as 

Until Joseph Bramah patented the beer engine in 1785, beer was served directly from the barrel and carried to the customer. The Old English "" ("carry; pull") developed into a series of related words including "drag", "draw", and "draught". By the time Bramah's beer pumps became popular, the use of the term "draught" to refer to the acts of serving or drinking beer was well established and transferred easily to beer served via the hand pumps. In time, the word came to be restricted to only such beer. The usual spelling is now "draught" in the United Kingdom, Ireland, Australia, and New Zealand and more commonly "draft" in North America, although it can be spelt either way. Regardless of spelling, the word is pronounced or depending on the region the speaker is from.

Canned draught is beer served from a pressurised container featuring a widget. Smooth flow (also known as cream flow, nitrokeg, or smooth) is the name brewers give to draught beers pressurised with a partial nitrogen gas blend.

In 1691, an article in the "London Gazette" mentioned John Lofting, who held a patent for a fire engine: "The said patentee has also projected a very useful engine for starting of beer, and other liquors which will draw from 20 to 30 barrels an hour, which are completely fixed with brass joints and screws at reasonable rates".

In the early 20th century, draught beer started to be served from pressurised containers. Artificial carbonation was introduced in the United Kingdom in 1936, with Watney's experimental pasteurised beer Red Barrel. Though this method of serving beer did not take hold in the UK until the late 1950s, it did become the favored method in the rest of Europe, where it is known by such terms as "en pression". The carbonation method of serving beer subsequently spread to the rest of the world; by the early 1970s the term "draught beer" almost exclusively referred to beer served under pressure as opposed to the traditional cask or barrel beer.

In Britain, the Campaign for Real Ale (CAMRA) was founded in 1971 to protect traditional—unpressurised—beer and brewing methods. The group devised the term "real ale" to differentiate between beer served from the cask and beer served under pressure. The term "real ale" has since been expanded to include bottle-conditioned beer.

Keg beer is often filtered and/or pasteurised, both of which are processes that render the yeast inactive.

In brewing parlance, a keg is different from a cask. A cask has a tap hole near the edge of the top, and a spile hole on the side used for conditioning the unfiltered and unpasteurised beer. A keg has a single opening in the centre of the top to which a flow pipe is attached. Kegs are artificially pressurised after fermentation with carbon dioxide or a mixture of carbon dioxide and nitrogen gas or especially in Czech Republic solely compressed air.

"Keg" has become a term of contempt used by some, particularly in the UK, since the 1960s when pasteurised draught beers started replacing traditional cask beers.

Keg beer was replacing traditional cask ale in all parts of the UK, primarily because it requires less care to handle. Since 1971, CAMRA has conducted a consumer campaign on behalf of those who prefer traditional cask beer. CAMRA has lobbied the British Parliament to ensure support for cask ale and microbreweries have sprung up to serve those consumers who prefer traditional cask beer.

Pressurised CO in the keg's headspace maintains carbonation in the beer. The CO pressure varies depending on the amount of CO already in the beer and the keg storage temperature. Occasionally the CO gas is blended with nitrogen gas. CO / nitrogen blends are used to allow a higher operating pressure in complex dispensing systems.

Nitrogen is used under high pressure when dispensing dry stouts (such as Guinness) and other creamy beers because it displaces CO to (artificially) form a rich tight head and a less carbonated taste. This makes the beer feel smooth on the palate and gives a foamy appearance. Premixed bottled gas for creamy beers is usually 75% nitrogen and 25% CO. This premixed gas, which only works well with creamy beers, is often referred to as Guinness Gas, Beer Gas, or Aligal (an Air Liquide brand name). Using "Beer Gas" with other beer styles can cause the last 5% to 10% of the beer in each keg to taste very flat and lifeless. In the UK, the term "keg beer" would imply the beer is pasteurised, in contrast to unpasteurised cask ale. Some of the newer microbreweries may offer a nitro keg stout which is filtered but not pasteurised.

Cask beer should be stored and served at a cellar temperature of . Once a cask is opened, it should be consumed within three days. Keg beer is given additional cooling just prior to being served either by flash coolers or a remote cooler in the cellar. This chills the beer to temperatures between .

The words "draft" and "draught" have been used as marketing terms to describe canned or bottled beers, implying that they taste and appear like beers from a cask or keg. Commercial brewers use this as a marketing tool although it is incorrect to call any beer not drawn from a cask or keg "draught". Two examples are Miller Genuine Draft, a pale lager which is produced using a cold filtering system, and Guinness stout in patented "Draught-flow" cans and bottles. Guinness is an example of beers that use a nitrogen widget to create a smooth beer with a dense head. Guinness has recently replaced the widget system from their bottled "draught" beer with a coating of cellulose fibres on the inside of the bottle. Statements indicate a new development in bottling technology that enables the mixture of nitrogen and carbon dioxide to be present in the beer without using a widget, making it according to Guinness "more drinkable" from the bottle.

In East Asian countries, such as China and Japan, the term "draft beer" () applied to canned or bottled beer indicates that the beer is not pasteurised (though it may be filtered), giving it a fresher taste but shorter shelf-life than conventional packaged beers.



Director

Director may refer to:








