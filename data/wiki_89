Louis Mountbatten, 1st Earl Mountbatten of Burma

Admiral of the Fleet Albert Victor Nicholas Louis Francis Mountbatten, 1st Earl Mountbatten of Burma (25 June 1900 – 27 August 1979) was a British statesman, naval officer, colonial administrator and close relative of the British royal family. He was born in the United Kingdom to the prominent Battenberg family. He was a maternal uncle of Prince Philip, Duke of Edinburgh, and a second cousin of King George VI. He joined the Royal Navy during the First World War and was appointed Supreme Allied Commander, South East Asia Command, in the Second World War. He later served as the last Viceroy of India and briefly as the first Governor-General of the Dominion of India.

Mountbatten attended the Royal Naval College, Osborne, before entering the Royal Navy in 1916. He saw action during the closing phase of the First World War, and after the war briefly attended Christ's College, Cambridge. During the interwar period, Mountbatten continued to pursue his naval career, specialising in naval communications.

Following the outbreak of the Second World War, Mountbatten commanded the destroyer and the 5th Destroyer Flotilla. He saw considerable action in Norway, in the English Channel, and in the Mediterranean. In August 1941, he received command of the aircraft carrier . He was appointed chief of Combined Operations and a member of the Chiefs of Staff Committee in early 1942, and organised the raids on St Nazaire and Dieppe. In August 1943, Mountbatten became Supreme Allied Commander South East Asia Command and oversaw the recapture of Burma and Singapore from the Japanese by the end of 1945. For his service during the war, Mountbatten was created viscount in 1946 and earl the following year.

In February 1947, Mountbatten was appointed Viceroy and Governor-General of India and oversaw the Partition of India into India and Pakistan. He then served as the first Governor-General of the Union of India until June 1948. In 1952, Mountbatten was appointed commander-in-chief of the British Mediterranean Fleet and NATO Commander Allied Forces Mediterranean. From 1955 to 1959, he was First Sea Lord, a position that had been held by his father, Prince Louis of Battenberg, some forty years earlier. Thereafter he served as chief of the Defence Staff until 1965, making him the longest-serving professional head of the British Armed Forces to date. During this period Mountbatten also served as chairman of the NATO Military Committee for a year.

In August 1979, Mountbatten was assassinated by a bomb planted aboard his fishing boat in Mullaghmore, County Sligo, Ireland, by members of the Provisional Irish Republican Army. He received a ceremonial funeral at Westminster Abbey and was buried in Romsey Abbey in Hampshire.

Mountbatten, then named Prince Louis of Battenberg, was born on 25 June 1900 at Frogmore House in the Home Park, Windsor, Berkshire. He was the youngest child and the second son of Prince Louis of Battenberg and his wife Princess Victoria of Hesse and by Rhine. Mountbatten's maternal grandparents were Louis IV, Grand Duke of Hesse, and Princess Alice of the United Kingdom, who was a daughter of Queen Victoria and Prince Albert of Saxe-Coburg and Gotha. His paternal grandparents were Prince Alexander of Hesse and by Rhine and Julia, Princess of Battenberg. Mountbatten's paternal grandparents' marriage was morganatic because his grandmother was not of royal lineage; as a result, he and his father were styled "Serene Highness" rather than "Grand Ducal Highness", were not eligible to be titled Princes of Hesse, and were given the less exalted Battenberg title. Mountbatten's elder siblings were Princess Alice of Battenberg (mother of Prince Philip, Duke of Edinburgh), Princess Louise of Battenberg (later Queen Louise of Sweden), and Prince George of Battenberg (later George Mountbatten, 2nd Marquess of Milford Haven).

Mountbatten was baptised in the large drawing room of Frogmore House on 17 July 1900 by the Dean of Windsor, Philip Eliot. His godparents were Queen Victoria (his maternal great-grandmother), Nicholas II of Russia (his maternal uncle through marriage and paternal second cousin, represented by the child's father) and Prince Francis Joseph of Battenberg (his paternal uncle, represented by Lord Edward Clinton). He wore the original 1841 royal christening gown at the ceremony.

Mountbatten's nickname among family and friends was "Dickie"; however "Richard" was not among his given names. This was because his great-grandmother, Queen Victoria, had suggested the nickname of "Nicky", but to avoid confusion with the many Nickys of the Russian Imperial Family ("Nicky" was particularly used to refer to Nicholas II, the last Tsar), "Nicky" was changed to "Dickie".

Mountbatten was educated at home for the first 10 years of his life; he was then sent to Lockers Park School in Hertfordshire and on to the Royal Naval College, Osborne, in May 1913.

Mountbatten's mother's younger sister was Russian Empress Alexandra Feodorovna. In childhood he visited the Imperial Court of Russia at St Petersburg and became intimate with the Russian Imperial Family, harbouring romantic feelings towards his maternal first cousin Grand Duchess Maria Nikolaevna, whose photograph he kept at his bedside for the rest of his life.

Mountbatten adopted his surname as a result of World War I. From 1914 to 1918, Britain and its allies were at war with the Central Powers, led by the German Empire. To appease British nationalist sentiment, in 1917 King George V issued a royal proclamation changing the name of the British royal house from the German House of Saxe-Coburg and Gotha to the House of Windsor. The king's British relatives with German names and titles followed suit with Mountbatten's father adopting the surname Mountbatten, an anglicization of Battenberg. The elder Mountbatten was subsequently created Marquess of Milford Haven.

At the age of 16, Mountbatten was posted as midshipman to the battlecruiser in July 1916 and, after seeing action in August 1916, transferred to the battleship during the closing phases of the First World War. In June 1917, when the royal family stopped using their German names and titles and adopted the more British-sounding "Windsor", Mountbatten acquired the courtesy title appropriate to a younger son of a marquess, becoming known as "Lord Louis Mountbatten" ("Lord Louis" for short) until he was created a peer in his own right in 1946. He paid a visit of ten days to the Western Front in July 1918.
While still an acting-sub-lieutenant, Mountbatten was appointed first lieutenant (second-in-command) of the P-class sloop HMS "P. 31" on 13 October 1918 and was confirmed as a substantive sub-lieutenant on 15 January 1919. HMS "P. 31" took part in the Peace River Pageant on 4 April 1919. Mountbatten attended Christ's College, Cambridge, for two terms, starting in October 1919, where he studied English literature (including John Milton and Lord Byron) in a programme designed to augment the education of junior officers which had been curtailed by the war. He was elected for a term to the Standing Committee of the Cambridge Union Society and was suspected of sympathy for the Labour Party, then emerging as a potential party of government for the first time.

Mountbatten was posted to the battlecruiser in March 1920 and accompanied Edward, Prince of Wales, on a royal tour of Australia in her. He was promoted lieutenant on 15 April 1920. HMS "Renown" returned to Portsmouth on 11 October 1920. Early in 1921 Royal Navy personnel were used for civil defence duties as serious industrial unrest seemed imminent. Mountbatten had to command a platoon of stokers, many of whom had never handled a rifle before, in Northern England. He transferred to the battlecruiser in March 1921 and accompanied the Prince of Wales on a Royal tour of India and Japan. Edward and Mountbatten formed a close friendship during the trip. Mountbatten survived the deep defence cuts known as the Geddes Axe. Fifty-two percent of the officers of his year had had to leave the Royal Navy by the end of 1923; although he was highly regarded by his superiors, it was rumoured that wealthy and well-connected officers were more likely to be retained. Mountbatten was posted to the battleship in the Mediterranean Fleet in January 1923.

Pursuing his interests in technological development and gadgetry, Mountbatten joined the Portsmouth Signals School in August 1924 and then went on briefly to study electronics at the Royal Naval College, Greenwich. Mountbatten became a Member of the Institution of Electrical Engineers (IEE), now the Institution of Engineering and Technology (IET). He was posted to the battleship in the Reserve Fleet in 1926 and became Assistant Fleet Wireless and Signals Officer of the Mediterranean Fleet under the command of Admiral Sir Roger Keyes in January 1927. Promoted lieutenant commander on 15 April 1928, Mountbatten returned to the Signals School in July 1929 as Senior Wireless Instructor. He was appointed Fleet Wireless Officer to the Mediterranean Fleet in August 1931 and, having been promoted commander on 31 December 1932, was posted to the battleship .

In 1934, Mountbatten was appointed to his first command – the destroyer . His ship was a new destroyer, which he was to sail to Singapore and exchange for an older ship, . He successfully brought "Wishart" back to port in Malta and then attended the Funeral of George V in January 1936. Mountbatten was appointed a personal naval aide-de-camp to King Edward VIII on 23 June 1936 and, having joined the Naval Air Division of the Admiralty in July 1936, he attended the coronation of King George VI and Queen Elizabeth in May 1937. Mountbatten was promoted captain on 30 June 1937 and was then given command of the destroyer in June 1939.

Within the Admiralty, Mountbatten was called "The Master of Disaster" for his penchant of getting into messes.

When war broke out in September 1939, Mountbatten became Captain (D) (commander) of the 5th Destroyer Flotilla aboard HMS "Kelly", which became famous for its exploits. In late 1939 he brought the Duke of Windsor back from exile in France and in early May 1940 Mountbatten led a British convoy in through the fog to evacuate the Allied forces participating in the Namsos Campaign during the Norwegian Campaign.

On the night of 9–10 May 1940, "Kelly" was torpedoed amidships by a German E-boat "S 31" off the Dutch coast, and Mountbatten thereafter commanded the 5th Destroyer Flotilla from the destroyer . On 29 November 1940 the 5th Flotilla engaged three German destroyers off Lizard Point, Cornwall. Mountbatten turned to port to match a German course change. This was "a rather disastrous move as the directors swung off and lost target" and it resulted in "Javelin" being struck by two torpedoes. He rejoined "Kelly" in December 1940, by which time the torpedo damage had been repaired.

"Kelly" was sunk by German dive bombers on 23 May 1941 during the Battle of Crete; the incident serving as the basis for Noël Coward's film "In Which We Serve". Coward was a personal friend of Mountbatten and copied some of his speeches into the film. Mountbatten was mentioned in despatches on 9 August 1940 and 21 March 1941 and awarded the Distinguished Service Order in January 1941.
In August 1941, Mountbatten was appointed captain of the aircraft carrier which lay in Norfolk, Virginia, for repairs following action at Malta in January. During this period of relative inactivity, he paid a flying visit to Pearl Harbor, three months before the Japanese attack on it. Mountbatten, appalled at the US naval base's lack of preparedness, drawing on Japan's history of launching wars with surprise attacks as well as the successful British surprise attack at the Battle of Taranto which had effectively knocked Italy's fleet out of the war, and the sheer effectiveness of aircraft against warships, accurately predicted that the US would enter the war after a Japanese surprise attack on Pearl Harbor.

Mountbatten was a favourite of Winston Churchill. On 27 October 1941, Mountbatten replaced Admiral of the Fleet Sir Roger Keyes as Chief of Combined Operations Headquarters and was promoted to commodore.

His duties in this role included inventing new technical aids to assist with opposed landings. Noteworthy technical achievements of Mountbatten and his staff include the construction of "PLUTO", an underwater oil pipeline to Normandy, an artificial Mulberry harbour constructed of concrete caissons and sunken ships, and the development of tank-landing ships. Another project Mountbatten proposed to Churchill was Project Habakkuk. It was to be an unsinkable 600-metre aircraft carrier made from reinforced ice ("Pykrete"): Habakkuk was never carried out due to its enormous cost.
As commander of Combined Operations, Mountbatten and his staff planned the highly successful Bruneval raid, which gained important information and captured part of a German Würzburg radar installation and one of the machine's technicians on 27 February 1942. It was Mountbatten who recognised that surprise and speed were essential to capture the radar, and saw that an airborne assault was the only viable method.

On 18 March 1942, he was promoted to the acting rank of vice admiral and given the honorary ranks of lieutenant general and air marshal to have the authority to carry out his duties in Combined Operations; and, despite the misgivings of General Sir Alan Brooke, the Chief of the Imperial General Staff, Mountbatten was placed in the Chiefs of Staff Committee. He was in large part responsible for the planning and organisation of the St Nazaire Raid on 28 March, which put out of action one of the most heavily defended docks in Nazi-occupied France until well after the war's end, the ramifications of which contributed to allied supremacy in the Battle of the Atlantic. After these two successes came the Dieppe Raid of 19 August 1942. He was central in the planning and promotion of the raid on the port of Dieppe. The raid was a marked failure, with casualties of almost 60%, the great majority of them Canadians. Following the Dieppe Raid, Mountbatten became a controversial figure in Canada, with the Royal Canadian Legion distancing itself from him during his visits there during his later career. His relations with Canadian veterans, who blamed him for the losses, "remained frosty" after the war.
Mountbatten claimed that the lessons learned from the Dieppe Raid were necessary for planning the Normandy invasion on D-Day nearly two years later. However, military historians such as Major-General Julian Thompson, a former member of the Royal Marines, have written that these lessons should not have needed a debacle such as Dieppe to be recognised. Nevertheless, as a direct result of the failings of the Dieppe Raid, the British made several innovations, most notably Hobart's Funnies – specialised armoured vehicles which, in the course of the Normandy Landings, undoubtedly saved many lives on those three beachheads upon which Commonwealth soldiers were landing (Gold Beach, Juno Beach and Sword Beach).

In August 1943, Churchill appointed Mountbatten the Supreme Allied Commander South East Asia Command (SEAC) with promotion to acting full admiral. His less practical ideas were sidelined by an experienced planning staff led by Lieutenant-Colonel James Allason, though some, such as a proposal to launch an amphibious assault near Rangoon, got as far as Churchill before being quashed.

British interpreter Hugh Lunghi recounted an embarrassing episode during the Potsdam Conference when Mountbatten, desiring to receive an invitation to visit the Soviet Union, repeatedly attempted to impress Joseph Stalin with his former connections to the Russian imperial family. The attempt fell predictably flat, with Stalin dryly inquiring whether "it was some time ago that he had been there". Says Lunghi, "The meeting was embarrassing because Stalin was so unimpressed. He offered no invitation. Mountbatten left with his tail between his legs."

During his time as Supreme Allied Commander of the Southeast Asia Theatre, his command oversaw the recapture of Burma from the Japanese by General Sir William Slim. A personal high point was the receipt of the Japanese surrender in Singapore when British troops returned to the island to receive the formal surrender of Japanese forces in the region led by General Itagaki Seishiro on 12 September 1945, codenamed Operation Tiderace. South East Asia Command was disbanded in May 1946 and Mountbatten returned home with the substantive rank of rear-admiral. That year, he was made a Knight Companion of the Garter and created Viscount Mountbatten of Burma, of Romsey in the County of Southampton, as a victory title for war service. He was then in 1947 further created Earl Mountbatten of Burma and Baron Romsey, of Romsey in the County of Southampton.

Following the war, Mountbatten was known to have largely shunned the Japanese for the rest of his life out of respect for his men killed during the war and, as per his will, Japan was not invited to send diplomatic representatives to his funeral in 1979, though he did meet Emperor Hirohito during his state visit to Britain in 1971, reportedly at the urging of the Queen.

Mountbatten's experience in the region and in particular his perceived Labour sympathies at that time led to Clement Attlee advising King George VI to appoint him Viceroy of India on 20 February 1947 charged with overseeing the transition of British India to independence no later than 30 June 1948. Mountbatten's instructions were to avoid partition and preserve a united India as a result of the transfer of power but authorised him to adapt to a changing situation in order to get Britain out promptly with minimal reputational damage. Mountbatten arrived in India on 22 March by air, from London. In the evening, he was taken to his residence and two days later, he took the Viceregal oath. His arrival saw large-scale communal riots in Delhi, Bombay and Rawalpindi. Mountbatten concluded that the situation was too volatile to wait even a year before granting independence to India. Although his advisers favoured a gradual transfer of independence, Mountbatten decided the only way forward was a quick and orderly transfer of power before 1947 was out. In his view, any longer would mean civil war. Mountbatten also hurried so he could return to his senior technical Navy courses.
Mountbatten was fond of Congress leader Jawaharlal Nehru and his liberal outlook for the country. He felt differently about the Muslim League leader Muhammad Ali Jinnah, but was aware of his power, stating "If it could be said that any single man held the future of India in the palm of his hand in 1947, that man was Mohammad Ali Jinnah." During his meeting with Jinnah on 5 April 1947, Mountbatten tried to persuade him of a united India, citing the difficult task of dividing the mixed states of Punjab and Bengal, but the Muslim leader was unyielding in his goal of establishing a separate Muslim state called Pakistan.
Given the British government's recommendations to grant independence quickly, Mountbatten concluded that a united India was an unachievable goal and resigned himself to a plan for partition, creating the independent nations of India and Pakistan. Mountbatten set a date for the transfer of power from the British to the Indians, arguing that a fixed timeline would convince Indians of his and the British government's sincerity in working towards a swift and efficient independence, excluding all possibilities of stalling the process.
Among the Indian leaders, Mahatma Gandhi emphatically insisted on maintaining a united India and for a while successfully rallied people to this goal. During his meeting with Mountbatten, Gandhi asked Mountbatten to invite Jinnah to form a new central government, but Mountbatten never uttered a word of Gandhi's ideas to Jinnah. When Mountbatten's timeline offered the prospect of attaining independence soon, sentiments took a different turn. Given Mountbatten's determination, Nehru and Sardar Patel's inability to deal with the Muslim League and, lastly, Jinnah's obstinacy, all Indian party leaders (except Gandhi) acquiesced to Jinnah's plan to divide India, which in turn eased Mountbatten's task. Mountbatten also developed a strong relationship with the Indian princes, who ruled those portions of India not directly under British rule. His intervention was decisive in persuading the vast majority of them to see advantages in opting to join the Indian Union. On one hand, the integration of the princely states can be viewed as one of the positive aspects of his legacy but on the other, the refusal of Hyderabad, Jammu and Kashmir, and Junagadh to join one of the dominions led to future wars between Pakistan and India.

Mountbatten brought forward the date of the partition from June 1948 to 15 August 1947. The uncertainty of the borders caused Muslims and Hindus to move into the direction where they felt they would get the majority. Hindus and Muslims were thoroughly terrified, and the Muslim movement from the East was balanced by the similar movement of Hindus from the West. A boundary committee chaired by Sir Cyril Radcliffe was charged with drawing boundaries for the new nations. With a mandate to leave as many Hindus and Sikhs in India and as many Muslims in Pakistan as possible, Radcliffe came up with a map that split the two countries along the Punjab and Bengal borders. This left 14 million people on the "wrong" side of the border, and very many of them fled to "safety" on the other side when the new lines were announced.

When India and Pakistan attained independence at midnight of 14–15 August 1947, Mountbatten was alone in his study at the Viceroy's house saying to himself just before the clock struck midnight that for still a few minutes, he was the most powerful man on Earth. At 12 am, as a last act of showmanship, he created Joan Falkiner, the Australian wife of the Nawab of Palanpur, a highness, an act that was apparently one of his favourite duties that was annulled at the stroke of midnight.

Mountbatten remained in New Delhi for 10 months, serving as the first governor-general of an independent India until June 1948. On Mountbatten's advice, India took the issue of Kashmir to the newly formed United Nations in January 1948. This issue would become a lasting thorn in his legacy and one that is not resolved to this day. Accounts differ on the future which Mountbatten desired for Kashmir. Pakistani accounts suggest that Mountbatten favoured the accession of Kashmir to India, citing his close relationship to Nehru. Mountbatten's own account says that he simply wanted Maharaja Hari Singh, to make up his mind. The viceroy made several attempts to mediate between the Congress leaders, Muhammad Ali Jinnah and Hari Singh on issues relating to the accession of Kashmir, though he was largely unsuccessful in resolving the conflict. After the tribal invasion of Kashmir, it was on his suggestion that India moved to secure the accession of Kashmir from Hari Singh before sending in military forces for his defence.
Notwithstanding the self-promotion of his own part in Indian independence – notably in the television series "The Life and Times of Admiral of the Fleet Lord Mountbatten of Burma", produced by his son-in-law Lord Brabourne, and "Freedom at Midnight" by Dominique Lapierre and Larry Collins (of which he was the main quoted source) – his record is seen as very mixed. One common view is that he hastened the process of independence unduly and recklessly, foreseeing vast disruption and loss of life and not wanting this to occur on his watch, but thereby actually helping it to occur (albeit in an indirect manner), especially in Punjab and Bengal. John Kenneth Galbraith, the Canadian-American Harvard University economist, who advised governments of India during the 1950s and was an intimate of Nehru who served as the American ambassador from 1961 to 1963, was a particularly harsh critic of Mountbatten in this regard. However, another view is that the British were forced to expedite the partition process to avoid involvement in a potential civil war with law and order having already broken down and Britain with limited resources after the Second World War. According to historian Lawrence James, Mountbatten was left with no other option but to cut and run, with the alternative being involvement in a potential civil war that would be difficult to get out of.

The creation of Pakistan was never emotionally accepted by many British leaders, among them Mountbatten. Mountbatten clearly expressed his lack of support and faith in the Muslim League's idea of Pakistan. Jinnah refused Mountbatten's offer to serve as Governor-General of Pakistan. When Mountbatten was asked by Collins and Lapierre if he would have sabotaged the creation of Pakistan had he known that Jinnah was dying of tuberculosis, he replied, "Most probably". 

After his tenure as Governor-General concluded, Mountbatten continued to enjoy close relations with Nehru and the post-Independence Indian leadership, and was welcomed as a former governor-general of India on subsequent visits to the country, including during an official trip in March 1956. The Pakistani government, by contrast, never forgave Mountbatten for his perceived hostile attitude towards Pakistan and deemed him "Persona non grata", barring him from transiting their airspace during the same visit.

After India, Mountbatten served as commander of the 1st Cruiser Squadron in the Mediterranean Fleet and, having been granted the substantive rank of vice-admiral on 22 June 1949, he became Second-in-Command of the Mediterranean Fleet in April 1950. He became Fourth Sea Lord at the Admiralty in June 1950. He then returned to the Mediterranean to serve as Commander-in-Chief, Mediterranean Fleet and NATO Commander Allied Forces Mediterranean from June 1952. He was promoted to the substantive rank of full admiral on 27 February 1953. In March 1953, he was appointed Personal Aide-de-Camp to the Queen.
Mountbatten served his final posting at the Admiralty as First Sea Lord and Chief of the Naval Staff from April 1955 to July 1959, the position which his father had held some forty years before. This was the first time in Royal Naval history that a father and son had both attained such high office. He was promoted to Admiral of the Fleet on 22 October 1956.

In the Suez Crisis of 1956, Mountbatten strongly advised his old friend Prime Minister Anthony Eden against the Conservative government's plans to seize the Suez Canal in conjunction with France and Israel. He argued that such a move would destabilize the Middle East, undermine the authority of the United Nations, divide the Commonwealth and diminish Britain's global standing. His advice was not taken. Eden insisted that Mountbatten not resign. Instead, he worked hard to prepare the Royal Navy for war with characteristic professionalism and thoroughness.

Despite his military rank, Mountbatten was ignorant as to the physics involved in a nuclear explosion and had to be reassured that the fission reactions from the Bikini Atoll tests would not spread through the oceans and blow up the planet. As Mountbatten became more familiar with this new form of weaponry, he increasingly grew opposed to its use in combat. Yet, he realised the potential for nuclear energy, especially with regard to submarines. Mountbatten expressed his feelings towards the use of nuclear weapons in combat in his article "A Military Commander Surveys The Nuclear Arms Race", which was published shortly after his death in "International Security" in the Winter of 1979–1980.

After leaving the Admiralty, Mountbatten took the position of Chief of the Defence Staff. He served in this post for six years during which he was able to consolidate the three service departments of the military branch into a single Ministry of Defence. Ian Jacob, co-author of the 1963 "Report on the Central Organisation of Defence" that served as the basis of these reforms, described Mountbatten as "universally mistrusted in spite of his great qualities". On their election in October 1964, the Wilson ministry had to decide whether to renew his appointment the following July. The Defence Secretary, Denis Healey, interviewed the forty most senior officials in the Ministry of Defence; only one, Sir Kenneth Strong, a personal friend of Mountbatten, recommended his reappointment. "When I told Dickie of my decision not to reappoint him," recalls Healey, "he slapped his thigh and roared with delight; but his eyes told a different story."

Mountbatten was appointed Colonel of The Life Guards and Gold Stick in Waiting on 29 January 1965 and Life Colonel Commandant of the Royal Marines the same year. He was Governor of the Isle of Wight from 20 July 1965 and then the first Lord Lieutenant of the Isle of Wight from 1 April 1974.

Mountbatten was elected a Fellow of the Royal Society and had received an honorary doctorate from Heriot-Watt University in 1968.

In 1969, Mountbatten tried unsuccessfully to persuade his second cousin, the Spanish pretender Infante Juan, Count of Barcelona, to ease the eventual accession of his son, Juan Carlos, to the Spanish throne by signing a declaration of abdication while in exile. The next year Mountbatten attended an official White House dinner during which he took the opportunity to have a 20-minute conversation with Richard Nixon and Secretary of State William P. Rogers, about which he later wrote, "I was able to talk to the President a bit about both Tino [Constantine II of Greece] and Juanito [Juan Carlos of Spain] to try and put over their respective points of view about Greece and Spain, and how I felt the US could help them." In January 1971, Nixon hosted Juan Carlos and his wife Sofia (sister of the exiled King Constantine) during a visit to Washington and later that year "The Washington Post" published an article alleging that Nixon's administration was seeking to persuade Franco to retire in favour of the young Bourbon prince.

From 1967 until 1978, Mountbatten was president of the United World Colleges Organisation, then represented by a single college: that of Atlantic College in South Wales. Mountbatten supported the United World Colleges and encouraged heads of state, politicians, and personalities throughout the world to share his interest. Under his presidency and personal involvement, the United World College of South East Asia was established in Singapore in 1971, followed by the United World College of the Pacific in Victoria, British Columbia, in 1974. In 1978, Mountbatten passed the presidency of the college to his great-nephew, Charles, Prince of Wales.

Mountbatten also helped to launch the International Baccalaureate; in 1971 he presented the first IB diplomas in the Greek Theatre of the International School of Geneva, Switzerland.

In 1975 Mountbatten finally visited the Soviet Union, leading the delegation from UK as personal representative of Queen Elizabeth II at the celebrations to mark the 30th anniversary of Victory Day in Second World War in Moscow.

Peter Wright, in his 1987 book "Spycatcher", claimed that in May 1968 Mountbatten attended a private meeting with press baron Cecil King and the government's Chief Scientific Adviser, Solly Zuckerman. Wright alleged that "up to thirty" MI5 officers had joined a secret campaign to undermine the crisis-stricken Labour government of Harold Wilson and that King was an MI5 agent. In the meeting, King allegedly urged Mountbatten to become the leader of a government of national salvation. Solly Zuckerman pointed out that it was "rank treachery" and the idea came to nothing because of Mountbatten's reluctance to act. In contrast, Andrew Lownie has suggested that it took the intervention of the Queen to dissuade Mountbatten from plotting against Wilson.

In 2006, the BBC documentary "The Plot Against Harold Wilson" alleged that there had been another plot involving Mountbatten to oust Wilson during his second term in office (1974–1976). The period was characterised by high inflation, increasing unemployment, and widespread industrial unrest. The alleged plot revolved around right-wing former military figures who were supposedly building private armies to counter the perceived threat from trade unions and the Soviet Union. They believed that the Labour Party was unable and unwilling to counter these developments and that Wilson was either a Soviet agent or at the very least a Communist sympathiser – claims Wilson strongly denied. The documentary makers alleged that a coup was planned to overthrow Wilson and replace him with Mountbatten using the private armies and sympathisers in the military and MI5.

The first official history of MI5, "The Defence of the Realm" (2009), implied that there was a plot against Wilson and that MI5 did have a file on him. Yet it also made clear that the plot was in no way official and that any activity centred on a small group of discontented officers. This much had already been confirmed by former cabinet secretary Lord Hunt, who concluded in a secret inquiry conducted in 1996 that "there is absolutely no doubt at all that a few, a very few, malcontents in MI5 ... a lot of them like Peter Wright who were right-wing, malicious and had serious personal grudges – gave vent to these and spread damaging malicious stories about that Labour government."

Mountbatten was married on 18 July 1922 to Edwina Cynthia Annette Ashley, daughter of Wilfred William Ashley, later 1st Baron Mount Temple, himself a grandson of the 7th Earl of Shaftesbury. She was the favourite granddaughter of the Edwardian magnate Sir Ernest Cassel and the principal heir to his fortune. The couple spent heavily on households, luxuries, and entertainment. There followed a honeymoon tour of European royal courts and North America which included a visit to Niagara Falls (because "all honeymooners went there"). During their honeymoon in California, the newlyweds starred in a silent home movie by Charlie Chaplin called "Nice And Friendly", which was not shown in cinemas.

Mountbatten admitted: "Edwina and I spent all our married lives getting into other people's beds." He maintained an affair for several years with Yola Letellier, the wife of Henri Letellier, publisher of "Le Journal" and mayor of Deauville (1925–28). Yola Letellier's life story was the inspiration for Colette's novel "Gigi".

After Edwina died in 1960, Mountbatten was involved in relationships with young women, according to his daughter Patricia, his secretary John Barratt, his valet Bill Evans, and William Stadiem, an employee of Madame Claude. He had a long-running affair with American actress Shirley MacLaine, whom he met in the 1960s.

Ron Perks, Mountbatten's driver in Malta in 1948, alleged that he used to visit the Red House, an upmarket gay brothel in Rabat used by naval officers. Andrew Lownie, a fellow of the Royal Historical Society, wrote that the United States Federal Bureau of Investigation (FBI) maintained files regarding Mountbatten's alleged homosexuality. Lownie also interviewed several young men who claimed to have been in a relationship with Mountbatten. John Barratt, Mountbatten's personal and private secretary for 20 years, has said Mountbatten was not a homosexual, and that it would have been impossible for such a fact to have been hidden from him.

In 2019, files became public showing that the FBI knew in the 1940s of allegations that Mountbatten was homosexual and a paedophile. The FBI file on Mountbatten, begun after he took on the role of Supreme Allied Commander in Southeast Asia in 1944, describes Mountbatten and his wife Edwina as "persons of extremely low morals", and contains a claim by American author Elizabeth, Baroness Decies, that Mountbatten was known to be a homosexual and had "a perversion for young boys". Norman Nield, Mountbatten's driver from 1942 to 1943, told the tabloid "New Zealand Truth" that he transported young boys aged 8 to 12 who had been procured for the Admiral to Mountbatten's official residence and was paid to keep quiet. Robin Bryans had also claimed to the Irish magazine "Now" that Mountbatten and Anthony Blunt, along with others, were part of a ring that engaged in homosexual orgies and procured boys in their first year at public schools such as the Portora Royal School in Enniskillen. Former residents of the Kincora Boys' Home in Belfast have asserted that they were trafficked to Mountbatten at Classiebawn Castle, his residence in Mullaghmore, County Sligo. These claims were dismissed by the Historical Institution Abuse (HIA) Inquiry. The HIA stated that the article making the original allegations "did not give any basis for the assertions that any of these people [Mountbatten and others] were connected with Kincora".

In October 2022 Arthur Smyth, a former resident of Kincora, waived his anonymity to make allegations of child abuse against Mountbatten. The allegations are part of a civil case against state authorities responsible for the care of children in Kincora.

Lord and Lady Mountbatten had two daughters: Patricia Knatchbull (14 February 1924 – 13 June 2017), sometime lady-in-waiting to Queen Elizabeth II, and Lady Pamela Hicks (born 19 April 1929), who accompanied them to India in 1947–1948 and was also sometime lady-in-waiting to the Queen.

Since Mountbatten had no sons when he was created Viscount Mountbatten of Burma, of Romsey in the County of Southampton on 27 August 1946 and then Earl Mountbatten of Burma and Baron Romsey, in the County of Southampton on 18 October 1947, the Letters Patent were drafted such that in the event he left no sons or issue in the male line, the titles could pass to his daughters, in order of seniority of birth.

Mountbatten was passionate about genealogy, an interest he shared with other European royalty and nobility; according to Ziegler, he spent a great deal of his leisure time in studying his links with European royal houses. From 1957 until his death, Lord Mountbatten was Patron of the Cambridge University Heraldic and Genealogical Society. He was equally passionate about orders, decorations and military ranks and uniforms, though he himself considered this interest to be a sign of vanity and constantly tried to distance himself from it, with limited success. Over the course of his career, he consistently attempted to secure as many orders and decorations as possible. Particular about details of dress, Mountbatten took an interest in fashion design, introducing trouser zips, a tail-coat with broad, high lapels and a "buttonless waistcoat" that could be pulled on over the head. In 1949, having by then relinquished the office of Governor-General of India but retaining a keen interest in Indian affairs, he designed new flags, insignia, and details of uniforms for the Indian Armed Forces ahead of the transition from British dominion to republic; many of his designs were implemented and remain in use.

Like many members of the royal family, Mountbatten was an aficionado of polo. Mountbatten introduced the sport to the Royal Navy in the 1920s and wrote a book on the subject. He received US patent 1,993,334 in 1931 for a polo stick. He also served as Commodore of Emsworth Sailing Club in Hampshire from 1931. He was a long-serving Patron of the Society for Nautical Research (1951–1979). Apart from official documents, Mountbatten was not much of a reader, though he liked P. G. Wodehouse's books. He enjoyed the cinema; his favourite stars were Fred Astaire, Rita Hayworth, Grace Kelly and Shirley MacLaine. In general, however, he had a limited interest in the arts.

Mountbatten was a strong influence in the upbringing of his great-nephew, the future King Charles III, and later as a mentor – "Honorary Grandfather" and "Honorary Grandson", they fondly called each other according to the Jonathan Dimbleby biography of the then-Prince – though according to both the Ziegler biography of Mountbatten and the Dimbleby biography of the Prince, the results may have been mixed. He from time to time strongly upbraided the Prince for showing tendencies towards the idle pleasure-seeking dilettantism of his predecessor as Prince of Wales, King Edward VIII, whom Mountbatten had known well in their youth. Yet he also encouraged the Prince to enjoy the bachelor life while he could, and then to marry a young and inexperienced girl so as to ensure a stable married life.

Mountbatten's qualification for offering advice to this particular heir to the throne was unique; it was he who had arranged the visit of King George VI and Queen Elizabeth to Dartmouth Royal Naval College on 22 July 1939, taking care to include the young Princesses Elizabeth and Margaret in the invitation, but assigning his nephew, Cadet Prince Philip of Greece, to keep them amused while their parents toured the facility. This was the first recorded meeting of Charles's future parents but a few months later, Mountbatten's efforts nearly came to naught when he received a letter from his sister Alice in Athens informing him that Philip was visiting her and had agreed to repatriate permanently to Greece. Within days, Philip received a command from his cousin and sovereign, King George II of Greece, to resume his naval career in Britain which, though given without explanation, the young prince obeyed.

In 1974, Mountbatten began corresponding with Charles about a potential marriage to his granddaughter, Amanda Knatchbull, who was also Charles's second cousin. It was about this time he also recommended that the 25-year-old prince get on with "sowing some wild oats". Charles dutifully wrote to Amanda's mother (who was also his godmother and his father's first cousin), Lady Brabourne, about his interest. Her answer was supportive, but advised him that she thought her daughter still rather young to be courted.

In February 1975, Charles visited New Delhi to play polo and was shown around Rashtrapati Bhavan, the former Viceroy's House, by Mountbatten.

Four years later, Mountbatten secured an invitation for himself and Amanda to accompany Charles on his planned 1980 tour of India. Their fathers promptly objected. Prince Philip thought that the Indian public's reception would more likely reflect their response to the uncle than to the nephew. Lord Brabourne counselled that the intense scrutiny of the press would be more likely to drive Mountbatten's godson and granddaughter apart than together.

Charles was rescheduled to tour India alone, but Mountbatten did not live to the planned date of departure. When Charles finally did propose marriage to Amanda later in 1979, the circumstances were changed and she refused him.

On 27 April 1977, shortly before his 77th birthday, Mountbatten became the first member of the Royal Family to appear on the TV guest show "This Is Your Life". In the UK, 22.22 million people tuned in to watch the programme.

Mountbatten usually holidayed at his summer home, Classiebawn Castle, on the Mullaghmore Peninsula in County Sligo, in the north-west of Ireland. The village was only from the border with County Fermanagh in Northern Ireland and near an area known to be used as a cross-border refuge by IRA members. In 1978, the IRA had allegedly attempted to shoot Mountbatten as he was aboard his boat, but poor weather had prevented the sniper taking his shot.

On 27 August 1979, Mountbatten went lobster-potting and tuna fishing in his wooden boat, "Shadow V", which had been moored in the harbour at Mullaghmore. IRA member Thomas McMahon had slipped onto the unguarded boat the previous night and attached a radio-controlled bomb weighing . When Mountbatten and his party had taken the boat just a few hundred yards from the shore, the bomb was detonated. The boat was destroyed by the force of the blast and Mountbatten's legs were almost blown off. Mountbatten, then aged 79, was pulled alive from the water by nearby fishermen, but died from his injuries before being brought to shore.

Also aboard the boat were his elder daughter Patricia, Lady Brabourne; her husband Lord Brabourne; their twin sons Nicholas and Timothy Knatchbull; Lord Brabourne's mother Doreen, Dowager Lady Brabourne; and Paul Maxwell, a young crew member from Enniskillen in County Fermanagh. Nicholas (aged 14) and Paul (aged 15) were killed by the blast and the others were seriously injured. Doreen, Dowager Lady Brabourne (aged 83), died from her injuries the following day.

The attack triggered outrage and condemnation around the world. Queen Elizabeth II received messages of condolence from leaders including US President Jimmy Carter and Pope John Paul II. Carter expressed his "profound sadness" at the death. The Irish American community was disgusted with the attack, especially since many American soldiers served under Mountbatten during World War II. Jim Rooney, son of Pittsburgh Steelers president Dan M. Rooney (who co-founded The Ireland Funds in 1976), recalled that:Mountbatten's murder shocked many Irish-Americans, my parents included, because they remembered him for the role he played in defeating the Axis. "It was quite sad because being in America, you were familiar with Lord Mountbatten because of World War II," my mother recalled. "It was a very sad time." But my father didn't give in to despair. "That didn't show down [my father] one bit. It more or less gave him more energy," my mother said.

Prime Minister Margaret Thatcher said:His death leaves a gap that can never be filled. The British people give thanks for his life and grieve at his passing.

George Colley, the "Tánaiste" (Deputy head of the Government of Ireland), said:No effort will be spared to bring those responsible to justice. It is understood that subversives have claimed responsibility for the explosion. Assuming that police investigations substantiate the claim, I know that the Irish people will join me in condemning this heartless and terrible outrage.

The IRA issued a statement afterward, saying:The IRA claim responsibility for the execution of Lord Louis Mountbatten. This operation is one of the discriminate ways we can bring to the attention of the English people the continuing occupation of our country. ... The death of Mountbatten and the tributes paid to him will be seen in sharp contrast to the apathy of the British Government and the English people to the deaths of over three hundred British soldiers, and the deaths of Irish men, women, and children at the hands of their forces.

Six weeks later, Sinn Féin vice-president Gerry Adams said of Mountbatten's death:The IRA gave clear reasons for the execution. I think it is unfortunate that anyone has to be killed, but the furor created by Mountbatten's death showed up the hypocritical attitude of the media establishment. As a member of the House of Lords, Mountbatten was an emotional figure in both British and Irish politics. What the IRA did to him is what Mountbatten had been doing all his life to other people; and with his war record I don't think he could have objected to dying in what was clearly a war situation. He knew the danger involved in coming to this country. In my opinion, the IRA achieved its objective: people started paying attention to what was happening in Ireland.

In 2015, Adams said in an interview, "I stand over what I said then. I'm not one of those people that engages in revisionism. Thankfully the war is over."

On the day of the bombing, the IRA also ambushed and killed eighteen British soldiers at the gates of Narrow Water Castle, just outside Warrenpoint, in County Down in Northern Ireland, sixteen of them from the Parachute Regiment, in what became known as the Warrenpoint ambush. It was the deadliest attack on the British Army during the Troubles.

On 5 September 1979, Mountbatten received a ceremonial funeral at Westminster Abbey, which was attended by Queen Elizabeth II, the royal family, and members of the European royal houses. Watched by thousands of people, the funeral procession, which started at Wellington Barracks, included representatives of all three British Armed Services, and military contingents from Burma, India, the United States (represented by 70 sailors of the US Navy and 50 US Marines), France (represented by the French Navy) and Canada. His coffin was drawn on a gun carriage by 118 Royal Navy ratings. During the televised service, his great-nephew Charles read the lesson from Psalm 107. In an address, the Archbishop of Canterbury, Donald Coggan, highlighted his various achievements and his "lifelong devotion to the Royal Navy". After the public ceremonies, which he had planned himself, Mountbatten was buried in Romsey Abbey. As part of the funeral arrangements, his body had been embalmed by Desmond Henley.

Two hours before the bomb detonated, Thomas McMahon had been arrested at a Garda checkpoint between Longford and Granard on suspicion of driving a stolen vehicle. He was tried for the assassinations in Ireland and convicted on 23 November 1979 based on forensic evidence supplied by James O'Donovan that showed flecks of paint from the boat and traces of nitroglycerine on his clothes. He was released in 1998 under the terms of the Good Friday Agreement.

On hearing of Mountbatten's death, the then Master of the Queen's Music, Malcolm Williamson, wrote the "Lament in Memory of Lord Mountbatten of Burma" for violin and string orchestra. The 11-minute work was given its first performance on 5 May 1980 by the Scottish Baroque Ensemble, conducted by Leonard Friedman.

On his death his estate was valued for probate purposes at £2,196,494 ().

Mountbatten's faults, according to his biographer Philip Ziegler, like everything else about him, "were on the grandest scale. His vanity though child-like, was monstrous, his ambition unbridled ... He sought to rewrite history with cavalier indifference to the facts to magnify his own achievements." However, Ziegler concludes that Mountbatten's virtues outweighed his defects:
He was generous and loyal ... He was warm-hearted, predisposed to like everyone he met, quick-tempered but never bearing grudges ... His tolerance was extraordinary; his readiness to respect and listen to the views of others was remarkable throughout his life.

Ziegler argues he was truly a great man, although not profound or original.
What he could do with superlative aplomb was to identify the object at which he was aiming, and force it through to its conclusion. A powerful, analytic mind of crystalline clarity, a superabundance of energy, great persuasive powers, endless resilience in the face of setback or disaster rendered him the most formidable of operators. He was infinitely resourceful, quick in his reactions, always ready to cut his losses and start again ... He was an executor of policy rather than an initiator; but whatever the policy, he espoused it with such energy and enthusiasm, made it so completely his own, that it became identified with him and, in the eyes of the outside world as well as his own, his creation.

Others were not so conflicted. Field Marshal Sir Gerald Templer, the former Chief of the Imperial General Staff, once told him, "You are so crooked, Dickie, that if you swallowed a nail, you would shit a corkscrew".

Mountbatten supported the burgeoning nationalist movements which grew up in the shadow of Japanese occupation. His priority was to maintain practical, stable government, but driving him was an idealism in which he believed every people should be allowed to control their own destiny. Critics said he was too ready to overlook their faults, and especially their subordination to communist control. Ziegler says that in Malaya, where the main resistance to the Japanese came from Chinese who were under considerable communist influence, "Mountbatten proved to have been naïve in his assessment. ... He erred, however, not because he was 'soft on Communism' ... but from an over-readiness to assume the best of those with whom he had dealings." Furthermore, Ziegler argues, he was following a practical policy based on the assumption that it would take a long and bloody struggle to drive the Japanese out, and he needed the support of all the anti-Japanese elements, most of which were either nationalists or communists.

Mountbatten took pride in enhancing intercultural understanding and in 1984, with his elder daughter as the patron, the Mountbatten Institute was developed to allow young adults the opportunity to enhance their intercultural appreciation and experience by spending time abroad. The IET annually awards the Mountbatten Medal for an outstanding contribution, or contributions over a period, to the promotion of electronics or information technology and their application.

Canada's capital city of Ottawa named Mountbatten Avenue in his memory. The Mountbatten estate in Singapore and Mountbatten MRT station were named after him.

Mountbatten's personal papers (containing approximately 250,000 papers and 50,000 photographs) are preserved in the University of Southampton Library.

He was appointed personal aide-de-camp by Edward VIII, George VI and Elizabeth II, and therefore bore the unusual distinction of being allowed to wear three royal cyphers on his epaulettes.




Elbridge Gerry

Elbridge Gerry (; July 17, 1744 – November 23, 1814) was an American Founding Father, merchant, politician, and diplomat who served as the fifth vice president of the United States under President James Madison from 1813 until his death in 1814. The political practice of gerrymandering is named after him. 

Born into a wealthy merchant family, Gerry vocally opposed British colonial policy in the 1760s and was active in the early stages of organizing the resistance in the American Revolutionary War. Elected to the Second Continental Congress, Gerry signed both the Declaration of Independence and Articles of Confederation. He was one of three men who attended the Constitutional Convention in 1787, but refused to sign the Constitution because originally it did not include a Bill of Rights. After its ratification, he was elected to the inaugural United States Congress, where he was actively involved in the drafting and passage of the Bill of Rights as an advocate of individual and state liberties.

Gerry was at first opposed to the idea of political parties and cultivated enduring friendships on both sides of the political divide between Federalists and Democratic-Republicans. He was a member of a diplomatic delegation to France that was treated poorly in the XYZ Affair, in which Federalists held him responsible for a breakdown in negotiations. Gerry thereafter became a Democratic-Republican, running unsuccessfully for Governor of Massachusetts several times before winning the office in 1810. During his second term, the legislature approved new state senate districts that led to the coining of the word "gerrymander"; he lost the next election, although the state senate remained Democratic-Republican. Gerry was nominated by the Democratic-Republican party and elected as vice president in the 1812 election. Advanced in age and in poor health, Gerry served 21 months of his term before dying in office. Gerry is the only signatory of the Declaration of Independence to be buried in Washington, D.C.

Gerry was born on July 17, 1744, in the North Shore town of Marblehead, Massachusetts. His father, Thomas Gerry, was a merchant who operated ships out of Marblehead, and his mother, Elizabeth (Greenleaf) Gerry, was the daughter of a successful Boston merchant. Gerry's first name came from John Elbridge, one of his mother's ancestors. Gerry's parents had 11 children in all, although only five survived to adulthood. Of these, Elbridge was the third. He was first educated by private tutors and entered Harvard College shortly before turning 14. After receiving a Bachelor of Arts in 1762 and a Master of Arts in 1765, he entered his father's merchant business. By the 1770s, the Gerrys numbered among the wealthiest Massachusetts merchants, with trading connections in Spain, the West Indies, and along the North American coast. Gerry's father, who had emigrated from England in 1730, was active in local politics and had a leading role in the local militia.

Gerry was from an early time a vocal opponent of Parliamentary efforts to tax the colonies after the French and Indian War ended in 1763. In 1770, he sat on a Marblehead committee that sought to enforce importation bans on taxed British goods. He frequently communicated with other Massachusetts opponents of British policy, including Samuel Adams, John Adams, Mercy Otis Warren, and others.

In May 1772, he won election to the Great and General Court of the Province of Massachusetts Bay, which served as the state's legislative assembly. He worked closely with Samuel Adams to advance colonial opposition to Parliamentary colonial policies. He was responsible for establishing Marblehead's committee of correspondence, one of the first to be set up after that of Boston. However, an incident of mob action prompted him to resign from the committee the next year. Gerry and other prominent Marbleheaders had established a hospital for performing smallpox inoculations on Cat Island; because the means of transmission of the disease were not known at the time, fears amongst the local population led to protests which escalated into violence that wrecked the hospital and threatened the proprietors' other properties.

Gerry reentered politics after the Boston Port Act closed that city's port in 1774, and Marblehead became an alternative port to which relief supplies from other colonies could be delivered. As one of the town's leading merchants and Patriots, Gerry played a major role in ensuring the storage and delivery of supplies from Marblehead to Boston, interrupting those activities only to care for his dying father. He was elected as a representative to the First Continental Congress in September 1774, but declined, still grieving the loss of his father.

Gerry was elected to the provincial assembly, which reconstituted itself as the Massachusetts Provincial Congress after Governor Thomas Gage dissolved the body in October 1774. He was assigned to its committee of safety, responsible for ensuring that the province's limited supplies of weapons and gunpowder did not fall into British hands. His actions were partly responsible for the storage of weapons and ammunition in Concord; these stores were the target of the British expedition that sparked the start of the American Revolutionary War with the battles of Lexington and Concord in April 1775. (Gerry was staying at an inn at Menotomy, now Arlington, when the British Army marched through on the night of April 18.) During the Siege of Boston that followed, Gerry continued to take a leading role in supplying the nascent Continental Army, something he would continue to do as the war progressed. He leveraged business contacts in France and Spain to acquire not just munitions, but supplies of all types, and was involved in the transfer of financial subsidies from Spain to Congress. He sent ships to ports all along the American coast and dabbled in financing privateering operations against British merchant shipping.

Unlike some other merchants, there is no evidence that Gerry profiteered directly from the hostilities. He spoke out against price gouging and in favor of price controls, although his war-related merchant activities notably increased the family's wealth. His gains were tempered to some extent by the precipitous decline in the value of paper currencies, which he held in large quantities and speculated in.

Gerry served in the Second Continental Congress in Philadelphia from February 1776 to 1780, when matters of the ongoing war occupied the body's attention. He was influential in convincing several delegates to support passage of the Declaration of Independence in the debates held during the summer of 1776; John Adams wrote of him, "If every Man here was a Gerry, the Liberties of America would be safe against the Gates of Earth and Hell." He was implicated as a member of the so-called "Conway Cabal", a group of Congressmen and military officers who were dissatisfied with the performance of General George Washington during the 1777 military campaign. However, Gerry took Pennsylvania leader Thomas Mifflin, one of Washington's critics, to task early in the episode and specifically denied knowledge of any sort of conspiracy against Washington in February 1778.

Gerry's political philosophy was one of limited central government, and he regularly advocated for the maintenance of civilian control of the military. He held these positions fairly consistently throughout his political career (wavering principally on the need for stronger central government in the wake of the 1786–87 Shays' Rebellion) and was well known for his personal integrity. In later years he opposed the idea of political parties, remaining somewhat distant from both the developing Federalist and Democratic-Republican parties until later in his career. It was not until 1800 that he formally associated with the Democratic-Republicans in opposition to what he saw as attempts by the Federalists to centralize too much power in the national government.

In 1780, he resigned from the Continental Congress over the issue and refused offers from the state legislature to return to the Congress. He also refused appointment to the state senate, claiming he would be more effective in the state's lower chamber, and also refused appointment as a county judge, comparing the offer by Governor John Hancock to those made by royally-appointed governors to benefit their political allies. He was elected a fellow of the American Academy of Arts and Sciences in 1781.

Gerry was convinced to rejoin the Confederation Congress in 1783, when the state legislature agreed to support his call for needed reforms. He served in that body, which met in New York City, until September 1785. The following year, he married Ann Thompson, the daughter of a wealthy New York City merchant who was 20 years his junior; his best man was his good friend James Monroe. The couple had ten children between 1787 and 1801, straining Ann's health.

The war made Gerry sufficiently wealthy that when it ended he sold off his merchant interests and began investing in land. In 1787, he purchased the Cambridge, Massachusetts, estate of the last royal lieutenant governor of Massachusetts, Thomas Oliver, which had been confiscated by the state. This property, known as Elmwood, became the family home for the rest of Gerry's life. He continued to own property in Marblehead and bought several properties in other Massachusetts communities. He also owned shares in the Ohio Company, prompting some political opponents to characterize him as an owner of vast tracts of western lands.

Gerry played a major role in the Constitutional Convention held in Philadelphia during the summer of 1787. In its deliberations, he consistently advocated for a strong delineation between state and federal government powers, with state legislatures shaping the membership of federal government positions. Gerry's opposition to popular election of representatives was rooted in part by the events of Shays' Rebellion in western Massachusetts in the year preceding the convention. He also sought to maintain individual liberties by providing checks on government power that might abuse or limit those freedoms.

He supported the idea that the Senate composition should not be determined by population; the view that it should instead be composed of equal numbers of members for each state prevailed in the Connecticut Compromise. The compromise was adopted on a narrow vote in which the Massachusetts delegation was divided, Gerry and Caleb Strong voting in favor. Gerry further proposed that senators of a state, rather than casting a single vote on behalf of the state, vote instead as individuals. Gerry was also vocal in opposing the Three-fifths Compromise, which counted slaves as three-fifths of a free person for the purposes of apportionment in the House of Representatives, whereas counting each slave individually would have given southern slave states a decided advantage. Gerry opposed slavery and said the constitution should have "nothing to do" with slavery so as "not to sanction it."

Because of his fear of demagoguery and belief the people of the United States could be easily misled, Gerry also advocated indirect elections. Although he was unsuccessful in obtaining them for the lower house of Congress, Gerry did obtain such indirect elections for the Senate, whose members were to be selected by the state legislatures. Gerry also advanced numerous proposals for indirect elections of the President of the United States, most of them involving limiting the right to vote to the state governors and electors.

Gerry was unhappy about the lack of enumeration of any specific individual liberties in the proposed constitution and generally opposed proposals that strengthened the central government. He was one of only three delegates who voted against the proposed constitution in the convention (the others were George Mason and Edmund Randolph), citing a concern about the convention's lack of authority to enact such major changes to the nation's system of government and to the constitution's lack of "federal features." Ultimately, Gerry refused to sign because of concerns over the rights of private citizens and the power of the legislature to raise armies and revenue.

During the ratification debates that took place in the states following the convention, Gerry continued his opposition, publishing a widely circulated letter documenting his objections to the proposed constitution. In the document, he cites the lack of a Bill of Rights as his primary objection but also expresses qualified approval of the Constitution, indicating that he would accept it with some amendment. Strong pro-Constitution forces attacked him in the press, comparing him unfavorably to the Shaysites. Henry Jackson was particularly vicious: "[Gerry has] done more injury to this country by that infamous Letter than he will be able to make atonement in his whole life", and Oliver Ellsworth, a convention delegate from Connecticut, charged him with deliberately courting the Shays faction.

One consequence of the furor over his letter was that he was not selected as a delegate to the Massachusetts ratifying convention although he was later invited to attend by the convention's leadership. The convention leadership was dominated by Federalists, and Gerry was not given any formal opportunity to speak. He left the convention after a shouting match with convention chair Francis Dana. Massachusetts ratified the constitution by a vote of 187 to 168. The debate had the result of estranging Gerry from several previously-friendly politicians, including chairman Dana and Rufus King.

Anti-Federalist forces nominated Gerry for governor in 1788, but he was predictably defeated by the popular incumbent John Hancock. Following its ratification, Gerry recanted his opposition to the Constitution, noting that other state ratifying conventions had called for amendments that he supported. He was nominated by friends (over his own opposition to the idea) for a seat in the inaugural House of Representatives, where he served two terms.

In June 1789, Gerry proposed that Congress consider all of the proposed constitutional amendments that various state ratifying conventions had called for (notably those of Rhode Island and North Carolina, which had at the time still not ratified the Constitution). In the debate that followed, he led opposition to some of the proposals, arguing that they did not go far enough in ensuring individual liberties. He successfully lobbied for inclusion of freedom of assembly in the First Amendment and was a leading architect of the Fourth Amendment protections against search and seizure. He sought unsuccessfully to insert the word "expressly" into the Tenth Amendment, which might have more significantly limited the federal government's power.

He was successful in efforts to severely limit the federal government's ability to control state militias. In tandem with this protection, he had once argued against the idea of the federal government controlling a large standing army, saying, "A standing army is like a standing member. It's an excellent assurance of domestic tranquility, but a dangerous temptation to foreign adventure."

Gerry vigorously supported Alexander Hamilton's reports on public credit, including the assumption at full value of state debts, and supported Hamilton's Bank of the United States, positions consistent with earlier calls he had made for economic centralization. Although he had speculated in depreciated Continental bills of credit (the IOUs at issue), there is no evidence he participated in large-scale speculation that attended the debate when it took place in 1790, and he became a major investor in the new bank. He used the floor of the House to speak out against aristocratic and monarchical tendencies he saw as threats to republican ideals, and generally opposed laws and their provisions that he perceived as limiting individual and state liberties. He opposed any attempt to give officers of the executive significant powers, specifically opposing establishment of the Treasury Department because its head might gain more power than the president. He opposed measures that strengthened the presidency, such as the ability to fire Cabinet officers, seeking instead to give the legislature more power over appointments.

Gerry did not stand for re-election in 1792, returning home to raise his children and care for his sickly wife. He agreed to serve as a presidential elector for John Adams in the 1796 election. During Adams' term in office, Gerry maintained good relations with both Adams and Vice President Thomas Jefferson, hoping that the divided executive might lead to less friction. His hopes were not realized: the split between Federalists (Adams) and Democratic-Republicans (Jefferson) widened.

President Adams appointed Gerry to be a member of a special diplomatic commission sent to Republican France in 1797. Tensions had risen between the two nations after the 1796 ratification of the Jay Treaty, made between the United States and Great Britain. It was seen by French leaders as signs of an Anglo-American alliance, and France had consequently stepped up seizures of American ships. Adams chose Gerry, over his cabinet's opposition (on political grounds that Gerry was insufficiently Federalist), because of their long-standing relationship; Adams described Gerry as one of the "two most impartial men in America" (Adams himself being the other).

Gerry joined co-commissioners Charles Cotesworth Pinckney and John Marshall in France in October 1797 and met briefly with Foreign Minister Talleyrand. Some days after that meeting, the delegation was approached by three French agents (at first identified as "X", "Y", and "Z" in published papers, leading the controversy to be called the "XYZ Affair") who demanded substantial bribes from the commissioners before negotiations could continue. The commissioners refused and sought unsuccessfully to engage Talleyrand in formal negotiations. Believing Gerry to be the most approachable of the commissioners, Talleyrand successively froze first Pinckney and then Marshall out of the informal negotiations, and they left France in April 1798. Gerry, who sought to leave with them, stayed behind because Talleyrand threatened war if he left. Gerry refused to make any significant negotiations afterward and left Paris in August.

By then, dispatches describing the commission's reception had been published in the United States, raising calls for war. The undeclared naval Quasi-War (1798–1800) followed. Federalists, notably Secretary of State Timothy Pickering, accused Gerry of supporting the French and abetting the breakdown of the talks, while Adams and Republicans such as Thomas Jefferson supported him. The negative press damaged Gerry's reputation, and he was burned in effigy by protestors in front of his home. He was only later vindicated, when his correspondence with Talleyrand was published in 1799. In response to the Federalist attacks on him, and because of his perception that the Federalist-led military buildup threatened republican values, Gerry formally joined the Democratic-Republican Party in early 1800, standing for election as Governor of Massachusetts.

For years (in the 1800, 1801, 1802, and 1803 elections) Gerry unsuccessfully sought the governorship of Massachusetts. His opponent in these races, Caleb Strong, was a popular moderate Federalist, whose party dominated the state's politics despite a national shift toward the Republicans. In 1803, Republicans in the state were divided, and Gerry only had regional support of the party. He decided not to run in the 1804 election, returning to semi-retirement and to deal with a personal financial crisis. His brother Samuel Russell had mismanaged his own business affairs, and Gerry had propped him up by guaranteeing a loan that was due. The matter ultimately ruined Gerry's finances for his remaining years.

Republican James Sullivan won the governor's seat from Strong in the 1807 election, but his successor was unable to hold the seat in the 1809 election, which went to Federalist Christopher Gore. Gerry stood for election again in the 1810 election against Gore and won a narrow victory. Republicans cast Gore as an ostentatious British-loving Tory who wanted to restore the monarchy (his parents were Loyalists during the Revolution), and Gerry as a patriotic American, while Federalists described Gerry as a "French partizan" and Gore as an honest man devoted to ridding the government of foreign influence. A temporary lessening in the threat of war with Britain aided Gerry. The two battled again in 1811, with Gerry once again victorious in a highly acrimonious campaign.

Gerry's first year as governor was less controversial than his second, because the Federalists controlled the state senate. He preached moderation in the political discourse, noting that it was important that the nation present a unified front in its dealings with foreign powers. In his second term, with full Republican control of the legislature, he became notably more partisan, purging much of the state government of Federalist appointees. The legislature also enacted "reforms" of the court system that resulted in an increase in the number of judicial appointments, which Gerry filled with Republican partisans. However, infighting within the party and a shortage of qualified candidates played against Gerry, and the Federalists scored points by complaining vocally about the partisan nature of the reforms.

Other legislation passed during Gerry's second year included a bill broadening the membership of Harvard's Board of Overseers to diversify its religious membership, and another that liberalized religious taxes. The Harvard bill had significant political slant because the recent split between orthodox Congregationalists and Unitarians also divided the state to some extent along party lines, and Federalist Unitarians had recently gained control over the Harvard board.

In 1812, the state adopted new constitutionally mandated electoral district boundaries. The Republican-controlled legislature had created district boundaries designed to enhance their party's control over state and national offices, leading to some oddly shaped legislative districts. Although Gerry was unhappy about the highly partisan districting (according to his son-in-law, he thought it "highly disagreeable"), he signed the legislation. The shape of one of the state senate districts in Essex County was compared to a salamander by a local Federalist newspaper in a political cartoon, calling it a "Gerry-mander". Ever since, the creation of such districts has been called gerrymandering.

Gerry also engaged in partisan investigations of potential libel against him by elements of the Federalist press, further damaging his popularity with moderates. The redistricting controversy, along with the libel investigation and the impending War of 1812, contributed to Gerry's defeat in 1812 (once again at the hands of Caleb Strong, whom the Federalists had brought out of retirement). The gerrymandering of the state Senate was a notable success in the 1812 election: the body was thoroughly dominated by Republicans, even though the house and the governor's seat went to Federalists by substantial margins.
Gerry's financial difficulties prompted him to ask President James Madison for a federal position after his loss in the 1812 election (which was held early in the year). He was chosen by the party Congressional nominating caucus to be Madison's vice presidential running mate in the 1812 presidential election, although the nomination was first offered to John Langdon. He was viewed as a relatively safe choice who would attract Northern votes but not pose a threat to James Monroe, who was thought likely to succeed Madison. Madison narrowly won re-election, and Gerry took the oath of office at Elmwood in March 1813. At that time the office of vice president was largely a sinecure; Gerry's duties included advancing the administration's agenda in Congress and dispensing patronage positions in New England. Gerry's actions in support of the War of 1812 had a partisan edge: he expressed concerns over a possible Federalist seizure of Fort Adams (as Boston's Fort Independence was then known) as a prelude to Anglo-Federalist cooperation and sought the arrest of printers of Federalist newspapers.

On November 23, 1814, Gerry suffered a heart attack while visiting Joseph Nourse of the Treasury Department, and he died soon after returning to his home in the Seven Buildings. He was 70 years old. 

He is buried in the Congressional Cemetery in Washington, D.C., with a memorial by John Frazee. He is the only signer of the Declaration of Independence who was buried in the nation's capital city. The estate he left his wife and children was rich in land and poor in cash, but he had managed to repay his brother's debts with his pay as vice president. Aged 68 at the start of his vice presidency, he was the oldest person to become vice president until Charles Curtis in 1929.

Gerry is generally remembered for the use of his name in the word "gerrymander", for his refusal to sign the United States Constitution, and for his role in the XYZ Affair. His path through the politics of the age has been difficult to characterize. Early biographers, including his son-in-law James T. Austin and Samuel Eliot Morison, struggled to explain his apparent changes in position. Biographer George Athan Billias posits that Gerry was a consistent advocate and practitioner of republicanism as it was originally envisioned, and that his role in the Constitutional Convention had a significant impact on the document it eventually produced.

Gerry had ten children, nine of whom survived into adulthood:

Gerry's grandson Elbridge Thomas Gerry became a distinguished lawyer and philanthropist in New York. His great-grandson, Peter G. Gerry, was a member of the U.S. House of Representatives and later a U.S. Senator from Rhode Island.
Gerry is depicted in two of John Trumbull's paintings, the "Declaration of Independence" and "General George Washington Resigning His Commission". Both are on view in the rotunda of the United States Capitol.

The upstate New York town of Elbridge is believed to have been named in his honor, as is the western New York town of Gerry. The town of Phillipston, Massachusetts was originally incorporated in 1786 under the name Gerry in his honor but was changed to its present name after the town submitted a petition in 1812, citing Democratic-Republican support for the War of 1812.

Gerry's Landing Road in Cambridge, Massachusetts, is located near the Eliot Bridge not far from Elmwood. During the 19th century, the area was known as Gerry's Landing (formerly known as Sir Richard's Landing) and was used by a Gerry relative for a short time as a landing and storehouse. The supposed house of his birth, the Elbridge Gerry House (it is uncertain whether he was born in the house currently standing on the site or an earlier structure) stands in Marblehead, and Marblehead's Elbridge Gerry School is named in his honor.





 

Encryption

In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor.

For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users.

Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often used in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes use the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.

One of the earliest forms of encryption is symbol replacement, which was first found in the tomb of Khnumhotep II, who lived in 1900 BC Egypt. Symbol replacement encryption is “non-standard,” which means that the symbols require a cipher or key to understand. This type of early encryption was used throughout Ancient Greece and Rome for military purposes. One of the most famous military encryption developments was the Caesar Cipher, which was a system in which a letter in normal text is shifted down a fixed number of positions down the alphabet to get the encoded letter. A message encoded with this type of encryption could be decoded with the fixed number on the Caesar Cipher.

Around 800 AD, Arab mathematician Al-Kindi developed the technique of frequency analysis – which was an attempt to systematically crack Caesar ciphers. This technique looked at the frequency of letters in the encrypted message to determine the appropriate shift. This technique was rendered ineffective after the creation of the polyalphabetic cipher by Leon Battista Alberti in 1465, which incorporated different sets of languages. In order for frequency analysis to be useful, the person trying to decrypt the message would need to know which language the sender chose.

Around 1790, Thomas Jefferson theorized a cipher to encode and decode messages in order to provide a more secure way of military correspondence. The cipher, known today as the Wheel Cipher or the Jefferson Disk, although never actually built, was theorized as a spool that could jumble an English message up to 36 characters. The message could be decrypted by plugging in the jumbled message to a receiver with an identical cipher.

A similar device to the Jefferson Disk, the M-94, was developed in 1917 independently by US Army Major Joseph Mauborne. This device was used in U.S. military communications until 1942.

In World War II, the Axis powers used a more advanced version of the M-94 called the Enigma Machine. The Enigma Machine was more complex because unlike the Jefferson Wheel and the M-94, each day the jumble of letters switched to a completely new combination. Each day's combination was only known by the Axis, so many thought the only way to break the code would be to try over 17,000 combinations within 24 hours. The Allies used computing power to severely limit the number of reasonable combinations they needed to check every day, leading to the breaking of the Enigma Machine.

Today, encryption is used in the transfer of communication over the Internet for security and commerce. As computing power continues to increase, computer encryption is constantly evolving to prevent eavesdropping attacks. With one of the first "modern" cipher suites, DES, utilizing a 56-bit key with 72,057,594,037,927,936 possibilities being able to be cracked in 22 hours and 15 minutes by EFF's DES cracker in 1999, which used a brute-force method of cracking. Modern encryption standards often use stronger key sizes often 256, like AES(256-bit mode), TwoFish, ChaCha20-Poly1305, Serpent(configurable up to 512-bit). Cipher suites utilizing a 128-bit or higher key, like AES, will not be able to be brute-forced due to the total amount of keys of 3.4028237e+38 possibilities. The most likely option for cracking ciphers with high key size is to find vulnerabilities in the cipher itself, like inherent biases and backdoors. For example, RC4, a stream cipher, was cracked due to inherent biases and vulnerabilities in the cipher.

In the context of cryptography, encryption serves as a mechanism to ensure confidentiality. Since data may be visible on the Internet, sensitive information such as passwords and personal communication may be exposed to potential interceptors. The process of encrypting and decrypting messages involves keys. The two main types of keys in cryptographic systems are symmetric-key and public-key (also known as asymmetric-key).

Many complex cryptographic algorithms often use simple modular arithmetic in their implementations.

In symmetric-key schemes, the encryption and decryption keys are the same. Communicating parties must have the same key in order to achieve secure communication. The German Enigma Machine utilized a new symmetric-key each day for encoding and decoding messages.

In public-key encryption schemes, the encryption key is published for anyone to use and encrypt messages. However, only the receiving party has access to the decryption key that enables messages to be read. Public-key encryption was first described in a secret document in 1973; beforehand, all encryption schemes were symmetric-key (also called private-key). Although published subsequently, the work of Diffie and Hellman was published in a journal with a large readership, and the value of the methodology was explicitly described. The method became known as the Diffie-Hellman key exchange.

RSA (Rivest–Shamir–Adleman) is another notable public-key cryptosystem. Created in 1978, it is still used today for applications involving digital signatures. Using number theory, the RSA algorithm selects two prime numbers, which help generate both the encryption and decryption keys.

A publicly available public-key encryption application called Pretty Good Privacy (PGP) was written in 1991 by Phil Zimmermann, and distributed free of charge with source code. PGP was purchased by Symantec in 2010 and is regularly updated.

Encryption has long been used by militaries and governments to facilitate secret communication. It is now commonly used in protecting information within many kinds of civilian systems. For example, the Computer Security Institute reported that in 2007, 71% of companies surveyed utilized encryption for some of their data in transit, and 53% utilized encryption for some of their data in storage. Encryption can be used to protect data "at rest", such as information stored on computers and storage devices (e.g. USB flash drives). In recent years, there have been numerous reports of confidential data, such as customers' personal records, being exposed through loss or theft of laptops or backup drives; encrypting such files at rest helps protect them if physical security measures fail. Digital rights management systems, which prevent unauthorized use or reproduction of copyrighted material and protect software against reverse engineering (see also copy protection), is another somewhat different example of using encryption on data at rest.

Encryption is also used to protect data in transit, for example data being transferred via networks (e.g. the Internet, e-commerce), mobile telephones, wireless microphones, wireless intercom systems, Bluetooth devices and bank automatic teller machines. There have been numerous reports of data in transit being intercepted in recent years. Data should also be encrypted when transmitted across networks in order to protect against eavesdropping of network traffic by unauthorized users.

Conventional methods for permanently deleting data from a storage device involve overwriting the device's whole content with zeros, ones, or other patterns – a process which can take a significant amount of time, depending on the capacity and the type of storage medium. Cryptography offers a way of making the erasure almost instantaneous. This method is called crypto-shredding. An example implementation of this method can be found on iOS devices, where the cryptographic key is kept in a dedicated 'effaceable storage'. Because the key is stored on the same device, this setup on its own does not offer full privacy or security protection if an unauthorized person gains physical access to the device.

Encryption is used in the 21st century to protect digital data and information systems. As computing power increased over the years, encryption technology has only become more advanced and secure. However, this advancement in technology has also exposed a potential limitation of today's encryption methods.

The length of the encryption key is an indicator of the strength of the encryption method. For example, the original encryption key, DES (Data Encryption Standard), was 56 bits, meaning it had 2^56 combination possibilities. With today's computing power, a 56-bit key is no longer secure, being vulnerable to brute force attacks.

Quantum computing utilizes properties of quantum mechanics in order to process large amounts of data simultaneously. Quantum computing has been found to achieve computing speeds thousands of times faster than today's supercomputers. This computing power presents a challenge to today's encryption technology. For example, RSA encryption utilizes the multiplication of very large prime numbers to create a semiprime number for its public key. Decoding this key without its private key requires this semiprime number to be factored, which can take a very long time to do with modern computers. It would take a supercomputer anywhere between weeks to months to factor in this key. However, quantum computing can use quantum algorithms to factor this semiprime number in the same amount of time it takes for normal computers to generate it. This would make all data protected by current public-key encryption vulnerable to quantum computing attacks. Other encryption techniques like elliptic curve cryptography and symmetric key encryption are also vulnerable to quantum computing.

While quantum computing could be a threat to encryption security in the future, quantum computing as it currently stands is still very limited. Quantum computing currently is not commercially available, cannot handle large amounts of code, and only exists as computational devices, not computers. Furthermore, quantum computing advancements will be able to be utilized in favor of encryption as well. The National Security Agency (NSA) is currently preparing post-quantum encryption standards for the future. Quantum encryption promises a level of security that will be able to counter the threat of quantum computing.

Encryption is an important tool but is not sufficient alone to ensure the security or privacy of sensitive information throughout its lifetime. Most applications of encryption protect information only at rest or in transit, leaving sensitive data in clear text and potentially vulnerable to improper disclosure during processing, such as by a cloud service for example. Homomorphic encryption and secure multi-party computation are emerging techniques to compute on encrypted data; these techniques are general and Turing complete but incur high computational and/or communication costs.

In response to encryption of data at rest, cyber-adversaries have developed new types of attacks. These more recent threats to encryption of data at rest include cryptographic attacks, stolen ciphertext attacks, attacks on encryption keys, insider attacks, data corruption or integrity attacks, data destruction attacks, and ransomware attacks. Data fragmentation and active defense data protection technologies attempt to counter some of these attacks, by distributing, moving, or mutating ciphertext so it is more difficult to identify, steal, corrupt, or destroy.

The question of balancing the need for national security with the right to privacy has been debated for years, since encryption has become critical in today's digital society. The modern encryption debate started around the '90 when US government tried to ban cryptography because, according to them, it would threaten national security. The debate is polarized around two opposing views. Those who see strong encryption as a problem making it easier for criminals to hide their illegal acts online and others who argue that encryption keep digital communications safe. The debate heated up in 2014, when Big Tech like Apple and Google set encryption by default in their devices. This was the start of a series of controversies that puts governments, companies and internet users at stake.

Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a message authentication code (MAC) or a digital signature usually done by a hashing algorithm or a PGP signature. Authenticated encryption algorithms are designed to provide both encryption and integrity protection together. Standards for cryptographic software and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single error in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See for example traffic analysis, TEMPEST, or Trojan horse.

Integrity protection mechanisms such as MACs and digital signatures must be applied to the ciphertext when it is first created, typically on the same device used to compose the message, to protect a message end-to-end along its full transmission path; otherwise, any node between the sender and the encryption agent could potentially tamper with it. Encrypting at the time of creation is only secure if the encryption device itself has correct keys and has not been tampered with. If an endpoint device has been configured to trust a root certificate that an attacker controls, for example, then the attacker can both inspect and tamper with encrypted data by performing a man-in-the-middle attack anywhere along the message's path. The common practice of TLS interception by network operators represents a controlled and institutionally sanctioned form of such an attack, but countries have also attempted to employ such attacks as a form of control and censorship.

Even when encryption correctly hides a message's content and it cannot be tampered with at rest or in transit, a message's "length" is a form of metadata that can still leak sensitive information about the message. For example, the well-known CRIME and BREACH attacks against HTTPS were side-channel attacks that relied on information leakage via the length of encrypted content. Traffic analysis is a broad class of techniques that often employs message lengths to infer sensitive implementation about traffic flows by aggregating information about a large number of messages.

Padding a message's payload before encrypting it can help obscure the cleartext's true length, at the cost of increasing the ciphertext's size and introducing or increasing bandwidth overhead. Messages may be padded randomly or deterministically, with each approach having different tradeoffs. Encrypting and padding messages to form padded uniform random blobs or PURBs is a practice guaranteeing that the cipher text leaks no metadata about its cleartext's content, and leaks asymptotically minimal formula_1 information via its length.


Einstein–Podolsky–Rosen paradox

The Einstein–Podolsky–Rosen (EPR) paradox is a thought experiment proposed by physicists Albert Einstein, Boris Podolsky and Nathan Rosen which argues that the description of physical reality provided by quantum mechanics is incomplete. In a 1935 paper titled "Can Quantum-Mechanical Description of Physical Reality be Considered Complete?", they argued for the existence of "elements of reality" that were not part of quantum theory, and speculated that it should be possible to construct a theory containing these hidden variables. Resolutions of the paradox have important implications for the interpretation of quantum mechanics.

The thought experiment involves a pair of particles prepared in what would later become known as an entangled state. Einstein, Podolsky, and Rosen pointed out that, in this state, if the position of the first particle were measured, the result of measuring the position of the second particle could be predicted. If instead the momentum of the first particle were measured, then the result of measuring the momentum of the second particle could be predicted. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is forbidden by the theory of relativity. They invoked a principle, later known as the "EPR criterion of reality", positing that: "If, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity." From this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. But quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. Einstein, Podolsky, and Rosen therefore concluded that quantum theory does not provide a complete description of reality.

The term "Einstein–Podolsky–Rosen paradox" or "EPR" arose from a paper written in 1934 after Einstein joined the Institute for Advanced Study, .
The original paper purports to describe what must happen to "two systems I and II, which we permit to interact", and after some time "we suppose that there is no longer any interaction between the two parts." The EPR description involves "two particles, A and B, [which] interact briefly and then move off in opposite directions." According to Heisenberg's uncertainty principle, it is impossible to measure both the momentum and the position of particle B exactly; however, it is possible to measure the exact position of particle A. By calculation, therefore, with the exact position of particle A known, the exact position of particle B can be known. Alternatively, the exact momentum of particle A can be measured, so the exact momentum of particle B can be worked out. As Manjit Kumar writes, "EPR argued that they had proved that ... [particle] B can have simultaneously exact values of position and momentum. ... Particle B has a position that is real and a momentum that is real. EPR appeared to have contrived a means to establish the exact values of "either" the momentum "or" the position of B due to measurements made on particle A, without the slightest possibility of particle B being physically disturbed."

EPR tried to set up a paradox to question the range of true application of quantum mechanics: Quantum theory predicts that both values cannot be known for a particle, and yet the EPR thought experiment purports to show that they must all have determinate values. The EPR paper says: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete." The EPR paper ends by saying: "While we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible." The 1935 EPR paper condensed the philosophical discussion into a physical argument. The authors claim that given a specific experiment, in which the outcome of a measurement is known before the measurement takes place, there must exist something in the real world, an "element of reality", that determines the measurement outcome. They postulate that these elements of reality are, in modern terminology, local, in the sense that each belongs to a certain point in spacetime. Each element may, again in modern terminology, only be influenced by events which are located in the backward light cone of its point in spacetime (i.e. in the past). These claims are founded on assumptions about nature that constitute what is now known as local realism.
Though the EPR paper has often been taken as an exact expression of Einstein's views, it was primarily authored by Podolsky, based on discussions at the Institute for Advanced Study with Einstein and Rosen. Einstein later expressed to Erwin Schrödinger that, "it did not come out as well as I had originally wanted; rather, the essential thing was, so to speak, smothered by the formalism." Einstein would later go on to present an individual account of his local realist ideas. Shortly before the EPR paper appeared in the "Physical Review," "The New York Times" ran a news story about it, under the headline "Einstein Attacks Quantum Theory". The story, which quoted Podolsky, irritated Einstein, who wrote to the "Times," "Any information upon which the article 'Einstein Attacks Quantum Theory' in your issue of May 4 is based was given to you without authority. It is my invariable practice to discuss scientific matters only in the appropriate forum and I deprecate advance publication of any announcement in regard to such matters in the secular press."

The "Times" story also sought out comment from physicist Edward Condon, who said, "Of course, a great deal of the argument hinges on just what meaning is to be attached to the word 'reality' in physics." The physicist and historian Max Jammer later noted, "[I]t remains a historical fact that the earliest criticism of the EPR paper — moreover, a criticism which correctly saw in Einstein's conception of physical reality the key problem of the whole issue — appeared in a daily newspaper prior to the publication of the criticized paper itself."

The publication of the paper prompted a response by Niels Bohr, which he published in the same journal ("Physical Review"), in the same year, using the same title. (This exchange was only one chapter in a prolonged debate between Bohr and Einstein about the nature of quantum reality.)
He argued that EPR had reasoned fallaciously. Bohr said measurements of position and of momentum are complementary, making the choice to measure one excludes the possibility of measuring the other. Consequently, a fact deduced regarding one arrangement of laboratory apparatus could not be combined with a fact deduced by means of the other, and so, the inference of predetermined position and momentum values for the second particle was not valid. Bohr concluded that EPR's "arguments do not justify their conclusion that the quantum description turns out to be essentially incomplete."

In his own publications and correspondence, Einstein indicated that he was not satisfied with the EPR paper and that Rosen had authored most of it. He later used a different argument to insist that quantum mechanics is an incomplete theory. He explicitly de-emphasized EPR's attribution of "elements of reality" to the position and momentum of particle B, saying that "I couldn't care less" whether the resulting states of particle B allowed one to predict the position and momentum with certainty.

For Einstein, the crucial part of the argument was the demonstration of nonlocality, that the choice of measurement done in particle A, either position or momentum, would lead to two different quantum states of particle B. He argued that, because of locality, the real state of particle B could not depend on which kind of measurement was done in A and that the quantum states therefore cannot be in one-to-one correspondence with the real states. Einstein struggled unsuccessfully for the rest of his life to find a theory that could better comply with his idea of locality. 

In 1951, David Bohm proposed a variant of the EPR thought experiment in which the measurements have discrete ranges of possible outcomes, unlike the position and momentum measurements considered by EPR. The EPR–Bohm thought experiment can be explained using electron–positron pairs. Suppose we have a source that emits electron–positron pairs, with the electron sent to destination "A", where there is an observer named Alice, and the positron sent to destination "B", where there is an observer named Bob. According to quantum mechanics, we can arrange our source so that each emitted pair occupies a quantum state called a spin singlet. The particles are thus said to be entangled. This can be viewed as a quantum superposition of two states, which we call state I and state II. In state I, the electron has spin pointing upward along the "z"-axis ("+z") and the positron has spin pointing downward along the "z"-axis (−"z"). In state II, the electron has spin −"z" and the positron has spin +"z". Because it is in a superposition of states, it is impossible without measuring to know the definite state of spin of either particle in the spin singlet.
Alice now measures the spin along the "z"-axis. She can obtain one of two possible outcomes: +"z" or −"z". Suppose she gets +"z". Informally speaking, the quantum state of the system collapses into state I. The quantum state determines the probable outcomes of any measurement performed on the system. In this case, if Bob subsequently measures spin along the "z"-axis, there is 100% probability that he will obtain −"z". Similarly, if Alice gets −"z", Bob will get +"z". There is nothing special about choosing the "z"-axis: according to quantum mechanics the spin singlet state may equally well be expressed as a superposition of spin states pointing in the "x" direction.

Whatever axis their spins are measured along, they are always found to be opposite. In quantum mechanics, the "x"-spin and "z"-spin are "incompatible observables", meaning the Heisenberg uncertainty principle applies to alternating measurements of them: a quantum state cannot possess a definite value for both of these variables. Suppose Alice measures the "z"-spin and obtains "+z", so that the quantum state collapses into state I. Now, instead of measuring the "z"-spin as well, Bob measures the "x"-spin. According to quantum mechanics, when the system is in state I, Bob's "x"-spin measurement will have a 50% probability of producing +"x" and a 50% probability of -"x". It is impossible to predict which outcome will appear until Bob actually "performs" the measurement. Therefore, Bob's positron will have a definite spin when measured along the same axis as Alice's electron, but when measured in the perpendicular axis its spin will be uniformly random. It seems as if information has propagated (faster than light) from Alice's apparatus to make Bob's positron assume a definite spin in the appropriate axis.

In 1964, John Stewart Bell published a paper investigating the puzzling situation at that time: on one hand, the EPR paradox purportedly showed that quantum mechanics was nonlocal, and suggested that a hidden-variable theory could heal this nonlocality. On the other hand, David Bohm had recently developed the first successful hidden-variable theory, but it had a grossly nonlocal character. Bell set out to investigate whether it was indeed possible to solve the nonlocality problem with hidden variables, and found out that first, the correlations shown in both EPR's and Bohm's versions of the paradox could indeed be explained in a local way with hidden variables, and second, that the correlations shown in his own variant of the paradox couldn't be explained by "any" local hidden-variable theory. This second result became known as the Bell theorem.

To understand the first result, consider the following toy hidden-variable theory introduced later by J.J. Sakurai: in it, quantum spin-singlet states emitted by the source are actually approximate descriptions for "true" physical states possessing definite values for the "z"-spin and "x"-spin. In these "true" states, the positron going to Bob always has spin values opposite to the electron going to Alice, but the values are otherwise completely random. For example, the first pair emitted by the source might be "(+"z", −"x") to Alice and (−"z", +"x") to Bob", the next pair "(−"z", −"x") to Alice and (+"z", +"x") to Bob", and so forth. Therefore, if Bob's measurement axis is aligned with Alice's, he will necessarily get the opposite of whatever Alice gets; otherwise, he will get "+" and "−" with equal probability.

Bell showed, however, that such models can only reproduce the singlet correlations when Alice and Bob make measurements on the same axis or on perpendicular axes. As soon as other angles between their axes are allowed, local hidden-variable theories become unable to reproduce the quantum mechanical correlations. This difference, expressed using inequalities known as "Bell's inequalities", is in principle experimentally testable. After the publication of Bell's paper, a variety of experiments to test Bell's inequalities were carried out, notably by the group of Alain Aspect in the 1980s; all experiments conducted to date have found behavior in line with the predictions of quantum mechanics. The present view of the situation is that quantum mechanics flatly contradicts Einstein's philosophical postulate that any acceptable physical theory must fulfill "local realism". The fact that quantum mechanics violates Bell inequalities indicates that any hidden-variable theory underlying quantum mechanics must be non-local; whether this should be taken to imply that quantum mechanics "itself" is non-local is a matter of debate.

Inspired by Schrödinger's treatment of the EPR paradox back in 1935, Howard M. Wiseman et al. formalised it in 2007 as the phenomenon of quantum steering. They defined steering as the situation where Alice's measurements on a part of an entangled state "steer" Bob's part of the state. That is, Bob's observations cannot be explained by a "local hidden state" model, where Bob would have a fixed quantum state in his side, that is classically correlated but otherwise independent of Alice's.

"Locality" has several different meanings in physics. EPR describe the principle of locality as asserting that physical processes occurring at one place should have no immediate effect on the elements of reality at another location. At first sight, this appears to be a reasonable assumption to make, as it seems to be a consequence of special relativity, which states that energy can never be transmitted faster than the speed of light without violating causality; however, it turns out that the usual rules for combining quantum mechanical and classical descriptions violate EPR's principle of locality without violating special relativity or causality. Causality is preserved because there is no way for Alice to transmit messages (i.e., information) to Bob by manipulating her measurement axis. Whichever axis she uses, she has a 50% probability of obtaining "+" and 50% probability of obtaining "−", completely at random; according to quantum mechanics, it is fundamentally impossible for her to influence what result she gets. Furthermore, Bob is able to perform his measurement only "once": there is a fundamental property of quantum mechanics, the no-cloning theorem, which makes it impossible for him to make an arbitrary number of copies of the electron he receives, perform a spin measurement on each, and look at the statistical distribution of the results. Therefore, in the one measurement he is allowed to make, there is a 50% probability of getting "+" and 50% of getting "−", regardless of whether or not his axis is aligned with Alice's.

As a summary, the results of the EPR thought experiment do not contradict the predictions of special relativity. Neither the EPR paradox nor any quantum experiment demonstrates that superluminal signaling is possible; however, the principle of locality appeals powerfully to physical intuition, and Einstein, Podolsky and Rosen were unwilling to abandon it. Einstein derided the quantum mechanical predictions as "spooky action at a distance". The conclusion they drew was that quantum mechanics is not a complete theory.

Bohm's variant of the EPR paradox can be expressed mathematically using the quantum mechanical formulation of spin. The spin degree of freedom for an electron is associated with a two-dimensional complex vector space "V", with each quantum state corresponding to a vector in that space. The operators corresponding to the spin along the "x", "y", and "z" direction, denoted "S", "S", and "S" respectively, can be represented using the Pauli matrices:
formula_1
where formula_2 is the reduced Planck constant (or the Planck constant divided by 2π).

The eigenstates of "S" are represented as
formula_3
and the eigenstates of "S" are represented as
formula_4

The vector space of the electron-positron pair is formula_5, the tensor product of the electron's and positron's vector spaces. The spin singlet state is
formula_6
where the two terms on the right hand side are what we have referred to as state I and state II above.

From the above equations, it can be shown that the spin singlet can also be written as
formula_7
where the terms on the right hand side are what we have referred to as state Ia and state IIa.

To illustrate the paradox, we need to show that after Alice's measurement of "S" (or "S"), Bob's value of "S" (or "S") is uniquely determined and Bob's value of "S" (or "S") is uniformly random. This follows from the principles of measurement in quantum mechanics. When "S" is measured, the system state formula_8 collapses into an eigenvector of "S". If the measurement result is "+z", this means that immediately after measurement the system state collapses to
formula_9

Similarly, if Alice's measurement result is −"z", the state collapses to
formula_10
The left hand side of both equations show that the measurement of "S" on Bob's positron is now determined, it will be −"z" in the first case or +"z" in the second case. The right hand side of the equations show that the measurement of "S" on Bob's positron will return, in both cases, +"x" or -"x" with probability 1/2 each.




Encapsulation

Encapsulation may refer to:




Ethnologue

Ethnologue: Languages of the World (stylized as Ethnoloɠue) is an annual reference publication in print and online that provides statistics and other information on the living languages of the world. It is the world's most comprehensive catalogue of languages. It was first issued in 1951, and is now published by SIL International, an American evangelical Christian non-profit organization.

"Ethnologue" has been published by SIL International (formerly known as the Summer Institute of Linguistics), a Christian linguistic service organization with an international office in Dallas, Texas. The organization studies numerous minority languages to facilitate language development, and to work with speakers of such language communities in translating portions of the Bible into their languages. Despite the Christian orientation of its publisher, "Ethnologue" is not ideologically or theologically biased.

"Ethnologue" includes alternative names and autonyms, the number of L1 and L2 speakers, language prestige, domains of use, literacy rates, locations, dialects, language classification, linguistic affiliations, typology, language maps, country maps, publication and use in media, availability of the Bible in each language and dialect described, religious affiliations of speakers, a cursory description of revitalization efforts where reported, intelligibility and lexical similarity with other dialects and languages, writing scripts, an estimate of language viability using the Expanded Graded Intergenerational Disruption Scale (EGIDS), and bibliographic resources. Coverage varies depending on languages. For instance, as of 2008, information on word order was present for 15% of entries while religious affiliations were mentioned for 38% of languages. According to Lyle Campbell "language maps are highly valuable" and most country maps are of high quality and user-friendly.

"Ethnologue" gathers information from SIL's thousands of field linguists, surveys done by linguists and literacy specialists, observations of Bible translators, and crowdsourced contributions. SIL's field linguists use an online collaborative research system to review current data, update it, or request its removal. SIL has a team of editors by geographical area who prepare reports to Ethnologue's general editor. These reports combine opinions from SIL area experts and feedback solicited from non-SIL linguists. Editors have to find compromises when opinions differ. Most of SIL's linguists have taken three to four semesters of graduate linguistics courses, and half of them have a master's degree. They're trained by 300 PhD linguists in SIL.
The determination of what characteristics define a single language depends upon sociolinguistic evaluation by various scholars; as the preface to "Ethnologue" states, "Not all scholars share the same set of criteria for what constitutes a 'language' and what features define a 'dialect'." The criteria used by "Ethnologue" are mutual intelligibility and the existence or absence of a common literature or ethnolinguistic identity. The number of languages identified has been steadily increasing, from 5,445 in the 10th edition (in 1984) to 6,909 in the 16th (in 2009), partly due to governments according designation as languages to mutually intelligible varieties and partly due to SIL establishing new Bible translation teams. "Ethnologue" codes were used as the base to create the new ISO 639-3 international standard. Since 2007, "Ethnologue" relies only on this standard, administered by SIL International, to determine what is listed as a language.

In addition to choosing a primary name for a language, "Ethnologue" provides listings of other name(s) for the language and any dialects that are used by its speakers, government, foreigners and neighbors. Also included are any names that have been commonly referenced historically, regardless of whether a name is considered official, politically correct or offensive; this allows more complete historic research to be done. These lists of names are not necessarily complete.

"Ethnologue" was founded in 1951 by Richard S. Pittman and was initially focused on minority languages, to share information on Bible translation needs. The first edition included information on 46 languages. Hand-drawn maps were introduced in the fourth edition (1953). The seventh edition (1969) listed 4,493 languages. In 1971, "Ethnologue" expanded its coverage to all known languages of the world.

"Ethnologue" database was created in 1971 at the University of Oklahoma under a grant from the National Science Foundation. In 1974 the database was moved to Cornell University. Since 2000, the database has been maintained by SIL International in their Dallas headquarters. In 1997 (13th edition), the website became the primary means of access.

In 1984, "Ethnologue" released a three-letter coding system, called an 'SIL code', to identify each language that it described. This set of codes significantly exceeded the scope of other existing standards, e.g. ISO 639-1 and ISO 639-2.

The 14th edition, published in 2000, included 7,148 language codes. In 2002, "Ethnologue" was asked to work with the International Organization for Standardization (ISO) to integrate its codes into a draft international standard. "Ethnologue" codes have then been adopted by ISO as the international standard, ISO 639-3. The 15th edition of "Ethnologue" was the first edition to use this standard. This standard is now administered separately from Ethnologue. SIL International is the registration authority for languages names and codes, according to rules established by ISO. Since then "Ethnologue" relies on the standard to determine what is listed as a language. In only one case, "Ethnologue" and the ISO standards treat languages slightly differently. ISO 639-3 considers Akan to be a macrolanguage consisting of two distinct languages, Twi and Fante, whereas "Ethnologue" considers Twi and Fante to be dialects of a single language (Akan), since they are mutually intelligible. This anomaly resulted because the ISO 639-2 standard has separate codes for Twi and Fante, which have separate literary traditions, and all 639-2 codes for individual languages are automatically part of 639-3, even though 639-3 would not normally assign them separate codes.

In 2014, with the 17th edition, "Ethnologue" introduced a numerical code for language status using a framework called EGIDS (Expanded Graded Intergenerational Disruption Scale), an elaboration of Fishman's GIDS (Graded Intergenerational Disruption Scale). It ranks a language from 0 for an international language to 10 for an extinct language, i.e. a language with which no-one retains a sense of ethnic identity.

In 2015, SIL's funds decreased and in December 2015, "Ethnologue" launched a metered paywall to cover its cost, as it is financially self-sustaining. Users in high-income countries who wanted to refer to more than seven pages of data per month had to buy a paid subscription. The 18th edition released that year included a new section on language policy country by country.

In 2016, "Ethnologue" added date about language planning agencies to the 19th edition.

As of 2017, "Ethnologue"'s 20th edition described 237 language families including 86 language isolates and six typological categories, namely sign languages, creoles, pidgins, mixed languages, constructed languages, and as yet unclassified languages.

The early focus of the Ethnologue was on native use (L1) but was gradually expanded to cover L2 use as well.

In 2019, "Ethnologue" disabled trial views and introduced a hard paywall to cover its nearly $1 million in annual operating costs (website maintenance, security, researchers, and SIL's 5,000 field linguists). Subscriptions start at $480 per person per year, while full access costs $2,400 per person per year. Users in low and middle-income countries as defined by the World Bank are eligible for free access and there are discounts for libraries and independent researchers. Subscribers are mostly institutions: 40% of the world's top 50 universities subscribe to "Ethnologue", and it is also sold to business intelligence firms and Fortune 500 companies. The introduction of the paywall was harshly criticized by the community of linguists who rely on "Ethnologue" to do their work and cannot afford the subscription The same year, "Ethnologue" launched its contributor program to fill gaps and improve accuracy, allowing contributors to submit corrections and additions and to get a complimentary access to the website. "Ethnologue"'s editors gradually review crowdsourced contributions before publication. As 2019 was the International Year of Indigenous Languages, this edition focused on language loss: it added the date when last fluent speaker of the language died, standardized the age range of language users, and improved the EGIDS estimates.

In 2020, the 23rd edition listed 7,117 living languages, an increase of 6 living languages from the 22nd edition. In this edition, "Ethnologue" expanded its coverage of immigrant languages: previous editions only had full entries for languages considered to be "established" within a country. From this edition, "Ethnologue" includes data about first and second languages of refugees, temporary foreign workers and immigrants.

In 2021, the 24th edition had 7,139 modern languages, an increase of 22 living languages from the 23rd edition. Editors especially improved data about language shift in this edition.

In 2022, the 25th edition listed a total of 7,151 living languages, an increase of 12 living languages from the 24th edition. This edition specifically improved the use of languages in education.

In 2023, the 26th edition listed a total of 7,168 living languages, an increase of 17 living languages from the 25th edition.

In 2024, the 27th edition listed a total of 7,164 living languages, a decrease of 4 living languages from the 26th edition.

In 1986, William Bright, then editor of the journal "Language", wrote of "Ethnologue" that it "is indispensable for any reference shelf on the languages of the world". The 2003 "International Encyclopedia of Linguistics" described "Ethnologue" as "a comprehensive listing of the world's languages, with genetic classification", and follows Ethnologue's classification. In 2005, linguists Lindsay J. Whaley and Lenore Grenoble considered that "Ethnologue" "continues to provide the most comprehensive and reliable count of numbers of speakers of the world's languages", still they recognize that "individual language surveys may have far more accurate counts for a specific language, but "The Ethnologue" is unique in bringing together speaker statistics on a global scale". In 2006, computational linguists John C. Paolillo and Anupam Das conducted a systematic evaluation of available information on language populations for the UNESCO Institute for Statistics. They reported that "Ethnologue" and Linguasphere were the only comprehensive sources of information about language populations and that "Ethnologue" had more specific information. They concluded that: "the language statistics available today in the form of the "Ethnologue" population counts are already good enough to be useful" According to linguist William Poser, "Ethnologue" was, as of 2006, the "best single source of information" on language classification. In 2008 linguists Lyle Campbell and Verónica Grondona highly commended "Ethnologue" in "Language". They described it as a highly valuable catalogue of the world's languages that "has become the standard reference" and whose "usefulness is hard to overestimate". They concluded that "Ethnologue" was "truly excellent, highly valuable, and the very best book of its sort available."

In a review of "Ethnologue"'s 2009 edition in "Ethnopolitics", Richard O. Collin, professor of politics, noted that ""Ethnologue" has become a standard resource for scholars in the other social sciences: anthropologists, economists, sociologists and, obviously, sociolinguists". According to Collin, "Ethnologue" is "stronger in languages spoken by indigenous peoples in economically less-developed portions of the world" and "when recent in-depth country-studies have been conducted, information can be very good; unfortunately [...] data are sometimes old".

In 2012, linguist Asya Pereltsvaig described "Ethnologue" as "a reasonably good source of thorough and reliable geographical and demographic information about the world's languages". She added in 2021 that its maps "are generally fairly accurate although they often depict the linguistic situation as it once was or as someone might imagine it to be but not as it actually is". Linguist George Tucker Childs wrote in 2012 that: ""Ethnologue" is the most widely referenced source for information on languages of the world", but he added that regarding African languages, "when evaluated against recent field experience [Ethnologue] seems at least out of date". In 2014, "Ethnologue" admitted that some of its data was out-of-date and switched from a four-year publication cycle (in print and online) to yearly online updates.

In 2017, Robert Phillipson and Tove Skutnabb-Kangas described "Ethnologue" as "the most comprehensive global source list for (mostly oral) languages". According to the 2018 "Oxford Research Encyclopedia of Linguistics", "Ethnologue" is a "comprehensive, frequently updated [database] on languages and language families'. According to quantitative linguists Simon Greenhill, "Ethnologue" offers, as of 2018, "sufficiently accurate reflections of speaker population size". Linguists Lyle Campbell and Kenneth Lee Rehg wrote in 2018 that "Ethnologue" was "the best source that list the non-endangered languages of the world". Lyle Campbell and Russell Barlow also noted that the 2017 edition of "Ethnologue" "improved [its] classification markedly". They note that "Ethnologue"'s genealogy is similar to that of the World Atlas of Language Structures (WALS) but different from that of the Catalogue of Endangered Languages (ELCat) and Glottolog. Linguist Lisa Matthewson commented in 2020 that "Ethnologue" offers "accurate information about speaker numbers". In a 2021 review of "Ethnologue" and Glottolog, linguist Shobhana Chelliah noted that "For better or worse, the impact of the site is indeed considerable. [...] Clearly, the site has influence on the field of linguistics and beyond." She added that she, among other linguists, integrated "Ethnologue" in her linguistics classes."

The "Encyclopedia of Language and Linguistics" uses "Ethnologue" as its primary source for the list of languages and language maps. According to linguist Suzanne Romaine, "Ethnologue" is also the leading source for research on language diversity. According to "The Oxford Handbook of Language and Society", "Ethnologue" is "the standard reference source for the listing and enumeration of Endangered Languages, and for all known and "living" languages of the world"." Similarly, linguist David Bradley describes "Ethnologue" as "the most comprehensive effort to document the level of endangerment in languages around the world." The US National Science Foundation uses "Ethnologue" to determine which languages are endangered. According to Hammarström et al., "Ethnologue" is, as of 2022, one of the three global databases documenting language endangerment with the "Atlas of the World's Languages in Danger" and the Catalogue of Endangered Languages (ELCat). The University of Hawaii Kaipuleohone language archive uses "Ethnologue"'s metadata as well. The "World Atlas of Language Structures" uses "Ethnologue"'s genealogical classification. The Rosetta Project uses "Ethnologue"'s language metadata.

In 2005, linguist Harald Hammarström wrote that "Ethnologue" was consistent with specialist views most of the time and was a catalog "of very high absolute value and by far the best of its kind". In 2011, Hammarström created Glottolog in response to the lack of a comprehensive language bibliography, especially in "Ethnologue". In 2015, Hammarström reviewed the 16th, 17th, and 18th editions of "Ethnologue" and described the frequent lack of citations as its only "serious fault" from a scientific perspective. He concluded: ""Ethnologue" is at present still better than any other nonderivative work of the same scope. [It] is an impressively comprehensive catalogue of world languages, and it is far superior to anything else produced prior to 2009. In particular, it is superior by virtue of being explicit." According to Hammarström, as of 2016, "Ethnologue" and Glottolog are the only global-scale continually maintained inventories of the world's languages. The main difference is that "Ethnologue" includes additional information (such as speaker numbers or vitality) but lacks systematic sources for the information given. In contrast, Glottolog provides no language context information but points to primary sources for further data. Contrary to "Ethnologue", Glottolog does not run its own surveys, but it uses "Ethnologue" as one of its primary sources. As of 2019, Hammarström uses "Ethnologue" in his articles, noting that it "has (unsourced, but) detailed information associated with each speech variety, such as speaker numbers and map location". In response to feedback about the lack of references, "Ethnologue" added in 2013 a link on each language to language resources from the Open Language Archives Community (OLAC) "Ethnologue" acknowledges that it rarely quotes any source verbatim but cites sources wherever specific statements are directly attributed to them, and corrects missing attributions upon notification. The website provides a list of all of the references cited. In her 2021 review, Shobhana Chelliah noted that Glottolog aims to be better than "Ethnologue" in language classification and genetic and areal relationships by using linguists' original sources.

Starting with the 17th edition, "Ethnologue" has been published every year, on February 21, which is International Mother Language Day.

Evaporation

Evaporation is a type of vaporization that occurs on the surface of a liquid as it changes into the gas phase. A high concentration of the evaporating substance in the surrounding gas significantly slows down evaporation, such as when humidity affects rate of evaporation of water. When the molecules of the liquid collide, they transfer energy to each other based on how they collide. When a molecule near the surface absorbs enough energy to overcome the vapor pressure, it will escape and enter the surrounding air as a gas. When evaporation occurs, the energy removed from the vaporized liquid will reduce the temperature of the liquid, resulting in evaporative cooling.

On average, only a fraction of the molecules in a liquid have enough heat energy to escape from the liquid. The evaporation will continue until an equilibrium is reached when the evaporation of the liquid is equal to its condensation. In an enclosed environment, a liquid will evaporate until the surrounding air is saturated.

Evaporation is an essential part of the water cycle. The sun (solar energy) drives evaporation of water from oceans, lakes, moisture in the soil, and other sources of water. In hydrology, evaporation and transpiration (which involves evaporation within plant stomata) are collectively termed evapotranspiration. Evaporation of water occurs when the surface of the liquid is exposed, allowing molecules to escape and form water vapor; this vapor can then rise up and form clouds. With sufficient energy, the liquid will turn into vapor.

For molecules of a liquid to evaporate, they must be located near the surface, they have to be moving in the proper direction, and have sufficient kinetic energy to overcome liquid-phase intermolecular forces. When only a small proportion of the molecules meet these criteria, the rate of evaporation is low. Since the kinetic energy of a molecule is proportional to its temperature, evaporation proceeds more quickly at higher temperatures. As the faster-moving molecules escape, the remaining molecules have lower average kinetic energy, and the temperature of the liquid decreases. This phenomenon is also called evaporative cooling. This is why evaporating sweat cools the human body.
Evaporation also tends to proceed more quickly with higher flow rates between the gaseous and liquid phase and in liquids with higher vapor pressure. For example, laundry on a clothes line will dry (by evaporation) more rapidly on a windy day than on a still day. Three key parts to evaporation are heat, atmospheric pressure (determines the percent humidity), and air movement.

On a molecular level, there is no strict boundary between the liquid state and the vapor state. Instead, there is a Knudsen layer, where the phase is undetermined. Because this layer is only a few molecules thick, at a macroscopic scale a clear phase transition interface cannot be seen.

Liquids that do not evaporate visibly at a given temperature in a given gas (e.g., cooking oil at room temperature) have molecules that do not tend to transfer energy to each other in a pattern sufficient to frequently give a molecule the heat energy necessary to turn into vapor. However, these liquids "are" evaporating. It is just that the process is much slower and thus significantly less visible.

If evaporation takes place in an enclosed area, the escaping molecules accumulate as a vapor above the liquid. Many of the molecules return to the liquid, with returning molecules becoming more frequent as the density and pressure of the vapor increases. When the process of escape and return reaches an equilibrium, the vapor is said to be "saturated", and no further change in either vapor pressure and density or liquid temperature will occur. For a system consisting of vapor and liquid of a pure substance, this equilibrium state is directly related to the vapor pressure of the substance, as given by the Clausius–Clapeyron relation:

where "P", "P" are the vapor pressures at temperatures "T", "T" respectively, Δ"H" is the enthalpy of vaporization, and "R" is the universal gas constant. The rate of evaporation in an open system is related to the vapor pressure found in a closed system. If a liquid is heated, when the vapor pressure reaches the ambient pressure the liquid will boil.

The ability for a molecule of a liquid to evaporate is based largely on the amount of kinetic energy an individual particle may possess. Even at lower temperatures, individual molecules of a liquid can evaporate if they have more than the minimum amount of kinetic energy required for vaporization.

Note: Air is used here as a common example of the surrounding gas; however, other gases may hold that role.


In the US, the National Weather Service measures, at various outdoor locations nationwide, the actual rate of evaporation from a standardized "pan" open water surface. Others do likewise around the world. The US data is collected and compiled into an annual evaporation map. The measurements range from under 30 to over per year.

Because it typically takes place in a complex environment, where 'evaporation is an extremely rare event', the mechanism for the evaporation of water is not completely understood. Theoretical calculations require prohibitively long and large computer simulations. 'The rate of evaporation of liquid water is one of the principal uncertainties in modern climate modeling.' 

Evaporation is an endothermic process, since heat is absorbed during evaporation.


Fuel droplets vaporize as they receive heat by mixing with the hot gases in the combustion chamber. Heat (energy) can also be received by radiation from any hot refractory wall of the combustion chamber.

Internal combustion engines rely upon the vaporization of the fuel in the cylinders to form a fuel/air mixture in order to burn well.
The chemically correct air/fuel mixture for total burning of gasoline has been determined to be 15 parts air to one part gasoline or 15/1 by weight. Changing this to a volume ratio yields 8000 parts air to one part gasoline or 8,000/1 by volume.

Thin films may be deposited by evaporating a substance and condensing it onto a substrate, or by dissolving the substance in a solvent, spreading the resulting solution thinly over a substrate, and evaporating the solvent. The Hertz–Knudsen equation is often used to estimate the rate of evaporation in these instances.


Esbat

An esbat is a coven meeting or ritual at a time other than one of the Sabbats within Wicca and other Wiccan-influenced forms of contemporary Paganism.

Esbats can span a wide range of purposes from coven business meetings and initiation ceremonies to social gatherings, times of merriment, and opportunities to commune with the divine. Janet and Stewart Farrar describe esbats as an opportunity for a "love feast, healing work, psychic training and all."

Esbats are typically held once per month on or near the night of a full moon or new moon. Due to the connection between the Moon and femininity in Wicca, esbats are associated with goddesses.

Esbats are a time set aside for formal worship and have been described as similar to Sundays for Christians or Friday nights for Jewish people. They can be solitary affairs but tend to be conducted in groups. Sources vary on whether these rituals are open to the public or only to initiated members.

The term "esbat" is derived from Old French "s'esbattre" (Modern French "ébat"), meaning "to frolic and amuse oneself", "diversion". It was a borrowing by 20th century anthropologist Margaret Murray's use of French witch trial sources on supposed Witches' Sabbaths in her attempts to "reconstruct" a "Witch Cult in Western Europe".

An esbat is commonly understood to be a ritual observance on the night of a full moon. However, the late high priestess Doreen Valiente distinguished between "full moon Esbat[s]" and other esbatic occasions.

The term "esbat" in this sense was described by Margaret Murray.
Esbats vary greatly and can be simple or elaborate. Rituals use symbolism to enhance the properties of a particular moon, of which there are 13 per solar year. 

They are typically held at coven members' homes or outdoors. Tools such as candles, athames, incense, pentacles, items from nature, bowls of water, mirrors, and crystals are commonly placed on an altar. The ceremony begins with participants establishing a sacred space by casting a circle or purifying the area by smudging. Then, they commune with the divine, pray, and meditate. Central elements are reflection on changes that have occurred in the past moon cycle, things you wish to change by the next moon, and gratitude. Once the ceremony is completed, a ritual closing is performed. Participants describe esbats as "spiritually fulfilling" and "immensely beneficial to our personal spiritual growth."

When esbat rituals occur during a new moon (also known as a dark moon), it is an occasion to worship the darker aspects of witchcraft. This represents elements that are hidden or in shadow, and is not necessarily associated with evil. New moon esbats may be used to worship a dark or maiden goddess, to banish something unwanted, or to end a phase in life. Because esbats typically occur when the moon is at its highest point, covens may choose for new moon esbats to take place in the mid-afternoon.

The full moon esbat tends to be a frenetic celebration. Spells for wholeness, children, mothers, families, clairvoyance, and love are performed. Most full moon esbats are held at midnight because the moon is most visible which allows participants to feel closer to it. 

One major component of full moon esbats is drawing down the moon. The idea is to draw the essence of the lunar goddess into the body of a coven member, usually a priestess or leader. This is done by another member who channels lunar energy down through a receptacle such as a chalice or knife. The two stand facing each other in the center of the circle and the group asks the goddess to come down. Once the divine energy enters the tool, it is touched to the head, chest, or abdomen of the priestess. Through her body, the goddess speaks to the group by answering questions, giving instructions, offering blessings, or simply "[pouring] her loving energy into the circle and [leading] you in a merry spiral dance." After returning the energy to the goddess, the sign of a pentagram may be used as a dismissal.

The ceremony of cakes and ale is the other main component which typically concludes the full moon esbat. This ritual uses cookies or a loaf of bread and a chalice of wine or other drink which are placed on a central altar. The bread and wine symbolize the body and blood of the mother goddess, who gives life to all things. Each participant takes turns blessing the bread, breaking off a piece, eating it, then passing it clockwise. The same process happens with the chalice. This allows everyone to honor the body and blood of the goddess as they wish.


Equal temperament

An equal temperament is a musical temperament or tuning system that approximates just intervals by dividing an octave (or other interval) into steps such that the ratio of the frequencies of any adjacent pair of notes is the same. This system yields pitch steps perceived as equal in size, due to the logarithmic changes in pitch frequency.

In classical music and Western music in general, the most common tuning system since the 18th century has been 12 equal temperament (also known as "12 tone equal temperament", ' or ', informally abbreviated as "12 equal"), which divides the octave into 12 parts, all of which are equal on a logarithmic scale, with a ratio equal to the 12th root of 2, (  ≈ 1.05946 ). That resulting smallest interval, the width of an octave, is called a semitone or half step. In Western countries the term "equal temperament", without qualification, generally means ".

In modern times, is usually tuned relative to a standard pitch of 440 Hz, called A 440, meaning one note, , is tuned to 440 hertz and all other notes are defined as some multiple of semitones away from it, either higher or lower in frequency. The standard pitch has not always been 440 Hz; it has varied considerably and generally risen over the past few hundred years.

Other equal temperaments divide the octave differently. For example, some music has been written in and , while the Arab tone system uses 

Instead of dividing an octave, an equal temperament can also divide a different interval, like the equal-tempered version of the Bohlen–Pierce scale, which divides the just interval of an octave and a fifth (ratio 3:1), called a "tritave" or a "pseudo-octave" in that system, into 13 equal parts.

For tuning systems that divide the octave equally, but are not approximations of just intervals, the term equal division of the octave, or " can be used.

Unfretted string ensembles, which can adjust the tuning of all notes except for open strings, and vocal groups, who have no mechanical tuning limitations, sometimes use a tuning much closer to just intonation for acoustic reasons. Other instruments, such as some wind, keyboard, and fretted instruments, often only approximate equal temperament, where technical limitations prevent exact tunings.
Some wind instruments that can easily and spontaneously bend their tone, most notably trombones, use tuning similar to string ensembles and vocal groups.

In an equal temperament, the distance between two adjacent steps of the scale is the same interval. Because the perceived identity of an interval depends on its ratio, this scale in even steps is a geometric sequence of multiplications. (An arithmetic sequence of intervals would not sound evenly spaced and would not permit transposition to different keys.) Specifically, the smallest interval in an equal-tempered scale is the ratio:

where the ratio divides the ratio (typically the octave, which is 2:1) into equal parts. ("See Twelve-tone equal temperament below.")

Scales are often measured in cents, which divide the octave into 1200 equal intervals (each called a cent). This logarithmic scale makes comparison of different tuning systems easier than comparing ratios, and has considerable use in ethnomusicology. The basic step in cents for any equal temperament can be found by taking the width of above in cents (usually the octave, which is 1200 cents wide), called below , and dividing it into parts:

In musical analysis, material belonging to an equal temperament is often given an integer notation, meaning a single integer is used to represent each pitch. This simplifies and generalizes discussion of pitch material within the temperament in the same way that taking the logarithm of a multiplication reduces it to addition. Furthermore, by applying the modular arithmetic where the modulus is the number of divisions of the octave (usually 12), these integers can be reduced to pitch classes, which removes the distinction (or acknowledges the similarity) between pitches of the same name, e.g., is 0 regardless of octave register. The MIDI encoding standard uses integer note designations.

12 tone equal temperament, which divides the octave into 12 intervals of equal size, is the musical system most widely used today, especially in Western music.

The two figures frequently credited with the achievement of exact calculation of equal temperament are Zhu Zaiyu (also romanized as Chu-Tsaiyu. Chinese: ) in 1584 and Simon Stevin in 1585. According to F.A. Kuttner, a critic of giving credit to Zhu, it is known that Zhu "presented a highly precise, simple and ingenious method for arithmetic calculation of equal temperament mono-chords in 1584" and that Stevin "offered a mathematical definition of equal temperament plus a somewhat less precise computation of the corresponding numerical values in 1585 or later."

The developments occurred independently.

Kenneth Robinson credits the invention of equal temperament to Zhu and provides textual quotations as evidence. In 1584 Zhu wrote:

Kuttner disagrees and remarks that his claim "cannot be considered correct without major qualifications". Kuttner proposes that neither Zhu nor Stevin achieved equal temperament and that neither should be considered its inventor.

Chinese theorists had previously come up with approximations for , but Zhu was the first person to mathematically solve 12 tone equal temperament, which he described in two books, published in 1580 and 1584. Needham also gives an extended account.

Zhu obtained his result by dividing the length of string and pipe successively by and for pipe length by such that after 12 divisions (an octave), the length was halved.

Zhu created several instruments tuned to his system, including bamboo pipes.

Some of the first Europeans to advocate equal temperament were lutenists Vincenzo Galilei, Giacomo Gorzanis, and Francesco Spinacino, all of whom wrote music in it.

Simon Stevin was the first to develop 12  based on the twelfth root of two, which he described in "van de Spiegheling der singconst" (), published posthumously in 1884.

Plucked instrument players (lutenists and guitarists) generally favored equal temperament, while others were more divided. In the end, 12-tone equal temperament won out. This allowed enharmonic modulation, new styles of symmetrical tonality and polytonality, atonal music such as that written with the 12-tone technique or serialism, and jazz (at least its piano component) to develop and flourish.

In 12 tone equal temperament, which divides the octave into 12 equal parts, the width of a semitone, i.e. the frequency ratio of the interval between two adjacent notes, is the twelfth root of two:

This interval is divided into 100 cents.

To find the frequency, , of a note in 12 , the following formula may be used:

In this formula represents the pitch, or frequency (usually in hertz), you are trying to find. is the frequency of a reference pitch. The indes numbers and are the labels assigned to the desired pitch () and the reference pitch (). These two numbers are from a list of consecutive integers assigned to consecutive semitones. For example, (the reference pitch) is the 49th key from the left end of a piano (tuned to 440 Hz), and (middle), and are the 40th and 46th keys, respectively. These numbers can be used to find the frequency of and :

To convert a frequency (in Hz) to its equal 12  counterpart, the following formula can be used:

The intervals of 12  closely approximate some intervals in just intonation.
The fifths and fourths are almost indistinguishably close to just intervals, while thirds and sixths are further away.

In the following table, the sizes of various just intervals are compared to their equal-tempered counterparts, given as a ratio as well as cents.

Violins, violas, and cellos are tuned in perfect fifths ( for violins and for violas and cellos), which suggests that their semitone ratio is slightly higher than in conventional 12 tone equal temperament. Because a perfect fifth is in 3:2 relation with its base tone, and this interval comprises seven steps, each tone is in the ratio of to the next (100.28 cents), which provides for a perfect fifth with ratio of 3:2, but a slightly widened octave with a rather than the usual 2:1, because 12 perfect fifths do not equal seven octaves. During actual play, however, violinists choose pitches by ear, and only the four unstopped pitches of the strings are guaranteed to exhibit this 3:2 ratio.

Five- and seven-tone equal temperament (' and ' ), with 240 cent and 171 cent steps, respectively, are fairly common.


According to Kunst (1949), Indonesian gamelans are tuned to but according to Hood (1966) and McPhee (1966) their tuning varies widely, and according to Tenzer (2000) they contain stretched octaves. It is now accepted that of the two primary tuning systems in gamelan music, slendro and pelog, only slendro somewhat resembles five-tone equal temperament, while pelog is highly unequal; however, in 1972 Surjodiningrat, Sudarjana and Susanto analyze pelog as equivalent to 9-TET (133-cent steps ).

A Thai xylophone measured by Morton in 1974 "varied only plus or minus 5 cents" from . According to Morton,

A South American Indian scale from a pre-instrumental culture measured by Boiles in 1969 featured 175 cent seven-tone equal temperament, which stretches the octave slightly, as with instrumental gamelan music.

Chinese music has traditionally used .















Other equal divisions of the octave that have found occasional use include 15 EDO, 17 EDO, and 22 EDO.

2, 5, 12, 41, 53, 306, 665 and 15601 are denominators of first convergents of log(3), so 2, 5, 12, 41, 53, 306, 665 and 15601 twelfths (and fifths), being in correspondent equal temperaments equal to an integer number of octaves, are better approximations of 2, 5, 12, 41, 53, 306, 665 and 15601 just twelfths/fifths than in any equal temperament with fewer tones.

1, 2, 3, 5, 7, 12, 29, 41, 53, 200, ... is the sequence of divisions of octave that provides better and better approximations of the perfect fifth. Related sequences containing divisions approximating other just intervals are listed in a footnote.

The equal-tempered version of the Bohlen–Pierce scale consists of the ratio 3:1 (1902 cents) conventionally a perfect fifth plus an octave (that is, a perfect twelfth), called in this theory a tritave (), and split into 13 equal parts. This provides a very close match to justly tuned ratios consisting only of odd numbers. Each step is 146.3 cents (), or .

Wendy Carlos created three unusual equal temperaments after a thorough study of the properties of possible temperaments with step size between 30 and 120 cents. These were called "alpha", "beta", and "gamma". They can be considered equal divisions of the perfect fifth. Each of them provides a very good approximation of several just intervals. Their step sizes:
Alpha and beta may be heard on the title track of Carlos's 1986 album "Beauty in the Beast".

In this section, "semitone" and "whole tone" may not have their usual 12 EDO meanings, as it discusses how they may be tempered in different ways from their just versions to produce desired relationships. Let the number of steps in a semitone be , and the number of steps in a tone be .

There is exactly one family of equal temperaments that fixes the semitone to any proper fraction of a whole tone, while keeping the notes in the right order (meaning that, for example, , , , , and are in ascending order if they preserve their usual relationships to ). That is, fixing to a proper fraction in the relationship also defines a unique family of one equal temperament and its multiples that fulfil this relationship.

For example, where is an integer, sets sets and sets The smallest multiples in these families (e.g. 12, 19 and 31 above) has the additional property of having no notes outside the circle of fifths. (This is not true in general; in 24 , the half-sharps and half-flats are not in the circle of fifths generated starting from .) The extreme cases are where and the semitone becomes a unison, and , where and the semitone and tone are the same interval.

Once one knows how many steps a semitone and a tone are in this equal temperament, one can find the number of steps it has in the octave. An equal temperament with the above properties (including having no notes outside the circle of fifths) divides the octave into and the perfect fifth into If there are notes outside the circle of fifths, one must then multiply these results by , the number of nonoverlapping circles of fifths required to generate all the notes (e.g., two in 24 , six in 72 ). (One must take the small semitone for this purpose: 19  has two semitones, one being tone and the other being . Similarly, 31  has two semitones, one being tone and the other being ).

The smallest of these families is and in particular, 12  is the smallest equal temperament with the above properties. Additionally, it makes the semitone exactly half a whole tone, the simplest possible relationship. These are some of the reasons 12  has become the most commonly used equal temperament. (Another reason is that 12 EDO is the smallest equal temperament to closely approximate 5 limit harmony, the next-smallest being 19 EDO.)

Each choice of fraction for the relationship results in exactly one equal temperament family, but the converse is not true: 47  has two different semitones, where one is tone and the other is , which are not complements of each other like in 19  ( and ). Taking each semitone results in a different choice of perfect fifth.

The diatonic tuning in "12 tone equal temperament" can be generalized to any regular diatonic tuning dividing the octave as a sequence of steps (or some circular shift or "rotation" of it). To be called a "regular" diatonic tuning, each of the two semitones () must be smaller than either of the tones (greater tone, , and lesser tone, ).
The comma is implicit as the size ratio between the greater and lesser tones: Expressed as frequencies or as cents .

The notes in a regular diatonic tuning are connected in a cycle of three perfect fifths , interrupted by a grave fifth ("grave" means "flat by a comma"), another sequence of two perfect fifths, and another grave fifth, and then it repeats indefinitely, flattening by two commas with every transition from natural to sharp pitches (or single sharps to double sharps), and reciprocally sharpening by two commas with every transition from natural pitches to flattened pitches (or flats to double flats). If left unmodified, the two grave fifths in each octave are the source of "wolf" intervals.

Since the comma, , expands the lesser tone into the greater tone, just intonation can be broken up into a sequence (or a circular shift of it) of diatonic semitones , chromatic semitones , and commas Various equal temperaments alter the interval sizes, usually breaking apart the three commas and then redistributing their parts into the seven diatonic semitones , or into the five chromatic semitones , or into both and , with some fixed proportion for each type of semitone.

The sequence of intervals , , and can be repeatedly appended to itself into a greater spiral of 12 fifths, and made to connect at its far ends by slight adjustments to the size of one or several of the intervals, or left unmodified with occasional less-than-perfect fifths, flat by a comma.

An equal temperament can be created if the sizes of the major and minor tones (, ) are altered to be the same (say, by setting , with the others expanded to still fill out the octave), and both semitones ( and ) the same size, then twelve equal semitones, two per tone, result. In [[12 equal temperament|, the semitone, , is exactly half the size of the same-size whole tones = .

Some of the intermediate sizes of tones and semitones can also be generated in equal temperament systems, by modifying the sizes of the comma and semitones. One obtains in the limit as the size of and tend to zero, with the octave kept fixed, and in the limit as and tend to zero; is of course, the case and For instance:







[[Category:Equal temperaments| ]]
[[Category:Chinese discoveries]]
Edward Gibbon

Edward Gibbon (; 8 May 173716 January 1794) was an English essayist, historian, and politician. His most important work, "The History of the Decline and Fall of the Roman Empire", published in six volumes between 1776 and 1789, is known for the quality and irony of its prose, its use of primary sources, and its polemical criticism of organised religion.

Edward Gibbon was born in 1737, the son of Edward and Judith Gibbon, at Lime Grove in the town of Putney, Surrey. He had five brothers and one sister, all of whom died in infancy. His grandfather, also named Edward, had lost his assets as a result of the South Sea bubble stock-market collapse in 1720 but eventually regained much of his wealth. Gibbon's father thus inherited a substantial estate. His paternal grandmother, Catherine Acton, was granddaughter of Sir Walter Acton, 2nd Baronet.

As a youth, Gibbon's health was under constant threat. He described himself as "a puny child, neglected by my Mother, starved by my nurse". At age nine, he was sent to Dr. Woddeson's school at Kingston upon Thames (now Kingston Grammar School), shortly after which his mother died. He then took up residence in the Westminster School boarding house, owned by his adored "Aunt Kitty", Catherine Porten. Soon after she died in 1786, he remembered her as rescuing him from his mother's disdain, and imparting "the first rudiments of knowledge, the first exercise of reason, and a taste for books which is still the pleasure and glory of my life". From 1747 Gibbon spent time at the family home in Buriton. By 1751, Gibbon's reading was already extensive and pointed toward his future pursuits: Laurence Echard's "Roman History" (1713), William Howel(l)'s "An Institution of General History" (1680–85), and several of the 65 volumes of the acclaimed "Universal History from the Earliest Account of Time" (1747–1768).

Following a stay at Bath in 1752 to improve his health at the age of 15, Gibbon was sent by his father to Magdalen College, Oxford, where he was enrolled as a gentleman-commoner. He was ill-suited, however, to the college atmosphere, and later rued his 14 months there as the "most idle and unprofitable" of his life. Because he says so in his autobiography, it used to be thought that a penchant from his aunt for "theological controversy" bloomed under the influence of the deist or rationalist theologian Conyers Middleton (1683–1750), the author of "Free Inquiry into the Miraculous Powers" (1749). In that tract, Middleton denied the validity of such powers; Gibbon promptly objected, or so the argument used to run. The product of that disagreement, with some assistance from the work of Catholic Bishop Jacques-Bénigne Bossuet (1627–1704), and that of the Elizabethan Jesuit Robert Parsons (1546–1610), yielded the most memorable event of his time at Oxford: his conversion to Roman Catholicism on 8 June 1753. He was further "corrupted" by the 'free thinking' deism of the playwright and poet David Mallet; and finally Gibbon's father, already "in despair," had had enough. David Womersley has shown, however, that Gibbon's claim to having been converted by a reading of Middleton is very unlikely, and was introduced only into the final draft of the "Memoirs" in 1792–93.

Within weeks of his conversion, he was removed from Oxford and sent to live under the care and tutelage of Daniel Pavillard, Reformed pastor of Lausanne, Switzerland. There, he made one of his life's two great friendships, that of Jacques Georges Deyverdun (the French-language translator of Goethe's "The Sorrows of Young Werther"), and that of John Baker Holroyd (later Lord Sheffield). Just a year and a half later, after his father threatened to disinherit him, on Christmas Day, 1754, he reconverted to Protestantism. "The various articles of the Romish creed," he wrote, "disappeared like a dream".

He also met the one romance in his life: the daughter of the pastor of Crassy, a young woman named Suzanne Curchod, who was later to become the wife of Louis XVI's finance minister Jacques Necker, and the mother of Madame de Staël. The two developed a warm affinity; Gibbon proceeded to propose marriage, but ultimately this was out of the question, blocked both by his father's staunch disapproval and Curchod's equally staunch reluctance to leave Switzerland. Gibbon returned to England in August 1758 to face his father. No refusal of the elder's wishes could be allowed. Gibbon put it this way: "I sighed as a lover, I obeyed as a son." He proceeded to cut off all contact with Curchod, even as she vowed to wait for him. Their final emotional break apparently came at Ferney, France, in early 1764, though they did see each other at least one more time a year later.

Upon his return to England, Gibbon published his first book, "Essai sur l'Étude de la Littérature" in 1761, which produced an initial taste of celebrity and distinguished him, in Paris at least, as a man of letters. From 1759 to 1770, Gibbon served on active duty and in reserve with the South Hampshire Militia, his deactivation in December 1762 coinciding with the militia's dispersal at the end of the Seven Years' War. The following year, he returned, via Paris, to Lausanne, where he made the acquaintance of a "prudent worthy young man" William Guise. On 18 April 1764, he and Guise set off for Italy, crossed the Alps, and after spending the summer in Florence arrived in Rome, via Lucca, Pisa, Livorno and Siena, in early October. In his autobiography, Gibbon vividly records his rapture when he finally neared "the great object of [my] pilgrimage":

...at the distance of twenty-five years I can neither forget nor express the strong emotions which agitated my mind as I first approached and entered the "eternal City". After a sleepless night, I trod, with a lofty step the ruins of the Forum; each memorable spot where Romulus "stood", or Tully spoke, or Caesar fell, was at once present to my eye; and several days of intoxication were lost or enjoyed before I could descend to a cool and minute investigation.

Here, Gibbon first conceived the idea of composing a history of the city, later extended to the entire empire, a moment he described later as his "Capitoline vision":

It was at Rome, on the fifteenth of October 1764, as I sat musing amidst the ruins of the Capitol, while the barefooted fryars were singing vespers in the temple of Jupiter, that the idea of writing the decline and fall of the City first started to my mind.

Womersley ("Oxford Dictionary of National Biography", p. 12) notes the existence of "good reasons" to doubt the statement's accuracy. Elaborating, Pocock ("Classical History," ¶ #2) refers to it as a likely "creation of memory" or a "literary invention", given that Gibbon, in his autobiography, claimed that his journal dated the reminiscence to 15 October, when in fact the journal gives no date.

In June 1765, Gibbon returned to his father's house, remaining there until the latter's death in 1770. These five years were considered by Gibbon as the worst of his life, but he tried to remain busy by making early attempts at full histories. His first historical narrative, known as the "History of Switzerland", representing Gibbon's love for Switzerland, was never finished nor published. Even under the guidance of Deyverdun, his German translator, Gibbon became too self-critical and completely abandoned the project after writing only 60 pages of text. 

Soon after abandoning his "History of Switzerland", Gibbon made another attempt towards completing a full history. His second work, "Memoires Litteraires de la Grande Bretagne", was a two-volume set describing the literary and social conditions of England at the time, such as Lord Lyttelton's history of Henry II and Nathaniel Lardner's "The Credibility of the Gospel History". Gibbon's "Memoires Litteraires" failed to gain any notoriety and was considered a flop by fellow historians and literary scholars.

After he tended to his father's estate—which was in poor condition—enough remained for Gibbon to settle fashionably in London at 7 Bentinck Street free of financial concern. By February 1773, he was writing in earnest, but not without the occasional self-imposed distraction. He took to London society quite easily, joined the better social clubs (including Dr. Johnson's Literary Club), and looked in from time to time on his friend Holroyd in Sussex. He succeeded Oliver Goldsmith at the Royal Academy as 'professor in ancient history', an honorary but prestigious position. In late 1774, he was initiated as a Freemason of the Premier Grand Lodge of England.

He was also, perhaps least productively in that same year, returned to the House of Commons for Liskeard, Cornwall through the intervention of his relative and patron, Edward Eliot. He became the archetypal back-bencher, benignly "mute" and "indifferent," his support of the Whig ministry invariably automatic. Gibbon lost the Liskeard seat in 1780 when Eliot joined the opposition, taking with him "the Electors of Leskeard [who] are commonly of the same opinion as Mr. El[l]iot." (Murray, p. 322.) The following year, owing to the good grace of Prime Minister Lord North, he was again returned to Parliament, this time for Lymington on a by-election.

After several rewrites, with Gibbon "often tempted to throw away the labours of seven years," the first volume of what was to become his life's major achievement, "The History of the Decline and Fall of the Roman Empire", was published on 17 February 1776. Through 1777, the reading public eagerly consumed three editions, for which Gibbon was rewarded handsomely: two-thirds of the profits, amounting to approximately £1,000.

Volumes II and III appeared on 1 March 1781, eventually rising "to a level with the previous volume in general esteem." Volume IV was finished in June 1784; the final two were completed during a second Lausanne sojourn (September 1783 to August 1787) where Gibbon reunited with his friend Deyverdun in leisurely comfort. By early 1787, he was "straining for the goal" and with great relief the project was finished in June. Gibbon later wrote:

Volumes IV, V, and VI finally reached the press in May 1788, their publication having been delayed since March so it could coincide with a dinner party celebrating Gibbon's 51st birthday (the 8th). Mounting a bandwagon of praise for the later volumes were such contemporary luminaries as Adam Smith, William Robertson, Adam Ferguson, Lord Camden, and Horace Walpole. Adam Smith told Gibbon that "by the universal assent of every man of taste and learning, whom I either know or correspond with, it sets you at the very head of the whole literary tribe at present existing in Europe."
In November 1788, he was elected a Fellow of the Royal Society, the main proposer being his good friend Lord Sheffield.

In 1783 Gibbon had been intrigued by the cleverness of Sheffield's 12-year-old eldest daughter, Maria, and he proposed to teach her himself. Over the following years he continued, creating a girl of sixteen who was both well educated, confident and determined to choose her own husband. Gibbon described her as a "mixture of just observation and lively imagery, the strong sense of a man expressed with the easy elegance of a female".

The years following Gibbon's completion of "The History" were filled largely with sorrow and increasing physical discomfort. He had returned to London in late 1787 to oversee the publication process alongside Lord Sheffield. With that accomplished, in 1789 it was back to Lausanne only to learn of and be "deeply affected" by the death of Deyverdun, who had willed Gibbon his home, La Grotte. He resided there with little commotion, took in the local society, received a visit from Sheffield in 1791, and "shared the common abhorrence" of the French Revolution. In 1793, word came of Lady Sheffield's death; Gibbon immediately left Lausanne and set sail to comfort a grieving but composed Sheffield. His health began to fail critically in December, and at the turn of the new year, he was on his last legs.

Among Edward Gibbon's maladies was gout. Gibbon is also believed to have suffered from an extreme case of scrotal swelling, probably a hydrocele testis, a condition that causes the scrotum to swell with fluid in a compartment overlying either testicle. In an age when close-fitting clothes were fashionable, his condition led to a chronic and disfiguring inflammation that left Gibbon a lonely figure. As his condition worsened, he underwent numerous procedures to alleviate the condition, but with no enduring success. In early January, the last of a series of three operations caused an unremitting peritonitis to set in and spread, from which he died.

The "English giant of the Enlightenment" finally succumbed at 12:45 pm, 16 January 1794 at age 56. He was buried in the Sheffield Mausoleum attached to the north transept of the Church of St Mary and St Andrew, Fletching, East Sussex, having died in Fletching while staying with his great friend, Lord Sheffield. Gibbon's estate was valued at approximately £26,000. He left most of his property to cousins. As stipulated in his will, Sheffield oversaw the sale of his library at auction to William Beckford for £950. What happened next suggests that Beckford may have known of Gibbon's moralistic, 'impertinent animadversion' at his expense in the presence of the Duchess of Devonshire at Lausanne. Gibbon's wish that his 6,000-book library would not be locked up 'under the key of a jealous master' was effectively denied by Beckford who retained it in Lausanne until 1801 before inspecting it, then locking it up again until at least as late as 1818 before giving most of the books back to Gibbon's physician Dr Scholl who had helped negotiate the sale in the first place. Beckford's annotated copy of the "Decline and Fall" turned up in Christie's in 1953, complete with his critique of what he considered the author's 'ludicrous self-complacency ... your frequent distortion of historical Truth to provoke a gibe, or excite a sneer ... your ignorance of oriental languages [etc.]'.

A view frequently attributed to Gibbon, that the Roman Empire fell due to its embrace of Christianity, is not widely accepted by scholars today. Gibbon argued that with the empire's new Christian character, large sums of wealth that would have otherwise been used in the secular affairs in promoting the state were transferred to promoting the activities of the Church. However, the pre-Christian empire also spent large financial sums on religious affairs and it is unclear whether or not the change of religion increased the amount of resources the empire spent on religion. Gibbon further argued that new attitudes in Christianity caused many Christians of wealth to renounce their lifestyles and enter a monastic lifestyle, and so stop participating in the support of the empire. However, while many Christians of wealth did become monastics, this paled in comparison to the participants in the imperial bureaucracy. Although Gibbon further pointed out that the importance Christianity placed on peace caused a decline in the number of people serving the military, the decline was so small as to be negligible for the army's effectiveness.

Many scholars argue that Gibbon did not in fact blame Christianity for the empire's fall, rather attributing its decline to the effects of luxury and the consequent erosion of its martial character. Such a view echoes the outlook of the Greek historian Polybius, who similarly explained the decadent Greek world's eclipse by the ascendant Roman Republic in Mediterranean affairs. In this understanding of Gibbon, the process of Rome's decay was well underway before Christian adherents numbered a large proportion of the empire. Hence, although Christianity may have helped hasten Rome's fall (e.g. by luring thousands of men to live ascetic lives in the deserts of Egypt in the fourth century), it was not the root cause.

Gibbon's work has been criticised for its scathing view of the Christian church as laid down in chapters XV and XVI, a situation that resulted in the banning of the book in several countries. Gibbon's alleged crime was disrespecting, and none too lightly, the character of sacred Christian doctrine, by "treat[ing] the Christian church as a phenomenon of general history, not a special case admitting supernatural explanations and disallowing criticism of its adherents". More specifically, the chapters excoriated the church for "supplanting in an unnecessarily destructive way the great culture that preceded it" and for "the outrage of [practising] religious intolerance and warfare".

Gibbon, in letters to Holroyd and others, expected some type of church-inspired backlash, but the harshness of the ensuing torrents exceeded anything he or his friends had anticipated. Contemporary detractors such as Joseph Priestley and Richard Watson stoked the nascent fire, but the most severe of these attacks was an "acrimonious" piece by the young cleric, Henry Edwards Davis.

Gibbon's apparent antagonism to Christian doctrine spilled over into the Jewish faith, leading to charges of anti-Semitism. For example, he wrote:

From the reign of Nero to that of Antoninus Pius, the Jews discovered a fierce impatience of the dominion of Rome, which repeatedly broke out in the most furious massacres and insurrections. Humanity is shocked at the recital of the horrid cruelties which they committed in the cities of Egypt, of Cyprus, and of Cyrene, where they dwelt in treacherous friendship with the unsuspecting natives; and we are tempted to applaud the severe retaliation which was exercised by the arms of legions against a race of fanatics, whose dire and credulous superstition seemed to render them the implacable enemies not only of the Roman government, but also of mankind.

Gibbon is considered to be a son of the Enlightenment and this is reflected in his famous verdict on the history of the Middle Ages: "I have described the triumph of barbarism and religion." Politically, he rejected the radical egalitarian movements of the time, notably the American and French Revolutions, and dismissed overly rationalistic applications of the rights of man.

Gibbon's work has been praised for its style, his piquant epigrams and its effective irony. Winston Churchill memorably noted in "My Early Life", "I set out upon...Gibbon's "Decline and Fall of the Roman Empire" [and] was immediately dominated both by the story and the style. ...I devoured Gibbon. I rode triumphantly through it from end to end and enjoyed it all." Churchill modelled much of his own literary style on Gibbon's. Like Gibbon, he dedicated himself to producing a "vivid historical narrative, ranging widely over period and place and enriched by analysis and reflection."

Unusually for the 18th century, Gibbon was never content with secondhand accounts when the primary sources were accessible (though most of these were drawn from well-known printed editions). "I have always endeavoured," he says, "to draw from the fountain-head; that my curiosity, as well as a sense of duty, has always urged me to study the originals; and that, if they have sometimes eluded my search, I have carefully marked the secondary evidence, on whose faith a passage or a fact were reduced to depend." In this insistence upon the importance of primary sources, Gibbon is considered by many to be one of the first modern historians:

In accuracy, thoroughness, lucidity, and comprehensive grasp of a vast subject, the 'History' is unsurpassable. It is the one English history which may be regarded as definitive...Whatever its shortcomings the book is artistically imposing as well as historically unimpeachable as a vast panorama of a great period.

The subject of Gibbon's writing, as well as his ideas and style, have influenced other writers. Besides his influence on Churchill, Gibbon was also a model for Isaac Asimov in his writing of "The Foundation Trilogy", which he said involved "a little bit of cribbin' from the works of Edward Gibbon".

Evelyn Waugh admired Gibbon's style, but not his secular viewpoint. In Waugh's 1950 novel "Helena", the early Christian author Lactantius worries about the possibility of "'a false historian, with the mind of Cicero or Tacitus and the soul of an animal,' and he nodded towards the gibbon who fretted his golden chain and chattered for fruit."


Most of this article, including quotations unless otherwise noted, has been adapted from Stephen's entry on Edward Gibbon in the "Dictionary of National Biography".





East Pakistan

East Pakistan was the eastern polity of the Islamic Republic of Pakistan, established in 1955 under the One Unit Policy, renaming and restructuring the province as such from East Bengal, which, in modern times, is split between India and Bangladesh. Its land borders were with India and Burma, with a coastline on the Bay of Bengal. East Pakistanis were popularly known as "Pakistani Bengalis"; to distinguish this region from India's state West Bengal (which is also known as "Indian Bengal"), East Pakistan was known as "Pakistani Bengal". In 1971, East Pakistan became the newly independent state Bangladesh, which means "country of Bengal" or "country of Bengalis" in Bengali language.

East Pakistan was renamed from East Bengal by the One Unit Scheme of Pakistani Prime Minister Mohammad Ali of Bogra. The Constitution of Pakistan of 1956 replaced the Pakistani monarchy with an Islamic republic. Bengali politician H.S. Suhrawardy served as the Prime Minister of Pakistan between 1956 and 1957 and a Bengali bureaucrat Iskander Mirza became the first President of Pakistan. The 1958 Pakistani coup d'état brought general Ayub Khan to power. Khan replaced Mirza as president and launched a crackdown against pro-democracy leaders. Khan enacted the Constitution of Pakistan of 1962 which ended universal suffrage. By 1966, Sheikh Mujibur Rahman emerged as the preeminent opposition leader in Pakistan and launched the six-point movement for autonomy and democracy. The 1969 uprising in East Pakistan contributed to Ayub Khan's overthrow. Another general, Yahya Khan, usurped the presidency and enacted martial law. In 1970, Yahya Khan organised Pakistan's first federal general election. The Awami League emerged as the single largest party, followed by the Pakistan Peoples Party. The military junta stalled in accepting the results, leading to civil disobedience, the Bangladesh Liberation War, 1971 Bangladesh genocide and persecution of Biharis. East Pakistan seceded with the help of India.

The East Pakistan Provincial Assembly was the legislative body of the territory, it was the largest provincial legislature in Pakistan and elections were held only twice in 1954 and 1970. During the Bangladesh Liberation War in 1971, most Bengali members elected to the Pakistani National Assembly and the East Pakistani provincial assembly became members of the Constituent Assembly of Bangladesh. 

Due to the strategic importance of East Pakistan, the Pakistani union was a member of the Southeast Asia Treaty Organization. The economy of East Pakistan grew at an average of 2.6% between 1960 and 1965. The federal government invested more funds and foreign aid in West Pakistan, even though East Pakistan generated a major share of exports. However, President Ayub Khan did implement significant industrialisation in East Pakistan. The Kaptai Dam was built in 1965. The Eastern Refinery was established in Chittagong. Dacca was declared as the "second capital" of Pakistan and planned as the home of the national parliament. The government recruited American architect Louis Kahn to design the national assembly complex in Dacca.

Chaudhry Rehmat Ali, who did not include Bengal in the coined word "PAKISTAN", created a state among many in India in his book "Now or Never pamphlet" (1933). He called Bengal "Bang-e-Islam" (call to prayer of Islam) and included all of Bengal, West Bengal too. Bengal was a Muslim-majority province. Although he had punned on the word. To common Pakistanis it was called "Oriental Pakistan" or Islamically, as "Bangalistan". The word "Mashriqi" implies as Eastern. Kazim, in his book of reviews, "Kal ki Baat" ("Readings Lahore", 2010), tells us that Aurangzeb's minister Abul Fazl had opined that Bangla was actually Bangal and that 'al' in it meant enclosure. Today, 'aal' is taken to mean home, from a sense of 'outer wall making an enclosure', which is exactly what Bangla-Desh is today.

In 1955, Prime Minister Mohammad Ali Bogra implemented the One Unit scheme which merged the four western provinces into a single unit called West Pakistan while East Bengal was renamed as East Pakistan.

Pakistan ended its dominion status and adopted a republican constitution in 1956, which proclaimed an Islamic republic. The populist leader H. S. Suhrawardy of East Pakistan was appointed prime minister of Pakistan. As soon as he became the prime minister, Suhrawardy initiated legal work reviving the joint electorate system. There was strong opposition and resentment to the joint electorate system in West Pakistan. The Muslim League had taken the cause to the public and began calling for the implementation of a separate electorate system. In contrast to West Pakistan, the joint electorate was highly popular in East Pakistan. The tug of war with the Muslim League to establish the appropriate electorate caused problems for his government.

The constitutionally obliged National Finance Commission Program (NFC Program) was immediately suspended by Prime Minister Suhrawardy despite the reserves of the four provinces of West Pakistan in 1956. Suhrawardy advocated for the USSR-based Five-Year Plans to centralise the national economy. In this view, East Pakistan's economy would be quickly centralised and all major economic planning would be shifted to West Pakistan.

Efforts leading to centralising the economy were met with great resistance in West Pakistan when the elite monopolist and the business community angrily refused to adhere to his policies. The business community in Karachi began its political struggle to undermine any attempts of financial distribution of the US$10 million ICA aid to the better part of East Pakistan and to set up a consolidated national shipping corporation. In the financial cities of West Pakistan, such as Karachi, Lahore, Quetta, and Peshawar, a series of major labour strikes against the economic policies of Suhrawardy were supported by the elite business community and the private sector.

Furthermore, in order to divert attention from the controversial One Unit Program, Prime Minister Suhrawardy tried to end the crisis by calling a small group of investors to set up small businesses in the country. Despite many initiatives and holding off the NFC Award Program, Suhrawardy's political position and image deteriorated in the four provinces in West Pakistan. Many nationalist leaders and activists of the Muslim League were dismayed by the suspension of the constitutionally obliged NFC Program. His critics and Muslim League leaders observed that with the suspension of the NFC Award Program, Suhrawardy tried to give more financial allocations, aids, grants, and opportunities to East Pakistan than West Pakistan, including West Pakistan's four provinces. During the last days of his Prime ministerial years, Suhrawardy tried to remove the economic disparity between the Eastern and Western wings of the country but to no avail. He also tried unsuccessfully to alleviate the food shortage in the country.

Suhrawardy strengthened relations with the United States by reinforcing Pakistani membership in the Central Treaty Organization and the Southeast Asia Treaty Organization. Suhrawardy also promoted relations with the People’s Republic of China.

His contribution in formulating the 1956 constitution of Pakistan was substantial as he played a vital role in incorporating provisions for civil liberties and universal adult franchise in line with his adherence to the parliamentary form of liberal democracy.

In 1958, President Iskandar Mirza enacted martial law as part of a military coup by the Pakistan Army's chief Ayub Khan. Roughly after two weeks, President Mirza's relations with Pakistan Armed Forces deteriorated leading Army Commander General Ayub Khan relieving the president from his presidency and forcefully exiling President Mirza to the United Kingdom. General Ayub Khan justified his actions after appearing on national radio declaring that: "the armed forces and the people demanded a clean break with the past...". Until 1962, the martial law continued while Field Marshal Ayub Khan purged a number of politicians and civil servants from the government and replaced them with military officers. Ayub called his regime a "revolution to clean up the mess of black marketing and corruption". Khan replaced Mirza as president and became the country’s strongman for eleven years. Martial law continued until 1962 when the government of Field Marshal Ayub Khan commissioned a constitutional bench under Chief Justice of Pakistan Muhammad Shahabuddin, composed of ten senior justices, each five from East Pakistan and five from West Pakistan. On 6 May 1961, the commission sent its draft to President Ayub Khan. He thoroughly examined the draft while consulting with his cabinet.

In January 1962, the cabinet finally approved the text of the new constitution, promulgated by President Ayub Khan on 1 March 1962, which came into effect on 8 June 1962. Under the 1962 constitution, Pakistan became a presidential republic. Universal suffrage was abolished in favour of a system dubbed 'Basic Democracy'. Under the system, an electoral college would be responsible for electing the president and national assembly. The 1962 constitution created a gubernatorial system in West and East Pakistan. Each province ran its own separate provincial gubernatorial governments. The constitution defined a division of powers between the central government and the provinces. Fatima Jinnah received strong support in East Pakistan during her failed bid to unseat Ayub Khan in the 1965 presidential election.

Dacca was declared as the "second capital" of Pakistan in 1962. It was designated as the legislative capital and Louis Kahn was tasked with designing a national assembly complex. Dacca's population increased in the 1960s. Seven natural gas fields were tapped in the province. The petroleum industry developed as the Eastern Refinery was established in the port city of Chittagong.

In 1966, Awami League leader Sheikh Mujibur Rahman announced the six-point movement in Lahore. The movement demanded greater provincial autonomy and the restoration of democracy in Pakistan. Rahman was indicted for treason during the Agartala Conspiracy Case after launching the six-point movement. He was later released in the 1969 uprising in East Pakistan. Ayub Khan resigned in March 1969. Below includes the historical six points:

Muhammad Ayub Khan was replaced by general Yahya Khan who became the Chief Martial Law Administrator. Khan organised the 1970 Pakistani general election. The 1970 Bhola cyclone was one of the deadliest natural disasters of the 20th century. The cyclone claimed half a million lives. The disastrous effects of the cyclone caused huge resentment against the federal government. After a decade of military rule, East Pakistan was a hotbed of Bengali nationalism. There were open calls for self-determination.

When the federal general election was held, the Awami League emerged as the single largest party in the Pakistani parliament. The League won 167 out of 169 seats in East Pakistan, thereby crossing the half way mark of 150 in the 300-seat National Assembly of Pakistan. In theory, this gave the League the right to form a government under the Westminster tradition. But the League failed to win a single seat in West Pakistan, where the Pakistan Peoples Party emerged as the single largest party with 81 seats. The military junta stalled the transfer of power and conducted prolonged negotiations with the League. A civil disobedience movement erupted across East Pakistan demanding the convening of parliament. Rahman announced a struggle for independence from Pakistan during a speech on 7 March 1971 and called for a non-cooperation movement from the Bengali populace. Between 7–26 March, East Pakistan was virtually under the popular control of the Awami League. On Pakistan's Republic Day on 23 March 1971, the first flag of Bangladesh was hoisted in many East Pakistani households. Pakistan Army was ordered to immediately launch a crackdown on 26 March whose purpose was to curb the resistance, some of these operations include Operation Searchlight and the 1971 Dhaka University massacre. This led to the Bangladeshi Declaration of Independence.

As the Bangladesh Liberation War and the 1971 Bangladesh genocide continued for nine months, East Pakistani military units like the East Bengal Regiment and the East Pakistan Rifles defected and formed the Bangladesh Forces. The Provisional Government of Bangladesh allied with neighbouring India which intervened in the final two weeks of the war and secured the surrender of Pakistan's eastern command.

With Ayub Khan ousted from office in 1969, Commander of the Pakistani Army, General Yahya Khan became the country's second ruling chief martial law administrator. Both Bhutto and Mujib strongly disliked General Khan, but patiently endured him and his government as he had promised to hold an election in 1970. During this time, strong nationalistic sentiments in East Pakistan were perceived by the Pakistani Armed Forces and the central military government. Therefore, Khan and his military government wanted to divert the nationalistic threats and violence against non-East Pakistanis. The Eastern Command was under constant pressure from the Awami League and requested an active-duty officer to control the command under such extreme pressure. The high flag rank officers, junior officers, and many high command officers from Pakistan's Armed Forces were highly cautious about their appointment in East-Pakistan, and the assignment of governing East Pakistan and appointment of an officer was considered highly difficult for the Pakistan High Military Command.
East Pakistan's Armed Forces, under the military administrations of Major-General Muzaffaruddin and Lieutenant-General Sahabzada Yaqub Khan, used an excessive amount of show of military force to curb the uprising in the province. With such action, the situation became highly critical and civil control over the province slipped away from the government. On 24 March, dissatisfied with the performance of his generals, Yahya Khan removed General Muzaffaruddin and General Yaqub Khan from office on 1 September 1969. The appointment of a military administrator was considered quite difficult and challenging with the crisis continually deteriorating. Vice-Admiral Syed Mohammad Ahsan, Commander-in-Chief of the Pakistan Navy, had previously served as political and military adviser of East Pakistan to former President Ayub Khan. Having such a strong background in administration, and being an expert on East Pakistan affairs, General Yahya Khan appointed Vice-Admiral Syed Mohammad Ahsan as Martial Law Administrator, with absolute authority in his command. He was relieved as naval chief and received an extension from the government.

The tense relations between East and West Pakistan reached a climax in 1970 when the Awami League, the largest East Pakistani political party, led by Sheikh Mujibur Rahman, (Mujib), won a landslide victory in the national elections in East Pakistan. The party won 160 of the 162 seats allotted to East Pakistan, and thus a majority of the 300 seats in the Parliament. This gave the Awami League the constitutional right to form a government without forming a coalition with any other party. Khan invited Mujib to Rawalpindi to take the charge of the office, and negotiations took place between the military government and the Awami Party. Bhutto was shocked with the results and threatened his fellow Peoples Party members if they attended the inaugural session at the National Assembly, famously saying he would "break the legs" of any member of his party who dared enter and attend the session. However, fearing East Pakistani separatism, Bhutto demanded Mujib to form a coalition government. After a secret meeting held in Larkana, Mujib agreed to give Bhutto the office of the presidency with Mujib as prime minister. General Yahya Khan and his military government were kept unaware of these developments and under pressure from his own military government, refused to allow Rahman to become the prime minister of Pakistan. This increased agitation for greater autonomy in East Pakistan. The military police arrested Mujib and Bhutto and placed them in Adiala Jail in Rawalpindi. The news spread like a fire in both East and West Pakistan, and the struggle for independence began in East Pakistan.

The senior high command officers in Pakistan Armed Forces, and Zulfikar Ali Bhutto, began to pressure General Yahya Khan to take armed action against Mujib and his party. Bhutto later distanced himself from Yahya Khan after he was arrested by Military Police along with Mujib. Soon after the arrests, a high-level meeting was chaired by Yahya Khan. During the meeting, high commanders of the Pakistan Armed Forces unanimously recommended an armed and violent military action. East Pakistan's Martial Law Administrator Admiral Ahsan, Governor of East Pakistan, and Air Commodore Zafar Masud, Air Officer Commanding of Dacca's only airbase, were the only officers to object to the plans. When it became obvious that military action in East Pakistan was inevitable, Admiral Ahsan resigned from his position as martial law administrator in protest, and immediately flew back to Karachi, West Pakistan. Disheartened and isolated, Admiral Ahsan took early retirement from the Navy and quietly settled in Karachi. Once Operation Searchlight and Operation Barisal commenced, Air Marshal Masud flew to West Pakistan, and unlike Admiral Ahsan, tried to stop the violence in East Pakistan. When he failed in his attempts to meet General Yahya Khan, Masud too resigned from his position as AOC of Dacca airbase and took retirement from Air Force.

Lieutenant-General Sahibzada Yaqub Khan was sent into East Pakistan in an emergency, following a major blow of the resignation of Vice Admiral Ahsan. General Yaqub temporarily assumed the control of the province, he was also made the corps-commander of Eastern Corps. General Yaqub mobilised the entire major forces in East Pakistan.

Sheikh Mujibur Rahman made a declaration of independence at Dacca on 26 March 1971. All major Awami League leaders including elected leaders of the National Assembly and Provincial Assembly fled to neighbouring India and an exile government was formed headed by Sheikh Mujibur Rahman. While he was in a Pakistan prison, Syed Nazrul Islam was the acting president with Tajuddin Ahmed as the prime minister. The exile government took oath on 17 April 1971 at Mujib Nagar, within East Pakistan territory of Kushtia district, and formally formed the government. Colonel MOG Osmani was appointed the Commander in Chief of Liberation Forces and whole East Pakistan was divided into eleven sectors headed by eleven sector commanders. All sector commanders were Bengali officers who had defected from the Pakistan Army. This started the nine-month long Bangladesh Liberation War in which the freedom fighters, joined in December 1971 by 400,000 Indian soldiers, faced the Pakistani Armed Forces of 365,000 plus paramilitary and collaborationist forces. An additional approximately 25,000 ill-equipped civilian volunteers and police forces also sided with the Pakistan Armed Forces. Bloody guerrilla warfare ensued in East Pakistan.

The Pakistan Armed Forces were unable to counter such threats. With no intel and low morale, they performed poorly and were inexperienced in guerrilla tactics, Pakistan Armed Forces and their assets were defeated by the Bangladesh Liberation Forces. In April 1971, Lieutenant-General Tikka Khan succeeded General Yaqub Khan as the Corps Commander. General Tikka Khan led the massive violent and massacre campaigns in the region. He is held responsible for killing hundreds of thousands of Bengali people in East Pakistan, mostly civilians and unarmed peoples. For his role, General Tikka Khan gained the title of "Butcher of Bengal". General Khan faced an international reaction against Pakistan, and therefore, General Tikka was removed as Commander of the Eastern front. He installed a civilian administration under Abdul Motaleb Malik on 31 August 1971, which proved to be ineffective. However, during the meeting, with no high officers willing to assume the command of East Pakistan, Lieutenant-General Amir Abdullah Khan Niazi volunteered for the command of East Pakistan. Inexperienced and the large magnitude of this assignment, the government sent Rear-Admiral Mohammad Shariff as Flag Officer Commanding of Eastern Naval Command (Pakistan). Admiral Shariff served as the deputy of General Niazi when doing joint military operations. However, General Niazi proved to be a failure and ineffective ruler. Therefore, General Niazi and Air Commodore Inamul Haque Khan, AOC, PAF Base Dacca, failed to launch any operation in East Pakistan against Indian or its allies. Except for Admiral Shariff who continued to keep pressure on the Indian Navy until the end of the conflict. Admiral Shariff's effective plans made it nearly impossible for the Indian Navy to land its naval forces on the shores of East Pakistan. The Indian Navy was unable to land forces in East Pakistan and the Pakistan Navy was still offering resistance. The Indian Army, entered East Pakistan from all three directions of the province. The Indian Navy then decided to wait near the Bay of Bengal until the Army reached the shore.

The Indian Air Force dismantled the capability of the Pakistan Air Force in East Pakistan. Air Commodore Inamul Haque Khan, Dacca airbase's AOC, failed to offer any serious resistance to the actions of the Indian Air Force. For the most part of the war, the IAF enjoyed complete dominance in the skies over East Pakistan.

On 16 December 1971, the Pakistan Armed Forces surrendered to the joint liberation forces of Mukti Bahini and the Indian Army, headed by Lieutenant-General Jagjit Singh Arora, the General Officer Commanding-in-Chief (GOC-in-C) of the Eastern Command of the Indian Army. Lieutenant General AAK Niazi, the last corps commander of Eastern Corps, signed the Instrument of Surrender at about 4:31 pm. Over 93,000 personnel, including Lt. General Niazi and Admiral Shariff, were taken as prisoners of war.

On 16 December 1971, the territory of East Pakistan was handed over to Indian Army under the surrender agreement from West Pakistan and in the Simla Agreement became the newly independent state of Bangladesh. The Eastern Command, civilian institutions, and paramilitary forces were disbanded in the following months.

In contrast to the desert and rugged mountainous terrain of West Pakistan, East Pakistan featured the world's largest delta, 700 rivers, and tropical hilly jungles. The Chittagong Division of East Pakistan was home to hill ranges and forests (mainly in the Chittagong Hill Tracts and Sylhet). The Khulna Division and parts of the Dacca and Chittagong Divisions were largely Deltaic. East Pakistan was almost entirely an alluvial plain which consists of lower course of the Padma and Jamuna. Climatically, East Pakistan was essentially humid, hot climate with heavy to very heavy rainfall. The implication of East Pakistan's heavy rainfall was that the main crops that were grown in East Pakistan were rice, tea, and jute.

East Pakistan inherited 17 districts from British Bengal. 

In 1960, Lower Tippera was renamed Comilla. 

In 1969, two new districts were created with Tangail separated from Mymensingh and Patuakhali from Bakerganj. 

East Pakistan's districts are listed in the following.

At the time of the Partition of British India, East Bengal had a plantation economy. The Chittagong Tea Auction was established in 1949 as the region was home to the world's largest tea plantations. The East Pakistan Stock Exchange Association was established in 1954. Many wealthy Muslim immigrants from India, Burma, and former British colonies settled in East Pakistan. The Ispahani family, Africawala brothers, and the Adamjee family were pioneers of industrialisation in the region. Many of modern Bangladesh's leading companies were born in the East Pakistan period.

An airline founded in British Bengal, Orient Airways, launched the vital air link between East and West Pakistan with DC-3 aircraft on the Dacca-Calcutta-Delhi-Karachi route. Orient Airways later evolved into Pakistan International Airlines, whose first chairman was the East Pakistan-based industrialist Mirza Ahmad Ispahani.

By the 1950s, East Bengal surpassed West Bengal in having the largest jute industries in the world. The Adamjee Jute Mills was the largest jute processing plant in history and its location in Narayanganj was nicknamed the "Dundee of the East". The Adamjees were descendants of Sir Haji Adamjee Dawood, who made his fortune in British Burma.

Natural gas was discovered in the northeastern part of East Pakistan in 1955 by the Burmah Oil Company. Industrial use of natural gas began in 1959. The Shell Oil Company and Pakistan Petroleum tapped 7 gas fields in the 1960s. The industrial seaport city of Chittagong hosted the headquarters of Burmah Eastern and Pakistan National Oil. Iran, an erstwhile leading oil producer, assisted in establishing the Eastern Refinery in Chittagong.

The Comilla Model of the Pakistan Academy for Rural Development (present-day Bangladesh Academy for Rural Development) was conceived by Akhtar Hameed Khan and replicated in many developing countries.

In 1965, Pakistan implemented the Kaptai Dam hydroelectric project in the southeastern part of East Pakistan with American assistance. It was the sole hydroelectric dam in East Pakistan. The project was controversial for displacing over 40,000 indigenous people from the area.

The centrally located metropolis Dacca witnessed significant urban growth.

Although East Pakistan had a larger population, West Pakistan dominated the divided country politically and received more money from the common budget. According to the World Bank, there was much economic discrimination against East Pakistan, including higher government spending on West Pakistan, financial transfers from East to West, and the use of the East's foreign exchange surpluses to finance the West's imports.

The discrimination occurred despite the fact that East Pakistan generated a major share of Pakistan's exports.

The annual rate of growth of the gross domestic product per capita was 4.4% in West Pakistan versus 2.6% in East Pakistan from 1960 to 1965. Bengali politicians pushed for more autonomy, arguing that much of Pakistan's export earnings were generated in East Pakistan from the exportation of Bengali jute and tea. As late as 1960, approximately 70% of Pakistan's export earnings originated in East Pakistan, although this percentage declined as international demand for jute dwindled. By the mid-1960s, East Pakistan was accounting for less than 60% of the nation's export earnings, and by the time Bangladesh gained its independence in 1971, this percentage had dipped below 50%. In 1966, Mujib demanded that separate foreign exchange accounts be kept and that separate trade offices be opened overseas. By the mid-1960s, West Pakistan was benefiting from Ayub's "Decade of Progress" with its successful Green Revolution in wheat and from the expansion of markets for West Pakistani textiles, while East Pakistan's standard of living remained at an abysmally low level. Bengalis were also upset that West Pakistan, the seat of the national government, received more foreign aid. However, East Pakistan did nonetheless benefit from industrialisation and development, which was discerned by the Kaptai Dam in the Chittagong Hill Tracts for instance.

Economists in East Pakistan argued a "Two Economies Theory" within Pakistan itself, which was founded on the Two-Nation Theory with India. The so-called Two Economies Theory suggested that East and West Pakistan had different economic features which should not be regulated by a federal government in Islamabad.

East Pakistan was home to 55% of Pakistan's population. The largest ethnic group of the province were Bengalis, who in turn were the largest ethnic group in Pakistan. Bengali Muslims formed the predominant majority, followed by Bengali Hindus, Bengali Buddhists and Bengali Christians. East Pakistan also had many tribal groups, including the Chakmas, Marmas, Tangchangyas, Garos, Manipuris, Tripuris, Santhals and Bawms. They largely followed the religions of Buddhism, Christianity and Hinduism. East Pakistan was home to immigrant Muslims from across the Indian subcontinent, including West Bengal, Bihar, Sindh, Gujarat, the Northwest Frontier Province, Assam, Orissa, the Punjab and Kerala. A small Armenian and Jewish minority resided in East Pakistan.

The Asiatic Society of Pakistan was founded in Old Dacca by Ahmad Hasan Dani in 1948. The Varendra Research Museum in Rajshahi was an important center of research on the Indus Valley civilization. The Bangla Academy was established in 1954.

Among East Pakistan's newspapers, "The Daily Ittefaq" was the leading Bengali language title; while "Holiday" was a leading English title.

At the time of partition, East Bengal had 80 cinemas. The first movie produced in East Pakistan was The Face and the Mask in 1955. Pakistan Television established its second studio in Dacca after Lahore in 1965. Runa Laila was Pakistan's first pop star and became popular in India as well. Shabnam was a leading actress from East Pakistan. Feroza Begum was a leading exponent of Bengali classical Nazrul geeti. Jasimuddin and Abbasuddin Ahmed promoted Bengali folk music. Munier Chowdhury, Syed Mujtaba Ali, Nurul Momen, Sufia Kamal and Shamsur Rahman were among the leading literary figures in East Pakistan. Several East Pakistanis were awarded the Sitara-e-Imtiaz and the Pride of Performance.

As per the 1951 census, East Pakistan had a population of 44,251,826 people, of which 34,029,654 followed Islam, 9,757,527 people followed Hinduism and 464,644 people followed other religions: Buddhism, Christianity and Animism. According to the 1961 census, Muslims made up 80.4% of the population, Hindus were 18.4%, and the remaining 1.2% belonged to other religions, mainly Christianity and Buddhism.

Bengalis were hugely under-represented in Pakistan's bureaucracy and military. In the federal government, only 15% of offices were occupied by East Pakistanis. Only 10% of the military were from East Pakistan. Cultural discrimination also prevailed, causing the eastern wing to forge a distinct political identity. There was a bias against Bengali culture in state media, such as a ban on broadcasts of the works of Nobel laureate Rabindranath Tagore.

Since its unification with Pakistan, the East Pakistan Army had consisted of only one infantry brigade made up of two battalions, the 1st East Bengal Regiment and the 1/14 or 3/8 Punjab Regiment in 1948. These two battalions boasted only five rifle companies between them (an infantry battalion normally had 5 companies). This weak brigade was under the command of Brigadier Ayub Khan (acting Major-General – GOC of 14th Army Division), together with the East Pakistan Rifles, which was tasked with defending East Pakistan during the Indo-Pakistani War of 1947. The PAF, Marines, and the Navy had little presence in the region. Only one PAF combatant squadron, No. 14 Squadron "Tail Choppers", was active in East Pakistan. This combatant squadron was commanded by Squadron Leader Parvaiz Mehdi Qureshi, who later became a four-star general. The East Pakistan military personnel were trained in combat diving, demolitions, and guerrilla/anti-guerrilla tactics by the advisers from the Special Service Group (Navy) who were also charged with intelligence data collection and management cycle.

The East Pakistan Navy had only one active-duty combatant destroyer, the PNS "Sylhet"; one submarine "Ghazi" (which was repeatedly deployed in the West); four gunboats, inadequate to function in deep water. The joint special operations were managed and undertaken by the Naval Special Service Group (SSG(N)) who was assisted by the army, air force, and marines unit. The entire service, the Marines were deployed in East Pakistan, initially tasked with conducting exercises and combat operations in riverine areas and at the near shoreline. The small directorate of Naval Intelligence (while the headquarters and personnel, facilities, and directions were coordinated by West) had a vital role in directing special and reconnaissance missions, and intelligence gathering also was charged with taking reasonable actions to slow down the Indian threat. The armed forces of East Pakistan also consisted of the paramilitary organisation, the "Razakars" from the intelligence unit of the ISI's Covert Action Division (CAD).

The trauma was extremely severe in Pakistan when the news of secession of East Pakistan as Bangladesh arrived—a psychological setback, complete and humiliating defeat that shattered the prestige of the Pakistan Armed Forces. The governor and martial law administrator, Lieutenant-General Amir Abdullah Khan Niazi, was defamed, his image was maligned and he was stripped of his honours. The people of Pakistan could not come to terms with the magnitude of the defeat, and spontaneous demonstrations and mass protests erupted on the streets of major cities in (West) Pakistan. General Yahya Khan surrendered powers to Nurul Amin of the Pakistan Muslim League, the first and last vice-president and prime minister of Pakistan.

Prime Minister Amin invited then-President Zulfikar Ali Bhutto and the Pakistan Peoples Party to take control of Pakistan in a colourful ceremony where Bhutto gave a daring speech to the nation on national television. At the ceremony, Bhutto waved his fist in the air and pledged to his nation to never again allow the surrender of his country like what happened with East Pakistan. He launched and orchestrated the large-scale atomic bomb project in 1972. In memorial of East Pakistan, the East-Pakistan diaspora in Pakistan established the East-Pakistan colony in Karachi, Sindh. In accordance, the East-Pakistani diaspora also composed patriotic tributes to Pakistan after the war; songs such as "Sohni Dharti" (lit. "Beautiful Land") and "Jeevay, Jeevay Pakistan" (lit. "long-live, long-live Pakistan"), were composed by Bengali singer Shahnaz Rahmatullah in the 1970s and 1980s.

According to William Langewiesche, writing for "The Atlantic", "it may seem obvious that the loss of Bangladesh was a blessing"—but it has never been seen that way in Pakistan. In the book "Scoop! Inside Stories from the Partition to the Present", Indian politician Kuldip Nayar opined, "Losing East Pakistan and Bhutto's releasing of Mujib did not mean anything to Pakistan's policy—as if there was no liberation war". Bhutto's policy, and even today the policy of Pakistan, is that "she will continue to fight for the honour and integrity of Pakistan".



E. O. Wilson

Edward Osborne Wilson (June 10, 1929 – December 26, 2021) was an American biologist, naturalist, ecologist, and entomologist known for developing the field of sociobiology.

Born in Alabama, Wilson found an early interest in nature and frequented the outdoors. At age seven, he was partially blinded in a fishing accident. Due to his reduced sight, Wilson resolved to study entomology. After matriculating at the University of Alabama, Wilson transferred to complete his dissertation at Harvard University, where he distinguished himself in multiple fields. In 1956, he co-authored a paper defining the theory of character displacement. In 1967, he developed the theory of island biogeography with Robert MacArthur.

Wilson was the Pellegrino University Research Professor Emeritus in Entomology for the Department of Organismic and Evolutionary Biology at Harvard University, a lecturer at Duke University, and a fellow of the Committee for Skeptical Inquiry. The Royal Swedish Academy awarded Wilson the Crafoord Prize. He was a humanist laureate of the International Academy of Humanism. He was a two-time winner of the Pulitzer Prize for General Nonfiction (for "On Human Nature" in 1979, and "The Ants" in 1991) and a "New York Times" bestselling author for "The Social Conquest of Earth", "Letters to a Young Scientist", and "The Meaning of Human Existence".

Wilson's work received both praise and criticism during his lifetime. His book "Sociobiology" was a particular flashpoint for controversy, and drew criticism from the Sociobiology Study Group. Wilson's interpretation of the theory of evolution resulted in a widely reported dispute with Richard Dawkins. Examinations of his letters after his death revealed that he had supported the psychologist J. Philippe Rushton, whose work on race and intelligence is widely regarded by the scientific community as deeply flawed and racist.

Edward Osborne Wilson was born on June 10, 1929, in Birmingham, Alabama. He was the only child of Inez Linnette Freeman and Edward Osborne Wilson Sr. According to his autobiography, "Naturalist", he grew up in various towns in the Southern United States which included Mobile, Decatur, and Pensacola. From an early age, he was interested in natural history. His father was an alcoholic who eventually committed suicide. His parents allowed him to bring home black widow spiders and keep them on the porch. They divorced when he was seven years old.

In the same year that his parents divorced, Wilson blinded himself in his right eye in a fishing accident. Despite the prolonged pain, he did not stop fishing. He did not complain because he was anxious to stay outdoors, and never sought medical treatment. Several months later, his right pupil clouded over with a cataract. He was admitted to Pensacola Hospital to have the lens removed. Wilson writes, in his autobiography, that the "surgery was a terrifying [19th] century ordeal". Wilson retained full sight in his left eye, with a vision of 20/10. The 20/10 vision prompted him to focus on "little things": "I noticed butterflies and ants more than other kids did, and took an interest in them automatically." Although he had lost his stereoscopic vision, he could still see fine print and the hairs on the bodies of small insects. His reduced ability to observe mammals and birds led him to concentrate on insects.

At the age of nine, Wilson undertook his first expeditions at Rock Creek Park in Washington, D.C. He began to collect insects and he gained a passion for butterflies. He would capture them using nets made with brooms, coat hangers, and cheesecloth bags. Going on these expeditions led to Wilson's fascination with ants. He describes in his autobiography how one day he pulled the bark of a rotting tree away and discovered citronella ants underneath. The worker ants he found were "short, fat, brilliant yellow, and emitted a strong lemony odor". Wilson said the event left a "vivid and lasting impression". He also earned the Eagle Scout award and served as Nature Director of his Boy Scouts summer camp. At age 18, intent on becoming an entomologist, he began by collecting flies, but the shortage of insect pins during World War II caused him to switch to ants, which could be stored in vials. With the encouragement of Marion R. Smith, a myrmecologist from the National Museum of Natural History in Washington, Wilson began a survey of all the ants of Alabama. This study led him to report the first colony of fire ants in the U.S., near the port of Mobile.

Wilson said he went to 15 or 16 schools during 11 years of schooling. He
was concerned that he might not be able to afford to go to a university, and he tried to enlist in the United States Army, intending to earn U.S. government financial support for his education. He failed the Army medical examination due to his impaired eyesight, but was able to afford to enroll in the University of Alabama, where he earned his Bachelor of Science in 1949 and Master of Science in biology in 1950. The next year, Wilson transferred to Harvard University. 
Appointed to the Harvard Society of Fellows, he could travel on overseas expeditions, collecting ant species of Cuba and Mexico and travel the South Pacific, including Australia, New Guinea, Fiji, and New Caledonia, as well as to Sri Lanka. In 1955, he received his Ph.D. and married Irene Kelley.

From 1956 until 1996, Wilson was part of the faculty of Harvard. He began as an ant taxonomist and worked on understanding their microevolution, how they developed into new species by escaping environmental disadvantages and moving into new habitats. He developed a theory of the "taxon cycle".

In collaboration with mathematician William H. Bossert, Wilson developed a classification of pheromones based on insect communication patterns. In the 1960s, he collaborated with mathematician and ecologist Robert MacArthur in developing the theory of species equilibrium. In the 1970s he and biologist Daniel S. Simberloff tested this theory on tiny mangrove islets in the Florida Keys. They eradicated all insect species and observed the repopulation by new species. Wilson and MacArthur's book "The Theory of Island Biogeography" became a standard ecology text.

In 1971, he published "The Insect Societies", which argued that insect behavior and the behavior of other animals are influenced by similar evolutionary pressures. In 1973, Wilson was appointed the curator of entomology at the Harvard Museum of Comparative Zoology. In 1975, he published the book "" applying his theories of insect behavior to vertebrates, and in the last chapter, to humans. He speculated that evolved and inherited tendencies were responsible for hierarchical social organization among humans. In 1978 he published "On Human Nature", which dealt with the role of biology in the evolution of human culture and won a Pulitzer Prize for General Nonfiction.

Wilson was named the Frank B. Baird, Jr., Professor of Science in 1976 and, after his retirement from Harvard in 1996, he became the Pellegrino University Professor Emeritus.

In 1981 after collaborating with biologist Charles Lumsden, he published "Genes, Mind and Culture", a theory of gene-culture coevolution. In 1990 he published "The Ants", co-written with zoologist Bert Hölldobler, winning his second Pulitzer Prize for General Nonfiction.

In the 1990s, he published "The Diversity of Life" (1992); an autobiography, "Naturalist" (1994); and "Consilience: The Unity of Knowledge" (1998) about the unity of the natural and social sciences. Wilson was praised for his environmental advocacy, and his secular-humanist and deist ideas pertaining to religious and ethical matters.

Wilson was characterized by several titles during his career, including the "father of biodiversity," "ant man," and "Darwin's heir." In a PBS interview, David Attenborough described Wilson as "a magic name to many of us working in the natural world, for two reasons. First, he is a towering example of a specialist, a world authority. Nobody in the world has ever known as much as Ed Wilson about ants. But, in addition to that intense knowledge and understanding, he has the widest of pictures. He sees the planet and the natural world that it contains in amazing detail but extraordinary coherence".

Although Dawkins defended Wilson during the so-called "sociobiology debate", a disagreement between them arose over the theory of evolution. The disagreement began in 2012 when Dawkins wrote a critical review of Wilson's book "The Social Conquest of Earth" in "Prospect Magazine". In the review, Dawkins criticized Wilson for rejecting kin selection and for supporting group selection, labeling it "bland" and "unfocused," and he wrote that the book's theoretical errors were "important, pervasive, and integral to its thesis in a way that renders it impossible to recommend". Wilson responded in the same magazine and wrote that Dawkins made "little connection to the part he criticizes" and accused him of engaging in rhetoric.

In 2014, Wilson said in an interview, "There is no dispute between me and Richard Dawkins and there never has been, because he's a journalist, and journalists are people that report what the scientists have found and the arguments I’ve had have actually been with scientists doing research". Dawkins responded in a tweet: "I greatly admire EO Wilson & his huge contributions to entomology, ecology, biogeography, conservation, etc. He's just wrong on kin selection" and later added, "Anybody who thinks I'm a journalist who reports what other scientists think is invited to read "The Extended Phenotype"". Biologist Jerry Coyne wrote that Wilson's remarks were "unfair, inaccurate, and uncharitable". In 2021, in an obituary to Wilson, Dawkins stated that their dispute was "purely scientific". Dawkins wrote that he stands by his critical review and doesn't regret "its outspoken tone", but noted that he also stood by his "profound admiration for Professor Wilson and his life work".

Prior to Wilson's death, his personal correspondences were donated to the Library of Congress at the library's request. Following his death, several articles were published discussing the discrepancy between Wilson's legacy as a champion of biogeography and conservation biology, and his support of scientific racist pseudoscientist J. Philippe Rushton over several years. Rushton was a controversial psychologist at the University of Western Ontario, who later headed the Pioneer Fund.

From the late 1980s to the early 1990s, Wilson wrote several emails to Rushton's colleagues defending Rushton's work in the face of widespread criticism for scholarly misconduct, misrepresentation of data, and confirmation bias, all of which were allegedly used by Rushton to support his personal ideas on race. Wilson also sponsored an article written by Rushton in "PNAS", and during the review process, Wilson intentionally sought out reviewers for the article who he believed would likely already agree with its premise. Wilson kept his support of Rushton's racist ideologies behind-the-scenes so as to not draw too much attention to himself or tarnish his own reputation. Wilson responded to another request from Rushton to sponsor a second PNAS article with the following: "You have my support in many ways, but for me to sponsor an article on racial differences in the PNAS would be counterproductive for both of us." Wilson also remarked that the reason Rushton's ideologies were not more widely supported is because of the "... fear of being called racist, which is virtually a death sentence in American academia if taken seriously. I admit that I myself have tended to avoid the subject of Rushton's work, out of fear."

In 2022, the E.O. Wilson Biodiversity Foundation issued a statement rejecting Wilson's support of Rushton and racism, on behalf of the board of directors and staff.

Wilson used sociobiology and evolutionary principles to explain the behavior of social insects and then to understand the social behavior of other animals, including humans, thus establishing sociobiology as a new scientific field. He argued that all animal behavior, including that of humans, is the product of heredity, environmental stimuli, and past experiences, and that free will is an illusion. He referred to the biological basis of behavior as the "genetic leash". The sociobiological view is that all animal social behavior is governed by epigenetic rules worked out by the laws of evolution. This theory and research proved to be seminal, controversial, and influential.

Wilson argued that the unit of selection is a gene, the basic element of heredity. The "target" of selection is normally the individual who carries an ensemble of genes of certain kinds. With regard to the use of kin selection in explaining the behavior of eusocial insects, the "new view that I'm proposing is that it was group selection all along, an idea first roughly formulated by Darwin."

Sociobiological research was at the time particularly controversial with regard to its application to humans. The theory established a scientific argument for rejecting the common doctrine of tabula rasa, which holds that human beings are born without any innate mental content and that culture functions to increase human knowledge and aid in survival and success.

"Sociobiology: The New Synthesis" was initially met with praise by most biologists. After substantial criticism of the book was launched by the Sociobiology Study Group, associated with the organization Science for the People, a major controversy known as the "sociobiology debate" ensued, and Wilson was accused of racism, misogyny, and support for eugenics. Several of Wilson's colleagues at Harvard, such as Richard Lewontin and Stephen Jay Gould, both members of the Group, were strongly opposed. Both focused their criticism mostly on Wilson's sociobiological writings. Gould, Lewontin, and other members, wrote "Against 'Sociobiology'" in an open letter criticizing Wilson's "deterministic view of human society and human action". Other public lectures, reading groups, and press releases were organized criticizing Wilson's work. In response, Wilson produced a discussion article entitled "Academic Vigilantism and the Political Significance of Sociobiology" in "BioScience".

In February 1978, while participating in a discussion on sociobiology at the annual meeting of the American Association for the Advancement of Science, Wilson was surrounded, chanted at and doused with water by members of the International Committee Against Racism, who accused Wilson of advocating racism and genetic determinism. Steven Jay Gould, who was present at the event, and Science for the People, which had previously protested Wilson, condemned the attack.

Philosopher Mary Midgley encountered "Sociobiology" in the process of writing "Beast and Man" (1979) and significantly rewrote the book to offer a critique of Wilson's views. Midgley praised the book for the study of animal behavior, clarity, scholarship, and encyclopedic scope, but extensively critiqued Wilson for conceptual confusion, scientism, and anthropomorphism of genetics.

Wilson wrote in his 1978 book "On Human Nature", "The evolutionary epic is probably the best myth we will ever have." Wilson's fame prompted use of the morphed phrase epic of evolution. The book won the Pulitzer Prize in 1979.

Wilson, along with Bert Hölldobler, carried out a systematic study of ants and ant behavior, culminating in the 1990 encyclopedic work "The Ants". Because much self-sacrificing behavior on the part of individual ants can be explained on the basis of their genetic interests in the survival of the sisters, with whom they share 75% of their genes (though the actual case is some species' queens mate with multiple males and therefore some workers in a colony would only be 25% related), Wilson argued for a sociobiological explanation for all social behavior on the model of the behavior of the social insects.

Wilson said in reference to ants that "Karl Marx was right, socialism works, it is just that he had the wrong species". He asserted that individual ants and other eusocial species were able to reach higher Darwinian fitness putting the needs of the colony above their own needs as individuals because they lack reproductive independence: individual ants cannot reproduce without a queen, so they can only increase their fitness by working to enhance the fitness of the colony as a whole. Humans, however, do possess reproductive independence, and so individual humans enjoy their maximum level of Darwinian fitness by looking after their own survival and having their own offspring.

In his 1998 book "Consilience: The Unity of Knowledge", Wilson discussed methods that have been used to unite the sciences and might be able to unite the sciences with the humanities. He argued that knowledge is a single, unified thing, not divided between science and humanistic inquiry. Wilson used the term "consilience" to describe the synthesis of knowledge from different specialized fields of human endeavor. He defined human nature as a collection of epigenetic rules, the genetic patterns of mental development. He argued that culture and rituals are products, not parts, of human nature. He said art is not part of human nature, but our appreciation of art is. He suggested that concepts such as art appreciation, fear of snakes, or the incest taboo (Westermarck effect) could be studied by scientific methods of the natural sciences and be part of interdisciplinary research.

Wilson coined the phrase "scientific humanism" as "the only worldview compatible with science's growing knowledge of the real world and the laws of nature". Wilson argued that it is best suited to improve the human condition. In 2003, he was one of the signers of the "Humanist Manifesto".

On the question of God, Wilson described his position as "provisional deism" and explicitly denied the label of "atheist", preferring "agnostic". He explained his faith as a trajectory away from traditional beliefs: "I drifted away from the church, not definitively agnostic or atheistic, just Baptist & Christian no more." Wilson argued that belief in God and the rituals of religion are products of evolution. He argued that they should not be rejected or dismissed, but further investigated by science to better understand their significance to human nature. In his book "The Creation", Wilson wrote that scientists ought to "offer the hand of friendship" to religious leaders and build an alliance with them, stating that "Science and religion are two of the most potent forces on Earth and they should come together to save the creation."

Wilson made an appeal to the religious community on the lecture circuit at Midland College, Texas, for example, and that "the appeal received a 'massive reply'", that a covenant had been written and that a "partnership will work to a substantial degree as time goes on".

In a "New Scientist" interview published on January 21, 2015, however, Wilson said that religious faith is "dragging us down", and:
Wilson said that, if he could start his life over he would work in microbial ecology, when discussing the reinvigoration of his original fields of study since the 1960s. He studied the mass extinctions of the 20th century and their relationship to modern society, and identifying mass extinction as the greatest threat to Earth's future. In 1998 argued for an ecological approach at the Capitol:

From the late 1970s Wilson was actively involved in the global conservation of biodiversity, contributing and promoting research. In 1984 he published "Biophilia", a work that explored the evolutionary and psychological basis of humanity's attraction to the natural environment. This work introduced the word biophilia which influenced the shaping of modern conservation ethics. In 1988 Wilson edited the "BioDiversity" volume, based on the proceedings of the first US national conference on the subject, which also introduced the term biodiversity into the language. This work was very influential in creating the modern field of biodiversity studies. In 2011, Wilson led scientific expeditions to the Gorongosa National Park in Mozambique and the archipelagos of Vanuatu and New Caledonia in the southwest Pacific. Wilson was part of the international conservation movement, as a consultant to Columbia University's Earth Institute, as a director of the American Museum of Natural History, Conservation International, The Nature Conservancy and the World Wildlife Fund.

Understanding the scale of the extinction crisis led him to advocate for forest protection, including the "Act to Save America's Forests", first introduced in 1998 and reintroduced in 2008, but never passed. The Forests Now Declaration called for new markets-based mechanisms to protect tropical forests. Wilson once said destroying a rainforest for economic gain was like burning a Renaissance painting to cook a meal. In 2014, Wilson called for setting aside 50% of Earth's surface for other species to thrive in as the only possible strategy to solve the extinction crisis. The idea became the basis for his book "Half-Earth" (2016) and for the Half-Earth Project of the E.O. Wilson Biodiversity Foundation. Wilson's influence regarding ecology through popular science was discussed by Alan G. Gross in "The Scientific Sublime" (2018).

Wilson was instrumental in launching the Encyclopedia of Life (EOL) initiative with the goal of creating a global database to include information on the 1.9 million species recognized by science. Currently, it includes information on practically all known species. This open and searchable digital repository for organism traits, measurements, interactions and other data has more than 300 international partners and countless scientists providing global users' access to knowledge of life on Earth. For his part, Wilson discovered and described more than 400 species of ants.

In 1996, Wilson officially retired from Harvard University, where he continued to hold the positions of Professor Emeritus and Honorary Curator in Entomology. 
He fully retired from Harvard in 2002 at age 73. After stepping down, he published more than a dozen books, including a digital biology textbook for the iPad.

He founded the E.O. Wilson Biodiversity Foundation, which finances the PEN/E. O. Wilson Literary Science Writing Award and is an "independent foundation" at the Nicholas School of the Environment at Duke University. Wilson became a special lecturer at Duke University as part of the agreement.

Wilson and his wife, Irene, resided in Lexington, Massachusetts. He had a daughter, Catherine. He was preceded in death by his wife (on August 7, 2021) and died in nearby Burlington on December 26, 2021, at the age of 92.

Wilson's scientific and conservation honors include:







Edwin Howard Armstrong

Edwin Howard Armstrong (December 18, 1890 – February 1, 1954) was an American electrical engineer and inventor, who developed FM (frequency modulation) radio and the superheterodyne receiver system. 

He held 42 patents and received numerous awards, including the first Medal of Honor awarded by the Institute of Radio Engineers (now IEEE), the French Legion of Honor, the 1941 Franklin Medal and the 1942 Edison Medal. He achieved the rank of major in the U.S. Army Signal Corps during World War I and was often referred to as "Major Armstrong" during his career. He was inducted into the National Inventors Hall of Fame and included in the International Telecommunication Union's roster of great inventors. Armstrong attended Columbia University, and served as a professor there for most of his life.

Armstrong was born in the Chelsea district of New York City, the oldest of John and Emily (née Smith) Armstrong's three children. His father began working at a young age at the American branch of the Oxford University Press, which published bibles and standard classical works, eventually advancing to the position of vice president. His parents first met at the North Presbyterian Church, located at 31st Street and Ninth Avenue. His mother's family had strong ties to Chelsea, and an active role in church functions. When the church moved north, the Smiths and Armstrongs followed, and in 1895 the Armstrong family moved from their brownstone row house at 347 West 29th Street to a similar house at 26 West 97th Street in the Upper West Side. The family was comfortably middle class.

At the age of eight, Armstrong contracted Sydenham's chorea (then known as St. Vitus' Dance), an infrequent but serious neurological disorder precipitated by rheumatic fever. For the rest of his life, Armstrong was afflicted with a physical tic exacerbated by excitement or stress. Due to this illness, he withdrew from public school and was home-tutored for two years. To improve his health, the Armstrong family moved to a house overlooking the Hudson River, at 1032 Warburton Avenue in Yonkers. The Smith family subsequently moved next door. Armstrong's tic and the time missed from school led him to become socially withdrawn.

From an early age, Armstrong showed an interest in electrical and mechanical devices, particularly trains. He loved heights and constructed a makeshift backyard antenna tower that included a bosun's chair for hoisting himself up and down its length, to the concern of neighbors. Much of his early research was conducted in the attic of his parents' house.

In 1909, Armstrong enrolled at Columbia University in New York City, where he became a member of the Epsilon Chapter of the Theta Xi engineering fraternity, and studied under Professor Michael Pupin at the Hartley Laboratories, a separate research unit at Columbia. Another of his instructors, Professor John H. Morecroft, later remembered Armstrong as being intensely focused on the topics that interested him, but somewhat indifferent to the rest of his studies. Armstrong challenged conventional wisdom and was quick to question the opinions of both professors and peers. In one case, he recounted how he tricked a visiting professor from Cornell University that he disliked into receiving a severe electrical shock. He also stressed the practical over the theoretical, stating that progress was more likely the product of experimentation and reasoning than on mathematical calculation and the formulae of "mathematical physics".

Armstrong graduated from Columbia in 1913, earning an electrical engineering degree.

During World War I, Armstrong served in the Signal Corps as a captain and later a major.

Following college graduation, he received a $600 one-year appointment as a laboratory assistant at Columbia, after which he nominally worked as a research assistant, for a salary of $1 a year, under Professor Pupin. Unlike most engineers, Armstrong never became a corporate employee. He set up a self-financed independent research and development laboratory at Columbia, and owned his patents outright.

In 1934, he filled the vacancy left by John H. Morecroft's death, receiving an appointment as a professor of Electrical Engineering at Columbia, a position he held the remainder of his life.

Armstrong began working on his first major invention while still an undergraduate at Columbia. In late 1906, Lee de Forest had invented the three-element (triode) "grid Audion" vacuum-tube. How vacuum tubes worked was not understood at the time. De Forest's initial Audions did not have a high vacuum and developed a blue glow at modest plate voltages; De Forest improved the vacuum for Federal Telegraph. By 1912, vacuum tube operation was understood, and regenerative circuits using high-vacuum tubes were appreciated.

While growing up, Armstrong had experimented with the early temperamental, "gassy" Audions. Spurred by the later discoveries, he developed a keen interest in gaining a detailed scientific understanding of how vacuum tubes worked. In conjunction with Professor Morecroft he used an oscillograph to conduct comprehensive studies. His breakthrough discovery was determining that employing positive feedback (also known as "regeneration") produced amplification hundreds of times greater than previously attained, with the amplified signals now strong enough so that receivers could use loudspeakers instead of headphones. Further investigation revealed that when the feedback was increased beyond a certain level a vacuum-tube would go into oscillation, thus could also be used as a continuous-wave radio transmitter.

Beginning in 1913 Armstrong prepared a series of comprehensive demonstrations and papers that carefully documented his research, and in late 1913 applied for patent protection covering the regenerative circuit. On October 6, 1914, was issued for his discovery. Although Lee de Forest initially discounted Armstrong's findings, beginning in 1915 de Forest filed a series of competing patent applications that largely copied Armstrong's claims, now stating that he had discovered regeneration first, based on a notebook entry made on August 6, 1912, while working for the Federal Telegraph company, prior to the date recognized for Armstrong of January 31, 1913. The result was an interference hearing at the patent office to determine priority. De Forest was not the only other inventor involved – the four competing claimants included Armstrong, de Forest, General Electric's Langmuir, and Alexander Meissner, who was a German national, which led to his application being seized by the Office of Alien Property Custodian during World War I.

Following the end of WWI Armstrong enlisted representation by the law firm of Pennie, Davis, Martin and Edmonds. To finance his legal expenses he began issuing non-transferable licenses for use of the regenerative patents to a select group of small radio equipment firms, and by November 1920, 17 companies had been licensed. These licensees paid 5% royalties on their sales which were restricted to only "amateurs and experimenters". Meanwhile, Armstrong explored his options for selling the commercial rights to his work. Although the obvious candidate was the Radio Corporation of America (RCA), on October 5, 1920, the Westinghouse Electric & Manufacturing Company took out an option for $335,000 for the commercial rights for both the regenerative and superheterodyne patents, with an additional $200,000 to be paid if Armstrong prevailed in the regenerative patent dispute. Westinghouse exercised this option on November 4, 1920.

Legal proceedings related to the regeneration patent became separated into two groups of court cases. An initial court action was triggered in 1919 when Armstrong sued de Forest's company in district court, alleging infringement of patent 1,113,149. This court ruled in Armstrong's favor on May 17, 1921. A second line of court cases, the result of the patent office interference hearing, had a different outcome. The interference board had also sided with Armstrong, but he was unwilling to settle with de Forest for less than what he considered full compensation. Thus pressured, de Forest continued his legal defense, and appealed the interference board decision to the District of Columbia district court. On May 8, 1924, that court ruled that it was de Forest who should be considered regeneration's inventor. Armstrong (along with much of the engineering community) was shocked by these events, and his side appealed this decision. Although the legal proceeding twice went before the US Supreme Court, in 1928 and 1934, he was unsuccessful in overturning the decision.

In response to the second Supreme Court decision upholding de Forest as the inventor of regeneration, Armstrong attempted to return his 1917 IRE Medal of Honor, which had been awarded "in recognition of his work and publications dealing with the action of the oscillating and non-oscillating audion". The organization's board refused to allow him, and issued a statement that it "strongly affirms the original award".

The United States entered WWI in April 1917. Later that year Armstrong was commissioned as a captain in the U.S. Army Signal Corps, and assigned to a laboratory in Paris, France to help develop radio communication for the Allied war effort. He returned to the US in the autumn of 1919, after being promoted to the rank of Major. (During both world wars, Armstrong gave the US military free use of his patents.)

During this period, Armstrong's most significant accomplishment was the development of a "supersonic heterodyne" – soon shortened to "superheterodyne" – radio receiver circuit. This circuit made radio receivers more sensitive and selective and is used extensively today. The key feature of the superheterodyne approach is the mixing of the incoming radio signal with a locally generated, different frequency signal within a radio set. That circuit is called the mixer. The result is a fixed, unchanging intermediate frequency, or I.F. signal which is easily amplified and detected by following circuit stages. In 1919, Armstrong filed an application for a US patent of the superheterodyne circuit which was issued the next year. This patent was subsequently sold to Westinghouse. The patent was challenged, triggering another patent office interference hearing. Armstrong ultimately lost this patent battle; although the outcome was less controversial than that involving the regeneration proceedings.

The challenger was Lucien Lévy of France who had worked developing Allied radio communication during WWI. He had been awarded French patents in 1917 and 1918 that covered some of the same basic ideas used in Armstrong's superheterodyne receiver. AT&T, interested in radio development at this time, primarily for point-to-point extensions of its wired telephone exchanges, purchased the US rights to Lévy's patent and contested Armstrong's grant. The subsequent court reviews continued until 1928, when the District of Columbia Court of Appeals disallowed all nine claims of Armstrong's patent, assigning priority for seven of the claims to Lévy, and one each to Ernst Alexanderson of General Electric and Burton W. Kendall of Bell Laboratories.

Although most early radio receivers used regeneration Armstrong approached RCA's David Sarnoff, whom he had known since giving a demonstration of his regeneration receiver in 1913, about the corporation offering superheterodynes as a superior offering to the general public. (The ongoing patent dispute was not a hindrance, because extensive cross-licensing agreements signed in 1920 and 1921 between RCA, Westinghouse and AT&T meant that Armstrong could freely use the Lévy patent.) Superheterodyne sets were initially thought to be prohibitively complicated and expensive as the initial designs required multiple tuning knobs and used nine vacuum tubes. In conjunction with RCA engineers, Armstrong developed a simpler, less costly design. RCA introduced its superheterodyne Radiola sets in the US market in early 1924, and they were an immediate success, dramatically increasing the corporation's profits. These sets were considered so valuable that RCA would not license the superheterodyne to other US companies until 1930.

The regeneration legal battle had one serendipitous outcome for Armstrong. While he was preparing apparatus to counteract a claim made by a patent attorney, he "accidentally ran into the phenomenon of super-regeneration", where, by rapidly "quenching" the vacuum-tube oscillations, he was able to achieve even greater levels of amplification. A year later, in 1922, Armstrong sold his super-regeneration patent to RCA for $200,000 plus 60,000 shares of corporation stock, which was later increased to 80,000 shares in payment for consulting services. This made Armstrong RCA's largest shareholder, and he noted that "The sale of that invention was to net me more than the sale of the regenerative circuit and the superheterodyne combined". RCA envisioned selling a line of super-regenerative receivers until superheterodyne sets could be perfected for general sales, but it turned out the circuit was not selective enough to make it practical for broadcast receivers.

"Static" interference – extraneous noises caused by sources such as thunderstorms and electrical equipment – bedeviled early radio communication using amplitude modulation and perplexed numerous inventors attempting to eliminate it. Many ideas for static elimination were investigated, with little success. In the mid-1920s, Armstrong began researching a solution. He initially, and unsuccessfully, attempted to resolve the problem by modifying the characteristics of AM transmissions.

One approach used frequency modulation (FM) transmissions. Instead of varying the strength of the carrier wave as with AM, the frequency of the carrier was changed to represent the audio signal. In 1922 John Renshaw Carson of AT&T, inventor of Single-sideband modulation (SSB), had published a detailed mathematical analysis which showed that FM transmissions did not provide any improvement over AM. Although the Carson bandwidth rule for FM is important today, Carson's review turned out to be incomplete, as it analyzed only (what is now known as) "narrow-band" FM.

In early 1928 Armstrong began researching the capabilities of FM. Although there were others involved in FM research at this time, he knew of an RCA project to see if FM shortwave transmissions were less susceptible to fading than AM. In 1931 the RCA engineers constructed a successful FM shortwave link transmitting the Schmeling–Stribling fight broadcast from California to Hawaii, and noted at the time that the signals seemed to be less affected by static. The project made little further progress.

Working in secret in the basement laboratory of Columbia's Philosophy Hall, Armstrong developed "wide-band" FM, in the process discovering significant advantages over the earlier "narrow-band" FM transmissions. In a "wide-band" FM system, the deviations of the carrier frequency are made to be much larger than the frequency of the audio signal which can be shown to provide better noise rejection. He was granted five US patents covering the basic features of the new system on December 26, 1933. Initially, the primary claim was that his FM system was effective at filtering out the noise produced in receivers, by vacuum tubes.

Armstrong had a standing agreement to give RCA the right of first refusal to his patents. In 1934 he presented his new system to RCA president Sarnoff. Sarnoff was somewhat taken aback by its complexity, as he had hoped it would be possible to eliminate static merely by adding a simple device to existing receivers. From May 1934 until October 1935 Armstrong conducted field tests of his FM technology from an RCA laboratory located on the 85th floor of the Empire State Building in New York City. An antenna attached to the building's spire transmitted signals for distances up to . These tests helped demonstrate FM's static-reduction and high-fidelity capabilities. RCA, which was heavily invested in perfecting TV broadcasting, chose not to invest in FM, and instructed Armstrong to remove his equipment.

Denied the marketing and financial clout of RCA, Armstrong decided to finance his own development and form ties with smaller members of the radio industry, including Zenith and General Electric, to promote his invention. Armstrong thought that FM had the potential to replace AM stations within 5 years, which he promoted as a boost for the radio manufacturing industry, then suffering from the effects of the Great Depression. Making existing AM radio transmitters and receivers obsolete would necessitate that stations buy replacement transmitters and listeners purchase FM-capable receivers. In 1936 he published a landmark paper in the "Proceedings of the IRE" that documented the superior capabilities of using wide-band FM. (This paper would be reprinted in the August 1984 issue of "Proceedings of the IEEE".) A year later, a paper by Murray G. Crosby (inventor of Crosby system for FM Stereo) in the same journal provided further analysis of the wide-band FM characteristics, and introduced the concept of "threshold", demonstrating that there is a superior signal-to-noise ratio when the signal is stronger than a certain level.

In June 1936, Armstrong gave a formal presentation of his new system at the US Federal Communications Commission (FCC) headquarters. For comparison, he played a jazz record using a conventional AM radio, then switched to an FM transmission. A United Press correspondent was present, and recounted in a wire service report that: "if the audience of 500 engineers had shut their eyes they would have believed the jazz band was in the same room. There were no extraneous sounds." Moreover, "Several engineers said after the demonstration that they consider Dr. Armstrong's invention one of the most important radio developments since the first earphone crystal sets were introduced." Armstrong was quoted as saying he could "visualize a time not far distant when the use of ultra-high frequency wave bands will play the leading role in all broadcasting", although the article noted that "A switchover to the ultra-high frequency system would mean the junking of present broadcasting equipment and present receivers in homes, eventually causing the expenditure of billions of dollars."

FCC studies comparing the Apex station transmissions with Armstrong's FM system concluded that his approach was superior. In early 1940, the FCC held hearings on whether to establish a commercial FM service. Following this review, the FCC announced the establishment of an FM band effective January 1, 1941, consisting of forty 200 kHz-wide channels on a band from 42 to 50 MHz, with the first five channels reserved for educational stations. Existing Apex stations were notified that they would not be allowed to operate after January 1, 1941, unless they converted to FM.

Although there was interest in the new FM band by station owners, construction restrictions that went into place during WWII limited the growth of the new service. Following the end of WWII, the FCC moved to standardize its frequency allocations. One area of concern was the effects of tropospheric and Sporadic E propagation, which at times reflected station signals over great distances, causing mutual interference. A particularly controversial proposal, spearheaded by RCA, was that the FM band needed to be shifted to higher frequencies to avoid this problem. This reassignment was fiercely opposed as unneeded by Armstrong, but he lost. The FCC made its decision final on June 27, 1945. It allocated 100 FM channels from 88 to 108 MHz, and assigned the former FM band to 'non government fixed and mobile' (42–44 MHz), and television channel 1 (44–50 MHz), now sidestepping the interference concerns. A period of allowing existing FM stations to broadcast on both low and high bands ended at midnight on January 8, 1949, at which time any low band transmitters were shut down, making obsolete 395,000 receivers that had already been purchased by the public for the original band. Although converters allowing low band FM sets to receive high band were manufactured, they ultimately proved to be complicated to install, and often as (or more) expensive than buying a new high band set outright.

Armstrong felt the FM band reassignment had been inspired primarily by a desire to cause a disruption that would limit FM's ability to challenge the existing radio industry, including RCA's AM radio properties that included the NBC radio network, plus the other major networks including CBS, ABC and Mutual. The change was thought to have been favored by AT&T, as the elimination of FM relaying stations would require radio stations to lease wired links from that company. Particularly galling was the FCC assignment of TV channel 1 to the 44–50 MHz segment of the old FM band. Channel 1 was later deleted, since periodic radio propagation would make local TV signals unviewable.

Although the FM band shift was an economic setback, there was reason for optimism. A book published in 1946 by Charles A. Siepmann heralded FM stations as "Radio's Second Chance". In late 1945, Armstrong contracted with John Orr Young, founding member of the public relations firm Young & Rubicam, to conduct a national campaign promoting FM broadcasting, especially by educational institutions. Article placements promoting both Armstrong personally and FM were made with general circulation publications including "The Nation", "Fortune", "The New York Times", "Atlantic Monthly", and "The Saturday Evening Post".

In 1940, RCA offered Armstrong $1,000,000 for a non-exclusive, royalty-free license to use his FM patents. He refused this offer, because he felt this would be unfair to the other licensed companies, which had to pay 2% royalties on their sales. Over time this impasse with RCA dominated Armstrong's life. RCA countered by conducting its own FM research, eventually developing what it claimed was a non-infringing FM system. The corporation encouraged other companies to stop paying royalties to Armstrong. Outraged by this, in 1948 Armstrong filed suit against RCA and the National Broadcasting Company, accusing them of patent infringement and that they had "deliberately set out to oppose and impair the value" of his invention, for which he requested treble damages. Although he was confident that this suit would be successful and result in a major monetary award, the protracted legal maneuvering that followed eventually began to impair his finances, especially after his primary patents expired in late 1950.

During World War II, Armstrong turned his attention to investigations of continuous-wave FM radar funded by government contracts. Armstrong hoped that the interference fighting characteristic of wide-band FM and a narrow receiver bandwidth to reduce noise would increase range. Primary development took place at Armstrong's Alpine, NJ laboratory. A duplicate set of equipment was sent to the U.S. Army's Evans Signal Laboratory. The results of his investigations were inconclusive, the war ended, and the project was dropped by the Army.

Under the name Project Diana, the Evans staff took up the possibility of bouncing radar signals off the moon. Calculations showed that standard pulsed radar like the stock SCR-271 would not do the job; higher average power, much wider transmitter pulses, and very narrow receiver bandwidth would be required. They realized that the Armstrong equipment could be modified to accomplish the task. The FM modulator of the transmitter was disabled and the transmitter keyed to produce quarter-second CW pulses. The narrow-band (57 Hz) receiver, which tracked the transmitter frequency, got an incremental tuning control to compensate for the possible 300 Hz Doppler shift on the lunar echoes. They achieved success on 10 January 1946.

Bitter and overtaxed by years of litigation and mounting financial problems, Armstrong lashed out at his wife one day with a fireplace poker, striking her on the arm. She left their apartment to stay with her sister.

Sometime during the night of January 31February 1, 1954, Armstrong jumped to his death from a window in his 12-room apartment on the 13th floor of River House in Manhattan, New York City. The "New York Times" described the contents of his two-page suicide note to his wife: "he was heartbroken at being unable to see her once again, and expressing deep regret at having hurt her, the dearest thing in his life." The note concluded, "God keep you and Lord have mercy on my Soul." David Sarnoff disclaimed any responsibility, telling Carl Dreher directly that "I did not kill Armstrong." After his death, a friend of Armstrong estimated that 90 percent of his time was spent on litigation against RCA. U.S. Senator Joseph McCarthy (R-Wisconsin) reported that Armstrong had recently met with one of his investigators, and had been "mortally afraid" that secret radar discoveries by him and other scientists "were being fed to the Communists as fast as they could be developed".

Following her husband's death, Marion Armstrong took charge of pursuing his estate's legal cases. In late December 1954, it was announced that through arbitration a settlement of "approximately $1,000,000" had been made with RCA. Dana Raymond of Cravath, Swaine & Moore in New York served as counsel in that litigation. Marion Armstrong was able to formally establish Armstrong as the inventor of FM following protracted court proceedings over five of his basic FM patents, with a series of successful suits, which lasted until 1967, against other companies that were found guilty of infringement.

It was not until the 1960s that FM stations in the United States started to challenge the popularity of the AM band, helped by the development of FM stereo by General Electric, followed by the FCC's FM Non-Duplication Rule, which limited large-city broadcasters with AM and FM licenses to simulcasting on those two frequencies for only half of their broadcast hours. Armstrong's FM system was also used for communications between NASA and the Apollo program astronauts.

A US Postage Stamp was released in his honor in 1983 in a series commemorating American Inventors.

Armstrong has been called "the most prolific and influential inventor in radio history". The superheterodyne process is still extensively used by radio equipment. Eighty years after its invention, FM technology has started to be supplemented, and in some cases replaced, by more efficient digital technologies. The introduction of digital television eliminated the FM audio channel that had been used by analog television, HD Radio has added digital sub-channels to FM band stations, and, in Europe and Pacific Asia, Digital Audio Broadcasting bands have been created that will, in some cases, eliminate existing FM stations altogether. However, FM broadcasting is still used internationally, and remains the dominant system employed for audio broadcasting services.

In 1923, combining his love for high places with courtship rituals, Armstrong climbed the WJZ (now WABC) antenna located atop a 20-story building in New York City, where he reportedly did a handstand, and when a witness asked him what motivated him to "do these damnfool things", Armstrong replied "I do it because the spirit moves me." Armstrong had arranged to have photographs taken, which he had delivered to David Sarnoff's secretary, Marion MacInnis. Armstrong and MacInnis married later that year. Armstrong bought a Hispano-Suiza motor car before the wedding, which he kept until his death, and which he drove to Palm Beach, Florida for their honeymoon. A publicity photograph was made of him presenting Marion with the world's first portable superheterodyne radio as a wedding gift.

He was an avid tennis player until an injury in 1940, and drank an Old Fashioned with dinner. Politically, he was described by one of his associates as "a revolutionist only in technology – in politics he was one of the most conservative of men."

In 1955, Marion Armstrong founded the Armstrong Memorial Research Foundation, and participated in its work until her death in 1979 at the age of 81. She was survived by two nephews and a niece.

Among Armstrong's living relatives are Steven McGrath, of Cape Elizabeth, Maine, formerly energy advisor to Maine's Governor, and Adam Brecht, a media executive in New York City, whose paternal great-grandfather, John Frank MacInnis, was the brother of Marion Armstrong. Edwin Howard Armstrong's niece, Jeanne Hammond, who represented the family in the Ken Burns documentary , died on May 1, 2019, in Scarborough, Maine. Ms. Hammond worked in her uncle's radio laboratory at Columbia University for several years following her graduation from Wellesley College in 1943.

In 1917, Armstrong was the first recipient of the IRE's (now IEEE) Medal of Honor.

For his wartime work on radio, the French government gave him the Legion of Honor in 1919. He was awarded the 1941 Franklin Medal, and in 1942 received the AIEEs Edison Medal "for distinguished contributions to the art of electric communication, notably the regenerative circuit, the superheterodyne, and frequency modulation." The ITU added him to its roster of great inventors of electricity in 1955.

He later received two honorary doctorates, from Columbia in 1929, and Muhlenberg College in 1941.

In 1980, he was inducted into the National Inventors Hall of Fame, and appeared on a U.S. postage stamp in 1983. The Consumer Electronics Hall of Fame inducted him in 2000, "in recognition of his contributions and pioneering spirit that have laid the foundation for consumer electronics." Columbia University established the Edwin Howard Armstrong Professorship in the School of Engineering and Applied Science in his memory.

Philosophy Hall, the Columbia building where Armstrong developed FM, was declared a National Historic Landmark. Armstrong's boyhood home in Yonkers, New York was recognized by the National Historic Landmark program and the National Register of Historic Places, although this was withdrawn when the house was demolished.

Armstrong Hall at Columbia was named in his honor. The hall, located at the northeast corner of Broadway and 112th Street, was originally an apartment house but was converted to research space after being purchased by the university. It is currently home to the Goddard Institute for Space Studies, a research institute dedicated to atmospheric and climate science that is jointly operated by Columbia and the National Aeronautics and Space Administration. A storefront in a corner of the building houses Tom's Restaurant, a longtime neighborhood fixture that inspired Susanne Vega's song "Tom's Diner" and was used for establishing shots for the fictional "Monk's diner" in the "Seinfeld" television series.

A second Armstrong Hall, also named for the inventor, is located at the United States Army Communications and Electronics Life Cycle Management Command (CECOM-LCMC) Headquarters at Aberdeen Proving Ground, Maryland.

E. H. Armstrong patents:

 U.S. Patent and Trademark Office Database Search

The following patents were issued to Armstrong's estate after his death:





EverQuest

EverQuest is a 3D fantasy-themed massively multiplayer online role-playing game (MMORPG) originally developed by Verant Interactive and 989 Studios for Windows PCs. It was released by Sony Online Entertainment in March 1999 in North America, and by Ubisoft in Europe in April 2000. A dedicated version for Mac OS X was released in June 2003, which operated for ten years before being shut down in November 2013. In June 2000, Verant Interactive was absorbed into Sony Online Entertainment, who took over full development and publishing duties of the title. Later, in February 2015, SOE's parent corporation, Sony Computer Entertainment, sold the studio to investment company Columbus Nova and it was rebranded as Daybreak Game Company, which continues to develop and publish "EverQuest".

It was the first commercially successful MMORPG to employ a 3D game engine, and its success was on an unprecedented scale. "EverQuest" has had a wide influence on subsequent releases within the market, and holds an important position in the history of massively multiplayer online games.

The game surpassed early subscription expectations and increased in popularity for many years after its release. It is now considered one of the greatest video games ever made. It has received numerous awards, including the 1999 GameSpot Game of the Year and a 2007 Technology & Engineering Emmy Award. While dozens of similar games have come and gone over the years, "EverQuest" still endures as a viable commercial enterprise with new expansions still being released on a regular basis, over twenty years after its initial launch. It has spawned a number of spin-off media, including books and video games, as well as a sequel, "EverQuest II", which launched in 2004.

Many of the elements in "EverQuest" have been drawn from text-based MUD (Multi-User Dungeon) games, particularly DikuMUDs, which in turn were inspired by traditional role-playing games such as "Dungeons & Dragons". In "EverQuest", players create a character (also known as an avatar, or colloquially as a "char" or "toon") by selecting one of twelve races in the game, which were humans, high-elves, wood-elves, half-elves, dark-elves, erudites, barbarians, dwarves, halflings, gnomes, ogres, and trolls. In the first expansion, lizard-people (Iksar) were introduced. Cat-people (Vah Shir), frog-people (Froglok), and dragon-people (Drakkin) were all introduced in later expansions. At creation, players select each character's adventuring occupation (such as a wizard, ranger, or cleric — called a "class" — see below for particulars), a patron deity, and starting city. Customization of the character facial appearance is available at creation (hair, hair color, face style, facial hair, facial hair color, eye color, etc.).

Players move their character throughout the medieval fantasy world of Norrath, often fighting monsters and enemies for treasure and experience points, and optionally mastering trade skills. As they progress, players advance in level, gaining power, prestige, spells, and abilities through valorous deeds such as entering overrun castles and keeps, defeating worthy opponents found within, and looting their remains. Experience and prestigious equipment can also be obtained by completing quests given out by non-player characters found throughout the land.

"EverQuest" allows players to interact with other people through role-play, joining player guilds, and dueling other players (in restricted situations – "EverQuest" only allows player versus player (PVP) combat on the PvP-specific server, specified arena zones and through agreed upon dueling).

The game-world of "EverQuest" consists of over five hundred zones.

Multiple instances of the world exist on various servers. In the past, game server populations were visible during log-in, and showed peaks of more than 3000 players per server. The design of "EverQuest", like other massively multiplayer online role-playing games, makes it highly amenable to cooperative play, with each player having a specific role within a given group.

"EverQuest" featured fourteen playable character classes upon release in 1999, with two others - Beastlord and Berzerker - added in the "Shadows of Luclin" (2001) and "Gates of Discord" (2004) expansions, respectively. Each class falls within one of four general categories based on playstyle and the type of abilities they use, with certain classes being restricted to particular races.

Melee classes are those which fight at close quarters and often use direct physical attacks as opposed to magic. These include the Warrior, a tank-based character which wears heavy armor and is designed to take damage for its group using a taunt ability; the Monk, a character which uses a combination of martial arts and barehanded fighting techniques; the Rogue, a combination of thief and assassin classes which can sneak and hide in the shadows as well as steal from enemies; and the Berserker, a strong fighter who specialize in two-handed weapons such as axes and are able to enter a state of increased fury and power.

Priest classes are primarily healers who learn magic that can heal their allies or themselves. The Priest classes are made up of the Cleric, a heavily specialized support class that wears heavy armor and is adept at healing and strengthening their allies; the Druid, a magic-user who draws power from nature which can restore the vitality and magic power of their teammates; and the Shaman, tribal warriors who draw upon the spirit realm to heal, empower those around them, and weaken their enemies.

Casters are magic-users and sorcerers which wear light armor but command powerful spells. Those among them include the Wizard, a specialized damage-dealing class which uses the power of fire, ice, and pure magic energy for devastating effect as well as teleportation abilities; the Magician, a summoner who is able to call upon elemental servants which aid them in dealing damage; the Necromancer, a dark caster who uses the power of disease and poison to wither away their opponents while commanding undead allies to aid them; and the Enchanter, an illusionist who can take on many forms, support allies with strengthening spells, and pacify enemies with mesmerizing abilities.

Hybrid classes are those which can perform multiple roles or have abilities of various types. These include Paladins, knights who possess the ability to take damage or heal with magic or laying on of hands; Shadowknights, dark warriors who use a combination of melee attacks and disease/poison abilities to damage foes as well as take damage for the party; the Bard, a minstrel who is able to use magical songs for a number of effects - including damaging enemies, strengthening allies, and improving the movement speed of themselves and others; Rangers, protectors of nature who learn healing and support magic in addition to being able to damage enemies in close combat or at a distance with bows and arrows; and Beastlords, primal fighters who are constantly joined by their animal wards which help them deal damage, and can assist their teammates with healing and support skills.

There are several deities in "EverQuest" who each have a certain area of responsibility and play a role in the backstory of the game setting. A wide array of armor and weapons are tied to certain deities, making it only possible for those who worship that specific deity to wear/equip them. Additionally, deities determine, to some extent, where characters may and may not go without being attacked on sight by the deity's minions and devoted followers.

The "EverQuest" universe is divided into more than five hundred zones. These zones represent a wide variety of geographical features, including plains, oceans, cities, deserts, and other planes of existence. One of the most popular zones in the game is the Plane of Knowledge, one of the few zones in which all races and classes can coexist harmoniously without interference. The Plane of Knowledge is also home to portals to many other zones, including portals to other planes and to the outskirts of nearly every starting city.

"EverQuest" began as a concept by John Smedley in 1996. The original design is credited to Brad McQuaid, Steve Clover, and Bill Trost. It was developed by Sony's 989 Studios and its early-1999 spin-off Verant Interactive, and published by Sony Online Entertainment (SOE). Since its acquisition of Verant in late 1999, EverQuest was developed by Sony Online Entertainment.<ref name="Sony/Verant"></ref>

The design and concept of "EverQuest" is heavily indebted to text-based MUDs, in particular DikuMUD, and as such "EverQuest" is considered a 3D evolution of the text MUD genre like some of the MMOs that preceded it, such as "Meridian 59" and "The Realm Online". John Smedley, Brad McQuaid, Steve Clover and Bill Trost, who jointly are credited with creating the world of "EverQuest", have repeatedly pointed to their shared experiences playing MUDs such as "Sojourn" and "TorilMUD" as the inspiration for the game. Famed book cover illustrator Keith Parkinson created the box covers for earlier installments of "EverQuest".

Development of "EverQuest" began in 1996 when Sony Interactive Studios America (SISA) executive John Smedley secured funding for a 3D game like text-based MUDs following the successful launch of "Meridian 59" the previous year. To implement the design, Smedley hired programmers Brad McQuaid and Steve Clover, who had come to Smedley's attention through their work on the single player RPG "Warwizard". McQuaid soon rose through the ranks to become executive producer for the "EverQuest" franchise and emerged during development of "EverQuest" as a popular figure among the fan community through his in-game avatar, Aradune. Other key members of the development team included Bill Trost, who created the history, lore and major characters of Norrath (including "EverQuest" protagonist Firiona Vie), Geoffrey "GZ" Zatkin, who implemented the spell system, and artist Milo D. Cooper, who did the original character modeling in the game.

The start of beta testing was announced by Brad McQuaid in November 1997.

"EverQuest" launched with modest expectations from Sony on 16 March 1999 under its Verant Interactive brand and quickly became successful. By the end of the year, it had surpassed competitor "Ultima Online" in number of subscriptions. Numbers continued rising rapidly until mid-2001 when growth slowed. The game initially launched with volunteer "Guides" who would act as basic customer service/support via 'petitions'. Issues could be forwarded to the Game Master assigned to the server or resolved by the volunteer. Other guides would serve in administrative functions within the program or assisting the Quest Troupe with dynamic and persistent live events throughout the individual servers. Volunteers were compensated with free subscription and expansions to the game. In 2003 the program changed for the volunteer guides taking them away from the customer service focus and placing them into their current roles as roving 'persistent characters' role-playing with the players.

In anticipation of PlayStation's launch, Sony Interactive Studios America made the decision to focus primarily on console titles under the banner 989 Studios, while spinning off its sole computer title, "EverQuest", which was ready to launch, to a new computer game division named Redeye (renamed Verant Interactive). Executives initially had very low expectations for "EverQuest", but in 2000, following the surprising continued success and unparalleled profits of "EverQuest", Sony reorganized Verant Interactive into Sony Online Entertainment (SOE) with Smedley retaining control of the company. Many of the original "EverQuest" team, including Brad McQuaid and Steve Clover left SOE by 2002.

The first four expansions were released in traditional physical boxes at roughly one-year intervals. These were highly ambitious and offered huge new landmasses, new playable races and new classes. The expansion "" (2001) gave a significant facelift to player character models, bringing the dated 1999 graphics up to modern standards. However, non-player characters which do not correspond to any playable race-gender-class combination (such as vendors) were not updated, leading to the coexistence of 1999-era and 2001-era graphics in many locations. The expansion "" (2002) introduced The Plane of Knowledge, a hub zone from which players could quickly teleport to many other destinations. This made the pre-existing roads and ships largely redundant, and long-distance overland travel is now virtually unheard of.

"EverQuest" made a push to enter the European market in 2002 with the "New Dawn" promotional campaign, which not only established local servers in Germany, France and Great Britain but also offered localized versions of the game in German and French to accommodate players who prefer those languages to English. In the following year the game also moved beyond the PC market with a Mac OS X version.

In 2003 experiments began with digital distribution of expansions, starting with the "Legacy of Ykesha". From this point on expansions would be less ambitious in scope than the original four, but the production rate increased to two expansions a year instead of one.

In the same year the franchise also ventured into the console market with "EverQuest Online Adventures", released for Sony's internet-capable PlayStation 2. It was the second MMORPG for this console, after "Final Fantasy XI". Story-wise it was a prequel, with the events taking place 500 years before the original "EverQuest". Other spin-off projects were the PC strategy game "Lords of EverQuest" (2003) and the co-op "Champions of Norrath" (2004) for the PlayStation 2.

After these side projects, the first proper sequel was released in late 2004, titled simply "EverQuest II". The game is set 500 years after the original. "EverQuest II" faced severe competition from Blizzard's "World of Warcraft", which was released at virtually the same time and quickly grew to dominate the MMORPG genre.

Since the release of "World of Warcraft" and other modern MMORPGs, there have been a number of signs that the "EverQuest" population is shrinking. The national "New Dawn" servers were discontinued in 2005 and merged into a general (English-language) European server.

The 2006 expansion "The Serpent's Spine" introduced the "adventure-friendly" city of Crescent Reach in which all races and classes are able (and encouraged) to start. Crescent Reach is supposed to provide a more pedagogic starting environment than the original 1999 cities, where players were given almost no guidance on what to do. The common starting city also concentrates the dwindling number of new players in a single location, making grouping easier. 2008's "" expansion introduced computer controlled companions called "mercenaries" that can join groups in place of human players, a response to the increasing difficulty of finding other players of appropriate level for group activities. As of "Seeds" the production rate also returned to one expansion a year instead of two.

In March 2012 "EverQuest" departed from the traditional monthly subscription business model by introducing three tiers of commitment: a completely free-to-play Bronze Level, a one-time fee Silver Level, and a subscription Gold Level. The same month saw the closure of "EverQuest Online Adventures". Just a few months earlier "EverQuest II" had gone free-to-play and SOE flagship "Star Wars Galaxies" was also closed.

In June of the same year SOE removed the ability to buy game subscription time with Station Cash without any warning to players. SOE apologized for this abrupt change in policy and reinstated the option for an additional week, after which it was removed permanently.

The sole Mac OS server Al'Kabor was closed on November 18, 2013.

In February 2015 Sony sold its online entertainment division to private equity group Columbus Nova, with Sony Online Entertainment subsequently renamed Daybreak Game Company (DBG). An initial period of uncertainty followed, with all projects such as expansions and sequels put on hold and staff laid off. The situation stabilized around the game's 16th anniversary celebrations, and a new expansion was released nine months later.

There have been thirty expansions to the original game since release. Expansions are purchased separately and provide additional content to the game (for example: raising the maximum character level; adding new races, classes, zones, continents, quests, equipment, game features). When the players purchase the latest expansion they receive all previous expansions they may not have previously purchased. Additionally, the game is updated through downloaded patches. The "EverQuest" expansions are as follows:

The game runs on multiple game servers, each with a unique name for identification. These names were originally the deities of the world of Norrath. In technical terms, each game server is actually a cluster of server machines. Once a character is created, it can be played only on that server unless the character is transferred to a new server by the customer service staff, generally for a fee. Each server often has a unique community and people often include the server name when identifying their character outside of the game.

There is an official "EverQuest" server list, as well as unofficial 3rd-party servers. For example, the Project 1999 EverQuest servers are intended to recreate "EverQuest" in the state it existed in the year it launched and the two subsequent expansions, referred to as the "Classic Trilogy".

SOE devoted one server (Al'Kabor) to an OS X version of the game, which opened for beta testing in early 2003, and officially released on June 24 of the same year. The game was never developed beyond the "Planes of Power" expansion, and contained multiple features and bugs not seen on PC servers, as a side-effect of the codebase having been split from an early "Planes of Power" date but not updated with the PC codebase. In January 2012, SOE announced plans to shut down the server, but based on the passionate response of the player base, rescinded the decision and changed Al'Kabor to a free-to-play subscription model. At about the same time, SOE revised the Macintosh client software to run natively on Intel processors. Players running on older, PowerPC-based systems lost access to the game at that point. SOE closed Al'Kabor server in November 2013.

Two SOE servers were set up to better support players in and around Europe: Antonius Bayle and Kane Bayle. Kane Bayle was merged into Antonius Bayle.

With the advent of the "New Dawn" promotion, three additional servers were set up and maintained by Ubisoft: Venril Sathir (British), Sebilis (French) and Kael Drakkal (German). The downside of the servers was that while it was possible to transfer to them, it was impossible to transfer off.

The servers were subsequently acquired by SOE and all three were merged into Antonius Bayle server.

Reviews of "Everquest" were mostly positive upon release in 1999, earning an 85 out of 100 score from aggregate review website Metacritic. Comparing it to other online role-playing titles at the time, critics called it "the best game in its class", and the "most immersive and most addictive online RPG to date". Dan Amrich of "GamePro" magazine declared that "the bar for online gaming has not so much been raised as obliterated" and that the game's developers had "created the first true online killer app". The reviewer would find fault with its repetitive gameplay in the early levels and lack of sufficient documentation to help new players, urging them to turn to fansites for help instead. Greg Kasavin of GameSpot similarly felt that the game's combat was "uninteresting" but did note that, unlike earlier games in the genre, "EverQuest" offered the opportunity to play on servers that wouldn't allow players to fight each other unless they chose to, and that it heavily promoted cooperation. Despite saying that the combat was little "boring", that the manual was "horrible", that the quest system is "half-baked", and the game having small share of miscellaneous bugs, he ultimately called "EverQuest" as one of the most memorable gaming experiences he had. Baldric of Game Revolution likewise stated that the game was more co-operative than "Ultima Online", but that there was less interaction with the environment, calling it more "player oriented" instead of "'world' oriented".

Despite server issues during the initial launch, reviewers felt that the game played well even on lower-end network cards, with Tal Blevins of IGN remarking that it rarely suffered from major lag issues. The reviewer did feel that the title suffered from a lack of player customization aside from different face types, meaning all characters of the same race looked mostly the same, but its visual quality on the whole was "excellent" with "particularly impressive" spell, lighting, and particle effects. "Next Generation" said that "EverQuest" set a high standards for its genre. "Computer Games Magazine" commended the game's three-dimensional graphics, first-person perspective, environments, and simple combat system, remarking that "EverQuest" gave the players the first step towards to the true virtual world.

"Everquest" was named GameSpot's 1999 Game of the Year in its Best & Worst of 1999 awards, remarking that after the game's release in March, the whole gaming industry was grounded to a halt, that a least one prominent game developer blamed "EverQuest" for product delays, and that for several weeks GameSpot's editors were spending more time exploring Norrath than they were doing their jobs. The website would also include the game in their list of the Greatest Games of All Time in 2004. GameSpot UK would also rank the title 14th on its list of the 100 Best Computer Games of the Millennium in 2000, calling it "a technological tour de force" and the first online RPG to bring the production values of single-player games to the online masses. The Academy of Interactive Arts and Sciences named "EverQuest" their "Online Game of the Year" during the 3rd Annual Interactive Achievement Awards, while Game Revolution named it the Best PC RPG of 1999. It was included in "Time" magazine's Best of 1999 in the "Tech" category, and "Entertainment Weekly" would include the game in their Top Ten Hall of Fame Video Games of the '90s. In 2007, Sony Online Entertainment received a Technology & Engineering Emmy Award for "EverQuest" under the category of "Development of Massively Multiplayer Online Graphical Role Playing Games". During the 2nd annual Game Developers Choice Online Awards in 2011, "EverQuest" received a Hall of Fame award for its long-term advancement of online gaming, such as being the first MMORPG to feature a guild system and raiding.

Editors of "Computer Gaming World" and GameSpot each nominated "EverQuest" for their 1999 "Role-Playing Game of the Year" awards, both of which ultimately went to "". CNET Gamecenter likewise nominated it in this category, but gave the award to "Asheron's Call". GameSpot would also nominate the title for Best Multiplayer Game of 1999, but would give the award to "Quake III Arena". In 2012, 1UP.com ranked "EverQuest" 57th on its list of the Top 100 Essential Games. Game Informer placed the game 33rd on their top 100 video games of all time in 2009.

"EverQuest" was the most pre-ordered PC title on EBGames.com prior to its release in March 1999. The game had 10,000 active subscribers 24 hours after launch, making it the high-selling online role-playing game up until that point. It achieved 60,000 subscribers by April 1999. Six months later, around 225,000 copies of the game had been sold in total, with 150,000 active subscribers. By early 2000, the game's domestic sales alone reached 231,093 copies, which drew revenues of $10.6 million. NPD Techworld, a firm that tracked sales the United States, reported 559,948 units sold of "EverQuest" by December 2002. Subscription numbers would rise to over 500,000 active accounts four years after release in 2003. By the end of 2004 the title's lifetime sales exceeded 3 million copies worldwide and reached an active subscriber peak of 550,000. As of September 2020, "EverQuest" had 66,000 subscribers and 82,000 monthly active players.

The sale of in-game objects for real currency is a controversial and lucrative industry with topics concerning issues practices of hacking/stealing accounts for profit. Critics often cite how it affects the virtual economy inside the game. In 2001, the sales of in-game items for real life currency was banned on eBay.

A practice in the real-world trade economy is of companies creating characters, powerleveling them to make them powerful, and then reselling the characters for large sums of money or in-game items of other games.

Sony discourages the payment of real-world money for online goods, except on certain "Station Exchange" servers in "EverQuest II", launched in July 2005. The program facilitates buying in-game items for real money from fellow players for a nominal fee. At this point this system only applies to select "EverQuest II" servers; none of the pre-"Station Exchange" "EverQuest II" or "EverQuest" servers are affected.

In 2012, Sony added an in-game item called a "Krono", which adds 30 days of game membership throughout "EverQuest" and "EverQuest II". The item can be initially bought starting at US$17.99. Up to 25 "Kronos" can be bought for US$424.99. Krono can be resold via player trading, which has allowed Krono to be frequently used in the real-world trade economy due to its inherent value.

In October 2000, Verant banned a player by the name of Mystere, allegedly for creating controversial fan fiction, causing outrage among some "EverQuest" players and sparking a debate about players' rights and the line between roleplaying and intellectual property infringement. The case was used by several academics in discussing such rights in the digital age.

Some argue the game has addictive qualities. Some players jokingly refer to it as "EverCrack" (a comparison to crack cocaine). There was one well-publicized suicide of an "EverQuest" user named Shawn Woolley, that inspired his mother, Liz, to found Online Gamers Anonymous. In November 2001, Shawn Woolley committed suicide. Although he had been diagnosed with depression and schizoid personality disorder, Shawn's mother said the suicide was due to a rejection or betrayal in the game from a character Shawn called "iluvyou".

Massively multiplayer online role-playing games (MMORPGs) are described by some players as "chat rooms with a graphical interface". The sociological aspects of "EverQuest" (and other MMORPGs) are explored in a series of online studies on a site known as "the HUB". The studies make use of data gathered from player surveys and discuss topics like virtual relationships, player personalities, gender issues, and more.

In May 2004, Woody Hearn of GU Comics called for all "EverQuest" gamers to boycott the "Omens of War" expansion in an effort to force SOE to address existing issues with the game rather than release another "quick-fire" expansion. The call to boycott was rescinded after SOE held a summit to address player concerns, improve (internal and external) communication, and correct specific issues within the game.

On 17 January 2008, the Judge of the 17th Federal Court of Minas Gerais State forbade the sales of the game in that Brazilian territory. The reason was that "the game leads the players to a loss of moral virtue and takes them into 'heavy' psychological conflicts because of the game quests".

Since "EverQuest"s release, Sony Online Entertainment has added several "EverQuest"-related games. These include:

A line of novels have been published in the world of "EverQuest", including:



Human evolution

Human evolution is the evolutionary process within the history of primates that led to the emergence of "Homo sapiens" as a distinct species of the hominid family that includes all the great apes. This process involved the gradual development of traits such as human bipedalism, dexterity, and complex language, as well as interbreeding with other hominins (a tribe of the African hominid subfamily), indicating that human evolution was not linear but weblike. The study of the origins of humans, also called anthropogeny, anthropogenesis, or anthropogony, involves several scientific disciplines, including physical and evolutionary anthropology, paleontology, and genetics.

Primates diverged from other mammals about (mya), in the Late Cretaceous period, with their earliest fossils appearing over 55 mya, during the Paleocene. Primates produced successive clades leading to the ape superfamily, which gave rise to the hominid and the gibbon families; these diverged some 15–20 mya. African and Asian hominids (including orangutans) diverged about 14 mya. Hominins (including the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8–9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the "Pan" genus (containing chimpanzees and bonobos) 4–7 mya. The "Homo" genus is evidenced by the appearance of "H. habilis" over 2 mya, while anatomically modern humans emerged in Africa approximately 300,000 years ago.

The evolutionary history of primates can be traced back 65 million years. One of the oldest known primate-like mammal species, the "Plesiadapis", came from North America; another, "Archicebus", came from China. Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.

David R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to "Dryopithecus", migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.

The earliest known catarrhine is "Kamoyapithecus" from the uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to "Aegyptopithecus", "Propliopithecus", and "Parapithecus" from the Faiyum, at around 35 mya. In 2010, "Saadanius" was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 mya, helping to fill an 11-million-year gap in the fossil record.

In the Early Miocene, about 22 million years ago, the many kinds of arboreally-adapted (tree-dwelling) primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to "Victoriapithecus", the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are "Proconsul", "Rangwapithecus", "Dendropithecus", "Limnopithecus", "Nacholapithecus", "Equatorius", "Nyanzapithecus", "Afropithecus", "Heliopithecus", and "Kenyapithecus", all from East Africa.

The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant, such as "Otavipithecus" from cave deposits in Namibia, and "Pierolapithecus" and "Dryopithecus" from France, Spain and Austria, is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, "Oreopithecus", is from coal beds in Italy that have been dated to 9 million years ago.

Molecular evidence indicates that the lineage of gibbons diverged from the line of great apes some 18–12 mya, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown Southeast Asian hominoid population, but fossil proto-orangutans may be represented by "Sivapithecus" from India and "Griphopithecus" from Turkey, dated to around 10 mya.

Hominidae subfamily Homininae (African hominids) diverged from Ponginae (orangutans) about 14 mya. Hominins (including humans and the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8–9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the "Pan" genus (containing chimpanzees and bonobos) 4–7 mya. The "Homo" genus is evidenced by the appearance of "H. habilis" over 2 mya, while anatomically modern humans emerged in Africa approximately 300,000 years ago.

Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by "Nakalipithecus" fossils found in Kenya and "Ouranopithecus" found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus "Pan") split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation– rain forest soils tend to be acidic and dissolve bone– and sampling bias probably contribute to this problem.

Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are "Sahelanthropus tchadensis" (7 Ma) and "Orrorin tugenensis" (6 Ma), followed by "Ardipithecus" (5.5–4.4 Ma), with species "Ar. kadabba" and "Ar. ramidus".

It has been argued in a study of the life history of "Ar. ramidus" that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape. This study demonstrated affinities between the skull morphology of "Ar. ramidus" and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos ("Pan paniscus") the less aggressive species of the genus "Pan", may have evolved via the process of self-domestication. Consequently, arguing against the so-called "chimpanzee referential model" the authors suggest it is no longer tenable to use chimpanzee ("Pan troglodytes") social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in "Ar. ramidus" and the implications this has for the evolution of hominin social psychology, they wrote:

The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.

The genus "Australopithecus" evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including "Australopithecus anamensis", "Au. afarensis", "Au. sediba", and "Au. africanus". There is still some debate among academics whether certain African hominid species of this time, such as "Au. robustus" and "Au. boisei", constitute members of the same genus; if so, they would be considered to be "Au. robust australopiths" whilst the others would be considered "Au. gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, "Paranthropus".
A new proposed species "Australopithecus deyiremeda" is claimed to have been discovered living at the same time period of "Au. afarensis". There is debate if Au. deyiremeda is a new species or is "Au. afarensis." "Australopithecus prometheus", otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus "Australopithecus" as old as "afarensis". Given the opposable big toe found on Little Foot, it seems that the specimen was a good climber. It is thought given the night predators of the region that he built a nesting platform at night in the trees in a similar fashion to chimpanzees and gorillas.

The earliest documented representative of the genus "Homo" is "Homo habilis", which evolved around , and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of "Homo erectus" and "Homo ergaster" in the fossil record, cranial capacity had doubled to 850 cm. (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that "H. erectus" and "H. ergaster" were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between .

According to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from "H. heidelbergensis", "H. rhodesiensis" or "H. antecessor" and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of "H. erectus", Denisova hominins, "H. floresiensis", "H. luzonensis" and "H. neanderthalensis". Archaic "Homo sapiens", the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited interbreeding between these species.<ref name="10.1126/science.1209202"></ref> The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago, according to some anthropologists, although others point to evidence that suggests that a gradual change in behavior took place over a longer time span.

"Homo sapiens" is the only extant species of its genus, "Homo". While some (extinct) "Homo" species might have been ancestors of "Homo sapiens", many, perhaps most, were likely "cousins", having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus "Homo". The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the early variation in the genus "Homo".

Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various "Homo" species and to study the role of diet in physical and behavioral evolution within "Homo".

Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences, killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today. The genetic and archaeological evidence for this remains in question however. Nonetheless, on 31 August 2023, researchers reported, based on genetic studies, that a human ancestor population bottleneck (from a possible 100,000 to 1000 individuals) occurred "around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction."

"Homo habilis" lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines with the development of smaller molars and larger brains. One of the first known hominins, it made tools from stone and perhaps animal bones, leading to its name "homo" "habilis" (Latin 'handy man') bestowed by discoverer Louis Leakey. Some scientists have proposed moving this species from "Homo" into "Australopithecus" due to the morphology of its skeleton being more adapted to living in trees rather than walking on two legs like later hominins.

In May 2010, a new species, "Homo gautengensis", was discovered in South Africa.

These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to "Homo habilis" is not yet clear.

The first fossils of "Homo erectus" were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material "Anthropopithecus erectus" (1892–1893, considered at this point as a chimpanzee-like fossil primate) and "Pithecanthropus erectus" (1893–1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes). Years later, in the 20th century, the German physician and paleoanthropologist Franz Weidenreich (1873–1948) compared in detail the characters of Dubois' Java Man, then named "Pithecanthropus erectus", with the characters of the Peking Man, then named "Sinanthropus pekinensis". Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus "Homo", the species "H. erectus".

"Homo erectus" lived from about 1.8 Ma to about 70,000 years ago – which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby "H. floresiensis" survived it. The early phase of "H. erectus", from 1.8 to 1.25 Ma, is considered by some to be a separate species, "H. ergaster", or as "H. erectus ergaster", a subspecies of "H. erectus". Many paleoanthropologists now use the term "Homo ergaster" for the non-Asian forms of this group, and reserve "H. erectus" only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from "H. ergaster".

In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of "Homo habilis" are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, "Homo erectus"—in Africa. The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. notes that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, and "brains [swollen] to their current, horrendously fuel-inefficient size", and hypothesizes that control of fire and cooking, which released increased nutritional value, was the key adaptation that separated Homo from tree-sleeping Australopithecines.
These are proposed as species intermediate between "H. erectus" and "H. heidelbergensis".

"H. heidelbergensis" ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as "Homo sapiens heidelbergensis" or "Homo sapiens paleohungaricus".


"Homo neanderthalensis", alternatively designated as "Homo sapiens neanderthalensis", lived in Europe and Asia from 400,000 to about 28,000 years ago.
There are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal specimens, many relating to the superior Neanderthal adaptation to cold environments. Neanderthal surface to volume ratio was even lower than that among modern Inuit populations, indicating superior retention of body heat.

Neanderthals also had significantly larger brains, as shown from brain endocasts, casting doubt on their intellectual inferiority to modern humans. However, the higher body mass of Neanderthals may have required larger brain mass for body control. Also, recent research by Pearce, Stringer, and Dunbar has shown important differences in brain architecture. The larger size of the Neanderthal orbital chamber and occipital lobe suggests that they had a better visual acuity than modern humans, useful in the dimmer light of glacial Europe.

Neanderthals may have had less brain capacity available for social functions. Inferring social group size from endocranial volume (minus occipital lobe size) suggests that Neanderthal groups may have been limited to 120 individuals, compared to 144 possible relationships for modern humans. Larger social groups could imply that modern humans had less risk of inbreeding within their clan, trade over larger areas (confirmed in the distribution of stone tools), and faster spread of social and technological innovations. All these may have all contributed to modern Homo sapiens replacing Neanderthal populations by 28,000 BP.

Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between "H. neanderthalensis" and "H. sapiens", and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans c. 45,000-80,000 years ago, around the time modern humans migrated out from Africa, but before they dispersed throughout Europe, Asia and elsewhere. The genetic sequencing of a 40,000-year-old human skeleton from Romania showed that 11% of its genome was Neanderthal, implying the individual had a Neanderthal ancestor 4–6 generations previously, in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.

All modern non-African humans have about 1% to 4% (or 1.5% to 2.6% by more recent data) of their DNA derived from Neanderthals. This finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although this interpretation has been questioned. Neanderthals and AMH "Homo sapiens" could have co-existed in Europe for as long as 10,000 years, during which AMH populations exploded, vastly outnumbering Neanderthals, possibly outcompeting them by sheer numbers.

In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of another human species, the Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.

While the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years, and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.

Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians, indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a higher rate of depression.

The flow of genes from Neanderthal populations to modern humans was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology reported in 2016 that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show more similarity to modern human genes than do European Neanderthal populations. This suggests Neanderthal populations interbred with modern humans around 100,000 years ago, probably somewhere in the Near East.

Studies of a Neanderthal child at Gibraltar show from brain development and tooth eruption that Neanderthal children may have matured more rapidly than Homo sapiens.

"H. floresiensis", which lived from approximately 190,000 to 50,000 years before present (BP), has been nicknamed the "hobbit" for its small size, possibly a result of insular dwarfism. "H. floresiensis" is intriguing both for its size and its age, being an example of a recent species of the genus "Homo" that exhibits derived traits not shared with modern humans. In other words, "H. floresiensis" shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm (considered small for a chimpanzee and less than a third of the "H. sapiens" average of 1400 cm).

However, there is an ongoing debate over whether "H. floresiensis" is indeed a separate species. Some scientists hold that "H. floresiensis" was a modern "H. sapiens" with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on "H. floresiensis" as a separate species is that it was found with tools only associated with "H. sapiens".

The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.

In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to "H. floresiensis" were discovered at Mata Menge, about from Liang Bua. They date to about 700,000 years ago and are noted by Australian archaeologist Gerrit van den Bergh for being even smaller than the later fossils.

A small number of specimens from the island of Luzon, dated 50,000 to 67,000 years ago, have recently been assigned by their discoverers, based on dental characteristics, to a novel human species, "H. luzonensis".

"H. sapiens" (the adjective "sapiens" is Latin for "wise" or "intelligent") emerged in Africa around 300,000 years ago, likely derived from "H. heidelbergensis" or a related lineage. In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans/"H. sapiens", representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.

Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from "H. erectus" to "H. sapiens". The direct evidence suggests there was a migration of "H. erectus" out of Africa, then a further speciation of "H. sapiens" from "H. erectus" in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed "H. erectus". This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. "H. sapiens" interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.

The Toba catastrophe theory, which postulates a population bottleneck for "H. sapiens" about 70,000 years ago, was controversial from its first proposal in the 1990s and by the 2010s had very little support. Distinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.

Since "Homo sapiens" separated from its last common ancestor shared with chimpanzees, human evolution is characterized by a number of morphological, developmental, physiological, behavioral, and environmental changes. Environmental (cultural) evolution discovered much later during the Pleistocene played a significant role in human evolution observed via human transitions between subsistence systems. The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in "H. erectus".

Bipedalism is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either "Sahelanthropus" or "Orrorin", both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorillas and chimpanzees, diverged from the hominin line over a period covering the same time, so either "Sahelanthropus" or "Orrorin" may be our last shared ancestor. "Ardipithecus", a full biped, arose approximately 5.6 million years ago.

The early bipeds eventually evolved into the australopithecines and still later into the genus "Homo". There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion, enabled long-distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat. A 2007 study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking. However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal. This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in habilines.

Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.

The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, thus permitting the passage of newborns due to the increase in cranial size. This is limited to the upper portion, since further increase can hinder normal bipedal movement.

The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth, which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a major effect upon the female reproductive cycle, and the more frequent appearance of alloparenting in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation, the grandmother hypothesis, providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.

The human species eventually developed a much larger brain than that of other primates—typically in modern humans, nearly three times the size of a chimpanzee or gorilla brain. After a period of stasis with "Australopithecus anamensis" and "Ardipithecus", species which had smaller brains as a result of their bipedal locomotion, the pattern of encephalization started with "Homo habilis", whose brain was slightly larger than that of chimpanzees. This evolution continued in "Homo erectus" with , and reached a maximum in Neanderthals with , larger even than modern "Homo sapiens". This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago. Encephalization may be due to a dependency on calorie-dense, difficult-to-acquire food.

Furthermore, the changes in the structure of human brains may be even more significant than the increase in size. Fossilized skulls shows the brain size in early humans fell within the range of modern humans 300,000 years ago, but only got its present-day brain shape between 100,000 and 35,000 years ago. The temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex, which has been related to complex decision-making and moderating social behavior. Encephalization has been tied to increased starches and meat in the diet, however a 2022 meta study called into question the role of meat. Other factors are the development of cooking, and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex. Changes in skull morphology, such as smaller mandibles and mandible muscle attachments, allowed more room for the brain to grow.

The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.

The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from "Homo erectus" to "Homo heidelbergensis" were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities, increases in size of social groups, and increased behavioral plasticity. Humans are unique in the ability to acquire information through social transmission and adapt that information. The emerging field of cultural evolution studies human sociocultural change from an evolutionary perspective.

The reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).

Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.

The ulnar opposition—the contact between the thumb and the tip of the little finger of the same hand—is unique to the genus "Homo", including Neanderthals, the Sima de los Huesos hominins and anatomically modern humans. In other primates, the thumb is short and unable to touch the little finger. The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.

A number of other changes have also characterized the evolution of humans, among them an increased reliance on vision rather than smell (highly reduced olfactory bulb); a longer juvenile developmental period and higher infant dependency; a smaller gut and small, misaligned teeth; faster basal metabolism; loss of body hair; an increase in
eccrine sweat gland density that is ten times higher than any other catarrhinian primates, yet humans uses 30% to 50% less water per day compared to chimps and gorillas; more REM sleep but less sleep in total; a change in the shape of the dental arcade from u-shaped to parabolic; development of a chin (found in "Homo sapiens" alone); styloid processes; and a descended larynx. As the human hand and arms adapted to the making of tools and were used less for climbing, the shoulder blades changed too. As a side effect, it allowed human ancestors to throw objects with greater force, speed and accuracy.

The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes, on average, about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.

Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.

Many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago. The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago. A "Homo" fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the "Homo" species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.

Bernard Wood noted that "Paranthropus" co-existed with the early "Homo" species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies "Paranthropus" as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early "Homo" species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, "Homo" was always present, but "Paranthropus" was not.

In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the "Homo" and "Paranthropus" species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.

Anthropologists describe modern human behavior to include cultural and behavioral traits such as specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (such as grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks, as well as more general traits such as language and complex symbolic thinking. Debate continues as to whether a "revolution" led to modern humans ("big bang of human consciousness"), or whether the evolution was more gradual.

Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase ("H. habilis", "H. ergaster", "H. neanderthalensis") marked a new technology, followed by very slow development until the next phase. Currently paleoanthropologists are debating whether these "Homo" species possessed some or many modern human behaviors. They seem to have been culturally conservative, maintaining the same technologies and foraging patterns over very long periods.

Around 50,000 BP, human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a "Great Leap Forward", or as the "Upper Palaeolithic Revolution", due to the sudden appearance in the archaeological record of distinctive signs of modern behavior and big game hunting. Evidence of behavioral modernity significantly earlier also exists from Africa, with older evidence of abstract imagery, widened subsistence strategies, more sophisticated tools and weapons, and other "modern" behaviors, and many scholars have recently argued that the transition to modernity occurred sooner than previously believed.

Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African "Homo sapiens" 300,000–200,000 years ago. Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a sea journey 60,000 years ago, which may diminish the significance of the Upper Paleolithic Revolution.

Modern humans started burying their dead, making clothing from animal hides, hunting with more sophisticated techniques (such as using pit traps or driving animals off cliffs), and cave painting. As human culture advanced, different populations innovated existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of cultural variation, which had not been seen prior to 50,000 BP. Typically, the older "H. neanderthalensis" populations did not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal imitations of "H. sapiens" Aurignacian technologies.

Anatomically modern human populations continue to evolve, as they are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in the modern age, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urbanization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations, and more recent research indicates that for some traits, the developments and innovations of human culture have driven a new form of selection that coexists with, and in some cases has largely replaced, natural selection.
Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.

Other evolution is related to endemic diseases: the presence of malaria selects for sickle cell trait (the heterozygous form of sickle cell gene), while in the absence of malaria, the health effects of sickle-cell anemia select against this trait. For another example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons. Some reported trends remain unexplained and the subject of ongoing research in the novel field of evolutionary medicine: polycystic ovary syndrome (PCOS) reduces fertility and thus is expected to be subject to extremely strong negative selection, but its relative commonality in human populations suggests a counteracting selection pressure. The identity of that pressure remains the subject of some debate.

Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals, as well as changes in metabolism due to changes in diet, such as lactase persistence.

Culturally-driven evolution can defy the expectations of natural selection: while human populations experience some pressure that drives a selection for producing children at younger ages, the advent of effective contraception, higher education, and changing social norms have driven the observed selection in the opposite direction. However, culturally-driven selection need not necessarily work counter or in opposition to natural selection: some proposals to explain the high rate of recent human brain expansion indicate a kind of feedback whereupon the brain's increased social learning efficiency encourages cultural developments that in turn encourage more efficiency, which drive more complex cultural developments that demand still-greater efficiency, and so forth. Culturally-driven evolution has an advantage in that in addition to the genetic effects, it can be observed also in the archaeological record: the development of stone tools across the Palaeolithic period connects to culturally-driven cognitive development in the form of skill acquisition supported by the culture and the development of increasingly complex technologies and the cognitive ability to elaborate them.

In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.<ref name="doi10.1073/pnas.0906199106"></ref>

The word "homo", the name of the biological genus to which humans belong, is Latin for "human". It was chosen originally by Carl Linnaeus in his classification system. The word "human" is from the Latin "humanus", the adjectival form of "homo". The Latin "homo" derives from the Indo-European root *"dhghem", or "earth". Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.

The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's "On the Origin of Species", in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."

The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and other apes, and did so particularly in his 1863 book "Evidence as to Man's Place in Nature". Many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans in his 1871 book "The Descent of Man, and Selection in Relation to Sex".

A major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of "On the Origin of Species", and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were the remains of a modern human who had suffered some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called "Homo erectus" at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described "Australopithecus africanus". The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.

Although the brain was small (410 cm), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.

During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. These searches were carried out by the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave, fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and "Homo" species, and even "H. erectus".

These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after "Lucy", the most complete fossil member of the species "Australopithecus afarensis", was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect. Lucy was classified as a new species, "Australopithecus afarensis", which is thought to be more closely related to the genus "Homo" as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range. (The specimen was nicknamed "Lucy" after the Beatles' song "Lucy in the Sky with Diamonds", which was played loudly and repeatedly in the camp during the excavations.) The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including "Ardipithecus ramidus" and "A. kadabba".

In 2013, fossil skeletons of "Homo naledi", an extinct species of hominin assigned (provisionally) to the genus "Homo", were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg. , fossils of at least fifteen individuals, amounting to 1,550 specimens, have been excavated from the cave. The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to "Australopithecus", and a cranial morphology (skull shape) similar to early "Homo" species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago, and thus are not a direct ancestor but a contemporary with the first appearance of larger-brained anatomically modern humans.

The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.

In their seminal 1967 paper in "Science", Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy", and reinterpretation of older fossil materials, notably "Ramapithecus", showed the younger estimates to be correct and validated the albumin method.

Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.

On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimpanzees noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimpanzees to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimpanzee populations in eight locations suggests that chimpanzees reproduce at age 26.5 years on average; which suggests the human divergence from chimpanzees occurred between 7 and 13 mya. And these data suggest that "Ardipithecus" (4.5 Ma), "Orrorin" (6 Ma) and "Sahelanthropus" (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.

Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between "proto-human" and "proto-chimpanzees" nonetheless occurred regularly enough to change certain genes in the new gene pool:
The research suggests:

In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered "Australopithecus anamensis". The find was overshadowed by Tim D. White's 1995 discovery of "Ardipithecus ramidus", which pushed back the fossil record to .

In 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named "Orrorin tugenensis". And in 2001, a team led by Michel Brunet discovered the skull of "Sahelanthropus tchadensis" which was dated as , and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin ( Hominidae; terms "hominids" and hominins).

Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus "Homo". Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that the genus "Homo" have migrated out of Africa at least three and possibly four times (e.g. "Homo erectus", "Homo heidelbergensis" and two or three times for "Homo sapiens"). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.

Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus "Homo" at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago. This suggests that the Asian "Chopper" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.

Up until the genetic evidence became available, there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus "Homo" contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff. In contrast, the "out of Africa" model proposed that modern "H. sapiens" speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in the nearly complete replacement of other "Homo" species. This model has been developed by Chris Stringer and Peter Andrews.

Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the "out of Africa" theory and weakened the views of multiregional evolutionism. Aligned in genetic tree differences were interpreted as supportive of a recent single origin.

"Out of Africa" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.

A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters". The research also located a possible origin of modern human migration in southwestern Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.

Recent sequencing of Neanderthal and Denisovan genomes shows that some admixture with these populations has occurred. All modern human groups outside Africa have 1–4% or (according to more recent research) about 1.5–2.6% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that some researchers speculate might be linked to the Toba supervolcano catastrophe, a fairly small group left Africa and interbred with Neanderthals, probably in the Middle East, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in southeastern Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations. The Denisovan EPAS1 gene has also been found in Tibetan populations. Studies of the human genome using machine learning have identified additional genetic contributions in Eurasians from an "unknown" ancestral population potentially related to the Neanderthal-Denisovan lineage.

There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory, which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant. This group seems to have been dependent upon marine resources for their survival.

Stephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated "big game hunting" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.

On the basis of the early date of Badoshan Iranian Aurignacian, Oppenheimer suggests that this second dispersal may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.

Recent genetic evidence suggests that all modern non-African populations, including those of Eurasia and Oceania, are descended from a single wave that left Africa between 65,000 and 50,000 years ago.

The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.

The closest living relatives of humans are bonobos and chimpanzees (both genus "Pan") and gorillas (genus "Gorilla"). With the sequencing of both the human and chimpanzee genome, estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.

The gibbons (family Hylobatidae) and then the orangutans (genus "Pongo") were the first groups to split from the line leading to the hominins, including humans—followed by gorillas (genus "Gorilla"), and, ultimately, by the chimpanzees (genus "Pan"). The splitting date between hominin and chimpanzee lineages is placed by some between , that is, during the Late Miocene. Speciation, however, appears to have been unusually drawn out. Initial divergence occurred sometime between , but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at .

Genetic evidence has also been employed to compare species within the genus "Homo", investigating gene flow between early modern humans and Neanderthals, and to enhance the understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.

Each time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants, a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.

Human evolutionary genetics studies how human genomes differ among individuals, the evolutionary past that gave rise to them, and their current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.

In May 2023, scientists reported a more complicated pathway of human evolution than previously understood. According to the studies, humans evolved from different places and times in Africa, instead of from a single location and period of time.

There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are "Sahelanthropus tchadensis" dating from , "Orrorin tugenensis" dating from , and "Ardipithecus kadabba" dating to . Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.

The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around and diverged into robust (also called "Paranthropus") and gracile branches, one of which (possibly "A. garhi") probably went on to become ancestors of the genus "Homo". The australopithecine species that is best represented in the fossil record is "Australopithecus afarensis" with more than 100 fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as "Au. robustus" (or alternatively "Paranthropus robustus") and "Au./P. boisei" are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.

The earliest member of the genus "Homo" is "Homo habilis" which evolved around . "H. habilis" is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider "Homo rudolfensis", a larger bodied group of fossils with similar morphology to the original "H. habilis" fossils, to be a separate species, while others consider them to be part of "H. habilis"—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.

During the next million years, a process of encephalization began and, by the arrival (about ) of "H. erectus" in the fossil record, cranial capacity had doubled. "H. erectus" were the first of the hominins to emigrate from Africa, and, from , this species spread through Africa, Asia, and Europe. One population of "H. erectus", also sometimes classified as separate species "H. ergaster", remained in Africa and evolved into "H. sapiens". It is believed that "H. erectus" and "H. ergaster" were the first to use fire and complex tools. In Eurasia, "H. erectus" evolved into species such as "H. antecessor", "H. heidelbergensis" and "H. neanderthalensis". The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 300–200,000 years ago such as the Herto and Omo remains of Ethiopia, Jebel Irhoud remains of Morocco, and Florisbad remains of South Africa; later fossils from the Skhul Cave in Israel and Southern Europe begin around 90,000 years ago ().

As modern humans spread out from Africa, they encountered other hominins such as "H. neanderthalensis" and the Denisovans, who may have evolved from populations of "H. erectus" that had left Africa around . The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.

This migration out of Africa is estimated to have begun about 70–50,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.

The hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century. The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species. In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins. Today, approximately 2% of DNA from all non-African populations (including Europeans, Asians, and Oceanians) is Neanderthal, with traces of Denisovan heritage. Also, 4–6% of modern Melanesian genetics are Denisovan. Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.

For example, comparative studies in the mid-2010s found several traits related to neurological, immunological, developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.

Although the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species. In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution. Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.

Stone tools are first attested around 2.6 million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000–10,000 years ago.

Archaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.

The period from 700,000 to 300,000 years ago is also known as the Acheulean, when "H. ergaster" (or "erectus") made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). Bone tools were also made by "H. sapiens" in Africa by 90–70,000 years ago and are also known from early "H. sapiens" sites in Eurasia by about 50,000 years ago.

This list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus "Homo". Please see articles for more information.



Evliya Çelebi

Dervish Mehmed Zillî (25 March 1611 – 1682), known as Evliya Çelebi (), was an Ottoman explorer who travelled through the territory of the Ottoman Empire and neighboring lands during the empire's cultural zenith. He travelled for over 40 years, recording his commentary in a travelogue called the "Seyahatnâme" ("Book of Travel"). The name Çelebi is an honorific meaning "gentleman" or "man of God".

Evliya Çelebi was born in Istanbul in 1611 to a wealthy family from Kütahya. Both his parents were attached to the Ottoman court, his father, Dervish Mehmed Zilli, as a jeweller, and his mother as an Abkhazian relation of the grand vizier Melek Ahmed Pasha. In his book, Evliya Çelebi traces his paternal genealogy back to Ahmad Yasawi, the earliest known Turkic poet and an early Sufi mystic. Evliya Çelebi received a court education from the Imperial "ulama" (scholars). He may have joined the Gulshani Sufi order, as he shows an intimate knowledge of their "khanqah" in Cairo, and a graffito exists in which he referred to himself as "Evliya-yı Gülşenî" ("Evliya of the Gülşenî").

A devout Muslim opposed to fanaticism, Evliya could recite the Quran from memory and joked freely about Islam. Though employed as a clergyman and entertainer at the Imperial Court of Sultan Murad IV, Evliya refused employment that would keep him from travelling. Çelebi had studied vocal and instrumental music as a pupil of a renowned Khalwati dervish by the name of 'Umar Gulshani, and his musical gifts earned him much favor at the Imperial Palace, impressing even the chief musician Amir Guna. He was also trained in the theory of music called . 

His journal-writing began in Istanbul, with the taking of notes on buildings, markets, customs and culture, and in 1640 it was augmented with accounts of his travels beyond the confines of the city. The collected notes of his travels form a ten-volume work called the "Seyahâtname" ("Travelogue"). Departing from the Ottoman literary convention of the time, he wrote in a mixture of vernacular and high Turkish, with the effect that the Seyahatname has remained a popular and accessible reference work about life in the Ottoman Empire in the 17th century, including two chapters on musical instruments.

Evliya Çelebi died in 1684, it is unclear whether he was in Istanbul or Cairo at the time.

Çelebi claimed to have encountered Native Americans as a guest in Rotterdam during his visit of 1663. He wrote: "[they] cursed those priests, saying, 'Our world used to be peaceful, but it has been filled by greedy people, who make war every year and shorten our lives.'"

While visiting Vienna in 1665–66, Çelebi noted some similarities between words in German and Persian, an early observation of the relationship between what would later be known as two Indo-European languages.

Çelebi visited Crete and in book II describes the fall of Chania to the Sultan; in book VIII he recounts the Candia campaign.

During his travels in the Balkan regions of the Ottoman Empire Çelebi visited various regions of the modern-day Croatia including northern Dalmatia, parts of Slavonia, Međimurje and Banija. He recorded variety of historiographic and ethnographic sources. They included descriptions of first hand encounters, third party narrator witnesses and invented elements.

Çelebi traveled to Circassia as well, in 1640. He commented on the women's beauty and talked about the absence of mosques and bazaars despite being a Muslim country. He talks about the hospitality of Circassians and mentions that he could not write the Circassian language using letters, and compared the language to a "magpie shout".

Evliya Çelebi visited the town of Mostar, then in Ottoman Bosnia. He wrote that the name "Mostar" means "bridge-keeper", in reference to the town's celebrated bridge, 28 meters long and 20 meters high. Çelebi wrote that it "is like a rainbow arch soaring up to the skies, extending from one cliff to the other. ...I, a poor and miserable slave of Allah, have passed through 16 countries, but I have never seen such a high bridge. It is thrown from rock to rock as high as the sky."
Evliya Çelebi, who traveled around Anatolia and the Balkans in the 17th century, mentioned the northeast of Bulgaria as the Uz (Oğuz) region, and that a Turkish speaking Muslim society named Çıtak consisting of medium-sized, cheerful and strong people lived in Silistra, and also known as the "Dobruca Çitakları" in Dobruja. He also emphasizes that "Çıtaklar" is made up of a mixture of Tatars, Vlachs, and Bulgarians.

In 1660 Çelebi went to Kosovo and referred to the central part of the region as "Arnavud" (آرناوود) and noted that in Vushtrri its inhabitants were speakers of Albanian or Turkish and few spoke Bosnian. The highlands around the Tetovo, Peja and Prizren areas Çelebi considered as being the "mountains of Arnavudluk". Çelebi referred to the "mountains of Peja" as being in Arnavudluk (آرناوودلق) and considered the Ibar river that converged in Mitrovica as forming Kosovo's border with Bosnia. He viewed the "Kılab" or Llapi river as having its source in Arnavudluk (Albania) and by extension the Sitnica as being part of that river. Çelebi also included the central mountains of Kosovo within Arnavudluk.

Çelebi travelled extensively throughout Albania, visiting it on 3 occasions. He visited Tirana, Lezha, Shkodra and Bushat in 1662, Delvina, Gjirokastra, Tepelena, Skrapar, Përmet, Berat, Kanina, Vlora, Bashtova, Durrës, Kavaja, Peqin, Elbasan, Pogradec, Kavaja and Durrës in 1670.

In 1667 Çelebi expressed his marvel at the Parthenon's sculptures and described the building as "like some impregnable fortress not made by human agency." He composed a poetic supplication that the Parthenon, as "a work less of human hands than of Heaven itself, should remain standing for all time."

Of oil merchants in Baku Çelebi wrote: "By Allah's decree oil bubbles up out of the ground, but in the manner of hot springs, pools of water are formed with oil congealed on the surface like cream. Merchants wade into these pools and collect the oil in ladles and fill goatskins with it, these oil merchants then sell them in different regions. Revenues from this oil trade are delivered annually directly to the Safavid Shah."

Evliya Çelebi remarked on the impact of Cossack raids from Azak upon the territories of the Crimean Khanate, destroying trade routes and severely depopulating the regions. By the time of Çelebi's arrival, many of the towns visited were affected by the Cossacks, and the only place in Crimea he reported as safe was the Ottoman fortress at Arabat.

Çelebi wrote of the slave trade in the Crimea:
Çelebi estimated that there were about 400,000 slaves in the Crimea but only 187,000 free Muslims.

In contrast to many European and some Jewish travelogues of Syria and Palestine in the 17th century, Çelebi wrote one of the few detailed travelogues from an Islamic point of view. Çelebi visited Palestine twice, once in 1649 and once in 1670–1. An English translation of the first part, with some passages from the second, was published in 1935–1940 by the self-taught Palestinian scholar Stephan Hanna Stephan who worked for the Palestine Department of Antiquities. Significant are the many references to Palestine, or "Land of Palestine", and Evliya notes, "All chronicles call this country Palestine."

Evliya reported that the sheriffs of Mecca promoted trade in the region by encouraging fairs from the wealthy merchants. Evliya went on to explain that a large amount of buying and selling occurred in Mecca during the pilgrimage season.

He wrote one of history's longest and most ambitious accounts of travel writing in any language, the "Seyahatnâme". Although many of the descriptions in the "Seyahatnâme" were written in an exaggerated manner or were plainly inventive fiction or third-source misinterpretation, his notes remain a useful guide to the culture and lifestyles of the 17th century Ottoman Empire. The first volume deals exclusively with Istanbul, the final volume with Egypt.

Currently there is no English translation of the entire "Seyahatnâme", although there are translations of various parts. The longest single English translation was published in 1834 by Joseph von Hammer-Purgstall, an Austrian orientalist: it may be found under the name "Evliya Efendi." Von Hammer-Purgstall's work covers the first two volumes (Istanbul and Anatolia) but its language is antiquated. Other translations include Erich Prokosch's nearly complete translation into German of the tenth volume, the 2004 introductory work entitled "The World of Evliya Çelebi: An Ottoman Mentality" written by Robert Dankoff, and Dankoff and Sooyong Kim's 2010 translation of select excerpts of the ten volumes, "An Ottoman Traveller: Selections from the Book of Travels of Evliya Çelebi".

Evliya is noted for having collected samples of the languages in each region he traveled in. There are some 30 Turkic dialects and languages cataloged in the "Seyahatnâme". Çelebi notes the similarities between several words from the German and Persian, though he denies any common Indo-European heritage. The "Seyahatnâme" also contains the first transcriptions of many languages of the Caucasus and Tsakonian, and the only extant specimens of written Ubykh outside the linguistic literature. He also wrote in detail about Arabian horses and their different strains.

In the 10 volumes of his "Seyahatnâme", he describes the following journeys:



It is found in drainages in western Anatolia in Turkey.







Ancient Egyptian religion

Ancient Egyptian religion was a complex system of polytheistic beliefs and rituals that formed an integral part of ancient Egyptian culture. It centered on the Egyptians' interactions with many deities believed to be present and in control of the world. About 1500 deities are known. Rituals such as prayer and offerings were provided to the gods to gain their favor. Formal religious practice centered on the pharaohs, the rulers of Egypt, believed to possess divine powers by virtue of their positions. They acted as intermediaries between their people and the gods, and were obligated to sustain the gods through rituals and offerings so that they could maintain Ma'at, the order of the cosmos, and repel Isfet, which was chaos. The state dedicated enormous resources to religious rituals and to the construction of temples.

Individuals could interact with the gods for their own purposes, appealing for help through prayer or compelling the gods to act through magic. These practices were distinct from, but closely linked with, the formal rituals and institutions. The popular religious tradition grew more prominent over the course of Egyptian history as the status of the pharaoh declined. Egyptian belief in the afterlife and the importance of funerary practices is evident in the great efforts made to ensure the survival of their souls after death – via the provision of tombs, grave goods and offerings to preserve the bodies and spirits of the deceased.

The religion had its roots in Egypt's prehistory and lasted for 3,500 years. The details of religious belief changed over time as the importance of particular gods rose and declined, and their intricate relationships shifted. At various times, certain gods became preeminent over the others, including the sun god Ra, the creator god Amun, and the mother goddess Isis. For a brief period, in the theology promulgated by the pharaoh Akhenaten, a single god, the Aten, replaced the traditional pantheon. Ancient Egyptian religion and mythology left behind many writings and monuments, along with significant influences on ancient and modern cultures.

The beliefs and rituals now referred to as "ancient Egyptian religion" were integral within every aspect of Egyptian culture thus the Egyptian language possessed no single term corresponding to the concept of religion. Ancient Egyptian religion consisted of a vast and varying set of beliefs and practices, linked by their common focus on the interaction between the world of humans and the world of the divine. The characteristics of the gods who populated the divine realm were inextricably linked to the Egyptians' understanding of the properties of the world in which they lived.

The Egyptians believed that the phenomena of nature were divine forces in and of themselves. These deified forces included the elements, animal characteristics, or abstract forces. The Egyptians believed in a pantheon of gods, which were involved in all aspects of nature and human society. Their religious practices were efforts to sustain and placate these phenomena and turn them to human advantage. This polytheistic system was very complex, as some deities were believed to exist in many different manifestations, and some had multiple mythological roles. Conversely, many natural forces, such as the sun, were associated with multiple deities. The diverse pantheon ranged from gods with vital roles in the universe to minor deities or "demons" with very limited or localized functions. It could include gods adopted from foreign cultures, and sometimes humans: deceased pharaohs were believed to be divine, and occasionally, distinguished commoners such as Imhotep also became deified.

The depictions of the gods in art were not meant as literal representations of how the gods might appear if they were visible, as the gods' true natures were believed to be mysterious. Instead, these depictions gave recognizable forms to the abstract deities by using symbolic imagery to indicate each god's role in nature. This iconography was not fixed, and many of the gods could be depicted in more than one form.

Many gods were associated with particular regions in Egypt where their cults were most important. However, these associations changed over time, and they did not mean that the god associated with a place had originated there. For instance, the god Montu was the original patron of the city of Thebes. Over the course of the Middle Kingdom, however, he was displaced in that role by Amun, who may have arisen elsewhere. The national popularity and importance of individual gods fluctuated in a similar way.

Deities had complex interrelationships, which partly reflected the interaction of the forces they represented. The Egyptians often grouped gods together to reflect these relationships. One of the more common combinations was a family triad consisting of a father, mother, and child, who were worshipped together. Some groups had wide-ranging importance. One such group, the Ennead, assembled nine deities into a theological system that was involved in the mythological areas of creation, kingship, and the afterlife.

The relationships between deities could also be expressed in the process of syncretism, in which two or more different gods were linked to form a composite deity. This process was a recognition of the presence of one god "in" another when the second god took on a role belonging to the first. These links between deities were fluid, and did not represent the permanent merging of two gods into one; therefore, some gods could develop multiple syncretic connections. Sometimes, syncretism combined deities with very similar characteristics. At other times, it joined gods with very different natures, as when Amun, the god of hidden power, was linked with Ra, the god of the sun. The resulting god, Amun-Ra, thus united the power that lay behind all things with the greatest and most visible force in nature.

Many deities could be given epithets that seem to indicate that they were greater than any other god, suggesting some kind of unity beyond the multitude of natural forces. This is particularly true of a few gods who, at various points, rose to supreme importance in Egyptian religion. These included the royal patron Horus, the sun-god Ra, and the mother-goddess Isis. During the New Kingdom (–), Amun held this position. The theology of the period described in particular detail Amun's presence in and rule over all things, so that he, more than any other deity, embodied the all-encompassing power of the divine.

The Egyptian conception of the universe centered on "Ma'at", a word that encompasses several concepts in English, including "truth", "justice", and "order". It was the fixed, eternal order of the universe, both in the cosmos and in human society, and was often personified as a goddess. It had existed since the creation of the world, and without it the world would lose its cohesion. In Egyptian belief, "Ma'at" was constantly under threat from the forces of disorder, so all of society was required to maintain it. On the human level this meant that all members of society should cooperate and coexist; on the cosmic level it meant that all of the forces of nature—the gods—should continue to function in balance. This latter goal was central to Egyptian religion. The Egyptians sought to maintain "Ma'at" in the cosmos by sustaining the gods through offerings and by performing rituals which staved off disorder and perpetuated the cycles of nature.

The most important part of the Egyptian view of the cosmos was the conception of time, which was greatly concerned with the maintenance of "Ma'at". Throughout the linear passage of time, a cyclical pattern recurred, in which "Ma'at" was renewed by periodic events which echoed the original creation. Among these events were the annual Nile flood and the succession from one king to another, but the most important was the daily journey of the sun god Ra.

When thinking of the shape of the cosmos, the Egyptians saw the earth as a flat expanse of land, personified by the god Geb, over which arched the sky goddess Nut. The two were separated by Shu, the god of air. Beneath the Earth lay a parallel underworld and undersky, and beyond the skies lay the infinite expanse of Nu, the chaos and primordial watery abyss that had existed before creation. The Egyptians also believed in a place called the Duat, a mysterious region associated with death and rebirth, that may have lain in the underworld or in the sky. Each day, Ra traveled over the earth across the underside of the sky, and at night he passed through the Duat to be reborn at dawn.

In Egyptian belief, this cosmos was inhabited by three types of sentient beings: one was the gods; another was the spirits of deceased humans, who existed in the divine realm and possessed many of the gods' abilities; living humans were the third category, and the most important among them was the pharaoh, who bridged the human and divine realms.

Egyptologists have long debated the degree to which the pharaoh was considered a god. It seems most likely that the Egyptians viewed royal authority itself as a divine force. Therefore, although the Egyptians recognized that the pharaoh was human and subject to human weakness, they simultaneously viewed him as a god, because the divine power of kingship was incarnated in him. He therefore acted as intermediary between Egypt's people and the gods. He was key to upholding "Ma'at", both by maintaining justice and harmony in human society and by sustaining the gods with temples and offerings. For these reasons, he oversaw all state religious activity. However, the pharaoh's real-life influence and prestige could differ from his portrayal in official writings and depictions, and beginning in the late New Kingdom his religious importance declined drastically.

The king was also associated with many specific deities. He was identified directly with Horus, who represented kingship itself, and he was seen as the son of Ra, who ruled and regulated nature as the pharaoh ruled and regulated society. By the New Kingdom he was also associated with Amun, the supreme force in the cosmos. Upon his death, the king became fully deified. In this state, he was directly identified with Ra, and was also associated with Osiris, god of death and rebirth and the mythological father of Horus. Many mortuary temples were dedicated to the worship of deceased pharaohs as gods.

The elaborate beliefs about death and the afterlife reinforced the Egyptians theology in humans possessions a "ka", or life-force, which left the body at the point of death. In life, the "ka" received its sustenance from food and drink, so it was believed that, to endure after death, the "ka" must continue to receive offerings of food, whose spiritual essence it could still consume. Each person also had a "ba", the set of spiritual characteristics unique to each individual. Unlike the "ka", the "ba" remained attached to the body after death. Egyptian funeral rituals were intended to release the "ba" from the body so that it could move freely, and to rejoin it with the "ka" so that it could live on as an "akh". However, it was also important that the body of the deceased be preserved, as the Egyptians believed that the "ba" returned to its body each night to receive new life, before emerging in the morning as an "akh".
In early times the deceased pharaoh was believed to ascend to the sky and dwell among the stars. Over the course of the Old Kingdom (–2181 BC), however, he came to be more closely associated with the daily rebirth of the sun god Ra and with the underworld ruler Osiris as those deities grew more important.

In the fully developed afterlife beliefs of the New Kingdom, the soul had to avoid a variety of supernatural dangers in the Duat, before undergoing a final judgement, known as the "Weighing of the Heart", carried out by Osiris and by the Assessors of Ma'at. In this judgement, the gods compared the actions of the deceased while alive (symbolized by the heart) to the feather of Ma'at, to determine whether he or she had behaved in accordance with Ma'at. If the deceased was judged worthy, his or her "ka" and "ba" were united into an "akh". Several beliefs coexisted about the "akh" destination. Often the dead were said to dwell in the realm of Osiris, a lush and pleasant land in the underworld. The solar vision of the afterlife, in which the deceased soul traveled with Ra on his daily journey, was still primarily associated with royalty, but could extend to other people as well. Over the course of the Middle and New Kingdoms, the notion that the "akh" could also travel in the world of the living, and to some degree magically affect events there, became increasingly prevalent.

During the New Kingdom the pharaoh Akhenaten abolished the official worship of other gods in favor of the sun-disk Aten. This is often seen as the first instance of true monotheism in history, although the details of Atenist theology are still unclear and the suggestion that it was monotheistic is disputed. The exclusion of all but one god from worship was a radical departure from Egyptian tradition and some see Akhenaten as a practitioner of monolatry or henotheism rather than monotheism, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten. Under Akhenaten's successors Egypt reverted to its traditional religion, and Akhenaten himself came to be reviled as a heretic.

While the Egyptians had no unified religious scripture, they produced many religious writings of various types. Together the disparate texts provide an extensive, but still incomplete, understanding of Egyptian religious practices and beliefs.

Egyptian myths were stories intended to illustrate and explain the gods' actions and roles in nature. The details of the events they recounted could change to convey different symbolic perspectives on the mysterious divine events they described, so many myths exist in different and conflicting versions. Mythical narratives were rarely written in full, and more often texts only contain episodes from or allusions to a larger myth. Knowledge of Egyptian mythology, therefore, is derived mostly from hymns that detail the roles of specific deities, from ritual and magical texts which describe actions related to mythic events, and from funerary texts which mention the roles of many deities in the afterlife. Some information is also provided by allusions in secular texts. Finally, Greeks and Romans such as Plutarch recorded some of the extant myths late in Egyptian history.

Among the significant Egyptian myths were the creation myths. According to these stories, the world emerged as a dry space in the primordial ocean of chaos. Because the sun is essential to life on earth, the first rising of Ra marked the moment of this emergence. Different forms of the myth describe the process of creation in various ways: a transformation of the primordial god Atum into the elements that form the world, as the creative speech of the intellectual god Ptah, and as an act of the hidden power of Amun. Regardless of these variations, the act of creation represented the initial establishment of Ma'at and the pattern for the subsequent cycles of time.The most important of all Egyptian myths was the Osiris myth. It tells of the divine ruler Osiris, who was murdered by his jealous brother Set, a god often associated with chaos. Osiris' sister and wife Isis resurrected him so that he could conceive an heir, Horus. Osiris then entered the underworld and became the ruler of the dead. Once grown, Horus fought and defeated Set to become king himself. Set's association with chaos, and the identification of Osiris and Horus as the rightful rulers, provided a rationale for pharaonic succession and portrayed the pharaohs as the upholders of order. At the same time, Osiris' death and rebirth were related to the Egyptian agricultural cycle, in which crops grew in the wake of the Nile inundation, and provided a template for the resurrection of human souls after death.

Another important mythic motif was the journey of Ra through the Duat each night. In the course of this journey, Ra met with Osiris, who again acted as an agent of regeneration, so that his life was renewed. He also fought each night with Apep, a serpentine god representing chaos. The defeat of Apep and the meeting with Osiris ensured the rising of the sun the next morning, an event that represented rebirth and the victory of order over chaos.

The procedures for religious rituals were frequently written on papyri, which were used as instructions for those performing the ritual. These ritual texts were kept mainly in the temple libraries. Temples themselves are also inscribed with such texts, often accompanied by illustrations. Unlike the ritual papyri, these inscriptions were not intended as instructions, but were meant to symbolically perpetuate the rituals even if, in reality, people ceased to perform them. Magical texts likewise describe rituals, although these rituals were part of the spells used for specific goals in everyday life. Despite their mundane purpose, many of these texts also originated in temple libraries and later became disseminated among the general populace.

The Egyptians produced numerous prayers and hymns, written in the form of poetry. Hymns and prayers follow a similar structure and are distinguished mainly by the purposes they serve. Hymns were written to praise particular deities. Like ritual texts, they were written on papyri and on temple walls, and they were probably recited as part of the rituals they accompany in temple inscriptions. Most are structured according to a set literary formula, designed to expound on the nature, aspects, and mythological functions of a given deity. They tend to speak more explicitly about fundamental theology than other Egyptian religious writings, and became particularly important in the New Kingdom, a period of particularly active theological discourse. Prayers follow the same general pattern as hymns, but address the relevant god in a more personal way, asking for blessings, help, or forgiveness for wrongdoing. Such prayers are rare before the New Kingdom, indicating that in earlier periods such direct personal interaction with a deity was not believed possible, or at least was less likely to be expressed in writing. They are known mainly from inscriptions on statues and stelae left in sacred sites as votive offerings.

Among the most significant and extensively preserved Egyptian writings are funerary texts designed to ensure that deceased souls reached a pleasant afterlife. The earliest of these are the Pyramid Texts. They are a loose collection of hundreds of spells inscribed on the walls of royal pyramids during the Old Kingdom, intended to magically provide pharaohs with the means to join the company of the gods in the afterlife. The spells appear in differing arrangements and combinations, and few of them appear in all of the pyramids.

At the end of the Old Kingdom a new body of funerary spells, which included material from the Pyramid Texts, began appearing in tombs, inscribed primarily on coffins. This collection of writings is known as the Coffin Texts, and was not reserved for royalty, but appeared in the tombs of non-royal officials. In the New Kingdom, several new funerary texts emerged, of which the best-known is the Book of the Dead. Unlike the earlier books, it often contains extensive illustrations, or vignettes. The book was copied on papyrus and sold to commoners to be placed in their tombs.

The Coffin Texts included sections with detailed descriptions of the underworld and instructions on how to overcome its hazards. In the New Kingdom, this material gave rise to several "books of the netherworld", including the Book of Gates, the Book of Caverns, and the Amduat. Unlike the loose collections of spells, these netherworld books are structured depictions of Ra's passage through the Duat, and by analogy, the journey of the deceased person's soul through the realm of the dead. They were originally restricted to pharaonic tombs, but in the Third Intermediate Period they came to be used more widely.

Temples existed from the beginning of Egyptian history, and at the height of the civilization they were present in most of its towns. They included both mortuary temples to serve the spirits of deceased pharaohs and temples dedicated to patron gods, although the distinction was blurred because divinity and kingship were so closely intertwined. The temples were not primarily intended as places for worship by the general populace, and the common people had a complex set of religious practices of their own. Instead, the state-run temples served as houses for the gods, in which physical images which served as their intermediaries were cared for and provided with offerings. This service was believed to be necessary to sustain the gods, so that they could in turn maintain the universe itself. Thus, temples were central to Egyptian society, and vast resources were devoted to their upkeep, including both donations from the monarchy and large estates of their own. Pharaohs often expanded them as part of their obligation to honor the gods, so that many temples grew to enormous size. However, not all gods had temples dedicated to them, as many gods who were important in official theology received only minimal worship, and many household gods were the focus of popular veneration rather than temple ritual.

The earliest Egyptian temples were small, impermanent structures, but through the Old and Middle Kingdoms their designs grew more elaborate, and they were increasingly built out of stone. In the New Kingdom, a basic temple layout emerged, which had evolved from common elements in Old and Middle Kingdom temples. With variations, this plan was used for most of the temples built from then on, and most of those that survive today adhere to it. In this standard plan, the temple was built along a central processional way that led through a series of courts and halls to the sanctuary, which held a statue of the temple's god. Access to this most sacred part of the temple was restricted to the pharaoh and the highest-ranking priests. The journey from the temple entrance to the sanctuary was seen as a journey from the human world to the divine realm, a point emphasized by the complex mythological symbolism present in temple architecture. Well beyond the temple building proper was the outermost wall. Between the two lay many subsidiary buildings, including workshops and storage areas to supply the temple's needs, and the library where the temple's sacred writings and mundane records were kept, and which also served as a center of learning on a multitude of subjects.

Theoretically it was the duty of the pharaoh to carry out temple rituals, as he was Egypt's official representative to the gods. In reality, ritual duties were almost always carried out by priests. During the Old and Middle Kingdoms, there was no separate class of priests; instead, many government officials served in this capacity for several months out of the year before returning to their secular duties. Only in the New Kingdom did professional priesthood become widespread, although most lower-ranking priests were still part-time. All were still employed by the state, and the pharaoh had final say in their appointments. However, as the wealth of the temples grew, the influence of their priesthoods increased, until it rivaled that of the pharaoh. In the political fragmentation of the Third Intermediate Period (–664 BC), the high priests of Amun at Karnak even became the effective rulers of Upper Egypt. The temple staff also included many people other than priests, such as musicians and chanters in temple ceremonies. Outside the temple were artisans and other laborers who helped supply the temple's needs, as well as farmers who worked on temple estates. All were paid with portions of the temple's income. Large temples were therefore very important centers of economic activity, sometimes employing thousands of people.

State religious practice included both temple rituals involved in the cult of a deity, and ceremonies related to divine kingship. Among the latter were coronation ceremonies and the Sed festival, a ritual renewal of the pharaoh's strength that took place periodically during his reign. There were numerous temple rituals, including rites that took place across the country and rites limited to single temples or to the temples of a single god. Some were performed daily, while others took place annually or on rare occasions. The most common temple ritual was the morning offering ceremony, performed daily in temples across Egypt. In it, a high-ranking priest, or occasionally the pharaoh, washed, anointed, and elaborately dressed the god's statue before presenting it with offerings. Afterward, when the god had consumed the spiritual essence of the offerings, the items themselves were taken to be distributed among the priests.

The less frequent temple rituals, or festivals, were still numerous, with dozens occurring every year. These festivals often entailed actions beyond simple offerings to the gods, such as reenactments of particular myths or the symbolic destruction of the forces of disorder. Most of these events were probably celebrated only by the priests and took place only inside the temple. However, the most important temple festivals, like the Opet Festival celebrated at Karnak, usually involved a procession carrying the god's image out of the sanctuary in a model barque to visit other significant sites, such as the temple of a related deity. Commoners gathered to watch the procession and sometimes received portions of the unusually large offerings given to the gods on these occasions.

At many sacred sites, the Egyptians worshipped individual animals which they believed to be manifestations of particular deities. These animals were selected based on specific sacred markings which were believed to indicate their fitness for the role. Some of these cult animals retained their positions for the rest of their lives, as with the Apis bull worshipped in Memphis as a manifestation of Ptah. Other animals were selected for much shorter periods. These cults grew more popular in later times, and many temples began raising stocks of such animals from which to choose a new divine manifestation. A separate practice developed in the Twenty-sixth Dynasty, when people began mummifying any member of a particular animal species as an offering to the god whom the species represented. Millions of mummified cats, birds, and other creatures were buried at temples honoring Egyptian deities. Worshippers paid the priests of a particular deity to obtain and mummify an animal associated with that deity, and the mummy was placed in a cemetery near the god's cult center.

The Egyptians used oracles to ask the gods for knowledge or guidance. Egyptian oracles are known mainly from the New Kingdom and afterward, though they probably appeared much earlier. People of all classes, including the king, asked questions of oracles. The most common means of consulting an oracle was to pose a question to the divine image while it was being carried in a festival procession, and interpret an answer from the barque's movements. Other methods included interpreting the behavior of cult animals, drawing lots, or consulting statues through which a priest apparently spoke. The means of discerning the god's will gave great influence to the priests who spoke and interpreted the god's message.

While the state cults were meant to preserve the stability of the Egyptian world, lay individuals had their own religious practices that related more directly to daily life. This popular religion left less evidence than the official cults, and because this evidence was mostly produced by the wealthiest portion of the Egyptian population, it is uncertain to what degree it reflects the practices of the populace as a whole.

Popular religious practice included ceremonies marking important transitions in life. These included birth, because of the danger involved in the process, and naming, because the name was held to be a crucial part of a person's identity. The most important of these ceremonies were those surrounding death, because they ensured the soul's survival beyond it. Other religious practices sought to discern the gods' will or seek their knowledge. These included the interpretation of dreams, which could be seen as messages from the divine realm, and the consultation of oracles. People also sought to affect the gods' behavior to their own benefit through magical rituals.

Individual Egyptians also prayed to gods and gave them private offerings. Evidence of this type of personal piety is sparse before the New Kingdom. This is probably due to cultural restrictions on depiction of nonroyal religious activity, which relaxed during the Middle and New Kingdoms. Personal piety became still more prominent in the late New Kingdom, when it was believed that the gods intervened directly in individual lives, punishing wrongdoers and saving the pious from disaster. Official temples were important venues for private prayer and offering, even though their central activities were closed to laypeople. Egyptians frequently donated goods to be offered to the temple deity and objects inscribed with prayers to be placed in temple courts. Often they prayed in person before temple statues or in shrines set aside for their use. Yet in addition to temples, the populace also used separate local chapels, smaller but more accessible than the formal temples. These chapels were very numerous and probably staffed by members of the community. Households, too, often had their own small shrines for offering to gods or deceased relatives.

The deities invoked in these situations differed somewhat from those at the center of state cults. Many of the important popular deities, such as the fertility goddess Taweret and the household protector Bes, had no temples of their own. However, many other gods, including Amun and Osiris, were very important in both popular and official religion. Some individuals might be particularly devoted to a single god. Often they favored deities affiliated with their own region, or with their role in life. The god Ptah, for instance, was particularly important in his cult center of Memphis, but as the patron of craftsmen he received the nationwide veneration of many in that occupation.

The word ""magic"" is normally used to translate the Egyptian term "heka", which meant, as James P. Allen puts it, "the ability to make things happen by indirect means".

"Heka" was believed to be a natural phenomenon, the force which was used to create the universe and which the gods employed to work their will. Humans could also use it, and magical practices were closely intertwined with religion. In fact, even the regular rituals performed in temples were counted as magical. Individuals also frequently employed magical techniques for personal purposes. Although these ends could be harmful to other people, no form of magic was considered inimical in itself. Instead, magic was seen primarily as a way for humans to prevent or overcome negative events.

Magic was closely associated with the priesthood. Because temple libraries contained numerous magical texts, great magical knowledge was ascribed to the lector priests, who studied these texts. These priests often worked outside their temples, hiring out their magical services to laymen. Other professions also commonly employed magic as part of their work, including doctors, scorpion-charmers, and makers of magical amulets. It is also possible that the peasantry used simple magic for their own purposes, but because this magical knowledge would have been passed down orally, there is limited evidence of it.

Language was closely linked with "heka", to such a degree that Thoth, the god of writing, was sometimes said to be the inventor of "heka". Therefore, magic frequently involved written or spoken incantations, although these were usually accompanied by ritual actions. Often these rituals invoked an appropriate deity to perform the desired action, using the power of "heka" to compel the deity to act. Sometimes this entailed casting the practitioner or subject of a ritual in the role of a character in mythology, thus inducing the god to act toward that person as it had in the myth.

Rituals also employed sympathetic magic, using objects believed to have a magically significant resemblance to the subject of the rite. The Egyptians also commonly used objects believed to be imbued with "heka" of their own, such as the magically protective amulets worn in great numbers by ordinary Egyptians.

Because it was considered necessary for the survival of the soul, preservation of the body was a central part of Egyptian funerary practices. Originally the Egyptians buried their dead in the desert, where the arid conditions mummified the body naturally. In the Early Dynastic Period, however, they began using tombs for greater protection, and the body was insulated from the desiccating effect of the sand and was subject to natural decay. Thus, the Egyptians developed their elaborate embalming practices, in which the corpse was artificially desiccated and wrapped to be placed in its coffin. The quality of the process varied according to cost, however, and those who could not afford it were still buried in desert graves.

Once the mummification process was complete, the mummy was carried from the deceased person's house to the tomb in a funeral procession that included his or her relatives and friends, along with a variety of priests. Before the burial, these priests performed several rituals, including the Opening of the mouth ceremony intended to restore the dead person's senses and give him or her the ability to receive offerings. Then the mummy was buried and the tomb sealed. Afterwards, relatives or hired priests gave food offerings to the deceased in a nearby mortuary chapel at regular intervals. Over time, families inevitably neglected offerings to long-dead relatives, so most mortuary cults only lasted one or two generations. However, while the cult lasted, the living sometimes wrote letters asking deceased relatives for help, in the belief that the dead could affect the world of the living as the gods did.

The first Egyptian tombs were mastabas, rectangular brick structures where kings and nobles were entombed. Each of them contained a subterranean burial chamber and a separate, above ground chapel for mortuary rituals. In the Old Kingdom the mastaba developed into the pyramid, which symbolized the primeval mound of Egyptian myth. Pyramids were reserved for royalty, and were accompanied by large mortuary temples sitting at their base. Middle Kingdom pharaohs continued to build pyramids, but the popularity of mastabas waned. Increasingly, commoners with sufficient means were buried in rock-cut tombs with separate mortuary chapels nearby, an approach which was less vulnerable to tomb robbery. By the beginning of the New Kingdom even the pharaohs were buried in such tombs, and they continued to be used until the decline of the religion itself.

Tombs could contain a great variety of other items, including statues of the deceased to serve as substitutes for the body in case it was damaged. Because it was believed that the deceased would have to do work in the afterlife, just as in life, burials often included small models of humans to do work in place of the deceased. Human sacrifices found in early royal tombs were probably meant to serve the pharaoh in his afterlife.

The tombs of wealthier individuals could also contain furniture, clothing, and other everyday objects intended for use in the afterlife, along with amulets and other items intended to provide magical protection against the hazards of the spirit world. Further protection was provided by funerary texts included in the burial. The tomb walls also bore artwork, such as images of the deceased eating food that were believed to allow him or her to magically receive sustenance even after the mortuary offerings had ceased.
The beginnings of Egyptian religion extend into prehistory, though evidence for them comes only from the sparse and ambiguous archaeological record. Careful burials during the Predynastic period imply that the people of this time believed in some form of an afterlife. At the same time, animals were ritually buried, a practice which may reflect the development of zoomorphic deities like those found in the later religion. The evidence is less clear for gods in human form, and this type of deity may have emerged more slowly than those in animal shape. Each region of Egypt originally had its own patron deity, but it is likely that as these small communities conquered or absorbed each other, the god of the defeated area was either incorporated into the other god's mythology or entirely subsumed by it. This resulted in a complex pantheon in which some deities remained only locally important while others developed more universal significance.

Archaeological data has suggested that the Egyptian religious system had close, cultural affinities with Eastern African populations and arose from an African substratum rather than deriving from the Mesopotamian or Mediterranean regions.

The Early Dynastic Period began with the unification of Egypt around 3000 BC. This event transformed Egyptian religion, as some deities rose to national importance and the cult of the divine pharaoh became the central focus of religious activity. Horus was identified with the king, and his cult center in the Upper Egyptian city of Nekhen was among the most important religious sites of the period. Another important center was Abydos, where the early rulers built large funerary complexes.

During the Old Kingdom, the priesthoods of the major deities attempted to organize the complicated national pantheon into groups linked by their mythology and worshipped in a single cult center, such as the Ennead of Heliopolis, which linked important deities such as Atum, Ra, Osiris, and Set in a single creation myth. Meanwhile, pyramids, accompanied by large mortuary temple complexes, replaced mastabas as the tombs of pharaohs. In contrast with the great size of the pyramid complexes, temples to gods remained comparatively small, suggesting that official religion in this period emphasized the cult of the divine king more than the direct worship of deities. The funerary rituals and architecture of this time greatly influenced the more elaborate temples and rituals used in worshipping the gods in later periods.
The Ancient Egyptians regarded the sun as a powerful life force. The sun god Ra had been worshipped from the Early Dynastic period (3100–2686 BCE), but it was not until the Old Kingdom (2686–2181 BCE), when Ra became the dominant figure in the Egyptian pantheon, that the Sun Cult took power. Early in the Old Kingdom, Ra grew in influence, and his cult center at Heliopolis became the nation's most important religious site. By the Fifth Dynasty, Ra was the most prominent god in Egypt and had developed the close links with kingship and the afterlife that he retained for the rest of Egyptian history. Around the same time, Osiris became an important afterlife deity. The Pyramid Texts, first written at this time, reflect the prominence of the solar and Osirian concepts of the afterlife, although they also contain remnants of much older traditions. The texts are an extremely important source for understanding early Egyptian theology.

Symbols such as the 'winged disc' took on new features. Originally, the solar disk with the wings of a hawk was originally the symbol of Horus and associated with his cult in the Delta town of Behdet. The sacred cobras were added on either side of the disc during the Old Kingdom. The winged disc had protective significance and was found on temple ceilings and ceremonial entrances.

In the 22nd century BC, the Old Kingdom collapsed into the disorder of the First Intermediate Period. Eventually, rulers from Thebes reunified the Egyptian nation in the Middle Kingdom (–1650 BC). These Theban pharaohs initially promoted their patron god Montu to national importance, but during the Middle Kingdom, he was eclipsed by the rising popularity of Amun. In this new Egyptian state, personal piety grew more important and was expressed more freely in writing, a trend that continued in the New Kingdom.

The Middle Kingdom crumbled in the Second Intermediate Period (–1550 BC), but the country was again reunited by Theban rulers, who became the first pharaohs of the New Kingdom. Under the new regime, Amun became the supreme state god. He was syncretized with Ra, the long-established patron of kingship and his temple at Karnak in Thebes became Egypt's most important religious center. Amun's elevation was partly due to the great importance of Thebes, but it was also due to the increasingly professional priesthood. Their sophisticated theological discussion produced detailed descriptions of Amun's universal power.

Increased contact with outside peoples in this period led to the adoption of many Near Eastern deities into the pantheon. At the same time, the subjugated Nubians absorbed Egyptian religious beliefs, and in particular, adopted Amun as their own.
The New Kingdom religious order was disrupted when Akhenaten acceded, and replaced Amun with the Aten as the state god. Eventually, he eliminated the official worship of most other gods and moved Egypt's capital to a new city at Amarna. This part of Egyptian history, the Amarna Period, is named after this. In doing so, Akhenaten claimed unprecedented status: only he could worship the Aten, and the populace directed their worship toward him. The Atenist system lacked well-developed mythology and afterlife beliefs, and the Aten seemed distant and impersonal, so the new order did not appeal to ordinary Egyptians. Thus, many probably continued to worship the traditional gods in private. Nevertheless, the withdrawal of state support for the other deities severely disrupted Egyptian society. Akhenaten's successors restored the traditional religious system, and eventually, they dismantled all Atenist monuments.

Before the Amarna Period, popular religion had trended toward more personal relationships between worshippers and their gods. Akhenaten's changes had reversed this trend, but once the traditional religion was restored, there was a backlash. The populace began to believe that the gods were much more directly involved in daily life. Amun, the supreme god, was increasingly seen as the final arbiter of human destiny, the true ruler of Egypt. The pharaoh was correspondingly more human and less divine. The importance of oracles as a means of decision-making grew, as did the wealth and influence of the oracles' interpreters, the priesthood. These trends undermined the traditional structure of society and contributed to the breakdown of the New Kingdom.

In the 1st millennium BC, Egypt was significantly weaker than in earlier times, and in several periods foreigners seized the country and assumed the position of pharaoh. The importance of the pharaoh continued to decline, and the emphasis on popular piety continued to increase. Animal cults, a characteristically Egyptian form of worship, became increasingly popular in this period, possibly as a response to the uncertainty and foreign influence of the time. Isis grew more popular as a goddess of protection, magic, and personal salvation, and became the most important goddess in Egypt.

In the 4th century BC, Egypt became a Hellenistic kingdom under the Ptolemaic dynasty (305–30 BC), which assumed the pharaonic role, maintaining the traditional religion and building or rebuilding many temples. The kingdom's Greek ruling class identified the Egyptian deities with their own. From this cross-cultural syncretism emerged Serapis, a god who combined Osiris and Apis with characteristics of Greek deities, and who became very popular among the Greek population. Nevertheless, for the most part the two belief systems remained separate, and the Egyptian deities remained Egyptian.

Ptolemaic-era beliefs changed little after Egypt became a province of the Roman Empire in 30 BC, with the Ptolemaic kings replaced by distant emperors. The cult of Isis appealed even to Greeks and Romans outside Egypt, and in Hellenized form it spread across the empire. In Egypt itself, as the empire weakened, official temples fell into decay, and without their centralizing influence religious practice became fragmented and localized. Meanwhile, Christianity spread across Egypt, and in the third and fourth centuries AD, edicts by Christian emperors and the missionary activity of Christians eroded traditional beliefs.

Nevertheless, the traditional Egyptian religion persisted for a long time. The traditional worship in the temples of the city of Philae apparently survived at least until the 5th century, despite the active Christianization of Egypt. In fact, the fifth-century historian Priscus mentions a treaty between the Roman commander Maximinus and the Blemmyes and Nobades in 452, which among other things ensured access to the cult image of Isis.

According to the 6th-century historian Procopius, the temples in Philae was closed down officially in AD 537 by the local commander Narses the Persarmenian in accordance with an order of Byzantine emperor Justinian I. This event is conventionally considered to mark the end of ancient Egyptian religion. However, its importance has recently come into question, following a major study by Jitse Dijkstra who argues that organized paganism at Philae ended in the fifth century, based on the fact that the last inscriptional evidence of an active pagan priesthood there dates to the 450s. Nevertheless, some adherence to traditional religion seems to have survived into the sixth century, based on a petition from Dioscorus of Aphrodito to the governor of the Thebaid dated to 567. The letter warns of an unnamed man (the text calls him "eater of raw meat") who, in addition to plundering houses and stealing tax revenue, is alleged to have restored paganism at "the sanctuaries," possibly referring to the temples at Philae.

While it persisted among the populace for some time, Egyptian religion slowly faded away.

Egyptian religion produced the temples and tombs which are ancient Egypt's most enduring monuments, but it also influenced other cultures. In pharaonic times many of its symbols, such as the sphinx and winged solar disk, were adopted by other cultures across the Mediterranean and Near East, as were some of its deities, such as Bes. Some of these connections are difficult to trace. The Greek concept of Elysium may have derived from the Egyptian vision of the afterlife. In late antiquity, the Christian conception of Hell was most likely influenced by some of the imagery of the Duat. Egyptian beliefs also influenced or gave rise to several esoteric belief systems developed by Greeks and Romans, who considered Egypt as a source of mystic wisdom. Hermeticism, for instance, derived from the tradition of secret magical knowledge associated with Thoth.

Traces of ancient beliefs remained in Egyptian folk traditions into modern times, but its influence on modern societies greatly increased with the French Campaign in Egypt and Syria in 1798 and their seeing the monuments and images. As a result of it, Westerners began to study Egyptian beliefs firsthand, and Egyptian religious motifs were adopted into Western art. Egyptian religion has since had a significant influence in popular culture. Due to continued interest in Egyptian beliefs, in the late 20th century, several new religious groups going under the blanket term of Kemetism have formed based on different reconstructions of ancient Egyptian religion. Kemetism is a neopagan religion and revival of the ancient Egyptian religion and related expressions of religion in classical and late antiquity, emerging during the 1970s. Kemetics do not consider themselves direct descendants of the ancient Egyptian religion but consistently speak of its recreation or restoration.




Educational psychology

Educational psychology is the branch of psychology concerned with the scientific study of human learning. The study of learning processes, from both cognitive and behavioral perspectives, allows researchers to understand individual differences in intelligence, cognitive development, affect, motivation, self-regulation, and self-concept, as well as their role in learning. The field of educational psychology relies heavily on quantitative methods, including testing and measurement, to enhance educational activities related to instructional design, classroom management, and assessment, which serve to facilitate learning processes in various educational settings across the lifespan.

Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. It is also informed by neuroscience. Educational psychology in turn informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education, classroom management, and student motivation. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks.

The field of educational psychology involves the study of memory, conceptual processes, and individual differences (via cognitive psychology) in conceptualizing new strategies for learning processes in humans. Educational psychology has been built upon theories of operant conditioning, functionalism, structuralism, constructivism, humanistic psychology, Gestalt psychology, and information processing.

Educational psychology has seen rapid growth and development as a profession in the last twenty years. School psychology began with the concept of intelligence testing leading to provisions for special education students, who could not follow the regular classroom curriculum in the early part of the 20th century. Another main focus of school psychology was to help close the gap for children of colour, as the fight against racial inequality and segregation was still very prominent, during the early to mid-1900s. However, "school psychology" itself has built a fairly new profession based upon the practices and theories of several psychologists among many different fields. Educational psychologists are working side by side with psychiatrists, social workers, teachers, speech and language therapists, and counselors in an attempt to understand the questions being raised when combining behavioral, cognitive, and social psychology in the classroom setting.

As a field of study, educational psychology is fairly new and was not considered a specific practice until the 20th century. Reflections on everyday teaching and learning allowed some individuals throughout history to elaborate on developmental differences in cognition, the nature of instruction, and the transfer of knowledge and learning. These topics are important to education and, as a result, they are important in understanding human cognition, learning, and social perception.

Some of the ideas and issues pertaining to educational psychology date back to the time of Plato and Aristotle. Philosophers as well as sophists discussed the purpose of education, training of the body and the cultivation of psycho-motor skills, the formation of good character, the possibilities and limits of moral education. Some other educational topics they spoke about were the effects of music, poetry, and the other arts on the development of the individual, role of the teacher, and the relations between teacher and student. Plato saw knowledge acquisition as an innate ability, which evolves through experience and understanding of the world. This conception of human cognition has evolved into a continuing argument of nature vs. nurture in understanding conditioning and learning today. Aristotle, on the other hand, ascribed to the idea of knowledge by association or schema. His four laws of association included succession, contiguity, similarity, and contrast. His studies examined recall and facilitated learning processes.

John Locke is considered one of the most influential philosophers in post-renaissance Europe, a time period that began around the mid-1600s. Locke is considered the "Father of English Psychology". One of Locke's most important works was written in 1690, named "An Essay Concerning Human Understanding". In this essay, he introduced the term "tabula rasa" meaning "blank slate." Locke explained that learning was attained through experience only and that we are all born without knowledge.

He followed by contrasting Plato's theory of innate learning processes. Locke believed the mind was formed by experiences, not innate ideas. Locke introduced this idea as "empiricism", or the understanding that knowledge is only built on knowledge and experience.

In the late 1600s, John Locke advanced the hypothesis that people learn primarily from external forces. He believed that the mind was like a blank tablet (tabula rasa), and that successions of simple impressions give rise to complex ideas through association and reflection. Locke is credited with establishing "empiricism" as a criterion for testing the validity of knowledge, thus providing a conceptual framework for later development of experimental methodology in the natural and social sciences.

In the 18th century the philosopher Jean-Jacques Rousseau espoused a set of theories which would become highly influential in the field of education, particularly through his philosophical novel "Emile, or On Education". Despite stating that the book should not be used as a practical guide to nurturing children, the pedagogical approach outlined in it was lauded by Enlightenment contemporaries including Immanuel Kant and Johann Wolfgang von Goethe. Rousseau advocated a child-centered approach to education, and that the age of the child should be accounted for in choosing what and how to teach them. In particular he insisted on the primacy of experiential education, in order to develop the child's ability to reason autonomously. Rousseau's philosophy influenced educational reformers including Johann Bernhard Basedow, whose practice in his model school the Philanthropinum drew upon his ideas, as well as Johann Heinrich Pestalozzi. More generally Rousseau's thinking had significant direct and indirect influence on the development of pedagogy in Germany, Switzerland and the Netherlands. In addition, Jean Piaget's stage-based approach to child development has been observed to have parallels to Rousseau's theories.

Philosophers of education such as Juan Vives, Johann Pestalozzi, Friedrich Fröbel, and Johann Herbart had examined, classified and judged the methods of education centuries before the beginnings of psychology in the late 1800s.

Juan Vives (1493–1540) proposed induction as the method of study and believed in the direct observation and investigation of the study of nature. His studies focused on humanistic learning, which opposed scholasticism and was influenced by a variety of sources including philosophy, psychology, politics, religion, and history. He was one of the first prominent thinkers to emphasize that the location of a school is important to learning. He suggested that a school should be located away from disturbing noises; the air quality should be good and there should be plenty of food for the students and teachers. Vives emphasized the importance of understanding individual differences of the students and suggested practice as an important tool for learning.

Vives introduced his educational ideas in his writing, "De anima et vita" in 1538. In this publication, Vives explores moral philosophy as a setting for his educational ideals; with this, he explains that the different parts of the soul (similar to that of Aristotle's ideas) are each responsible for different operations, which function distinctively. The first book covers the different "souls": "The Vegetative Soul"; this is the soul of nutrition, growth, and reproduction, "The Sensitive Soul", which involves the five external senses; "The Cogitative soul", which includes internal senses and cognitive facilities. The second book involves functions of the rational soul: mind, will, and memory. Lastly, the third book explains the analysis of emotions.

Johann Pestalozzi (1746–1827), a Swiss educational reformer, emphasized the child rather than the content of the school. Pestalozzi fostered an educational reform backed by the idea that early education was crucial for children, and could be manageable for mothers. Eventually, this experience with early education would lead to a "wholesome person characterized by morality." Pestalozzi has been acknowledged for opening institutions for education, writing books for mother's teaching home education, and elementary books for students, mostly focusing on the kindergarten level. In his later years, he published teaching manuals and methods of teaching.

During the time of The Enlightenment, Pestalozzi's ideals introduced "educationalization". This created the bridge between social issues and education by introducing the idea of social issues to be solved through education. Horlacher describes the most prominent example of this during The Enlightenment to be "improving agricultural production methods."

Johann Herbart (1776–1841) is considered the father of educational psychology. He believed that learning was influenced by interest in the subject and the teacher. He thought that teachers should consider the students' existing mental sets—what they already know—when presenting new information or material. Herbart came up with what are now known as the formal steps. The 5 steps that teachers should use are:


There were three major figures in educational psychology in this period: William James, G. Stanley Hall, and John Dewey. These three men distinguished themselves in general psychology and educational psychology, which overlapped significantly at the end of the 19th century.

The period of 1890–1920 is considered the golden era of educational psychology when aspirations of the new discipline rested on the application of the scientific methods of observation and experimentation to educational problems. From 1840 to 1920 37 million people immigrated to the United States. This created an expansion of elementary schools and secondary schools. The increase in immigration also provided educational psychologists the opportunity to use intelligence testing to screen immigrants at Ellis Island. Darwinism influenced the beliefs of the prominent educational psychologists. Even in the earliest years of the discipline, educational psychologists recognized the limitations of this new approach. The pioneering American psychologist William James commented that:

James is the father of psychology in America, but he also made contributions to educational psychology. In his famous series of lectures "Talks to Teachers on Psychology", published in 1899, James defines education as "the organization of acquired habits of conduct and tendencies to behavior". He states that teachers should "train the pupil to behavior" so that he fits into the social and physical world. Teachers should also realize the importance of habit and instinct. They should present information that is clear and interesting and relate this new information and material to things the student already knows about. He also addresses important issues such as attention, memory, and association of ideas.

Alfred Binet published "Mental Fatigue" in 1898, in which he attempted to apply the experimental method to educational psychology. In this experimental method he advocated for two types of experiments, experiments done in the lab and experiments done in the classroom. In 1904 he was appointed the Minister of Public Education. This is when he began to look for a way to distinguish children with developmental disabilities. Binet strongly supported special education programs because he believed that "abnormality" could be cured. The Binet-Simon test was the first intelligence test and was the first to distinguish between "normal children" and those with developmental disabilities. Binet believed that it was important to study individual differences between age groups and children of the same age. He also believed that it was important for teachers to take into account individual students' strengths and also the needs of the classroom as a whole when teaching and creating a good learning environment. He also believed that it was important to train teachers in observation so that they would be able to see individual differences among children and adjust the curriculum to the students. Binet also emphasized that practice of material was important. In 1916 Lewis Terman revised the Binet-Simon so that the average score was always 100. The test became known as the Stanford-Binet and was one of the most widely used tests of intelligence. Terman, unlike Binet, was interested in using intelligence test to identify gifted children who had high intelligence. In his longitudinal study of gifted children, who became known as the Termites, Terman found that gifted children become gifted adults.

Edward Thorndike (1874–1949) supported the scientific movement in education. He based teaching practices on empirical evidence and measurement. Thorndike developed the theory of instrumental conditioning or the law of effect. The law of effect states that associations are strengthened when it is followed by something pleasing and associations are weakened when followed by something not pleasing. He also found that learning is done a little at a time or in increments, learning is an automatic process and its principles apply to all mammals. Thorndike's research with Robert Woodworth on the theory of transfer found that learning one subject will only influence your ability to learn another subject if the subjects are similar. This discovery led to less emphasis on learning the classics because they found that studying the classics does not contribute to overall general intelligence. Thorndike was one of the first to say that individual differences in cognitive tasks were due to how many stimulus-response patterns a person had rather than general intellectual ability. He contributed word dictionaries that were scientifically based to determine the words and definitions used. The dictionaries were the first to take into consideration the users' maturity level. He also integrated pictures and easier pronunciation guide into each of the definitions. Thorndike contributed arithmetic books based on learning theory. He made all the problems more realistic and relevant to what was being studied, not just to improve the general intelligence. He developed tests that were standardized to measure performance in school-related subjects. His biggest contribution to testing was the CAVD intelligence test which used a multidimensional approach to intelligence and was the first to use a ratio scale. His later work was on programmed instruction, mastery learning, and computer-based learning:

John Dewey (1859–1952) had a major influence on the development of progressive education in the United States. He believed that the classroom should prepare children to be good citizens and facilitate creative intelligence. He pushed for the creation of practical classes that could be applied outside of a school setting. He also thought that education should be student-oriented, not subject-oriented. For Dewey, education was a social experience that helped bring together generations of people. He stated that students learn by doing. He believed in an active mind that was able to be educated through observation, problem-solving, and enquiry. In his 1910 book "How We Think", he emphasizes that material should be provided in a way that is stimulating and interesting to the student since it encourages original thought and problem-solving. He also stated that material should be relative to the student's own experience.

Jean Piaget (1896–1980) was one of the most powerful researchers in of developmental psychology during the 20th century. He developed the theory of cognitive development. The theory stated that intelligence developed in four different stages. The stages are the sensorimotor stage from birth to 2 years old, the preoperational state from 2 to 7 years old, the concrete operational stage from 7 to 10 years old, and the formal operational stage from 12 years old and up. He also believed that learning was constrained to the child's cognitive development. Piaget influenced educational psychology because he was the first to believe that cognitive development was important and something that should be paid attention to in education. Most of the research on Piagetian theory was carried out by American educational psychologists.

The number of people receiving a high school and college education increased dramatically from 1920 to 1960. Because very few jobs were available to teens coming out of eighth grade, there was an increase in high school attendance in the 1930s. The progressive movement in the United States took off at this time and led to the idea of progressive education. John Flanagan, an educational psychologist, developed tests for combat trainees and instructions in combat training. In 1954 the work of Kenneth Clark and his wife on the effects of segregation on black and white children was influential in the Supreme Court case Brown v. Board of Education. From the 1960s to present day, educational psychology has switched from a behaviorist perspective to a more cognitive-based perspective because of the influence and development of cognitive psychology at this time.

Jerome Bruner is notable for integrating Piaget's cognitive approaches into educational psychology. He advocated for discovery learning where teachers create a problem solving environment that allows the student to question, explore and experiment. In his book "The Process of Education" Bruner stated that the structure of the material and the cognitive abilities of the person are important in learning. He emphasized the importance of the subject matter. He also believed that how the subject was structured was important for the student's understanding of the subject and that it was the goal of the teacher to structure the subject in a way that was easy for the student to understand. In the early 1960s, Bruner went to Africa to teach math and science to school children, which influenced his view as schooling as a cultural institution. Bruner was also influential in the development of MACOS, , which was an educational program that combined anthropology and science. The program explored human evolution and social behavior. He also helped with the development of the head start program. He was interested in the influence of culture on education and looked at the impact of poverty on educational development.

Benjamin Bloom (1903–1999) spent over 50 years at the University of Chicago, where he worked in the department of education. He believed that all students can learn. He developed the taxonomy of educational objectives. The objectives were divided into three domains: cognitive, affective, and psychomotor. The cognitive domain deals with how we think. It is divided into categories that are on a continuum from easiest to more complex. The categories are knowledge or recall, comprehension, application, analysis, synthesis, and evaluation. The affective domain deals with emotions and has 5 categories. The categories are receiving phenomenon, responding to that phenomenon, valuing, organization, and internalizing values. The psychomotor domain deals with the development of motor skills, movement, and coordination and has 7 categories that also go from simplest to most complex. The 7 categories of the psychomotor domain are perception, set, guided response, mechanism, complex overt response, adaptation, and origination. The taxonomy provided broad educational objectives that could be used to help expand the curriculum to match the ideas in the taxonomy. The taxonomy is considered to have a greater influence internationally than in the United States. Internationally, the taxonomy is used in every aspect of education from the training of the teachers to the development of testing material. Bloom believed in communicating clear learning goals and promoting an active student. He thought that teachers should provide feedback to the students on their strengths and weaknesses. Bloom also did research on college students and their problem-solving processes. He found that they differ in understanding the basis of the problem and the ideas in the problem. He also found that students differ in process of problem-solving in their approach and attitude toward the problem.

Nathaniel Gage (1917–2008) is an important figure in educational psychology as his research focused on improving teaching and understanding the processes involved in teaching. He edited the book "Handbook of Research on Teaching" (1963), which helped develop early research in teaching and educational psychology. Gage founded the Stanford Center for Research and Development in Teaching, which contributed research on teaching as well as influencing the education of important educational psychologists.

Applied behavior analysis, a research-based science utilizing behavioral principles of operant conditioning, is effective in a range of educational settings. For example, teachers can alter student behavior by systematically rewarding students who follow classroom rules with praise, stars, or tokens exchangeable for sundry items. Despite the demonstrated efficacy of awards in changing behavior, their use in education has been criticized by proponents of self-determination theory, who claim that praise and other rewards undermine intrinsic motivation. There is evidence that tangible rewards decrease intrinsic motivation in specific situations, such as when the student already has a high level of intrinsic motivation to perform the goal behavior. But the results showing detrimental effects are counterbalanced by evidence that, in other situations, such as when rewards are given for attaining a gradually increasing standard of performance, rewards enhance intrinsic motivation. Many effective therapies have been based on the principles of applied behavior analysis, including pivotal response therapy which is used to treat autism spectrum disorders.

Among current educational psychologists, the cognitive perspective is more widely held than the behavioral perspective, perhaps because it admits causally related mental constructs such as traits, beliefs, memories, motivations, and emotions. Cognitive theories claim that memory structures determine how information is perceived, processed, stored, retrieved and forgotten. Among the memory structures theorized by cognitive psychologists are separate but linked visual and verbal systems described by Allan Paivio's dual coding theory. Educational psychologists have used dual coding theory and cognitive load theory to explain how people learn from multimedia presentations.
The spaced learning effect, a cognitive phenomenon strongly supported by psychological research, has broad applicability within education. For example, students have been found to perform better on a test of knowledge about a text passage when a second reading of the passage is delayed rather than immediate (see figure). Educational psychology research has confirmed the applicability to the education of other findings from cognitive psychology, such as the benefits of using mnemonics for immediate and delayed retention of information.

Problem solving, according to prominent cognitive psychologists, is fundamental to learning. It resides as an important research topic in educational psychology. A student is thought to interpret a problem by assigning it to a schema retrieved from long-term memory. A problem students run into while reading is called "activation." This is when the student's representations of the text are present during working memory. This causes the student to read through the material without absorbing the information and being able to retain it. When working memory is absent from the reader's representations of the working memory, they experience something called "deactivation." When deactivation occurs, the student has an understanding of the material and is able to retain information. If deactivation occurs during the first reading, the reader does not need to undergo deactivation in the second reading. The reader will only need to reread to get a "gist" of the text to spark their memory. When the problem is assigned to the wrong schema, the student's attention is subsequently directed away from features of the problem that are inconsistent with the assigned schema. The critical step of finding a mapping between the problem and a pre-existing schema is often cited as supporting the centrality of analogical thinking to problem-solving.

Each person has an individual profile of characteristics, abilities, and challenges that result from predisposition, learning, and development. These manifest as individual differences in intelligence, creativity, cognitive style, motivation, and the capacity to process information, communicate, and relate to others. The most prevalent disabilities found among school age children are attention deficit hyperactivity disorder (ADHD), learning disability, dyslexia, and speech disorder. Less common disabilities include intellectual disability, hearing impairment, cerebral palsy, epilepsy, and blindness.

Although theories of intelligence have been discussed by philosophers since Plato, intelligence testing is an invention of educational psychology and is coincident with the development of that discipline. Continuing debates about the nature of intelligence revolve on whether it can be characterized by a single factor known as general intelligence, multiple factors (e.g., Gardner's theory of multiple intelligences), or whether it can be measured at all. In practice, standardized instruments such as the Stanford-Binet IQ test and the WISC are widely used in economically developed countries to identify children in need of individualized educational treatment. Children classified as gifted are often provided with accelerated or enriched programs. Children with identified deficits may be provided with enhanced education in specific skills such as phonological awareness. In addition to basic abilities, the individual's personality traits are also important, with people higher in conscientiousness and hope attaining superior academic achievements, even after controlling for intelligence and past performance.

Developmental psychology, and especially the psychology of cognitive development, opens a special perspective for educational psychology. This is so because education and the psychology of cognitive development converge on a number of crucial assumptions. First, the psychology of cognitive development defines human cognitive competence at successive phases of development. Education aims to help students acquire knowledge and develop skills that are compatible with their understanding and problem-solving capabilities at different ages. Thus, knowing the students' level on a developmental sequence provides information on the kind and level of knowledge they can assimilate, which, in turn, can be used as a frame for organizing the subject matter to be taught at different school grades. This is the reason why Piaget's theory of cognitive development was so influential for education, especially mathematics and science education. In the same direction, the neo-Piagetian theories of cognitive development suggest that in addition to the concerns above, sequencing of concepts and skills in teaching must take account of the processing and working memory capacities that characterize successive age levels.

Second, the psychology of cognitive development involves understanding how cognitive change takes place and recognizing the factors and processes which enable cognitive competence to develop. Education also capitalizes on cognitive change, because the construction of knowledge presupposes effective teaching methods that would move the student from a lower to a higher level of understanding. Mechanisms such as reflection on actual or mental actions vis-à-vis alternative solutions to problems, tagging new concepts or solutions to symbols that help one recall and mentally manipulate them are just a few examples of how mechanisms of cognitive development may be used to facilitate learning.

Finally, the psychology of cognitive development is concerned with individual differences in the organization of cognitive processes and abilities, in their rate of change, and in their mechanisms of change. The principles underlying intra- and inter-individual differences could be educationally useful, because knowing how students differ in regard to the various dimensions of cognitive development, such as processing and representational capacity, self-understanding and self-regulation, and the various domains of understanding, such as mathematical, scientific, or verbal abilities, would enable the teacher to cater for the needs of the different students so that no one is left behind.

Constructivism is a category of learning theory in which emphasis is placed on the agency and prior "knowing" and experience of the learner, and often on the social and cultural determinants of the learning process. Educational psychologists distinguish individual (or psychological) constructivism, identified with Piaget's theory of cognitive development, from social constructivism. The social constructivist paradigm views the context in which the learning occurs as central to the learning itself. It regards learning as a process of enculturation. People learn by exposure to the culture of practitioners. They observe and practice the behavior of practitioners and 'pick up relevant jargon, imitate behavior, and gradually start to act in accordance with the norms of the practice'. So, a student learns to become a mathematician through exposure to mathematician using tools to solve mathematical problems. So in order to master a particular domain of knowledge it is not enough for students to learn the concepts of the domain. They should be exposed to the use of the concepts in authentic activities by the practitioners of the domain.

A dominant influence on the social constructivist paradigm is Lev Vygotsky's work on sociocultural learning, describing how interactions with adults, more capable peers, and cognitive tools are internalized to form mental constructs. "Zone of Proximal Development" (ZPD) is a term Vygotsky used to characterize an individual's mental development. He believed that tasks individuals can do on their own do not give a complete understanding of their mental development. He originally defined the ZPD as “the distance between the actual developmental level as determined by independent problem solving and the level of potential development as determined through problem solving under adult guidance or in collaboration with more capable peers.” He cited a famous example to make his case. Two children in school who originally can solve problems at an eight-year-old developmental level (that is, typical for children who were age 8) might be at different developmental levels. If each child received assistance from an adult, one was able to perform at a nine-year-old level and one was able to perform at a twelve-year-old level. He said “This difference between twelve and eight, or between nine and eight, is what we call "the zone of proximal development."” He further said that the ZPD “defines those functions that have not yet matured but are in the process of maturation, functions that will mature tomorrow but are currently in an embryonic state.” The zone is bracketed by the learner's current ability and the ability they can achieve with the aid of an instructor of some capacity.

Vygotsky viewed the ZPD as a better way to explain the relation between children's learning and cognitive development. Prior to the ZPD, the relation between learning and development could be boiled down to the following three major positions: 1) Development always precedes learning (e.g., constructivism): children first need to meet a particular maturation level before learning can occur; 2) Learning and development cannot be separated, but instead occur simultaneously (e.g., behaviorism): essentially, learning is development; and 3) learning and development are separate, but interactive processes (e.g., gestaltism): one process always prepares the other process, and vice versa. Vygotsky rejected these three major theories because he believed that learning should always precede development in the ZPD. According to Vygotsky, through the assistance of a more knowledgeable other, a child can learn skills or aspects of a skill that go beyond the child's actual developmental or maturational level. The lower limit of ZPD is the level of skill reached by the child working independently (also referred to as the child's developmental level). The upper limit is the level of potential skill that the child can reach with the assistance of a more capable instructor. In this sense, the ZPD provides a prospective view of cognitive development, as opposed to a retrospective view that characterizes development in terms of a child's independent capabilities. The advancement through and attainment of the upper limit of the ZPD is limited by the instructional and scaffolding-related capabilities of the more knowledgeable other (MKO). The MKO is typically assumed to be an older, more experienced teacher or parent, but often can be a learner's peer or someone their junior. The MKO need not even be a person, it can be a machine or book, or other source of visual and/or audio input.

Elaborating on Vygotsky's theory, Jerome Bruner and other educational psychologists developed the important concept of instructional scaffolding, in which the social or information environment offers supports for learning that are gradually withdrawn as they become internalized.

Jean Piaget was interested in how an organism adapts to its environment. Piaget hypothesized that infants are born with a schema operating at birth that he called "reflexes". Piaget identified four stages in cognitive development. The four stages are sensorimotor stage, pre-operational stage, concrete operational stage, and formal operational stage.

To understand the characteristics of learners in childhood, adolescence, adulthood, and old age, educational psychology develops and applies theories of human development. Often represented as stages through which people pass as they mature, developmental theories describe changes in mental abilities (cognition), social roles, moral reasoning, and beliefs about the nature of knowledge.

For example, educational psychologists have conducted research on the instructional applicability of Jean Piaget's theory of development, according to which children mature through four stages of cognitive capability. Piaget hypothesized that children are not capable of abstract logical thought until they are older than about 11 years, and therefore younger children need to be taught using concrete objects and examples. Researchers have found that transitions, such as from concrete to abstract logical thought, do not occur at the same time in all domains. A child may be able to think abstractly about mathematics but remain limited to concrete thought when reasoning about human relationships. Perhaps Piaget's most enduring contribution is his insight that people actively construct their understanding through a self-regulatory process.

Piaget proposed a developmental theory of moral reasoning in which children progress from a naïve understanding of morality based on behavior and outcomes to a more advanced understanding based on intentions. Piaget's views of moral development were elaborated by Lawrence Kohlberg into a stage theory of moral development. There is evidence that the moral reasoning described in stage theories is not sufficient to account for moral behavior. For example, other factors such as modeling (as described by the social cognitive theory of morality) are required to explain bullying.

Rudolf Steiner's model of child development interrelates physical, emotional, cognitive, and moral development in developmental stages similar to those later described by Piaget.

Developmental theories are sometimes presented not as shifts between qualitatively different stages, but as gradual increments on separate dimensions. Development of epistemological beliefs (beliefs about knowledge) have been described in terms of gradual changes in people's belief in: certainty and permanence of knowledge, fixedness of ability, and credibility of authorities such as teachers and experts. People develop more sophisticated beliefs about knowledge as they gain in education and maturity.

Motivation is an internal state that activates, guides and sustains behavior. Motivation can have several impacting effects on how students learn and how they behave towards subject matter:

Educational psychology research on motivation is concerned with the volition or will that students bring to a task, their level of interest and intrinsic motivation, the personally held goals that guide their behavior, and their belief about the causes of their success or failure. As intrinsic motivation deals with activities that act as their own rewards, extrinsic motivation deals with motivations that are brought on by consequences or punishments. A form of attribution theory developed by Bernard Weiner describes how students' beliefs about the causes of academic success or failure affect their emotions and motivations. For example, when students attribute failure to lack of ability, and ability is perceived as uncontrollable, they experience the emotions of shame and embarrassment and consequently decrease effort and show poorer performance. In contrast, when students attribute failure to lack of effort, and effort is perceived as controllable, they experience the emotion of guilt and consequently increase effort and show improved performance.

The self-determination theory (SDT) was developed by psychologists Edward Deci and Richard Ryan. SDT focuses on the importance of in driving human behavior and posits inherent growth and development tendencies. It emphasizes the degree to which an individual's behavior is self-motivated and self-determined. When applied to the realm of education, the self-determination theory is concerned primarily with promoting in students an interest in learning, a value of education, and a confidence in their own capacities and attributes.

Motivational theories also explain how learners' goals affect the way they engage with academic tasks. Those who have "mastery goals" strive to increase their ability and knowledge. Those who have "performance approach goals" strive for high grades and seek opportunities to demonstrate their abilities. Those who have "performance avoidance" goals are driven by fear of failure and avoid situations where their abilities are exposed. Research has found that mastery goals are associated with many positive outcomes such as persistence in the face of failure, preference for challenging tasks, creativity, and intrinsic motivation. Performance avoidance goals are associated with negative outcomes such as poor concentration while studying, disorganized studying, less self-regulation, shallow information processing, and test anxiety. Performance approach goals are associated with positive outcomes, and some negative outcomes such as an unwillingness to seek help and shallow information processing.

Locus of control is a salient factor in the successful academic performance of students. During the 1970s and '80s, Cassandra B. Whyte did significant educational research studying locus of control as related to the academic achievement of students pursuing higher education coursework. Much of her educational research and publications focused upon the theories of Julian B. Rotter in regard to the importance of internal control and successful academic performance. Whyte reported that individuals who perceive and believe that their hard work may lead to more successful academic outcomes, instead of depending on luck or fate, persist and achieve academically at a higher level. Therefore, it is important to provide education and counseling in this regard.

Instructional design, the systematic design of materials, activities, and interactive environments for learning, is broadly informed by educational psychology theories and research. For example, in defining learning goals or objectives, instructional designers often use a taxonomy of educational objectives created by Benjamin Bloom and colleagues. Bloom also researched mastery learning, an instructional strategy in which learners only advance to a new learning objective after they have mastered its prerequisite objectives. Bloom discovered that a combination of mastery learning with one-to-one tutoring is highly effective, producing learning outcomes far exceeding those normally achieved in classroom instruction. Gagné, another psychologist, had earlier developed an influential method of task analysis in which a terminal learning goal is expanded into a hierarchy of learning objectives connected by prerequisite relationships.
The following list of technological resources incorporate computer-aided instruction and intelligence for educational psychologists and their students:

Technology is essential to the field of educational psychology, not only for the psychologist themselves as far as testing, organization, and resources, but also for students. Educational psychologists who reside in the K-12 setting focus most of their time on special education students. It has been found that students with disabilities learning through technology such as iPad applications and videos are more engaged and motivated to learn in the classroom setting. Liu et al. explain that learning-based technology allows for students to be more focused, and learning is more efficient with learning technologies. The authors explain that learning technology also allows for students with social-emotional disabilities to participate in distance learning.

Research on classroom management and pedagogy is conducted to guide teaching practice and form a foundation for teacher education programs. The goals of classroom management are to create an environment conducive to learning and to develop students' self-management skills. More specifically, classroom management strives to create positive teacher-student and peer relationships, manage student groups to sustain on-task behavior, and use counseling and other psychological methods to aid students who present persistent psychosocial problems.

Introductory educational psychology is a commonly required area of study in most North American teacher education programs. When taught in that context, its content varies, but it typically emphasizes learning theories (especially cognitively oriented ones), issues about motivation, assessment of students' learning, and classroom management. A developing gives more detail about the educational psychology topics that are typically presented in preservice teacher education.

In order to become an educational psychologist, students can complete an undergraduate degree of their choice. They then must go to graduate school to study education psychology, counseling psychology, or school counseling. Most students today are also receiving their doctoral degrees in order to hold the "psychologist" title. Educational psychologists work in a variety of settings. Some work in university settings where they carry out research on the cognitive and social processes of human development, learning and education. Educational psychologists may also work as consultants in designing and creating educational materials, classroom programs and online courses. Educational psychologists who work in K–12 school settings (closely related are school psychologists in the US and Canada) are trained at the master's and doctoral levels. In addition to conducting assessments, school psychologists provide services such as academic and behavioral intervention, counseling, teacher consultation, and crisis intervention. However, school psychologists are generally more individual-oriented towards students.

Many high schools and colleges are increasingly offering educational psychology courses, with some colleges offering it as a general education requirement. Similarly, colleges offer students opportunities to obtain a Ph.D. in educational psychology.

Within the UK, students must hold a degree that is accredited by the British Psychological Society (either undergraduate or at the master's level) before applying for a three-year doctoral course that involves further education, placement, and a research thesis.

In recent years, many university training programs in the US have included curriculum that focuses on issues of race, gender, disability, trauma, and poverty, and how those issues affect learning and academic outcomes. A growing number of universities offer specialized certificates that allow professionals to work and study in these fields (i.e. autism specialists, trauma specialists).

Anticipated to grow by 18–26%, employment for psychologists in the United States is expected to grow faster than most occupations in 2014. One in four psychologists is employed in educational settings. In the United States, the median salary for psychologists in primary and secondary schools is US$58,360 as of May 2004.

In recent decades, the participation of women as professional researchers in North American educational psychology has risen dramatically.

As opposed to some other fields of educational research, quantitative methods are the predominant mode of inquiry in educational psychology, but qualitative and mixed-methods studies are also common. Educational psychology, as much as any other field of psychology relies on a balance of observational, correlational, and experimental study designs. Given the complexities of modeling dependent data and psychological variables in school settings, educational psychologists have been at the forefront of the development of several common statistical tools, including psychometric methods, meta-analysis, regression discontinuity and latent variable modeling.




EFTPOS

Electronic funds transfer at point of sale (EFTPOS; ) is an electronic payment system involving electronic funds transfers based on the use of payment cards, such as debit cards or credit cards, at payment terminals located at points of sale. EFTPOS technology was developed during the 1980s. 

In Australia and New Zealand, it is also the brand name of a specific system used for such payments; these systems are country-specific and do not interconnect. Other countries use different brand names for their EFTPOS systems, such as NETS in Singapore. Since the early 2010s, country specific EFTPOS systems have been overtaken by global EMV based systems with contactless payments or QR code payment systems.

The payment cards used by EFTPOS systems are plastic cards complying with ISO/IEC 7810 ID-1 standard that have a bank card number conforming with the ISO/IEC 7812 numbering standard.

EFTPOS technology originated in the United States in 1981 and was rolled out in 1982. Initially, a number of nationwide systems were set up, such as "Interlink", which were limited to participating correspondent banking relationships, not being linked to each other. Consumers and merchants were slow to accept it, and there was minimal marketing. As a result, growth and market penetration of EFTPOS was minimal in the US up to the turn of the century.

In a short time, other countries adopted the EFTPOS technology, these systems were limited to the national borders. Each country adopted various interbank co-operative models. In Australia, in 1984 Westpac was the first major Australian bank to implement an EFTPOS system, at BP petrol stations. The other major banks implemented EFTPOS systems during 1984, initially with petrol stations. The banks' existing debit and credit cards (but only allowed to access debit accounts) were used in the EFTPOS systems. In 1985, the State Bank of Victoria developed the capacity to host connect individual ATMS and helped create the ATM (Financial) Network. Banks started to link their EFTPOS systems to provide access for all customers across all EFTPOS devices. Cards issued by all banks could then be used at all EFTPOS terminals nationally, but debit cards issued in other countries could not. Prior to 1986, the Australian banks organised a widespread uniform credit card, called Bankcard, which had been in existence since 1974. There was a dispute between the banks whether Bankcard (or credit cards in general) should be permitted into the proposed EFTPOS system. At that time several banks were actively promoting MasterCard and Visa credit cards. Store cards and proprietary cards were shut out of the new system.

In New Zealand, a trial scheme of EFTPOS began in 1984, with a terminal in a Shell petrol station connected to a bank computer. The Bank of New Zealand started issuing EFTPOS debit cards in 1985, with the first merchant terminals being installed in petrol stations.

In 1996, mobile EFTPOS arrived, with hotels in Singapore installing systems in 1997 and the first example of a pizza delivery in Singapore accepting Visa card via cellular payment in 1998, which was a collaboration between Singnet, Visa, Citibank, and Dynamic Data Systems, beginning the rollout of mobile systems in Asia. By 2004, Cellular based Eftpos infrastructure had really taken off, and by 2010, Cellular Eftpos had become the standard for the global market.

Since 2002, the use of EFTPOS has grown significantly, and it has become the standard payment method, displacing the use of cash. Subsequently, networks facilitating the process of money transfer and payment settlement between the consumer and the merchant grew from a small number of nationwide systems to the majority of payment processing transactions. For EFTPOS, USA based systems allow the use of debit cards or credit cards.

In Australia, debit and credit cards are the most common non-cash payment methods at “points of sale” (POS) or via ATMs. Not all merchants provide EFTPOS facilities, but those who wish to accept EFTPOS payments must enter an agreement with one of the many (originally seven) merchant service providers, which rent an EFTPOS terminal to the merchant. The EFTPOS system in Australia is managed by Eftpos Payments Australia Ltd, which also sets the EFTPOS interchange fee. For credit cards to be accepted by a merchant a separate agreement must be entered into with each credit card company, each of which has its own flexible merchant fee rate. Eftpos machines for merchants are provided by larger banks and specialists such as Live eftpos.

The clearing arrangements for EFTPOS are managed by Australian Payments Clearing Association (APCA). The system for ATM and EFTPOS interchanges is called Issuers and Acquirers Community (formerly Consumer Electronic Clearing System; CECS) also called CS3. CECS required authorisations from the Australian Competition & Consumer Commission (ACCC), which was obtained in 2001 and reaffirmed in 2009. ATM and EFTPOS clearances are the made under individual bilateral arrangements between the institutions involved.

Australian financial institutions provide their customers with a plastic card, which can be used as a debit card or as an ATM card, and sometimes as a credit card. The card merely provides the means by which a customer's linked bank or other accounts can be accessed using an EFTPOS terminal or ATM. These cards can also be used on some vending machines and other automatic payment mechanisms, such as ticket vending machines.

Each Australian bank has given a different name to its debit cards, such as:

Some banks offer alternative debit card facilities to their customers using the Visa or MasterCard clearance system. For example, St George Bank offers a Visa Debit Card, as does the National Australia Bank. The main difference with regular debit cards is that these cards can be used outside Australia where the respective credit card is accepted.

Those merchants that enter the EFTPOS payment system must accept debit cards issued by any Australian bank, and some also accept various credit cards and other cards. Some merchants set minimum transaction amounts for EFTPOS transactions, which can be different for debit and credit card transactions. Some merchants impose a surcharge on the use of EFTPOS. These can vary between merchants and on the type of card being used, and generally are not imposed on debit card transactions, and widely not on MasterCard and Visa credit card transactions.

A feature of a debit card is that an EFTPOS transaction will only be accepted if there is an available credit balance in the bank cheque or savings account linked to the card.

Australian debit cards normally cannot be used outside Australia. They can only be used outside Australia if they carry the MasterCard/Maestro/Cirrus or Visa/Plus or other similar logos, in which case the non-Australian transaction will be processed through those transaction systems. Similarly, non-Australian debit and credit cards can only be used at Australian EFTPOS terminals or ATMs if they have these logos or the MasterCard or Visa logos. Diners Club and/or American Express cards will be accepted only if the merchant has an agreement with those card companies, or increasingly if the merchant has modern alternative payment options available for those cards, such as through PayPal. The Discover Card is accepted in Australia as a Diners Club card.

In addition, credit card companies issue prepaid cards which act like generic gift cards, which are anonymous and not linked to any bank accounts. These cards are accepted by merchants who accept credit cards and are processed through the EFTPOS terminal in the same way as credit cards.

A number of merchants permit customers using a debit card to withdraw cash as part of the EFTPOS transaction. In Australia, this facility (known as debit card cashback in many other countries) is known as "cash out". For the merchant, cash out is a way of reducing their net cash takings, saving on banking of cash. There is no additional cost to the merchant in providing cash out because banks charge a merchant a debit card transaction fee per EFTPOS transaction, and not on the transaction value. Cash out is a facility provided by the merchant, and not the bank, so the merchant can limit or vary how much cash can be withdrawn at a time, or suspend the facility at any time. When available, cash out is convenient for the customer, who can bypass having to visit a bank branch or ATM. Cash out is also cheaper for the customer, since only one bank transaction is involved. For people in some remote areas, cash out may be the only way they can withdraw cash from their personal accounts. However, most merchants who provide the facility set a relatively low limit on cash out, generally $50, and some also charge for the service. Some merchants in Australia only allow cash out with the purchase of goods; other merchants allow cash out whether or not customers buy any goods. Cash out is not available in association with credit card sales because on credit card transactions the merchant is charged a percentage commission based on the transaction value, and also because cash withdrawals are treated differently from purchase transactions by the credit card company. (However, though inconsistent with a merchant's agreement with each credit card company, the merchant may treat a cash withdrawal as part of an ordinary credit card sale.)

EFTPOS transactions involving a debit, credit or prepaid card are primarily authenticated via the entry of a personal identification number (PIN) at the point of sale. Historically, these transactions were authenticated by the merchant using the cardholder's signature, as signed on their receipt. However, merchants had become increasingly lax in enforcing this verification, resulting in an increase in fraud. Australian banks have since deployed chip and PIN technology using the global EMV card standard; as of 1 August 2014, Australian merchants no longer accept signatures on transactions by domestic customers at point of sale terminals.

As a further security measure, if a user enters an incorrect PIN three times, the card may be locked out of EFTPOS and require reactivation over the phone or at a bank branch. In the case of an ATM, the card will not be returned, and the cardholder will need to visit the branch to retrieve the card, or request a new card to be issued.

All debit cards now have a magnetic stripe on which is encoded the card's service codes, consisting of three-digit values. These codes are used to convey instructions to merchant terminals on how a card should be processed. The first digit indicates if a card can be used internationally or is valid for domestic use only. It is also used to signal if the card is chip-enabled. The second digit indicates if the transaction must be sent online for authorisation always or if transactions that are below floor limit can take place without authorisation. The third digit is used to indicate the preferred card verification method (e.g., PIN) and the environment where the card can be used (e.g., at point of sale only). Merchant terminals are required to recognise and act on service codes or send all transactions for online authorisation.

In the late 2000s, MasterCard and Visa introduced contactless smart debit cards under the brand names MasterCard PayPass and Visa payWave. These payments are made using either electronic payment networks separate from the regular EFTPOS payment networks, or newer EFTPOS with tap sensors, and is an alternative to the previous swipe or chip systems. These networks are operated by MasterCard and Visa, and not by the banks as is the EFTPOS network, through EFTPOS Payments Australia Limited (ePAL).

These cards are based on EMV technology and contain a RFID chip and antenna loop embedded in the plastic of the card. To pay using this system, a customer passes the card within 4 cm of a reader at a merchant checkout. Using this method, for transactions under a specified limit, the customer does not need to authenticate their identity by PIN entry or signature, as on a regular EFTPOS machine. For transactions over the above limit, PIN verification is required.

The facility is only available for cards branded with the MasterCard PayPass or Visa payWave logos, indicating that they have the system-permitted embedded chip. ANZ launched an ATM solution based on Visa payWave in 2015, where the customer taps the card on a reader installed at the ATM and inserts their PIN to finalise cash withdrawals. Since 2018, these ATMs work with Apple Pay and Google Pay as well, where a customer taps their NFC-enabled phone instead of their card. Bank debit cards and other credit cards do not currently offer a contactless payment facility. ePAL is developing a contactless payment system for debit cards based on EMV technology as well as an extension of debit cards for use for on-line transactions, and a mobile payment system. Using contactless debit cards on tap-and-go terminals routes the transaction through the more expensive credit card system instead of the EFTPOS route, adding to the cost to the merchant, and ultimately the consumer.

The name and logo for EFTPOS in Australia were originally owned by Shiyombo Makasa and were trade marks from 1986 until 1991. The ownership was for convenience and all the banks used the name and logo (commonly called "fat-E") on their cards and advertising.

In 1991, dialup EFTPOS was conceived by Key Corp (John Wood) and deployment of dialup commenced in 1993. Until 1993, communications, connections and transactions between banks, ATM banks and EFTPOS devices where conducted via leased lines (a specific power assisted communication line that detects any attempt to tamper with it) but in 1993, mobile wireless EFTPOS was conceived by Dynamic Data Systems (H. Daniel Elbaum). In 1995, Dynamic Data Systems and the banking industry worked together to implement, certify and introduce protocols and standards for cellular networks, and by 1998, the use of mobile EFTPOS began to appear in Australia.

In 2006, Commonwealth Bank and MasterCard ran a six-month trial of the contactless smart card system PayPass in Sydney and Wollongong, 
supplementing the traditional EFTPOS swipe or chip system. The system was rolled out across Australia in 2009; other systems being rolled out are Westpac Bank's MasterCard PayPass and Visa payWave branded cards.

In April 2009, a company, “EFTPOS Payments Australia Ltd” (ePal) was formed to manage and promote the EFTPOS system in Australia. ePal regulation commenced in January 2011. The initial members of EFTPOS Payments Australia Ltd were:


In Australia, store cards have been excluded from participation in the EFTPOS and ATM systems. Consequently, several larger store accounts have entered into co-branding arrangements with credit card networks for the store-based accounts to be widely accepted. This was the case with Coles (previously, Coles-Myer) which co-branded with MasterCard, Myer which co-branded with Visa, and David Jones which co-branded with American Express. Woolworths organised its credit card called Everyday Rewards (now Woolworths Money) which initially was partnered with credit provider HSBC Bank, but changed on 26 October 2014 to Macquarie Bank.

As of June 2018, there were 961,247 EFTPOS terminals in Australia and 30,940 ATMs. Of the terminals, over 60,000 offered cash withdrawals. In 2010, 183 million transactions, worth A$12 billion, were made using Australian EFTPOS terminals per month.

In 2011, these figures increased to 750,000 terminals, with 325,000 individual businesses, processing over 2 billion transactions with combined value of approximately $131 billion for the year.

The EFT network in Australia is made up of seven proprietary networks in which peers have interchange agreements, making an effective single network. A merchant who wishes to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent the terminal to the merchant. All the merchant's EFTPOS transactions are processed through one of these gateways. Some of these peers are:

Other organisations may have peering agreements with the one or more of the central peers.

The network uses the AS 2805 protocol, which is closely related to ISO 8583.

EFTPOS is highly popular in New Zealand. The system is operated by two providers, Paymark Limited (formerly Electronic Transaction Services Limited) which processes 75% of all electronic transactions in New Zealand, and EFTPOS New Zealand. Although the term eftpos is popularly used to describe the system, EFTPOS is a trademark of EFTPOS New Zealand, the smaller of the two providers. Both providers run an interconnected financial network that allows the processing of not only of debit cards at point of sale terminals but also credit cards and charge cards.

The Bank of New Zealand introduced EFTPOS to New Zealand in 1984 through a pilot scheme with petrol stations.

In 1989 the system was officially launched and two providers owned by the major banks now run the system. The larger of the two providers, Paymark Limited (formerly Electronic Transaction Services Limited), is owned by French company Ingenico, following its sale in 2018 by ASB Bank, Westpac, Bank of New Zealand and ANZ Bank New Zealand (formerly ANZ National Bank). The second is operated by EFTPOS New Zealand, which is fully owned by VeriFone Systems, following its sale by ANZ New Zealand in December 2012.

1995 was the first deployment of cellular Eftpos in NZ, by Dynamic Data Systems.

During July 2006 the five billionth EFTPOS payment was processed, and at the start of 2012 the 10 billionth transaction was processed.

EFTPOS is highly popular in New Zealand, and being used for about 60% of all retail transactions. In 2009, there were 200 EFTPOS transactions per person.

Paymark process over 900 million transactions (worth over NZ$48 billion) yearly. More than 75,000 merchants and over 110,000 EFTPOS terminals are connected to Paymark.

In Singapore, NETS was founded in 1985 by a consortium of the country's local banks of DBS Bank, OCBC Bank and United Overseas Bank (UOB) to establish the debit network and drive the adoption of electronic payments in Singapore.

NETS was officially launched on 18 January 1986, allowing millions of ATM card holders in Singapore to make transactions through the initial network of 195 terminals located in various retail outlets and by 1993, consumer spending through NETS reached S$1.14 billion. Since the late-2010s, NETS has also adopted QR code payments through NETS QR, which is also integrated with SGQR.

The nationwide acceptance infrastructure is the largest in Singapore and includes 54,000 Unified Point-of-Sale (Unified POS) terminals and 94,000 QR acceptance points. In 2011, NETS’ debit system was designated as national payment system by the Monetary Authority of Singapore (MAS).


Epistle to the Laodiceans

The Epistle to the Laodiceans is a purported lost letter of Paul the Apostle, the original existence of which is inferred from an instruction in the Epistle to the Colossians that the congregation should send their letter to the believing community in Laodicea, and likewise obtain a copy of the letter "from Laodicea" (, "ek Laodikeas").
This letter is generally regarded as being lost. However, some ancient sources, such as Hippolytus of Rome, and some modern scholars consider that the epistle "from Laodicea" was never a lost epistle, but simply Paul re-using one of his other letters (the most common candidate is the canonical Epistle to the Ephesians), just as he asks for the copying and forwarding of the Letter to Colossians to Laodicea. An additional complication is that many scholars do not believe that "Colossians" was itself written by Paul, in which case the indicated Letter might itself not be Pauline even if it existed.

At least two ancient texts purporting to be the missing "Epistle to the Laodiceans" are known to have existed. These are generally considered, both in antiquity and by modern scholarship, to be attempts to supply a forged copy of a lost document. The sole version that survived is a Latin ("Epistle to the Laodiceans"), first witnessed in Codex Fuldensis. The Latin epistle is actually a short compilation of verses from other Pauline epistles, principally Philippians. It too is generally considered a "clumsy forgery" and an attempt to fill the "gap" suggested by Colossians 4:16.

Paul, the earliest known Christian author, wrote several letters (or epistles) in Greek to various churches. Paul apparently dictated all his epistles through a secretary (or amanuensis), but wrote the final few paragraphs of each letter by his own hand. Many survived and are included in the New Testament, but others are known to have been lost. The Epistle to the Colossians, which is traditionally attributed to Paul, includes a seeming reference to a presumably Pauline letter in the possession of the church at Laodicea. An interlinear gloss of Colossians 4:16 reads as follows:
The last words can be interpreted as "letter written to the Laodiceans", but also "letter written from Laodicea". The New American Standard Bible (NASB) translates this verse in the latter manner, and a few translations in other languages also translate it likewise, such as the Dutch Statenvertaling: "When this letter is read among you, have it also read in the church of the Laodiceans; and you, for your part read my letter (that is coming) from Laodicea." Those who read here "letter written to the Laodiceans" presume that, at the time that the Epistle to the Colossians was written, Paul also had written an epistle to the community of believers in Laodicea.

Another possibility exists: that no such epistle to the Laodiceans was ever created, despite the verse in Colossians. Colossians is considered a deutero-Pauline work by many scholars (meaning a letter written in Paul's name by an associate or someone else), based on a number of differences in writing style and assumed situation from Paul's earlier letters. While this is explained by some as due to increasing use of a secretary (amanuensis) later in Paul's life, a more skeptical approach is to suggest that Colossians was not written by Paul at all. If Colossians was forged in Paul's name, then the reference to the other letter to the Laodiceans could merely be a verisimilitude—a small detail to make the letter seem real. The letter would never have been sent to Colossae in this scenario, but rather used as an example of Paul's doctrine to win a theological dispute far from Colossae, and there would be nobody to recognize that the claimed letter to the Laodiceans was non-existent.

Some scholars have suggested that it refers to the canonical Epistle to the Ephesians, contending that it was a circular letter (an "encyclical") to be read to many churches in the Laodicean area. Others dispute this view.

Others have suggested that it refers to the canonical Epistle to Philemon.

According to the Muratorian fragment, Marcion's canon contained an epistle called the Epistle to the Laodiceans which is commonly thought to be a forgery written to conform to his own point of view. This is not at all clear, however, since none of the text survives. It is not known what this letter might have contained. Most scholars believe it was explicitly Marcionist in its outlook, hence its condemnation.

Others believe it to be the Epistle to the Ephesians; the proto-Orthodox author Tertullian accuses Marcion's group of using an edited version of Ephesians which was referred to as the Epistle to the Laodiceans.

A claimed Epistle to the Laodiceans from Paul exists in Latin. It is quite short at only 20 verses. It is mentioned by various writers from the fourth century onwards, notably by Pope Gregory the Great; the oldest known copy of this epistle is in the Fulda manuscript written for Victor of Capua in 546. Possibly due to Gregory's endorsement of it, many Western Latin Bibles contained this epistle for centuries afterward. It also featured in early English Bibles: John Wycliffe included Paul's letter to the Laodiceans in his Bible translation from the Latin to English. Medieval German Bibles included it as well, until it was excluded from the Luther Bible in the 1500s. However, the epistle is essentially unknown in Eastern Christianity, where it was never used or published; the Second Council of Nicea of 787 rejected it. There is no evidence of a Greek text, the language Paul wrote in. The text was almost unanimously considered pseudepigraphal when the Christian Biblical canon was decided upon, and does not appear in any Greek copies of the Bible at all, nor is it known in Syriac or other versions. Jerome, who wrote the Latin Vulgate translation, wrote in the 4th century, "it is rejected by everyone".

Scholars are unanimous in concurring with Jerome and believing this epistle forged long after Paul's death. Additionally, the epistle is derided for having no theological content. It includes Pauline greetings and farewells, but does not appear to have any substantive content: it does not address any problem or advocate for any position. and wrote that the epistle is "nothing other than a worthless patching together of [canonical] Pauline passages and phrases, mainly from the Epistle to the Philippians." M. R. James wrote that "It is not easy to imagine a more feebly constructed cento of Pauline phrases." Wilhelm Schneemelcher was "amazed that it ever found a place in Bible manuscripts." However, it evidently gained a certain degree of respect, having appeared in over 100 surviving early Latin copies of the Bible. According to "Biblia Sacra iuxta vulgatam versionem", there are Latin Vulgate manuscripts containing this epistle dating between the 6th and 12th century, including Latin manuscripts F (Codex Fuldensis), M, Q, B, D (Ardmachanus), C, and Lambda.

The apocryphal epistle is generally considered a transparent attempt to supply this supposed lost sacred document. Some scholars, such as Wolfgang Speyer, suggest that it was created to offset the popularity of the Marcionite epistle; it would be easier to reject the Marcionite version if the "real" Epistle to the Laodiceans could be provided to counter it.

An obvious question is if the Latin epistle and the Marcionite epistle are actually the same document: is it possible that the Muratorian fragment was referring to an early version of the Latin epistle? While the occasional scholar advocates for this (Adolf von Harnack for one), most scholars consider this unlikely, because the Latin epistle does not include any Marcionite theology or character.

Extermination camp

Nazi Germany used six extermination camps (), also called death camps (), or killing centers (), in Central Europe during World War II to systematically murder over 2.7 million peoplemostly Jewsin the Holocaust. The victims of death camps were primarily murdered by gassing, either in permanent installations constructed for this specific purpose, or by means of gas vans. The six extermination camps were Chełmno, Belzec, Sobibor, Treblinka, Majdanek and Auschwitz-Birkenau. Extermination through labour was also used at the Auschwitz and Majdanek death camps. Millions were also murdered in concentration camps, in the Aktion T4, or directly on site.

The idea of mass extermination with the use of stationary facilities, to which the victims were taken by train, was the result of earlier Nazi experimentation with chemically manufactured poison gas during the secretive Aktion T4 euthanasia programme against hospital patients with mental and physical disabilities. The technology was adapted, expanded, and applied in wartime to unsuspecting victims of many ethnic and national groups; the Jews were the primary target, accounting for over 90 percent of extermination camp victims. The genocide of the Jews of Europe was Nazi Germany's "Final Solution to the Jewish question".

After the invasion of Poland in September 1939, the secret Aktion T4 euthanasia programmethe systematic murder of German, Austrian and Polish hospital patients with mental or physical disabilities authorized by Hitlerwas initiated by the "SS" in order to eliminate "life unworthy of life" (), a Nazi designation for people who they considered to have no right to life. In 1941, the experience gained in the secretive killing of these hospital patients led to the creation of extermination camps for the implementation of the Final Solution. By then, the Jews were already confined to new ghettos and interned in Nazi concentration camps along with other targeted groups, including Roma, and the Soviet POWs. The Nazi's so-called "Final Solution of the Jewish Question", based on the systematic murder of Europe's Jews by gassing, began during Operation Reinhard, after the June 1941 onset of the Nazi–Soviet war. The adoption of the gassing technology by Nazi Germany was preceded by a wave of hands-on killings carried out by the SS , who followed the army during Operation Barbarossa on the Eastern Front.

The camps designed specifically for the mass gassings of Jews were established in the months following the Wannsee Conference chaired by Reinhard Heydrich in January 1942 in which the principle was made clear that the Jews of Europe were to be exterminated. Responsibility for the logistics was to be handled by the programme administrator, Adolf Eichmann.

On 13 October 1941, the SS and Police Leader Odilo Globocnik stationed in Lublin received an oral order from Heinrich Himmleranticipating the fall of Moscowto start immediate construction work on the killing centre at Bełżec in the General Government territory of occupied Poland. Notably, the order preceded the Wannsee Conference by three months,<ref name="M/MPwB"></ref> but the gassings at Chełmno north of Łódź using gas vans began already in December, under Herbert Lange. The camp at Bełżec was operational by March 1942, with leadership brought in from Germany under the guise of  (OT). By mid-1942, two more death camps had been built on Polish lands for Operation Reinhard: Sobibór (ready in May 1942) under the command of Franz Stangl, and Treblinka (operational by July 1942) under Irmfried Eberl from T4, the only doctor to have served in such a capacity. Auschwitz concentration camp was fitted with brand new gas chambers in March 1942. Majdanek had them built in September.

The Nazis distinguished between extermination and concentration camps. The terms "extermination camp" () and "death camp" () were interchangeable in the Nazi system, each referring to camps whose primary function was genocide. Six camps meet this definition, though extermination of people happened at every sort of concentration camp or transit camp; the use of the term extermination camp with its exclusive purpose is carried over from Nazi terminology. The six camps were Chełmno, Belzec, Sobibor, Treblinka, Majdanek and Auschwitz (also called Auschwitz-Birkenau).

Death camps were designed specifically for the systematic killing of people delivered en masse by the Holocaust trains. Deportees were normally murdered within a few hours of arrival at Bełżec, Sobibór, and Treblinka. The Reinhard extermination camps were under Globocnik's direct command; each of them was run by 20 to 35 men from the branch of the , augmented by about one hundred Trawnikisauxiliaries mostly from Soviet Ukraine, and up to one thousand slave labourers each. The Jewish men, women and children were delivered from the ghettos for "special treatment" in an atmosphere of terror by uniformed police battalions from both Orpo and Schupo.

Death camps differed from concentration camps located in Germany proper, such as Bergen-Belsen, Oranienburg, Ravensbrück, and Sachsenhausen, which were prison camps set up prior to World War II for people defined as 'undesirable'. From March 1936, all Nazi concentration camps were managed by the (the Skull Units, SS-TV), who operated extermination camps from 1941 as well. An SS anatomist, Johann Kremer, after witnessing the gassing of victims at Birkenau, wrote in his diary on 2 September 1942: "Dante's Inferno seems to me almost a comedy compared to this. They don't call Auschwitz the camp of annihilation for nothing!" The distinction was evident during the Nuremberg trials, when Dieter Wisliceny (a deputy to Adolf Eichmann) was asked to name the camps, and he identified Auschwitz and Majdanek as such. Then, when asked, "How do you classify the camps Mauthausen, Dachau, and Buchenwald?", he replied, "They were normal concentration camps, from the point of view of the department of Eichmann."

Murders were not limited to these camps. Sites for the "Holocaust by Bullets" are marked on the map of The Holocaust in Occupied Poland by white skulls (without the black background), where people were lined up next to a ravine and shot by soldiers with rifles. Sites included Bronna Góra, Ponary, Rumbula and others.

Irrespective of round-ups for extermination camps, the Nazis abducted millions of foreigners for slave labour in other types of camps, which provided perfect cover for the extermination programme. Prisoners represented about a quarter of the total workforce of the Reich, with mortality rates exceeding 75 percent due to starvation, disease, exhaustion, executions, and physical brutality.

In the early years of World War II, the Jews were primarily sent to forced labour camps and ghettoised, but from 1942 onward they were deported to the extermination camps under the guise of "resettlement". For political and logistical reasons, the most infamous Nazi German killing factories were built in occupied Poland, where most of the intended victims lived; Poland had the greatest Jewish population in Nazi-controlled Europe. On top of that, the new death camps outside of Germany's prewar borders could be kept secret from the German civil populace.

During the initial phase of the Final Solution, gas vans producing poisonous exhaust fumes were developed in the occupied Soviet Union (USSR) and at the Chełmno extermination camp in occupied Poland, before being used elsewhere. The killing method was based on experience gained by the SS during the secretive programme of involuntary euthanasia. There were two types of death chambers operating during the Holocaust.

Unlike at Auschwitz, where cyanide-based Zyklon B was used to exterminate trainloads of prisoners under the guise of "relocation", the camps at Treblinka, Bełżec, and Sobibór, built during Operation Reinhard (October 1941November 1943), used lethal exhaust fumes produced by large internal combustion engines. The three killing centres of were constructed predominantly for the extermination of Poland's Jews trapped in the Nazi ghettos. At first, the victims' bodies were buried with the use of crawler excavators, but they were later exhumed and incinerated in open-air pyres to hide the evidence of genocide in what became known as .

The six camps considered to be purely for extermination were Chełmno extermination camp, Bełżec extermination camp, Sobibor extermination camp, Treblinka extermination camp, Majdanek extermination camp and Auschwitz extermination camp (also called Auschwitz-Birkenau).

Whereas the Auschwitz II (Auschwitz–Birkenau) and Majdanek camps were parts of a labor camp complex, the Chełmno and Operation Reinhard death camps (that is, Bełżec, Sobibór, and Treblinka) were built exclusively for the rapid extermination of entire communities of people (primarily Jews) within hours of their arrival. All were constructed near branch lines that linked to the Polish railway system, with staff members transferring between locations. These camps had almost identical design: they were several hundred metres in length and width, and were equipped with only minimal staff housing and support installations not meant for the victims crammed into the railway transports.
The Nazis deceived the victims upon their arrival, telling them that they were at a temporary transit stop, and would soon continue to German (work camps) farther to the east. Selected able-bodied prisoners delivered to the death camps were not immediately killed, but instead were pressed into labor units called to help with the extermination process by removing corpses from the gas chambers and burning them.

At the camps of Operation Reinhard, including Bełżec, Sobibór, and Treblinka, trainloads of prisoners were murdered immediately after arrival in gas chambers designed exclusively for that purpose. The mass killing facilities were developed at about the same time inside the Auschwitz II-Birkenau subcamp of a forced labour complex, and at the Majdanek concentration camp. In most other camps prisoners were selected for slave labor first; they were kept alive on starvation rations and made available to work as required. Auschwitz, Majdanek, and Jasenovac were retrofitted with Zyklon B gas chambers and crematoria buildings as the time went on, remaining operational until war's end in 1945.

Heinrich Himmler visited the outskirts of Minsk in 1941 to witness a mass shooting. He was told by the commanding officer there that the shootings were proving psychologically damaging to those being asked to pull the triggers. Thus, Himmler concluded that another method of mass killing was required. Auschwitz Commandant Rudolf Höss claimed in his memoir that many were "unable to endure wading through blood any longer" and went mad or killed themselves, but he gives no specific numbers to support this claim.

The Nazis had first used gassing with carbon monoxide cylinders to murder 70,000 disabled people in Germany in what they called a 'euthanasia programme' to disguise that mass murder was taking place. Despite the lethal effects of carbon monoxide, this was seen as unsuitable for use in the East due to the cost of transporting the carbon monoxide in cylinders.

Each extermination camp operated differently, yet each had designs for quick and efficient industrialized killing. While Höss was away on an official journey in late August 1941 his deputy, Karl Fritzsch, tested out an idea. At Auschwitz clothes infested with lice were treated with crystallised prussic acid. The crystals were made to order by the IG Farben chemicals company for which the brand name was Zyklon B. Once released from their container, Zyklon B crystals in the air released a lethal cyanide gas. Fritzsch tried out the effect of Zyklon B on Soviet POWs, who were locked up in cells in the basement of the bunker for this experiment. Höss on his return was briefed and impressed with the results and this became the camp strategy for extermination as it was also to be at Majdanek. Besides gassing, the camp guards continued killing prisoners via mass shooting, starvation, torture, etc.

SS Kurt Gerstein of the Institute for Hygiene of the , told a Swedish diplomat during the war, about life in a death camp. He recounted that on 19 August 1942, he arrived at Bełżec extermination camp (which was equipped with carbon monoxide gas chambers) and was shown the unloading of 45 train cars filled with 6,700 Jews, many already dead. The rest were marched naked to the gas chambers, where:

Auschwitz Camp Commandant Rudolf Höss reported that the first time Zyklon B pellets were used on the Jews, many suspected they were to be killeddespite having been deceived into believing they were to be deloused and then returned to the camp. As a result, the Nazis identified and isolated "difficult individuals" who might alert the prisoners, and removed them from the masslest they incite revolt among the deceived majority of prisoners en route to the gas chambers. The "difficult" prisoners were led to a site out of view to be killed off discreetly.

According to Höss, enslaved prisoners, euphemistically called (Special Detachment), assisted in the process of extermination; they encouraged the Jews to undress and accompanied them into the gas chambers which were outfitted to appear as shower rooms (with nonworking water nozzles, and tile walls); and remained with the victims until just before the chamber door closed. To psychologically maintain the "calming effect" of the delousing deception, an SS man stood at the door until the end. The talked to the victims about life in the camp to pacify the suspicious ones, and hurried them inside; to that effect, they also assisted the aged and the very young in undressing. Many young mothers hid their infants beneath their piled clothes fearing that the delousing "disinfectant" might harm them. Camp Commandant Höss reported that the "men of the Special Detachment were particularly on the look-out for this", and encouraged the women to take their children into the "shower room". Likewise, the comforted older children who might cry "because of the strangeness of being undressed in this fashion".

Yet, not every prisoner was deceived by such tactics; Commandant Höss spoke of Jews "who either guessed, or knew, what awaited them, nevertheless ... [they] found the courage to joke with the children, to encourage them, despite the mortal terror visible in their own eyes". Some women would suddenly "give the most terrible shrieks while undressing, or tear their hair, or scream like maniacs"; these prisoners were taken away for execution by shooting. In such circumstances, others, meaning to save themselves at the gas chamber's threshold, betrayed the identities and "revealed the addresses of those members of their race still in hiding".

Once the door of the filled gas chamber was sealed, pellets of Zyklon B were dropped through special holes in the roof. Regulations required that the Camp Commandant supervise the preparations, the gassing (through a peephole), and the aftermath looting of the corpses. Commandant Höss reported that the gassed victims "showed no signs of convulsion"; the Auschwitz camp physicians attributed that to the "paralyzing effect on the lungs" of the Zyklon B gas, which killed "before" the victim began suffering convulsions. The corpses were additionally found half-squatting, their skin discolored pink with red and green spots, with some foaming at the mouth or bleeding from their ears, exacerbated by the crowding in gas chambers.

As a matter of political training, some high-ranked Nazi Party leaders and SS officers were sent to Auschwitz–Birkenau to witness the gassings. As the Auschwitz Camp Commandant Rudolf Höss justified the extermination by explaining the need for "the iron determination with which we must carry out Hitler's orders".

After the gassings, the removed the corpses from the gas chambers, then extracted any gold teeth. Initially, the victims were buried in mass graves, but were later cremated during in all camps of Operation Reinhard.

The was responsible for burning the corpses in the pits, stoking the fires, draining surplus body fat and turning over the "mountain of burning corpses ... so that the draft might fan the flames", wrote Commandant Höss in his memoir while in the Polish custody. He was impressed by the diligence of prisoners from the so-called Special Detachment who carried out their duties despite their being well aware that they, too, would meet exactly the same fate in the end. At the Lazaret killing station they held the sick so they would never see the gun while being shot. They did it "in such a matter-of-course manner that they might, themselves, have been the exterminators", wrote Höss. He further said that the men ate and smoked "even when engaged in the grisly job of burning corpses which had been lying for some time in mass graves." They occasionally encountered the corpse of a relative, or saw them entering the gas chambers. According to Höss, they were obviously shaken by this but "it never led to any incident". He mentioned the case of a who found the body of his wife, yet continued to drag corpses along "as though nothing had happened".

At Auschwitz, the corpses were incinerated in crematoria and the ashes either buried, scattered, or dumped in the river. At Sobibór, Treblinka, Bełżec, and Chełmno, the corpses were incinerated on pyres. The efficiency of industrialised murder at Auschwitz-Birkenau led to the construction of three buildings with crematoria designed by specialists from the firm J. A. Topf & Söhne. They burned bodies 24 hours a day, and yet the death rate was at times so high that corpses also needed to be burned in open-air pits.<ref name="B/G-199"></ref>

The estimated total number of people who were murdered in the six Nazi extermination camps is 2.7 million, according to the United States Holocaust Memorial Museum.
The Nazis attempted to either partially or completely dismantle the extermination camps in order to hide any evidence that people had been murdered there. This was an attempt to conceal not only the extermination process but also the buried remains. As a result of the secretive , the camps were dismantled by commandos of condemned prisoners, their records were destroyed, and the mass graves were dug up. Some extermination camps that remained uncleared of evidence were liberated by Soviet troops, who followed different standards of documentation and openness than the Western allies did.

Nonetheless Majdanek was captured nearly intact due to the rapid advance of the Soviet Red Army during Operation Bagration.

In the post-war period the government of the People's Republic of Poland created monuments at the extermination camp sites. These early monuments mentioned no ethnic, religious, or national particulars of the Nazi victims. The extermination camps sites have been accessible to everyone in recent decades. They are popular destinations for visitors from all over the world, especially the most infamous Nazi death camp, Auschwitz near the town of Oświęcim. In the early 1990s, the Jewish Holocaust organisations debated with the Polish Catholic groups about "What religious symbols of martyrdom are appropriate as memorials in a Nazi death camp such as Auschwitz?" The Jews opposed the placement of Christian memorials such as the Auschwitz cross near Auschwitz I where mostly Poles were killed. The Jewish victims of the Holocaust were mostly killed at Auschwitz II Birkenau.

The March of the Living is organized in Poland annually since 1988. Marchers come from countries as diverse as Estonia, New Zealand, Panama, and Turkey.

Holocaust deniers or negationists are people and organizations who assert that the Holocaust did not occur, or that it did not occur in the historically recognized manner and extent. Holocaust deniers claim that the extermination camps were actually transit camps from which Jews were deported farther east. However, these theories are disproven by surviving German documents, which show that Jews were sent to the camps to be murdered.

Extermination camp research is difficult because of extensive attempts by the SS and Nazi regime to conceal the existence of the extermination camps. The existence of the extermination camps is firmly established by testimonies of camp survivors and Final Solution perpetrators, material evidence (the remaining camps, etc.), Nazi photographs and films of the killings, and camp administration records.

In 2017 a Körber Foundation survey found that 40 percent of 14-year-olds in Germany did not know what Auschwitz was. A 2018 survey organized in the United States by the Claims Conference, United States Holocaust Memorial Museum, and others found that 66 percent of the American millennials who were surveyed (and 41 percent of all U.S. adults) did not know what Auschwitz was. In 2019, a survey of 1,100 Canadians found that 49 percent of them could not name any of the Nazi camps which were located in German-occupied Europe.




Enterprise

Enterprise (or the archaic spelling Enterprize) may refer to:








Australia


United States














Excommunication

Excommunication is an institutional act of religious censure used to deprive, suspend, or limit membership in a religious community or to restrict certain rights within it, in particular those of being in communion with other members of the congregation, and of receiving the sacraments.

It is practiced by all of the ancient churches (such as the Catholic Church, Oriental Orthodox churches and the Eastern Orthodox churches) as well as by other Christian denominations, but it is also used more generally to refer to similar types of institutional religious exclusionary practices and shunning among other religious groups. The Amish have also been known to excommunicate members that were either seen or known for breaking rules, or questioning the church, a practice known as shunning. Jehovah's Witnesses use the term disfellowship to refer to their form of excommunication.

The word "excommunication" means putting a specific individual or group out of communion. In some denominations, excommunication includes spiritual condemnation of the member or group. Excommunication may involve banishment, shunning, and shaming, depending on the group, the offense that caused excommunication, or the rules or norms of the religious community. The grave act is often revoked in response to manifest repentance.

Excommunication among Bahá'ís is rare and generally not used for transgressions of community standards, intellectual dissent, or conversion to other religions. Instead, it is the most severe punishment, reserved for suppressing organized dissent that threatens the unity of believers. "Covenant-breaker" is a term used by Bahá'ís to refer to a person who has been excommunicated from the Bahá'í community for breaking the 'Covenant': actively promoting schism in the religion or otherwise opposing the legitimacy of the chain of succession of leadership.

Currently, the Universal House of Justice has the sole authority to declare a person a Covenant-breaker, and once identified, all Bahá'ís are expected to shun them, even if they are family members. According to 'Abdu'l Baha Covenant-breaking is a contagious disease. The Bahá'í writings forbid association with Covenant-breakers and Bahá'ís are urged to avoid their literature, thus providing an exception to the Bahá'í principle of "independent investigation of truth". Most Bahá'ís are unaware of the small Bahá'í divisions that exist.

The purpose of excommunication is to exclude from the church those members who have behaviors or teachings contrary to the beliefs of a Christian community (heresy). It aims to protect members of the church from abuses and allow the offender to recognize their error and repent.

Within the Catholic Church, there are differences between the discipline of the majority Latin Church regarding excommunication and that of the Eastern Catholic Churches.

Excommunication can be either (automatic, incurred at the moment of committing the offense for which canon law imposes that penalty) or (incurred only when imposed by a legitimate superior or declared as the sentence of an ecclesiastical court).
The Catholic Church teaches in the Council of Trent that "excommunicated persons are not members of the Church, because they have been cut off by her sentence from the number of her children and belong not to her communion until they repent".

In the papal bull (May 16, 1520), Pope Leo X condemned Luther's twenty-third proposition according to which "excommunications are merely external punishments, nor do they deprive a man of the common spiritual prayers of the Church". Pope Pius VI in (August 28, 1794) condemned the notion which maintained that the effect of excommunication is only exterior because of its own nature it excludes only from exterior communion with the Church, as if, said the pope, excommunication were not a spiritual penalty binding in heaven and affecting souls. The excommunicated person, being excluded from the society of the Church, still bears the indelible mark of Baptism and is subject to the jurisdiction of the Church. They are excluded from engaging in certain activities. These activities are listed in Canon 1331 §1, and prohibit the individual from any ministerial participation in celebrating the sacrifice of the Eucharist or any other ceremonies of worship; celebrating or receiving the sacraments; or exercising any ecclesiastical offices, ministries, or functions.
Under current Catholic canon law, excommunicates remain bound by ecclesiastical obligations such as attending Mass, even though they are barred from receiving the Eucharist and from taking an active part in the liturgy (reading, bringing the offerings, etc.). "Excommunicates lose rights, such as the right to the sacraments, but they are still bound to the obligations of the law; their rights are restored when they are reconciled through the remission of the penalty."

These are the only effects for those who have incurred a excommunication. For instance, a priest may not refuse Communion publicly to those who are under an automatic excommunication, as long as it has not been officially declared to have been incurred by them, even if the priest knows that they have incurred it—although if the person's offence was a "manifest grave sin", then the priest is obliged to refuse their communion by canon 915. On the other hand, if the priest knows that excommunication has been imposed on someone or that an automatic excommunication has been declared (and is no longer merely an undeclared automatic excommunication), he is forbidden to administer Holy Communion to that person.

In the Catholic Church, excommunication is normally resolved by a declaration of repentance, profession of the Creed (if the offense involved heresy) and an Act of Faith, or renewal of obedience (if that was a relevant part of the offending act, i.e., an act of schism) by the excommunicated person and the lifting of the censure (absolution) by a priest or bishop empowered to do this. "The absolution can be in the internal (private) forum only, or also in the external (public) forum, depending on whether scandal would be given if a person were privately absolved and yet publicly considered unrepentant."

In the Eastern Catholic Churches, excommunication is imposed only by decree, never incurred automatically by excommunication. A distinction is made between minor and major excommunication. Those on whom minor excommunication has been imposed are excluded from receiving the Eucharist and can also be excluded from participating in the Divine Liturgy. They can even be excluded from entering a church when divine worship is being celebrated there. The decree of excommunication must indicate the precise effect of the excommunication and, if required, its duration.

Those under major excommunication are in addition forbidden to receive not only the Eucharist but also the other sacraments, to administer sacraments or sacramentals, to exercise any ecclesiastical offices, ministries, or functions whatsoever, and any such exercise by them is null and void. They are to be removed from participation in the Divine Liturgy and any public celebrations of divine worship. They are forbidden to make use of any privileges granted to them and cannot be given any dignity, office, ministry, or function in the church, they cannot receive any pension or emoluments associated with these dignities etc., and they are deprived of the right to vote or to be elected.

In the Eastern Orthodox Church, excommunication is the exclusion of a member from the Eucharist. It is not expulsion from the churches. This can happen for such reasons as not having confessed within that year; excommunication can also be imposed as part of a penitential period. It is generally done with the goal of restoring the member to full communion. Before an excommunication of significant duration is imposed, the bishop is usually consulted. The Eastern Orthodox do have a means of expulsion, by pronouncing anathema, but this is reserved only for acts of serious and unrepentant heresy. As an example of this, the Second Council of Constantinople in 553, in its eleventh capitula, declared: "If anyone does not anathematize Arius, Eunomius, Macedonius, Apollinaris, Nestorius, Eutyches and Origen, as well as their impious writings, as also all other heretics already condemned and anathematized by the Holy Catholic and Apostolic Church, and by the aforesaid four Holy Synods and [if anyone does not equally anathematize] all those who have held and hold or who in their impiety persist in holding to the end the same opinion as those heretics just mentioned: let him be anathema."

Although Lutheranism technically has an excommunication process, some denominations and congregations do not use it. In the Smalcald Articles Luther differentiates between the "great" and "small" excommunication. The "small" excommunication is simply barring an individual from the Lord's Supper and "other fellowship in the church". While the "great" excommunication excluded a person from both the church and political communities which he considered to be outside the authority of the church and only for civil leaders. A modern Lutheran practice is laid out in the Lutheran Church–Missouri Synod's 1986 explanation to the Small Catechism, defined beginning at Questions No. 277–284, in "The Office of Keys". 

Many Lutheran denominations operate under the premise that the entire congregation (as opposed to the pastor alone) must take appropriate steps for excommunication, and there are not always precise rules, to the point where individual congregations often set out rules for excommunicating laymen (as opposed to clergy). For example, churches may sometimes require that a vote must be taken at Sunday services; some congregations require that this vote be unanimous.

In the Church of Sweden and the Church of Denmark, excommunicated individuals are turned out from their parish in front of their congregation. They are not forbidden, however, to attend church and participate in other acts of devotion, although they are to sit in a place appointed by the priest (which was at a distance from others).

The Lutheran process, though rarely used, has created unusual situations in recent years due to its somewhat democratic excommunication process. One example was an effort to get serial killer Dennis Rader excommunicated from his denomination (the Evangelical Lutheran Church in America) by individuals who tried to "lobby" Rader's fellow church members into voting for his excommunication.

The Church of England does not have any specific canons regarding how or why a member can be excommunicated, although it has a canon according to which ecclesiastical burial may be refused to someone "declared excommunicate for some grievous and notorious crime and no man to testify to his repentance".

The punishment of imprisonment for being excommunicated from the Church of England was removed from English law in 1963.

Historian Christopher Hill found that, in pre-revolutionary England, excommunication was common but fell into disrepute because it was applied unevenly and could be avoided on payment of fines.

The ECUSA is in the Anglican Communion, and shares many canons with the Church of England which would determine its policy on excommunication.

In the Reformed Churches, excommunication has generally been seen as the culmination of church discipline, which is one of the three marks of the Church. The Westminster Confession of Faith sees it as the third step after "admonition" and "suspension from the sacrament of the Lord's Supper for a season." Yet, John Calvin argues in his "Institutes of the Christian Religion" that church censures do not "consign those who are excommunicated to perpetual ruin and damnation", but are designed to induce repentance, reconciliation and restoration to communion. Calvin notes, "though ecclesiastical discipline does not allow us to be on familiar and intimate terms with excommunicated persons, still we ought to strive by all possible means to bring them to a better mind, and recover them to the fellowship and unity of the Church."

At least one modern Reformed theologian argues that excommunication is not the final step in the disciplinary process. Jay E. Adams argues that in excommunication, the offender is still seen as a brother, but in the final step they become "as the heathen and tax collector" (Matthew 18:17). Adams writes, "Nowhere in the Bible is excommunication (removal from the fellowship of the Lord's Table, according to Adams) equated with what happens in step 5; rather, step 5 is called 'removing from the midst, handing over to Satan,' and the like."

Former Princeton president and theologian, Jonathan Edwards, addresses the notion of excommunication as "removal from the fellowship of the Lord's Table" in his treatise entitled "The Nature and End of Excommunication". Edwards argues:
"Particularly, we are forbidden such a degree of associating ourselves with (excommunicants), as there is in making them our guests at our tables, or in being their guests at their tables; as is manifest in the text, where we are commanded to have no company with them, no not to eat [...] That this respects not eating with them at the Lord's supper, but a common eating, is evident by the words, that the eating here forbidden, is one of the lowest degrees of keeping company, which are forbidden. Keep no company with such a one, saith the apostle, no not to eat – as much as to say, no not in so low a degree as to eat with him. But eating with him at the Lord's supper, is the very highest degree of visible Christian communion. Who can suppose that the apostle meant this: Take heed and have no company with a man, no not so much as in the highest degree of communion that you can have? Besides, the apostle mentions this eating as a way of keeping company which, however, they might hold with the heathen. He tells them, not to keep company with fornicators. Then he informs them, he means not with fornicators of this world, that is, the heathens; but, saith he, 'if any man that is called a brother be a fornicator, etc. with such a one keep no company, no not to eat.' This makes it most apparent, that the apostle doth not mean eating at the Lord's table; for so, they might not keep company with the heathens, any more than with an excommunicated person".

In the Methodist Episcopal Church, individuals were able to be excommunicated following "trial before a jury of his peers, and after having had the privilege of an appeal to a higher court". Nevertheless, an excommunication could be lifted after sufficient penance.

John Wesley, the founder of the Methodist Churches, excommunicated sixty-four members from the Newcastle Methodist society alone for the following reasons:
The Allegheny Wesleyan Methodist Connection, in its 2014 "Discipline", includes "homosexuality, lesbianism, bi-sexuality, bestiality, incest, fornication, adultery, and any attempt to alter one's gender by surgery", as well as remarriage after divorce among its excommunicable offences.

The Evangelical Wesleyan Church, in its 2015 "Discipline", states that "Any member of our church who is accused of neglect of the means of grace or other duties required by the Word of God, the indulgence of sinful tempers, words or actions, the sowing of dissension, or any other violation of the order and discipline of the church, may, after proper labor and admonition, be censured, placed on probation, or expelled by the official board of the circuit of which he is a member. If he request a trial, however, within thirty dates of the final action of the official board, it shall be granted."

Amish communities practice variations of excommunication known as "shunning". This practice may include isolation from community events or the cessation of all communiction.
For Baptists, excommunication is used as a last resort by denominations and churches for members who do not want to repent of beliefs or behavior at odds with the confession of faith of the community. The vote of community members, however, can restore a person who has been excluded.

The Church of Jesus Christ of Latter-day Saints (LDS Church) practices excommunication as a penalty for those who commit serious sins, "i.e.", actions that significantly impair the name or moral influence of the church or pose a threat to other people. In 2020, the church ceased using the term "excommunication" and instead refers to "withdrawal of membership". According to the church leadership "General Handbook", the purposes of withdrawing membership or imposing membership restrictions are, (1) to help protect others; (2) to help a person access the redeeming power of Jesus Christ through repentance; and (3) to protect the integrity of the Church. The origins of LDS disciplinary procedures and excommunications are traced to a revelation Joseph Smith dictated on 9 February 1831, later canonized as Doctrine and Covenants, section 42 and codified in the "General Handbook".

The LDS Church also practices the lesser sanctions of private counsel and caution and informal and formal membership restrictions. (Informal membership restrictions was formerly known as "probation"; formal membership restrictions was formerly known as "disfellowshipment".)

Formal membership restrictions are used for serious sins that do not rise to the level of membership withdrawal. Formal membership restriction denies some privileges but does not include a loss of church membership. Once formal membership restrictions are in place, persons may not take the sacrament or enter church temples, nor may they offer public prayers or sermons. Such persons may continue to attend most church functions and are allowed to wear temple garments, pay tithes and offerings, and participate in church classes if their conduct is orderly. Formal membership restrictions typically lasts for one year, after which one may be reinstated as a member in good standing.

In the more grievous or recalcitrant cases, withdrawal of membership becomes a disciplinary option. Such an action is generally reserved for what are seen as the most serious sins, including committing serious crimes such as murder, child abuse, and incest; committing adultery; involvement in or teaching of polygamy; involvement in homosexual conduct; apostasy; participation in an abortion; teaching false doctrine; or openly criticizing church leaders. The "General Handbook" states that formally joining another church constitutes apostasy and is worthy of membership withdrawal; however, merely attending another church does not constitute apostasy.

A withdrawal of membership can occur only after a formal church membership council. Formerly called a "disciplinary council" or a "church court", the councils were renamed to avoid focusing on guilt and instead to emphasize the availability of repentance.

The decision to withdraw the membership of a Melchizedek priesthood holder is generally the province of the leadership of a stake. In such a disciplinary council, the stake presidency and, sometimes in more difficult cases, the stake high council attend. It is possible to appeal a decision of a stake membership council to the church's First Presidency.

For females and for male members not initiated into the Melchizedek priesthood, a ward membership council is held. In such cases, a bishop determines whether withdrawal of membership or a lesser sanction is warranted. He does this in consultation with his two counselors, with the bishop making the final determination after prayer. The decision of a ward membership council can be appealed to the stake president.

The following list of variables serves as a general set of guidelines for when membership withdrawal or lesser action may be warranted, beginning with those more likely to result in severe sanction:


Notices of withdrawal of membership may be made public, especially in cases of apostasy, where members could be misled. However, the specific reasons for individual withdrawal of membership are typically kept confidential and are seldom made public by church leadership.

Those who have their membership withdrawn lose the right to partake of the sacrament. Such persons are permitted to attend church meetings but participation is limited: they cannot offer public prayers, preach sermons, and cannot enter temples. Such individuals are also prohibited from wearing or purchasing temple garments and from paying tithes. A person whose membership has been withdrawn may be re-baptized after a waiting period of at least one year and sincere repentance, as judged by a series of interviews with church leaders.

Some critics have charged that LDS Church leaders have used the threat of membership withdrawal to silence or punish church members and researchers who disagree with established policy and doctrine, who study or discuss controversial subjects, or who may be involved in disputes with local, stake leaders or general authorities; see, e.g., Brian Evenson, a former BYU professor and writer whose fiction came under criticism from BYU officials and LDS Leadership. Another notable case of excommunication from the LDS Church was the "September Six", a group of intellectuals and professors, five of whom were excommunicated and the sixth disfellowshipped. However, church policy dictates that local leaders are responsible for membership withdrawal, without influence from church headquarters. The church thus argues that this policy is evidence against any systematic persecution of scholars or dissenters. Data shows per-capita excommunication rates among the LDS Church have varied dramatically over the years, from a low of about 1 in 6,400 members in the early 1900s to one in 640 by the 1970s, an increase which has been speculatively attributed to "informal guidance from above" in enforcing the growing list of possible transgressions added to "General Handbook" editions over time.

Jehovah's Witnesses practice a form of excommunication, using the term "disfellowshipping", in cases where a member is believed to have unrepentantly committed one or more of several documented "serious sins".

When a member confesses to, or is accused of, a "serious sin", a "judicial committee" of at least three elders is formed. This committee investigates the case and determines the magnitude of the sin committed. If the person is deemed guilty of a disfellowshipping offense, the committee then decides, on the basis of the person's attitude and "works befitting repentance".

Disfellowshipping is a severing of friendly relationships between all Jehovah's Witnesses and the disfellowshipped person. Interaction with extended family is typically restricted to a minimum, such as presence at the reading of wills and providing essential care for the elderly. Within a household, typical family contact may continue, but without spiritual fellowship such as family Bible study and religious discussions. Parents of disfellowshipped minors living in the family home may continue to attempt to convince the child about the group's teachings. Jehovah's Witnesses believe that this form of discipline encourages the disfellowshipped individual to conform to biblical standards and prevents the person from influencing other members of the congregation.

Along with breaches of the Witnesses' moral code, openly disagreeing with the teachings of Jehovah's Witnesses is considered grounds for shunning. These persons are labeled as "apostates" and are described in Watch Tower Society literature as "mentally diseased". Descriptions of "apostates" appearing in the Witnesses literature have been the subject of investigation in the UK to determine if they violate religious hatred laws. Sociologist Andrew Holden claims many Witnesses who would otherwise defect because of disillusionment with the organization and its teachings, remain affiliated out of fear of being shunned and losing contact with friends and family members. Shunning employs what is known "as relational aggression" in psychological literature. When used by church members and member-spouse parents against excommunicant parents it contains elements of what psychologists call "parental alienation". Extreme shunning may cause trauma to the shunned (and to their dependents) similar to what is studied in the psychology of torture.

Disassociation is a form of shunning where a member expresses verbally or in writing that they do not wish to be associated with Jehovah's Witnesses, rather than for having committed any specific 'sin'. Elders may also decide that an individual has disassociated, without any formal statement by the individual, by actions such as accepting a blood transfusion, or for joining another religious or military organization. Individuals who are deemed by the elders to have disassociated are given no right of appeal.

Each year, congregation elders are instructed to consider meeting with disfellowshipped individuals to determine changed circumstances and encourage them to pursue reinstatement. Reinstatement is not automatic after a certain time period, nor is there a minimum duration; disfellowshipped persons may talk to elders at any time but must apply in writing to be considered for reinstatement into the congregation. Elders consider each case individually, and are instructed to ensure "that sufficient time has passed for the disfellowshipped person to prove that his profession of repentance is genuine". A judicial committee meets with the individual to determine their repentance, and if this is established, the person is reinstated into the congregation and may participate with the congregation in their formal ministry (such as house-to-house preaching).

A Witness who has been formally reproved or reinstated cannot be appointed to any "special privilege of service" for at least one year. Serious sins involving child sex abuse permanently disqualify the sinner from appointment to any congregational "privilege of service", regardless of whether the sinner was convicted of any secular crime.

Similarly to many groups having their origins in the 1830s Restoration Movement, Christadelphians call their form of excommunication "disfellowshipping", though they do not practice "shunning". Disfellowshipping can occur for moral reasons, changing beliefs, or (in some ecclesias) for not attending communion (referred to as "the emblems" or "the breaking of bread").

In such cases, the person involved is usually required to discuss the issues. If they do not conform, the church ('meeting' or 'ecclesia') is recommended by the management committee ("Arranging Brethren") to vote on disfellowshipping the person. These procedures were formulated 1863 onwards by early Christadelphians, and then in 1883 codified by Robert Roberts in "A Guide to the Formation and Conduct of Christadelphian Ecclesias" (colloquially "The Ecclesial Guide"). However Christadelphians justify and apply their practice not only from this document but also from passages such as the exclusion in 1Co.5 and recovery in 2Co.2.

Christadelphians typically avoid the term "excommunication" which many associate with the Catholic Church; and may feel the word carries implications they do not agree with, such as undue condemnation and punishment, as well as failing to recognise the remedial intention of the measure.


In the case of adultery and divorce, the passage of time usually means a member can be restored if he or she wants to be. In the case of ongoing behaviour, cohabitation, homosexual activity, then the terms of the suspension have not been met.

The mechanics of "refellowship" follow the reverse of the original process; the individual makes an application to the "ecclesia", and the "Arranging Brethren" give a recommendation to the members who vote. If the "Arranging Brethren" judge that a vote may divide the ecclesia, or personally upset some members, they may seek to find a third party ecclesia which is willing to "refellowship" the member instead. According to the Ecclesial Guide a third party ecclesia may also take the initiative to "refellowship" another meeting's member. However this cannot be done unilaterally, as this would constitute heteronomy over the autonomy of the original ecclesia's members.

Among many of the Society of Friends groups (Quakers) one is "read out of meeting" for behaviour inconsistent with the sense of the meeting. In Britain a meeting may record a minute of disunity. However it is the responsibility of each meeting, quarterly meeting, and yearly meeting, to act with respect to their own members. For example, during the Vietnam War many Friends were concerned about Friend Richard Nixon's position on war which seemed at odds with their beliefs; however, it was the responsibility of Nixon's own meeting, the East Whittier Meeting of Whittier, California, to act if indeed that meeting felt the leading. They did not.

In the 17th century, before the founding of abolitionist societies, Friends who too forcefully tried to convince their coreligionists of the evils of slavery were read out of meeting. Benjamin Lay was read out of the Philadelphia Yearly Meeting for this. During the American Revolution over 400 Friends were read out of meeting for their military participation or support.

 practices expulsion of members it deems to have gravely sinned or gone against the teachings and doctrines of the church. The Sanggunian, the church's council, has jurisdiction to expel members from the church. People expelled by the church are referred to as "dismissed" (). Offenses that may be grounds for expulsion include marrying a non-member, having a romantic relationship with a non-member, becoming pregnant out of wedlock (unless the couple marries before the child is born) and most especially disagreeing with the church administration. An expelled member can be re-admitted by pledging obedience to the church administration and its rules, values and teachings.

Unitarian Universalism, being a liberal religious group and a congregational denomination, has a wide diversity of opinions and sentiments. Nonetheless, Unitarian Universalists have had to deal with disruptive individuals. Congregations which had no policies on disruptive individuals have sometimes found themselves having to create such policies, up to (and including) expulsion.

By the late 1990s, several churches were using the West Shore UU Church's policy as a model. If someone is threatening, disruptive, or distracting from the appeal of the church to its membership, a church using this model has three recommended levels of response to the offending individual. While the first level involves dialogue between a committee or clergy member and the offender, the second and third levels involve expulsion, either from the church itself or a church activity.

There is no direct equivalent to excommunication in Buddhism. However, in the Theravadan monastic community monks can be expelled from monasteries for heresy or other acts. In addition, monks have four vows, called the four defeats, which are abstaining from sexual intercourse, stealing, and murder, and refraining from lying about spiritual gains (e.g., having special power or ability to perform miracles). If even one is broken, the monk is automatically a layman again and can never become a monk in his or her current life.

Most Japanese Buddhist sects hold ecclesiastical authority over their followers and have their own rules for expelling members of the sangha, lay or bishopric. The lay Japanese Buddhist organization Sōka Gakkai was expelled from the Nichiren Shoshu sect in 1991.

Hinduism is too diverse to be seen as a homogenous and monolithic religion, it is often described an unorganised and syncretist religion with a conspicuous absence of any listed doctrines, there are multiple religious institutions (ecclesia is the Christian equivalent) within Hinduism that teach slight variations of Dharma and Karma, hence Hinduism has no concept of excommunication and hence no Hindu may be ousted from the Hindu religion, although a person may easily lose caste status through gramanya for a very wide variety of infringements of caste prohibitions. This may or may not be recoverable. However, some of the modern organised sects within Hinduism may practice something equivalent to excommunication today, by ousting a person from their own sect.

In medieval and early-modern times (and sometimes even now) in South Asia, excommunication from one's "caste" ("jāti" or "varna") used to be practiced (by the caste-councils) and was often with serious consequences, such as abasement of the person's caste status and even throwing him into the sphere of the untouchables or bhangi. In the 19th century, a Hindu faced excommunication for going abroad, since it was presumed he/she would be forced to break caste restrictions and, as a result, become polluted.

After excommunication, it would depend upon the caste-council whether they would accept any form of repentance (ritual or otherwise) or not. Such current examples of excommunication in Hinduism are often more political or social rather than religious, for example the excommunication of lower castes for refusing to work as scavengers in Tamil Nadu.

Another example of caste-related violence and discrimination occurred in the case of the Gupti Ismailis from the Hindu Kachhiya caste. Interestingly, Hindu members of this caste began prayers with the inclusion of the mantra “OM, by the command, in the name of Allah, the Compassionate, the Merciful” ("om farmānjī bi’smi’l-lāh al-raḥmān al-raḥīm"), but never found it objectionable or Islamic. However, in the early 1930s, after some conflict with caste members due to their profession of allegiance to the Ismaili Imam, this group, known as the Guptis, were excommunicated from the caste completely as they appeared to be breaking caste solidarity. This was also significant for the Gupti community as, for the first time, they could be identified as a distinct group based on their religious persuasion. Some of the more daring Guptis also abandoned their former practice of pious circumspection ("taqiyya") as Hindus, claiming that since they had been excommunicated, the caste no longer had any jurisdiction over their actions.

An earlier example of excommunication in Hinduism is that of Shastri Yagnapurushdas, who voluntarily left and was later expelled from the Vadtal Gadi of the Swaminarayan Sampraday by the then Vadtal acharya in 1906. He went on to form his own institution, "Bochasanwasi Swaminarayan Sanstha" or "BSS" (now BAPS) claiming Gunatitanand Swami was the rightful spiritual successor to Swaminarayan.

"Patit" is a Sikh term which is sometimes translated into English as apostate. It refers to a person who initiated into Sikh religion, but violated the religious rules of Sikhi. The Sikh Rehat Maryada (Code of Conduct), Section Six states the transgressions which cause a person to become a "patit":


These four kurahit causes of apostasy were first listed by Guru Gobind Singh in his 52 hukams (commandments).

Since there has been no universally and univocally recognized religious authority among the many Islamic denominations that have emerged throughout history, papal excommunication has no exact equivalent in Islam, at least insofar as the attitudes of any conflicting religious authorities with regard to an individual or another sect are judged to be coordinate, not subordinate to one another. Nonetheless, condemning heterodoxy and punishing heretics through shunning and ostracism is comparable with the practice in non-Catholic Christian faiths.

Islamic theologians commonly employ two terms when describing measurements to be taken against schismatics and heresy: هَجْر ("hajr", "abandoning") and تَكْفِير ("takfīr", "making or declaring to be a nonbeliever"). The former signifies the act of abandoning somewhere (such as migration, as in the Islamic prophet's journey out of Mecca, which is called al-Hijra ("the (e)migration")) or someone (used in the Qur'an in the case of disciplining a dissonant or disobedient wife or avoiding a harmful person), whereas the latter means a definitive declaration that denounces a person as a kāfir ("infidel"). However, because such a charge would entail serious consequences for the accused, who would then be deemed to be a مُرْتَدّ ("murtadd", "a backslider; an apostate), less extreme denunciations, such as an accusation of بِدْعَة ("bidʽah", "[deviant] innovation; heresy") followed by shunning and excommunication have historically preponderated over apostasy trials.

"Takfīr" has often been practiced through the courts. More recently, cases have taken place where individuals have been considered nonbelievers. These decisions followed lawsuits against individuals, mainly in response to their writings that some have viewed as anti-Islamic. The most famous cases are of Salman Rushdie, Nasr Abu Zayd, and Nawal El-Saadawi and Ahmadiyya Muslim Community. The repercussions of such cases have included divorce, since under traditional interpretations of Islamic law, Muslim women are not permitted to marry non-Muslim men.

"Herem" is the highest ecclesiastical censure in Judaism. It is the total exclusion of a person from the Jewish community. Except for cases in the Charedi community, "cherem" stopped existing after The Enlightenment, when local Jewish communities lost their political autonomy, and Jews were integrated into the gentile nations in which they lived. A "siruv" order, equivalent to a contempt of court, issued by a Rabbinical court may also limit religious participation.

Rabbinical conferences of movements do expel members from time to time, but sometimes choose the lesser penalty of censuring the offending rabbi. Between 2010 and 2015, the Reform Jewish Central Conference of American Rabbis expelled six rabbis, the Orthodox Jewish Rabbinical Council of America expelled three, and the Conservative Jewish Rabbinical Assembly expelled one, suspended three, and caused one to resign without eligibility for reinstatement. While the CCAR and RCA were relatively shy about their reasons for expelling rabbis, the RA was more open about its reasons for kicking rabbis out. Reasons for expulsion from the three conferences include sexual misconduct, failure to comply with ethics investigations, setting up conversion groups without the conference's approval, stealing money from congregations, other financial misconduct, and getting arrested.

Judaism, like Unitarian Universalism, tends towards congregationalism, and so decisions to exclude from a community of worship often depend on the congregation. Congregational bylaws sometimes enable the board of a synagogue to ask individuals to leave or not to enter.




Electrochemical cell

An electrochemical cell is a device that generates electrical energy from chemical reactions. Electrical energy can also be applied to these cells to cause chemical reactions to occur. Electrochemical cells that generate an electric current are called voltaic or galvanic cells and those that generate chemical reactions, via electrolysis for example, are called electrolytic cells.

Both galvanic and electrolytic cells can be thought of as having two half-cells: consisting of separate oxidation and reduction reactions. 

When one or more electrochemical cells are connected in parallel or series they make a battery. Primary cells are single use batteries.

A galvanic cell (voltaic cell) named after Luigi Galvani (Alessandro Volta) is an electrochemical cell that generates electrical energy from spontaneous redox reactions.

A wire connects two different metals (ex. Zinc and Copper). Each metal is in a separate solution; often the aqueous sulphate or nitrate forms of the metal, however more generally metal salts and water which conduct current. A salt bridge or porous membrane connects the two solutions, keeping electric neutrality and the avoidance of charge accumulation. The metal's differences in oxidation/reduction potential drive the reaction until equilibrium.

Key features:


Galvanic cells consists of two half-cells. Each half-cell consists of an electrode and an electrolyte (both half cells may use the same or different electrolytes).

The chemical reactions in the cell involve the electrolyte, electrodes, and/or an external substance (fuel cells may use hydrogen gas as a reactant). In a full electrochemical cell, species from one half-cell lose electrons (oxidation) to their electrode while species from the other half-cell gain electrons (reduction) from their electrode.

A "salt bridge" (e.g., filter paper soaked in KNO NaCl, or some other electrolyte) is used to ionically connect two half-cells with different electrolytes, but it prevents the solutions from mixing and unwanted side reactions. An alternative to a salt bridge is to allow direct contact (and mixing) between the two half-cells, for example in simple electrolysis of water.

As electrons flow from one half-cell to the other through an external circuit, a difference in charge is established. If no ionic contact were provided, this charge difference would quickly prevent the further flow of electrons. A salt bridge allows the flow of negative or positive ions to maintain a steady-state charge distribution between the oxidation and reduction vessels, while keeping the contents otherwise separate. Other devices for achieving separation of solutions are porous pots and gelled solutions. A porous pot is used in the Bunsen cell.

Each half-cell has a characteristic voltage (depending on the metal and its characteristic reduction potential). Each reaction is undergoing an equilibrium reaction between different oxidation states of the ions: when equilibrium is reached, the cell cannot provide further voltage. In the half-cell performing oxidation, the closer the equilibrium lies to the ion/atom with the more positive oxidation state the more potential this reaction will provide. Likewise, in the reduction reaction, the closer the equilibrium lies to the ion/atom with the more "negative" oxidation state the higher the potential.

The cell potential can be predicted through the use of electrode potentials (the voltages of each half-cell). These half-cell potentials are defined relative to the assignment of 0 volts to the standard hydrogen electrode (SHE). (See table of standard electrode potentials). The difference in voltage between electrode potentials gives a prediction for the potential measured. When calculating the difference in voltage, one must first rewrite the half-cell reaction equations to obtain a balanced oxidation-reduction equation.


Cell potentials have a possible range of roughly zero to 6 volts. Cells using water-based electrolytes are usually limited to cell potentials less than about 2.5 volts due to high reactivity of the powerful oxidizing and reducing agents with water that is needed to produce a higher voltage. Higher cell potentials are possible with cells using other solvents instead of water. For instance, lithium cells with a voltage of 3 volts are commonly available.

The cell potential depends on the concentration of the reactants, as well as their type. As the cell is discharged, the concentration of the reactants decreases and the cell potential also decreases.

An electrolytic cell is an electrochemical cell in which applied electrical energy drives a non-spontaneous redox reaction.

They are often used to decompose chemical compounds, in a process called electrolysis. (The Greek word "lysis" (λύσις) means "loosing" or "setting free".)

Important examples of electrolysis are the decomposition of water into hydrogen and oxygen, and of bauxite into aluminium and other chemicals. Electroplating (e.g. of copper, silver, nickel or chromium) is done using an electrolytic cell. Electrolysis is a technique that uses a direct electric current (DC).

The components of an electrolytic cell are:


When driven by an external voltage (potential difference) applied to the electrodes, the ions in the electrolyte are attracted to the electrode with the opposite potential, where charge-transferring (also called faradaic or redox) reactions can take place. Only with a sufficient external voltage can an electrolytic cell decompose a normally stable, or inert chemical compound in the solution. Thus the electrical energy provided produces a chemical reaction which would not occur spontaneously otherwise.

Key features:


A primary cell produces current by irreversible chemical reactions (ex. small disposable batteries) and is not rechargeable.

They are used for their portability, low cost, and short lifetime.

Primary cells are made in a range of standard sizes to power small household appliances such as flashlights and portable radios.

As chemical reactions proceed in a primary cell, the battery uses up the chemicals that generate the power; when they are gone, the battery stops producing electricity.
Primary batteries make up about 90% of the $50 billion battery market, but secondary batteries have been gaining market share. About 15 billion primary batteries are thrown away worldwide every year, virtually all ending up in landfills. Due to the toxic heavy metals and strong acids or alkalis they contain, batteries are hazardous waste. Most municipalities classify them as such and require separate disposal. The energy needed to manufacture a battery is about 50 times greater than the energy it contains. Due to their high pollutant content compared to their small energy content, the primary battery is considered a wasteful, environmentally unfriendly technology. Due mainly to increasing sales of wireless devices and cordless tools, which cannot be economically powered by primary batteries and come with integral rechargeable batteries, the secondary battery industry has high growth and has slowly been replacing the primary battery in high end products.

A secondary cell produces current by reversible chemical reactions (ex. lead-acid battery car battery) and is rechargeable.

Lead-acid batteries are used in an automobile to start an engine and to operate the car's electrical accessories when the engine is not running. The alternator, once the car is running, recharges the battery.

It can perform as a galvanic cell and an electrolytic cell. It is a convenient way to store electricity: when current flows one way, the levels of one or more chemicals build up (charging); while it is discharging, they reduce and the resulting electromotive force can do work.

They are used for their high voltage, low costs, reliability, and long lifetime.

A fuel cell is an electrochemical cell that reacts hydrogen fuel with oxygen or another oxidizing agent, to convert chemical energy to electricity.

Fuel cells are different from batteries in requiring a continuous source of fuel and oxygen (usually from air) to sustain the chemical reaction, whereas in a battery the chemical energy comes from chemicals already present in the battery.

Fuel cells can produce electricity continuously for as long as fuel and oxygen are supplied.

They are used for primary and backup power for commercial, industrial and residential buildings and in remote or inaccessible areas. They are also used to power fuel cell vehicles, including forklifts, automobiles, buses, boats, motorcycles and submarines.

Fuel cells are classified by the type of electrolyte they use and by the difference in startup time, which ranges from 1 second for proton-exchange membrane fuel cells (PEM fuel cells, or PEMFC) to 10 minutes for solid oxide fuel cells (SOFC).

There are many types of fuel cells, but they all consist of:


A related technology are flow batteries, in which the fuel can be regenerated by recharging. Individual fuel cells produce relatively small electrical potentials, about 0.7 volts, so cells are "stacked", or placed in series, to create sufficient voltage to meet an application's requirements. In addition to electricity, fuel cells produce water, heat and, depending on the fuel source, very small amounts of nitrogen dioxide and other emissions. The energy efficiency of a fuel cell is generally between 40 and 60%; however, if waste heat is captured in a cogeneration scheme, efficiencies up to 85% can be obtained.

In 2022, the global fuel cell market was estimated to be $6.3 billion, and is expected to increase by 19.9% by 2030. Many countries are attempting to enter the market by setting renewable energy GW goals.


Ecdysis

Ecdysis is the moulting of the cuticle in many invertebrates of the clade Ecdysozoa. Since the cuticle of these animals typically forms a largely inelastic exoskeleton, it is shed during growth and a new, larger covering is formed. The remnants of the old, empty exoskeleton are called exuviae.

After moulting, an arthropod is described as "teneral", a "callow"; it is "fresh", pale and soft-bodied. Within one or two hours, the cuticle hardens and darkens following a tanning process analogous to the production of leather. During this short phase the animal expands, since growth is otherwise constrained by the rigidity of the exoskeleton. Growth of the limbs and other parts normally covered by the hard exoskeleton is achieved by transfer of body fluids from soft parts before the new skin hardens. A spider with a small abdomen may be undernourished but more probably has recently undergone ecdysis. Some arthropods, especially large insects with tracheal respiration, expand their new exoskeleton by swallowing or otherwise taking in air. The maturation of the structure and colouration of the new exoskeleton might take days or weeks in a long-lived insect; this can make it difficult to identify an individual if it has recently undergone ecdysis.

Ecdysis allows damaged tissue and missing limbs to be regenerated or substantially re-formed. Complete regeneration may require a series of moults, the stump becoming a little larger with each moult until the limb is a normal, or near normal, size.

The term "ecdysis" comes from Ancient Greek () 'to take off, strip off'.

In preparation for ecdysis, the arthropod becomes inactive for a period of time, undergoing apolysis or separation of the old exoskeleton from the underlying epidermal cells. For most organisms, the resting period is a stage of preparation during which the secretion of fluid from the moulting glands of the epidermal layer and the loosening of the underpart of the cuticle occurs.
Once the old cuticle has separated from the epidermis, a digesting fluid is secreted into the space between them. However, this fluid remains inactive until the upper part of the new cuticle has been formed. Then, by crawling movements, the organism pushes forward in the old integumentary shell, which splits down the back allowing the animal to emerge. Often, this initial crack is caused by a combination of movement and increase in blood pressure within the body, forcing an expansion across its exoskeleton, leading to an eventual crack that allows for certain organisms such as spiders to extricate themselves.
While the old cuticle is being digested, the new layer is secreted. All cuticular structures are shed at ecdysis, including the inner parts of the exoskeleton, which includes terminal linings of the alimentary tract and of the tracheae if they are present.
Each stage of development between moults for insects in the taxon Endopterygota is called an instar, or stadium, and each stage between moults of insects in the Exopterygota is called a nymph: there may be up to 15 nymphal stages. Endopterygota tend to have only four or five instars. Endopterygotes have more alternatives to moulting, such as expansion of the cuticle and collapse of air sacs to allow growth of internal organs.

The process of moulting in insects begins with the separation of the cuticle from the underlying epidermal cells (apolysis) and ends with the shedding of the old cuticle (ecdysis). In many species it is initiated by an increase in the hormone ecdysone. This hormone causes:

After apolysis the insect is known as a pharate. Moulting fluid is then secreted into the exuvial space between the old cuticle and the epidermis, this contains inactive enzymes which are activated only after the new epicuticle is secreted. This prevents the new procuticle from getting digested as it is laid down. The lower regions of the old cuticle, the endocuticle and mesocuticle, are then digested by the enzymes and subsequently absorbed. The exocuticle and epicuticle resist digestion and are hence shed at ecdysis.

Spiders generally change their skin for the first time while still inside the egg sac, and the spiderling that emerges broadly resembles the adult. The number of moults varies, both between species and sexes, but generally will be between five times and nine times before the spider reaches maturity. Not surprisingly, since males are generally smaller than females, the males of many species mature faster and do not undergo ecdysis as many times as the females before maturing.
Members of the Mygalomorphae are very long-lived, sometimes 20 years or more; they moult annually even after they mature.

Spiders stop feeding at some time before moulting, usually for several days. The physiological processes of releasing the old exoskeleton from the tissues beneath typically cause various colour changes, such as darkening. If the old exoskeleton is not too thick it may be possible to see new structures, such as setae, from the outside. However, contact between the nerves and the old exoskeleton is maintained until a very late stage in the process.

The new, teneral exoskeleton has to accommodate a larger frame than the previous instar, while the spider has had to fit into the previous exoskeleton until it has been shed. This means the spider does not fill out the new exoskeleton completely, so it commonly appears somewhat wrinkled.
Most species of spiders hang from silk during the entire process, either dangling from a drop line, or fastening their claws into webbed fibres attached to a suitable base. The discarded, dried exoskeleton typically remains hanging where it was abandoned once the spider has left.
To open the old exoskeleton, the spider generally contracts its abdomen (opisthosoma) to supply enough fluid to pump into the prosoma with sufficient pressure to crack it open along its lines of weakness. The carapace lifts off from the front, like a helmet, as its surrounding skin ruptures, but it remains attached at the back. Now the spider works its limbs free and typically winds up dangling by a new thread of silk attached to its own exuviae, which in turn hang from the original silk attachment.
At this point the spider is a callow; it is teneral and vulnerable. As it dangles, its exoskeleton hardens and takes shape. The process may take minutes in small spiders, or some hours in the larger Mygalomorphs. Some spiders, such as some "Synema" species, members of the Thomisidae (crab spiders), mate while the female is still callow, during which time she is unable to eat the male.

Eurypterids are a group of chelicerates that became extinct in the Late Permian. They underwent ecdysis similarly to extant chelicerates, and most fossils are thought to be of exuviae, rather than cadavers.


Ebor, New South Wales

Ebor is a village on Waterfall Way on the Northern Tablelands in New South Wales, Australia. It is situated about east of Armidale and about a third of the way between Armidale and the coast. Dorrigo to the east is away with the Coffs Coast away along Waterfall Way. In the , Ebor's zone had a population of 149.

The village is situated in the traditional lands of the Gumbaynggirr peoples.

Ebor shares its name with a nearby set of waterfalls, Ebor Falls, which is a local tourist attraction. Gumbaynggirr people traditionally called the falls "Martiam" (meaning 'the great falls').

The village's wooden Catholic church burned down in October 1946.

At the , Ebor had a population of 149 people.

Although "The Heart of Waterfall Way", Ebor is on the eastern edge of Armidale Regional Council, and close to the border of Clarence Valley Council and Bellingen Shire Council. Until the amalgamation of Guyra and Armidale councils, one side of Ebor was under Armidale council, and the other under Guyra shire. Likewise, Ebor is close to three state electoral districts (Northern Tablelands, Oxley and Clarence) and three federal electoral boundaries (New England, Cowper and Page).

Amenities in the area include a cafe, a combined post office, fuel station and general store, a pub/motel with camp ground, and a NSW DEC primary school. The local sports ground is home of the Ebor Campdraft.

There are also Rural Fire Service and National Parks and Wildlife Service depots in the area, but no police or ambulance services based in Ebor. The nearest hospital and 24h emergency department is in Dorrigo.

Due to its central position on Waterfall Way, Ebor offers easy access for residents and tourists to Guy Fawkes River National Park, Cathedral Rock National Park, Cunnawarra National Park, New England National Park, part of Oxley Wild Rivers National Park, Nymboi-Binderay National Park and Mount Hyland Nature Reserve.

The natural environment of the surrounding district includes several areas which have been cleared for pastoralism and forestry. Nonetheless, the national parks around Ebor have been described as a bush walking "Mecca". The main tourist attraction is the twin Ebor Falls.

In 1930 Sydney Smith Jr. wrote that: "During a recent visit to Ebor I was much impressed with the possibilities of this part of the State as a tourist resort... Around Ebor and Guy Fawkes can be seen some of the most magnificent scenery in this State if not Australia. ...The two falls are scenes of beauty, and in winter time are sometimes frozen, making a beautiful spectacle as they hang in huge icicles. The water from the Ebor eventually finds an outlet in the Clarence River. ...The view, ...as regards expansiveness, ruggedness, and beauty, must compare more than favourably with views of a similar nature in any part of the Commonwealth. It reminded me of the Valley of a Thousand Hills, outside Durban, in South Africa".

In 1976, local historian Eric Fahey also wrote: "I believe the future of Dorrigo will depend largely on tourism. The area has a lot to offer, both in peerless scenery and because of the native fauna which can be seen in large numbers in their natural state."

Wagyu beef specialists Stone Axe have a large holding, "Glen Alvie", on the northern boundary of the village. Stone Axe also acquired "Alfreda" in the nearby locality of Wongwibinda.

Black truffles (tuber melanosporum) are grown at the Guy Fawkes Truffle Company outside of Ebor on the Guyra Rd.

Trout are another local product. The Dutton Trout Hatchery on Point Lookout Road was established in 1950 and is one of the largest hatcheries in the state. Visitors can see the various stages of trout development prior to their release in the mountain streams. The release of trout into local streams is believed to have led to decline of the endangered Tusked frog.

There are two short walks close to the village. One takes walkers through the recreation reserve. This walk follows the Guy Fawkes River upstream for about half of the walk. Some bird life can be seen. The second walk is accessed by crossing the Guy Fawkes River bridge, and following the pedestrian path that winds downstream under the bridge. This path follows the Guy Fawkes River north and meets the national park's Upper and Lower Falls paths. Wallabies, kangaroos, bird life and fire-flies can be seen depending on the season. Platypus have also been sighted in August in the pool above the falls.

The Bicentennial National Trail (BNT) passes through Ebor, which sits on the boundary of sections 7 and 8 of the BNT.

The Ebor Falls area is sometimes used for rock climbing, and is described as "holding a rather special place in the History of New England climbing".

Ebor has a noted problem with speeding vehicles. Both passenger cars and heavy vehicles regularly exceed the posted speed limit of 50 km/h. Traffic noise is also a problem. Waterfall Way has an entry on the Dangerous Roads website.

Ebor's Post Office opened on 2 March 1868, closed in 1869 and reopened in 1910. It is currently located at the Ebor petrol station/store having moved from Fusspots Cafe.

Ebor has a number of cultural heritage sites, including several Aboriginal meeting places, and massacre sites.

"Gwenda Gardens" is an abandoned homestead on the Guyra-Ebor Road.

Other sites include:

The village of Ebor is at high altitude by Australian standards. It has cold winters with frequent overnight frost and occasional light snow falls. The average rain fall is about .


Ancient history of Afghanistan

The ancient history of Afghanistan, also referred to as the pre-Islamic period of Afghanistan, dates back to the prehistoric era and the Helmand civilization around 3300–2350 BCE. Archaeological exploration began in Afghanistan in earnest after World War II and proceeded until the late 1970s during the Soviet–Afghan War. Archaeologists and historians suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world. Urbanized culture has existed in the land from between 3000 and 2000 BC. Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found inside Afghanistan.

After the Indus Valley civilization stretched up to northeast Afghanistan, it was inhabited by the Iranic tribes and controlled by the Medes until about 500 BC when Darius the Great (Darius I) marched with his Persian army to make it part of the Achaemenid Empire. In 330 BC, Alexander the Great of Macedonia invaded the land after defeating Darius III of Persia in the Battle of Gaugamela. Much of Afghanistan became part of the Seleucid Empire followed by the Greco-Bactrian Kingdom. Seleucus I Nicator was defeated by Chandragupta Maurya and gave his daughter in peace treaty. The land was inhabited by various tribes and ruled by many different kingdoms for the next two millenniums. Before the arrival of Islam in the 7th century, there were a number of religions practiced in modern day Afghanistan, including Zoroastrianism, Ancient Iranian religions, Buddhism and Hinduism. The Kafiristan (present-day Nuristan) region, in the Hindu Kush mountain range, was not converted until the 19th century.

Louis Dupree, the University of Pennsylvania, the Smithsonian Institution and others suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world.
Afghanistan seems in prehistory, as well as in ancient and modern times, to have been connected by culture and trade with the neighbouring regions. Urban civilization may have begun as early as 3000 to 2000 BC. Archaeological finds indicate the possible beginnings of the Bronze Age, which would ultimately spread throughout the ancient world from Afghanistan. It is also believed that the region had early trade contacts with Mesopotamia.

The Indus Valley civilization (IVC) was a Bronze Age civilization (3300–1300 BCE; mature period 2600–1900 BCE) extending from what today is northwest Pakistan to northwest India and northeast Afghanistan. An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan. Apart from Shortughai is Mundigak, another notable site. There are several smaller IVC colonies to be found in Afghanistan.

Between 2000–1200 BC, a branch of Indo-European-speaking tribes known as the Aryans began migrating into the region. This is part of a dispute in regards to the Indo-Aryan migration. They split into Iranic peoples, Nuristani, and Indo-Aryan groups at an early stage, possibly between 1500 and 1000 BC in what is today Afghanistan or much earlier as eastern remnants of the Indo-Aryans drifted much further west as with the Mitanni. The Iranians dominated the modern day plateau, while the Indo-Aryans ultimately headed towards the Indian subcontinent. The Avesta is believed to have been composed possibly as early as 1800 BC and written in ancient Ariana (Aryana), the earliest name of Afghanistan which indicates an early link with today's Iranian tribes to the west, or adjacent regions in Central Asia or northeastern Iran in the 6th century BC. Due to the similarity between early Avestan and Sanskrit (and other related early Indo-European languages such as Latin and Ancient Greek), it is believed that the split between the old Persians and Indo-Aryan tribes had taken place at least by 1000 BC. There are striking similarities between Avestan and Sanskrit, which may support the notion that the split was contemporary with the Indo-Aryans living in Afghanistan at a very early stage. Also, the Avesta itself divides into Old and New sections and neither mention the Medes who are known to have ruled Afghanistan starting around 700 BC. This suggests an early time-frame for the Avesta that has yet to be exactly determined as most academics believe it was written over the course of centuries if not millennia. Much of the archaeological data comes from the Bactria-Margiana Archaeological Complex (BMAC and Indus Valley civilization) that probably played a key role in early Aryanic civilization in Afghanistan.

The Indo-Aryan inhabitants of the region- mainly in the southern and eastern parts of the country were adherents of Hinduism. Notable among these were the Gandharis
The Pashayi and Nuristanis are present day examples of these Indo-Iranian people.
The Medes, a Western Iranian people, arrived from what is today Kurdistan sometime around the 700s BC and came to dominate most of ancient Afghanistan. They were an early tribe that forged the first empire on the present Iranian plateau and sister-nations with the Persians whom they initially dominated in the province of Fars to the south. Median control of parts of far off Afghanistan would last until Cyrus the Great, prince of the Persians, assassinated and ultimately replaced his Median emperor father-in-law from rule.

The city of Bactra (which later became Balkh), is believed to have been the home of Zarathustra, who founded the Zoroastrian religion. The Avesta refers to eastern Bactria as being the home of the Zoroastrian faith. Regardless of the debate as to where Zoroaster was from, Zoroastrianism spread to become one of the world's most influential religions and became the main faith of the old Aryan people for centuries. It also remained the official religion of Persia until the defeat of the Sassanian ruler Yazdegerd III—over a thousand years after its founding—by Muslim Arabs. In what is today southern Iran, the Persians emerged to challenge Median supremacy on the Iranian plateau. By 550 BC, the Persians had replaced Median rule with their own dominion and even began to expand past previous Median imperial borders. Both Gandhara and Kamboja Mahajanapadas of the Buddhist texts soon fell a prey to the Achaemenian Dynasty during the reign of Achaemenid, Cyrus the Great (558–530 BC), or in the first year of Darius I, marking the region or of the easternmost provinces of the empire, located partly in nowadays Afghanistan. According to Pliny's evidence, Cyrus the Great (Cyrus II) had destroyed Kapisa in Capiscene which was a Kamboja city. The former region of Gandhara and Kamboja (upper Indus) had constituted seventh satrapy of the Achaemenid Empire and annually contributed 170 talents of gold dust as a tribute to the Achaemenids.

Bactria had a special position in old Afghanistan, being the capital of a vice-kingdom. By the 4th century BC, Persian control of outlying areas and the internal cohesion of the empire had become somewhat tenuous. Although distant provinces like Bactriana had often been restless under Achaemenid rule, Bactrian troops nevertheless fought in the decisive Battle of Gaugamela in 330 BC against the advancing armies of Alexander the Great. The Achaemenids were decisively defeated by Alexander and retreated from his advancing army of Greco-Macedonians and their allies. Darius III, the last Achaemenid ruler, tried to flee to Bactria but was assassinated by a subordinate lord, the Bactrian-born Bessus, who proclaimed himself the new ruler of Persia as Artaxerxes (V). Bessus was unable to mount a successful resistance to the growing military might of Alexander's army so he fled to his native Bactria, where he attempted to rally local tribes to his side but was instead turned over to Alexander who proceeded to have him tortured and executed for having committed regicide.

Moving thousands of kilometers eastward from recently subdued Persia, the Macedonian leader Alexander the Great, encountered fierce resistance from the local tribes of Aria, Drangiana, Arachosia (South and Eastern Afghanistan, North-West Pakistan) and Bactria (North and Central Afghanistan). One of the fiercest battles that he faced was in Herat. One of his top commanding officers was killed by the rebels and he had to go there himself. He couldn't defeat them in time and he ended up burning down the forest to finish the rebellion.

Upon Alexander's death in 323 BC, his empire, which had never been politically consolidated, broke apart as his companions began to divide it amongst themselves. Alexander's cavalry commander, Seleucus, took nominal control of the eastern lands and founded the Seleucid dynasty. Under the Seleucids, as under Alexander, Greek colonists and soldiers colonized Bactria, roughly corresponding to modern Afghanistan's borders. However, the majority of Macedonian soldiers of Alexander the Great wanted to leave the east and return home to Greece. Later, Seleucus sought to guard his eastern frontier and moved Ionian Greeks (also known as Yavanas to many local groups) to Bactria in the 3rd century BC.

Greece had one of the most advanced civilizations at that period. Wherever they went, they left and gained something from cultures and ultimately, they had a civilization that was compromised from other top civilizations of the time. Greek men were marrying with other women and this helped the process of mixing the cultures a lot. 

While the Diadochi were warring amongst themselves, the Mauryan Empire was developing in the northern part of the Indian subcontinent. The founder of the empire, Chandragupta Maurya, confronted a Macedonian invasion force led by Seleucus I in 305 BC and following a brief conflict, an agreement was reached as Seleucus ceded Gandhara and Arachosia (centered around ancient Kandahar) and areas south of Bagram (corresponding to the extreme south-east of modern Afghanistan) to the Mauryans. During the 120 years of the Mauryans in southern Afghanistan, Buddhism was introduced and eventually become a major religion alongside Zoroastrianism and local pagan beliefs. The ancient Grand Trunk Road was built linking what is now Kabul to various cities in the Punjab and the Gangetic Plain. Commerce, art, and architecture (seen especially in the construction of stupas) developed during this period. It reached its high point under Emperor Ashoka whose edicts, roads, and rest stops were found throughout the subcontinent. Although the vast majority of them throughout the subcontinent were written in Prakrit, Afghanistan is notable for the inclusion of 2 Greek and Aramaic ones alongside the court language of the Mauryans.

Inscriptions made by the Mauryan Emperor Ashoka, a fragment of Edict 13 in Greek, as well as a full Edict, written in both Greek and Aramaic has been discovered in Old Kandahar. It is said to be written in excellent Classical Greek, using sophisticated philosophical terms. In this Edict, Ashoka uses the word Eusebeia ("Piety") as the Greek translation for the ubiquitous "Dharma" of his other Edicts written in Prakrit:

The last ruler in the region was probably Subhagasena (Sophagasenus of Polybius), who, in all probability, belonged to the Ashvaka (q.v.) background.

In the middle of the 3rd century BC, an independent, Hellenistic state was declared in Bactria and eventually the control of the Seleucids and Mauryans was overthrown in western and southern Afghanistan. Graeco-Bactrian rule spread until it included a large territory which stretched from Turkmenistan in the west to the Punjab in India in the east by about 170 BC. Graeco-Bactrian rule was eventually defeated by a combination of internecine disputes that plagued Greek and Hellenized rulers to the west, continual conflict with Indian kingdoms, as well as the pressure of two groups of nomadic invaders from Central Asia—the Parthians and Sakas.

In the 3rd and 2nd centuries BC, the Parthians, a nomadic Iranian peoples, arrived in Western Asia. While they made large inroads into the modern-day territory of Afghanistan, about 100 years later another Indo-European group from the north—the Kushans (a subgroup of the tribe called the Yuezhi by the Chinese)—entered the region of Afghanistan and established an empire lasting almost four centuries, which would dominate most of the Afghanistan region.

The Kushan Empire spread from the Kabul River valley to defeat other Central Asian tribes that had previously conquered parts of the northern central Iranian Plateau once ruled by the Parthians. By the middle of the 1st century BC, the Kushans' base of control became Afghanistan and their empire spanned from the north of the Pamir mountains to the Ganges river valley in India. During rule of Kanishka, they had 2 seasonal capital cities which were Kabul in Spring and Summer then moving to Peshawr for Fall and Winter. Early in the 2nd century under Kanishka, the most powerful of the Kushan rulers, the empire reached its greatest geographic and cultural breadth to become a center of literature and art. Kanishka extended Kushan control to the mouth of the Indus River on the Arabian Sea, into Kashmir, and into what is today the Chinese-controlled area north of Tibet. Kanishka was a patron of religion and the arts. It was during his reign that Buddhism, which was promoted in northern India earlier by the Mauryan emperor Ashoka (c. 260 BC–232 BC), reached its zenith in Central Asia. Though the Kushanas supported local Buddhists and Hindus as well as the worship of various local deities.

In the 3rd century, Kushan control fragmented into semi-independent kingdoms that became easy targets for conquest by the rising Iranian dynasty, the Sasanians (c. 224–561) which annexed Afghanistan by 300 AD. In these far off easternmost territories, they established vassal kings as rulers, known as the Kushanshahs. Sasanian control was tenuous at times as numerous challenges from Central Asian tribes led to instability and constant warfare in the region.

The disunited Kushan and Sasanian kingdoms were in a poor position to meet the threat several waves of Xionite/Huna invaders from the north from the 4th century onwards. In particular, the Hephthalites (or "Ebodalo"; Bactrian script ηβοδαλο) swept out of Central Asia during the 5th century into Bactria and Iran, overwhelming the last of the Kushan kingdoms. Historians believe that Hephthalite control continued for a century and was marked by constant warfare with the Sassanians to the west who exerted nominal control over the region.
By the middle of the 6th century the Hephthalites were defeated in the territories north of the Amu Darya (the Oxus River of antiquity) by another group of Central Asian nomads, the Göktürks, and by the resurgent Sassanians in the lands south of the Amu Darya. It was the ruler of western Göktürks, Sijin (a.k.a. Sinjibu, Silzibul and Yandu Muchu Khan) who led the forces against the Hepthalites who were defeated at the Battle of Chach (Tashkent) and at the Battle of Bukhara.

The Shahi dynasties ruled portions of the Kabul Valley (in eastern Afghanistan) and the old province of Gandhara (northern Pakistan and Kashmir) from the decline of the Kushan Empire in the 3rd century to the early 9th century. They are split into two eras the Buddhist Turk Shahis and the later Hindu Shahis with the change-over occurring around 870, and ruled up until the Islamic conquest of Afghanistan.

When Xuanzang visited the region early in the 7th century, the Kabul region was ruled by a Kshatriya king, who is identified as the "Shahi Khingal", and whose name has been found in an inscription found in Gardez. The Turkic Shahi regency was overthrown and replaced by a Mohyal Shahi dynasty of Brahmins who began the first phase of the Hindu Shahi dynasty.

These Hindu kings of Kabul and Gandhara may have had links to some ruling families in neighboring Kashmir and other areas to the east. The Shahis were rulers of predominantly Buddhist, Zoroastrian, Hindu and Muslim populations and were thus patrons of numerous faiths, and various artifacts and coins from their rule have been found that display their multicultural domain. In 964 AD, the last Mohyal Shahi was succeeded by the Janjua overlord, Jayapala, of the Panduvanshi dynasty. The last Shahi emperors Jayapala, Anandapala and Tirlochanpala fought the Muslim Ghaznavids of Ghazna and were gradually defeated. Their remaining army were eventually exiled into northern India.

Most of the Zoroastrian, Greek, Hellenistic, Buddhist, Hindu and other indigenous cultures were replaced by the coming of Islam and little influence remains in Afghanistan today. Along ancient trade routes, however, stone monuments of the once flourishing Buddhist culture did exist as reminders of the past. The two massive sandstone Buddhas of Bamyan, 35 and 53 meters high, overlooked the ancient route through Bamyan to Balkh and dated from the 3rd and 5th centuries. They survived until 2001, when they were destroyed by the Taliban. In this and other key places in Afghanistan, archaeologists have located frescoes, stucco decorations, statuary, and rare objects from as far away as China, Phoenicia, and Rome, which were crafted as early as the 2nd century and bear witness to the influence of these ancient civilizations upon Afghanistan.

One of the early Buddhist schools, the Mahāsāṃghika-Lokottaravāda, were known to be prominent in the area of Bamiyan. The Chinese Buddhist monk Xuanzang visited a Lokottaravāda monastery in the 7th century CE, at Bamiyan, Afghanistan, and this monastery site has since been rediscovered by archaeologists. Birchbark and palm leaf manuscripts of texts in this monastery's collection, including Mahāyāna sūtras, have been discovered at the site, and these are now located in the Schøyen Collection. Some manuscripts are in the Gāndhārī language and Kharoṣṭhī script, while others are in Sanskrit and written in forms of the Gupta script. Manuscripts and fragments that have survived from this monastery's collection include well-known Buddhist texts such as the "Mahāparinirvāṇa Sūtra" (from the "Āgamas"), the "Diamond Sūtra" ("Vajracchedikā Prajñāpāramitā"), the "Medicine Buddha Sūtra", and the "Śrīmālādevī Siṃhanāda Sūtra".

In 2010, reports stated that about 42 Buddhist relics have been discovered in the Logar Province of Afghanistan, which is south of Kabul. Some of these items date back to the 2nd century according to Archaeologists. The items included two Buddhist temples (Stupas), Buddha statues, frescos, silver and gold coins and precious beads.




Gravitational redshift

In physics and general relativity, gravitational redshift (known as Einstein shift in older literature) is the phenomenon that electromagnetic waves or photons travelling out of a gravitational well (seem to) lose energy. This loss of energy corresponds to a decrease in the wave frequency and increase in the wavelength, known more generally as a "redshift". The opposite effect, in which photons (seem to) gain energy when travelling into a gravitational well, is known as a gravitational blueshift (a type of "blueshift"). The effect was first described by Einstein in 1907, eight years before his publication of the full theory of relativity.

Gravitational redshift can be interpreted as a consequence of the equivalence principle (that gravity and acceleration are equivalent and the redshift is caused by the Doppler effect) or as a consequence of the mass–energy equivalence and conservation of energy ('falling' photons gain energy), though there are numerous subtleties that complicate a rigorous derivation. A gravitational redshift can also equivalently be interpreted as gravitational time dilation at the source of the radiation: if two oscillators (attached to transmitters producing electromagnetic radiation) are operating at different gravitational potentials, the oscillator at the higher gravitational potential (farther from the attracting body) will seem to ‘tick’ faster; that is, when observed from the same location, it will have a higher measured frequency than the oscillator at the lower gravitational potential (closer to the attracting body).

To first approximation, gravitational redshift is proportional to the difference in gravitational potential divided by the speed of light squared, formula_1, thus resulting in a very small effect. Light escaping from the surface of the Sun was predicted by Einstein in 1911 to be redshifted by roughly 2 ppm or 2 × 10. Navigational signals from GPS satellites orbiting at 20,000 km altitude are perceived blueshifted by approximately 0.5 ppb or 5 × 10, corresponding to a (negligible) increase of less than 1 Hz in the frequency of a 1.5 GHz GPS radio signal (however, the accompanying gravitational time dilation affecting the atomic clock in the satellite "is" crucially important for accurate navigation). On the surface of the Earth the gravitational potential is proportional to height, formula_2, and the corresponding redshift is roughly 10 (0.1 part per quadrillion) per meter of change in elevation and/or altitude.

In astronomy, the magnitude of a gravitational redshift is often expressed as the velocity that would create an equivalent shift through the relativistic Doppler effect. In such units, the 2 ppm sunlight redshift corresponds to a 633 m/s receding velocity, roughly of the same magnitude as convective motions in the Sun, thus complicating the measurement. The GPS satellite gravitational blueshift velocity equivalent is less than 0.2 m/s, which is negligible compared to the actual Doppler shift resulting from its orbital velocity. In astronomical objects with strong gravitational fields the redshift can be much greater; for example, light from the surface of a white dwarf is gravitationally redshifted on average by around 50 km/s/c (around 170 ppm).

Observing the gravitational redshift in the Solar System is one of the classical tests of general relativity. Measuring the gravitational redshift to high precision with atomic clocks can serve as a test of Lorentz symmetry and guide searches for dark matter.

Einstein's theory of general relativity incorporates the equivalence principle, which can be stated in various different ways. One such statement is that gravitational effects are locally undetectable for a free-falling observer. Therefore, in a laboratory experiment at the surface of the Earth, all gravitational effects should be equivalent to the effects that would have been observed if the laboratory had been accelerating through outer space at "g". One consequence is a gravitational Doppler effect. If a light pulse is emitted at the floor of the laboratory, then a free-falling observer says that by the time it reaches the ceiling, the ceiling has accelerated away from it, and therefore when observed by a detector fixed to the ceiling, it will be observed to have been Doppler shifted toward the red end of the spectrum. This shift, which the free-falling observer considers to be a kinematical Doppler shift, is thought of by the laboratory observer as a gravitational redshift. Such an effect was verified in the 1959 Pound–Rebka experiment. In a case such as this, where the gravitational field is uniform, the change in wavelength is given by

where formula_4 is the change in height. Since this prediction arises directly from the equivalence principle, it does not require any of the mathematical apparatus of general relativity, and its verification does not specifically support general relativity over any other theory that incorporates the equivalence principle.

On Earth's surface (or in a spaceship accelerating at 1 g), the gravitational redshift is approximately 1.1 × 10, the equivalent of a 3.3 × 10 m/s Doppler shift, for every meter of height differential.

When the field is not uniform, the simplest and most useful case to consider is that of a spherically symmetric field. By Birkhoff's theorem, such a field is described in general relativity by the Schwarzschild metric, formula_5, where formula_6 is the clock time of an observer at distance "R" from the center, formula_7 is the time measured by an observer at infinity, formula_8 is the Schwarzschild radius formula_9, "..." represents terms that vanish if the observer is at rest, formula_10 is Newton's gravitational constant, formula_11 the mass of the gravitating body, and formula_12 the speed of light. The result is that frequencies and wavelengths are shifted according to the ratio

where

This can be related to the redshift parameter conventionally defined as formula_17.

In the case where neither the emitter nor the observer is at infinity, the transitivity of Doppler shifts allows us to generalize the result to formula_18. The redshift formula for the frequency formula_19 is formula_20. When formula_21 is small, these results are consistent with the equation given above based on the equivalence principle.

The redshift ratio may also be expressed in terms of a (Newtonian) escape velocity formula_22 at formula_23, resulting in the corresponding Lorentz factor:

For an object compact enough to have an event horizon, the redshift is not defined for photons emitted inside the Schwarzschild radius, both because signals cannot escape from inside the horizon and because an object such as the emitter cannot be stationary inside the horizon, as was assumed above. Therefore, this formula only applies when formula_16 is larger than formula_8. When the photon is emitted at a distance equal to the Schwarzschild radius, the redshift will be "infinitely" large, and it will not escape to "any" finite distance from the Schwarzschild sphere. When the photon is emitted at an infinitely large distance, there is no redshift.

In the Newtonian limit, i.e. when formula_16 is sufficiently large compared to the Schwarzschild radius formula_8, the redshift can be approximated as

where formula_30 is the gravitational acceleration at formula_16. For Earth's surface with respect to infinity, "z" is approximately 7 × 10 (the equivalent of a 0.2 m/s radial Doppler shift); for the Moon it is approximately 3 × 10 (about 1 cm/s). The value for the surface of the Sun is about 2 × 10, corresponding to 0.64 km/s. (For non-relativisitc velocities, the radial Doppler equivalent velocity can be approximated by multiplying "z" with the speed of light.)

The z-value can be expressed succinctly in terms of the escape velocity at formula_16, since the gravitational potential is equal to half the square of the escape velocity, thus:

where formula_22 is the escape velocity at formula_16.

It can also be related to the circular orbit velocity formula_36 at formula_16, which equals formula_38, thus

For example, the gravitational blueshift of distant starlight due to the Sun's gravity, which the Earth is orbiting at about 30 km/s, would be approximately 1 × 10 or the equivalent of a 3 m/s radial Doppler shift.

For an object in a (circular) orbit, the gravitational redshift is of comparable magnitude as the transverse Doppler effect, formula_40 where "β"="v"/"c", while both are much smaller than the radial Doppler effect, for which formula_41.

A number of experimenters initially claimed to have identified the effect using astronomical measurements, and the effect was considered to have been finally identified in the spectral lines of the star Sirius B by W.S. Adams in 1925. However, measurements by Adams have been criticized as being too low and these observations are now considered to be measurements of spectra that are unusable because of scattered light from the primary, Sirius A. The first accurate measurement of the gravitational redshift of a white dwarf was done by Popper in 1954, measuring a 21 km/s gravitational redshift of 40 Eridani B. The redshift of Sirius B was finally measured by Greenstein "et al." in 1971, obtaining the value for the gravitational redshift of 89±16 km/s, with more accurate measurements by the Hubble Space Telescope, showing 80.4±4.8 km/s.

James W. Brault, a graduate student of Robert Dicke at Princeton University, measured the gravitational redshift of the sun using optical methods in 1962. In 2020, a team of scientists published the most accurate measurement of the solar gravitational redshift so far, made by analyzing Fe spectral lines in sunlight reflected by the Moon; their measurement of a mean global 638 ± 6 m/s lineshift is in agreement with the theoretical value of 633.1 m/s. Measuring the solar redshift is complicated by the Doppler shift caused by the motion of the Sun's surface, which is of similar magnitude as the gravitational effect.

In 2011, the group of Radek Wojtak of the Niels Bohr Institute at the University of Copenhagen collected data from 8000 galaxy clusters and found that the light coming from the cluster centers tended to be red-shifted compared to the cluster edges, confirming the energy loss due to gravity.

In 2018, the star S2 made its closest approach to Sgr A*, the 4-million solar mass supermassive black hole at the centre of the Milky Way, reaching 7650 km/s or about 2.5% of the speed of light while passing the black hole at a distance of just 120 AU, or 1400 Schwarzschild radii. Independent analyses by the GRAVITY collaboration (led by Reinhard Genzel) and the KECK/UCLA Galactic Center Group (led by Andrea Ghez) revealed a combined transverse Doppler and gravitational redshift up to 200 km/s/c, in agreement with general relativity predictions.

In 2021, Mediavilla (IAC, Spain) & Jiménez-Vicente (UGR, Spain) were able to use measurements of the gravitational redshift in quasars up to cosmological redshift of z~3 to confirm the predictions of Einstein's Equivalence Principle and the lack of cosmological evolution within 13%.

The effect is now considered to have been definitively verified by the experiments of Pound, Rebka and Snider between 1959 and 1965. The Pound–Rebka experiment of 1959 measured the gravitational redshift in spectral lines using a terrestrial Fe gamma source over a vertical height of 22.5 metres. This paper was the first determination of the gravitational redshift which used measurements of the change in wavelength of gamma-ray photons generated with the Mössbauer effect, which generates radiation with a very narrow line width. The accuracy of the gamma-ray measurements was typically 1%.

An improved experiment was done by Pound and Snider in 1965, with an accuracy better than the 1% level.

A very accurate gravitational redshift experiment was performed in 1976, where a hydrogen maser clock on a rocket was launched to a height of 10,000 km, and its rate compared with an identical clock on the ground. It tested the gravitational redshift to 0.007%.

Later tests can be done with the Global Positioning System (GPS), which must account for the gravitational redshift in its timing system, and physicists have analyzed timing data from the GPS to confirm other tests. When the first satellite was launched, it showed the predicted shift of 38 microseconds per day. This rate of the discrepancy is sufficient to substantially impair the function of GPS within hours if not accounted for. An excellent account of the role played by general relativity in the design of GPS can be found in Ashby 2003.

In 2010, an experiment placed two aluminum-ion quantum clocks close to each other, but with the second elevated 33 cm compared to the first, making the gravitational red shift effect visible in everyday lab scales.

In 2020, a group at the University of Tokyo measured the gravitational redshift of two strontium-87 optical lattice clocks. The measurement took place at Tokyo Skytree where the clocks were separated by approximately 450 m and connected by telecom fibers. The gravitational redshift can be expressed as

where formula_43 is the gravitational redshift, formula_44 is the optical clock transition frequency, formula_45 is the difference in gravitational potential, and formula_46 denotes the violation from general relativity. By Ramsey spectroscopy of the strontium-87 optical clock transition (429 THz, 698 nm) the group determined the gravitational redshift between the two optical clocks to be 21.18 Hz, corresponding to a "z"-value of approximately 5 × 10. Their measured value of formula_46, formula_48, is an agreement with recent measurements made with hydrogen masers in elliptical orbits.

In October 2021, a group at JILA led by physicist Jun Ye reported a measurement of gravitational redshift in the submillimeter scale. The measurement is done on the Sr clock transition between the top and the bottom of a millimeter-tall ultracold cloud of 100,000 strontium atoms in an optical lattice.

The gravitational weakening of light from high-gravity stars was predicted by John Michell in 1783 and Pierre-Simon Laplace in 1796, using Isaac Newton's concept of light corpuscles (see: emission theory) and who predicted that some stars would have a gravity so strong that light would not be able to escape. The effect of gravity on light was then explored by Johann Georg von Soldner (1801), who calculated the amount of deflection of a light ray by the Sun, arriving at the Newtonian answer which is half the value predicted by general relativity. All of this early work assumed that light could slow down and fall, which is inconsistent with the modern understanding of light waves.

Once it became accepted that light was an electromagnetic wave, it was clear that the frequency of light should not change from place to place, since waves from a source with a fixed frequency keep the same frequency everywhere. One way around this conclusion would be if time itself were altered if clocks at different points had different rates. This was precisely Einstein's conclusion in 1911. He considered an accelerating box, and noted that according to the special theory of relativity, the clock rate at the "bottom" of the box (the side away from the direction of acceleration) was slower than the clock rate at the "top" (the side toward the direction of acceleration). Indeed, in a frame moving (in formula_49 direction) with velocity formula_50 relative to the rest frame, the clocks at a nearby position formula_51 are ahead by formula_52 (to the first order); so an acceleration formula_30 (that changes speed by formula_54 per time formula_7) makes clocks at the position formula_51 to be ahead by formula_57, that is, tick at a rate 

The equivalence principle implies that this change in clock rate is the same whether the acceleration formula_30 is that of an accelerated frame without gravitational effects, or caused by a gravitational field in a stationary frame. Since acceleration due to gravitational potential formula_60 is formula_61, we get
so – in weak fields – the change formula_63 in the clock rate is equal to formula_64.

Since the light would be slowed down by gravitational time dilation (as seen by outside observer), the regions with lower gravitational potential would act like a medium with higher refractive index causing light to deflect. This reasoning allowed Einstein in 1911 to reproduce the incorrect Newtonian value for the deflection of light. At the time he only considered the time-dilating manifestation of gravity, which is the dominating contribution at non-relativistic speeds; however relativistic objects travel through space a comparable amount as they do though time, so purely spatial curvature becomes just as important. After constructing the full theory of general relativity, Einstein solved in 1915 the full post-Newtonian approximation for the Sun's gravity and calculated the correct amount of light deflection – double the Newtonian value. Einstein's prediction was confirmed by many experiments, starting with Arthur Eddington's 1919 solar eclipse expedition.

The changing rates of clocks allowed Einstein to conclude that light waves change frequency as they move, and the frequency/energy relationship for photons allowed him to see that this was best interpreted as the effect of the gravitational field on the mass–energy of the photon. To calculate the changes in frequency in a nearly static gravitational field, only the time component of the metric tensor is important, and the lowest order approximation is accurate enough for ordinary stars and planets, which are much bigger than their Schwarzschild radius.



Easter Rising

The Easter Rising (), also known as the Easter Rebellion, was an armed insurrection in Ireland during Easter Week in April 1916. The Rising was launched by Irish republicans against British rule in Ireland with the aim of establishing an independent Irish Republic while the United Kingdom was fighting the First World War. It was the most significant uprising in Ireland since the rebellion of 1798 and the first armed conflict of the Irish revolutionary period. Sixteen of the Rising's leaders were executed starting in May 1916. The nature of the executions, and subsequent political developments, ultimately contributed to an increase in popular support for Irish independence.

Organised by a seven-man Military Council of the Irish Republican Brotherhood, the Rising began on Easter Monday, 24 April 1916 and lasted for six days. Members of the Irish Volunteers, led by schoolmaster and Irish language activist Patrick Pearse, joined by the smaller Irish Citizen Army of James Connolly and 200 women of Cumann na mBan, seized strategically important buildings in Dublin and proclaimed the Irish Republic. The British Army brought in thousands of reinforcements as well as artillery and a gunboat. There was street fighting on the routes into the city centre, where the rebels slowed the British advance and inflicted many casualties. Elsewhere in Dublin, the fighting mainly consisted of sniping and long-range gun battles. The main rebel positions were gradually surrounded and bombarded with artillery. There were isolated actions in other parts of Ireland; Volunteer leader Eoin MacNeill had issued a countermand in a bid to halt the Rising, which greatly reduced the extent of the rebel actions.

With much greater numbers and heavier weapons, the British Army suppressed the Rising. Pearse agreed to an unconditional surrender on Saturday 29 April, although sporadic fighting continued briefly. After the surrender, the country remained under martial law. About 3,500 people were taken prisoner by the British and 1,800 of them were sent to internment camps or prisons in Britain. Most of the leaders of the Rising were executed following courts martial. The Rising brought physical force republicanism back to the forefront of Irish politics, which for nearly fifty years had been dominated by constitutional nationalism. Opposition to the British reaction to the Rising contributed to changes in public opinion and the move toward independence, as shown in the December 1918 election in Ireland which was won by the Sinn Féin party, which convened the First Dáil and declared independence.

Of the 485 people killed, 260 were civilians, 143 were British military and police personnel, and 82 were Irish rebels, including 16 rebels executed for their roles in the Rising. More than 2,600 people were wounded. Many of the civilians were killed or wounded by British artillery fire or were mistaken for rebels. Others were caught in the crossfire during firefights between the British and the rebels. The shelling and resulting fires left parts of central Dublin in ruins.

The Acts of Union 1800 united the Kingdom of Great Britain and the Kingdom of Ireland as the United Kingdom of Great Britain and Ireland, abolishing the Irish Parliament and giving Ireland representation in the British Parliament. From early on, many Irish nationalists opposed the union and the continued lack of adequate political representation, along with the British government's handling of Ireland and Irish people, particularly the Great Famine. The union was closely preceded by and formed partly in response to an Irish uprising – whose centenary would prove an influence on the Easter Rising. Three more rebellions ensued: one in 1803, another in 1848 and one in 1867 – all were failures.

Opposition took other forms: constitutional (the Repeal Association; the Home Rule League) and social (disestablishment of the Church of Ireland; the Land League). The Irish Home Rule movement sought to achieve self-government for Ireland, within the United Kingdom. In 1886, the Irish Parliamentary Party under Charles Stewart Parnell succeeded in having the First Home Rule Bill introduced in the British parliament, but it was defeated. The Second Home Rule Bill of 1893 was passed by the House of Commons but rejected by the House of Lords.

After the death of Parnell, younger and more radical nationalists became disillusioned with parliamentary politics and turned toward more extreme forms of separatism. The Gaelic Athletic Association, the Gaelic League, and the cultural revival under W. B. Yeats and Augusta, Lady Gregory, together with the new political thinking of Arthur Griffith expressed in his newspaper "Sinn Féin" and organisations such as the National Council and the Sinn Féin League, led many Irish people to identify with the idea of an independent Gaelic Ireland.

The Third Home Rule Bill was introduced by British Liberal Prime Minister H. H. Asquith in 1912. Irish Unionists, who were overwhelmingly Protestants, opposed it, as they did not want to be ruled by a Catholic-dominated Irish government. Led by Sir Edward Carson and James Craig, they formed the Ulster Volunteers (UVF) in January 1913. The UVF's opposition included arming themselves, in the event that they had to resist by force.

Seeking to defend Home Rule, the Irish Volunteers was formed in November 1913. Although sporting broadly open membership and without avowed support for separatism, the executive branch of the Irish Volunteers – excluding leadership – was dominated by the Irish Republican Brotherhood (IRB) who rose to prominence via the organisation, having had restarted recuritment in 1909. These members feared that Home Rule's enactment would result in a broad, seemingly perpetual, contentment with the British Empire. Another militant group, the Irish Citizen Army, was formed by trade unionists as a result of the Dublin Lock-out of that year. The issue of Home Rule, appeared to some, as the basis of an "imminent civil war".

Although the Third Home Rule Bill was eventually enacted, the outbreak of the First World War resulted in its implementation being postponed for the war's duration. It was widely believed at the time that the war would not last more than a few months. The Irish Volunteers split. The vast majority – thereafter known as the National Volunteers – enlisted in the British Army. The minority that objected – retaining the name – did so in accordance with separatist principles, official policy thus becoming "the abolition of the system of governing Ireland through Dublin Castle and the British military power and the establishment of a National Government in its place"; the Volunteers believed that "England’s difficulty" was "Ireland’s opportunity".

The Supreme Council of the IRB met on 5 September 1914, just over a month after the British government had declared war on Germany. At this meeting, they elected to stage an uprising before the war ended and to secure help from Germany. Responsibility for the planning of the rising was given to Tom Clarke and Seán Mac Diarmada. Patrick Pearse, Michael Joseph O’Rahilly, Joseph Plunkett and Bulmer Hobson would assume general control of the Volunteers by March 1915.

In May 1915, Clarke and Mac Diarmada established a Military Council within the IRB, consisting of Pearse, Plunkett and Éamonn Ceannt – and soon themselves – to devise plans for a rising. The Military Council functioned independently and in opposition to those who considered a possible uprising inopportune. Volunteer Chief-of-Staff Eoin MacNeill supported a rising only if the British government attempted to suppress the Volunteers or introduce conscription in Ireland, and if such a rising had some chance of success. Hobson and IRB President Denis McCullough held similar views as did much of the executive branches of both organisations.

The Military Council kept its plans secret, so as to prevent the British authorities from learning of the plans, and to thwart those within the organisation who might try to stop the rising. The secrecy of the plans was such that the Military Council largely superseded the IRB's Supreme Council with even McCullough being unaware of some of the plans, whereas the likes of MacNeill were only informed as the Rising rapidly approached. Although most Volunteers were oblivious to any plans their training increased in the preceding year. The public nature of their training hightened tensions with authorities, which, come the next year, manifested in rumours of the Rising. Public displays likewise existed in the espousal of anti-recruitiment. The number of Volunteers also increased: between December 1914 and February 1916 the rank and file rose from 9,700 to 12,215. Although the likes of the civil servants were discouraged from joining the Volunteers, the organisation was permitted by law.

Shortly after the outbreak of World War I, Roger Casement and John Devoy went to Germany and began negotiations with the German government and military. Casement – later accompanied by Plunkett – persuaded the Germans to announce their support for Irish independence in November 1914. Casement envisioned the recruitment of Irish prisoners of war, to be known as the Irish Brigade, aided by a German expeditionary force who would secure the line of the River Shannon, before advancing on the capital. Neither intention came to fruition, but the German military did agree to ship arms and ammunition to the Volunteers, gunrunning having become difficult and dangerous on account of the war.

Head of the Irish Citizen Army, James Connolly, was unaware of the IRB's plans, and threatened to start a rebellion on his own if other parties failed to act. The IRB leaders met with Connolly in Dolphin's Barn in January 1916 and convinced him to join forces with them. They agreed that they would launch a rising together at Easter and made Connolly the sixth member of the Military Council. Thomas MacDonagh would later become the seventh and final member.

The death of the old Fenian leader Jeremiah O'Donovan Rossa in New York City in August 1915 was an opportunity to mount a spectacular demonstration. His body was sent to Ireland for burial in Glasnevin Cemetery, with the Volunteers in charge of arrangements. Huge crowds lined the route and gathered at the graveside. Pearse made a dramatic funeral oration, a rallying call to republicans, which ended with the words "Ireland unfree shall never be at peace".

In early April, Pearse issued orders to the Irish Volunteers for three days of "parades and manoeuvres" beginning on Easter Sunday. He had the authority to do this, as the Volunteers' Director of Organisation. The idea was that IRB members within the organisation would know these were orders to begin the rising, while men such as MacNeill and the British authorities would take it at face value.

On 9 April, the German Navy dispatched the SS "Libau" for County Kerry, disguised as the Norwegian ship "Aud". It was loaded with 20,000 rifles, one million rounds of ammunition, and explosives. Casement also left for Ireland aboard the German submarine "U-19". He was disappointed with the level of support offered by the Germans and he intended to stop or at least postpone the rising. During this time, the Volunteers amassed ammunition from various sources, including the adolescent Michael McCabe.

On Wednesday 19 April, Alderman Tom Kelly, a Sinn Féin member of Dublin Corporation, read out at a meeting of the corporation a document purportedly leaked from Dublin Castle, detailing plans by the British authorities to shortly arrest leaders of the Irish Volunteers, Sinn Féin and the Gaelic League, and occupy their premises. Although the British authorities said the "Castle Document" was fake, MacNeill ordered the Volunteers to prepare to resist. Unbeknownst to MacNeill, the document had been forged by the Military Council to persuade moderates of the need for their planned uprising. It was an edited version of a real document outlining British plans in the event of conscription. That same day, the Military Council informed senior Volunteer officers that the rising would begin on Easter Sunday. However, it chose not to inform the rank-and-file, or moderates such as MacNeill, until the last minute.

The following day, MacNeill got wind that a rising was about to be launched and threatened to do everything he could to prevent it, short of informing the British. He and Hobson confronted Pearse, but refrained from decisive action as to avoiding instigating a rebellion of any kind; Hobson would be detained by Volunteers until the Rising occurred.

The "SS Libau" (disguised as the "Aud") and the "U-19" reached the coast of Kerry on Good Friday, 21 April. This was earlier than the Volunteers expected and so none were there to meet the vessels. The Royal Navy had known about the arms shipment and intercepted the "SS Libau", prompting the captain to scuttle the ship. Furthermore, Casement was captured shortly after he landed at Banna Strand.

When MacNeill learned that the arms shipment had been lost, he reverted to his original position. With the support of other leaders of like mind, notably Bulmer Hobson and The O'Rahilly, he issued a countermand to all Volunteers, cancelling all actions for Sunday. This countermanding order was relayed to Volunteer officers and printed in the Sunday morning newspapers. The order resulted in a delay to the rising by a day, and some confusion over strategy for those who took part.

British Naval Intelligence had been aware of the arms shipment, Casement's return, and the Easter date for the rising through radio messages between Germany and its embassy in the United States that were intercepted by the Royal Navy and deciphered in Room 40 of the Admiralty. It is unclear how extensive Room 40's decryptions preceding the Rising were. On the eve of the Rising, John Dillon wrote to Redmond of Dublin being "full of most extraordinary rumours. And I have no doubt in my mind that the Clan men – are planning some devilish business – what it is I cannot make out. It may not come off – But you must not be surprised if something very unpleasant and mischievous happens this week".

The information was passed to the Under-Secretary for Ireland, Sir Matthew Nathan, on 17 April, but without revealing its source; Nathan was doubtful about its accuracy. When news reached Dublin of the capture of the "SS Libau" and the arrest of Casement, Nathan conferred with the Lord Lieutenant, Lord Wimborne. Nathan proposed to raid Liberty Hall, headquarters of the Citizen Army, and Volunteer properties at Father Matthew Park and at Kimmage, but Wimborne insisted on wholesale arrests of the leaders. It was decided to postpone action until after Easter Monday, and in the meantime, Nathan telegraphed the Chief Secretary, Augustine Birrell, in London seeking his approval. By the time Birrell cabled his reply authorising the action, at noon on Monday 24 April 1916, the Rising had already begun.

On the morning of Easter Sunday, 23 April, the Military Council met at Liberty Hall to discuss what to do in light of MacNeill's countermanding order. They decided that the Rising would go ahead the following day, Easter Monday, and that the Irish Volunteers and Irish Citizen Army would go into action as the 'Army of the Irish Republic'. They elected Pearse as president of the Irish Republic, and also as Commander-in-Chief of the army; Connolly became Commandant of the Dublin Brigade. That weekend was largely spent preparing rations and manufacturing ammunition and bombs. Messengers were then sent to all units informing them of the new orders.

On the morning of Monday 24 April, about 1,200 members of the Irish Volunteers and Irish Citizen Army mustered at several locations in central Dublin. Among them were members of the all-female Cumann na mBan. Some wore Irish Volunteer and Citizen Army uniforms, while others wore civilian clothes with a yellow Irish Volunteer armband, military hats, and bandoliers. They were armed mostly with rifles (especially 1871 Mausers), but also with shotguns, revolvers, a few Mauser C96 semi-automatic pistols, and grenades. The number of Volunteers who mobilised was much smaller than expected. This was due to MacNeill's countermanding order, and the fact that the new orders had been sent so soon beforehand. However, several hundred Volunteers joined the Rising after it began.

Shortly before midday, the rebels began to seize important sites in central Dublin. The rebels' plan was to hold Dublin city centre. This was a large, oval-shaped area bounded by two canals: the Grand to the south and the Royal to the north, with the River Liffey running through the middle. On the southern and western edges of this district were five British Army barracks. Most of the rebels' positions had been chosen to defend against counter-attacks from these barracks. The rebels took the positions with ease. Civilians were evacuated and policemen were ejected or taken prisoner. Windows and doors were barricaded, food and supplies were secured, and first aid posts were set up. Barricades were erected on the streets to hinder British Army movement.

A joint force of about 400 Volunteers and the Citizen Army gathered at Liberty Hall under the command of Commandant James Connolly. This was the headquarters battalion, and it also included Commander-in-Chief Patrick Pearse, as well as Tom Clarke, Seán Mac Diarmada and Joseph Plunkett. They marched to the General Post Office (GPO) on O'Connell Street, Dublin's main thoroughfare, occupied the building and hoisted two republican flags. Pearse stood outside and read the Proclamation of the Irish Republic. Copies of the Proclamation were also pasted on walls and handed out to bystanders by Volunteers and newsboys. The GPO would be the rebels' headquarters for most of the Rising. Volunteers from the GPO also occupied other buildings on the street, including buildings overlooking O'Connell Bridge. They took over a wireless telegraph station and sent out a radio broadcast in Morse code, announcing that an Irish Republic had been declared. This was the first radio broadcast in Ireland.

Elsewhere, some of the headquarters battalion under Michael Mallin occupied St Stephen's Green, where they dug trenches and barricaded the surrounding roads. The 1st battalion, under Edward 'Ned' Daly, occupied the Four Courts and surrounding buildings, while a company under Seán Heuston occupied the Mendicity Institution, across the River Liffey from the Four Courts. The 2nd battalion, under Thomas MacDonagh, occupied Jacob's biscuit factory. The 3rd battalion, under Éamon de Valera, occupied Boland's Mill and surrounding buildings. The 4th battalion, under Éamonn Ceannt, occupied the South Dublin Union and the distillery on Marrowbone Lane. From each of these garrisons, small units of rebels established outposts in the surrounding area. 

The rebels also attempted to cut transport and communication links. As well as erecting roadblocks, they took control of various bridges and cut telephone and telegraph wires. Westland Row and Harcourt Street railway stations were occupied, though the latter only briefly. The railway line was cut at Fairview and the line was damaged by bombs at Amiens Street, Broadstone, Kingsbridge and Lansdowne Road.

Around midday, a small team of Volunteers and Fianna Éireann members swiftly captured the Magazine Fort in the Phoenix Park and disarmed the guards. The goal was to seize weapons and blow up the ammunition store to signal that the Rising had begun. They seized weapons and planted explosives, but the blast was not loud enough to be heard across the city. The 23-year-old son of the fort's commander was fatally shot when he ran to raise the alarm.

A contingent under Seán Connolly occupied Dublin City Hall and adjacent buildings. They attempted to seize neighbouring Dublin Castle, the heart of British rule in Ireland. As they approached the gate a lone and unarmed police sentry, James O'Brien, attempted to stop them and was shot dead by Connolly. According to some accounts, he was the first casualty of the Rising. The rebels overpowered the soldiers in the guardroom but failed to press further. The British Army's chief intelligence officer, Major Ivon Price, fired on the rebels while the Under-Secretary for Ireland, Sir Matthew Nathan, helped shut the castle gates. Unbeknownst to the rebels, the Castle was lightly guarded and could have been taken with ease. The rebels instead laid siege to the Castle from City Hall. Fierce fighting erupted there after British reinforcements arrived. The rebels on the roof exchanged fire with soldiers on the street. Seán Connolly was shot dead by a sniper, becoming the first rebel casualty. By the following morning, British forces had re-captured City Hall and taken the rebels prisoner.

The rebels did not attempt to take some other key locations, notably Trinity College, in the heart of the city centre and defended by only a handful of armed unionist students. Failure to capture the telephone exchange in Crown Alley left communications in the hands of the Government with GPO staff quickly repairing telephone wires that had been cut by the rebels. The failure to occupy strategic locations was attributed to lack of manpower. In at least two incidents, at Jacob's and Stephen's Green, the Volunteers and Citizen Army shot dead civilians trying to attack them or dismantle their barricades. Elsewhere, they hit civilians with their rifle butts to drive them off.

The British military were caught totally unprepared by the Rising and their response of the first day was generally un-coordinated. Two squadrons of British cavalry were sent to investigate what was happening. They took fire and casualties from rebel forces at the GPO and at the Four Courts. As one troop passed Nelson's Pillar, the rebels opened fire from the GPO, killing three cavalrymen and two horses and fatally wounding a fourth man. The cavalrymen retreated and were withdrawn to barracks. On Mount Street, a group of Volunteer Training Corps men stumbled upon the rebel position and four were killed before they reached Beggars Bush Barracks.

The only substantial combat of the first day of the Rising took place at the South Dublin Union where a piquet from the Royal Irish Regiment encountered an outpost of Éamonn Ceannt's force at the northwestern corner of the South Dublin Union. The British troops, after taking some casualties, managed to regroup and launch several assaults on the position before they forced their way inside and the small rebel force in the tin huts at the eastern end of the Union surrendered. However, the Union complex as a whole remained in rebel hands. A nurse in uniform, Margaret Keogh, was shot dead by British soldiers at the Union. She is believed to have been the first civilian killed in the Rising.

Three unarmed Dublin Metropolitan Police were shot dead on the first day of the Rising and their Commissioner pulled them off the streets. Partly as a result of the police withdrawal, a wave of looting broke out in the city centre, especially in the area of O'Connell Street (still officially called "Sackville Street" at the time). 

Lord Wimborne, the Lord Lieutenant, declared martial law on Tuesday evening and handed over civil power to Brigadier-General William Lowe. British forces initially put their efforts into securing the approaches to Dublin Castle and isolating the rebel headquarters, which they believed was in Liberty Hall. The British commander, Lowe, worked slowly, unsure of the size of the force he was up against, and with only 1,269 troops in the city when he arrived from the Curragh Camp in the early hours of Tuesday 25 April. City Hall was taken from the rebel unit that had attacked Dublin Castle on Tuesday morning.

In the early hours of Tuesday, 120 British soldiers, with machine guns, occupied two buildings overlooking St Stephen's Green: the Shelbourne Hotel and United Services Club. At dawn they opened fire on the Citizen Army occupying the green. The rebels returned fire but were forced to retreat to the Royal College of Surgeons building. They remained there for the rest of the week, exchanging fire with British forces.

Fighting erupted along the northern edge of the city centre on Tuesday afternoon. In the northeast, British troops left Amiens Street railway station in an armoured train, to secure and repair a section of damaged tracks. They were attacked by rebels who had taken up position at Annesley Bridge. After a two-hour battle, the British were forced to retreat and several soldiers were captured. At Phibsborough, in the northwest, rebels had occupied buildings and erected barricades at junctions on the North Circular Road. The British summoned 18-pounder field artillery from Athlone and shelled the rebel positions, destroying the barricades. After a fierce firefight, the rebels withdrew.

That afternoon Pearse walked out into O'Connell Street with a small escort and stood in front of Nelson's Pillar. As a large crowd gathered, he read out a ',' calling on them to support the Rising.

The rebels had failed to take either of Dublin's two main railway stations or either of its ports, at Dublin Port and Kingstown. As a result, during the following week, the British were able to bring in thousands of reinforcements from Britain and from their garrisons at the Curragh and Belfast. By the end of the week, British strength stood at over 16,000 men. Their firepower was provided by field artillery which they positioned on the Northside of the city at Phibsborough and at Trinity College, and by the patrol vessel "Helga", which sailed up the Liffey, having been summoned from the port at Kingstown. On Wednesday, 26 April, the guns at Trinity College and "Helga" shelled Liberty Hall, and the Trinity College guns then began firing at rebel positions, first at Boland's Mill and then in O'Connell Street. Some rebel commanders, particularly James Connolly, did not believe that the British would shell the 'second city' of the British Empire.

The principal rebel positions at the GPO, the Four Courts, Jacob's Factory and Boland's Mill saw little action. The British surrounded and bombarded them rather than assault them directly. One Volunteer in the GPO recalled, "we did practically no shooting as there was no target". Entertainment ensued within the factory, "everybody merry & cheerful", bar the "occasional sniping", noted one Volunteer. However, where the rebels dominated the routes by which the British tried to funnel reinforcements into the city, there was fierce fighting.

At 5:25 PM Volunteers Eamon Martin, Garry Holohan, Robert Beggs, Sean Cody, Dinny O'Callaghan, Charles Shelley, Peadar Breslin and five others attempted to occupy Broadstone railway station on Church Street, the attack was unsuccessful and Martin was injured.

On Wednesday morning, hundreds of British troops encircled the Mendicity Institution, which was occupied by 26 Volunteers under Seán Heuston. British troops advanced on the building, supported by snipers and machine-gun fire, but the Volunteers put up stiff resistance. Eventually, the troops got close enough to hurl grenades into the building, some of which the rebels threw back. Exhausted and almost out of ammunition, Heuston's men became the first rebel position to surrender. Heuston had been ordered to hold his position for a few hours, to delay the British, but had held on for three days.

Reinforcements were sent to Dublin from Britain and disembarked at Kingstown on the morning of Wednesday 26 April. Heavy fighting occurred at the rebel-held positions around the Grand Canal as these troops advanced towards Dublin. More than 1,000 Sherwood Foresters were repeatedly caught in a crossfire trying to cross the canal at Mount Street Bridge. Seventeen Volunteers were able to severely disrupt the British advance, killing or wounding 240 men. Despite there being alternative routes across the canal nearby, General Lowe ordered repeated frontal assaults on the Mount Street position. The British eventually took the position, which had not been reinforced by the nearby rebel garrison at Boland's Mills, on Thursday, but the fighting there inflicted up to two-thirds of their casualties for the entire week for a cost of just four dead Volunteers. It had taken nearly nine hours for the British to advance .

On Wednesday Linenhall Barracks on Constitution Hill was burnt down under the orders of Commandant Edward Daly to prevent its reoccupation by the British.

The rebel position at the South Dublin Union (site of the present-day St. James's Hospital) and Marrowbone Lane, further west along the canal, also inflicted heavy losses on British troops. The South Dublin Union was a large complex of buildings and there was vicious fighting around and inside the buildings. Cathal Brugha, a rebel officer, distinguished himself in this action and was badly wounded. By the end of the week, the British had taken some of the buildings in the Union, but others remained in rebel hands. British troops also took casualties in unsuccessful frontal assaults on the Marrowbone Lane Distillery.

The third major scene of fighting during the week was in the area of North King Street, north of the Four Courts. The rebels had established strong outposts in the area, occupying numerous small buildings and barricading the streets. From Thursday to Saturday, the British made repeated attempts to capture the area, in what was some of the fiercest fighting of the Rising. As the troops moved in, the rebels continually opened fire from windows and behind chimneys and barricades. At one point, a platoon led by Major Sheppard made a bayonet charge on one of the barricades but was cut down by rebel fire. The British employed machine guns and attempted to avoid direct fire by using makeshift armoured trucks, and by mouse-holing through the inside walls of terraced houses to get near the rebel positions. By the time of the rebel headquarters' surrender on Saturday, the South Staffordshire Regiment under Colonel Taylor had advanced only down the street at a cost of 11 dead and 28 wounded. The enraged troops broke into the houses along the street and shot or bayoneted fifteen unarmed male civilians whom they accused of being rebel fighters.

Elsewhere, at Portobello Barracks, an officer named Bowen Colthurst summarily executed six civilians, including the pacifist nationalist activist, Francis Sheehy-Skeffington. These instances of British troops killing Irish civilians would later be highly controversial in Ireland.

The headquarters garrison at the GPO was forced to evacuate after days of shelling when a fire caused by the shells spread to the GPO. Connolly had been incapacitated by a bullet wound to the ankle and had passed command on to Pearse. The O'Rahilly was killed in a sortie from the GPO. They tunnelled through the walls of the neighbouring buildings in order to evacuate the Post Office without coming under fire and took up a new position in 16 Moore Street. The young Seán McLoughlin was given military command and planned a breakout, but Pearse realised this plan would lead to further loss of civilian life.

On Saturday 29 April, from this new headquarters, Pearse issued an order for all companies to surrender. Pearse surrendered unconditionally to Brigadier-General Lowe. The surrender document read:

The other posts surrendered only after Pearse's surrender order, carried by nurse Elizabeth O'Farrell, reached them. Sporadic fighting, therefore, continued until Sunday, when word of the surrender was got to the other rebel garrisons. Command of British forces had passed from Lowe to General John Maxwell, who arrived in Dublin just in time to take the surrender. Maxwell was made temporary military governor of Ireland.

The Rising was planned to occur across the nation, but MacNeill's countermanding order coupled with the failure to secure German arms hindered this objective significantly. Charles Townshend contended that serious intentions for a national Rising were meagre, being diminished by a focus upon Dublin – although this is an increasingly contentious notion.

In the south, around 1,200 Volunteers commanded by Tomás Mac Curtain mustered on the Sunday in Cork, but they dispersed on Wednesday after receiving nine contradictory orders by dispatch from the Volunteer leadership in Dublin. At their Sheares Street headquarters, some of the Volunteers engaged in a standoff with British forces. Much to the anger of many Volunteers, MacCurtain, under pressure from Catholic clergy, agreed to surrender his men's arms to the British. The only violence in County Cork occurred when the RIC attempted to raid the home of the Kent family. The Kent brothers, who were Volunteers, engaged in a three-hour firefight with the RIC. An RIC officer and one of the brothers were killed, while another brother was later executed. Virtually all rebel family homes were raided, either during or after the Rising.

In the north, Volunteer companies were mobilised in County Tyrone at Coalisland (including 132 men from Belfast led by IRB President Dennis McCullough) and Carrickmore, under the leadership of Patrick McCartan. They also mobilised at Creeslough, County Donegal under Daniel Kelly and James McNulty. However, in part because of the confusion caused by the countermanding order, the Volunteers in these locations dispersed without fighting.

In north County Dublin, about 60 Volunteers mobilised near Swords. They belonged to the 5th Battalion of the Dublin Brigade (also known as the Fingal Battalion), and were led by Thomas Ashe and his second in command, Richard Mulcahy. Unlike the rebels elsewhere, the Fingal Battalion successfully employed guerrilla tactics. They set up camp and Ashe split the battalion into four sections: three would undertake operations while the fourth was kept in reserve, guarding camp and foraging for food. The Volunteers moved against the RIC barracks in Swords, Donabate and Garristown, forcing the RIC to surrender and seizing all the weapons. They also damaged railway lines and cut telegraph wires. The railway line at Blanchardstown was bombed to prevent a troop train from reaching Dublin. This derailed a cattle train, which had been sent ahead of the troop train.

The only large-scale engagement of the Rising, outside Dublin city, was at Ashbourne, County Meath. On Friday, about 35 Fingal Volunteers surrounded the Ashbourne RIC barracks and called on it to surrender, but the RIC responded with a volley of gunfire. A firefight followed, and the RIC surrendered after the Volunteers attacked the building with a homemade grenade. Before the surrender could be taken, up to sixty RIC men arrived in a convoy, sparking a five-hour gun battle, in which eight RIC men were killed and 18 wounded. Two Volunteers were also killed and five wounded, and a civilian was fatally shot. The RIC surrendered and were disarmed. Ashe let them go after warning them not to fight against the Irish Republic again. Ashe's men camped at Kilsalaghan near Dublin until they received orders to surrender on Saturday. The Fingal Battalion's tactics during the Rising foreshadowed those of the IRA during the War of Independence that followed.

Volunteer contingents also mobilised nearby in counties Meath and Louth but proved unable to link up with the North Dublin unit until after it had surrendered. In County Louth, Volunteers shot dead an RIC man near the village of Castlebellingham on 24 April, in an incident in which 15 RIC men were also taken prisoner.

In County Wexford, 100–200 Volunteers—led by Robert Brennan, Séamus Doyle and Seán Etchingham—took over the town of Enniscorthy on Thursday 27 April until Sunday. Volunteer officer Paul Galligan had cycled 200 km from rebel headquarters in Dublin with orders to mobilise. They blocked all roads into the town and made a brief attack on the RIC barracks, but chose to blockade it rather than attempt to capture it. They flew the tricolour over the Athenaeum building, which they had made their headquarters, and paraded uniformed in the streets. They also occupied Vinegar Hill, where the United Irishmen had made a last stand in the 1798 rebellion. The public largely supported the rebels and many local men offered to join them.

By Saturday, up to 1,000 rebels had been mobilised, and a detachment was sent to occupy the nearby village of Ferns. In Wexford, the British assembled a column of 1,000 soldiers (including the Connaught Rangers), two field guns and a 4.7 inch naval gun on a makeshift armoured train. On Sunday, the British sent messengers to Enniscorthy, informing the rebels of Pearse's surrender order. However, the Volunteer officers were sceptical. Two of them were escorted by the British to Arbour Hill Prison, where Pearse confirmed the surrender order.

In County Galway, 600–700 Volunteers mobilised on Tuesday under Liam Mellows. His plan was to "bottle up the British garrison and divert the British from concentrating on Dublin". However, his men were poorly armed, with only 25 rifles, 60 revolvers, 300 shotguns and some homemade grenades – many of them only had pikes. Most of the action took place in a rural area to the east of Galway city. They made unsuccessful attacks on the RIC barracks at Clarinbridge and Oranmore, captured several officers, and bombed a bridge and railway line, before taking up position near Athenry. There was also a skirmish between rebels and an RIC mobile patrol at Carnmore crossroads. A constable, Patrick Whelan, was shot dead after he had called to the rebels: "Surrender, boys, I know ye all".

On Wednesday, arrived in Galway Bay and shelled the countryside on the northeastern edge of Galway. The rebels retreated southeast to Moyode, an abandoned country house and estate. From here they set up lookout posts and sent out scouting parties. On Friday, landed 200 Royal Marines and began shelling the countryside near the rebel position. The rebels retreated further south to Limepark, another abandoned country house. Deeming the situation to be hopeless, they dispersed on Saturday morning. Many went home and were arrested following the Rising, while others, including Mellows, went "on the run". By the time British reinforcements arrived in the west, the Rising there had already disintegrated.

In County Limerick, 300 Irish Volunteers assembled at Glenquin Castle near Killeedy, but they did not take any military action.

In County Clare, Micheal Brennan marched with 100 Volunteers (from Meelick, Oatfield, and Cratloe) to the River Shannon on Easter Monday to await orders from the Rising leaders in Dublin, and weapons from the expected Casement shipment. However, neither arrived and no actions were taken.

The Easter Rising resulted in at least 485 deaths, according to the Glasnevin Trust.
Of those killed:

More than 2,600 were wounded; including at least 2,200 civilians and rebels, at least 370 British soldiers and 29 policemen. All 16 police fatalities and 22 of the British soldiers killed were Irishmen. About 40 of those killed were children (under 17 years old), four of whom were members of the rebel forces.

The number of casualties each day steadily rose, with 55 killed on Monday and 78 killed on Saturday. The British Army suffered their biggest losses in the Battle of Mount Street Bridge on Wednesday when at least 30 soldiers were killed. The rebels also suffered their biggest losses on that day. The RIC suffered most of their casualties in the Battle of Ashbourne on Friday.

The majority of the casualties, both killed and wounded, were civilians. Most of the civilian casualties and most of the casualties overall were caused by the British Army. This was due to the British using artillery, incendiary shells and heavy machine guns in built-up areas, as well as their "inability to discern rebels from civilians". One Royal Irish Regiment officer recalled, "they regarded, not unreasonably, every one they saw as an enemy, and fired at anything that moved". Many other civilians were killed when caught in the crossfire. Both sides, British and rebel, also shot civilians deliberately on occasion; for not obeying orders (such as to stop at checkpoints), for assaulting or attempting to hinder them, and for looting. There were also instances of British troops killing unarmed civilians out of revenge or frustration: notably in the North King Street Massacre, where fifteen were killed, and at Portobello Barracks, where six were shot. Furthermore, there were incidents of friendly fire. On 29 April, the Royal Dublin Fusiliers under Company Quartermaster Sergeant Robert Flood shot dead two British officers and two Irish civilian employees of the Guinness Brewery after he decided they were rebels. Flood was court-martialled for murder but acquitted.

According to the historian Fearghal McGarry, the rebels attempted to avoid needless bloodshed. Desmond Ryan stated that Volunteers were told "no firing was to take place except under orders or to repel attack". Aside from the engagement at Ashbourne, policemen and unarmed soldiers were not systematically targeted, and a large group of policemen was allowed to stand at Nelson's Pillar throughout Monday. McGarry writes that the Irish Citizen Army "were more ruthless than Volunteers when it came to shooting policemen" and attributes this to the "acrimonious legacy" of the Dublin Lock-out.

The vast majority of the Irish casualties were buried in Glasnevin Cemetery in the aftermath of the fighting. British families came to Dublin Castle in May 1916 to reclaim the bodies of British soldiers, and funerals were arranged. Soldiers whose bodies were not claimed were given military funerals in Grangegorman Military Cemetery.

In the immediate aftermath, the Rising was commonly described as the "Sinn Féin Rebellion", reflecting a popular belief that Sinn Féin, a separatist organisation that was neither militant nor republican, was behind it. General Maxwell, for example, signalled his intention "to arrest all dangerous Sinn Feiners", including "those who have taken an active part in the movement although not in the present rebellion".

A total of 3,430 men and 79 women were arrested, including 425 people for looting – roughly, 1,500 of these arrests accounted for the rebels. Detainees were overwhelmingly young, Catholic and religious. 1,424 men and 73 women were released after a few weeks of imprisonment; those interned without trial in England and Wales (see below) were released on Christmas Eve, 1916; the remaining majority of convicts were held until June 1917.

A series of courts martial began on 2 May, in which 187 people were tried. Controversially, Maxwell decided that the courts martial would be held in secret and without a defence, which Crown law officers later ruled to have been illegal. Some of those who conducted the trials had commanded British troops involved in suppressing the Rising, a conflict of interest that the Military Manual prohibited. Only one of those tried by courts martial was a woman, Constance Markievicz, who was also the only woman to be kept in solitary confinement. Ninety were sentenced to death. Fifteen of those (including all seven signatories of the Proclamation) had their sentences confirmed by Maxwell and fourteen were executed by firing squad at Kilmainham Gaol between 3 and 12 May.

Maxwell stated that only the "ringleaders" and those proven to have committed "cold-blooded murder" would be executed. However, some of those executed were not leaders and did not kill anyone, such as Willie Pearse and John MacBride; Thomas Kent did not come out at all—he was executed for the killing of a police officer during the raid on his house the week after the Rising. The most prominent leader to escape execution was Éamon de Valera, Commandant of the 3rd Battalion, who did so partly because of his American birth. Hobson went into hiding, re-emerging after the June amnesty, largely to scorn.

Most of the executions took place over a ten-day period:

The arrests greatly affected hundreds of families and communities; anti-English sentiment developed among the public, as separatists declared the arrests as indicative of a draconian approach. The public, at large, feared that the response was "an assault on the entirety of the Irish national cause". This radical transformation was recognised in the moment and had become a point of concern among British authorities; after Connolly's execution, the remaining death sentences were commuted to penal servitude. Growing support for republicanism can be found as early as June 1916; imprisonment largely failed to deter militants – interned rebels would proceed to fight at higher rates than those who weren't – who thereafter quickly reorganised the movement.

Under Regulation 14B of the Defence of the Realm Act 1914 1,836 men were interned at internment camps and prisons in England and Wales. As urban areas were becoming the nexus for republicanism, Internees were largely from such areas. Many Internees had not taken part in the Rising; many thereafter became sympathetic to the nationalist cause. 

Internees occupied themselves with the likes of lectures, craftwork, music and sports. These activities – which included games of Gaelic football, crafting of Gaelic symbols, and lessons in Irish – regularly had a nationalist character and the cause itself developed a sense of cohesion within the camps. The military studies included discussion of the Rising. Interment lasted until December of that year with releases having started in July. Martial law had ceased by the end of November.

Casement was tried in London for high treason and hanged at Pentonville Prison on 3 August.

On Tuesday 25 April, Dubliner Francis Sheehy-Skeffington, a pacifist nationalist activist, was arrested and then taken as hostage and human shield by Captain John Bowen-Colthurst; that night Bowen-Colthurst shot dead a teenage boy. Skeffington was executed the next day – alongside two journalists. Two hours later, Bowen-Colthurst captured the Labour Party councillor and IRB lieutenant, Richard O'Carroll and had him shot in the street. Major Sir Francis Vane raised concerns over Bowen-Colthurst's actions and saw to him being court martialled. Bowen-Colthurst was found guilty but insane and was sentenced to an insane asylum. Owing to political pressure, an inquiry soon transpired, revealing the murders and their cover-up. The killing of Skeffington and others provoked outrage among citizens.

The other incident was the "North King Street Massacre". On the night of 28–29 April, British soldiers of the South Staffordshire Regiment, under Colonel Henry Taylor, had burst into houses on North King Street and killed fifteen male civilians whom they accused of being rebels. The soldiers shot or bayoneted the victims, and then secretly buried some of them in cellars or backyards after robbing them. The area saw some of the fiercest fighting of the Rising and the British had taken heavy casualties for little gain. Maxwell attempted to excuse the killings and argued that the rebels were ultimately responsible. He claimed that "the rebels wore no uniform" and that the people of North King Street were rebel sympathisers. Maxwell concluded that such incidents "are absolutely unavoidable in such a business as this" and that "under the circumstance the troops [...] behaved with the greatest restraint". A private brief, prepared for the Prime Minister, said the soldiers "had orders not to take any prisoners" but took it to mean they were to shoot any suspected rebel. The City Coroner's inquest found that soldiers had killed "unarmed and unoffending" residents. The military court of inquiry ruled that no specific soldiers could be held responsible, and no action was taken.

A Royal Commission was set up to enquire into the causes of the Rising. It began hearings on 18 May under the chairmanship of Lord Hardinge of Penshurst. The Commission heard evidence from Sir Matthew Nathan, Augustine Birrell, Lord Wimborne, Sir Neville Chamberlain (Inspector-General of the Royal Irish Constabulary), General Lovick Friend, Major Ivor Price of Military Intelligence and others. The report, published on 26 June, was critical of the Dublin administration, saying that "Ireland for several years had been administered on the principle that it was safer and more expedient to leave the law in abeyance if collision with any faction of the Irish people could thereby be avoided." Birrell and Nathan had resigned immediately after the Rising. Wimborne resisted the pressure to resign, but was recalled to London by Asquith. He was re-appointed in July 1916. Chamberlain also resigned.

At first, many Dubliners were bewildered by the outbreak of the Rising. James Stephens, who was in Dublin during the week, thought, "None of these people were prepared for Insurrection. The thing had been sprung on them so suddenly they were unable to take sides." Eyewitnesses compared the ruin of Dublin with the destruction of towns in Europe in the war: the physical damage, which included over ninety fires, was largely confined to Sackville Street. In the immediate aftermath, the Irish government was in disarray. 

There was great hostility towards the Volunteers in some parts of the city which escalated to physical violence in some instances. Historian Keith Jeffery noted that most of the opposition came from the dependents of British Army personnel. The death and destruction, which resulted in disrupted trade, considerable looting and unemployment, contributed to the antagonism of the Volunteers, who were denounced as "murderers" and "starvers of the people" – the monetary consequences of the Rising were estimated to be at £2,500,000. International aid was supplied to residents – nationalists aided the dependents of Volunteers. The British Government compensated the consequences to the sum of £2,500,000.

Support for the rebels did exist among Dubliners, expressed through both crowds cheering at prisoners and reverent silence. With martial law seeing this expression prosecuted, many would-be supporters elected to remain silent although "a strong undercurrent of disloyalty" was still felt. Drawing upon this support, and amidst the deluge of nationalist ephemera, the significantly popular "Catholic Bulletin" eulogised Volunteers killed in action and implored readers to donate; entertainment was offered as an extension of those intentions, targeting local sectors to great success. The "Bulletin"'s Catholic character allowed it to evade the widespread censorship of press and seizure of republican propaganda; it therefore exposed many unaware readers to such propaganda.

A meeting called by Count Plunkett on 19 April 1917 led to the formation of a broad political movement under the banner of Sinn Féin which was formalised at the Sinn Féin Ard Fheis of 25 October 1917. The Conscription Crisis of 1918 further intensified public support for Sinn Féin before the general elections to the British Parliament on 14 December 1918, which resulted in a landslide victory for Sinn Féin, winning 73 seats out of 105, whose Members of Parliament (MPs) gathered in Dublin on 21 January 1919 to form Dáil Éireann and adopt the Declaration of Independence. 

During that election, they drew directly upon the Rising and their popularity was significantly accreditable to that association, one that accrued political prestige until the end of the century. Many participants of the Rising would soon assume electoral positions. Sinn Féin served as an alternative to the Irish Parliamentary Party whose support for British establishments alienated voters.

Sinn Féin would become closely aligned with the Irish Republican Army, who sought to continue the IRB's ideals and waged armed conflict against British forces.

1916 – containing both the Rising and the Battle of the Somme, events paramount to the memory of Irish Republicans and Ulster Unionists, respectively – had a profound effect on Ireland and is remembered accordingly. The Rising was among the events that ended colonial rule in Ireland, succeeded by the Irish War of Independence. The legacy of the Rising possess many dimensions although the declaration of the Republic and the ensuing executions remain focal points. 

Annual parades in celebration of the Rising occurred for many years, however, ceased after The Troubles in Northern Ireland began, being seen as supportive of republican paramilitary violence – the Rising is a common feature of republican murals in Northern Ireland. These commemorations celebrated the Rising as the origin of the Irish state, a stance reiterated through extensive analysis. Unionists contend that the Rising was an illegal attack on the British State that should not be celebrated. Revivalism of the parades has inspired significant public debate, although the centenary of the Rising, which featured the likes of ceremonies and memorials, was largely successful and praised for its sensitivity.

The leaders of the Rising were "instantly apotheosized" and remembrance was situated within a larger republican tradition of claimed martyrdom – the Catholic Church would contend this narrative as the foundational myth of the Irish Free State, assuming a place within the remembrance as an association between republicanism and Catholicism grew. The "Pearsean combination of Catholicism, Gaelicism, and spiritual nationalism" would become dominant within republicanism. Within the Free State, the Rising was sanctified by officials, positioned as a "highly disciplined military operation". Historians largely agree that the Rising succeeded by offering a symbolic display of sacrifice, while the military action was a considerable failure. As Monk Gibbon remarked, the "shots from khaki-uniformed firing parties did more to create the Republic of Ireland than any shot fired by a Volunteer in the course of Easter week".

Literature surrounding the Rising was significant: MacDonagh, Plunkett, and Pearse were themselves poets, whose ideals were granted a spiritual dimension in their work; Arnold Bax, Francis Ledwidge, George William Russell and W. B. Yeats responded through verse that ranged from endorsement to elegies. Although James Joyce was ambivalent to the insurgence, metaphors of and imagery consistent with the Rising appear in his later work. Hugh Leonard, Denis Johnston, Tom Murphy, Roddy Doyle and Sorley MacLean are among writers would later invoke the Rising. Now extensively dramatised, its theatricality was identified in the moment and has been stressed in its remembrance. Literary and political evocation position the Rising as a "watershed moment" central to Irish history. 

Black, Basque, Breton, Catalan and Indian nationalists have drawn upon the Rising and its consequences. For the latter, Jawaharlal Nehru noted, the symbolic display was the appeal, that of the transcendent, "invincible spirit of a nation"; such was broadly appealing in America, where diasporic, occasionally socialist, nationalism occurred. Vladimir Lenin was effusive, ascribing its anti-imperialism a singular significance within geopolitics – his only misgiving was its estrangement from the broader wave of revolution occurring.

During the Troubles, significant revisionism of the Rising occurred. Revisionists contended that it was not a "heroic drama" as thought but rather informed the violence transpiring, by having legitimised a "cult of 'blood sacrifice'". With the advent of a Provisional IRA ceasefire and the beginning of what became known as the Peace Process during the 1990s, the government's view of the Rising grew more positive and in 1996 an 80th anniversary commemoration at the Garden of Remembrance in Dublin was attended by the Taoiseach and leader of Fine Gael, John Bruton.




Eschrichtiidae

Eschrichtiidae or the gray whales is a family of baleen whale (Parvorder Mysticeti) with a single extant species, the gray whale ("Eschrichtius robustus"), as well as three described fossil genera: "Archaeschrichtius" and "Eschrichtioides" from the Miocene and Pliocene of Italy respectively, and "Gricetoides" from the Pliocene of North Carolina. More recent phylogenetic studies have found this family to be invalid, with its members nesting inside the Balaenopteridae. The names of the extant genus and the family honours Danish zoologist Daniel Eschricht.

In his morphological analysis, found that eschrichtiids and Cetotheriidae ("Cetotherium", "Mixocetus" and "Metopocetus") form a monophyletic sister group of Balaenopteridae.

A specimen from the Late Pliocene of northern Italy, named ""Cetotherium" gastaldii" by and renamed ""Balaenoptera" gastaldii" by , was identified as a basal eschrichtiid by who recombined it to "Eschrichtioides gastaldii".

Fossils of Eschrichtiidae have been found in all major oceanic basins in the Northern Hemisphere, and the family is believed date back to the Late Miocene. Today, gray whales are only present in the northern Pacific, but a population was also present in the northern Atlantic before being driven to extinction by European whalers three centuries ago.

Fossil eschrichtiids from before the Holocene are rare compared to other fossil mysticetes. The only Pleistocene fossil from the Pacific referred to "E. eschrichtius" is a partial skeleton and an associated skull from California, estimated to be about 200 thousand years old. However, a late Pliocene fossil from Hokkaido, Japan, referred to "Eschrichtius" sp. is estimated to be and a similar unnamed fossil has been reported from California.

In their description of "Archaeschrichtius ruggieroi" from the late Miocene of Italy, argued that eschrichtiids most likely originated in the Mediterranean Basin about and remained there, either permanently or intermittently, at least until the Early Pliocene (5–3 Mya), (but see Messinian salinity crisis.)



Edmund I

Edmund I or Eadmund I (920/921 – 26 May 946) was King of the English from 27 October 939 until his death. He was the elder son of King Edward the Elder and his third wife, Queen Eadgifu, and a grandson of King Alfred the Great. After Edward died in 924, he was succeeded by his eldest son, Edmund's half-brother Æthelstan. Edmund was crowned after Æthelstan died childless in 939. He had two sons, Eadwig and Edgar, by his first wife Ælfgifu, and none by his second wife Æthelflæd. His sons were young children when he was killed in a brawl with an outlaw at Pucklechurch in Gloucestershire, and he was succeeded by his younger brother Eadred, who died in 955 and was followed by Edmund's sons in succession.

Æthelstan had succeeded as the king of England south of the Humber and he became the first king of all England when he conquered Viking-ruled York in 927, but after his death Anlaf Guthfrithson was accepted as King of York and extended Viking rule to the Five Boroughs of north-east Mercia. Edmund was initially forced to accept the reverse, the first major setback for the West Saxon dynasty since Alfred's reign, but he was able to recover his position following Anlaf's death in 941. In 942 Edmund took back control of the Five Boroughs and in 944 he regained control over the whole of England when he expelled the Viking kings of York. Eadred had to deal with further revolts when he became king, and York was not finally conquered until 954. Æthelstan had achieved a dominant position over other British kings and Edmund maintained this, perhaps apart from Scotland. The north Welsh king Idwal Foel may have allied with the Vikings as he was killed by the English in 942. The British kingdom of Strathclyde may also have sided with the Vikings as Edmund ravaged it in 945 and then ceded it to Malcolm I of Scotland. Edmund also continued his brother's friendly relations with Continental rulers, several of whom were married to his half-sisters.

Edmund inherited his brother's interests and leading advisers, such as Oda, whom he appointed Archbishop of Canterbury in 941, Æthelstan Half-King, ealdorman of East Anglia, and Ælfheah the Bald, Bishop of Winchester. Government at the local level was mainly carried on by ealdormen, and Edmund made substantial changes in personnel during his reign, with a move from Æthelstan's main reliance on West Saxons to a greater prominence of men with Mercian connections. Unlike the close relatives of previous kings, his mother and brother attested many of Edmund's charters, suggesting a high degree of family cooperation. Edmund was also an active legislator, and three of his codes survive. Provisions include ones which attempt to regulate feuds and emphasise the sanctity of the royal person.

The major religious movement of the tenth century, the English Benedictine Reform, reached its peak under Edgar, but Edmund's reign was important in its early stages. He appointed Dunstan abbot of Glastonbury, where he was joined by Æthelwold. They were to be two of the leaders of the reform and they made the abbey the first important centre for disseminating it. Unlike the circle of his son Edgar, Edmund did not take the view that Benedictine monasticism was the only worthwhile religious life, and he also patronised unreformed (non-Benedictine) establishments.

In the ninth century the four Anglo-Saxon kingdoms of Wessex, Mercia, Northumbria and East Anglia came under increasing attack from Vikings, culminating in invasion by the Great Heathen Army in 865. By 878, the Vikings had overrun East Anglia, Northumbria, and Mercia, and nearly conquered Wessex, but in that year the West Saxons fought back under Alfred the Great and achieved a decisive victory at the Battle of Edington. In the 880s and 890s the Anglo-Saxons ruled Wessex and western Mercia, but the rest of England was under Viking kings. Alfred constructed a network of fortresses, and these helped him to frustrate renewed Viking attacks in the 890s with the assistance of his son-in-law, Æthelred, Lord of the Mercians, and his elder son Edward, who became king when Alfred died in 899. In 909 Edward sent a force of West Saxons and Mercians to attack the Northumbrian Danes, and the following year the Danes retaliated with a raid on Mercia. While they were marching back to Northumbria, they were caught by an Anglo-Saxon army and decisively defeated at the Battle of Tettenhall, ending the threat from the Northumbrian Vikings for a generation. In the 910s Edward and Æthelflæd, his sister and Æthelred's widow, extended Alfred's network of fortresses and conquered Viking-ruled eastern Mercia and East Anglia. When Edward died in 924, he controlled all England south of the Humber.

Edward was succeeded by his eldest son Æthelstan, who seized control of Northumbria in 927, thus becoming the first king of all England. He then styled himself in charters as king of the English, and soon afterwards Welsh kings and the kings of Scotland and Strathclyde acknowledged his overlordship. After this, he adopted more grandiose titles such as (king of the whole of Britain). In 934 he invaded Scotland and in 937 an alliance of armies of Scotland, Strathclyde and the Vikings invaded England. Æthelstan secured a decisive victory at the Battle of Brunanburh, cementing his dominant position in Britain.

Benedictine monasticism had flourished in England in the seventh and eighth centuries, but it severely declined in the late eighth and ninth centuries. By the time Alfred came to the throne in 871, monasteries and knowledge of Latin were at a low ebb, but there was a gradual revival from Alfred's time onwards. This accelerated during Æthelstan's reign, and two leaders of the later tenth-century English Benedictine Reform, Dunstan and Æthelwold, reached maturity in Æthelstan's cosmopolitan, intellectual court of the 930s.

Edmund's father, Edward the Elder, had three wives, eight or nine daughters, several of whom married Continental royalty, and five sons. Æthelstan was the only known son of Edward's first wife, Ecgwynn. His second wife, Ælfflæd, had two sons: Ælfweard, who may have been acknowledged in Wessex as king when his father died in 924 but who died less than a month later, and Edwin, who drowned in 933. In about 919 Edward married Eadgifu, the daughter of Sigehelm, ealdorman of Kent. Edmund, who was born in 920 or 921, was Eadgifu's elder son. Her younger son Eadred succeeded him as king. Edmund had one or two full sisters. Eadburh was a nun at Winchester who was later venerated as a saint. The twelfth-century historian William of Malmesbury gives Edmund a second full sister who married Louis, prince of Aquitaine; she was called Eadgifu, the same name as her mother. William's account is accepted by the historians Ann Williams and Sean Miller, but Æthelstan's biographer Sarah Foot argues that she did not exist, and that William confused her with Ælfgifu, a daughter of Ælfflæd.

Edmund was a young child when his half-brother Æthelstan became king in 924. He grew up at Æthelstan's court, probably with two important Continental exiles, his nephew Louis, future King of the West Franks, and Alain, future Duke of Brittany. According to William of Malmesbury, Æthelstan showed great affection towards Edmund and Eadred: "mere infants at his father's death, he brought them up lovingly in childhood, and when they grew up gave them a share in his kingdom". Edmund may have been a member of the expedition to Scotland in 934 as, according to the (History of Saint Cuthbert), Æthelstan instructed that in the event of his death Edmund was to take his body to Cuthbert's shrine at Chester-le-Street. Edmund fought at the Battle of Brunanburh in 937, and in a poem commemorating the victory in the "Anglo-Saxon Chronicle" ("ASC"), Edmund (prince of the royal house) is given such a prominent role – and praised for his heroism alongside Æthelstan – that the historian Simon Walker has suggested that the poem was written during Edmund's reign. At a royal assembly shortly before Æthelstan's death in 939, Edmund and Eadred attested a grant to their full sister, Eadburh, both as (king's brother). Their attestations may have been because of the family connection, but they also may have been intended to display the throneworthiness of the king's half-brothers when it was known that he did not have long to live. This is the only charter of Æthelstan attested by Edmund, the authenticity of which has not been questioned. Æthelstan died childless on 27 October 939 and Edmund's succession to the throne was undisputed. He was the first king to succeed to the throne of all England, and was probably crowned at Kingston-upon-Thames, perhaps on Advent Sunday, 1 December 939.

Brunanburh saved England from destruction as a united kingdom, and it helped to ensure that Edmund would succeed smoothly to the throne, but it did not preserve him from challenges to his rule once he became king. The chronology of the Viking challenge is disputed, but according to the most widely accepted version, Æthelstan's death encouraged the York Vikings to accept the kingship of Anlaf Guthfrithson, the King of Dublin who had led the Viking forces defeated at Brunanburh. According to "ASC D": "Here the Northumbrians belied their pledges and chose Anlaf from Ireland as their king." Anlaf was in York by the end of 939 and the following year he invaded north-east Mercia, aiming to recover the southern territories of the York kingdom which had been conquered by Edward and Æthelflæd. He marched on Northampton, where he was repulsed, and then stormed the ancient Mercian royal centre of Tamworth, with considerable loss of life on both sides. On his way back north he was caught at Leicester by an army under Edmund, but battle was averted by the mediation of Archbishop Wulfstan of York, on behalf of the Vikings, and probably the Archbishop of Canterbury acting for the English. They arranged a treaty at Leicester which surrendered the Five Boroughs of Lincoln, Leicester, Nottingham, Stamford and Derby, to Guthfrithson. This was the first serious setback for the English since Edward the Elder began to roll back Viking conquests in the early tenth century, and it was described by the historian Frank Stenton as "an ignominious surrender". Guthfrithson had coins struck at York with the lower Viking weight than the English standard.

Guthfrithson died in 941, allowing Edmund to reverse his losses. In 942 he recovered the Five Boroughs, and his victory was considered so significant that it was commemorated by a poem in the "Anglo-Saxon Chronicle":

Like other tenth-century poems in the "Anglo-Saxon Chronicle", this one shows a concern with English nationalism and the West Saxon royal dynasty, and in this case displays the Christian English and Danes as united under Edmund in their victorious opposition to Norse (Norwegian) pagans. Stenton commented that the poem brings out the highly significant fact that the Danes of eastern Mercia, after fifteen years of Æthelstan's government, had come to regard themselves as the rightful subjects of the English king. Above all, it emphasises the antagonism between Danes and Norsemen, which is often ignored by modern writers, but underlies the whole history of England in this period. It is the first political poem in the English language, and its author understood political realities.

However, Williams is sceptical, arguing that the poem is not contemporary, and that it is doubtful whether contemporaries saw their situation in those terms. In the same year Edmund granted large estates in northern Mercia to a leading nobleman, Wulfsige the Black, continuing the policy of his father of granting land in the Danelaw to supporters in order to give them an interest in resisting the Vikings.

Guthfrithson was succeeded as king of York by his cousin, Anlaf Sihtricson, who was baptised in 943 with Edmund as his godfather, suggesting that he accepted West Saxon overlordship. Sihtricson issued his own coinage, but he clearly had rivals in York as coins were also issued there in two other names: Ragnall, a brother of Anlaf Guthfrithson who also accepted baptism under Edmund's sponsorship, and an otherwise unknown Sihtric. The coins of all three men were issued with the same design, which may suggest joint authority. In 944 Edmund expelled the Viking rulers of York and seized control of the city with the assistance of Archbishop Wulfstan, who had previously supported the Vikings, and an ealdorman in Mercia, probably Æthelmund, who had been appointed by Edmund in 940.

When Edmund died, his successor Eadred faced further revolts in Northumbria, which were not finally defeated until 954. In Miller's view, Edmund's reign "shows clearly that although Æthelstan had conquered Northumbria, it was still not really part of a united England, nor would it be until the end of Eadred's reign". The Northumbrians' repeated revolts show that they retained separatist ambitions, which they only abandoned under pressure from successive southern kings. Unlike Æthelstan, Edmund and Eadred rarely claimed jurisdiction over the whole of Britain, although each did sometimes describe himself as 'king of the English' even at times when he did not control Northumbria. In charters Edmund sometimes even called himself by the lesser title of 'king of the Anglo-Saxons' in 940 and 942, and only claimed to be king of all Britain once he had gained full control over Northumbria in 945. He never described himself as on his coinage.

Edmund inherited overlordship over the kings of Wales from Æthelstan, but Idwal Foel, king of Gwynedd in north Wales, apparently took advantage of Edmund's early weakness to withhold fealty and may have supported Anlaf Guthfrithson, as according to the he was killed by the English in 942. Between 942 and 950 his kingdom was conquered by Hywel Dda, the king of Deheubarth in south Wales, who is described by the historian of Wales Thomas Charles-Edwards as "the firmest ally of the English 'emperors of Britain' among all the kings of his day". Attestations of Welsh kings to English charters appear to have been rare compared with those in Æthelstan's reign, but in the historian David Dumville's view there is no reason to doubt that Edmund retained his overlordship over the Welsh kings. In a charter of 944 disposing of land in Devon, Edmund is styled "King of the English and ruler of this British province", suggesting that the former British kingdom of Dumnonia was still not regarded as fully integrated into England, although the historian Simon Keynes "suspects some 'local' interference" in the wording of Edmund's title.

By 945 both Scotland and Strathclyde had kings who had assumed the throne since Brunanburh, and it is likely that whereas Scotland allied with England, Strathclyde held to its alliance with the Vikings. In that year Edmund ravaged Strathclyde. According to the thirteenth-century chronicler Roger of Wendover, the invasion was supported by Hywel Dda, and Edmund had two sons of the king of Strathclyde blinded, perhaps to deprive their father of throneworthy heirs. Edmund then gave the kingdom to Malcolm I of Scotland in return for a pledge to defend it on land and on sea, a decision variously interpreted by historians. Dumville and Charles-Edwards regard it as granting Strathclyde to the Scottish king in return for an acknowledgement of Edmund's overlordship, whereas Williams thinks it probably means that he agreed to Malcolm's overlordship of the area in return for an alliance against the Dublin Vikings, and Stenton and Miller see it as recognition by Edmund that Northumbria was the northern limit of Anglo-Saxon England.

According to the hagiography of a Gaelic monk called Cathróe he travelled through England on his journey from Scotland to the Continent; Edmund summoned him to court and Oda, Archbishop of Canterbury, then ceremonially conducted him to his ship at Lympne. Travelling clerics played an important part in the circulation of manuscripts and ideas in this period, and Cathróe is unlikely to have been the only Celtic cleric at Edmund's court.

Edmund inherited strong Continental contacts from Æthelstan's cosmopolitan court, and these were enhanced by their sisters' marriages to foreign kings and princes. Edmund carried on his brother's Continental policies and maintained his alliances, especially with his nephew King Louis IV of West Francia and Otto I, King of East Francia and future Holy Roman Emperor. Louis was both nephew and brother-in-law of Otto, while Otto and Edmund were brothers-in-law. There were almost certainly extensive diplomatic contacts between Edmund and Continental rulers which have not been recorded, but it is known that Otto sent delegations to Edmund's court. In the early 940s some Norman lords sought the help of the Danish prince Harald against Louis, and in 945 Harald captured Louis and handed him to Hugh the Great, Duke of the Franks, who kept him prisoner. Edmund and Otto both protested and demanded his immediate release, but this only took place in exchange for the surrender of the town of Laon to Hugh.

Edmund's name is in the confraternity book of Pfäfers Abbey in Switzerland, perhaps at the request of Archbishop Oda when staying there on his way to or from Rome to collect his pallium. As with the diplomatic delegations, this probably represents rare surviving evidence of extensive contacts between English and Continental churchmen which continued from Æthelstan's reign.

Edmund inherited his brother's interests and leading advisers, such as Æthelstan Half-King, ealdorman of East Anglia, Ælfheah the Bald, bishop of Winchester, and Oda, bishop of Ramsbury, who was appointed as Archbishop of Canterbury by Edmund in 941. Æthelstan Half-King first witnessed a charter as an ealdorman in 932, and within three years of Edmund's accession he had been joined by two of his brothers as ealdormen; their territories covered more than half of England and his wife fostered the future King Edgar. The historian Cyril Hart compares the brothers' power during Edmund's reign to that of the Godwins a century later. Edmund's mother, Eadgifu, who had been in eclipse during her step-son's reign, was also very influential.

For the first half of 940 there were no changes in the attestations of ealdormen compared with the end of Æthelstan's reign, but later in the year the number of ealdormen was doubled from four to eight, with three of the new ealdormen covering Mercian districts. There was an increased reliance on the family of Æthelstan Half-King, which was enriched by grants in 942. The appointments may have been part of Edmund's measures to deal with Anlaf's incursion.

Eadgifu and Eadred attested many of Edmund's charters, showing a high degree of family cooperation; initially Eadgifu attested first, but from sometime in late 943 or early 944 Eadred took precedence, perhaps reflecting his growing authority. Eadgifu attested around one third, always as (king's mother), including all grants to religious institutions and individuals. Eadred attested over half of his brother's charters. Eadgifu's and Eadred's prominence in charter attestations is unparalleled by any other West Saxon king's mother and male relative.

The period from around 925 to 975 was the golden age of Anglo-Saxon royal charters, when they were at their peak as instruments of royal government, and the scribes who drew up most of Edmund's charters constituted a royal secretariat which he inherited from his brother. From 928 until 935 charters were produced by the very learned scribe designated by scholars as Æthelstan A in a highly elaborate style. Keynes comments: "It is only by dwelling on the glories and complexities of the diplomas drafted and written by Æthelstan A that one can appreciate the elegant simplicity of the diplomas that followed." A scribe known as Edmund C wrote an inscription in a gospel book (BL Cotton Tiberius A. ii folio 15v) during Æthelstan's reign and wrote charters for Edmund and Eadred between 944 and 949.

Most of Edmund's charters belong to the diplomatic 'mainstream', including those of Edmund C, but four are part of a group, dating mainly to Eadred's reign, called the 'alliterative charters'. They were drafted by a very learned scholar, almost certainly someone in the circle of Cenwald, Bishop of Worcester, or perhaps the bishop himself. These charters are characterised both by a high proportion of words starting with the same letter and by the use of unusual words. Ben Snook describes the charters as "impressive literary works", and like much of the writing of the period their style displays the influence of Aldhelm, a leading scholar and early eighth-century bishop of Sherborne.
The only coin in common use in the tenth century was the penny. The main coin designs in Edmund's reign were H (Horizontal) types, with a cross or other decoration on the obverse surrounded by a circular inscription including the king's name, and the moneyer's name horizontally on the reverse. There were also substantial numbers of BC (Bust Crowned) types in East Anglia and the Danish shires; these had a portrait of the king, often crudely drawn, on the obverse. For a period in Æthelstan's reign many coins showed the mint town, but this had become rare by the time of Edmund's accession, except in Norwich, where it continued during the 940s for BC types.

After the reign of Edward the Elder there was a slight decline in the weight of coins under Æthelstan, and the deterioration increased after around 940, continuing until Edgar's reform of the coinage in around 973. However, based on a very small sample, there is no evidence of a decline in the silver content under Edmund. His reign saw an increase in regional diversity of the coinage which lasted for twenty years until a return to relative unity of design early in Edgar's reign.

Three law codes of Edmund survive, carrying on Æthelstan's tradition of legal reform. They are called I Edmund, II Edmund and III Edmund. The order in which they were issued is clear, but not the dates of issue. I Edmund is concerned with ecclesiastical matters, while the other codes deal with public order.

I Edmund was promulgated at a council in London convened by Edmund and attended by archbishops Oda and Wulfstan. The code is very similar to "Constitutions" previously promulgated by Oda. Uncelibate clerics were threatened with the loss of property and forbidden burial in consecrated ground, and there were also provisions regarding church dues and the restoration of church property. A clause forbidding a murderer from coming into the neighbourhood of the king, unless he had done penance for his crime, reflected an increasing emphasis on the sanctity of kingship. Edmund was one of the few Anglo-Saxon kings to promulgate laws concerned with sorcery and idolatry, and the code condemns false witness and the use of magical drugs. The association between perjury and the use of drugs in magic was traditional, probably because they both involved the breaking of a religious oath.

In II Edmund, the king and his counsellors are stated to be "greatly distressed by the manifold illegal deeds of violence which are in our midst", and aimed to promote "peace and concord". The main focus is on regulating and controlling blood feuds. The authorities () are required to put a stop to vendettas following murders: the killer should instead pay (compensation) to the relatives of the victim. If no wergeld is paid, the killer has to bear the feud, but attacks on him are forbidden in churches and royal manor houses. If the killer's kin abandon him and refuse to contribute to a wergeld and to protect him, then it is the king's will that they are to be exempt from the feud: any of the victim's kin taking vengeance on them shall incur the hostility of the king and his friends and shall lose all their possessions. In the view of the historian Dorothy Whitelock the need for legislation to control the feud was partly due to the influx of Danish settlers who believed that it was more manly to pursue a vendetta than to settle a dispute by accepting compensation. Several Scandinavian loan words are first recorded in this code, such as , the crime of attacking a homestead; the penalty is loss of all the offender's property, while the king decides whether he also loses his life. Scandinavian loan words are not found in Edmund's other codes, and this one may have been particularly aimed at his Danish subjects. In contrast to Edmund's concern about the level of violence, he congratulated his people on their success in suppressing thefts. The code encourages greater local initiative in upholding the law, while emphasising Edmund's royal dignity and authority.

The relationship between Anglo-Saxon kings and their leading men was personal; kings were lords and protectors in return for pledges of loyalty and obedience, and this is spelled out in terms based on Carolingian legislation for the first time in III Edmund, issued at Colyton in Devon. This requires that "all shall swear in the name of the Lord, before whom that holy thing is holy, that they will be faithful to King Edmund, even as it behoves a man to be faithful to his lord, without any dispute or dissension, openly or in secret, favouring what he favours and discountenancing what he discountenances." The threat of divine retribution was important in a society which had limited coercive power to punish law breaking and disloyalty. The military historian Richard Abels argues that "all" () shall swear does not mean literally all, but should be understood to mean those men qualified to take oaths administered by royal reeves at shire courts, that is the middling and great landholders, and that Edmund's oath united his diverse peoples by binding them all to him personally. The emphasis on lordship is further seen in provisions setting out the duties of lords to take responsibility for their followers and stand surety for them.

III Edmund was also concerned to prevent theft, especially cattle rustling. The local community is required to cooperate in catching thieves, dead or alive, and to assist in tracking down stolen cattle, while trading had to be witnessed by a high reeve, priest, treasurer or port reeve. According to a provision described by the legal historian Patrick Wormald as gruesome: "we have declared with regard to slaves that, if a number of them commit theft, their leader shall be captured and slain, or hanged, and each of the others shall be scourged three times and have his scalp removed and his little finger mutilated as a token of his guilt". The code has the first reference to the hundred as an administrative unit of local government in a provision requiring anyone who refuses to assist in the apprehension of a thief to pay 120 shillings to the king and 30 shillings to the hundred.

Williams comments "In both the second code and the Colyton legislation, the functions of the four pillars of medieval society, kingship, lordship, family, and neighbourhood, are clearly evident." Wormald describes the codes as "an object-lesson in the variety of Anglo-Saxon legal texts", but he sees what they have in common as more important, especially a heightened rhetorical tone which extends to treating murder as an affront to the royal person. The historian Alaric Trousdale sees "explicit funding of local administrative institutions and the greater empowerment of local officials in the application of the law" as original contributions of Edmund's legislation. Edmund is listed in laws of his grandson Æthelred the Unready as one of the wise law-givers of the past.

The major religious movement of the tenth century, the English Benedictine Reform, reached its peak under Edgar, but Edmund's reign was important in the early stages, which were led by Oda and Ælfheah, both of whom were monks. Oda had strong connections with Continental centres of reform, especially Fleury Abbey. He had been a leading counsellor of Æthelstan and had helped to negotiate the return of Louis to France as king of the Franks in 936. Dunstan was to be a key figure in the reform and Archbishop of Canterbury, and according to his first biographer he was a leading figure at Edmund's court until his enemies persuaded Edmund to expel him, only for the king to have a change of heart after a narrow escape from death and give him a royal estate at Glastonbury, including its abbey. Williams rejects the story because there is no evidence that he was influential in this period; his brother attested charters, but he did not. Edmund may have given Dunstan the abbey to keep him at a distance because he was too much of a disruptive influence at court. He was joined by Æthelwold, another future reform leader, and they spent much of the next decade studying Benedictine texts at Glastonbury, which became the first centre for disseminating monastic reform.

Edmund visited the shrine of St Cuthbert in Chester-le-Street church, probably on his way to Scotland in 945. He prayed at the shrine and commended himself and his army to the saint. His men gave 60 pounds to the shrine, and Edmund placed two gold bracelets on the saint's body and wrapped two costly (lengths of Greek cloth) around it. One of the was probably an excellent Byzantine silk found in Cuthbert's tomb known as the 'Nature Goddess silk'. He also "granted peace and law better than any it ever had to the whole territory of St Cuthbert". Edmund's show of respect and support for the shrine reflected both the political power of the community of St Cuthbert in the north and southern reverence for him. According to William of Malmesbury, Edmund brought the relics of important Northumbrian saints such as Aidan south to Glastonbury Abbey.

Another sign of the religious revival was the number of aristocratic women who adopted a religious life. Several received grants from Edmund, including a nun called Ælfgyth, who was a patron of Wilton Abbey, and Wynflæd, the mother of Edmund's first wife. Æthelstan had granted two estates to religious women, Edmund made seven such grants and Eadred four. After this the practice ceased abruptly, apart from one further donation. The significance of the donations is uncertain, but the most likely explanation is that in the mid-tenth century some religious aristocratic women were granted the estates so that they could choose how to pursue their vocation, whether by establishing a nunnery or living a religious life in their own homes.

In the reign of Edmund's son Edgar, Æthelwold and his circle insisted that Benedictine monasticism was the only worthwhile form of religious life, but this was not the view of earlier kings such as Edmund. He was concerned to support religion, but was not committed to a particular ideology of religious development. In his grants he continued Æthelstan's policies. When Gérard of Brogne reformed the Abbey of Saint Bertin by imposing the Benedictine rule in 944, monks who rejected the changes fled to England and Edmund gave them a church owned by the crown at Bath. He may have had personal motives for his assistance, as the monks had given burial to his half-brother, Edwin, who had drowned at sea in 933, but the incident shows that Edmund did not see only one monastic rule as valid. He may also have granted privileges to the unreformed (non-Benedictine) Bury St Edmunds Abbey, but the charter's authenticity is disputed.

Latin learning revived in Æthelstan's reign, influenced by Continental models and by the hermeneutic style of the leading seventh-century scholar and Bishop of Sherborne, Aldhelm. The revival continued in Edmund's reign, and Welsh book production became increasingly influential. Welsh manuscripts were studied and copied, and they influenced the early use of Carolingian minuscule script in England, although Continental sources are also important. Edmund's reign also saw the development of a new style of the native square minuscule script, which was used in mid-century royal diplomas. Oda's school at Canterbury was praised by post-Conquest chroniclers, especially for the presence there of Frithegod, a brilliant Continental scholar and the most skilful poet in mid-tenth-century England. The "Vatican" recension of the was produced in England in Edmund's reign, probably in 944.

Edmund probably married his first wife Ælfgifu around the time of his accession to the throne, as their second son was born in 943. Their sons Eadwig and Edgar both became kings of England. Ælfgifu's father is not known, but her mother is identified by a charter of Edgar which confirms a grant by his grandmother Wynflæd of land to Shaftesbury Abbey. Ælfgifu was also a benefactor of Shaftesbury Abbey; when she died in 944 she was buried there and venerated as a saint. Edmund had no known children by his second wife, Æthelflæd, who died after 991. Her father Ælfgar became ealdorman of Essex in 946. Edmund presented him with a sword lavishly decorated with gold and silver, which Ælfgar later presented to King Eadred. Æthelflæd's second husband was Æthelstan Rota, a south-east Mercian ealdorman, and her will survives.

On 26 May 946 Edmund was killed in a brawl at Pucklechurch in Gloucestershire. According to the post-Conquest chronicler, John of Worcester:

The historians Clare Downham and Kevin Halloran dismiss John of Worcester's account and suggest that the king was the victim of a political assassination, but this view has not been accepted by other historians.

Like his son Edgar thirty years later, Edmund was buried at Glastonbury Abbey. The location may have reflected its spiritual prestige and royal endorsement of the monastic reform movement, but as his death was unexpected it is more likely that Dunstan was successful in claiming the body. His sons were still young children, so he was succeeded as king by his brother Eadred, who was in turn succeeded by Edmund's elder son Eadwig in 955.

Historians' views of Edmund's character and record differ widely. The historian Barbara Yorke comments that when substantial powers were delegated there was a danger that subjects would become over-powerful: the kings following Æthelstan came to the throne young and had short reigns, and the families of Æthelstan 'Half-King' and Ælfhere, Ealdorman of Mercia, developed unassailable positions. In the view of Cyril Hart: "For the whole of his brief reign, the young king Edmund remained strongly under the influence of his mother Eadgifu and the 'Half King', who between them must have decided much of the national policy." In contrast, Williams describes Edmund as "an energetic and forceful ruler" and Stenton commented that "he proved himself to be both warlike and politically effective", while in Dumville's view, but for his early death "he might yet have been remembered as one of the more remarkable of Anglo-Saxon kings".

The historian Ryan Lavelle comments that "a case can be made, as Alaric Trousdale has recently done [in his PhD thesis on Edmund's reign], for assigning Edmund a central role to the achievements of the tenth-century English state". Trousdale comments that the period between the reigns of Æthelstan and Edgar has been comparatively neglected by historians: the reigns of Edmund, Eadred and Eadwig "are often lumped together as a sort of interim period between the much more interesting reigns of Æthelstan and Edgar". He argues that "King Edmund's legislation shows an ambition towards tighter control of the localities through increased cooperation between all levels of government, and that king and archbishop were working closely together in restructuring the English administrative framework". Trousdale sees a transition which "was marked in part by a small yet significant shift away from a reliance on traditional West Saxon administrative structures and the power blocs that had enjoyed influence under King Æthelstan, towards increased cooperation with interests and families from Mercia and East Anglia". He also sees Edmund as moving away from Æthelstan's centralisation of power to a more collegial relationship with local secular and ecclesiastical authorities. Trousdale's picture contrasts with that of other historians such as Sarah Foot, who emphasises the achievements of Æthelstan, and George Molyneaux in his study of the formation of the late Anglo-Saxon state in the reign of Edgar.



Endothermic process

An endothermic process is a chemical or physical process that absorbs heat from its surroundings. In terms of thermodynamics and thermochemistry, it is a thermodynamic process with an increase in the enthalpy (or internal energy ) of the system. In an endothermic process, the heat that a system absorbs is thermal energy transfer into the system. Thus, an endothermic reaction generally leads to an increase in the temperature of the system and a decrease in that of the surroundings.

The term was coined by 19th-century French chemist Marcellin Berthelot. The term "endothermic" comes from the Greek ἔνδον ("endon") meaning 'within' and θερμ- ("therm") meaning 'hot' or 'warm'.

An endothermic process may be a chemical process, such as dissolving ammonium nitrate () in water (), or a physical process, such as the melting of ice cubes.

The opposite of an endothermic process is an exothermic process, one that releases or "gives out" energy, usually in the form of heat and sometimes as electrical energy. Thus, "endo" in endothermic refers to energy or heat going in, and "exo" in exothermic refers to energy or heat going out. In each term (endothermic and exothermic) the prefix refers to where heat (or electrical energy) goes as the process occurs.

Due to bonds breaking and forming during various processes (changes in state, chemical reactions), there is usually a change in energy. If the energy of the forming bonds is greater than the energy of the breaking bonds, then energy is released. This is known as an exothermic reaction. However, if more energy is needed to break the bonds than the energy being released, energy is taken up. Therefore, it is an endothermic reaction.

Whether a process can occur spontaneously depends not only on the enthalpy change but also on the entropy change () and absolute temperature . If a process is a spontaneous process at a certain temperature, the products have a lower Gibbs free energy than the reactants (an exergonic process), even if the enthalpy of the products is higher. Thus, an endothermic process usually requires a favorable entropy increase () in the system that overcomes the unfavorable increase in enthalpy so that still . While endothermic phase transitions into more disordered states of higher entropy, e.g. melting and vaporization, are common, spontaneous chemical processes at moderate temperatures are rarely endothermic. The enthalpy increase in a hypothetical strongly endothermic process usually results in , which means that the process will not occur (unless driven by electrical or photon energy). An example of an endothermic and exergonic process is


The terms "endothermic" and "endotherm" are both derived from Greek ' "within" and ' "heat", but depending on context, they can have very different meanings.

In physics, thermodynamics applies to processes involving a system and its surroundings, and the term "endothermic" is used to describe a reaction where energy is taken "(with)in" by the system (vs. an "exothermic" reaction, which releases energy "outwards").

In biology, thermoregulation is the ability of an organism to maintain its body temperature, and the term "endotherm" refers to an organism that can do so from "within" by using the heat released by its internal bodily functions (vs. an "ectotherm", which relies on external, environmental heat sources) to maintain an adequate temperature.


Earle Page

Sir Earle Christmas Grafton Page (8 August 188020 December 1961) was an Australian politician and surgeon who was the 11th prime minister of Australia, holding office for 19 days after the death of Joseph Lyons in 1939. He was the leader of the Country Party from 1921 to 1939, and was the most influential figure in its early years.

Page was born in Grafton, New South Wales. He entered the University of Sydney at the age of 15, and completed a degree in medicine at the age of 21. After completing his medical residency at Sydney's Royal Prince Alfred Hospital, he moved back to Grafton and opened a private hospital. He soon became involved in local politics, and in 1915 purchased a part-share in "The Daily Examiner", a local newspaper. He also briefly was a military surgeon during World War I. Page gained prominence as an advocate of various development schemes for the Northern Rivers region, especially those involving hydroelectricity. He also helped found a movement for New England statehood.

In 1919, Page was elected to Federal Parliament representing the Division of Cowper. He joined the new Country Party the following year as its inaugural whip, and then replaced William McWilliams as party leader in 1921. Page opposed the economic policies of Prime Minister Billy Hughes, and when the Country Party gained the balance of power at the 1922 election, he demanded Hughes' resignation as the price for a coalition with the Nationalist Party. He was subsequently made Treasurer of Australia under the new prime minister, Stanley Bruce, serving in that role from 1923 to 1929. He had a significant degree of influence on domestic policy, with Bruce concentrating on international issues.

Page returned to cabinet after the 1934 election, when the Country Party entered a new coalition with Joseph Lyons' United Australia Party (UAP). He was appointed Minister for Commerce, and concentrated on agricultural issues. When Lyons died in office in April 1939, Page was commissioned as his successor in a caretaker capacity while the UAP elected a new leader, Robert Menzies. Page subsequently denounced Menzies and refused to serve in his cabinet, withdrawing the Country Party from the coalition, but this proved unpopular and he resigned the party leadership after a few months. The coalition was eventually reconstituted, and Page served again as Minister for Commerce under Menzies and Arthur Fadden until the government's defeat in October 1941.

Page's last major role was as Minister for Health (1949–1956) in the post-war Menzies Government. He retired from cabinet at the age of 76, and died a short time after losing his seat at the 1961 election. Page served in parliament for almost 42 years, the third longest-serving Australian parliamentarian of all time; only Menzies lasted longer as the leader of a major Australian political party. He secured his party's independence by refusing overtures to merge with the Nationalists and the UAP, and the policies that he favoured – decentralisation, agrarianism, and government support of primary industry – have remained the basis of its platform up to the present day. The coalitions that he established and maintained with Bruce and Lyons have served as a model for all subsequent coalition governments.

Earle Christmas Grafton Page was born in Grafton, New South Wales, on 8 August 1880. His first middle name, which he disliked, was given to him to carry on the surname of a childless relative, while his second middle name was in honour of his birthplace. Page was the fifth of eleven children born to Charles Page and Mary Johanna Haddon (Annie) Cox. His older brother Rodger was chaplain to the royal family of Tonga and his younger brother Harold was the deputy administrator of the Territory of New Guinea and a Japanese prisoner of war. Page's parents had both lived in Grafton since they were children. His mother was born in Tasmania to an English father and a Scottish mother. His father, born in London, was a successful businessman and a member of the Grafton City Council, serving a single term as mayor in 1908. The family business was a hardware manufacturing firm, which had its origins in a coachbuilding firm established in 1858 by Page's maternal grandfather, Edwin Cox. His other grandfather, James Page, arrived in Grafton in 1855, serving as the town's first schoolmaster and first town clerk.

Page began his schooling at Grafton Public School, where he excelled academically. His family could not afford to send him to boarding school, as a result of financial difficulties caused by the banking crisis of 1893. Page consequently had to rely on scholarships to advance his education. He won a bursary to attend Sydney Boys High School in 1895, where he passed the university entrance exams, and the following year – aged 15 – began studying a liberal arts course at the University of Sydney. He was equal top in mathematics in his first year, and was also awarded the lucrative Struth Exhibition for "general proficiency in the arts", which allowed him to switch to medicine and covered his first four years of medical school. His role model was Grafton Smith, who had followed a similar path from Grafton Public School to university. At Sydney Medical School, Page's lecturers included William Haswell (biology), James Hill (biology), Charles Martin (physiology), Anderson Stuart (physiology), and James Wilson (anatomy). He graduated at the top of his class in 1901, with the degrees of Bachelor of Medicine (M.B.) and Master of Surgery (Ch.M.).

Page's first professional posting came before he had even been registered as a medical practitioner. Due to a shortage of doctors, he was acting superintendent of the Royal Alexandra Hospital for Children for one month. In 1902, he took up a position as a resident at the Royal Prince Alfred Hospital, serving in a variety of roles including as house surgeon under Robert Scot Skirving. During that time he contracted a near-fatal infection from a postmortem examination. He also met his future wife, nurse Ethel Blunt. Page returned to his home town in 1903, taking over a practice in South Grafton. He and two partners subsequently established a new private hospital, Clarence House Hospital, which opened in 1904 and served both Grafton and the surrounding region.

Page was a keen adopter of new technologies. In 1904, he bought what he claimed was "the first Rover car in Australia", which was powered by kerosene. He upgraded to an Itala in 1908, and had the chassis enlarged so it could be used as an ambulance. He also had an x-ray machine installed in his hospital, one of the first in Australia outside a major city. Page developed a reputation for surgical innovation, taking a number of patients from Sydney and even some from interstate. One operation that brought him particular fame was the removal of a patient's diseased lung, a procedure that had only been invented a few years previously. Page became an inaugural Fellow of the Royal Australasian College of Surgeons (FRACS) in 1927, and in 1942 was made an honorary Fellow of the Royal College of Surgeons of England (FRCS).

In February 1916, Page enlisted in the Australian Army Medical Corps. He was chief medical officer aboard the troopship "Ballarat", and was then stationed at an army hospital in Cairo for several months. He was transferred to a hospital in England in July 1916, and concluded his service as a surgical specialist at a casualty clearing station in France. Page returned to Australia in March 1917 and was discharged from the military in July 1917. Although his active involvement in medicine declined as his political career progressed, he was frequently called upon to treat his fellow MPs or parliamentary staff. This was particularly true after the federal government moved to Canberra, as the new capital had only a handful of qualified surgeons. In 1928, for instance, he performed an emergency appendectomy on Parker Moloney.

Page's medical career brought him considerable wealth, and he began investing in land. He bought several large farming properties in South-East Queensland, including in Nerang, Kandanga, and the Numinbah Valley; Pages Pinnacle in the Numinbah State Forest is named after him. His entry into public life came about as a result of his passion for hydroelectricity, which he first observed in New Zealand while attending a medical convention in 1910. He believed that it could be applied to the Northern Rivers region, which was still mostly unelectrified outside of the major towns. Page was elected to the South Grafton Municipal Council in 1913, believing his position as an alderman would be useful in his lobbying efforts. However, his overtures to the state government were rebuffed. In 1915, Page was one of the founders of the Northern New South Wales Separation League, which advocated the creation of a new state in the New England region. He toured a number of towns to raise awareness of the new movement, but interest waned as a result of the ongoing war. Later that year, he was part of a syndicate that bought "The Daily Examiner", the local newspaper in Grafton.

Page visited a number of hydroelectric sites in North America in 1917, on his way back from military service in France. He was elected mayor of South Grafton in 1918, serving until 1920, and also became the inaugural president of the North Coast Development League. He developed more concrete plans for a hydroelectric project on the Clarence River, and put forward various other development schemes relating to roads, railways, and ports, all of which served to raise his profile in the local district. Page was elected to the Australian House of Representatives at the 1919 federal election, defeating the sitting Nationalist MP, John Thomson in the Division of Cowper. He stood as an independent with the endorsement of the Farmers' and Settlers' Association, and after the election joined the new Country Party, along with 10 other MPs from rural seats. Page continued to advocate for hydroelectricity throughout his political career, and many such projects were built in New South Wales. However, the specific scheme he favoured for the Clarence River was never put in place, only the smaller Nymboida Power Station. Decentralisation also remained a pet project, with Page frequently arguing for New South Wales and Queensland to be divided into smaller states to aid regional development. The movement for New England statehood waned in the 1920s, but re-emerged in the 1950s; a legally binding referendum on the subject was finally held in 1967, after Page's death, but was narrowly defeated in controversial circumstances.

Page was elected leader of the Country Party in 1921, replacing William McWilliams. At the 1922 federal election the party campaigned on a platform which included the establishment of a national sinking fund, national insurance scheme covering "sickness, unemployment, poverty and age", and conversion of the Commonwealth Bank of Australia into a full central bank. The party emerged from the election with the balance of power in the House; the Nationalist government of Billy Hughes lost its majority and could not govern without Country Party support. It soon became apparent that the price for that support would be a full coalition with the Nationalists. However, the Country Party had been formed partly due to discontent with Hughes' rural policy, and Page's animosity toward Hughes was such that he would not even consider supporting him. Indeed, he would not even begin talks with the Nationalists as long as Hughes remained leader. Bowing to the inevitable, Hughes resigned.

Page then began negotiations with Hughes' successor as leader of the Nationalists, Stanley Bruce. His terms were stiff; he wanted his Country Party to have five seats in an 11-man cabinet, including the post of Treasurer and the second rank in the ministry for himself. These demands were unprecedented for a prospective junior coalition partner in a Westminster system, and especially so for such a new party. Nonetheless, Bruce agreed rather than force another election. For all intents and purposes, Page was the first Deputy Prime Minister of Australia (a title that did not officially exist until 1968). Since then, the leader of the Country/National Party has been the second-ranking member in nearly every non-Labor government. Page was acting prime minister on several occasions, and in January 1924 chaired the first meeting of Federal Cabinet ever held in Canberra, at Yarralumla. Parliament did not move to Canberra until 1927.

As Treasurer, Page formed a close working relationship with Bruce. Due to favourable economic conditions the government was able to abolish land tax, cut income tax, and establishment the national sinking fund that Page had campaigned on. The government also established an investment fund for the Council for Scientific and Industrial Research and sponsored the first national housing program. The final years of Page's treasurership were marked by the beginnings of an economic downturn. The budget went into deficit in 1927 and his 1929 budget speech referred to a "temporary financial depression". He was a strong believer in orthodox finance and conservative policies, as well as a "high protectionist" supporting tariff barriers to protect Australian rural industries.

Page introduced a series of reforms to the Commonwealth Bank to enhance its central banking functions. In 1924, he announced that the government would place the Commonwealth Bank under an independent board, comprising a governor, the Treasury secretary, and representatives of industry. The same bill placed banknotes under the direct control of the bank, whereas previously it had been under a nominally independent Note Issue Board. Later reforms saw the establishment of a Rural Credits Department within the bank, the profits of which were partly hypothecated to agricultural research. In March 1925, cabinet decided to return Australia to the gold standard, which it had left during World War I. It delayed its announcement until the United Kingdom had decided it would do the same, which "disguised what was arguably Australia’s first explicit macroeconomic policy decision".

In 1924, Bruce and Page established the Loan Council to coordinate public-sector borrowings between the state and federal governments. It was given constitutional force with an amendment passed in 1928. The government abolished the previous system of per-capita grants to states that had been implemented in 1911 and began introducing tied grants, initially for road building. It also announced a royal commission into a national insurance scheme chaired by Senator John Millen. Page was one of the chief supporters of the "National Insurance Bill 1928", which would have provided "sickness, old age, disability and maternity benefits", as well as payments to orphans and a limited form of child endowment. It was to be paid for by compulsory contributions from workers and co-contributions from employers. The government took the policy to the 1928 Australian federal election but failed to pass the bill by the time of its defeat in 1929.

As Treasurer, Page continued his professional medical practice. On 22 October 1924, he had to tell his best friend, Thomas Shorten Cole (1870–1957), the news that his wife Mary Ann Crane had just died on the operating table from complications of intestinal or stomach cancer, reputed by their daughter Dorothy May Cole to be "the worst day of his life". Due to a shortage of surgeons in Canberra, in 1928 Page performed an appendectomy on fellow MP Parker Moloney.

The Bruce-Page government was heavily defeated by Labor in 1929 (with Bruce losing his own seat), and Page went into opposition. In 1931, a group of dissident Labor MPs led by Joseph Lyons merged with the Nationalists to form the United Australia Party under Lyons' leadership. Lyons and the UAP won majority government at the 1931 election. Although Lyons was keen to form a coalition with the Country Party, talks broke down, and Lyons opted to govern alone—to date, the last time that the Country/National Party has not had any posts in a non-Labor government. In 1934, however, the UAP suffered an eight-seat swing, forcing Lyons to take the Country Party back into his government in a full-fledged Coalition. Page became Minister for Commerce. He was made a Knight Grand Cross of the Order of St Michael and St George (GCMG) in the New Year's Day Honours of 1938. While nine Australian Prime Ministers were knighted (and Bruce was elevated to the peerage), Page is the only one who was knighted before becoming Prime Minister.

When Lyons died suddenly in 1939, the Governor-General of Australia Lord Gowrie appointed Page as caretaker Prime Minister pending the UAP choosing a new leader. He held the office for three weeks until the UAP elected former deputy leader Robert Menzies as its new leader, and hence Prime Minister. Page had been close to Lyons, but disliked Menzies, whom he charged publicly with having been disloyal to Lyons. Page contacted Stanley Bruce (now in London as Australian High Commissioner to the UK) and offered to resign his seat if Bruce would return to Australia to seek re-election to the parliament in a by-election for Page's old seat, and then seek election as UAP leader. Bruce said that he would only re-enter the parliament as an independent.

When Menzies was elected UAP leader, Page refused to serve under him, and made an extraordinary personal attack on him in the House, accusing him not only of ministerial incompetence but of physical cowardice (for failing to enlist during World War I). His party soon rebelled, though, and Page was deposed as Country Party leader in favour of Archie Cameron.

In March 1940, Archie Cameron led the Country Party back into coalition with the UAP. However, he resigned as party leader on 16 October, following the 1940 federal election. Page attempted to regain the party's leadership, but was deadlocked with John McEwen over multiple ballots. As a compromise, the party elected Arthur Fadden as acting leader; he was confirmed in the position a few months later. Page replaced Cameron as Minister for Commerce in the reconstituted ministry.
Fadden replaced Menzies as prime minister in August 1941. A few weeks later, cabinet decided to send Page to London as resident minister, with the intention that he would be granted access to the British War Cabinet. While he was en route to England, the Fadden government lost a confidence motion and was replaced by an ALP minority government. The new prime minister John Curtin nonetheless allowed Page to take up the position, declining his offer to return to Australia. The attack on Pearl Harbor in December changed the dynamic of Anglo-Australian relations, as the War in the Pacific became the primary concern of the Australian government. Page assisted in the creation of the Pacific War Council early the following year. He later recalled Winston Churchill's frustration in war cabinet meetings with Curtin's decision to withdraw troops from the Middle East and North Africa and return them to Australia. He credited himself with helping negate the tensions between the two men, but in February 1942 mistakenly advised Churchill that the Australian government was amenable to diverting the 7th Division to Burma rather than return it directly to Australia. He was heavily rebuked by Curtin and external affairs minister H. V. Evatt for his error.

Page wrote to Curtin in April 1942 that since January he had been through "the worst period of acute mental distress of my whole life". His tenure was not regarded as a success, and he was said to have suffered from a lack of experience in diplomacy. Field Marshal Alan Brooke, the Chief of the Imperial General Staff, recalled that in war cabinet meetings he had "the mentality of a greengrocer". Page left London in June 1942 following a severe bout of pneumonia. He had been made a Member of the Order of the Companions of Honour (CH) before his departure. He returned to Australia in August, travelling via the United States, and quickly turned his attention to planning for post-war reconstruction.

Page spent the remaining years of the Curtin and Chifley governments on the opposition backbench. He served on the Advisory War Council and was a delegate to the constitutional convention in Canberra in late 1942, which included members of all major political parties. However, he was frustrated by the government's failure to offer him any formal role in developing post-war policy, which he believed was due to him given his past work. Page's brother Harold and nephew Robert were killed by the Japanese during the war.

Page was reappointed Minister for Health after the Coalition won the 1949 federal election, at the age of 69. He was the chief architect of the "National Health Act 1953", which established a national public health scheme based on government subsidies of voluntary private insurance and free medical services for pensioners. He played a key role in securing the support of the medical profession, which had strongly opposed the Chifley government's attempt to introduce universal health care. Unlike in previous governments, Page had little influence beyond his own policy area and was frustrated by the lack of interest in his ideas for national development. In 1951 when Senator Gordon Brown of the ALP suffered a stroke while speaking in the Senate, Page, a trained surgeon rushed in from the House to treat him before medical professionals could take Brown to hospital for treatment.

Upon the death of Billy Hughes in October 1952, Page became the Father of the House of Representatives and Father of the Parliament. In 1954, he became the first chancellor of the University of New England, which had become fully autonomous from the University of Sydney. He retired from cabinet at the age of 76, moving to the backbench in January 1956 after the December 1955 election.

Upon Arthur Fadden's retirement in 1958, Page became the only former Prime Minister returned at that year's election.

Page sought a 17th term in parliament at the 1961 election, having joined Billy Hughes two years earlier as only the second person to serve over 40 years in federal parliament. Two weeks before the election, he experienced stomach pains while visiting the home of Ian Robinson near Coraki, New South Wales. His health then dramatically declined and he was admitted to the Royal Prince Alfred Hospital in Sydney. He was diagnosed with bowel cancer and underwent immediate surgery.

Page died in hospital on 20 December 1961, aged 81. He was granted a state funeral at St Andrew's Cathedral, Sydney. At his request, his ashes were scattered over the Clarence River near his home. On the same date Page died, the election result in Cowper was declared and recorded his defeat by the Australian Labor Party (ALP) candidate Frank McGuren, as part of a nationwide swing against the Coalition.

Page had represented Cowper for just four days short of 42 years, making him the longest-serving Australian federal parliamentarian who represented the same seat throughout his career. Only Billy Hughes and Philip Ruddock have served in Parliament longer than Page. He was the last former Prime Minister to lose his seat until Tony Abbott lost his seat of Warringah in 2019, though John Howard would lose his seat of Bennelong as a sitting Prime Minister in 2007.

Page's defeat/death saw the Australian Federal Parliament having no former Prime Ministers among its members, for the first time since the period between Sir Joseph Cook's resignation from Parliament in 1921 to become Australia's High Commissioner to the United Kingdom and Billy Hughes' forced resignation as Prime Minister in 1923.

Page married Ethel Blunt on 18 September 1906. They had met at Royal Prince Alfred Hospital while he was undertaking his medical residency; she was a senior nurse there. Page soon began courting her, and convinced her to become the matron of his new hospital in Grafton. She gave up nursing after their marriage, but was active in politics and community organisations. The couple had five children: Mary (b. 1909), Earle Jr. (b. 1910), Donald (b. 1912), Iven (b. 1914), and Douglas (b. 1916). Their grandchildren include Don Page, who was active in New South Wales state politics, and Geoff Page, a poet.

Page was predeceased by his first wife and his oldest son. Earle Jr., a qualified veterinarian, was killed by a lightning strike in January 1933, aged 22. Ethel died in May 1958, aged 82, after a long illness. On 20 July 1959 at St Paul's Cathedral, London, Page married for a second time, wedding his long-serving secretary Jean Thomas (32 years his junior). Stanley Bruce was his best man. The second Lady Page lived for almost 50 years after her husband's death, dying on 20 June 2011; her ashes were interred at Northern Suburbs Crematorium.



Notes

Citations

 

Ephrem the Syrian

Ephrem the Syrian (), also known as Saint Ephrem, Saint Ephraim, Ephrem of Edessa or Aprem of Nisibis, was a prominent Christian theologian and writer who is revered as one of the most notable hymnographers of Eastern Christianity. He was born in Nisibis, served as a deacon and later lived in Edessa.

Ephrem is venerated as a saint by all traditional Churches. He is especially revered in Syriac Christianity, both in East Syriac tradition and West Syriac tradition, and also counted as a Holy and Venerable Father (i.e., a sainted monk) in the Eastern Orthodox Church, especially in the Slovak Tradition. He was declared a Doctor of the Church in the Roman Catholic Church in 1920. Ephrem is also credited as the founder of the School of Nisibis, which, in later centuries, was the centre of learning of the Church of the East.

Ephrem wrote a wide variety of hymns, poems, and sermons in verse, as well as prose exegesis. These were works of practical theology for the edification of the Church in troubled times. Some of these works have been examined by feminist scholars who have analyzed the incorporation of feminine imagery in his texts. They also examine the performance practice of all-women choirs singing his madrāšê, or his teaching hymns. Ephrem's works were so popular that, for centuries after his death, Christian authors wrote hundreds of pseudepigraphal works in his name. He has been called the most significant of all of the fathers of the Syriac-speaking church tradition. In Syriac Christian tradition, he is considered patron of the Syriac Aramaic people.

Ephrem was born around the year 306 in the city of Nisibis (modern Nusaybin, Turkey), in the Roman province of Mesopotamia, that was recently acquired by the Roman Empire. Internal evidence from Ephrem's hymnody suggests that both his parents were part of the growing Christian community in the city, although later hagiographers wrote that his father was a pagan priest. In those days, religious culture in the region of Nisibis included local polytheism, Judaism and several varieties of the Early Christianity. Most of the population spoke the Aramaic language, while Greek and Latin were languages of administration. The city had a complex ethnic composition, consisting of "Assyrians, Arabs, Greeks, Jews, Parthians, Romans, and Iranians".

Jacob, the second bishop of Nisibis, was appointed in 308, and Ephrem grew up under his leadership of the community. Jacob of Nisibis is recorded as a signatory at the First Council of Nicea in 325. Ephrem was baptized as a youth and almost certainly became a son of the covenant, an unusual form of Syriac proto-monasticism. Jacob appointed Ephrem as a teacher (Syriac "malp̄ānâ", a title that still carries great respect for Syriac Christians). He was ordained as a deacon either at his baptism or later. He began to compose hymns and write biblical commentaries as part of his educational office. In his hymns, he sometimes refers to himself as a "herdsman" (, "‘allānâ"), to his bishop as the "shepherd" (, "rā‘yâ"), and to his community as a 'fold' (, "dayrâ"). Ephrem is popularly credited as the founder of the School of Nisibis, which, in later centuries, was the centre of learning of the Church of the East.

In 337, Emperor Constantine I, who had legalised and promoted the practice of Christianity in the Roman Empire, died. Seizing on this opportunity, Shapur II of Persia began a series of attacks into Roman North Mesopotamia. Nisibis was besieged in 338, 346 and 350. During the first siege, Ephrem credits Bishop Jacob as defending the city with his prayers. In the third siege, of 350, Shapur rerouted the River Mygdonius to undermine the walls of Nisibis. The Nisibenes quickly repaired the walls while the Persian elephant cavalry became bogged down in the wet ground. Ephrem celebrated what he saw as the miraculous salvation of the city in a hymn that portrayed Nisibis as being like Noah's Ark, floating to safety on the flood.

One important physical link to Ephrem's lifetime is the baptistery of Nisibis. The inscription tells that it was constructed under Bishop Vologeses in 359. In that year, Shapur attacked again. The cities around Nisibis were destroyed one by one, and their citizens killed or deported. Constantius II was unable to respond; the campaign of Julian in 363 ended with his death in battle. His army elected Jovian as the new emperor, and to rescue his army, he was forced to surrender Nisibis to Persia (also in 363) and to permit the expulsion of the entire Christian population. Ephrem declined being ordinated a bishop by feigning madness, because he regarded himself unworthy for it.

Ephrem, with the others, went first to Amida (Diyarbakır), eventually settling in Edessa (Urhay, in Aramaic) in 363. Ephrem, in his late fifties, applied himself to ministry in his new church and seems to have continued his work as a teacher, perhaps in the School of Edessa. Edessa had been an important center of the Aramaic-speaking world, and the birthplace of a specific Middle Aramaic dialect that came to be known as the Syriac language. The city was rich with rivaling philosophies and religions. Ephrem comments that orthodox Nicene Christians were simply called "Palutians" in Edessa, after a former bishop. Arians, Marcionites, Manichees, Bardaisanites and various gnostic sects proclaimed themselves as the true church. In this confusion, Ephrem wrote a great number of hymns defending Nicene orthodoxy. A later Syriac writer, Jacob of Serugh, wrote that Ephrem rehearsed all-female choirs to sing his hymns set to Syriac folk tunes in the forum of Edessa. In 370 he visited Basil the Great at Caesarea, and then journeyed to the monks of Egypt. As he preached a panegyrie on St. Basil, who died in 379, his own death must be placed at a later date. After a ten-year residency in Edessa, in his sixties, Ephrem succumbed to the plague as he ministered to its victims. The most reliable date for his death is after 379.

Ephrem wrote exclusively in his native Aramaic language, using the local Edessan ("Urhaya") dialect, that later came to be known as the Classical Syriac. Ephrem's works contain several endonymic (native) references to his language ("Aramaic"), homeland ("Aram") and people ("Arameans"). He is therefore known as "the authentic voice of Aramaic Christianity".

In the early stages of modern scholarly studies, it was believed that some examples of the long-standing Greek practice of labeling Aramaic as "Syriac", that are found in the "Cave of Treasures", can be attributed to Ephrem, but later scholarly analyses have shown that the work in question was written much later ( 600) by an unknown author, thus also showing that Ephrem's original works still belonged to the tradition unaffected by exonymic (foreign) labeling.

One of the early admirers of Ephrem's works, theologian Jacob of Serugh (d. 521), who already belonged to the generation that accepted the custom of a double naming of their language not only as Aramaic ("Ārāmāyā") but also as "Syriac" ("Suryāyā"), wrote a homily ("memrā") dedicated to Ephrem, praising him as the "crown" or "wreath" of the "Arameans" (), and the same praise was repeated in early liturgical texts. Only later, under the Greek influence already prevalent in the works of the middle fifth century author Theodoret of Cyrus, did it became customary to associate Ephrem with Syriac identity, and label him only as "the Syrian" (), thus blurring his Aramaic self-identification, attested by his own writings and works of other Aramaic-speaking writers, and also by examples from the earliest liturgical tradition.

Some of those problems persisted up to the recent times, even in scholarly literature, as a consequence of several methodological problems within the field of source editing. During the process of critical editing and translation of sources within Syriac studies, some scholars have practiced various forms of arbitrary (and often unexplained) interventions, including the occasional disregard for the importance of original terms, used as endonymic (native) designations for Arameans and their language (ārāmāyā). Such disregard was manifested primarily in translations and commentaries, by replacement of authentic terms with polysemic Syrian/Syriac labels. In previously mentioned "memrā", dedicated to Ephrem, one of the terms for "Aramean people" ( / Arameandom) was published correctly in original script of the source, but in the same time it was translated in English as "Syriac nation""," and then enlisted among quotations related to "Syrian/Syriac" identity, without any mention of Aramean-related terms in the source. Even when noticed and corrected by some scholars, such replacements of terms continue to create problems for others.

Several translations of his writings exist in Classical Armenian, Coptic, Old Georgian, Koine Greek and other languages. Some of his works are extant only in translation (particularly in Armenian).

Over four hundred hymns composed by Ephrem still exist. Granted that some have been lost, Ephrem's productivity is not in doubt. The church historian Sozomen credits Ephrem with having written over three million lines. Ephrem combines in his writing a threefold heritage: he draws on the models and methods of early Rabbinic Judaism, he engages skillfully with Greek science and philosophy, and he delights in the Mesopotamian/Persian tradition of mystery symbolism.

The most important of his works are his lyric, teaching hymns (ܡܕܖ̈ܫܐ, "madrāšê"). These hymns are full of rich, poetic imagery drawn from biblical sources, folk tradition, and other religions and philosophies. The "madrāšê" are written in stanzas of syllabic verse and employ over fifty different metrical schemes. The form is defined by an antiphon, or congregational refrain (ܥܘܢܝܬܐ, "‘ûnîṯâ"), between each independent strophe (or verse), and the refrain's melody mimics that of the opening half of the strophe. Each "madrāšâ" had its "qālâ" (ܩܠܐ), a traditional tune identified by its opening line. All of these "qālê" are now lost. It seems that Bardaisan and Mani composed "madrāšê", and Ephrem felt that the medium was a suitable tool to use against their claims. The "madrāšê" are gathered into various hymn cycles. Each group has a title — "Carmina Nisibena", "On Faith", "On Paradise", "On Virginity", "Against Heresies" — but some of these titles do not do justice to the entirety of the collection (for instance, only the first half of the "Carmina Nisibena" is about Nisibis). Some of these hymn cycles provide implicit insight into Ephrem's perceived level of comfort with incorporating feminine imagery into his writings. One such hymn cycle was "Hymns on the Nativity", centered around Mary, which contained 28 hymns and had the clearest pervasive theme of Ephrem's hymn cycles. An example of feminine imagery is found when Ephrem writes of the baby Jesus: "he was lofty but he sucked Mary's milk and from his blessings all creation sucks."

Particularly influential were his "Hymns Against Heresies". Ephrem used these to warn his flock of the heresies that threatened to divide the early church. He lamented that the faithful were "tossed to and fro and carried around with every wind of doctrine, by the cunning of men, by their craftiness and deceitful wiles" (Eph 4:14). He devised hymns laden with doctrinal details to inoculate right-thinking Christians against heresies such as docetism. The "Hymns Against Heresies" employ colourful metaphors to describe the Incarnation of Christ as fully human and divine. Ephrem asserts that Christ's unity of humanity and divinity represents peace, perfection and salvation; in contrast, docetism and other heresies sought to divide or reduce Christ's nature and, in doing so, rend and devalue Christ's followers with their false teachings.

The relationship between Ephrem's compositions and femininity is shown again in documentation suggesting that the madrāšê were sung by all-women choirs with an accompanying lyre. These women's choirs were composed of members of the Daughters of the Covenant, an important institution in historical Syriac Christianity, but they weren't always labeled as such. Ephrem, like many Syriac liturgical poets, believed that women's voices were important to hear in the church as they were modeled after Mary, mother of Jesus, whose acceptance of God's call led to salvation for all through the birth of Jesus. One variety of the madrāšê, the "soghyatha", was sung in a conversational style between male and female choirs. The women's choir would sing the role of biblical women, and the men's choir would sing the male role. Through the role of singing Ephrem's madrāšê, women's choirs were granted a role in worship.

Ephrem also wrote verse homilies (, "mêmrê"). These sermons in poetry are far fewer in number than the madrāšê. The mêmrê were written in a heptosyllabic couplets (pairs of lines of seven syllables each).

The third category of Ephrem's writings is his prose work. He wrote a biblical commentary on the Diatessaron (the single gospel harmony of the early Syriac church), the Syriac original of which was found in 1957. His "Commentary on Genesis and Exodus" is an exegesis of Genesis and Exodus. Some fragments exist in Armenian of his commentaries on the Acts of the Apostles and Pauline Epistles.

He also wrote refutations against Bardaisan, Mani, Marcion and others.

Syriac churches still use many of Ephrem's hymns as part of the annual cycle of worship. However, most of these liturgical hymns are edited and conflated versions of the originals.

The most complete, critical text of authentic Ephrem was compiled between 1955 and 1979 by Dom Edmund Beck, OSB, as part of the "Corpus Scriptorum Christianorum Orientalium".

Ephrem is attributed with writing hagiographies such as "The Life of Saint Mary the Harlot", though this credit is called into question.

One of the works attributed to Ephrem was the "Cave of Treasures", written by a much later but unknown author, who lived at the end of the 6th and the beginning of the 7th century.

Ephrem's writings contain a rich variety of symbols and metaphors. Christopher Buck gives a summary of analysis of a selection of six key scenarios (the way, robe of glory, sons and daughters of the Covenant, wedding feast, harrowing of hell, Noah's Ark/Mariner) and six root metaphors (physician, medicine of life, mirror, pearl, Tree of life, paradise).

Ephrem's meditations on the symbols of Christian faith and his stand against heresy made him a popular source of inspiration throughout the church. There is a huge corpus of Ephrem pseudepigraphy and legendary hagiography in many languages. Some of these compositions are in verse, often mimicking Ephrem's heptasyllabic couplets.

There is a very large number of works by "Ephrem" extant in Greek. In the literature this material is often referred to as "Greek Ephrem", or "Ephraem Graecus" (as opposed to the real Ephrem the Syrian), as if it was by a single author. This is not the case, but the term is used for convenience. Some texts are in fact Greek translations of genuine works by Ephrem. Most are not. The best known of these writings is the "Prayer of Saint Ephrem", which is recited at every service during Great Lent and other fasting periods in Eastern Christianity.

There are also works by "Ephrem" in Latin, Slavonic and Arabic. "Ephrem Latinus" is the term given to Latin translations of "Ephrem Graecus". None is by Ephrem the Syrian. "Pseudo Ephrem Latinus" is the name given to Latin works under the name of Ephrem which are imitations of the style of Ephrem Latinus.

There has been very little critical examination of any of these works. They were edited uncritically by Assemani, and there is also a modern Greek edition by Phrantzolas.

Soon after Ephrem's death, legendary accounts of his life began to circulate. One of the earlier "modifications" is the statement that Ephrem's father was a pagan priest of Abnil or Abizal. However, internal evidence from his authentic writings suggest that he was raised by Christian parents.

Ephrem is venerated as an example of monastic discipline in Eastern Christianity. In the Eastern Orthodox scheme of hagiography, Ephrem is counted as a Venerable Father (i.e., a sainted monk). His feast day is celebrated on 28 January and on the Saturday of the Venerable Fathers (Cheesefare Saturday), which is the Saturday before the beginning of Great Lent.

On 5 October 1920, Pope Benedict XV proclaimed Ephrem a Doctor of the Church ("Doctor of the Syrians").

The most popular title for Ephrem is "Harp of the Spirit" (Syriac: , "Kenārâ d-Rûḥâ"). He is also referred to as the Deacon of Edessa, the Sun of the Syrians and a Pillar of the Church.

His Roman Catholic feast day of 9 June conforms to his date of death. For 48 years (1920–1969), it was on 18 June, and this date is still observed in the Extraordinary Form.

Ephrem is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on June 10.

Ephrem is remembered in the Church of England with a commemoration on 9 June.





Amiga Enhanced Chip Set

The Enhanced Chip Set (ECS) is the second generation of the Amiga computer's chipset, offering minor improvements over the original chipset (OCS) design. ECS was introduced in 1990 with the launch of the Amiga 3000. Amigas produced from 1990 onwards featured a mix of OCS and ECS chips, such as later versions of the Amiga 500 and the Commodore CDTV. Other ECS models were the Amiga 500+ in 1991 and lastly the Amiga 600 in 1992.

Notable improvements were the "Super Agnus" and the "HiRes Denise" chips. The sound and floppy controller chip, "Paula", remained unchanged from the OCS design. Super Agnus supports 2 MB of Chip RAM, whereas the original Agnus/"Fat Agnus" and subsequent "Fatter Agnus" can address 512 KB and 1 MB, respectively. The ECS Denise chip offers "Productivity" VGA output (640×480 non-interlaced) and "SuperHiRes" (1280×200 or 1280×256) display modes (also available in interlaced mode), which are however limited to only 4 bits on-screen colors. Essentially, a 35 ns pixel mode was added plus the ability to run arbitrary horizontal and vertical scan rates. This made other display modes possible, but only the aforementioned modes were supported originally out of the box. For example, the Linux Amiga framebuffer device driver allows the use of several other display modes. Other improvements were the ability of the blitter to copy regions larger than 1024×1024 pixels in one operation and the ability to display sprites in border regions (outside of any display window where bitplanes are shown). ECS also allows software switching between 60 Hz and 50 Hz video modes.

These improvements largely favored application software, which benefited from higher resolution and VGA-like display modes, rather than games. As an incremental update, ECS was intended to be backward compatible with software designed for OCS machines, though some pre-ECS games were found to be incompatible. Additionally, features from the improved Kickstart 2 operating system were used in subsequent software, and since these two technologies largely overlap, some users misjudged the significance of ECS. It is possible to upgrade some OCS machines, such as the Amiga 500, to obtain partial or full ECS functionality by replacing OCS chips with ECS versions. ECS was followed by the third generation AGA chipset with the launch of the Amiga 4000 and Amiga 1200 in 1992.


European Space Operations Centre

The European Space Operations Centre (ESOC) serves as the main mission control centre for the European Space Agency (ESA) and is located in Darmstadt, Germany. ESOC's primary function is the operation of uncrewed spacecraft on behalf of ESA and the launch and early orbit phases (LEOP) of ESA and third-party missions. The Centre is also responsible for a range of operations-related activities within ESA and in cooperation with ESA's industry and international partners, including ground systems engineering, software development, flight dynamics and navigation, development of mission control tools and techniques and space debris studies.

ESOC's current major activities comprise operating planetary and solar missions, such as Mars Express and the Trace Gas Orbiter, astronomy & fundamental physics missions, such as Gaia and XMM Newton, and Earth observation missions such as CryoSat2 and Swarm.

ESOC is responsible for developing, operating and maintaining ESA's ESTRACK network of ground stations. Teams at the Centre are also involved in research and development related to advanced mission control concepts and Space Situational Awareness, and standardisation activities related to frequency management; mission operations; tracking, telemetry and telecommanding; and space debris.

ESOC's current missions comprise the following:

Planetary and solar missions

Astronomy and fundamental physics missions

Earth observation missions

In addition, the ground segment and mission control teams for several missions are in preparation and training, including:


ESOC hosts the control centre for the Agency's European Tracking ESTRACK station network. The core network comprises seven stations in seven countries: Kourou (French Guiana), Cebreros (Spain), Redu (Belgium), Santa Maria (Portugal), Kiruna (Sweden), Malargüe (Argentina) and New Norcia (Australia). Operators are on duty at ESOC 24 hours/day, year round, to conduct tracking passes, uploading telecommands and downloading telemetry and data.

In addition to 'pure' mission operations, a number of other activities take place at the Centre, most of which are directly related to ESA's broader space operations activities.


The European Space Operations Centre was formally inaugurated in Darmstadt, Germany, on 8 September 1967 by the then-Minister of Research of the Federal Republic of Germany, Gerhard Stoltenberg. Its role was to provide satellite control for the European Space Research Organisation (ESRO), which is today known as its successor organisation, the European Space Agency (ESA).

The 90-person ESOC facility was, as it is today, located on the west side of Darmstadt; it employed the staff and resources previously allocated to the European Space Data Centre (ESDAC), which had been established in 1963 to conduct orbit calculations. These were augmented by mission control staff transferred from ESTEC to operate satellites and manage the ESTRACK tracking station network.

Within just eight months, ESOC, as part of ESRO, was already operating its first mission, ESRO-2B, a scientific research satellite and the first of many operated from ESOC for ESRO, and later ESA.

By July 2012, ESOC had operated over 56 missions spanning science, Earth observation, orbiting observatories, meteorology and space physics.

ESOC is located on the west side of the city of Darmstadt, some from the main train station, at Robert-Bosch-Straße 5. In 2011, ESA announced the first phase of the ESOC II modernisation and expansion project valued at €60 million. The new construction will be located across Robert-Bosch-Straße, opposite the current centre.

At ESOC, ESA employs approximately 800, comprising some 250 permanent staff and about 550 contractors. Staff from ESOC are routinely dispatched to work at other ESA establishments, ESTRACK stations, the ATV Control Centre (Toulouse), the Columbus Control Centre (Oberpfaffenhofen) and at partner facilities in several countries.



European Space Agency

The European Space Agency (ESA) is a 22-member intergovernmental body devoted to space exploration. With its headquarters in Paris and a staff of around 2,200 people globally as of 2022, ESA was founded in 1975. Its 2024 annual budget was €7.8 billion.

ESA's space flight programme includes human spaceflight (mainly through participation in the International Space Station program); the launch and operation of crewless exploration missions to other planets (such as Mars) and the Moon; Earth observation, science and telecommunication; designing launch vehicles; and maintaining a major spaceport, the Guiana Space Centre at Kourou (French Guiana), France. The main European launch vehicle Ariane 6 will be operated through Arianespace with ESA sharing in the costs of launching and further developing this launch vehicle. The agency is also working with NASA to manufacture the Orion spacecraft service module that flies on the Space Launch System.

After World War II, many European scientists left Western Europe in order to work with the United States. Although the 1950s boom made it possible for Western European countries to invest in research and specifically in space-related activities, Western European scientists realised solely national projects would not be able to compete with the two main superpowers. In 1958, only months after the Sputnik shock, Edoardo Amaldi (Italy) and Pierre Auger (France), two prominent members of the Western European scientific community, met to discuss the foundation of a common Western European space agency. The meeting was attended by scientific representatives from eight countries.

The Western European nations decided to have two agencies: one concerned with developing a launch system, ELDO (European Launcher Development Organisation), and the other the precursor of the European Space Agency, ESRO (European Space Research Organisation). The latter was established on 20 March 1964 by an agreement signed on 14 June 1962. From 1968 to 1972, ESRO launched seven research satellites, but ELDO was not able to deliver a launch vehicle. Both agencies struggled with the underfunding and diverging interests of their participants.

ESA in its current form was founded with the ESA Convention in 1975, when ESRO was merged with ELDO. ESA had ten founding member states: Belgium, Denmark, France, West Germany, Italy, the Netherlands, Spain, Sweden, Switzerland, and the United Kingdom. These signed the ESA Convention in 1975 and deposited the instruments of ratification by 1980, when the convention came into force. During this interval the agency functioned in a de facto fashion. ESA launched its first major scientific mission in 1975, Cos-B, a space probe monitoring gamma-ray emissions in the universe, which was first worked on by ESRO.

ESA collaborated with NASA on the International Ultraviolet Explorer (IUE), the world's first high-orbit telescope, which was launched in 1978 and operated successfully for 18 years. A number of successful Earth-orbit projects followed, and in 1986 ESA began Giotto, its first deep-space mission, to study the comets Halley and Grigg–Skjellerup. Hipparcos, a star-mapping mission, was launched in 1989 and in the 1990s SOHO, "Ulysses" and the Hubble Space Telescope were all jointly carried out with NASA. Later scientific missions in cooperation with NASA include the "Cassini–Huygens" space probe, to which ESA contributed by building the Titan landing module "Huygens".

As the successor of ELDO, ESA has also constructed rockets for scientific and commercial payloads. Ariane 1, launched in 1979, carried mostly commercial payloads into orbit from 1984 onward. The next two versions of the Ariane rocket were intermediate stages in the development of a more advanced launch system, the Ariane 4, which operated between 1988 and 2003 and established ESA as the world leader in commercial space launches in the 1990s. Although the succeeding Ariane 5 experienced a failure on its first flight, it has since firmly established itself within the heavily competitive commercial space launch market with 112 successful launches until 2021. The successor launch vehicle, the Ariane 6, is under development and had a successful long-firing engine test in November 2023. The ESA plans for the Ariane 6 to launch in June or July 2024.

The beginning of the new millennium saw ESA become, along with agencies like NASA, JAXA, ISRO, the CSA and Roscosmos, one of the major participants in scientific space research. Although ESA had relied on co-operation with NASA in previous decades, especially the 1990s, changed circumstances (such as tough legal restrictions on information sharing by the United States military) led to decisions to rely more on itself and on co-operation with Russia. A 2011 press issue thus stated:

Notable ESA programmes include SMART-1, a probe testing cutting-edge space propulsion technology, the "Mars Express" and "Venus Express" missions, as well as the development of the Ariane 5 rocket and its role in the ISS partnership. ESA maintains its scientific and research projects mainly for astronomy-space missions such as Corot, launched on 27 December 2006, a milestone in the search for exoplanets.

On 21 January 2019, ArianeGroup and Arianespace announced a one-year contract with ESA to study and prepare for a mission to mine the Moon for lunar regolith.

In 2021 the ESA ministerial council agreed to the "Matosinhos manifesto" which set three priority areas (referred to as "accelerators") "space for a green future, a rapid and resilient crisis response, and the protection of space assets", and two further high visibility projects (referred to as "inspirators") an icy moon sample return mission; and human space exploration. In the same year the recruitment process began for the 2022 European Space Agency Astronaut Group.

1 July 2023 saw the launch of the Euclid spacecraft, developed jointly with the Euclid Consortium, after 10 years of planning and building it is designed to better understand dark energy and dark matter by accurately measuring the accelerating expansion of the universe.

The agency's facilities date back to ESRO and are deliberately distributed among various countries and areas. The most important are the following centres:


The treaty establishing the European Space Agency reads:

ESA is responsible for setting a unified space and related industrial policy, recommending space objectives to the member states, and integrating national programs like satellite development, into the European program as much as possible.

Jean-Jacques Dordain – ESA's Director General (2003–2015) – outlined the European Space Agency's mission in a 2003 interview:

ESA describes its work in two overlapping ways:

These are either mandatory or optional.

According to the ESA website, the activities are:

Every member country (known as 'Member States') must contribute to these programmes: The European Space Agency Science Programme is a long-term programme of space science missions.


Depending on their individual choices the countries can contribute to the following programmes, becoming 'Participating States', listed according to:
As of 2023, ESA employs around 2200 people, and thousands of contractors. Initially, new employees are contracted for a expandable four-year term, which is until the organization's retirement age of 63. According to ESA's documents, the staff can receive myriad of perks, such as financial childcare support, retirement plans, and financial help when migrating. ESA also allows employees prevent any private documents or correspondences from disclosure to outside parties. "Ars Technica"'s 2023 report, which contained testimonies of 18 people, suggested that there is a widespread harassment between management and its employees, especially with its contractors. Since ESA is an international organization, unaffiliated with any single nation, any form of legal action is difficult to raise against the organization.

By 2015, ESA was an intergovernmental organisation of 22 member states. Member states participate to varying degrees in the mandatory (25% of total expenditures in 2008) and optional space programmes (75% of total expenditures in 2008). The 2008 budget amounted to €3.0 billion whilst the 2009 budget amounted to €3.6 billion. The total budget amounted to about €3.7 billion in 2010, €3.99 billion in 2011, €4.02 billion in 2012, €4.28 billion in 2013, €4.10 billion in 2014 and €4.33 billion in 2015.

English and French are the two official languages within ESA. Additionally, official documents are also provided in German and documents regarding the Spacelab are also provided in Italian. If found appropriate, the agency may conduct its correspondence in any language of a member state.

The following table lists all the member states and adjunct members, their ESA convention ratification dates, and their contributions in 2022:

Previously associated members were Austria, Norway and Finland, all of which later joined ESA as full members. As of November 8, 2023 there are five associate members: Slovenia, Latvia, Lithuania, Slovakia and Canada. The first four members have shown interest in full membership and may eventually apply within the next years.

Since 2016, Slovenia has been an associated member of the ESA. In November 2023 Slovenia formally applied for full membership, and it is expected that the final decision will be made by the ESA Council in 2024.

Latvia became the second current associated member on 30 June 2020, when the Association Agreement was signed by ESA Director Jan Wörner and the Minister of Education and Science of Latvia, Ilga Šuplinska in Riga. The Saeima ratified it on 27 July.

In May 2021, Lithuania became the third current associated member. As a consequence its citizens became eligible to apply to the 2022 ESA Astronaut group, applications for which were scheduled to close one week later. The deadline was therefore extended by three weeks to allow Lithuanians a fair chance to apply.

Slovakia's Associate membership came into effect on 13 October 2022, for an initial duration of seven years. The Association Agreement supersedes the European Cooperating State (ECS) Agreement, which entered into force upon Slovakia's subscription to the Plan for European Cooperating States Charter on 4 February 2016, a scheme introduced at ESA in 2001. The ECS Agreement was subsequently extended until 3 August 2022.

Since 1 January 1979, Canada has had the special status of a Cooperating State within ESA. By virtue of this accord, the Canadian Space Agency takes part in ESA's deliberative bodies and decision-making and also in ESA's programmes and activities. Canadian firms can bid for and receive contracts to work on programmes. The accord has a provision ensuring a fair industrial return to Canada. The most recent Cooperation Agreement was signed on 15 December 2010 with a term extending to 2020. For 2014, Canada's annual assessed contribution to the ESA general budget was €6,059,449 (CAD$8,559,050). For 2017, Canada has increased its annual contribution to €21,600,000 (CAD$30,000,000).

ESA is funded from annual contributions by individual states as well as from an annual contribution by the European Union (EU).

The budget of ESA was €5.250 billion in 2016. Every 3–4 years, ESA member states agree on a budget plan for several years at an ESA member states conference. This plan can be amended in future years, however provides the major guideline for ESA for several years. The 2016 budget allocations for major areas of ESA activity are shown in the chart on the right.

Countries typically have their own space programmes that differ in how they operate organisationally and financially with ESA. For example, the French space agency CNES has a total budget of €2015 million, of which €755 million is paid as direct financial contribution to ESA. Several space-related projects are joint projects between national space agencies and ESA (e.g. COROT). Also, ESA is not the only European governmental space organisation (for example European Union Satellite Centre and the European Union Space Programme Agency).

After the decision of the ESA Council of 21/22 March 2001, the procedure for accession of the European states was detailed as described the document titled "The Plan for European Co-operating States (PECS)". Nations that want to become a full member of ESA do so in 3 stages. First a Cooperation Agreement is signed between the country and ESA. In this stage, the country has very limited financial responsibilities. If a country wants to co-operate more fully with ESA, it signs a European Cooperating State (ECS) Agreement, albeit to be a candidate for said agreement, a country must be European. The ECS Agreement makes companies based in the country eligible for participation in ESA procurements. The country can also participate in all ESA programmes, except for the Basic Technology Research Programme. While the financial contribution of the country concerned increases, it is still much lower than that of a full member state. The agreement is normally followed by a Plan For European Cooperating State (or PECS Charter). This is a 5-year programme of basic research and development activities aimed at improving the nation's space industry capacity. At the end of the 5-year period, the country can either begin negotiations to become a full member state or an associated state or sign a new PECS Charter. Many countries, most of which joined the EU in both 2004 and 2007, have started to co-operate with ESA on various levels:

During the Ministerial Meeting in December 2014, ESA ministers approved a resolution calling for discussions to begin with Israel, Australia and South Africa on future association agreements. The ministers noted that "concrete cooperation is at an advanced stage" with these nations and that "prospects for mutual benefits are existing".

A separate space exploration strategy resolution calls for further co-operation with the United States, Russia and China on "LEO exploration, including a continuation of ISS cooperation and the development of a robust plan for the coordinated use of space transportation vehicles and systems for exploration purposes, participation in robotic missions for the exploration of the Moon, the robotic exploration of Mars, leading to a broad Mars Sample Return mission in which Europe should be involved as a full partner, and human missions beyond LEO in the longer term."

In August 2019, ESA and the Australian Space Agency signed a joint statement of intent "to explore deeper cooperation and identify projects in a range of areas including deep space, communications, navigation, remote asset management, data analytics and mission support." Details of the cooperation were laid out in a framework agreement signed by the two entities.

On 17 November 2020, ESA signed a memorandum of understanding (MOU) with the South African National Space Agency (SANSA). SANSA CEO Dr. Valanathan Munsami tweeted: "Today saw another land mark event for SANSA with the signing of an MoU with ESA. This builds on initiatives that we have been discussing for a while already and which gives effect to these. Thanks Jan for your hand of friendship and making this possible."

The ESA currently has only one operational launch vehicle, Vega, and another, Ariane 6, in development. Rocket launches are carried out by Arianespace, which has 23 shareholders representing the industry that manufactures the Ariane 5 as well as CNES, at ESA's Guiana Space Centre. Because many communication satellites have equatorial orbits, launches from French Guiana are able to take larger payloads into space than from spaceports at higher latitudes. In addition, equatorial launches give spacecraft an extra 'push' of nearly 500 m/s due to the higher rotational velocity of the Earth at the equator compared to near the Earth's poles where rotational velocity approaches zero.

Vega is ESA's carrier for small satellites. Developed by seven ESA members led by Italy, it is capable of carrying a payload with a mass of between 300 and 1500 kg to an altitude of 700 km, for low polar orbit. Its maiden launch from Kourou was on 13 February 2012. Vega began full commercial exploitation in December 2015.

The rocket has three solid propulsion stages and a liquid propulsion upper stage (the AVUM) for accurate orbital insertion and the ability to place multiple payloads into different orbits.

A larger version of the Vega launcher, Vega-C had its first flight in July 2022. The new evolution of the rocket incorporates a larger first stage booster, the P120C replacing the P80, an upgraded Zefiro (rocket stage) second stage, and the AVUM+ upper stage. This new variant enables larger single payloads, dual payloads, return missions, and orbital transfer capabilities.

Historically, the Ariane family rockets have been funded primarily "with money contributed by ESA governments seeking to participate in the program rather than through competitive industry bids. This [has meant that] governments commit multiyear funding to the development with the expectation of a roughly 90% return on investment in the form of industrial workshare." ESA is proposing changes to this scheme by moving to competitive bids for the development of the Ariane 6.

Future projects include the Prometheus reusable engine technology demonstrator, Phoebus (an upgraded second stage for Ariane 6), and Themis (a reusable first stage).

At the time ESA was formed, its main goals did not encompass human space flight; rather it considered itself to be primarily a scientific research organisation for uncrewed space exploration in contrast to its American and Soviet counterparts. It is therefore not surprising that the first non-Soviet European in space was not an ESA astronaut on a European space craft; it was Czechoslovak Vladimír Remek who in 1978 became the first non-Soviet or American in space (the first man in space being Yuri Gagarin of the Soviet Union) – on a Soviet Soyuz spacecraft, followed by the Pole Mirosław Hermaszewski and East German Sigmund Jähn in the same year. This Soviet co-operation programme, known as Intercosmos, primarily involved the participation of Eastern bloc countries. In 1982, however, Jean-Loup Chrétien became the first non-Communist Bloc astronaut on a flight to the Soviet Salyut 7 space station.

Because Chrétien did not officially fly into space as an ESA astronaut, but rather as a member of the French CNES astronaut corps, the German Ulf Merbold is considered the first ESA astronaut to fly into space. He participated in the STS-9 Space Shuttle mission that included the first use of the European-built Spacelab in 1983. STS-9 marked the beginning of an extensive ESA/NASA joint partnership that included dozens of space flights of ESA astronauts in the following years. Some of these missions with Spacelab were fully funded and organisationally and scientifically controlled by ESA (such as two missions by Germany and one by Japan) with European astronauts as full crew members rather than guests on board. Beside paying for Spacelab flights and seats on the shuttles, ESA continued its human space flight co-operation with the Soviet Union and later Russia, including numerous visits to Mir.

During the latter half of the 1980s, European human space flights changed from being the exception to routine and therefore, in 1990, the European Astronaut Centre in Cologne, Germany was established. It selects and trains prospective astronauts and is responsible for the co-ordination with international partners, especially with regard to the International Space Station. As of 2006, the ESA astronaut corps officially included twelve members, including nationals from most large European countries except the United Kingdom.

In 2008, ESA started to recruit new astronauts so that final selection would be due in spring 2009. Almost 10,000 people registered as astronaut candidates before registration ended in June 2008. 8,413 fulfilled the initial application criteria. Of the applicants, 918 were chosen to take part in the first stage of psychological testing, which narrowed down the field to 192. After two-stage psychological tests and medical evaluation in early 2009, as well as formal interviews, six new members of the European Astronaut Corps were selected – five men and one woman.

The astronauts of the European Space Agency are:


In the 1980s, France pressed for an independent European crew launch vehicle. Around 1978, it was decided to pursue a reusable spacecraft model and starting in November 1987 a project to create a mini-shuttle by the name of Hermes was introduced. The craft was comparable to early proposals for the Space Shuttle and consisted of a small reusable spaceship that would carry 3 to 5 astronauts and 3 to 4 metric tons of payload for scientific experiments. With a total maximum weight of 21 metric tons it would have been launched on the Ariane 5 rocket, which was being developed at that time. It was planned solely for use in low Earth orbit space flights. The planning and pre-development phase concluded in 1991; the production phase was never fully implemented because at that time the political landscape had changed significantly. With the fall of the Soviet Union ESA looked forward to co-operation with Russia to build a next-generation space vehicle. Thus the Hermes programme was cancelled in 1995 after about 3 billion dollars had been spent. The Columbus space station programme had a similar fate.

In the 21st century, ESA started new programmes in order to create its own crew vehicles, most notable among its various projects and proposals is Hopper, whose prototype by EADS, called Phoenix, has already been tested. While projects such as Hopper are neither concrete nor to be realised within the next decade, other possibilities for human spaceflight in co-operation with the Russian Space Agency have emerged. Following talks with the Russian Space Agency in 2004 and June 2005, a co-operation between ESA and the Russian Space Agency was announced to jointly work on the Russian-designed Kliper, a reusable spacecraft that would be available for space travel beyond LEO (e.g. the moon or even Mars). It was speculated that Europe would finance part of it. A €50 million participation study for Kliper, which was expected to be approved in December 2005, was finally not approved by the ESA member states. The Russian state tender for the project was subsequently cancelled in 2006.

In June 2006, ESA member states granted 15 million to the Crew Space Transportation System (CSTS) study, a two-year study to design a spacecraft capable of going beyond Low-Earth orbit based on the current Soyuz design. This project was pursued with Roskosmos instead of the cancelled Kliper proposal. A decision on the actual implementation and construction of the CSTS spacecraft was contemplated for 2008.
In mid-2009 EADS Astrium was awarded a €21 million study into designing a crew vehicle based on the European ATV which is believed to now be the basis of the Advanced Crew Transportation System design.

In November 2012, ESA decided to join NASA's Orion programme. The ATV would form the basis of a propulsion unit for NASA's new crewed spacecraft. ESA may also seek to work with NASA on Orion's launch system as well in order to secure a seat on the spacecraft for its own astronauts.

In September 2014, ESA signed an agreement with Sierra Nevada Corporation for co-operation in Dream Chaser project. Further studies on the Dream Chaser for European Utilization or DC4EU project were funded, including the feasibility of launching a Europeanised Dream Chaser onboard Ariane 5.

ESA has signed co-operation agreements with the following states that currently neither plan to integrate as tightly with ESA institutions as Canada, nor envision future membership of ESA: Argentina, Brazil, China, India (for the Chandrayan mission), Russia and Turkey.

Additionally, ESA has joint projects with the EUSPA of the European Union, NASA of the United States and is participating in the International Space Station together with the United States (NASA), Russia and Japan (JAXA).


ESA has a long history of collaboration with NASA. Since ESA's astronaut corps was formed, the Space Shuttle has been the primary launch vehicle used by ESA's astronauts to get into space through partnership programmes with NASA. In the 1980s and 1990s, the Spacelab programme was an ESA-NASA joint research programme that had ESA develop and manufacture orbital labs for the Space Shuttle for several flights on which ESA participate with astronauts in experiments.

In robotic science mission and exploration missions, NASA has been ESA's main partner. "Cassini–Huygens" was a joint NASA-ESA mission, along with the Infrared Space Observatory, INTEGRAL, SOHO, and others. Also, the Hubble Space Telescope is a joint project of NASA and ESA. Future ESA-NASA joint projects include the James Webb Space Telescope and the proposed Laser Interferometer Space Antenna. NASA has supported ESA's MarcoPolo-R mission which landed on asteroid Bennu in October 2020 and is scheduled to return a sample to Earth for further analysis in 2023. NASA and ESA will also likely join for a Mars sample-return mission. In October 2020, the ESA entered into a memorandum of understanding (MOU) with NASA to work together on the Artemis program, which will provide an orbiting Lunar Gateway and also accomplish the first crewed lunar landing in 50 years, whose team will include the first woman on the Moon. Astronaut selection announcements are expected within two years of the 2024 scheduled launch date. ESA also purchases seats on the NASA operated Commercial Crew Program. The first ESA astronaut to be on a Commercial Crew Program mission is Thomas Pesquet. Pesquet launched into space aboard Crew Dragon Endeavour on the Crew-2 mission. ESA also has seats on Crew-3 with Matthias Maurer and Crew-4 with Samantha Cristoforetti.

In 2023, following the successful launch of the Euclid telescope in July on a Falcon 9 rocket, ESA approached SpaceX to launch four Galileo communication satellites on two Falcon 9 rockets in 2024, however it would require approval from the European Commission and all member states of the European Union to proceed.

Since China has invested more money into space activities, the Chinese Space Agency has sought international partnerships. Besides the Russian Space Agency, ESA is one of its most important partners. Both space agencies cooperated in the development of the Double Star Mission. In 2017, ESA sent two astronauts to China for two weeks sea survival training with Chinese astronauts in Yantai, Shandong.

ESA entered into a major joint venture with Russia in the form of the CSTS, the preparation of French Guiana spaceport for launches of Soyuz-2 rockets and other projects. With India, ESA agreed to send instruments into space aboard the ISRO's Chandrayaan-1 in 2008. ESA is also co-operating with Japan, the most notable current project in collaboration with JAXA is the "BepiColombo" mission to Mercury.

With regard to the International Space Station (ISS), ESA is not represented by all of its member states: 11 of the 22 ESA member states currently participate in the project: Belgium, Denmark, France, Germany, Italy, Netherlands, Norway, Spain, Sweden, Switzerland and United Kingdom. Austria, Finland and Ireland chose not to participate, because of lack of interest or concerns about the expense of the project. Portugal, Luxembourg, Greece, the Czech Republic, Romania, Poland, Estonia and Hungary joined ESA after the agreement had been signed.

ESA takes part in the construction and operation of the ISS, with contributions such as Columbus, a science laboratory module that was brought into orbit by NASA's STS-122 Space Shuttle mission, and the Cupola observatory module that was completed in July 2005 by Alenia Spazio for ESA. The current estimates for the ISS are approaching €100 billion in total (development, construction and 10 years of maintaining the station) of which ESA has committed to paying €8 billion. About 90% of the costs of ESA's ISS share will be contributed by Germany (41%), France (28%) and Italy (20%). German ESA astronaut Thomas Reiter was the first long-term ISS crew member.

ESA has developed the Automated Transfer Vehicle for ISS resupply. Each ATV has a cargo capacity of . The first ATV, "Jules Verne", was launched on 9 March 2008 and on 3 April 2008 successfully docked with the ISS. This manoeuvre, considered a major technical feat, involved using automated systems to allow the ATV to track the ISS, moving at 27,000 km/h, and attach itself with an accuracy of 2 cm. Five vehicles were launched before the program ended with the launch of the fifth ATV, "Georges Lemaître", in 2014.

As of 2020, the spacecraft establishing supply links to the ISS are the Russian Progress and Soyuz, Japanese Kounotori (HTV), and the United States vehicles Cargo Dragon 2 and Cygnus stemmed from the Commercial Resupply Services program.

European Life and Physical Sciences research on board the International Space Station (ISS) is mainly based on the European Programme for Life and Physical Sciences in Space programme that was initiated in 2001.


The ESA is an independent space agency and not under the jurisdiction of the European Union, although they have common goals, share funding, and work together often.
The initial aim of the European Union (EU) was to make the European Space Agency an agency of the EU by 2014. While the EU and its member states fund together 86% of the budget of ESA, it is not an EU agency. Furthermore, ESA has several non-EU members, most notably the United Kingdom which had left the EU while remaining a full member of ESA. ESA is partnered with the EU on its two current flagship space programs, the Copernicus series of Earth observation satellites and the Galileo satellite navigation system, with ESA providing technical oversight and, in the case of Copernicus, some of the funding. The EU, though, has shown an interest in expanding into new areas, whence the proposal to rename and expand its satellite navigation agency (the European GNSS Agency) into the EU Agency for the Space Programme. The proposal drew strong criticism from ESA, as it was perceived as encroaching on ESA's turf.

In January 2021, after years of acrimonious relations, EU and ESA officials mended their relationship, with the EU Internal Market commissioner Thierry Breton saying "The European space policy will continue to rely on ESA and its unique technical, engineering and science expertise," and that "ESA will continue to be the European agency for space matters. If we are to be successful in our European strategy for space, and we will be, I will need ESA by my side." ESA director Aschbacher reciprocated, saying "I would really like to make ESA the main agency, the go-to agency of the European Commission for all its flagship programs." ESA and EUSPA are now seen to have distinct roles and competencies, which will be officialized in the Financial Framework Partnership Agreement (FFPA). Whereas ESA's focus will be on the technical elements of the EU space programs, EUSPA will handle the operational elements of those programs.

On 3 August 1984, ESA's Paris headquarters were severely damaged and six people were hurt when a bomb exploded. It was planted by the far-left armed Action Directe group.

On 14 December 2015, hackers from Anonymous breached ESA's subdomains and leaked thousands of login credentials.




Embouchure

Embouchure () or lipping is the use of the lips, facial muscles, tongue, and teeth in playing a wind instrument. This includes shaping the lips to the mouthpiece of a woodwind instrument or the mouthpiece of a brass instrument. The word is of French origin and is related to the root "", 'mouth'. Proper embouchure allows instrumentalists to play their instrument at its full range with a full, clear tone and without strain or damage to their muscles.

While performing on a brass instrument, the sound is produced by the player buzzing their lips into a mouthpiece. Pitches are changed in part through altering the amount of muscular contraction in the lip formation. The performer's use of the air, tightening of cheek and jaw muscles, as well as tongue manipulation can affect how the embouchure works.

Maintaining an effective embouchure is an essential skill for any brass instrumentalist, but its personal and particular characteristics mean that different pedagogues and researchers have advocated differing, even contradictory, advice on what proper embouchure is and how it should be taught. One point on which there is some agreement is that proper embouchure is not one-size-fits-all: individual differences in dental structure, lip shape and size, jaw shape and the degree of jaw malocclusion, and other anatomical factors will affect whether a particular embouchure technique will be effective or not.

In 1962, Philip Farkas hypothesized that the air stream traveling through the lip aperture should be directed straight down the shank of the mouthpiece. He believed that it would be illogical to "violently deflect" the air stream downward at the point of where the air moves past the lips. In this text, Farkas also recommends that the lower jaw be protruded so that the upper and lower teeth are aligned.

In 1970, Farkas published a second text which contradicted his earlier writing. Out of 40 subjects, Farkas showed that 39 subjects directed the air downward to varying degrees and one subject directed the air in an upward direction at various degrees. The lower jaw position seen in these photographs shows more variation from his earlier text as well.

This supports what was written by trombonist and brass pedagogue Donald S. Reinhardt in 1942. In 1972, Reinhardt described and labeled different embouchure patterns according to such characteristics as mouthpiece placement and the general direction of the air stream as it travels past the lips. According to this later text, players who place the mouthpiece higher on the lips, so that more upper lip is inside the mouthpiece, will direct the air downwards to varying degrees while playing. Performers who place the mouthpiece lower, so that more lower lip is inside the mouthpiece, will direct the air to varying degrees in an upward manner. In order for the performer to be successful, the air stream direction and mouthpiece placement need to be personalized based on individual anatomical differences. Lloyd Leno confirmed the existence of both upstream and downstream embouchures.

More controversial was Reinhardt's description and recommendations regarding a phenomenon he termed a "pivot". According to Reinhardt, a successful brass embouchure depends on a motion wherein the performer moves both the mouthpiece and lips as a single unit along the teeth in an upward and downward direction. As the performer ascends in pitch, he or she will either move the lips and mouthpiece together slightly up towards the nose or pull them down together slightly towards the chin, and use the opposite motion to descend in pitch. Whether the player uses one general pivot direction or the other, and the degree to which the motion is performed, depends on the performer's anatomical features and stage of development. The placement of the mouthpiece upon the lips doesn't change, but rather the relationship of the rim and lips to the teeth. While the angle of the instrument may change as this motion follows the shape of the teeth and placement of the jaw, contrary to what many brass performers and teachers believe, the angle of the instrument does not actually constitute the motion Reinhardt advised as a pivot.

Later research supports Reinhardt's claim that this motion exists and might be advisable for brass performers to adopt. John Froelich describes how mouthpiece pressure towards the lips (vertical forces) and shear pressure (horizontal forces) functioned in three test groups, student trombonists, professional trombonists, and professional symphonic trombonists. Froelich noted that the symphonic trombonists used the least amount of both direct and shear forces and recommends this model be followed. Other research notes that virtually all brass performers rely upon the upward and downward embouchure motion. Other authors and pedagogues remain skeptical about the necessity of this motion, but scientific evidence supporting this view has not been sufficiently developed at this time.

Some noted brass pedagogues prefer to instruct the use of the embouchure from a less analytical point of view. Arnold Jacobs, a tubist and well-regarded brass teacher, believed that it was best for the student to focus on his or her use of the air and musical expression to allow the embouchure to develop naturally on its own. Other instructors, such as Carmine Caruso, believed that the brass player's embouchure could best be developed through coordination exercises and drills that bring all the muscles into balance that focus the student's attention on his or her time perception. Still other authors who have differing approaches to embouchure development include Louis Maggio, Jeff Smiley, Jerome Callet and Clint McLaughlin.

Most professional performers, as well as instructors, use a combination called a puckered smile. Farkas told people to blow as if they were trying to cool soup. Raphael Mendez advised saying the letter "M". The skin under the lower lip will be taut with no air pocket. The lips do not overlap nor do they roll in or out. The corners of the mouth are held firmly in place. To play with an extended range one should use a pivot, tongue arch and lip to lip compression.

According to Farkas the mouthpiece should have upper lip and lower lip (French horn), lower lip and upper lip (trumpet and cornet), and more latitude for lower brass (trombone, baritone, and tuba). For trumpet, some also advocate upper lip and lower lip. Farkas claimed placement was more important for the instruments with smaller mouthpieces. The lips should not overlap each other, nor should they roll in or out. The mouth corners should be held firm. Farkas speculated that the horn should be held in a downward angle to allow the air stream to go straight into the mouthpiece, although his later text shows that air stream direction actually is either upstream or downstream and is dependent upon the ratio of upper or lower lip inside the mouthpiece, not the horn angle. Farkas advised to moisten the outside of the lips, then form the embouchure and gently place the mouthpiece on it. He also recommended there must be a gap of or so between the teeth so that the air flows freely.

Arban and Saint-Jacome were both cornet soloists and authors of well respected and still used method books. Arban stated undogmatically that he believed the mouthpiece should be placed on the top lip. Saint-Jacome to the contrary said dogmatically that the mouthpiece should be placed "two-thirds for the upper and the rest for the under according to all professors and one-third for the upper and two-thirds for the under according to one sole individual, whom I shall not name."

The Farkas set is the basis of most lip buzzing embouchures. Mendez did teach lip buzzing by making the student lip buzz for a month before they could play their trumpet and got great results. One can initiate this type of buzz by using the same sensation as spitting seeds, but maintaining a continued flow of air. This technique assists the development of the Farkas approach by preventing the player from using an aperture that is too open.

Stevens–Costello embouchure has its origins in the William Costello embouchure and was further developed by Roy Stevens. It uses a slight rolling in of both lips and touching evenly all the way across. It also uses mouthpiece placement of about 40–50% top lip and 50–60% lower lip. The teeth will be about apart and the teeth are parallel or the jaw slightly forward.

There is relative mouthpiece pressure to the given air column. One exercise to practice the proper weight to air relationship is the palm exercise where the player holds the horn by laying it on its side in the palm of the hand, not grasping it. The lips are placed on the mouthpiece and the player blows utilizing the weight of the horn in establishing a sound.

A puckered embouchure, used by most players, and sometimes used by jazz players for extremely high "screamer" notes. Maggio claimed that the pucker embouchure gives more endurance than some systems. Carlton MacBeth is the main proponent of the pucker embouchure. The Maggio system was established because Louis Maggio had sustained an injury which prevented him from playing. In this system the player cushions the lips by extending them or puckering (like a monkey). This puckering enables the players to overcome physical malformations. It also lets the player play for an extended time in the upper register. The pucker can make it easy to use to open an aperture. Much very soft practice can help overcome this. Claude Gordon was a student of Louis Maggio and Herbert L. Clarke and systematized the concepts of these teachers. Claude Gordon made use of pedal tones for embouchure development as did Maggio and Herbert L. Clarke. All three stressed that the mouthpiece should be placed higher on the top lip for a more free vibration of the lips.

This embouchure method, advocated by a minority of brass pedagogues such as Jerome Callet, has not yet been sufficiently researched to support the claims that this system is the most effective approach for all brass performers.

Advocates of Callet's approach believe that this method was recommended and taught by the great brass instructors of the early 20th century. Two French trumpet technique books, authored by Jean-Baptiste Arban and Saint-Jacome, were translated into English for use by American players. According to some, due to a misunderstanding arising from differences in pronunciation between French and English, the commonly used brass embouchure in Europe was incorrectly interpreted. Callet attributes this difference in embouchure technique as the reason the great players of the past were able to play at the level of technical virtuosity which they did, although the increased difficulty of contemporary compositions for brass seem to indicate that the level of brass technique achieved by today's performers equals or even exceeds that of most performers from the late 19th and early 20th centuries.

Callet's method of brass embouchure consists of the tongue remaining forward and through the teeth at all times. The corners of the mouth always remain relaxed, and only a small amount of air is used. The top and bottom lips curl inward and grip the forward tongue. The tongue will force the teeth, and subsequently the throat, wide open, supposedly resulting in a bigger, more open sound. The forward tongue resists the pressure of the mouthpiece, controls the flow of air for lower and higher notes, and protects the lips and teeth from damage or injury from mouthpiece pressure. Because of the importance of the tongue in this method many refer to this as a "tongue-controlled embouchure". This technique facilitates the use of a smaller mouthpiece and larger bore instruments. It results in improved intonation and stronger harmonically related partials across the player's range.

A variety of transverse flute embouchures are employed by professional flautists, though the most natural form is perfectly symmetrical, the corners of the mouth relaxed (i.e. not smiling), the lower lip placed along and at a short distance from the embouchure hole. It must be stressed, however, that achieving a symmetrical, or perfectly centred blowing hole ought not to be an end in itself. Indeed, French flautist Marcel Moyse did not play with a symmetrical embouchure.

The end-blown xiao, kaval, shakuhachi and hocchiku flutes demand especially difficult embouchures, sometimes requiring many lessons before any sound can be produced.

The embouchure is an important element to tone production. The right embouchure, developed with "time, patience, and intelligent work", will produce a beautiful sound and a correct intonation. The embouchure is produced with the muscles around the lips: principally the orbicularis oris muscle and the depressor anguli oris, whilst avoiding activation of zygomaticus major, which will produce a smile, flattening the top lip against the maxillary (upper jaw) teeth. Beginner flute-players tend to suffer fatigue in these muscles, and notably struggle to use the depressor muscle, which necessarily helps to keep the top lip directing the flow of air across the embouchure hole. These muscles have to be properly warmed up and exercised before practicing. Tone-development exercises including long notes and harmonics must be done as part of the warm up daily.

Some further adjustments to the embouchure are necessary when moving from the transverse orchestral flute to the piccolo. With the piccolo, it becomes necessary to place the near side of the embouchure hole slightly higher on the lower lip, i.e. above the lip margin, and greater muscle tone from the lip muscles is needed to keep the stream/pressure of air directed across the smaller embouchure hole, particularly when playing in higher piccolo registers.

With the woodwinds, aside from the flute, piccolo, and recorder, the sound is generated by a reed and not with the lips. The embouchure is therefore based on sealing the area around the reed and mouthpiece. This serves to prevent air from escaping while simultaneously supporting the reed, allowing it to vibrate, and constrict the reed preventing it from vibrating too much. With woodwinds, it is important to ensure that the mouthpiece is not placed too far into the mouth, which would result in too much vibration (no control), often creating a sound an octave (or harmonic twelfth for the clarinet) above the intended note. If the mouthpiece is not placed far enough into the mouth, no sound will be generated, as the reed will not vibrate.

The standard embouchures for single reed woodwinds like the clarinet and saxophone are variants of the "single lip embouchure", formed by resting the reed upon the bottom lip, which rests on the teeth and is supported by the chin muscles and the buccinator muscles on the sides of the mouth. The top teeth rest on top of the mouthpiece. The manner in which the lower lip rests against the teeth differs between clarinet and saxophone embouchures. In clarinet playing, the lower lip is rolled over the teeth and corners of the mouth are drawn back, which has the effect of drawing the upper lip around the mouthpiece to create a seal due to the angle at which the mouthpiece rests in the mouth. With the saxophone embouchure, the lower lip rests against, but not over, the teeth as in pronouncing the letter "V" and the corners of the lip are drawn in (similar to a drawstring bag). With the less common double-lip embouchure, the top lip is placed under (around) the top teeth, an alternative embouchure sometimes recommended by dentists for single-reed players for whom the single-lip approach is potentially harmful. In both instances, the position of the tongue in the mouth plays a vital role in focusing and accelerating the air stream blown by the player. This results in a more mature and full sound, rich in overtones.

The double reed woodwinds, the oboe and bassoon, have no mouthpiece. Instead the reed is two pieces of cane extending from a metal tube (oboe – staple) or placed on a bocal (bassoon, English horn). The reed is placed directly on the lips and then played like the double-lip embouchure described above. Compared to the single reed woodwinds, the reed is very small and subtle changes in the embouchure can have a dramatic effect on tuning, tone and pitch control.




Elephant 6

The Elephant 6 Recording Company is a loosely defined musical collective from the United States. Notable bands associated with the collective include the Apples in Stereo, Beulah, Circulatory System, Elf Power, the Minders, Neutral Milk Hotel, of Montreal, and the Olivia Tremor Control. Although bands in Elephant 6 explore many different genres, they have a shared interest in psychedelic pop of the 1960s, with particular influence from bands such as the Beach Boys, the Beatles, and the Zombies. Their music sometimes features intentionally low fidelity production and experimental recording techniques.

The collective started in Ruston, Louisiana in the late 1980s. The name was occasionally used to denote the home recordings made by four high school friends: Bill Doss, Will Cullen Hart, Jeff Mangum, and Robert Schneider. After high school, Schneider formed the Apples in Stereo; Doss, Hart, and Mangum formed the Olivia Tremor Control; and Mangum independently formed Neutral Milk Hotel. These three bands would serve as the basis for Elephant 6, and soon, many other bands joined. Athens, Georgia, and Denver, Colorado, became major hub cities, and the mid-to-late 1990s represented the peak years of activity for the collective.

Due to the confluence of new bands and the dissolution of Neutral Milk Hotel and the Olivia Tremor Control, the collective stagnated in activity in the early 2000s. A brief resurgence in the late 2000s ended with the death of Doss, and in recent years the collective has remained relatively dormant. Journalists have described Elephant 6 as an important underground music movement, and a key contributor to the emergence of alternative rock and indie rock in the 1990s.

Noel Murray and Marcus Gilmer of "The A.V. Club" note the difficulty in defining the exact parameters of the collective due to the multitude of associated acts. Each act has their own unique sound, and musicians are often members of multiple bands. This problem is compounded by the fact that members will sometimes obfuscate the truth, such as misleading a "Rolling Stone" reporter into believing they lived in a communal compound in Athens. In 2012, the official Elephant 6 website read: "A collective, a label ... a cult? Elephant 6 may be all of these things or none of these depending on your point of view. And we're certainly not going to try to define what it is now!"

Elephant 6 originated in Ruston, Louisiana, in the late 1980s. The name was occasionally used to denote home recordings made by four high school friends: Bill Doss, Will Cullen Hart, Jeff Mangum, and Robert Schneider. These recordings were circulated between the four of them, and they did not seek approval from record labels or fanzines. Musician Laura Carter said: "They were just 13-year-old boys yelling, 'Fuck your mama,' and bashing on the drums as hard as they can. It was just kids having fun, and they would fill up a whole cassette tape with this." When the group decided to create an imaginary label for their music, Hart came up with the name Elephant 6.

When the four friends graduated high school, they dispersed to different cities in the United States, but continued to mail tapes to each other. Schneider moved to Denver, Colorado and formed a band called the Apples in 1992 with Jim McIntyre, Hilarie Sidney and Chris Parfitt. Doss, Hart, and Mangum moved to Athens, Georgia. The three were drawn to the city's burgeoning music scene, and played in a band called the Synthetic Flying Machine. While in Athens, the group began collaborating with New York musician Julian Koster. In 1993, the Synthetic Flying Machine evolved into a band called the Olivia Tremor Control, and the band gained local attention for their psychedelic sound, which was in contrast to the prevalent grunge sound of the 1990s.

In the 1990s, bands joined Elephant 6 through invitation. Inspired by the Surrealist Manifestos, members of the collective issued their own manifesto in small hand-drawn catalogs, found within early releases. According to Schneider: "We wanted [to find] these little pockets of people in different cities who listened to Pavement and the Beach Boys and were recording on 4-tracks." Schneider notes that another way a band may join is by simply having a similar sound. He uses Beulah as an example, and in reference to the band's sound, he said: "This is a kindred spirit. This is Elephant 6."

Schneider created a record label called the Elephant 6 Recording Company as a vehicle for the Apples music, and in 1993, the first recording released on the label was an extended play titled "Tidal Wave". Around this time, Mangum left the Olivia Tremor Control, and became a vagabond. While living in Seattle, Mangum released the song "Everything Is" on Cher Doll Records in 1994, and was the first member of the collective to have their music released on a mainstream label, although the release was not directly affiliated with the Elephant 6 collective and did not feature the Elephant 6 logo. Mangum released the song under the name Neutral Milk Hotel. The Apples were later known as the Apples in Stereo.

The mid-to-late 1990s saw the greatest amount of activity for the collective. The three main bands associated with Elephant 6 at the time–the Apples in Stereo, the Olivia Tremor Control, and Neutral Milk Hotel–grew in popularity, and each respectively released a notable album: "Fun Trick Noisemaker", "", and "In the Aeroplane Over the Sea". "The A.V. Club" wrote highly of "In the Aereoplane Over the Sea", saying it "is the culmination of everything the [Elephant 6] collective was about in the mid-'90s: distinctive, ragged, catchy records ripped straight from their makers' veins."

Many bands associated with the collective were formed during this period, and Athens became a major hub city. Elf Power, of Montreal, and Doss' solo project the Sunshine Fix were among the more notable Athens based groups. of Montreal frontperson Kevin Barnes said: "The heyday, most of the late 1990s, everyone was involved in each others lives, and we would collaborate more, have dinners where everyone would make something." Schneider compares this period to the Summer of Love, and said the driving force for many of the bands was "out-weirding [their] neighbor" with their music. Elephant 6 bands would tour with each other, the larger bands allowed the smaller bands to open for them.

Denver was the smaller of the two hub cities. In addition to the Apples in Stereo, the major bands from Denver were the Minders, Dressy Bessy, and McIntyre's solo project Von Hemmling. The main draw for Elephant 6 bands in Denver was Pet Sounds Studio, a recording studio Schneider built in McIntyre's house. Many Elephant 6 albums were recorded at Pet Sounds, and were produced by Schneider. In addition to the two main hub cities, Elephant 6 bands began forming in various cities in the United States, such as the Essex Green and the Ladybug Transistor in Brooklyn, and Beulah in San Francisco.

In the early 2000s, Elephant 6 stagnated in activity. Neutral Milk Hotel member and the Gerbils frontman Scott Spillane identifies the sudden uptick of bands across the country as an important factor to this period. "At the time the Elephant 6 thing was getting out of hand, and we started seeing all of these bands that had little Elephant 6 logos on them all over the place" said Spillane. Bands began to tour more often, and the members had less time to interact with each other. Additionally, Neutral Milk Hotel and the Olivia Tremor Control went on hiatus. Mangum became reclusive as he struggled to cope with his newfound stardom, while the members of the Olivia Tremor Control wanted to record their own solo music.

Beulah member Pat Noel said many bands were dismayed at how journalists would "pigeonhole" them to the collective. "We kind of made a conscious decision to distance ourselves a little bit from the whole thing." Schneider took a break from producing albums, and the final album to be affixed with the Elephant 6 Recording Company logo was "Cul-De-Sacs and Dead Ends" by the Minders in 1999. The collective slowly dissipated, although bands like the Apples in Stereo, Elf Power, and of Montreal continued making music throughout the 2000s.

The collective was relatively dormant until the release of "New Magnetic Wonder", a 2007 album by the Apples in Stereo. "New Magnetic Wonder" featured all four of the collective's originating members. While recording the album, they discussed new ideas, which in turn facilitated a need to make more music. The following year, Koster organized the "Elephant 6 Holiday Surprise Tour," a short concert tour that featured fifteen artists and ten Elephant 6 bands. Koster said "Elephant 6 is back," and added: "Somehow, everything's happening for us now. I don't know why we were ever interrupted, and why all this is happening now. But we're all just so happy." The Olivia Tremor Control reunited in 2009, and Mangum returned to the public eye with solo concerts over the next few years.

On July 30, 2012, Doss died from a reported aneurysm. His death came as a shock to the collective, and stalled nearly all recordings at the time. Schneider said: "I can't say what it means for the Elephant 6 or the Apples ... On a musical level it's too soon to say. I mean, I don't want to say definitively that I don't want to make music again, but on a musical level there's no way to come to terms with the loss." The Olivia Tremor Control continued making music, and in 2017 Schneider confirmed he was producing unfinished recordings. Today, the Elephant 6 collective still exists, albeit on a much smaller scale. Bands like Elf Power and of Montreal continue to record music, and many bands have moved onto Elephant 6 offshoot labels such as Orange Twin Records and Cloud Recordings.

Elephant 6 bands explore a variety of music genres, including indie rock, synth-pop, and twee pop. A common interest for nearly every associated band, however, is psychedelic pop of the 1960s. Bands such as the Beach Boys, the Beatles, and the Zombies are important influences for Elephant 6 groups like the Apples in Stereo, Beulah, and the Olivia Tremor Control. Elephant 6's de facto leader Robert Schneider notes the particular influence of the Beach Boys' unfinished album "Smile", calling it the "Holy Grail" for many members of the collective. He notes how he and other members were obsessed with Beach Boys albums, and attempted to create the type of music they felt would have been included in "Smile".

Most Elephant 6 members are anti-consumerism and possess a DIY ethic. Their music sometimes features intentionally low quality production, and bands may experiment with unique recording methods; for example, "Music from the Unrealized Film Script: Dusk at Cubist Castle" features recording techniques such as tape manipulation and sound collages. Schneider notes his hatred of both indie music and modern pop music, and said that his vision for Elephant 6 is a "perfect pop world," untarnished by commercial interests.

Several journalists regard Elephant 6 as an important underground music movement, and a key contributor to the alternative rock and indie rock explosion in the 1990s. Lee M. Shook Jr. of "Paste" wrote: "The Elephant 6 Recording Company would raise the bar for wide-scale countercultural activity and underground pop art—both musical, visual and otherwise—well into the 21st century." Tom Murphy of "Westword" expanded on this statement, by writing: "It became a movement of sorts because the music was so accessible and inclusive of a wide range of musical expression, allowing for immediate and enduring growth, however loose the association."

The collective has influenced many indie rock bands, including Arcade Fire, Franz Ferdinand, and Tame Impala. Chris Chu of the Canadian band the Morning Benders said: "Elephant 6 was the gateway for me. They seemed to be bridging that tradition from the 60s to a more modern, more indie approach. It was exactly what I was looking for, a new take on that stuff."

In 2022 a documentary was released called "A Future History Of: The Elephant 6 Recording Co." On RottenTomatoes the film currently has a 100% positive review rate.

The official Elephant 6 website lists forty-seven acts associated with the collective, although Shook Jr. reports there are more than fifty. Among the collective's more notable acts include:


Echolocation

Echolocation is the use of sound as a form of navigation.




Evangelicalism

Evangelicalism (), also called evangelical Christianity or evangelical Protestantism, is a worldwide interdenominational movement within Protestant Christianity that emphasizes the centrality of sharing the "good news" of Christianity, being "born again" in which an individual experiences personal conversion, as authoritatively guided by the Bible, God's revelation to humanity. The word "evangelical" comes from the Greek word for "good news" ("euangelion").

The theological nature of evangelicalism was first explored during the Protestant Reformation in 16th century Europe. Martin Luther's Ninety-Five Theses in 1517 emphasized that scripture and the preaching of the gospel had ultimate authority over the practices of the Church. The origins of modern evangelicalism are usually traced to 1738, with various theological streams contributing to its foundation, including Pietism and Radical Pietism, Puritanism, Quakerism, Presbyterianism and Moravianism (in particular its bishop Nicolaus Zinzendorf and his community at Herrnhut). Preeminently, John Wesley and other early Methodists were at the root of sparking this new movement during the First Great Awakening. Today, evangelicals are found across many Protestant branches, as well as in various denominations around the world, not subsumed to a specific branch. Among leaders and major figures of the evangelical Protestant movement were Nicolaus Zinzendorf, George Fox, John Wesley, George Whitefield, Jonathan Edwards, Billy Graham, Bill Bright, Harold Ockenga, Gudina Tumsa, John Stott, Francisco Olazábal, William J. Seymour, and Martyn Lloyd-Jones.

The movement has long had a presence in the Anglosphere before spreading further afield in the 19th, 20th, and early 21st centuries. The movement gained significant momentum during the 18th and 19th centuries with the Great Awakening in Great Britain and the United States.

The word "evangelical" has its etymological roots in the Greek word for "gospel" or "good news": "euangelion", from "eu" "good", "angel"- the stem of, among other words, "angelos" "messenger, angel", and the neuter suffix -"ion". By the English Middle Ages, the term had expanded semantically to include not only the message, but also the New Testament which contained the message as well as more specifically the Gospels, which portray the life, death, and resurrection of Jesus. The first published use of "evangelical" in English was in 1531, when William Tyndale wrote "He exhorteth them to proceed constantly in the evangelical truth." One year later, Thomas More wrote the earliest recorded use in reference to a theological distinction when he spoke of "Tyndale [and] his evangelical brother Barns."

During the Reformation, Protestant theologians embraced the term as referring to "gospel truth." Martin Luther referred to the "evangelische Kirche" ("evangelical church") to distinguish Protestants from Catholics in the Catholic Church. Into the 21st century, "evangelical" has continued in use as a synonym for Mainline Protestant in continental Europe. This usage is reflected in the names of Protestant denominations, such as the Evangelical Lutheran Church in America. The German term more accurately corresponds to the broad English term "Protestant" and should not be confused with the narrower German term evangelikal," or the term pietistisch (a term etymologically related to the Pietist and Radical Pietist movements), which are used to described Evangelicalism in the sense used in this article. Mainline Protestant denominations with a Lutheran or semi-Lutheran background, like the Evangelical Lutheran Church in America, the Evangelical Lutheran Church in Canada, and the Evangelical Lutheran Church of England, who are not evangelical in the "evangelikal"" sense but Protestant in the ""evangelisch"" sense, have translated the German term ""evangelisch"" (or Protestant) into the English term "Evangelical", although the two German words have different meanings. In other parts of the world, especially in the English-speaking world, evangelical (German: evangelikal or pietistisch) is commonly applied to describe the interdenominational Born-Again believing movement.

Christian historian David W. Bebbington writes that, "Although 'evangelical,' with a lower-case initial, is occasionally used to mean 'of the gospel,' the term 'Evangelical' with a capital letter, is applied to any aspect of the movement beginning in the 1730s." According to the "Oxford English Dictionary", "evangelicalism" was first used in 1831. In 1812, the term "evangelicalism" appeared in "The History of Lynn" by William Richards. In the summer of 1811 the term "evangelicalists" was used in "The Sin and Danger of Schism" by Rev. Dr. Andrew Burnaby, Archdeacon of Leicester.

The term may also be used outside any religious context to characterize a generic missionary, reforming, or redeeming impulse or purpose. For example, "The Times Literary Supplement" refers to "the rise and fall of evangelical fervor within the Socialist movement." This usage refers to evangelism, rather than evangelicalism as discussed here; though sharing an etymology and conceptual basis, the words have diverged significantly in meaning.

One influential definition of evangelicalism has been proposed by historian David Bebbington. Bebbington notes four distinctive aspects of evangelical faith: conversionism, biblicism, crucicentrism, and activism, noting, "Together they form a quadrilateral of priorities that is the basis of Evangelicalism."

Conversionism, or belief in the necessity of being "born again," has been a constant theme of evangelicalism since its beginnings. To evangelicals, the central message of the gospel is justification by faith in Christ and repentance, or turning away, from sin. Conversion differentiates the Christian from the non-Christian, and the change in life it leads to is marked by both a rejection of sin and a corresponding personal holiness of life. A conversion experience can be emotional, including grief and sorrow for sin followed by great relief at receiving forgiveness. The stress on conversion differentiates evangelicalism from other forms of Protestantism by the associated belief that an assurance will accompany conversion. Among evangelicals, individuals have testified to both sudden and gradual conversions.

Biblicism is reverence for the Bible and high regard for biblical authority. All evangelicals believe in biblical inspiration, though they disagree over how this inspiration should be defined. Many evangelicals believe in biblical inerrancy, while other evangelicals believe in biblical infallibility.

Crucicentrism is the centrality that evangelicals give to the Atonement, the saving death and the resurrection of Jesus, that offers forgiveness of sins and new life. This is understood most commonly in terms of a substitutionary atonement, in which Christ died as a substitute for sinful humanity by taking on himself the guilt and punishment for sin.

Activism describes the tendency toward active expression and sharing of the gospel in diverse ways that include preaching and social action. This aspect of evangelicalism continues to be seen today in the proliferation of evangelical voluntary religious groups and parachurch organizations.

The word "church" has several meanings among evangelicals. It can refer to the universal church (the body of Christ) including all Christians everywhere. It can also refer to the church (congregation), which is the visible representation of the invisible church. It is responsible for teaching and administering the sacraments or ordinances (baptism and the Lord's Supper, but some evangelicals also count footwashing as an ordinance as well).

Many evangelical traditions adhere to the doctrine of the believers' Church, which teaches that one becomes a member of the Church by the new birth and profession of faith. This originated in the Radical Reformation with Anabaptists but is held by denominations that practice believer's baptism. Evangelicals in the Anglican, Methodist and Reformed traditions practice infant baptism as one's initiation into the community of faith and the New Testament counterpart to circumcision, while also stressing the necessity of personal conversion later in life for salvation.

Some evangelical denominations operate according to episcopal polity or presbyterian polity. However, the most common form of church government within Evangelicalism is congregational polity. This is especially common among nondenominational evangelical churches. Many churches are members of a national and international denomination for a cooperative relationship in common organizations, for the mission and social areas, such as humanitarian aid, schools, theological institutes and hospitals. Common ministries within evangelical congregations are pastor, elder, deacon, evangelist and worship leader. The ministry of bishop with a function of supervision over churches on a regional or national scale is present in all the Evangelical Christian denominations, even if the titles president of the council or general overseer are mainly used for this function. The term bishop is explicitly used in certain denominations. Some evangelical denominations are members of the World Evangelical Alliance and its 129 national alliances.

Some evangelical denominations officially authorize the ordination of women in churches. The female ministry is justified by the fact that Mary Magdalene was chosen by Jesus to announce his resurrection to the apostles. The first Baptist woman who was consecrated pastor is the American Clarissa Danforth in the denomination Free Will Baptist in 1815. In 1882, in the American Baptist Churches USA. In the Assemblies of God of the United States, since 1927. In 1965, in the National Baptist Convention, USA. In 1969, in the Progressive National Baptist Convention. In 1975, in The Foursquare Church.

For evangelicals, there are three interrelated meanings to the term "worship". It can refer to living a "God-pleasing and God-focused way of life," specific actions of praise to God, and a public Worship service. Diversity characterizes evangelical worship practices. Liturgical, contemporary, charismatic and seeker-sensitive worship styles can all be found among evangelical churches. Overall, evangelicals tend to be more flexible and experimental with worship practices than mainline Protestant churches. It is usually run by a Christian pastor. A service is often divided into several parts, including congregational singing, a sermon, intercessory prayer, and other ministry. During worship there is usually a nursery for babies. Children and young people receive an adapted education, Sunday school, in a separate room.
Places of worship are usually called "churches." In some megachurches, the building is called "campus." The architecture of places of worship is mainly characterized by its sobriety. The Latin cross is one of the only spiritual symbols that can usually be seen on the building of an evangelical church and that identifies the place's belonging.

Some services take place in theaters, schools or multipurpose rooms, rented for Sunday only. Because of their understanding of the second of the Ten Commandments, some evangelicals do not have religious material representations such as statues, icons, or paintings in their places of worship. There is usually a baptistery on what is variously known as the chancel (also called sanctuary) or stage, though they may be alternatively found in a separate room, for the baptisms by immersion.

In some countries of the world which apply sharia or communism, government authorizations for worship are complex for Evangelical Christians. Because of persecution of Christians, Evangelical house churches are the only option for many Christians to live their faith in community. For example, there is the Evangelical house churches in China movement. The meetings thus take place in private houses, in secret and in illegality.

The main Christian feasts celebrated by the Evangelicals are Christmas, Pentecost (by a majority of Evangelical denominations) and Easter for all believers.

Evangelical churches have been involved in the establishment of elementary and secondary schools. It also enabled the development of several bible colleges, colleges and universities in the United States during the 19th century. Other evangelical universities have been established in various countries of the world.

The Council for Christian Colleges and Universities was founded in 1976. In 2023, the CCCU had 185 members in 21 countries.

The Association of Christian Schools International was founded in 1978 by 3 American associations of evangelical Christian schools. Various international schools have joined the network. In 2023, it had 23,000 schools in 100 countries.

The International Council for Evangelical Theological Education was founded in 1980 by the Theological Commission of the World Evangelical Alliance. In 2023, it had 850 member schools in 113 countries.

In matters of sexuality, several evangelical churches promote the virginity pledge (abstinence pledge) among young evangelical Christians, who are invited to commit themselves, during a public ceremony, to sexual abstinence until Christian marriage. This pledge is often symbolized by a purity ring.

In some evangelical churches, young adults and unmarried couples are encouraged to marry early in order to live a sexuality according to the will of God.

A 2009 American study of the National Campaign to Prevent Teen and Unplanned Pregnancy reported that 80 percent of young, unmarried evangelicals had had sex and that 42 percent were in a relationship with sex, when surveyed.

The majority of evangelical Christian churches are against abortion and support adoption agencies and social support agencies for young mothers.

Masturbation is seen as forbidden by some evangelical pastors because of the sexual thoughts that may accompany it. However, evangelical pastors have pointed out that the practice has been erroneously associated with Onan by scholars, that it is not a sin if it is not practiced with fantasies or compulsively, and that it was useful in a married couple, if his or her partner did not have the same frequency of sexual needs.

Some evangelical churches speak only of sexual abstinence and do not speak of sexuality in marriage. Other evangelical churches in the United States and Switzerland speak of satisfying sexuality as a gift from God and a component of a Christian marriage harmonious, in messages during worship services or conferences. Many evangelical books and websites are specialized on the subject. The book "" published in 1976 by Baptist pastor Tim LaHaye and his wife Beverly LaHaye was a pioneer in the field.

The perceptions of homosexuality in the Evangelical Churches are varied. They range from liberal to fundamentalist or moderate Conservative and neutral. A 2011 Pew Research Center study found that 84 percent of evangelical leaders surveyed believed homosexuality should be discouraged. It is in the fundamentalist conservative positions, that there are antigay activists on TV or radio who claim that homosexuality is the cause of many social problems, such as terrorism. Some churches have a Conservative moderate position. Although they do not approve homosexual practices, they claim to show sympathy and respect for homosexuals. Some evangelical denominations have adopted neutral positions, leaving the choice to local churches to decide for same-sex marriage. There are some international evangelical denominations that are gay-friendly.

The christian marriage is presented by some churches as a protection against sexual misconduct and a compulsory step to obtain a position of responsibility in the church. This concept, however, has been challenged by numerous sex scandals involving married evangelical leaders. Finally, evangelical theologians recalled that celibacy should be more valued in the Church today, since the gift of celibacy was taught and lived by Jesus Christ and Paul of Tarsus.

For a majority of evangelical Christians, a belief in biblical inerrancy ensures that the miracles described in the Bible are still relevant and may be present in the life of the believer. Healings, academic or professional successes, the birth of a child after several attempts, the end of an addiction, etc., would be tangible examples of God's intervention with the faith and prayer, by the Holy Spirit. In the 1980s, the neo-charismatic movement re-emphasized miracles and faith healing. In certain churches, a special place is thus reserved for faith healings with laying on of hands during worship services or for evangelization campaigns. Faith healing or divine healing is considered to be an inheritance of Jesus acquired by his death and resurrection. This view is typically ascribed to Pentecostal denominations, and not others that are cessationist (believing that miraculous gifts have ceased.)

In terms of denominational beliefs regarding science and the origin of the earth and human life, some evangelicals support young Earth creationism. For example, Answers in Genesis, founded in Australia in 1986, is an evangelical organization that seeks to defend the thesis. In 2007, they founded the Creation Museum in Petersburg, in Kentucky 
and in 2016 the Ark Encounter in Williamstown. Since the end of the 20th century, literalist creationism has been abandoned by some evangelicals in favor of intelligent design. For example, the think tank Discovery Institute, established in 1991 in Seattle, defends this thesis. Other evangelicals who accept the scientific consensus on evolution and the age of Earth believe in theistic evolution or evolutionary creation—the notion that God used the process of evolution to create life; a Christian organization that espouses this view is the BioLogos Foundation.

The Reformed, Baptist, Methodist, Pentecostal, Churches of Christ, Plymouth Brethren, charismatic Protestant, and nondenominational Protestant traditions have all had strong influence within contemporary evangelicalism. Some Anabaptist denominations (such as the Brethren Church) are evangelical, and some Lutherans self-identify as evangelicals. There are also evangelical Anglicans and Quakers.

In the early 20th century, evangelical influence declined within mainline Protestantism and Christian fundamentalism developed as a distinct religious movement. Between 1950 and 2000 a mainstream evangelical consensus developed that sought to be more inclusive and more culturally relevant than fundamentalism while maintaining theologically conservative Protestant teaching. According to Brian Stanley, professor of world Christianity, this new postwar consensus is termed "neoevangelicalism", the "new evangelicalism", or simply "evangelicalism" in the United States, while in Great Britain and in other English-speaking countries, it is commonly termed "conservative evangelicalism". Over the years, less conservative evangelicals have challenged this mainstream consensus to varying degrees. Such movements have been classified by a variety of labels, such as progressive, open, postconservative, and postevangelical.

Evangelical leaders like Tony Perkins of the Family Research Council have called attention to the problem of equating the term "Christian right" with theological conservatism and Evangelicalism. Although evangelicals constitute the core constituency of the Christian right within the United States, not all evangelicals fit that political description (and not all of the Christian right are evangelicals). The problem of describing the Christian right which in most cases is conflated with theological conservatism in secular media, is further complicated by the fact that the label "religious conservative" or "conservative Christian" applies to other religious groups who are theologically, socially, and culturally conservative but do not have overtly political organizations associated with some of these Christian denominations, which are usually uninvolved, uninterested, apathetic, or indifferent towards politics. Tim Keller, an Evangelical theologian and Presbyterian Church in America pastor, shows that Conservative Christianity (theology) predates the Christian right (politics), and that being a theological conservative didn't necessitate being a political conservative, that some political progressive views around economics, helping the poor, the redistribution of wealth, and racial diversity are compatible with theologically conservative Christianity. Rod Dreher, a senior editor for "The American Conservative", a secular conservative magazine, also argues the same differences, even claiming that a "traditional Christian" a theological conservative, can simultaneously be left on economics (economic progressive) and even a socialist at that while maintaining traditional Christian beliefs.

Outside of self-consciously evangelical denominations, there is a broader "evangelical streak" in mainline Protestantism. Mainline Protestant churches predominantly have a liberal theology while evangelical churches predominantly have a fundamentalist or moderate conservative theology.

Some commentators have complained that Evangelicalism as a movement is too broad and its definition too vague to be of any practical value. Theologian Donald Dayton has called for a "moratorium" on use of the term. Historian D. G. Hart has also argued that "evangelicalism needs to be relinquished as a religious identity because it does not exist".

Christian fundamentalism has been called a subset or "subspecies" of Evangelicalism. Fundamentalism regards biblical inerrancy, the virgin birth of Jesus, penal substitutionary atonement, the literal resurrection of Christ, and the Second Coming of Christ as fundamental Christian doctrines. 
Fundamentalism arose among evangelicals in the 1920s—primarily as an American phenomenon, but with counterparts in Britain and British Empire—to combat modernist or liberal theology in mainline Protestant churches. Failing to reform the mainline churches, fundamentalists separated from them and established their own churches, refusing to participate in ecumenical organizations (such as the National Council of Churches, founded in 1950), and making separatism (rigid separation from nonfundamentalist churches and their culture) a true test of faith. Most fundamentalists are Baptists and dispensationalist or Pentecostals and Charismatics.

Great emphasis is placed on the literal interpretation of the Bible as the primary method of Bible study as well as the biblical inerrancy and the infallibility of their interpretation. Adherence to conspiracy theories is particularly important.

Mainstream evangelicalism is historically divided between two main orientations: confessionalism and revivalism. These two streams have been critical of each other. Confessional evangelicals have been suspicious of unguarded religious experience, while revivalist evangelicals have been critical of overly intellectual teaching that (they suspect) stifles vibrant spirituality. In an effort to broaden their appeal, many contemporary evangelical congregations intentionally avoid identifying with any single form of evangelicalism. These "generic evangelicals" are usually theologically and socially conservative, but their churches often present themselves as nondenominational (or, if a denominational member, strongly deemphasize its ties to such, such as a church name which excludes the denominational name) within the broader evangelical movement.

In the words of Albert Mohler, president of the Southern Baptist Theological Seminary, confessional evangelicalism refers to "that movement of Christian believers who seek a constant convictional continuity with the theological formulas of the Protestant Reformation". While approving of the evangelical distinctions proposed by Bebbington, confessional evangelicals believe that authentic evangelicalism requires more concrete definition in order to protect the movement from theological liberalism and from heresy. According to confessional evangelicals, subscription to the ecumenical creeds and to the Reformation-era confessions of faith (such as the confessions of the Reformed churches) provides such protection. Confessional evangelicals are represented by conservative Presbyterian churches (emphasizing the Westminster Confession), certain Baptist churches that emphasize historic Baptist confessions such as the Second London Confession, evangelical Anglicans who emphasize the Thirty-Nine Articles (such as in the Anglican Diocese of Sydney, Australia), Methodist churches that adhere to the Articles of Religion, and some confessional Lutherans with pietistic convictions.

The emphasis on historic Protestant orthodoxy among confessional evangelicals stands in direct contrast to an anticreedal outlook that has exerted its own influence on evangelicalism, particularly among churches strongly affected by revivalism and by pietism. Revivalist evangelicals are represented by some quarters of Methodism, the Wesleyan Holiness churches, the Pentecostal and charismatic churches, some Anabaptist churches, and some Baptists and Presbyterians. Revivalist evangelicals tend to place greater emphasis on religious experience than their confessional counterparts.

Moderate evangelical Christianity emerged in the 1940s in the United States in response to the Fundamentalist movement of the 1910s. In the late 1940s, evangelical theologians from Fuller Theological Seminary founded in Pasadena, California, in 1947, championed the Christian importance of social activism. In this movement called neo-evangelicalism, new organizations, social agencies, media and Bible colleges were established in the 1950s.

Evangelicals dissatisfied with the movement's fundamentalism mainstream have been variously described as progressive evangelicals, postconservative evangelicals, open evangelicals and postevangelicals. Progressive evangelicals, also known as the evangelical left, share theological or social views with other progressive Christians while also identifying with evangelicalism. Progressive evangelicals commonly advocate for women's equality, pacifism and social justice.

As described by Baptist theologian Roger E. Olson, postconservative evangelicalism is a theological school of thought that adheres to the four marks of evangelicalism, while being less rigid and more inclusive of other Christians. According to Olson, postconservatives believe that doctrinal truth is secondary to spiritual experience shaped by Scripture. Postconservative evangelicals seek greater dialogue with other Christian traditions and support the development of a multicultural evangelical theology that incorporates the voices of women, racial minorities, and Christians in the developing world. Some postconservative evangelicals also support open theism and the possibility of near universal salvation.

The term "open evangelical" refers to a particular Christian school of thought or churchmanship, primarily in Great Britain (especially in the Church of England). Open evangelicals describe their position as combining a traditional evangelical emphasis on the nature of scriptural authority, the teaching of the ecumenical creeds and other traditional doctrinal teachings, with an approach towards culture and other theological points-of-view which tends to be more inclusive than that taken by other evangelicals. Some open evangelicals aim to take a middle position between conservative and charismatic evangelicals, while others would combine conservative theological emphases with more liberal social positions.

British author Dave Tomlinson coined the phrase "postevangelical" to describe a movement comprising various trends of dissatisfaction among evangelicals. Others use the term with comparable intent, often to distinguish evangelicals in the emerging church movement from postevangelicals and antievangelicals. Tomlinson argues that "linguistically, the distinction "[between evangelical and postevangelical]" resembles the one that sociologists make between the modern and postmodern eras".

Evangelicalism emerged in the 18th century, first in Britain and its North American colonies. Nevertheless, there were earlier developments within the larger Protestant world that preceded and influenced the later evangelical revivals. According to religion scholar Randall Balmer, Evangelicalism resulted "from the confluence of Pietism, Presbyterianism, and the vestiges of Puritanism. Evangelicalism picked up the peculiar characteristics from each strain – warmhearted spirituality from the Pietists (for instance), doctrinal precisionism from the Presbyterians, and individualistic introspection from the Puritans". Historian Mark Noll adds to this list High Church Anglicanism, which contributed to Evangelicalism a legacy of "rigorous spirituality and innovative organization." Historian Rick Kennedy has identified New England Puritan clergyman Cotton Mather as the "first American Evangelical".

During the 17th century, Pietism emerged in Europe as a movement for the revival of piety and devotion within the Lutheran church. As a protest against "cold orthodoxy" or against an overly formal and rational Christianity, Pietists advocated for an experiential religion that stressed high moral standards both for clergy and for lay people. The movement included both Christians who remained in the liturgical, state churches as well as separatist groups who rejected the use of baptismal fonts, altars, pulpits, and confessionals. As Radical Pietism spread, the movement's ideals and aspirations influenced and were absorbed by evangelicals.

When George Fox, who is considered the father of Quakerism, was eleven, he wrote that God spoke to him about "keeping pure and being faithful to God and man." After being troubled when his friends asked him to drink alcohol with them at the age of nineteen, Fox spent the night in prayer and soon afterwards he left his home in a four year search for spiritual satisfaction. In his "Journal", at age 23, he believed that he "found through faith in Jesus Christ the full assurance of salvation." Fox began to spread his message and his emphasis on "the necessity of an inward transformation of heart", as well as the possibility of Christian perfection, drew opposition from English clergy and laity. In the mid-1600s, many people became attracted to Fox's preaching and his followers became known as the Religious Society of Friends. By 1660, the Quakers grew to 35,000 and are considered to be among the first in the evangelical Christian movement.

The Presbyterian heritage not only gave Evangelicalism a commitment to Protestant orthodoxy but also contributed a revival tradition that stretched back to the 1620s in Scotland and Northern Ireland. Central to this tradition was the communion season, which normally occurred in the summer months. For Presbyterians, celebrations of Holy Communion were infrequent but popular events preceded by several Sundays of preparatory preaching and accompanied with preaching, singing, and prayers.

Puritanism combined Calvinism with a doctrine that conversion was a prerequisite for church membership and with an emphasis on the study of Scripture by lay people. It took root in the colonies of New England, where the Congregational church became an established religion. There the Half-Way Covenant of 1662 allowed parents who had not testified to a conversion experience to have their children baptized, while reserving Holy Communion for converted church members alone. By the 18th century Puritanism was in decline and many ministers expressed alarm at the loss of religious piety. This concern over declining religious commitment led many people to support evangelical revival.

High-Church Anglicanism also exerted influence on early Evangelicalism. High Churchmen were distinguished by their desire to adhere to primitive Christianity. This desire included imitating the faith and ascetic practices of early Christians as well as regularly partaking of Holy Communion. High Churchmen were also enthusiastic organizers of voluntary religious societies. Two of the most prominent were the Society for Promoting Christian Knowledge (founded in London in 1698), which distributed Bibles and other literature and built schools, and the Society for the Propagation of the Gospel in Foreign Parts, which was founded in England in 1701 to facilitate missionary work in British colonies (especially among colonists in North America). Samuel and Susanna Wesley, the parents of John and Charles Wesley (born 1703 and 1707 respectively), were both devoted advocates of High-Church ideas.

In the 1730s, Evangelicalism emerged as a distinct phenomenon out of religious revivals that began in Britain and New England. While religious revivals had occurred within Protestant churches in the past, the evangelical revivals that marked the 18th century were more intense and radical. Evangelical revivalism imbued ordinary men and women with a confidence and enthusiasm for sharing the gospel and converting others outside of the control of established churches, a key discontinuity with the Protestantism of the previous era.

It was developments in the doctrine of assurance that differentiated Evangelicalism from what went before. Bebbington says, "The dynamism of the Evangelical movement was possible only because its adherents were assured in their faith." He goes on:

The first local revival occurred in Northampton, Massachusetts, under the leadership of Congregationalist minister Jonathan Edwards. In the fall of 1734, Edwards preached a sermon series on "Justification By Faith Alone", and the community's response was extraordinary. Signs of religious commitment among the laity increased, especially among the town's young people. The revival ultimately spread to 25 communities in western Massachusetts and central Connecticut until it began to wane by the spring of 1735. Edwards was heavily influenced by Pietism, so much so that one historian has stressed his "American Pietism". One practice clearly copied from European Pietists was the use of small groups divided by age and gender, which met in private homes to conserve and promote the fruits of revival.

At the same time, students at Yale University (at that time Yale College) in New Haven, Connecticut, were also experiencing revival. Among them was Aaron Burr, Sr., who would become a prominent Presbyterian minister and future president of Princeton University. In New Jersey, Gilbert Tennent, another Presbyterian minister, was preaching the evangelical message and urging the Presbyterian Church to stress the necessity of converted ministers.

The spring of 1735 also marked important events in England and Wales. Howell Harris, a Welsh schoolteacher, had a conversion experience on May 25 during a communion service. He described receiving assurance of God's grace after a period of fasting, self-examination, and despair over his sins. Sometime later, Daniel Rowland, the Anglican curate of Llangeitho, Wales, experienced conversion as well. Both men began preaching the evangelical message to large audiences, becoming leaders of the Welsh Methodist revival. At about the same time that Harris experienced conversion in Wales, George Whitefield was converted at Oxford University after his own prolonged spiritual crisis. Whitefield later remarked, "About this time God was pleased to enlighten my soul and bring me into the knowledge of His free grace, and the necessity of being justified in His sight by "faith only.""

Whitefield's fellow Holy Club member and spiritual mentor, Charles Wesley, reported an evangelical conversion in 1738. In the same week, Charles' brother and future founder of Methodism, John Wesley was also converted after a long period of inward struggle. During this spiritual crisis, John Wesley was directly influenced by Pietism. Two years before his conversion, Wesley had traveled to the newly established colony of Georgia as a missionary for the Society for Promoting Christian Knowledge. He shared his voyage with a group of Moravian Brethren led by August Gottlieb Spangenberg. The Moravians' faith and piety deeply impressed Wesley, especially their belief that it was a normal part of Christian life to have an assurance of one's salvation. Wesley recounted the following exchange with Spangenberg on February 7, 1736:

Wesley finally received the assurance he had been searching for at a meeting of a religious society in London. While listening to a reading from Martin Luther's preface to the Epistle to the Romans, Wesley felt spiritually transformed:

Pietism continued to influence Wesley, who had translated 33 Pietist hymns from German to English. Numerous German Pietist hymns became part of the English Evangelical repertoire. By 1737, Whitefield had become a national celebrity in England where his preaching drew large crowds, especially in London where the Fetter Lane Society had become a center of evangelical activity. Whitfield joined forces with Edwards to "fan the flame of revival" in the Thirteen Colonies in 1739–40. Soon the First Great Awakening stirred Protestants throughout America.

Evangelical preachers emphasized personal salvation and piety more than ritual and tradition. Pamphlets and printed sermons crisscrossed the Atlantic, encouraging the revivalists. The Awakening resulted from powerful preaching that gave listeners a sense of deep personal revelation of their need of salvation by Jesus Christ. Pulling away from ritual and ceremony, the Great Awakening made Christianity intensely personal to the average person by fostering a deep sense of spiritual conviction and redemption, and by encouraging introspection and a commitment to a new standard of personal morality. It reached people who were already church members. It changed their rituals, their piety and their self-awareness. To the evangelical imperatives of Reformation Protestantism, 18th century American Christians added emphases on divine outpourings of the Holy Spirit and conversions that implanted within new believers an intense love for God. Revivals encapsulated those hallmarks and forwarded the newly created Evangelicalism into the early republic.

By the 1790s, the Evangelical party in the Church of England remained a small minority but were not without influence. John Newton and Joseph Milner were influential evangelical clerics. Evangelical clergy networked together through societies such as the Eclectic Society in London and the Elland Society in Yorkshire. The Old Dissenter denominations (the Baptists, Congregationalists and Quakers) were falling under evangelical influence, with the Baptists most affected and Quakers the least. Evangelical ministers dissatisfied with both Anglicanism and Methodism often chose to work within these churches. In the 1790s, all of these evangelical groups, including the Anglicans, were Calvinist in orientation.

Methodism (the "New Dissent") was the most visible expression of evangelicalism by the end of the 18th century. The Wesleyan Methodists boasted around 70,000 members throughout the British Isles, in addition to the Calvinistic Methodists in Wales and the Countess of Huntingdon's Connexion, which was organized under George Whitefield's influence. The Wesleyan Methodists, however, were still nominally affiliated with the Church of England and would not completely separate until 1795, four years after Wesley's death. The Wesleyan Methodist Church's Arminianism distinguished it from the other evangelical groups.

At the same time, evangelicals were an important faction within the Presbyterian Church of Scotland. Influential ministers included John Erskine, Henry Wellwood Moncrieff and Stevenson Macgill. The church's General Assembly, however, was controlled by the Moderate Party, and evangelicals were involved in the First and Second Secessions from the national church during the 18th century.

The start of the 19th century saw an increase in missionary work and many of the major missionary societies were founded around this time (see Timeline of Christian missions). Both the Evangelical and high church movements sponsored missionaries.

The Second Great Awakening (which actually began in 1790) was primarily an American revivalist movement and resulted in substantial growth of the Methodist and Baptist churches. Charles Grandison Finney was an important preacher of this period.

In Britain in addition to stressing the traditional Wesleyan combination of "Bible, cross, conversion, and activism", the revivalist movement sought a universal appeal, hoping to include rich and poor, urban and rural, and men and women. Special efforts were made to attract children and to generate literature to spread the revivalist message.

"Christian conscience" was used by the British Evangelical movement to promote social activism. Evangelicals believed activism in government and the social sphere was an essential method in reaching the goal of eliminating sin in a world drenched in wickedness. The Evangelicals in the Clapham Sect included figures such as William Wilberforce who successfully campaigned for the abolition of slavery.

In the late 19th century, the revivalist Wesleyan-Holiness movement based on John Wesley's doctrine of "entire sanctification" came to the forefront, and while many adherents remained within mainline Methodism, others established new denominations, such as the Free Methodist Church and Wesleyan Methodist Church. In urban Britain the Holiness message was less exclusive and censorious.

Keswickianism taught the doctrine of the second blessing in non-Methodist circles and came to influence evangelicals of the Calvinistic (Reformed) tradition, leading to the establishment of denominations such as the Christian and Missionary Alliance.

John Nelson Darby of the Plymouth Brethren was a 19th-century Irish Anglican minister who devised modern dispensationalism, an innovative Protestant theological interpretation of the Bible that was incorporated in the development of modern Evangelicalism. Cyrus Scofield further promoted the influence of dispensationalism through the explanatory notes to his Scofield Reference Bible. According to scholar Mark S. Sweetnam, who takes a cultural studies perspective, dispensationalism can be defined in terms of its Evangelicalism, its insistence on the literal interpretation of Scripture, its recognition of stages in God's dealings with humanity, its expectation of the imminent return of Christ to rapture His saints, and its focus on both apocalypticism and premillennialism.

During the 19th century, the megachurches, churches with more than 2,000 people, began to develop. The first evangelical megachurch, the Metropolitan Tabernacle with a 6000-seat auditorium, was inaugurated in 1861 in London by Charles Spurgeon. Dwight L. Moody founded the Illinois Street Church in Chicago.

An advanced theological perspective came from the Princeton theologians from the 1850s to the 1920s, such as Charles Hodge, Archibald Alexander and B.B. Warfield.

After 1910 the Fundamentalist movement dominated Evangelicalism in the early part of the 20th century; the Fundamentalists rejected liberal theology and emphasized the inerrancy of the Scriptures.

Following the 1904–1905 Welsh revival, the Azusa Street Revival in 1906 began the spread of Pentecostalism in North America.

The 20th century also marked by the emergence of the televangelism. Aimee Semple McPherson, who founded the megachurch "Angelus Temple" in Los Angeles, used radio in the 1920s to reach a wider audience.

After the Scopes trial in 1925, "Christian Century" wrote of "Vanishing Fundamentalism". In 1929 Princeton University, once the bastion of conservative theology, added several modernists to its faculty, resulting in the departure of J. Gresham Machen and a split in the Presbyterian Church in the United States of America.

Evangelicalism began to reassert itself in the second half of the 1930s. One factor was the advent of the radio as a means of mass communication. When <nowiki>[</nowiki>Charles E. Fuller<nowiki>]</nowiki> began his "Old Fashioned Revival Hour" on October 3, 1937, he sought to avoid the contentious issues that had caused fundamentalists to be characterized as narrow.

One hundred forty-seven representatives from thirty-four denominations met from April 7 through 9, 1942, in St. Louis, Missouri, for a "National Conference for United Action among Evangelicals." The next year six hundred representatives in Chicago established the National Association of Evangelicals (NAE) with Harold Ockenga as its first president. The NAE was partly a reaction to the founding of the American Council of Christian Churches (ACCC) under the leadership of the fundamentalist Carl McIntire. The ACCC in turn had been founded to counter the influence of the Federal Council of Churches (later merged into the National Council of Churches), which fundamentalists saw as increasingly embracing modernism in its ecumenism. Those who established the NAE had come to view the name fundamentalist as "an embarrassment instead of a badge of honor."

Evangelical revivalist radio preachers organized themselves in the National Religious Broadcasters in 1944 in order to regulate their activity.

With the founding of the NAE, American Protestantism was divided into three large groups—the fundamentalists, the modernists, and the new evangelicals, who sought to position themselves between the other two. In 1947 Harold Ockenga coined the term neo-evangelicalism to identify a movement distinct from fundamentalism. The neo-evangelicals had three broad characteristics that distinguished them from the conservative fundamentalism of the ACCC:

Each of these characteristics took concrete shape by the mid-1950s. In 1947 Carl F. H. Henry's book "The Uneasy Conscience of Fundamentalism" called on evangelicals to engage in addressing social concerns:

In the same year Fuller Theological Seminary was established with Ockenga as its president and Henry as the head of its theology department.

The strongest impetus, however, was the development of the work of Billy Graham. In 1951, with producer Dick Ross, he founded the film production company World Wide Pictures. Graham had begun his career with the support of McIntire and fellow conservatives Bob Jones Sr. and John R. Rice. However, in broadening the reach of his London crusade of 1954, he accepted the support of denominations that those men disapproved of. When he went even further in his 1957 New York crusade, conservatives strongly condemned him and withdrew their support. According to William Martin:

A fourth development—the founding of "Christianity Today" ("CT") with Henry as its first editor—was strategic in giving neo-evangelicals a platform to promote their views and in positioning them between the fundamentalists and modernists. In a letter to Harold Lindsell, Graham said that "CT" would:

The postwar period also saw growth of the ecumenical movement and the founding of the World Council of Churches, which the Evangelical community generally regarded with suspicion.

In the United Kingdom, John Stott (1921–2011) and Martyn Lloyd-Jones (1899–1981) emerged as key leaders in Evangelical Christianity.

The charismatic movement began in the 1960s and resulted in the introduction of Pentecostal theology and practice into many mainline denominations. New charismatic groups such as the Association of Vineyard Churches and Newfrontiers trace their roots to this period (see also British New Church Movement).

The closing years of the 20th century saw controversial postmodern influences entering some parts of Evangelicalism, particularly with the emerging church movement. Also controversial is the relationship between spiritualism and contemporary military metaphors and practices animating many branches of Christianity but especially relevant in the sphere of Evangelicalism. Spiritual warfare is the latest iteration in a long-standing partnership between religious organization and militarization, two spheres that are rarely considered together, although aggressive forms of prayer have long been used to further the aims of expanding Evangelical influence. Major moments of increased political militarization have occurred concurrently with the growth of prominence of militaristic imagery in evangelical communities. This paradigmatic language, paired with an increasing reliance on sociological and academic research to bolster militarized sensibility, serves to illustrate the violent ethos that effectively underscores militarized forms of evangelical prayer.
In Nigeria, evangelical megachurches, such as Redeemed Christian Church of God and Living Faith Church Worldwide, have built autonomous cities with houses, supermarkets, banks, universities, and power plants.

Evangelical Christian film production societies were founded, such as Pure Flix in 2005 and Kendrick Brothers in 2013.

The growth of evangelical churches continues with the construction of new places of worship or enlargements in various regions of the world.

According to a 2011 Pew Forum study on global Christianity, 285,480,000 or 13.1 percent of all Christians are Evangelicals. These figures do not include the Pentecostalism and Charismatic movements. The study states that the category "Evangelicals" should not be considered as a separate category of "Pentecostal and Charismatic" categories, since some believers consider themselves in both movements where their church is affiliated with an Evangelical association.

In 2015, the World Evangelical Alliance is "a network of churches in 129 nations that have each formed an Evangelical alliance and over 100 international organizations joining together to give a world-wide identity, voice, and platform to more than 600 million Evangelical Christians". The Alliance was formed in 1951 by Evangelicals from 21 countries. It has worked to support its members to work together globally.

According to Sébastien Fath of CNRS, in 2016, there are 619 million Evangelicals in the world, one in four Christians. In 2017, about 630 million, an increase of 11 million, including Pentecostals.

Operation World estimates the number of Evangelicals at 545.9 million, which makes for 7.9 percent of the world's population. From 1960 to 2000, the global growth of the number of reported Evangelicals grew three times the world's population rate, and twice that of Islam. According to Operation World, the Evangelical population's current annual growth rate is 2.6 percent, still more than twice the world's population growth rate.

In the 21st century, there are Evangelical churches active in many African countries. They have grown especially since independence came in the 1960s, the strongest movements are based on Pentecostal beliefs. There is a wide range of theology and organizations, including some international movements.

In Nigeria the Evangelical Church Winning All (formerly "Evangelical Church of West Africa") is the largest church organization with five thousand congregations and over ten million members. It sponsors three seminaries and eight Bible colleges, and 1600 missionaries who serve in Nigeria and other countries with the Evangelical Missionary Society (EMS). There have been serious confrontations since 1999 between Muslims and Christians standing in opposition to the expansion of Sharia law in northern Nigeria. The confrontation has radicalized and politicized the Christians. Violence has been escalating.

In Ethiopia, Eritrea, and the Ethiopian and Eritrean diaspora, P'ent'ay (from Ge'ez: ጴንጤ), also known as Ethiopian–Eritrean Evangelicalism, or Wenigēlawī (from Ge'ez: ወንጌላዊ – which directly translates to "Evangelical") are terms used for Evangelical Christians and other Eastern/Oriental-oriented Protestant Christians within Ethiopia and Eritrea, and the Ethiopian and Eritrean diaspora abroad. Prominent movements among them have been Pentecostalism (Ethiopian Full Gospel Believers' Church), the Baptist tradition (Ethiopian Kale Heywet Church), Lutheranism (Ethiopian Evangelical Church Mekane Yesus and Evangelical Lutheran Church of Eritrea), and the Mennonite-Anabaptist tradition (Meserete Kristos Church).

In Kenya, mainstream Evangelical denominations have taken the lead in promoting political activism and backers, with the smaller Evangelical sects of less importance. Daniel arap Moi was president 1978 to 2002 and claimed to be an Evangelical; he proved intolerant of dissent or pluralism or decentralization of power.

The Berlin Missionary Society (BMS) was one of four German Protestant mission societies active in South Africa before 1914. It emerged from the German tradition of Pietism after 1815 and sent its first missionaries to South Africa in 1834. There were few positive reports in the early years, but it was especially active 1859–1914. It was especially strong in the Boer republics. The World War cut off contact with Germany, but the missions continued at a reduced pace. After 1945 the missionaries had to deal with decolonization across Africa and especially with the apartheid government. At all times the BMS emphasized spiritual inwardness, and values such as morality, hard work and self-discipline. It proved unable to speak and act decisively against injustice and racial discrimination and was disbanded in 1972.

Since 1974, young professionals have been the active proselytizers of Evangelicalism in the cities of Malawi.

In Mozambique, Evangelical Protestant Christianity emerged around 1900 from black migrants whose converted previously in South Africa. They were assisted by European missionaries, but, as industrial workers, they paid for their own churches and proselytizing. They prepared southern Mozambique for the spread of Evangelical Protestantism. During its time as a colonial power in Mozambique, the Catholic Portuguese government tried to counter the spread of Evangelical Protestantism.

The East African Revival was a renewal movement within Evangelical churches in East Africa during the late 1920s and 1930s that began at a Church Missionary Society mission station in the Belgian territory of Ruanda-Urundi in 1929, and spread to: Uganda, Tanzania and Kenya during the 1930s and 1940s contributing to the significant growth of the church in East Africa through the 1970s and had a visible influence on Western missionaries who were observer-participants of the movement.

In modern Latin America, the term "Evangelical" is often simply a synonym for "Protestant".

Protestantism in Brazil largely originated with German immigrants and British and American missionaries in the 19th century, following up on efforts that began in the 1820s.

In the late nineteenth century, while the vast majority of Brazilians were nominal Catholics, the nation was underserved by priests, and for large numbers their religion was only nominal. The Catholic Church in Brazil was de-established in 1890, and responded by increasing the number of dioceses and the efficiency of its clergy. Many Protestants came from a large German immigrant community, but they were seldom engaged in proselytism and grew mostly by natural increase.

Methodists were active along with Presbyterians and Baptists. The Scottish missionary Robert Reid Kalley, with support from the Free Church of Scotland, moved to Brazil in 1855, founding the first Evangelical church among the Portuguese-speaking population there in 1856. It was organized according to the Congregational policy as the Igreja Evangélica Fluminense; it became the mother church of Congregationalism in Brazil. The Seventh-day Adventists arrived in 1894, and the YMCA was organized in 1896. The missionaries promoted schools colleges and seminaries, including a liberal arts college in São Paulo, later known as Mackenzie, and an agricultural school in Lavras. The Presbyterian schools in particular later became the nucleus of the governmental system. In 1887 Protestants in Rio de Janeiro formed a hospital. The missionaries largely reached a working-class audience, as the Brazilian upper-class was wedded either to Catholicism or to secularism. By 1914, Protestant churches founded by American missionaries had 47,000 communicants, served by 282 missionaries. In general, these missionaries were more successful than they had been in Mexico, Argentina or elsewhere in Latin America.

There were 700,000 Protestants by 1930, and increasingly they were in charge of their own affairs. In 1930, the Methodist Church of Brazil became independent of the missionary societies and elected its own bishop. Protestants were largely from a working-class, but their religious networks help speed their upward social mobility.

Protestants accounted for fewer than 5 percent of the population until the 1960s but grew exponentially by proselytizing and by 2000 made up over 15 percent of Brazilians affiliated with a church. Pentecostals and charismatic groups account for the vast majority of this expansion.

Pentecostal missionaries arrived early in the 20th century. Pentecostal conversions surged during the 1950s and 1960s, when native Brazilians began founding autonomous churches. The most influential included Brasil Para o Cristo (Brazil for Christ), founded in 1955 by Manoel de Mello. With an emphasis on personal salvation, on God's healing power, and on strict moral codes these groups have developed broad appeal, particularly among the booming urban migrant communities. In Brazil, since the mid-1990s, groups committed to uniting black identity, antiracism, and Evangelical theology have rapidly proliferated. Pentecostalism arrived in Brazil with Swedish and American missionaries in 1911. it grew rapidly but endured numerous schisms and splits. In some areas the Evangelical Assemblies of God churches have taken a leadership role in politics since the 1960s. They claimed major credit for the election of Fernando Collor de Mello as president of Brazil in 1990.

According to the 2000 census, 15.4 percent of the Brazilian population was Protestant. Recent research conducted by the Datafolha institute shows that 25 percent of Brazilians are Protestants, of which 19 percent are followers of Pentecostal denominations. The 2010 census found out that 22.2 percent were Protestant at that date. Protestant denominations saw a rapid growth in their number of followers since the last decades of the 20th century. They are politically and socially conservative, and emphasize that God's favor translates into business success. The rich and the poor remained traditional Catholics, while most Evangelical Protestants were in the new lower-middle class – known as the "C class" (in a A–E classification system).

Chesnut argues that Pentecostalism has become "one of the principal organizations of the poor", for these churches provide the sort of social network that teach members the skills they need to thrive in a rapidly developing meritocratic society.

One large Evangelical church that originated from Brazil is the Universal Church of the Kingdom of God (IURD), a neo‐Pentecostal denomination begun in 1977. It now has a presence in many countries, and claims millions of members worldwide.

Protestants remained a small portion of the population until the late-twentieth century, when various Protestant groups experienced a demographic boom that coincided with the increasing violence of the Guatemalan Civil War. Two former Guatemalan heads of state, General Efraín Ríos Montt and Jorge Serrano Elías have been practicing Evangelical Protestants, as is Guatemala's former President, Jimmy Morales. General Montt, an Evangelical from the Pentecostal tradition, came to power through a coup. He escalated the war against leftist guerrilla insurgents as a holy war against atheistic "forces of evil".

Protestant missionary activity in Asia was most successful in Korea. American Presbyterians and Methodists arrived in the 1880s and were well received. Between 1910 and 1945, when Korea was a Japanese colony, Christianity became in part an expression of nationalism in opposition to Japan's efforts to enforce the Japanese language and the Shinto religion. In 1914, out of 16 million people, there were 86,000 Protestants and 79,000 Catholics; by 1934, the numbers were 168,000 and 147,000. Presbyterian missionaries were especially successful. Since the Korean War (1950–53), many Korean Christians have migrated to the U.S., while those who remained behind have risen sharply in social and economic status. Most Korean Protestant churches in the 21st century emphasize their Evangelical heritage. Korean Evangelicalism is characterized by theological conservatism coupled with an emotional revivalist style. Most churches sponsor revival meetings once or twice a year. Missionary work is a high priority, with 13,000 men and women serving in missions across the world, putting Korea in second place just behind the US.

Sukman argues that since 1945, Protestantism has been widely seen by Koreans as the religion of the middle class, youth, intellectuals, urbanites, and modernists. It has been a powerful force supporting South Korea's pursuit of modernity and emulation of the United States, and opposition to the old Japanese colonialism and to the authoritarianism of North Korea.

South Korea has been referred as an "evangelical superpower" for being the home to some of the largest and most dynamic Christian churches in the world; South Korea is also second to the U.S. in the number of missionaries sent abroad.

According to 2015 South Korean census, 9.7 million or 19.7 percent of the population described themselves as Protestants, many of whom belong to Presbyterian churches shaped by Evangelicalism.

According to the 2010 census, 2.68 percent of Filipinos are Evangelicals. The Philippine Council of Evangelical Churches (PCEC), an organization of more than seventy Evangelical and Mainline Protestant churches, and more than 210 para-church organizations in the Philippines, counts more than 11 million members as of 2011.

In 2019, it was reported that Evangelicalism in France was growing, and a new Evangelical church was built every 10 days and now counts 700,000 followers across France.

John Wesley (1703–1791) was an Anglican cleric and theologian who, with his brother Charles Wesley (1707–1788) and fellow cleric George Whitefield (1714–1770), founded Methodism. After 1791 the movement became independent of the Anglican Church as the "Methodist Connection". It became a force in its own right, especially among the working class.

The "Clapham Sect" was a group of Church of England evangelicals and social reformers based in Clapham, London; they were active 1780s–1840s). John Newton (1725–1807) was the founder. They are described by the historian Stephen Tomkins as "a network of friends and families in England, with William Wilberforce as its center of gravity, who were powerfully bound together by their shared moral and spiritual values, by their religious mission and social activism, by their love for each other, and by marriage".

Evangelicalism was a major force in the Anglican Church from about 1800 to the 1860s. By 1848 when an evangelical John Bird Sumner became Archbishop of Canterbury, between a quarter and a third of all Anglican clergy were linked to the movement, which by then had diversified greatly in its goals and they were no longer considered an organized faction.

In the 21st century there are an estimated 2 million Evangelicals in the UK. According to research performed by the Evangelical Alliance in 2013, 87 percent of UK evangelicals attend Sunday morning church services every week and 63 percent attend weekly or fortnightly small groups. An earlier survey conducted in 2012 found that 92 percent of evangelicals agree it is a Christian's duty to help those in poverty and 45 percent attend a church which has a fund or scheme that helps people in immediate need, and 42 percent go to a church that supports or runs a foodbank. 63 percent believe in tithing, and so give around 10 percent of their income to their church, Christian organizations and various charities 83 percent of UK evangelicals believe that the Bible has supreme authority in guiding their beliefs, views and behavior and 52 percent read or listen to the Bible daily. The Evangelical Alliance, formed in 1846, was the first ecumenical evangelical body in the world and works to unite evangelicals, helping them listen to, and be heard by, the government, media and society.

Since the 70s, the number of Evangelicals and Evangelical congregations has grown strongly in Switzerland. Population censuses suggest that these congregations saw the number of their members triple from 1970 to 2000, qualified as a "spectacular development" by specialists. Sociologists Jörg Stolz and Olivier Favre show that the growth is due to charismatic and Pentecostal groups, while classical evangelical groups are stable and fundamentalist groups are in decline. A quantitative national census on religious congregations reveals the important diversity of evangelicalism in Switzerland.

By the late 19th to early 20th century, most American Protestants were Evangelicals. A bitter divide had arisen between the more liberal-modernist mainline denominations and the fundamentalist denominations, the latter typically consisting of Evangelicals. Key issues included the truth of the Bible—literal or figurative, and teaching of evolution in the schools.

During and after World War II, Evangelicals became increasingly organized. There was a great expansion of Evangelical activity within the United States, "a revival of revivalism". Youth for Christ was formed; it later became the base for Billy Graham's revivals. The National Association of Evangelicals formed in 1942 as a counterpoise to the mainline Federal Council of Churches. In 1942–43, the Old-Fashioned Revival Hour had a record-setting national radio audience. With this organization, though, fundamentalist groups separated from Evangelicals.

According to a Pew Forum on Religion and Public Life study, Evangelicals can be broadly divided into three camps: traditionalist, centrist, and modernist. A 2004 Pew survey identified that while 70.4 percent of Americans call themselves "Christian", Evangelicals only make up 26.3 percent of the population, while Catholics make up 22 percent and mainline Protestants make up 16 percent. Among the Christian population in 2020, mainline Protestants began to outnumber Evangelicals.

Evangelicals have been socially active throughout US history, a tradition dating back to the abolitionist movement of the Antebellum period and the prohibition movement. As a group, evangelicals are most often associated with the Christian right. However, a large number of black self-labeled Evangelicals, and a small proportion of liberal white self-labeled Evangelicals, gravitate towards the Christian left.

Recurrent themes within American Evangelical discourse include abortion, evolution denial, secularism, and the notion of the United States as a Christian nation.

In the 1940s, in the United States, neo-evangelicalism developed the importance of social justice and Christian humanitarian aid actions in Evangelical churches. The majority of evangelical Christian humanitarian organizations were founded in the second half of the 20th century. Among those with the most partner countries, there was the foundation of World Vision International (1950), Samaritan's Purse (1970), Mercy Ships (1978), Prison Fellowship International (1979), International Justice Mission (1997).



Euphonium

The euphonium is a medium-sized, 3 or 4-valve, often compensating, conical-bore, tenor-voiced brass instrument that derives its name from the Ancient Greek word "euphōnos", meaning "well-sounding" or "sweet-voiced" ( "eu" means "well" or "good" and "phōnē" means "sound", hence "of good sound"). The euphonium is a valved instrument. Nearly all current models have piston valves, though some models with rotary valves do exist.

Euphonium music may be notated in the bass clef as a non-transposing instrument or in the treble clef as a transposing instrument in B. In British brass bands, it is typically treated as a treble-clef instrument, while in American band music, parts may be written in either treble clef or bass clef, or both.

A person who plays the euphonium is known as a euphoniumist, a euphonist, a euphophonist or simply a euphonium player.

The euphonium is in the family of brass instruments, more particularly low-brass instruments with many relatives. It is extremely similar to a baritone horn. The difference is that the bore size of the baritone horn is typically smaller than that of the euphonium, and the baritone has a primarily cylindrical bore, whereas the euphonium has a predominantly conical bore. It is controversial whether this is sufficient to make them two different instruments. In the trombone family large and small bore trombones are both called trombones, while the cylindrical trumpet and the conical flugelhorn are given different names. As with the trumpet and flugelhorn, the two instruments are easily doubled by one player, with some modification of breath and embouchure, since the two have identical range and essentially identical fingering. 
The "American baritone", featuring three valves on the front of the instrument and a curved, forward-pointing bell, was dominant in American school bands throughout most of the 20th century, its weight, shape, and configuration conforming to the needs of the marching band. While this instrument is a conical-cylindrical bore hybrid, somewhere between the classic baritone horn and euphonium, it was almost universally labelled a "baritone" by both band directors and composers, thus contributing to the confusion of terminology in the United States.

Several late 19th century music catalogs (such as Pepper and Lyon & Healy) sold a euphonium-like instrument called the "B bass" (to distinguish it from the E and BB bass). In these catalog drawings, the B Bass had thicker tubing than the baritone; both had three valves. Along the same lines, drum and bugle corps introduced the "Bass-baritone", and distinguished it from the baritone. The thicker tubing of the three-valve B bass allowed for production of strong false-tones, providing chromatic access to the pedal register.

Ferdinand Sommer's original name for the instrument was the "euphonion". It is sometimes called the tenor tuba in B, although this can also refer to other varieties of tuba. Names in other languages, as included in scores, can be ambiguous as well. They include French "basse", "saxhorn basse", and "tuba basse"; German "Baryton", "Tenorbass", and "Tenorbasshorn"; Italian "baritono", "bombardino", "eufonio", and "flicorno basso". The most common German name, "Baryton", may have influenced Americans to adopt the name "baritone" for the instrument, due to the influx of German musicians to the United States in the nineteenth century.

As a baritone-voiced brass instrument, the euphonium traces its ancestry to the ophicleide and ultimately back to the serpent. The search for a satisfactory foundational wind instrument that could support massed sound above its pitch took many years. While the serpent was used for over two centuries dating back to the late Renaissance, it was notoriously difficult to control its pitch and tone quality due to its disproportionately small open finger holes. The ophicleide, which was used in bands and orchestras for a few decades in the early to mid-19th century, used a system of keys and was an improvement over the serpent but was still unreliable, especially in the high register.

With the invention of the piston valve system 1818, the construction of brass instruments with an even sound and facility of playing in all registers became possible. The euphonium is said to have been invented, as a "wide-bore, valved bugle of baritone range", by Ferdinand Sommer of Weimar in 1843, though Carl Moritz in 1838 and Adolphe Sax in 1843 have also been credited. While Sax's family of saxhorns were invented at about the same time and the bass saxhorn is very similar to a euphonium, there are also differences—such as the bass saxhorn being narrower throughout the length of the instrument.

The "British-style" compensating euphonium was developed in 1874 by David Blaikley, of Boosey & Co, and has been in use in Britain since then, with the basic construction little changed.

Modern-day euphonium makers have been working to further enhance the construction of the instrument. Companies such as Adams and Besson have been leading the way in that respect. Adams euphoniums have developed an adjustable lead-pipe receiver, which allows players to change the timbre of the instrument to whatever they find preferable. Besson has been credited with introducing an adjustable main tuning-slide trigger, which allows players more flexibility with intonation.

The euphonium, like the tenor trombone, is pitched in concert B. For a valved brass instrument like the euphonium, this means that when no valves are in use the instrument will produce partials of the B harmonic series. It is generally orchestrated as a non-transposing instrument like the trombone, written at concert pitch in the bass clef with higher passages in the tenor clef. Treble clef euphonium parts transposing down a major ninth are included in much concert band music: in the British-style brass band tradition, euphonium music is always written this way. In continental European band music, parts for the euphonium may be written in the bass clef as a B transposing instrument sounding a major second lower than written.

Professional models have three top-action valves, played with the first three fingers of the right hand, plus a fourth valve, generally found midway down the right side of the instrument, played with the left index finger; such an instrument is shown at the top of this page. Such models also have compensating "knuckles" to resolve intonation issues below E. Beginner models often have only the three top-action valves, while some intermediate "student" models may have a fourth top-action valve, played with the fourth finger of the right hand. Compensating systems are expensive to build, and there is in general a substantial difference in price between compensating and non-compensating models. For a thorough discussion of the valves and the compensation system, see the article on brass instruments.

The euphonium has an extensive range, from E to about F for intermediate players (using scientific pitch notation). In professional hands this may extend from B to as high as B. The lowest notes obtainable depend on the valve set-up of the instrument. All instruments are chromatic down to E, but four-valved instruments extend that down to at least C. Non-compensating four-valved instruments suffer from intonation problems from E down to C and cannot produce the low B; compensating instruments do not have such intonation problems and can play the low B. From B down lies the "pedal range", i.e., the fundamentals of the instrument's harmonic series. They are easily produced on the euphonium as compared to other brass instruments, and the extent of the range depends on the make of the instrument in exactly the same way as just described. Thus, on a compensating four-valved instrument, the lowest note possible is B, sometimes called double pedal B, which is six ledger lines below the bass clef.

As with the other conical-bore instruments, the cornet, flugelhorn, horn, and tuba, the euphonium's tubing (excepting the tubing in the valve section, which is necessarily cylindrical) gradually increases in diameter throughout its length, resulting in a softer, gentler tone compared to cylindrical-bore instruments such as the trumpet, trombone, sudrophone, and baritone horn. While a truly characteristic euphonium sound is rather hard to define precisely, most players would agree that an ideal sound is dark, rich, warm, and velvety, with virtually no hardness to it. This also has to do with the different models preferred by British and American players.

Though the euphonium's fingerings are no different from those of the trumpet or tuba, beginning euphoniumists will likely experience significant problems with intonation, response and range compared to other beginning brass players.

The compensating euphonium is common among professionals. It utilizes a three-plus-one-valve system with three upright valves and one side valve. The compensating valve system uses extra tubing, usually coming off the back of the three upright valves, in order to achieve proper intonation in the lower range of the instrument. This range being from E down to B. Not all four-valve and three-plus-one-valve euphoniums are compensating. Only those designed with extra tubing are compensating. There were, at one time, three-valve compensating euphoniums available. This configuration utilized extra tubing, just as the three-plus-one compensating models did, in order to bring the notes C and B in tune. This three-valve compensating configuration is still available in British style baritone horns, usually on professional models.

A creation unique to the United States was the double-bell euphonium, featuring a second smaller bell in addition to the main one; the player could switch bells for certain passages or even for individual notes by use of an additional valve, operated with the left hand. Ostensibly, the smaller bell was intended to emulate the sound of a trombone (it was cylindrical-bore) and was possibly intended for performance situations in which trombones were not available. The extent to which the difference in sound and timbre was apparent to the listener, however, is up for debate. Michele Raffayolo of the Patrick S. Gilmore band introduced the instrument in the U.S. by 1880, and it was used widely in both school and service bands for several decades. "Harold Brasch" (see "List of important players" below) brought the British-style compensating euphonium to the United States c. 1939, but the double-belled euphonium may have remained in common use even into the 1950s and 1960s. In any case, they have become rare (they were last in Conn's advertisements in the 1940s, and King's catalog in the 1960s), and are generally unknown to younger players. They are chiefly known now through their mention in the song "Seventy-Six Trombones" from the musical "The Music Man" by Meredith Willson.

Marching euphoniums are used by marching bands and in drum and bugle corps. Typically in a drum corps, there will be two baritone parts and one euphonium part, with the euphonium playing the lower parts comparatively. Some corps (such as the Blue Devils) march all-euphonium sections rather than only marching baritone or a mix of both. In high school marching bands, the two will often be used interchangeably.

Depending on the manufacturer, the weight of these instruments can be straining to the average marcher and require great strength to hold during practices and performances, leading to nerve problems in the right pinky, a callus on the left hand, and possibly back and arm problems. Marching euphoniums and marching baritones commonly have 3 valves, opposed to the regular euphonium having 4.

Another form of the marching euphonium is the convertible euphonium. Recently widely produced, the horn resembles a convertible tuba, being able to change from a concert upright to a marching forward bell on either the left or right shoulder. These are mainly produced by Jupiter or Yamaha, but other less expensive versions can be found.

The five-valve euphonium (non-compensating) is an extremely rare variation of the euphonium manufactured in the late 19th and early 20th centuries by Britain's Besson musical instrument company and Highams of Manchester Musical Instrument Company. 
Higham and Besson's "Clearbore" five-valve euphonium was economical but not widely used.

The Besson five-valve euphonium featured the standard three piston valves horizontally not on top, but had an additional two piston valves off to the side. The standard euphonium has eight possible fingering and non-fingering positions by which sound is produced. The Besson and the Highams "clearbore" model rare fourth and fifth extra "side" valves change the possible fingering and non-fingering positions from eight to thirty-two.

The term 'five-valve euphonium' does not refer to variations of the double bell euphonium made by various brass instrument companies during the same time period. Some of the double-bell euphoniums had five valves, with the fifth valve either not on top with the other four, or by itself off to the side, but the double-bell fifth valve was used for switching the sound to the second smaller trombone-sized bell, and not for changing the fingering pitch of the instrument. Also, Cerveny Musical Instruments manufactures several euphoniums with five vertical rotary valves today, but this is an unrelated recent development.

German Ferdinand Sommer, if one discounts the claims of Moritz and Sax each of whose horns also approached a euphonium in nature, in addition to being credited with inventing the euphonium as the Sommerhorn in 1843, as a soloist on the horn, qualifies as the first euphonium player to significantly advance and alter the understanding of the instrument.





The "euphonium repertoire" consists of solo literature and orchestral, or, more commonly, concert band parts written for the euphonium. Since its invention in 1843, the euphonium has always had an important role in ensembles, but solo literature was slow to appear, consisting of only a handful of lighter solos until the 1960s. Since then, however, the breadth and depth of the solo euphonium repertoire has increased dramatically.

In the current age, there has been a huge number of new commissions and repertoire development and promotion through Steven Mead's World of the Euphonium Series and the Beyond the Horizon series from Euphonium.com. There has also been a vast number of new commissions by more and more players and a proliferation of large scale Consortium Commissions that are occurring including current ones in 2008 and 2009 organized by Brian Meixner (Libby Larson), Adam Frey (The Euphonium Foundation Consortium), and Jason Ham (David Gillingham).

Upon its invention, it was clear that the euphonium had, compared to its predecessors the serpent and ophicleide, a wide range and had a consistently rich, pleasing sound throughout that range. It was flexible both in tone quality and intonation and could blend well with a variety of ensembles, gaining it immediate popularity with composers and conductors as the principal tenor-voices solo instrument in brass band settings, especially in Britain. It is no surprise, then, that when British composers – some of the same ones who were writing for brass bands – began to write serious, original music for the concert band in the early 20th century, they used the euphonium in a very similar role.

When American composers also began writing for the concert band as its own artistic medium in the 1930s and 1940s, they continued the British brass and concert band tradition of using the euphonium as the principal tenor-voiced solo.
This is not to say that composers, then and now, valued the euphonium only for its lyrical capabilities. Indeed, examination of a large body of concert band literature reveals that the euphonium functions as a "jack of all trades."

Though the euphonium was, as previously noted, embraced from its earliest days by composers and arrangers in band settings, orchestral composers have, by and large, not taken advantage of this capability. There are, nevertheless, several orchestral works, a few of which are standard repertoire, in which composers have called for instruments, such as the Wagner tuba, for which euphonium is commonly substituted in the present.

In contrast to the long-standing practice of extensive euphonium use in wind bands and orchestras, there was, until approximately forty years ago, literally no body of solo literature written specifically for the euphonium, and euphonium players were forced to borrow the literature of other instruments. Fortunately, given the instrument's multifaceted capabilities discussed above, solos for many different instruments are easily adaptable to performance on the euphonium.

The earliest surviving solo composition written specifically for euphonium or one of its saxhorn cousins is the "Concerto per Flicorno Basso" (1872) by Amilcare Ponchielli. For almost a century after this, the euphonium solo repertoire consisted of only a dozen or so virtuosic pieces, mostly light in character. However, in the 1960s and 1970s, American composers began to write the first of the "new school" of serious, artistic solo works specifically for euphonium. Since then, there has been a virtual explosion of solo repertoire for the euphonium. In a mere four decades, the solo literature has expanded from virtually zero to thousands of pieces. More and more composers have become aware of the tremendous soloistic capabilities of the euphonium, and have constantly "pushed the envelope" with new literature in terms of tessitura, endurance, technical demands, and extended techniques.

Finally, the euphonium has, thanks to a handful of enterprising individuals, begun to make inroads in jazz, pop and other non-concert performance settings. One well-known euphonium player from the world of popular music is Don McGlashan, the New Zealand musician who began his musical career as an orchestral brass player before finding success in popular music with bands such as Blam Blam Blam and The Mutton Birds. 




Entire function

In complex analysis, an entire function, also called an integral function, is a complex-valued function that is holomorphic on the whole complex plane. Typical examples of entire functions are polynomials and the exponential function, and any finite sums, products and compositions of these, such as the trigonometric functions sine and cosine and their hyperbolic counterparts sinh and cosh, as well as derivatives and integrals of entire functions such as the error function. If an entire function formula_1 has a 
root at formula_2, then formula_3, taking the limit value at formula_2, is an entire function. On the other hand, the natural logarithm, the reciprocal function, and the square root are all not entire functions, nor can they be continued analytically to an entire function.

A transcendental entire function is an entire function that is not a polynomial.

Just as meromorphic functions can be viewed as a generalization of rational fractions, entire functions can be viewed as a generalization of polynomials. In particular, if for meromorphic functions one can generalize the factorization into simple fractions (the Mittag-Leffler theorem on the decomposition of a meromorphic function), then for entire functions there is a generalization of the factorization — the Weierstrass theorem on entire functions.

Every entire function formula_5 can be represented as a single power series
formula_6
that converges everywhere in the complex plane, hence uniformly on compact sets. The radius of convergence is infinite, which implies that
formula_7
or, equivalently,
formula_8
Any power series satisfying this criterion will represent an entire function.

If (and only if) the coefficients of the power series are all real then the function evidently takes real values for real arguments, and the value of the function at the complex conjugate of formula_9 will be the complex conjugate of the value at formula_10 Such functions are sometimes called self-conjugate (the conjugate function, formula_11 being given by 

If the real part of an entire function is known in a neighborhood of a point then both the real and imaginary parts are known for the whole complex plane, up to an imaginary constant. For instance, if the real part is known in a neighborhood of zero, then we can find the coefficients for formula_12 from the following derivatives with respect to a real variable formula_13:

formula_14

Note however that an entire function is not determined by its real part on all curves. In particular, if the real part is given on any curve in the complex plane where the real part of some other entire function is zero, then any multiple of that function can be added to the function we are trying to determine. For example, if the curve where the real part is known is the real line, then we can add formula_15 times any self-conjugate function. If the curve forms a loop, then it is determined by the real part of the function on the loop since the only functions whose real part is zero on the curve are those that are everywhere equal to some imaginary number.

The Weierstrass factorization theorem asserts that any entire function can be represented by a product involving its zeroes (or "roots").

The entire functions on the complex plane form an integral domain (in fact a Prüfer domain). They also form a commutative unital associative algebra over the complex numbers.

Liouville's theorem states that any bounded entire function must be constant.

As a consequence of Liouville's theorem, any function that is entire on the whole Riemann sphere
is constant. Thus any non-constant entire function must have a singularity at the complex point at infinity, either a pole for a polynomial or an essential singularity for a transcendental entire function. Specifically, by the Casorati–Weierstrass theorem, for any transcendental entire function formula_16 and any complex formula_17 there is a sequence formula_18 such that 

Picard's little theorem is a much stronger result: Any non-constant entire function takes on every complex number as value, possibly with a single exception. When an exception exists, it is called a lacunary value of the function. The possibility of a lacunary value is illustrated by the exponential function, which never takes on the value One can take a suitable branch of the logarithm of an entire function that never hits so that this will also be an entire function (according to the Weierstrass factorization theorem). The logarithm hits every complex number except possibly one number, which implies that the first function will hit any value other than 0 an infinite number of times. Similarly, a non-constant, entire function that does not hit a particular value will hit every other value an infinite number of times.

Liouville's theorem is a special case of the following statement:

Entire functions may grow as fast as any increasing function: for any increasing function 
formula_20 there exists an entire function formula_21 such that 
formula_22 for all real formula_23. Such a function formula_21 may be easily found of the form:

formula_25

for a constant formula_26 and a strictly increasing sequence of positive integers formula_27. Any such sequence defines an entire function formula_1, and if the powers are chosen appropriately we may satisfy the inequality formula_22 for all real formula_23. (For instance, it certainly holds if one chooses formula_31 and, for any integer formula_32 one chooses an even exponent formula_33 such that formula_34).

The order (at infinity) of an entire function formula_1 is defined using the limit superior as:

formula_36

where formula_37 is the disk of radius formula_38 and formula_39 denotes the supremum norm of formula_1 on formula_37. The order is a non-negative real number or infinity (except when formula_42 for all formula_43. In other words, the order of formula_1 is the infimum of all formula_45 such that: 

formula_46

The example of formula_47 shows that this does not mean formula_48 if 
formula_1 is of order formula_45.

If formula_51 one can also define the type:

formula_52

If the order is 1 and the type is formula_53, the function is said to be "of exponential type formula_53". If it is of order less than 1 it is said to be of exponential type 0.

If formula_55 then the order and type can be found by the formulas
formula_56

Let formula_57 denote the formula_58-th derivative of formula_21, then we may restate these formulas in terms of the derivatives at any arbitrary point formula_60:

formula_61

The type may be infinite, as in the case of the reciprocal gamma function, or zero (see example below under ).

Another way to find out the order and type is Matsaev's theorem.

Here are some examples of functions of various orders:

For arbitrary positive numbers formula_62 and formula_53 one can construct an example of an entire function of order formula_62 and type formula_53 using:

formula_66


formula_68 
where formula_69

formula_70
where
formula_71

formula_72 with formula_73 (for which the type is given by formula_74)





Entire functions of finite order have Hadamard's canonical representation (Hadamard factorization theorem):

formula_90

where formula_91 are those roots of formula_21 that are not zero (formula_93), formula_45 is the order of the zero of formula_21 at formula_96 (the case formula_97 being taken to mean formula_98), formula_99 a polynomial (whose degree we shall call formula_100), and formula_101 is the smallest non-negative integer such that the series

formula_102

converges. The non-negative integer formula_103 is called the genus of the entire function formula_21.

If the order formula_62 is not an integer, then formula_106 is the integer part of formula_62. If the order is a positive integer, then there are two possibilities: formula_108 or formula_109.

For example, formula_110, formula_111 and formula_112 are entire functions of genus formula_113.

According to J. E. Littlewood, the Weierstrass sigma function is a 'typical' entire function. This statement can be made precise in the theory of random entire functions: the asymptotic behavior of almost all entire functions is similar to that of the sigma function. Other examples include the Fresnel integrals, the Jacobi theta function, and the reciprocal Gamma function. The exponential function and the error function are special cases of the Mittag-Leffler function. According to the fundamental theorem of Paley and Wiener, Fourier transforms of functions (or distributions) with bounded support are entire functions of order formula_114 and finite type.

Other examples are solutions of linear differential equations with polynomial coefficients. If the coefficient at the highest derivative is constant, then all solutions of such equations are entire functions. For example, the exponential function, sine, cosine, Airy functions and Parabolic cylinder functions arise in this way. The class of entire functions is closed with respect to compositions. This makes it possible to study dynamics of entire functions.

An entire function of the square root of a complex number is entire if the original function is even, for example formula_115.

If a sequence of polynomials all of whose roots are real converges in a neighborhood of the origin to a limit which is not identically equal to zero, then this limit is an entire function. Such entire functions form the Laguerre–Pólya class, which can also be characterized in terms of the Hadamard product, namely, formula_21 belongs to this class if and only if in the Hadamard representation all formula_117 are real, formula_118, and 
formula_119, where formula_120 and formula_26 are real, and formula_122. For example, the sequence of polynomials 

formula_123 

converges, as formula_58 increases, to formula_125. The polynomials 

formula_126 

have all real roots, and converge to formula_127. The polynomials 

formula_128 

also converge to formula_127, showing the buildup of the Hadamard product for cosine.



Essay

An essay is, generally, a piece of writing that gives the author's own argument, but the definition is vague, overlapping with those of a letter, a paper, an article, a pamphlet, and a short story. Essays have been sub-classified as formal and informal: formal essays are characterized by "serious purpose, dignity, logical organization, length," whereas the informal essay is characterized by "the personal element (self-revelation, individual tastes and experiences, confidential manner), humor, graceful style, rambling structure, unconventionality or novelty of theme," etc.

Essays are commonly used as literary criticism, political manifestos, learned arguments, observations of daily life, recollections, and reflections of the author. Almost all modern essays are written in prose, but works in verse have been dubbed essays (e.g., Alexander Pope's "An Essay on Criticism" and "An Essay on Man"). While brevity usually defines an essay, voluminous works like John Locke's "An Essay Concerning Human Understanding" and Thomas Malthus's "An Essay on the Principle of Population" are counterexamples.

In some countries (e.g., the United States and Canada), essays have become a major part of formal education. Secondary students are taught structured essay formats to improve their writing skills; admission essays are often used by universities in selecting applicants, and in the humanities and social sciences essays are often used as a way of assessing the performance of students during final exams.

The concept of an "essay" has been extended to other media beyond writing. A film essay is a movie that often incorporates documentary filmmaking styles and focuses more on the evolution of a theme or idea. A photographic essay covers a topic with a linked series of photographs that may have accompanying text or captions.

The word "essay" derives from the French infinitive , "to try" or "to attempt". In English "essay" first meant "a trial" or "an attempt", and this is still an alternative meaning. The Frenchman Michel de Montaigne (1533–1592) was the first author to describe his work as essays; he used the term to characterize these as "attempts" to put his thoughts into writing.

Subsequently, "essay" has been defined in a variety of ways. One definition is a "prose composition with a focused subject of discussion" or a "long, systematic discourse".
It is difficult to define the genre into which essays fall. Aldous Huxley, a leading essayist, gives guidance on the subject. He notes that "the essay is a literary device for saying almost everything about almost anything", and adds that "by tradition, almost by definition, the essay is a short piece". Furthermore, Huxley argues that "essays belong to a literary species whose extreme variability can be studied most effectively within a three-poled frame of reference".
These three poles (or worlds in which the essay may exist) are:
Huxley adds that the most satisfying essays "...make the best not of one, not of two, but of all the three worlds in which it is possible for the essay to exist."

Montaigne's "attempts" grew out of his commonplacing. Inspired in particular by the works of Plutarch, a translation of whose "Œuvres Morales" ("Moral works") into French had just been published by Jacques Amyot, Montaigne began to compose his essays in 1572; the first edition, entitled "Essais", was published in two volumes in 1580. For the rest of his life, he continued revising previously published essays and composing new ones. A third volume was published posthumously; together, their over 100 examples are widely regarded as the predecessor of the modern essay.

While Montaigne's philosophy was admired and copied in France, none of his most immediate disciples tried to write essays. But Montaigne, who liked to fancy that his family (the Eyquem line) was of English extraction, had spoken of the English people as his "cousins", and he was early read in England, notably by Francis Bacon.

Bacon's essays, published in book form in 1597 (only five years after the death of Montaigne, containing the first ten of his essays), 1612, and 1625, were the first works in English that described themselves as "essays". Ben Jonson first used the word "essayist" in 1609, according to the "Oxford English Dictionary".
Other English essayists included Sir William Cornwallis, who published essays in 1600 and 1617 that were popular at the time, Robert Burton (1577–1641) and Sir Thomas Browne (1605–1682). In Italy, Baldassare Castiglione wrote about courtly manners in his essay "Il Cortigiano". In the 17th century, the Spanish Jesuit Baltasar Gracián wrote about the theme of wisdom.

In England, during the Age of Enlightenment, essays were a favored tool of polemicists who aimed at convincing readers of their position; they also featured heavily in the rise of periodical literature, as seen in the works of Joseph Addison, Richard Steele and Samuel Johnson. Addison and Steele used the journal "Tatler" (founded in 1709 by Steele) and its successors as storehouses of their work, and they became the most celebrated eighteenth-century essayists in England. Johnson's essays appear during the 1750s in various similar publications. As a result of the focus on journals, the term also acquired a meaning synonymous with "article", although the content may not the strict definition. On the other hand, Locke's "An Essay Concerning Human Understanding" is not an essay at all, or cluster of essays, in the technical sense, but still it refers to the experimental and tentative nature of the inquiry which the philosopher was undertaking.

In the 18th and 19th centuries, Edmund Burke and Samuel Taylor Coleridge wrote essays for the general public. The early 19th century, in particular, saw a proliferation of great essayists in English—William Hazlitt, Charles Lamb, Leigh Hunt and Thomas De Quincey all penned numerous essays on diverse subjects, reviving the earlier graceful style. Thomas Carlyle's essays were highly influential, and one of his readers, Ralph Waldo Emerson, became a prominent essayist himself. Later in the century, Robert Louis Stevenson also raised the form's literary level. In the 20th century, a number of essayists, such as T.S. Eliot, tried to explain the new movements in art and culture by using essays. Virginia Woolf, Edmund Wilson, and Charles du Bos wrote literary criticism essays.

In France, several writers produced longer works with the title of that were not true examples of the form. However, by the mid-19th century, the "Causeries du lundi", newspaper columns by the critic Sainte-Beuve, are literary essays in the original sense. Other French writers followed suit, including Théophile Gautier, Anatole France, Jules Lemaître and Émile Faguet.

As with the novel, essays existed in Japan several centuries before they developed in Europe with a genre of essays known as "zuihitsu"—loosely connected essays and fragmented ideas. Zuihitsu have existed since almost the beginnings of Japanese literature. Many of the most noted early works of Japanese literature are in this genre. Notable examples include "The Pillow Book" (c. 1000), by court lady Sei Shōnagon, and "Tsurezuregusa" (1330), by particularly renowned Japanese Buddhist monk Yoshida Kenkō. Kenkō described his short writings similarly to Montaigne, referring to them as "nonsensical thoughts" written in "idle hours". Another noteworthy difference from Europe is that women have traditionally written in Japan, though the more formal, Chinese-influenced writings of male writers were more prized at the time.

The eight-legged essay (Chinese: 八股文; pinyin: "bāgǔwén"; 'eight bone text') was a style of essay in imperial examinations during the Ming and Qing dynasties in China. The eight-legged essay was needed for those test takers in these civil service tests to show their merits for government service, often focusing on Confucian thought and knowledge of the Four Books and Five Classics, in relation to governmental ideals. Test takers could not write in innovative or creative ways, but needed to conform to the standards of the eight-legged essay. Various skills were examined, including the ability to write coherently and to display basic logic. In certain times, the candidates were expected to spontaneously compose poetry upon a set theme, whose value was also sometimes questioned, or eliminated as part of the test material. This was a major argument in favor of the eight-legged essay, arguing that it were better to eliminate creative art in favor of prosaic literacy. In the history of Chinese literature, the eight-legged essay is often said to have caused China's "cultural stagnation and economic backwardness" in the 19th century.

This section describes the different forms and styles of essay writing. These are used by an array of authors, including university students and professional essayists.

The defining features of a "cause and effect" essay are causal chains that connect from a cause to an effect, careful language, and chronological or emphatic order. A writer using this rhetorical method must consider the subject, determine the purpose, consider the audience, think critically about different causes or consequences, consider a thesis statement, arrange the parts, consider the language, and decide on a conclusion.

Classification is the categorization of objects into a larger whole while division is the breaking of a larger whole into smaller parts.

Compare and contrast essays are characterized by a basis for comparison, points of comparison, and analogies. It is grouped by the object (chunking) or by point (sequential). The comparison highlights the similarities between two or more similar objects while contrasting highlights the differences between two or more objects. When writing a compare/contrast essay, writers need to determine their purpose, consider their audience, consider the basis and points of comparison, consider their thesis statement, arrange and develop the comparison, and reach a conclusion. Compare and contrast is arranged emphatically.

An expository essay is used to inform, describe or explain a topic, using important facts to teach the reader about a topic. Mostly written in third-person, using "it", "he", "she", "they," the expository essay uses formal language to discuss someone or something. Examples of expository essays are: a medical or biological condition, social or technological process, life or character of a famous person. The writing of an expository essay often consists of the following steps: organizing thoughts (brainstorming), researching a topic, developing a thesis statement, writing the introduction, writing the body of essay, and writing the conclusion. Expository essays are often assigned as a part of SAT and other standardized testing or as homework for high school and college students.

Descriptive writing is characterized by sensory details, which appeal to the physical senses, and details that appeal to a reader's emotional, physical, or intellectual sensibilities. Determining the purpose, considering the audience, creating a dominant impression, using descriptive language, and organizing the description are the rhetorical choices to consider when using a description. A description is usually arranged spatially but can also be chronological or emphatic. The focus of a description is the scene. Description uses tools such as denotative language, connotative language, figurative language, metaphor, and simile to arrive at a dominant impression. One university essay guide states that "descriptive writing says what happened or what another author has discussed; it provides an account of the topic".
Lyric essays are an important form of descriptive essays.

In the dialectic form of the essay, which is commonly used in philosophy, the writer makes a thesis and argument, then objects to their own argument (with a counterargument), but then counters the counterargument with a final and novel argument. This form benefits from presenting a broader perspective while countering a possible flaw that some may present. This type is sometimes called an ethics paper.

An exemplification essay is characterized by a generalization and relevant, representative, and believable examples including anecdotes. Writers need to consider their subject, determine their purpose, consider their audience, decide on specific examples, and arrange all the parts together when writing an exemplification essay.
An essayist writes a "familiar essay" if speaking to a single reader, writing about both themselves, and about particular subjects. Anne Fadiman notes that "the genre's heyday was the early nineteenth century," and that its greatest exponent was Charles Lamb. She also suggests that while critical essays have more brain than the heart, and personal essays have more heart than brain, familiar essays have equal measures of both.

A history essay sometimes referred to as a thesis essay describes an argument or claim about one or more historical events and supports that claim with evidence, arguments, and references. The text makes it clear to the reader why the argument or claim is as such.

A narrative uses tools such as flashbacks, flash-forwards, and transitions that often build to a climax. The focus of a narrative is the plot. When creating a narrative, authors must determine their purpose, consider their audience, establish their point of view, use dialogue, and organize the narrative. A narrative is usually arranged chronologically.

An argumentative essay is a critical piece of writing, aimed at presenting objective analysis of the subject matter, narrowed down to a single topic. The main idea of all the criticism is to provide an opinion either of positive or negative implication. As such, a critical essay requires research and analysis, strong internal logic and sharp structure. Its structure normally builds around introduction with a topic's relevance and a thesis statement, body paragraphs with arguments linking back to the main thesis, and conclusion. In addition, an argumentative essay may include a refutation section where conflicting ideas are acknowledged, described, and criticized. Each argument of an argumentative essay should be supported with sufficient evidence, relevant to the point.

A process essay is used for an explanation of making or breaking something. Often, it is written in chronological order or numerical order to show step-by-step processes. It has all the qualities of a technical document with the only difference is that it is often written in descriptive mood, while a technical document is mostly in imperative mood.

An economic essay can start with a thesis, or it can start with a theme. It can take a narrative course and a descriptive course. It can even become an argumentative essay if the author feels the need. After the introduction, the author has to do his/her best to expose the economic matter at hand, to analyze it, evaluate it, and draw a conclusion. If the essay takes more of a narrative form then the author has to expose each aspect of the economic puzzle in a way that makes it clear and understandable for the reader

A "reflective essay" is an analytical piece of writing in which the writer describes a real or imaginary scene, event, interaction, passing thought, memory, or form—adding a personal reflection on the meaning of the topic in the author's life. Thus, the focus is not merely descriptive. The writer doesn't just describe the situation, but revisits the scene with more detail and emotion to examine what went well, or reveal a need for additional learning—and may relate what transpired to the rest of the author's life.

The logical progression and organizational structure of an essay can take many forms. Understanding how the movement of thought is managed through an essay has a profound impact on its overall cogency and ability to impress. A number of alternative logical structures for essays have been visualized as diagrams, making them easy to implement or adapt in the construction of an argument.

In countries like the United States and the United Kingdom, essays have become a major part of a formal education in the form of free response questions. Secondary students in these countries are taught structured essay formats to improve their writing skills, and essays are often used by universities in these countries in selecting applicants ("see" admissions essay). In both secondary and tertiary education, essays are used to judge the mastery and comprehension of the material. Students are asked to explain, comment on, or assess a topic of study in the form of an essay. In some courses, university students must complete one or more essays over several weeks or months. In addition, in fields such as the humanities and social sciences, mid-term and end of term examinations often require students to write a short essay in two or three hours.

In these countries, so-called academic essays, also called "papers", are usually more formal than literary ones. They may still allow the presentation of the writer's own views, but this is done in a logical and factual manner, with the use of the first person often discouraged. Longer academic essays (often with a word limit of between 2,000 and 5,000 words) are often more discursive. They sometimes begin with a short summary analysis of what has previously been written on a topic, which is often called a literature review.

Longer essays may also contain an introductory page that defines words and phrases of the essay's topic. Most academic institutions require that all substantial facts, quotations, and other supporting material in an essay be referenced in a bibliography or works cited page at the end of the text. This scholarly convention helps others (whether teachers or fellow scholars) to understand the basis of facts and quotations the author uses to support the essay's argument. The bibliography also helps readers evaluate to what extent the argument is supported by evidence and to evaluate the quality of that evidence. The academic essay tests the student's ability to present their thoughts in an organized way and is designed to test their intellectual capabilities.

One of the challenges facing universities is that in some cases, students may submit essays purchased from an essay mill (or "paper mill") as their own work. An "essay mill" is a ghostwriting service that sells pre-written essays to university and college students. Since plagiarism is a form of academic dishonesty or academic fraud, universities and colleges may investigate papers they suspect are from an essay mill by using plagiarism detection software, which compares essays against a database of known mill essays and by orally testing students on the contents of their papers.

Essays often appear in magazines, especially magazines with an intellectual bent, such as "The Atlantic" and "Harpers". Magazine and newspaper essays use many of the essay types described in the section on forms and styles (e.g., descriptive essays, narrative essays, etc.). Some newspapers also print essays in the op-ed section.
Employment essays detailing experience in a certain occupational field are required when applying for some jobs, especially government jobs in the United States. Essays known as Knowledge Skills and Executive Core Qualifications are required when applying to certain US federal government positions.

A KSA, or "Knowledge, Skills, and Abilities", is a series of narrative statements that are required when applying to Federal government job openings in the United States. KSAs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The knowledge, skills, and abilities necessary for the successful performance of a position are contained on each job vacancy announcement. KSAs are brief and focused essays about one's career and educational background that presumably qualify one to perform the duties of the position being applied for.

An Executive Core Qualification, or ECQ, is a narrative statement that is required when applying to Senior Executive Service positions within the US Federal government. Like the KSAs, ECQs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The Office of Personnel Management has established five executive core qualifications that all applicants seeking to enter the Senior Executive Service must demonstrate.

A film essay (also essay film or cinematic essay) consists of the evolution of a theme or an idea rather than a plot per se, or the film literally being a cinematic accompaniment to a narrator reading an essay. From another perspective, an essay film could be defined as a documentary film visual basis combined with a form of commentary that contains elements of self-portrait (rather than autobiography), where the signature (rather than the life story) of the filmmaker is apparent. The cinematic essay often blends documentary, fiction, and experimental film making using tones and editing styles.

The genre is not well-defined but might include propaganda works of early Soviet filmmakers like Dziga Vertov, present-day filmmakers including Chris Marker, Michael Moore ("Roger & Me", "Bowling for Columbine" and "Fahrenheit 9/11"), Errol Morris ("The Thin Blue Line"), Morgan Spurlock ("Supersize Me") and Agnès Varda. Jean-Luc Godard describes his recent work as "film-essays". Two filmmakers whose work was the antecedent to the cinematic essay include Georges Méliès and Bertolt Brecht. Méliès made a short film ("The Coronation of Edward VII" (1902)) about the 1902 coronation of King Edward VII, which mixes actual footage with shots of a recreation of the event. Brecht was a playwright who experimented with film and incorporated film projections into some of his plays. Orson Welles made an essay film in his own pioneering style, released in 1974, called "F for Fake", which dealt specifically with art forger Elmyr de Hory and with the themes of deception, "fakery", and authenticity in general.

David Winks Gray's article "The essay film in action" states that the "essay film became an identifiable form of filmmaking in the 1950s and '60s". He states that since that time, essay films have tended to be "on the margins" of the filmmaking the world. Essay films have a "peculiar searching, questioning tone ... between documentary and fiction" but without "fitting comfortably" into either genre. Gray notes that just like written essays, essay films "tend to marry the personal voice of a guiding narrator (often the director) with a wide swath of other voices". The University of Wisconsin Cinematheque website echoes some of Gray's comments; it calls a film essay an "intimate and allusive" genre that "catches filmmakers in a pensive mood, ruminating on the margins between fiction and documentary" in a manner that is "refreshingly inventive, playful, and idiosyncratic".

In the realm of music, composer Samuel Barber wrote a set of "Essays for Orchestra", relying on the form and content of the music to guide the listener's ear, rather than any extra-musical plot or story.

A photographic essay strives to cover a topic with a linked series of photographs. Photo essays range from purely photographic works to photographs with captions or small notes to full-text essays with a few or many accompanying photographs. Photo essays can be sequential in nature, intended to be viewed in a particular order—or they may consist of non-ordered photographs viewed all at once or in an order that the viewer chooses. All photo essays are collections of photographs, but not all collections of photographs are photo essays. Photo essays often address a certain issue or attempt to capture the character of places and events.
In the visual arts, an essay is a preliminary drawing or sketch that forms a basis for a final painting or sculpture, made as a test of the work's composition (this meaning of the term, like several of those following, comes from the word "essay"'s meaning of "attempt" or "trial").




Error detection and correction

In information theory and coding theory with applications in computer science and telecommunication, error detection and correction (EDAC) or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.

"Error detection" is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver.

"Error correction" is the detection of errors and reconstruction of the original, error-free data.

In classical antiquity, copyists of the Hebrew Bible were paid for their work according to the number of s (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters. This also helped ensure accuracy in the transmission of the text with the production of subsequent copies. Between the 7th and 10th centuries CE a group of Jewish scribes formalized and expanded this to create the Numerical Masorah to ensure accurate reproduction of the sacred text. It included counts of the number of words in a line, section, book and groups of books, noting the middle stich of a book, word use statistics, and commentary. Standards became such that a deviation in even a single letter in a Torah scroll was considered unacceptable. The effectiveness of their error correction method was verified by the accuracy of copying through the centuries demonstrated by discovery of the Dead Sea Scrolls in 1947–1956, dating from .

The modern development of error correction codes is credited to Richard Hamming in 1947. A description of Hamming's code appeared in Claude Shannon's "A Mathematical Theory of Communication" and was quickly generalized by Marcel J. E. Golay.

All error-detection and correction schemes add some redundancy (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message and to recover data that has been determined to be corrupted. Error detection and correction schemes can be either systematic or non-systematic. In a systematic scheme, the transmitter sends the original (error-free) data and attaches a fixed number of "check bits" (or "parity data"), which are derived from the data bits by some encoding algorithm. If error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. If error correction is required, a receiver can apply the decoding algorithm to the received data bits and the received check bits to recover the original error-free data. In a system that uses a non-systematic code, the original message is transformed into an encoded message carrying the same information and that has at least as many bits as the original message.

Good error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common channel models include memoryless models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in bursts. Consequently, error-detecting and correcting codes can be generally distinguished between "random-error-detecting/correcting" and "burst-error-detecting/correcting". Some codes can also be suitable for a mixture of random errors and burst errors.

If the channel characteristics cannot be determined, or are highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as automatic repeat request (ARQ), and is most notably used in the Internet. An alternate approach for error control is hybrid automatic repeat request (HARQ), which is a combination of ARQ and error-correction coding.
There are three major types of error correction:

Automatic repeat request (ARQ) is an error control method for data transmission that makes use of error-detection codes, acknowledgment and/or negative acknowledgment messages, and timeouts to achieve reliable data transmission. An "acknowledgment" is a message sent by the receiver to indicate that it has correctly received a data frame.

Usually, when the transmitter does not receive the acknowledgment before the timeout occurs (i.e., within a reasonable amount of time after sending the data frame), it retransmits the frame until it is either correctly received or the error persists beyond a predetermined number of retransmissions.

Three types of ARQ protocols are Stop-and-wait ARQ, Go-Back-N ARQ, and Selective Repeat ARQ.

ARQ is appropriate if the communication channel has varying or unknown capacity, such as is the case on the Internet. However, ARQ requires the availability of a back channel, results in possibly increased latency due to retransmissions, and requires the maintenance of buffers and timers for retransmissions, which in the case of network congestion can put a strain on the server and overall network capacity.

For example, ARQ is used on shortwave radio data links in the form of ARQ-E, or combined with multiplexing as ARQ-M.

Forward error correction (FEC) is a process of adding redundant data such as an error-correcting code (ECC) to a message so that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) are introduced, either during the process of transmission or on storage. Since the receiver does not have to ask the sender for retransmission of the data, a backchannel is not required in forward error correction. Error-correcting codes are used in lower-layer communication such as cellular network, high-speed fiber-optic communication and Wi-Fi, as well as for reliable storage in media such as flash memory, hard disk and RAM.

Error-correcting codes are usually distinguished between convolutional codes and block codes:

Shannon's theorem is an important theorem in forward error correction, and describes the maximum information rate at which reliable communication is possible over a channel that has a certain error probability or signal-to-noise ratio (SNR). This strict upper limit is expressed in terms of the channel capacity. More specifically, the theorem says that there exist codes such that with increasing encoding length the probability of error on a discrete memoryless channel can be made arbitrarily small, provided that the code rate is smaller than the channel capacity. The code rate is defined as the fraction "k/n" of "k" source symbols and "n" encoded symbols.

The actual maximum code rate allowed depends on the error-correcting code used, and may be lower. This is because Shannon's proof was only of existential nature, and did not show how to construct codes that are both optimal and have efficient encoding and decoding algorithms.

Hybrid ARQ is a combination of ARQ and forward error correction. There are two basic approaches:

The latter approach is particularly attractive on an erasure channel when using a rateless erasure code.
Error detection is most commonly realized using a suitable hash function (or specifically, a checksum, cyclic redundancy check or other algorithm). A hash function adds a fixed-length "tag" to a message, which enables receivers to verify the delivered message by recomputing the tag and comparing it with the one provided.

There exists a vast variety of different hash function designs. However, some are of particularly widespread use because of either their simplicity or their suitability for detecting certain kinds of errors (e.g., the cyclic redundancy check's performance in detecting burst errors).

A random-error-correcting code based on minimum distance coding can provide a strict guarantee on the number of detectable errors, but it may not protect against a preimage attack.

A repetition code is a coding scheme that repeats the bits across a channel to achieve error-free communication. Given a stream of data to be transmitted, the data are divided into blocks of bits. Each block is transmitted some predetermined number of times. For example, to send the bit pattern , the four-bit block can be repeated three times, thus producing . If this twelve-bit pattern was received as – where the first block is unlike the other two – an error has occurred.

A repetition code is very inefficient and can be susceptible to problems if the error occurs in exactly the same place for each group (e.g., in the previous example would be detected as correct). The advantage of repetition codes is that they are extremely simple, and are in fact used in some transmissions of numbers stations.

A "parity bit" is a bit that is added to a group of source bits to ensure that the number of set bits (i.e., bits with value 1) in the outcome is even or odd. It is a very simple scheme that can be used to detect single or any other odd number (i.e., three, five, etc.) of errors in the output. An even number of flipped bits will make the parity bit appear correct even though the data is erroneous.

Parity bits added to each "word" sent are called transverse redundancy checks, while those added at the end of a stream of "words" are called longitudinal redundancy checks. For example, if each of a series of m-bit "words" has a parity bit added, showing whether there were an odd or even number of ones in that word, any word with a single error in it will be detected. It will not be known where in the word the error is, however. If, in addition, after each stream of n words a parity sum is sent, each bit of which shows whether there were an odd or even number of ones at that bit-position sent in the most recent group, the exact position of the error can be determined and the error corrected. This method is only guaranteed to be effective, however, if there are no more than 1 error in every group of n words. With more error correction bits, more errors can be detected and in some cases corrected.

There are also other bit-grouping techniques.

A "checksum" of a message is a modular arithmetic sum of message code words of a fixed word length (e.g., byte values). The sum may be negated by means of a ones'-complement operation prior to transmission to detect unintentional all-zero messages.

Checksum schemes include parity bits, check digits, and longitudinal redundancy checks. Some checksum schemes, such as the Damm algorithm, the Luhn algorithm, and the Verhoeff algorithm, are specifically designed to detect errors commonly introduced by humans in writing down or remembering identification numbers.

A "cyclic redundancy check" (CRC) is a non-secure hash function designed to detect accidental changes to digital data in computer networks. It is not suitable for detecting maliciously introduced errors. It is characterized by specification of a "generator polynomial", which is used as the divisor in a polynomial long division over a finite field, taking the input data as the dividend. The remainder becomes the result.

A CRC has properties that make it well suited for detecting burst errors. CRCs are particularly easy to implement in hardware and are therefore commonly used in computer networks and storage devices such as hard disk drives.

The parity bit can be seen as a special-case 1-bit CRC.

The output of a "cryptographic hash function", also known as a "message digest", can provide strong assurances about data integrity, whether changes of the data are accidental (e.g., due to transmission errors) or maliciously introduced. Any modification to the data will likely be detected through a mismatching hash value. Furthermore, given some hash value, it is typically infeasible to find some input data (other than the one given) that will yield the same hash value. If an attacker can change not only the message but also the hash value, then a "keyed hash" or message authentication code (MAC) can be used for additional security. Without knowing the key, it is not possible for the attacker to easily or conveniently calculate the correct keyed hash value for a modified message.

Any error-correcting code can be used for error detection. A code with "minimum Hamming distance", "d", can detect up to "d" − 1 errors in a code word. Using minimum-distance-based error-correcting codes for error detection can be suitable if a strict limit on the minimum number of errors to be detected is desired.

Codes with minimum Hamming distance "d" = 2 are degenerate cases of error-correcting codes and can be used to detect single errors. The parity bit is an example of a single-error-detecting code.

Applications that require low latency (such as telephone conversations) cannot use automatic repeat request (ARQ); they must use forward error correction (FEC). By the time an ARQ system discovers an error and re-transmits it, the re-sent data will arrive too late to be usable.

Applications where the transmitter immediately forgets the information as soon as it is sent (such as most television cameras) cannot use ARQ; they must use FEC because when an error occurs, the original data is no longer available.

Applications that use ARQ must have a return channel; applications having no return channel cannot use ARQ.

Applications that require extremely low error rates (such as digital money transfers) must use ARQ due to the possibility of uncorrectable errors with FEC.

Reliability and inspection engineering also make use of the theory of error-correcting codes.

In a typical TCP/IP stack, error control is performed at multiple levels:

The development of error-correction codes was tightly coupled with the history of deep-space missions due to the extreme dilution of signal power over interplanetary distances, and the limited power availability aboard space probes. Whereas early missions sent their data uncoded, starting in 1968, digital error correction was implemented in the form of (sub-optimally decoded) convolutional codes and Reed–Muller codes. The Reed–Muller code was well suited to the noise the spacecraft was subject to (approximately matching a bell curve), and was implemented for the Mariner spacecraft and used on missions between 1969 and 1977.

The Voyager 1 and Voyager 2 missions, which started in 1977, were designed to deliver color imaging and scientific information from Jupiter and Saturn. This resulted in increased coding requirements, and thus, the spacecraft were supported by (optimally Viterbi-decoded) convolutional codes that could be concatenated with an outer Golay (24,12,8) code. The Voyager 2 craft additionally supported an implementation of a Reed–Solomon code. The concatenated Reed–Solomon–Viterbi (RSV) code allowed for very powerful error correction, and enabled the spacecraft's extended journey to Uranus and Neptune. After ECC system upgrades in 1989, both crafts used V2 RSV coding.

The Consultative Committee for Space Data Systems currently recommends usage of error correction codes with performance similar to the Voyager 2 RSV code as a minimum. Concatenated codes are increasingly falling out of favor with space missions, and are replaced by more powerful codes such as Turbo codes or LDPC codes.

The different kinds of deep space and orbital missions that are conducted suggest that trying to find a one-size-fits-all error correction system will be an ongoing problem. For missions close to Earth, the nature of the noise in the communication channel is different from that which a spacecraft on an interplanetary mission experiences. Additionally, as a spacecraft increases its distance from Earth, the problem of correcting for noise becomes more difficult.

The demand for satellite transponder bandwidth continues to grow, fueled by the desire to deliver television (including new channels and high-definition television) and IP data. Transponder availability and bandwidth constraints have limited this growth. Transponder capacity is determined by the selected modulation scheme and the proportion of capacity consumed by FEC.

Error detection and correction codes are often used to improve the reliability of data storage media. A parity track capable of detecting single-bit errors was present on the first magnetic tape data storage in 1951. The optimal rectangular code used in group coded recording tapes not only detects but also corrects single-bit errors. Some file formats, particularly archive formats, include a checksum (most often CRC32) to detect corruption and truncation and can employ redundancy or parity files to recover portions of corrupted data. Reed-Solomon codes are used in compact discs to correct errors caused by scratches.

Modern hard drives use Reed–Solomon codes to detect and correct minor errors in sector reads, and to recover corrupted data from failing sectors and store that data in the spare sectors. RAID systems use a variety of error correction techniques to recover data when a hard drive completely fails. Filesystems such as ZFS or Btrfs, as well as some RAID implementations, support data scrubbing and resilvering, which allows bad blocks to be detected and (hopefully) recovered before they are used. The recovered data may be re-written to exactly the same physical location, to spare blocks elsewhere on the same piece of hardware, or the data may be rewritten onto replacement hardware.

Dynamic random-access memory (DRAM) may provide stronger protection against soft errors by relying on error-correcting codes. Such error-correcting memory, known as "ECC" or "EDAC-protected" memory, is particularly desirable for mission-critical applications, such as scientific computing, financial, medical, etc. as well as extraterrestrial applications due to the increased radiation in space.

Error-correcting memory controllers traditionally use Hamming codes, although some use triple modular redundancy. Interleaving allows distributing the effect of a single cosmic ray potentially upsetting multiple physically neighboring bits across multiple words by associating neighboring bits to different words. As long as a single-event upset (SEU) does not exceed the error threshold (e.g., a single error) in any particular word between accesses, it can be corrected (e.g., by a single-bit error-correcting code), and the illusion of an error-free memory system may be maintained.

In addition to hardware providing features required for ECC memory to operate, operating systems usually contain related reporting facilities that are used to provide notifications when soft errors are transparently recovered. One example is the Linux kernel's "EDAC" subsystem (previously known as "Bluesmoke"), which collects the data from error-checking-enabled components inside a computer system; besides collecting and reporting back the events related to ECC memory, it also supports other checksumming errors, including those detected on the PCI bus. A few systems also support memory scrubbing to catch and correct errors early before they become unrecoverable.




Euclidean domain

In mathematics, more specifically in ring theory, a Euclidean domain (also called a Euclidean ring) is an integral domain that can be endowed with a Euclidean function which allows a suitable generalization of the Euclidean division of integers. This generalized Euclidean algorithm can be put to many of the same uses as Euclid's original algorithm in the ring of integers: in any Euclidean domain, one can apply the Euclidean algorithm to compute the greatest common divisor of any two elements. In particular, the greatest common divisor of any two elements exists and can be written as a linear combination 
of them (Bézout's identity). Also every ideal in a Euclidean domain is principal, which implies a suitable generalization of the fundamental theorem of arithmetic: every Euclidean domain is a unique factorization domain.

It is important to compare the class of Euclidean domains with the larger class of principal ideal domains (PIDs). An arbitrary PID has much the same "structural properties" of a Euclidean domain (or, indeed, even of the ring of integers), but when an explicit algorithm for Euclidean division is known, one may use the Euclidean algorithm and extended Euclidean algorithm to compute greatest common divisors and Bézout's identity. In particular, the existence of efficient algorithms for Euclidean division of integers and of polynomials in one variable over a field is of basic importance in computer algebra.

So, given an integral domain , it is often very useful to know that has a Euclidean function: in particular, this implies that is a PID. However, if there is no "obvious" Euclidean function, then determining whether is a PID is generally a much easier problem than determining whether it is a Euclidean domain.

Euclidean domains appear in the following chain of class inclusions:

Let be an integral domain. A Euclidean function on is a function from to the non-negative integers satisfying the following fundamental division-with-remainder property:


A Euclidean domain is an integral domain which can be endowed with at least one Euclidean function. A particular Euclidean function is "not" part of the definition of a Euclidean domain, as, in general, a Euclidean domain may admit many different Euclidean functions.

In this context, and are called respectively a "quotient" and a "remainder" of the "division" (or "Euclidean division") of by . In contrast with the case of integers and polynomials, the quotient is generally not uniquely defined, but when a quotient has been chosen, the remainder is uniquely defined. 

Most algebra texts require a Euclidean function to have the following additional property:


However, one can show that (EF1) alone suffices to define a Euclidean domain; if an integral domain is endowed with a function satisfying (EF1), then can also be endowed with a function satisfying both (EF1) and (EF2) simultaneously. Indeed, for in , one can define as follows:

In words, one may define to be the minimum value attained by on the set of all non-zero elements of the principal ideal generated by .

A Euclidean function is multiplicative if and is never zero. It follows that . More generally, if and only if is a unit.

Many authors use other terms in place of "Euclidean function", such as "degree function", "valuation function", "gauge function" or "norm function". Some authors also require the domain of the Euclidean function to be the entire ring ; however, this does not essentially affect the definition, since (EF1) does not involve the value of . The definition is sometimes generalized by allowing the Euclidean function to take its values in any well-ordered set; this weakening does not affect the most important implications of the Euclidean property.

The property (EF1) can be restated as follows: for any principal ideal of with nonzero generator , all nonzero classes of the quotient ring have a representative with . Since the possible values of are well-ordered, this property can be established by showing that for any with minimal value of in its class. Note that, for a Euclidean function that is so established, there need not exist an effective method to determine and in (EF1).

Examples of Euclidean domains include:


Examples of domains that are "not" Euclidean domains include:
Let "R" be a domain and "f" a Euclidean function on "R". Then:


However, in many finite extensions of Q with trivial class group, the ring of integers is Euclidean (not necessarily with respect to the absolute value of the field norm; see below).
Assuming the extended Riemann hypothesis, if "K" is a finite extension of Q and the ring of integers of "K" is a PID with an infinite number of units, then the ring of integers is Euclidean.
In particular this applies to the case of totally real quadratic number fields with trivial class group.
In addition (and without assuming ERH), if the field "K" is a Galois extension of Q, has trivial class group and unit rank strictly greater than three, then the ring of integers is Euclidean.
An immediate corollary of this is that if the number field is Galois over Q, its class group is trivial and the extension has degree greater than 8 then the ring of integers is necessarily Euclidean.

Algebraic number fields "K" come with a canonical norm function on them: the absolute value of the field norm "N" that takes an algebraic element "α" to the product of all the conjugates of "α". This norm maps the ring of integers of a number field "K", say "O", to the nonnegative rational integers, so it is a candidate to be a Euclidean norm on this ring. If this norm satisfies the axioms of a Euclidean function then the number field "K" is called "norm-Euclidean" or simply "Euclidean". Strictly speaking it is the ring of integers that is Euclidean since fields are trivially Euclidean domains, but the terminology is standard.

If a field is not norm-Euclidean then that does not mean the ring of integers is not Euclidean, just that the field norm does not satisfy the axioms of a Euclidean function. In fact, the rings of integers of number fields may be divided in several classes:

The norm-Euclidean quadratic fields have been fully classified; they are formula_6 where formula_12 takes the values

Every Euclidean imaginary quadratic field is norm-Euclidean and is one of the five first fields in the preceding list.


Euclidean algorithm

In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his "Elements" ().
It is an example of an "algorithm", a step-by-step procedure for performing a calculation according to well-defined rules,
and is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.

The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (as and , and the same number 21 is also the GCD of 105 and . Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. When that occurs, they are the GCD of the original two numbers. By reversing the steps or using the extended Euclidean algorithm, the GCD can be expressed as a linear combination of the two original numbers, that is the sum of the two numbers, each multiplied by an integer (for example, ). The fact that the GCD can always be expressed in this way is known as Bézout's identity.

The version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two (with this version, the algorithm stops when reaching a zero remainder). With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844 (Lamé's Theorem), and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.

The Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it can be used as a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations.

The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.

The Euclidean algorithm calculates the greatest common divisor (GCD) of two natural numbers and . The greatest common divisor is the largest natural number that divides both and without leaving a remainder. Synonyms for GCD include "greatest common factor" (GCF), "highest common factor" (HCF), "highest common divisor" (HCD), and "greatest common measure" (GCM). The greatest common divisor is often written as or, more simply, as , although the latter notation is ambiguous, also used for concepts such as an ideal in the ring of integers, which is closely related to GCD.

If , then and are said to be coprime (or relatively prime). This property does not imply that or are themselves prime numbers. For example, 6 and 35 factor as 6 = 2 × 3 and 35 = 5 × 7, so they are not prime, but their prime factors are different, so 6 and 35 are coprime, with no common factors other than 1.

Let . Since and are both multiples of , they can be written and , and there is no larger number for which this is true. The natural numbers and must be coprime, since any common factor could be factored out of and to make greater. Thus, any other number that divides both and must also divide . The greatest common divisor of and is the unique (positive) common divisor of and that is divisible by any other common divisor .

The greatest common divisor can be visualized as follows. Consider a rectangular area by , and any common divisor that divides both and exactly. The sides of the rectangle can be divided into segments of length , which divides the rectangle into a grid of squares of side length . The GCD is the largest value of for which this is possible. For illustration, a rectangular area can be divided into a grid of: squares, squares, squares, squares, squares or squares. Therefore, is the GCD of and . A rectangular area can be divided into a grid of squares, with two squares along one edge () and five squares along the other ().

The greatest common divisor of two numbers and is the product of the prime factors shared by the two numbers, where each prime factor can be repeated as many times as it divides both and . For example, since can be factored into , and can be factored into , the GCD of and equals , the product of their shared prime factors (with 3 repeated since divides both). If two numbers have no common prime factors, their GCD is (obtained here as an instance of the empty product); in other words, they are coprime. A key advantage of the Euclidean algorithm is that it can find the GCD efficiently without having to compute the prime factors. Factorization of large integers is believed to be a computationally very difficult problem, and the security of many widely used cryptographic protocols is based upon its infeasibility.

Another definition of the GCD is helpful in advanced mathematics, particularly ring theory. The greatest common divisor of two nonzero numbers and is also their smallest positive integral linear combination, that is, the smallest positive number of the form where and are integers. The set of all integral linear combinations of and is actually the same as the set of all multiples of (, where is an integer). In modern mathematical language, the ideal generated by and is the ideal generated by  alone (an ideal generated by a single element is called a principal ideal, and all ideals of the integers are principal ideals). Some properties of the GCD are in fact easier to see with this description, for instance the fact that any common divisor of and also divides the GCD (it divides both terms of ). The equivalence of this GCD definition with the other definitions is described below.

The GCD of three or more numbers equals the product of the prime factors common to all the numbers, but it can also be calculated by repeatedly taking the GCDs of pairs of numbers. For example,

Thus, Euclid's algorithm, which computes the GCD of two integers, suffices to calculate the GCD of arbitrarily many integers.

The Euclidean algorithm can be thought of as constructing a sequence of non-negative integers that begins with the two given integers formula_1 and formula_2 and will eventually terminate with the integer zero: formula_3 with formula_4. The integer formula_5 will then be the GCD and we can state formula_6. The algorithm indicates how to construct the intermediate remainders formula_7 via division-with-remainder on the preceding pair formula_8 by finding an integer quotient formula_9 so that:

Because the sequence of non-negative integers formula_11 is strictly decreasing, it eventually must terminate. In other words, since formula_12 for every formula_13, and each formula_7 is an integer that is strictly smaller than the preceding formula_15, there eventually cannot be a non-negative integer smaller than zero, and hence the algorithm must terminate. In fact, the algorithm will always terminate at the n-th step with formula_16 equal to zero.

To illustrate, suppose the GCD of 1071 and 462 is requested. The sequence is initially formula_17 and in order to find formula_18, we need to find integers formula_19 and formula_20 such that:

This is the quotient formula_22 since formula_23. This determines formula_24 and so the sequence is now formula_25. The next step is to continue the sequence to find formula_26 by finding integers formula_27 and formula_28 such that:

This is the quotient formula_30 since formula_31. This determines formula_32 and so the sequence is now formula_33. The next step is to continue the sequence to find formula_34 by finding integers formula_35 and formula_36 such that:

This is the quotient formula_38 since formula_39. This determines formula_40 and so the sequence is completed as formula_41 as no further non-negative integer smaller than formula_42 can be found. The penultimate remainder formula_43 is therefore the requested GCD:

We can generalize slightly by dropping any ordering requirement on the initial two values formula_45 and formula_46. If formula_47, the algorithm may continue and trivially find that formula_48 as the sequence of remainders will be formula_49. If formula_50, then we can also continue since formula_51, suggesting the next remainder should be formula_45 itself, and the sequence is formula_53. Normally, this would be invalid because it breaks the requirement formula_20 but now we have formula_50 by construction, so the requirement is automatically satisfied and the Euclidean algorithm can continue as normal. Therefore, dropping any ordering between the first two integers does not affect the conclusion that the sequence must eventually terminate because the next remainder will always satisfy formula_56 and everything continues as above. The only modifications that need to be made are that formula_57 only for formula_58, and that the sub-sequence of non-negative integers formula_59 for formula_58 is strictly decreasing, therefore excluding formula_61 from both statements.

The validity of the Euclidean algorithm can be proven by a two-step argument. In the first step, the final nonzero remainder "r" is shown to divide both "a" and "b". Since it is a common divisor, it must be less than or equal to the greatest common divisor "g". In the second step, it is shown that any common divisor of "a" and "b", including "g", must divide "r"; therefore, "g" must be less than or equal to "r". These two opposite inequalities imply "r" = "g".

To demonstrate that "r" divides both "a" and "b" (the first step), "r" divides its predecessor "r"

since the final remainder "r" is zero. "r" also divides its next predecessor "r"

because it divides both terms on the right-hand side of the equation. Iterating the same argument, "r" divides all the preceding remainders, including "a" and "b". None of the preceding remainders "r", "r", etc. divide "a" and "b", since they leave a remainder. Since "r" is a common divisor of "a" and "b", "r" ≤ "g".

In the second step, any natural number "c" that divides both "a" and "b" (in other words, any common divisor of "a" and "b") divides the remainders "r". By definition, "a" and "b" can be written as multiples of "c" : "a" = "mc" and "b" = "nc", where "m" and "n" are natural numbers. Therefore, "c" divides the initial remainder "r", since "r" = "a" − "q""b" = "mc" − "q""nc" = ("m" − "q""n")"c". An analogous argument shows that "c" also divides the subsequent remainders "r", "r", etc. Therefore, the greatest common divisor "g" must divide "r", which implies that "g" ≤ "r". Since the first part of the argument showed the reverse ("r" ≤ "g"), it follows that "g" = "r". Thus, "g" is the greatest common divisor of all the succeeding pairs:

For illustration, the Euclidean algorithm can be used to find the greatest common divisor of "a" = 1071 and "b" = 462. To begin, multiples of 462 are subtracted from 1071 until the remainder is less than 462. Two such multiples can be subtracted ("q" = 2), leaving a remainder of 147:

Then multiples of 147 are subtracted from 462 until the remainder is less than 147. Three multiples can be subtracted ("q" = 3), leaving a remainder of 21:

Then multiples of 21 are subtracted from 147 until the remainder is less than 21. Seven multiples can be subtracted ("q" = 7), leaving no remainder:

Since the last remainder is zero, the algorithm ends with 21 as the greatest common divisor of 1071 and 462. This agrees with the gcd(1071, 462) found by prime factorization . In tabular form, the steps are:

The Euclidean algorithm can be visualized in terms of the tiling analogy given above for the greatest common divisor. Assume that we wish to cover an "a"×"b" rectangle with square tiles exactly, where "a" is the larger of the two numbers. We first attempt to tile the rectangle using "b"×"b" square tiles; however, this leaves an "r"×"b" residual rectangle untiled, where "r" < "b". We then attempt to tile the residual rectangle with "r"×"r" square tiles. This leaves a second residual rectangle "r"×"r", which we attempt to tile using "r"×"r" square tiles, and so on. The sequence ends when there is no residual rectangle, i.e., when the square tiles cover the previous residual rectangle exactly. The length of the sides of the smallest square tile is the GCD of the dimensions of the original rectangle. For example, the smallest square tile in the adjacent figure is 21×21 (shown in red), and 21 is the GCD of 1071 and 462, the dimensions of the original rectangle (shown in green).

At every step "k", the Euclidean algorithm computes a quotient "q" and remainder "r" from two numbers "r" and "r"

where the "r" is non-negative and is strictly less than the absolute value of "r". The theorem which underlies the definition of the Euclidean division ensures that such a quotient and remainder always exist and are unique.

In Euclid's original version of the algorithm, the quotient and remainder are found by repeated subtraction; that is, "r" is subtracted from "r" repeatedly until the remainder "r" is smaller than "r". After that "r" and "r" are exchanged and the process is iterated. Euclidean division reduces all the steps between two exchanges into a single step, which is thus more efficient. Moreover, the quotients are not needed, thus one may replace Euclidean division by the modulo operation, which gives only the remainder. Thus the iteration of the Euclidean algorithm becomes simply

Implementations of the algorithm may be expressed in pseudocode. For example, the division-based version may be programmed as

At the beginning of the "k"th iteration, the variable "b" holds the latest remainder "r", whereas the variable "a" holds its predecessor, "r". The step "b" := "a" mod "b" is equivalent to the above recursion formula "r" ≡ "r" mod "r". The temporary variable "t" holds the value of "r" while the next remainder "r" is being calculated. At the end of the loop iteration, the variable "b" holds the remainder "r", whereas the variable "a" holds its predecessor, "r".

In the subtraction-based version, which was Euclid's original version, the remainder calculation (codice_1) is replaced by repeated subtraction. Contrary to the division-based version, which works with arbitrary integers as input, the subtraction-based version supposes that the input consists of positive integers and stops when "a" = "b":

The variables "a" and "b" alternate holding the previous remainders "r" and "r". Assume that "a" is larger than "b" at the beginning of an iteration; then "a" equals "r", since "r" > "r". During the loop iteration, "a" is reduced by multiples of the previous remainder "b" until "a" is smaller than "b". Then "a" is the next remainder "r". Then "b" is reduced by multiples of "a" until it is again smaller than "a", giving the next remainder "r", and so on.

The recursive version is based on the equality of the GCDs of successive remainders and the stopping condition gcd("r", 0) = "r".

For illustration, the gcd(1071, 462) is calculated from the equivalent gcd(462, 1071 mod 462) = gcd(462, 147). The latter GCD is calculated from the gcd(147, 462 mod 147) = gcd(147, 21), which in turn is calculated from the gcd(21, 147 mod 21) = gcd(21, 0) = 21.

In another version of Euclid's algorithm, the quotient at each step is increased by one if the resulting negative remainder is smaller in magnitude than the typical positive remainder. Previously, the equation

assumed that . However, an alternative negative remainder can be computed:
if or
if .

If is replaced by when , then one gets a variant of Euclidean algorithm such that
at each step.

Leopold Kronecker has shown that this version requires the fewest steps of any version of Euclid's algorithm. More generally, it has been proven that, for every input numbers "a" and "b", the number of steps is minimal if and only if is chosen in order that formula_62 where formula_63 is the golden ratio.

The Euclidean algorithm is one of the oldest algorithms in common use. It appears in Euclid's "Elements" (c. 300 BC), specifically in Book 7 (Propositions 1–2) and Book 10 (Propositions 2–3). In Book 7, the algorithm is formulated for integers, whereas in Book 10, it is formulated for lengths of line segments. (In modern usage, one would say it was formulated there for real numbers. But lengths, areas, and volumes, represented as real numbers in modern usage, are not measured in the same units and there is no natural unit of length, area, or volume; the concept of real numbers was unknown at that time.) The latter algorithm is geometrical. The GCD of two lengths "a" and "b" corresponds to the greatest length "g" that measures "a" and "b" evenly; in other words, the lengths "a" and "b" are both integer multiples of the length "g".

The algorithm was probably not discovered by Euclid, who compiled results from earlier mathematicians in his "Elements". The mathematician and historian B. L. van der Waerden suggests that Book VII derives from a textbook on number theory written by mathematicians in the school of Pythagoras. The algorithm was probably known by Eudoxus of Cnidus (about 375 BC). The algorithm may even pre-date Eudoxus, judging from the use of the technical term ἀνθυφαίρεσις ("anthyphairesis", reciprocal subtraction) in works by Euclid and Aristotle.

Centuries later, Euclid's algorithm was discovered independently both in India and in China, primarily to solve Diophantine equations that arose in astronomy and making accurate calendars. In the late 5th century, the Indian mathematician and astronomer Aryabhata described the algorithm as the "pulverizer", perhaps because of its effectiveness in solving Diophantine equations. Although a special case of the Chinese remainder theorem had already been described in the Chinese book "Sunzi Suanjing", the general solution was published by Qin Jiushao in his 1247 book "Shushu Jiuzhang" (數書九章 "Mathematical Treatise in Nine Sections"). The Euclidean algorithm was first described "numerically" and popularized in Europe in the second edition of Bachet's "Problèmes plaisants et délectables" ("Pleasant and enjoyable problems", 1624). In Europe, it was likewise used to solve Diophantine equations and in developing continued fractions. The extended Euclidean algorithm was published by the English mathematician Nicholas Saunderson, who attributed it to Roger Cotes as a method for computing continued fractions efficiently.

In the 19th century, the Euclidean algorithm led to the development of new number systems, such as Gaussian integers and Eisenstein integers. In 1815, Carl Gauss used the Euclidean algorithm to demonstrate unique factorization of Gaussian integers, although his work was first published in 1832. Gauss mentioned the algorithm in his "Disquisitiones Arithmeticae" (published 1801), but only as a method for continued fractions. Peter Gustav Lejeune Dirichlet seems to have been the first to describe the Euclidean algorithm as the basis for much of number theory. Lejeune Dirichlet noted that many results of number theory, such as unique factorization, would hold true for any other system of numbers to which the Euclidean algorithm could be applied. Lejeune Dirichlet's lectures on number theory were edited and extended by Richard Dedekind, who used Euclid's algorithm to study algebraic integers, a new general type of number. For example, Dedekind was the first to prove Fermat's two-square theorem using the unique factorization of Gaussian integers. Dedekind also defined the concept of a Euclidean domain, a number system in which a generalized version of the Euclidean algorithm can be defined (as described below). In the closing decades of the 19th century, the Euclidean algorithm gradually became eclipsed by Dedekind's more general theory of ideals.

Other applications of Euclid's algorithm were developed in the 19th century. In 1829, Charles Sturm showed that the algorithm was useful in the Sturm chain method for counting the real roots of polynomials in any given interval.

The Euclidean algorithm was the first integer relation algorithm, which is a method for finding integer relations between commensurate real numbers. Several novel integer relation algorithms have been developed, such as the algorithm of Helaman Ferguson and R.W. Forcade (1979) and the LLL algorithm.

In 1969, Cole and Davie developed a two-player game based on the Euclidean algorithm, called "The Game of Euclid", which has an optimal strategy. The players begin with two piles of "a" and "b" stones. The players take turns removing "m" multiples of the smaller pile from the larger. Thus, if the two piles consist of "x" and "y" stones, where "x" is larger than "y", the next player can reduce the larger pile from "x" stones to "x" − "my" stones, as long as the latter is a nonnegative integer. The winner is the first player to reduce one pile to zero stones.

Bézout's identity states that the greatest common divisor "g" of two integers "a" and "b" can be represented as a linear sum of the original two numbers "a" and "b". In other words, it is always possible to find integers "s" and "t" such that "g" = "sa" + "tb".

The integers "s" and "t" can be calculated from the quotients "q", "q", etc. by reversing the order of equations in Euclid's algorithm. Beginning with the next-to-last equation, "g" can be expressed in terms of the quotient "q" and the two preceding remainders, "r" and "r":

Those two remainders can be likewise expressed in terms of their quotients and preceding remainders,

Substituting these formulae for "r" and "r" into the first equation yields "g" as a linear sum of the remainders "r" and "r". The process of substituting remainders by formulae involving their predecessors can be continued until the original numbers "a" and "b" are reached:

After all the remainders "r", "r", etc. have been substituted, the final equation expresses "g" as a linear sum of "a" and "b", so that "g" = "sa" + "tb".

The Euclidean algorithm, and thus Bezout's identity, can be generalized to the context of Euclidean domains.

Bézout's identity provides yet another definition of the greatest common divisor "g" of two numbers "a" and "b". Consider the set of all numbers "ua" + "vb", where "u" and "v" are any two integers. Since "a" and "b" are both divisible by "g", every number in the set is divisible by "g". In other words, every number of the set is an integer multiple of "g". This is true for every common divisor of "a" and "b". However, unlike other common divisors, the greatest common divisor is a member of the set; by Bézout's identity, choosing "u" = "s" and "v" = "t" gives "g". A smaller common divisor cannot be a member of the set, since every member of the set must be divisible by "g". Conversely, any multiple "m" of "g" can be obtained by choosing "u" = "ms" and "v" = "mt", where "s" and "t" are the integers of Bézout's identity. This may be seen by multiplying Bézout's identity by "m",

Therefore, the set of all numbers "ua" + "vb" is equivalent to the set of multiples "m" of "g". In other words, the set of all possible sums of integer multiples of two numbers ("a" and "b") is equivalent to the set of multiples of gcd("a", "b"). The GCD is said to be the generator of the ideal of "a" and "b". This GCD definition led to the modern abstract algebraic concepts of a principal ideal (an ideal generated by a single element) and a principal ideal domain (a domain in which every ideal is a principal ideal).

Certain problems can be solved using this result. For example, consider two measuring cups of volume "a" and "b". By adding/subtracting "u" multiples of the first cup and "v" multiples of the second cup, any volume "ua" + "vb" can be measured out. These volumes are all multiples of "g" = gcd("a", "b").

The integers "s" and "t" of Bézout's identity can be computed efficiently using the extended Euclidean algorithm. This extension adds two recursive equations to Euclid's algorithm

with the starting values

Using this recursion, Bézout's integers "s" and "t" are given by "s" = "s" and "t" = "t", where "N+1" is the step on which the algorithm terminates with "r" = 0.

The validity of this approach can be shown by induction. Assume that the recursion formula is correct up to step "k" − 1 of the algorithm; in other words, assume that

for all "j" less than "k". The "k"th step of the algorithm gives the equation

Since the recursion formula has been assumed to be correct for "r" and "r", they may be expressed in terms of the corresponding "s" and "t" variables

Rearranging this equation yields the recursion formula for step "k", as required

The integers "s" and "t" can also be found using an equivalent matrix method. The sequence of equations of Euclid's algorithm

can be written as a product of 2×2 quotient matrices multiplying a two-dimensional remainder vector

Let M represent the product of all the quotient matrices

This simplifies the Euclidean algorithm to the form

To express "g" as a linear sum of "a" and "b", both sides of this equation can be multiplied by the inverse of the matrix M. The determinant of M equals (−1), since it equals the product of the determinants of the quotient matrices, each of which is negative one. Since the determinant of M is never zero, the vector of the final remainders can be solved using the inverse of M

Since the top equation gives

the two integers of Bézout's identity are "s" = (−1)"m" and "t" = (−1)"m". The matrix method is as efficient as the equivalent recursion, with two multiplications and two additions per step of the Euclidean algorithm.

Bézout's identity is essential to many applications of Euclid's algorithm, such as demonstrating the unique factorization of numbers into prime factors. To illustrate this, suppose that a number "L" can be written as a product of two factors "u" and "v", that is, "L" = "uv". If another number "w" also divides "L" but is coprime with "u", then "w" must divide "v", by the following argument: If the greatest common divisor of "u" and "w" is 1, then integers "s" and "t" can be found such that

by Bézout's identity. Multiplying both sides by "v" gives the relation:

Since "w" divides both terms on the right-hand side, it must also divide the left-hand side, "v". This result is known as Euclid's lemma. Specifically, if a prime number divides "L", then it must divide at least one factor of "L". Conversely, if a number "w" is coprime to each of a series of numbers "a", "a", ..., "a", then "w" is also coprime to their product, "a" × "a" × ... × "a".

Euclid's lemma suffices to prove that every number has a unique factorization into prime numbers. To see this, assume the contrary, that there are two independent factorizations of "L" into "m" and "n" prime factors, respectively

Since each prime "p" divides "L" by assumption, it must also divide one of the "q" factors; since each "q" is prime as well, it must be that "p" = "q". Iteratively dividing by the "p" factors shows that each "p" has an equal counterpart "q"; the two prime factorizations are identical except for their order. The unique factorization of numbers into primes has many applications in mathematical proofs, as shown below.

Diophantine equations are equations in which the solutions are restricted to integers; they are named after the 3rd-century Alexandrian mathematician Diophantus. A typical "linear" Diophantine equation seeks integers "x" and "y" such that

where "a", "b" and "c" are given integers. This can be written as an equation for "x" in modular arithmetic:

Let "g" be the greatest common divisor of "a" and "b". Both terms in "ax" + "by" are divisible by "g"; therefore, "c" must also be divisible by "g", or the equation has no solutions. By dividing both sides by "c"/"g", the equation can be reduced to Bezout's identity

where "s" and "t" can be found by the extended Euclidean algorithm. This provides one solution to the Diophantine equation, "x" = "s" ("c"/"g") and "y" = "t" ("c"/"g").

In general, a linear Diophantine equation has no solutions, or an infinite number of solutions. To find the latter, consider two solutions, ("x", "y") and ("x", "y"), where

or equivalently

Therefore, the smallest difference between two "x" solutions is "b"/"g", whereas the smallest difference between two "y" solutions is "a"/"g". Thus, the solutions may be expressed as

By allowing "u" to vary over all possible integers, an infinite family of solutions can be generated from a single solution ("x", "y"). If the solutions are required to be "positive" integers ("x" > 0, "y" > 0), only a finite number of solutions may be possible. This restriction on the acceptable solutions allows some systems of Diophantine equations with more unknowns than equations to have a finite number of solutions; this is impossible for a system of linear equations when the solutions can be any real number (see Underdetermined system).

A finite field is a set of numbers with four generalized operations. The operations are called addition, subtraction, multiplication and division and have their usual properties, such as commutativity, associativity and distributivity. An example of a finite field is the set of 13 numbers {0, 1, 2, ..., 12} using modular arithmetic. In this field, the results of any mathematical operation (addition, subtraction, multiplication, or division) is reduced modulo 13; that is, multiples of 13 are added or subtracted until the result is brought within the range 0–12. For example, the result of 5 × 7 = 35 mod 13 = 9. Such finite fields can be defined for any prime "p"; using more sophisticated definitions, they can also be defined for any power "m" of a prime "p". Finite fields are often called Galois fields, and are abbreviated as GF("p") or GF("p").

In such a field with "m" numbers, every nonzero element "a" has a unique modular multiplicative inverse, "a" such that This inverse can be found by solving the congruence equation "ax" ≡ 1 mod "m", or the equivalent linear Diophantine equation

This equation can be solved by the Euclidean algorithm, as described above. Finding multiplicative inverses is an essential step in the RSA algorithm, which is widely used in electronic commerce; specifically, the equation determines the integer used to decrypt the message. Although the RSA algorithm uses rings rather than fields, the Euclidean algorithm can still be used to find a multiplicative inverse where one exists. The Euclidean algorithm also has other applications in error-correcting codes; for example, it can be used as an alternative to the Berlekamp–Massey algorithm for decoding BCH and Reed–Solomon codes, which are based on Galois fields.

Euclid's algorithm can also be used to solve multiple linear Diophantine equations. Such equations arise in the Chinese remainder theorem, which describes a novel method to represent an integer "x". Instead of representing an integer by its digits, it may be represented by its remainders "x" modulo a set of "N" coprime numbers "m":

The goal is to determine "x" from its "N" remainders "x". The solution is to combine the multiple equations into a single linear Diophantine equation with a much larger modulus "M" that is the product of all the individual moduli "m", and define "M" as

Thus, each "M" is the product of all the moduli "except" "m". The solution depends on finding "N" new numbers "h" such that

With these numbers "h", any integer "x" can be reconstructed from its remainders "x" by the equation

Since these numbers "h" are the multiplicative inverses of the "M", they may be found using Euclid's algorithm as described in the previous subsection.

The Euclidean algorithm can be used to arrange the set of all positive rational numbers into an infinite binary search tree, called the Stern–Brocot tree.
The number 1 (expressed as a fraction 1/1) is placed at the root of the tree, and the location of any other number "a"/"b" can be found by computing gcd("a","b") using the original form of the Euclidean algorithm, in which each step replaces the larger of the two given numbers by its difference with the smaller number (not its remainder), stopping when two equal numbers are reached. A step of the Euclidean algorithm that replaces the first of the two numbers corresponds to a step in the tree from a node to its right child, and a step that replaces the second of the two numbers corresponds to a step in the tree from a node to its left child. The sequence of steps constructed in this way does not depend on whether "a"/"b" is given in lowest terms, and forms a path from the root to a node containing the number "a"/"b". This fact can be used to prove that each positive rational number appears exactly once in this tree.

For example, 3/4 can be found by starting at the root, going to the left once, then to the right twice:

The Euclidean algorithm has almost the same relationship to another binary tree on the rational numbers called the Calkin–Wilf tree. The difference is that the path is reversed: instead of producing a path from the root of the tree to a target, it produces a path from the target to the root.

The Euclidean algorithm has a close relationship with continued fractions. The sequence of equations can be written in the form

The last term on the right-hand side always equals the inverse of the left-hand side of the next equation. Thus, the first two equations may be combined to form

The third equation may be used to substitute the denominator term "r"/"r", yielding

The final ratio of remainders "r"/"r" can always be replaced using the next equation in the series, up to the final equation. The result is a continued fraction

In the worked example above, the gcd(1071, 462) was calculated, and the quotients "q" were 2, 3 and 7, respectively. Therefore, the fraction 1071/462 may be written

as can be confirmed by calculation.

Calculating a greatest common divisor is an essential step in several integer factorization algorithms, such as Pollard's rho algorithm, Shor's algorithm, Dixon's factorization method and the Lenstra elliptic curve factorization. The Euclidean algorithm may be used to find this GCD efficiently. Continued fraction factorization uses continued fractions, which are determined using Euclid's algorithm.

The computational efficiency of Euclid's algorithm has been studied thoroughly. This efficiency can be described by the number of division steps the algorithm requires, multiplied by the computational expense of each step. The first known analysis of Euclid's algorithm is due to A. A. L. Reynaud in 1811, who showed that the number of division steps on input ("u", "v") is bounded by "v"; later he improved this to "v"/2  + 2. Later, in 1841, P. J. E. Finck showed that the number of division steps is at most 2 log "v" + 1, and hence Euclid's algorithm runs in time polynomial in the size of the input. Émile Léger, in 1837, studied the worst case, which is when the inputs are consecutive Fibonacci numbers. Finck's analysis was refined by Gabriel Lamé in 1844, who showed that the number of steps required for completion is never more than five times the number "h" of base-10 digits of the smaller number "b".

In the uniform cost model (suitable for analyzing the complexity of gcd calculation on numbers that fit into a single machine word), each step of the algorithm takes constant time, and Lamé's analysis implies that the total running time is also "O"("h"). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in the algorithm can be as large as "O"("h"). In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also "O"("h"). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.

The number of steps to calculate the GCD of two natural numbers, "a" and "b", may be denoted by "T"("a", "b"). If "g" is the GCD of "a" and "b", then "a" = "mg" and "b" = "ng" for two coprime numbers "m" and "n". Then

as may be seen by dividing all the steps in the Euclidean algorithm by "g". By the same argument, the number of steps remains the same if "a" and "b" are multiplied by a common factor "w": "T"("a", "b") = "T"("wa", "wb"). Therefore, the number of steps "T" may vary dramatically between neighboring pairs of numbers, such as T("a", "b") and T("a", "b" + 1), depending on the size of the two GCDs.

The recursive nature of the Euclidean algorithm gives another equation

where "T"("x", 0) = 0 by assumption.

If the Euclidean algorithm requires "N" steps for a pair of natural numbers "a" > "b" > 0, the smallest values of "a" and "b" for which this is true are the Fibonacci numbers "F" and "F", respectively. More precisely, if the Euclidean algorithm requires "N" steps for the pair "a" > "b", then one has "a" ≥ "F" and "b" ≥ "F". This can be shown by induction. If "N" = 1, "b" divides "a" with no remainder; the smallest natural numbers for which this is true is "b" = 1 and "a" = 2, which are "F" and "F", respectively. Now assume that the result holds for all values of "N" up to "M" − 1. The first step of the "M"-step algorithm is "a" = "q""b" + "r", and the Euclidean algorithm requires "M" − 1 steps for the pair "b" > "r". By induction hypothesis, one has "b" ≥ "F" and "r" ≥ "F". Therefore, "a" = "q""b" + "r" ≥ "b" + "r" ≥ "F" + "F" = "F",
which is the desired inequality.
This proof, published by Gabriel Lamé in 1844, represents the beginning of computational complexity theory, and also the first practical application of the Fibonacci numbers.

This result suffices to show that the number of steps in Euclid's algorithm can never be more than five times the number of its digits (base 10). For if the algorithm requires "N" steps, then "b" is greater than or equal to "F" which in turn is greater than or equal to "φ", where "φ" is the golden ratio. Since "b" ≥ "φ", then "N" − 1 ≤ log"b". Since log"φ" > 1/5, ("N" − 1)/5 < log"φ" log"b" = log"b". Thus, "N" ≤ 5 log"b". Thus, the Euclidean algorithm always needs less than "O"("h") divisions, where "h" is the number of digits in the smaller number "b".

The average number of steps taken by the Euclidean algorithm has been defined in three different ways. The first definition is the average time "T"("a") required to calculate the GCD of a given number "a" and a smaller natural number "b" chosen with equal probability from the integers 0 to "a" − 1

with the residual error being of order "a", where "ε" is infinitesimal. The constant "C" in this formula is called Porter's constant and equals

where "γ" is the Euler–Mascheroni constant and ζ' is the derivative of the Riemann zeta function. The leading coefficient (12/π) ln 2 was determined by two independent methods.

Since the first average can be calculated from the tau average by summing over the divisors "d" of "a"

it can be approximated by the formula

where Λ("d") is the Mangoldt function.

A third average "Y"("n") is defined as the mean number of steps required when both "a" and "b" are chosen randomly (with uniform distribution) from 1 to "n"

Substituting the approximate formula for "T"("a") into this equation yields an estimate for "Y"("n")

In each step "k" of the Euclidean algorithm, the quotient "q" and remainder "r" are computed for a given pair of integers "r" and "r"

The computational expense per step is associated chiefly with finding "q", since the remainder "r" can be calculated quickly from "r", "r", and "q"

The computational expense of dividing "h"-bit numbers scales as , where is the length of the quotient.

For comparison, Euclid's original subtraction-based algorithm can be much slower. A single integer division is equivalent to the quotient "q" number of subtractions. If the ratio of "a" and "b" is very large, the quotient is large and many subtractions will be required. On the other hand, it has been shown that the quotients are very likely to be small integers. The probability of a given quotient "q" is approximately where . For illustration, the probability of a quotient of 1, 2, 3, or 4 is roughly 41.5%, 17.0%, 9.3%, and 5.9%, respectively. Since the operation of subtraction is faster than division, particularly for large numbers, the subtraction-based Euclid's algorithm is competitive with the division-based version. This is exploited in the binary version of Euclid's algorithm.

Combining the estimated number of steps with the estimated computational expense per step shows that the Euclid's algorithm grows quadratically ("h") with the average number of digits "h" in the initial two numbers "a" and "b". Let represent the number of digits in the successive remainders . Since the number of steps "N" grows linearly with "h", the running time is bounded by

Euclid's algorithm is widely used in practice, especially for small numbers, due to its simplicity. For comparison, the efficiency of alternatives to Euclid's algorithm may be determined.

One inefficient approach to finding the GCD of two natural numbers "a" and "b" is to calculate all their common divisors; the GCD is then the largest common divisor. The common divisors can be found by dividing both numbers by successive integers from 2 to the smaller number "b". The number of steps of this approach grows linearly with "b", or exponentially in the number of digits. Another inefficient approach is to find the prime factors of one or both numbers. As noted above, the GCD equals the product of the prime factors shared by the two numbers "a" and "b". Present methods for prime factorization are also inefficient; many modern cryptography systems even rely on that inefficiency.

The binary GCD algorithm is an efficient alternative that substitutes division with faster operations by exploiting the binary representation used by computers. However, this alternative also scales like "O"("h"²). It is generally faster than the Euclidean algorithm on real computers, even though it scales in the same way. Additional efficiency can be gleaned by examining only the leading digits of the two numbers "a" and "b". The binary algorithm can be extended to other bases ("k"-ary algorithms), with up to fivefold increases in speed. Lehmer's GCD algorithm uses the same general principle as the binary algorithm to speed up GCD computations in arbitrary bases.

A recursive approach for very large integers (with more than 25,000 digits) leads to quasilinear integer GCD algorithms, such as those of Schönhage, and Stehlé and Zimmermann. These algorithms exploit the 2×2 matrix form of the Euclidean algorithm given above. These quasilinear methods generally scale as 

Although the Euclidean algorithm is used to find the greatest common divisor of two natural numbers (positive integers), it may be generalized to the real numbers, and to other mathematical objects, such as polynomials, quadratic integers and Hurwitz quaternions. In the latter cases, the Euclidean algorithm is used to demonstrate the crucial property of unique factorization, i.e., that such numbers can be factored uniquely into irreducible elements, the counterparts of prime numbers. Unique factorization is essential to many proofs of number theory.

Euclid's algorithm can be applied to real numbers, as described by Euclid in Book 10 of his "Elements". The goal of the algorithm is to identify a real number such that two given real numbers, and , are integer multiples of it: and , where and are integers. This identification is equivalent to finding an integer relation among the real numbers and ; that is, it determines integers and such that . If such an equation is possible, "a" and "b" are called commensurable lengths, otherwise they are incommensurable lengths.

The real-number Euclidean algorithm differs from its integer counterpart in two respects. First, the remainders are real numbers, although the quotients are integers as before. Second, the algorithm is not guaranteed to end in a finite number of steps. If it does, the fraction is a rational number, i.e., the ratio of two integers

and can be written as a finite continued fraction . If the algorithm does not stop, the fraction is an irrational number and can be described by an infinite continued fraction . Examples of infinite continued fractions are the golden ratio and the square root of two, . The algorithm is unlikely to stop, since almost all ratios of two real numbers are irrational.

An infinite continued fraction may be truncated at a step to yield an approximation to that improves as is increased. The approximation is described by convergents ; the numerator and denominators are coprime and obey the recurrence relation

where and are the initial values of the recursion. The convergent is the best rational number approximation to with denominator :

Polynomials in a single variable "x" can be added, multiplied and factored into irreducible polynomials, which are the analogs of the prime numbers for integers. The greatest common divisor polynomial of two polynomials and is defined as the product of their shared irreducible polynomials, which can be identified using the Euclidean algorithm. The basic procedure is similar to that for integers. At each step , a quotient polynomial and a remainder polynomial are identified to satisfy the recursive equation

where and . Each quotient polynomial is chosen such that each remainder is either zero or has a degree that is smaller than the degree of its predecessor: . Since the degree is a nonnegative integer, and since it decreases with every step, the Euclidean algorithm concludes in a finite number of steps. The last nonzero remainder is the greatest common divisor of the original two polynomials, and .

For example, consider the following two quartic polynomials, which each factor into two quadratic polynomials

Dividing by yields a remainder . In the next step, is divided by yielding a remainder . Finally, dividing by yields a zero remainder, indicating that is the greatest common divisor polynomial of and , consistent with their factorization.

Many of the applications described above for integers carry over to polynomials. The Euclidean algorithm can be used to solve linear Diophantine equations and Chinese remainder problems for polynomials; continued fractions of polynomials can also be defined.

The polynomial Euclidean algorithm has other applications, such as Sturm chains, a method for counting the zeros of a polynomial that lie inside a given real interval. This in turn has applications in several areas, such as the Routh–Hurwitz stability criterion in control theory.

Finally, the coefficients of the polynomials need not be drawn from integers, real numbers or even the complex numbers. For example, the coefficients may be drawn from a general field, such as the finite fields described above. The corresponding conclusions about the Euclidean algorithm and its applications hold even for such polynomials.

The Gaussian integers are complex numbers of the form , where and are ordinary integers and is the square root of negative one. By defining an analog of the Euclidean algorithm, Gaussian integers can be shown to be uniquely factorizable, by the argument above. This unique factorization is helpful in many applications, such as deriving all Pythagorean triples or proving Fermat's theorem on sums of two squares. In general, the Euclidean algorithm is convenient in such applications, but not essential; for example, the theorems can often be proven by other arguments.

The Euclidean algorithm developed for two Gaussian integers and is nearly the same as that for ordinary integers, but differs in two respects. As before, we set and , and the task at each step is to identify a quotient and a remainder such that

where every remainder is strictly smaller than its predecessor: . The first difference is that the quotients and remainders are themselves Gaussian integers, and thus are complex numbers. The quotients are generally found by rounding the real and complex parts of the exact ratio (such as the complex number ) to the nearest integers. The second difference lies in the necessity of defining how one complex remainder can be "smaller" than another. To do this, a norm function is defined, which converts every Gaussian integer into an ordinary integer. After each step of the Euclidean algorithm, the norm of the remainder is smaller than the norm of the preceding remainder, . Since the norm is a nonnegative integer and decreases with every step, the Euclidean algorithm for Gaussian integers ends in a finite number of steps. The final nonzero remainder is , the Gaussian integer of largest norm that divides both and ; it is unique up to multiplication by a unit, or .

Many of the other applications of the Euclidean algorithm carry over to Gaussian integers. For example, it can be used to solve linear Diophantine equations and Chinese remainder problems for Gaussian integers; continued fractions of Gaussian integers can also be defined.

A set of elements under two binary operations, denoted as addition and multiplication, is called a Euclidean domain if it forms a commutative ring and, roughly speaking, if a generalized Euclidean algorithm can be performed on them. The two operations of such a ring need not be the addition and multiplication of ordinary arithmetic; rather, they can be more general, such as the operations of a mathematical group or monoid. Nevertheless, these general operations should respect many of the laws governing ordinary arithmetic, such as commutativity, associativity and distributivity.

The generalized Euclidean algorithm requires a "Euclidean function", i.e., a mapping from into the set of nonnegative integers such that, for any two nonzero elements and in , there exist and in such that and . Examples of such mappings are the absolute value for integers, the degree for univariate polynomials, and the norm for Gaussian integers above. The basic principle is that each step of the algorithm reduces "f" inexorably; hence, if can be reduced only a finite number of times, the algorithm must stop in a finite number of steps. This principle relies on the well-ordering property of the non-negative integers, which asserts that every non-empty set of non-negative integers has a smallest member.

The fundamental theorem of arithmetic applies to any Euclidean domain: Any number from a Euclidean domain can be factored uniquely into irreducible elements. Any Euclidean domain is a unique factorization domain (UFD), although the converse is not true. The Euclidean domains and the UFD's are subclasses of the GCD domains, domains in which a greatest common divisor of two numbers always exists. In other words, a greatest common divisor may exist (for all pairs of elements in a domain), although it may not be possible to find it using a Euclidean algorithm. A Euclidean domain is always a principal ideal domain (PID), an integral domain in which every ideal is a principal ideal. Again, the converse is not true: not every PID is a Euclidean domain.

The unique factorization of Euclidean domains is useful in many applications. For example, the unique factorization of the Gaussian integers is convenient in deriving formulae for all Pythagorean triples and in proving Fermat's theorem on sums of two squares. Unique factorization was also a key element in an attempted proof of Fermat's Last Theorem published in 1847 by Gabriel Lamé, the same mathematician who analyzed the efficiency of Euclid's algorithm, based on a suggestion of Joseph Liouville. Lamé's approach required the unique factorization of numbers of the form , where and are integers, and is an th root of 1, that is, . Although this approach succeeds for some values of (such as , the Eisenstein integers), in general such numbers do factor uniquely. This failure of unique factorization in some cyclotomic fields led Ernst Kummer to the concept of ideal numbers and, later, Richard Dedekind to ideals.

The quadratic integer rings are helpful to illustrate Euclidean domains. Quadratic integers are generalizations of the Gaussian integers in which the imaginary unit "i" is replaced by a number . Thus, they have the form , where and are integers and has one of two forms, depending on a parameter . If does not equal a multiple of four plus one, then

If, however, "D" does equal a multiple of four plus one, then

If the function corresponds to a norm function, such as that used to order the Gaussian integers above, then the domain is known as "norm-Euclidean". The norm-Euclidean rings of quadratic integers are exactly those where is one of the values −11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57, or 73. The cases and yield the Gaussian integers and Eisenstein integers, respectively.

If is allowed to be any Euclidean function, then the list of possible values of for which the domain is Euclidean is not yet known. The first example of a Euclidean domain that was not norm-Euclidean (with ) was published in 1994. In 1973, Weinberger proved that a quadratic integer ring with is Euclidean if, and only if, it is a principal ideal domain, provided that the generalized Riemann hypothesis holds.

The Euclidean algorithm may be applied to some noncommutative rings such as the set of Hurwitz quaternions. Let and represent two elements from such a ring. They have a common right divisor if and for some choice of and in the ring. Similarly, they have a common left divisor if and for some choice of and in the ring. Since multiplication is not commutative, there are two versions of the Euclidean algorithm, one for right divisors and one for left divisors. Choosing the right divisors, the first step in finding the by the Euclidean algorithm can be written

where represents the quotient and the remainder. This equation shows that any common right divisor of and is likewise a common divisor of the remainder . The analogous equation for the left divisors would be

With either choice, the process is repeated as above until the greatest common right or left divisor is identified. As in the Euclidean domain, the "size" of the remainder (formally, its norm) must be strictly smaller than , and there must be only a finite number of possible sizes for , so that the algorithm is guaranteed to terminate.

Most of the results for the GCD carry over to noncommutative numbers. For example, Bézout's identity states that the right can be expressed as a linear combination of and . In other words, there are numbers and such that

The analogous identity for the left GCD is nearly the same:

Bézout's identity can be used to solve Diophantine equations. For instance, one of the standard proofs of Lagrange's four-square theorem, that every positive integer can be represented as a sum of four squares, is based on quaternion GCDs in this way.




European Centre for Medium-Range Weather Forecasts

The European Centre for Medium-Range Weather Forecasts (ECMWF) is an independent intergovernmental organisation supported by most of the nations of Europe. It is based at three sites: Shinfield Park, Reading, United Kingdom; Bologna, Italy; and Bonn, Germany. It operates one of the largest supercomputer complexes in Europe and the world's largest archive of numerical weather prediction data.

ECMWF was established in 1975, in recognition of the need to pool the scientific and technical resources of Europe's meteorological services and institutions for the production of weather forecasts for medium-range timescales (up to approximately two weeks) and of the economic and social benefits expected from it. The Centre employs about 350 staff, mostly appointed from across the member states and co-operating states.

In 2017, the centre's member states accepted an offer from the Italian Government to move ECMWF's data centre to Bologna, Italy. The new site, a former tobacco factory, would be redesigned by the architecture firm gmp.

During 2020, the Centre arranged to move its Copernicus operations away from Reading and into European Union territory. Following bids from Toulouse, Italy, Austria, Germany, Spain and Ireland, eventually Bonn (Germany) was chosen. The move has been directly attributed to Brexit.

In August 2023, the centre partnered with Huawei on its cloud, AI-powered Pangu-Weather model for 10-day global weather prediction accuracies.

ECMWF aims to provide accurate medium-range global weather forecasts out to 15 days and seasonal forecasts out to 12 months. Its products are provided to the national weather services of its member states and co-operating states as a complement to their national short-range and climatological activities, and those national states use ECMWF's products for their own national duties, in particular to give early warning of potentially damaging severe weather.

ECMWF's core mission is to:

To deliver this core mission, the Centre provides:

The Centre develops and operates global atmospheric models and data assimilation systems for the dynamics, thermodynamics and composition of the Earth's atmosphere and for interacting parts of the Earth-system. It uses numerical weather prediction methods to prepare forecasts and their initial conditions, and it contributes to monitoring the relevant parts of the Earth system.

Numerical weather prediction (NWP) requires input of meteorological data, collected by satellites and earth observation systems such as automatic and crewed weather stations, aircraft (including commercial flights), ships and weather balloons. Assimilation of this data is used to produce an initial state of a computer model of the atmosphere, from which an atmospheric model is used to forecast the weather. These forecasts are typically:

Over the past three decades ECMWF's wide-ranging programme of research has played a major role in developing such assimilation and modelling systems. This improves the accuracy and reliability of weather forecasting by about a day per decade, so that a seven-day forecast now (2015) is as accurate as a three-day forecast was four decades ago (1975).

ECMWF's monthly and seasonal forecasts provide early predictions of events such as heat waves, cold spells and droughts, as well as their impacts on sectors such as agriculture, energy and health. Since ECMWF runs a wave model, there are also predictions of coastal waves and storm surges in European waters which can be used to provide warnings.

Forecasts of severe weather events allow appropriate mitigating action to be taken and contingency plans to be put into place by the authorities and the public. The increased time gained by issuing accurate warnings can save lives, for instance by evacuating people from a storm surge area. Authorities and businesses can plan to maintain services around threats such as high winds, floods or snow.

In October 2012 the ECMWF model suggested seven days in advance that Hurricane Sandy was likely to make landfall on the East Coast of the United States.
It also predicted the intensity and track of the November 2012 nor'easter, which impacted the east coast a week after Sandy.

ECMWF's Extreme Forecast Index (EFI) was developed as a tool to identify where the EPS (Ensemble Prediction System) forecast distribution differs substantially from that of the model climate. It contains information regarding variability of weather parameters, in location and time and can highlight an abnormality of a weather situation without having to define specific space- and time-dependent thresholds.

ECMWF, through its partnerships with EUMETSAT, ESA, the EU and others, exploits satellite data for operational numerical weather prediction and operational seasonal forecasting with coupled atmosphere–ocean–land models. The increasing amount of satellite data and the development of more sophisticated ways of extracting information from that data have made a major contribution to improving the accuracy and utility of NWP forecasts. ECMWF continuously endeavours to improve the use of satellite observations for NWP.

ECMWF supports research on climate variability using an approach known as reanalysis. This involves feeding weather observations collected over decades into a NWP system to recreate past atmospheric, sea- and land-surface conditions over specific time periods to obtain a clearer picture of how the climate has changed. Reanalysis provides a four-dimensional picture of the atmosphere and effectively allows monitoring of the variability and change of global climate, thereby contributing also to the understanding and attribution of climate change.

To date, and with support from Europe's National Meteorological Services and the European Commission, ECMWF has conducted several major reanalyses of the global atmosphere: the first ECMWF re-analysis (ERA-15) project generated reanalyses from December 1978 to February 1994; the ERA-40 project generated reanalyses from September 1957 to August 2002. The ERA-Interim reanalysis covered the period from 1979 onwards. A reanalysis product (ERA5) with higher spatial resolution (31 km) was released by ECMWF in 2019 as part of the Copernicus Climate Change Service.

ECMWF's operational forecasts are produced from its "Integrated Forecast System" (sometimes informally known in the United States as the "European model") which is run every twelve hours and forecasts out to ten days.

It includes both a "deterministic forecast" mode and an ensemble. The deterministic forecast is a single model run that is relatively high in resolution as well as in computational expense. The ensemble is relatively low (about half that of the deterministic) in resolution (and in computational expense), so less accurate. But it is run 51 times in parallel, from slightly different initial conditions to give a spread of likelihood over the range of the forecast.

As of 2021, the ECMWF's weather model is generally considered to be the most accurate weather forecasting model.

The centre currently serves as the Entrusted Entity responsible for delivery of two of the Services of the EU's Copernicus Programme. The two services are the Copernicus Atmosphere Monitoring Service (CAMS) and the Copernicus Climate Change Service (C3S).

The Centre arranged to move its Copernicus operations away from Reading and into Bonn (Germany). The move has been directly attributed to Brexit.

ECMWF comprises 23 European countries:

It also has co-operation agreements with other states: Bulgaria, Czech Republic, Georgia, Hungary, Israel, Latvia, Lithuania, North Macedonia, Montenegro, Morocco, Romania and Slovakia.



