Dalek

The Daleks ( ) are a fictional extraterrestrial race of extremely xenophobic mutants principally portrayed in the British science fiction television programme "Doctor Who". They were conceived by writer Terry Nation and first appeared in the 1963 "Doctor Who" serial "The Daleks", in casings designed by Raymond Cusick.

Drawing inspiration from the Nazis, Nation portrayed the Daleks as violent, merciless and pitiless cyborg aliens, completely absent of any emotion other than hate, who demand total conformity to the will of the Dalek with the highest authority, and are bent on the conquest of the universe and the extermination of any other forms of life, including other "impure" Daleks which are deemed inferior for being different to them. Collectively, they are the greatest enemies of "Doctor Who"s protagonist, the Time Lord known as "the Doctor". During the second year of the original "Doctor Who" programme (1963–1989), the Daleks developed their own form of time travel. At the beginning of the second "Doctor Who" TV series that debuted in 2005, it was established that the Daleks had engaged in a Time War against the Time Lords that affected much of the universe and altered parts of history.

In the programme's narrative, the planet Skaro suffered a thousand-year war between two societies: the Kaleds and the Thals. During this time-period, many natives of Skaro became badly mutated by fallout from nuclear weapons and chemical warfare. The Kaled government believed in genetic purity and swore to "exterminate the Thals" for being inferior. Believing his own society was becoming weak and that it was his duty to create a new master race from the ashes of his people, the Kaled scientist Davros genetically modified several Kaleds into squid-like life-forms he called Daleks, removing "weaknesses" such as mercy and sympathy while increasing aggression and survival-instinct. He then integrated them with tank-like robotic shells equipped with advanced technology based on the same life-support system he himself used since being burned and blinded by a nuclear attack. His creations became intent on dominating the universe by enslaving or purging all "inferior" non-Dalek life.

The Daleks are the series' most popular and famous villains and their returns to television over the decades have often gained media attention. Their battle cry, a staccato "Exterminate!" has entered common usage as a popular catchphrase.

The Daleks were created by Terry Nation and designed by the BBC designer Raymond Cusick. They were introduced in December 1963 in the second "Doctor Who" serial, "The Daleks". 
The Daleks became an immediate hit with viewers, returning for subsequent appearances throughout 1960s. As early as one year after first appearing on "Doctor Who", the Daleks had become popular enough to be recognized even by non-viewers. In December 1964 editorial cartoonist Leslie Gilbert Illingworth published a cartoon in the "Daily Mail" captioned "THE DEGAULLEK", caricaturing French President Charles de Gaulle arriving at a NATO meeting as a Dalek with de Gaulle's prominent nose.

The Daleks have become as synonymous with "Doctor Who" as the Doctor himself, and their behaviour and catchphrases are now part of British popular culture. "Hiding behind the sofa whenever the Daleks appear" has been cited as an element of British cultural identity, and a 2008 survey indicated that nine out of ten British children were able to identify a Dalek correctly. In 1999 a Dalek photographed by Lord Snowdon appeared on a postage stamp celebrating British popular culture. In 2010, readers of science fiction magazine "SFX" voted the Dalek as the all-time greatest monster, beating competition including Japanese movie monster Godzilla and J. R. R. Tolkien's Gollum, of "The Lord of the Rings".

The word "Dalek" has entered major dictionaries, including the "Oxford English Dictionary", which defines "Dalek" as "In the BBC television science-fiction series Doctor Who: a member of a race of aggressive alien mutants in mobile armoured casings. Frequently in extended, allusive, or similative use." English-speakers sometimes use the term metaphorically to describe people, usually authority figures, who act like robots unable to break from their programming. For example, John Birt, the Director-General of the BBC from 1992 to 2000, was called a "croak-voiced Dalek" by playwright Dennis Potter in the MacTaggart Lecture at the 1993 Edinburgh Television Festival.

Externally, Daleks resemble human-sized pepper pots with a single mechanical eyestalk mounted on a rotating dome, a gun-mount containing an energy-weapon ("gunstick" or "death ray") resembling an egg-whisk, and a telescopic manipulator arm usually tipped by an appendage resembling a sink-plunger. Daleks have been known to use their plungers to interface with technology, crush a man's skull by suction, measure the intelligence of a subject, and extract information from a man's mind. Dalek casings are made of a bonded polycarbide material called "Dalekanium" by a member of the human resistance in "The Dalek Invasion of Earth" and the Dalek comics, as well as by the Cult of Skaro in "Daleks in Manhattan".

The lower half of a Dalek's shell is covered with hemispherical protrusions, or 'Dalek-bumps', which are shown in the episode "Dalek" to be spheres embedded in the casing. Both the BBC-licensed "Dalek Book" (1964) and "The Doctor Who Technical Manual" (1983) describe these items as being part of a sensory array, while in the 2005 series episode "Dalek" they are integral to a Dalek's forcefield mechanism, which evaporates most bullets and resists most types of energy weapons. The forcefield seems to be concentrated around the Dalek's midsection (where the mutant is located), as normally ineffective firepower can be concentrated on the eyestalk to blind a Dalek. In 2019 episode "Resolution" the bumps give way to reveal missile launchers capable of wiping out a military tank with ease. Daleks have a very limited visual field, with no peripheral sight at all, and are relatively easy to hide from in fairly exposed places. Their own energy weapons are capable of destroying them. Their weapons fire a beam that has electrical tendencies, is capable of propagating through water, and may be a form of plasma or electrolaser. The eyepiece is a Dalek's most vulnerable spot; impairing its vision often leads to a blind, panicked firing of its weapon while exclaiming "My vision is impaired; I cannot see!" Russell T Davies subverted the catchphrase in his 2008 episode "The Stolen Earth", in which a Dalek vaporises a paintball that has blocked its vision while proclaiming, "My vision is "not" impaired!"

The creature inside the mechanical casing is soft and repulsive in appearance, and vicious in temperament. The first-ever glimpse of a Dalek mutant, in "The Daleks", was a claw peeking out from under a Thal cloak after it had been removed from its casing. The mutants' actual appearance has varied, but often adheres to the Doctor's description of the species in "Remembrance of the Daleks" as "little green blobs in bonded polycarbide armour". In "Resurrection of the Daleks" a Dalek creature, separated from its casing, attacks and severely injures a human soldier; in "Remembrance of the Daleks" there are two Dalek factions (Imperial and Renegade), and the creatures inside have a different appearance in each case, one resembling the amorphous creature from "Resurrection", the other the crab-like creature from the original Dalek serial. As the creature inside is rarely seen on screen there is a common misconception that Daleks are wholly mechanical robots. In the new series Daleks are retconned to be squid-like in appearance, with small tentacles, one or two eyes, and an exposed brain. In the new series, a Dalek creature separated from its casing is shown capable of inserting a tentacle into the back of a human's neck and controlling them.

Daleks' voices are electronic; when out of its casing the mutant is only able to squeak. Once the mutant is removed the casing itself can be entered and operated by humanoids; for example, in "The Daleks", Ian Chesterton (William Russell) enters a Dalek shell to masquerade as a guard as part of an escape plan.

For many years it was assumed that, due to their design and gliding motion, Daleks were unable to climb stairs, and that this provided a simple way of escaping them. A cartoon from "Punch" pictured a group of Daleks at the foot of a flight of stairs with the caption, "Well, this certainly buggers our plan to conquer the Universe". In a scene from the serial "Destiny of the Daleks", the Doctor and companions escape from Dalek pursuers by climbing into a ceiling duct. The Fourth Doctor calls down, "If you're supposed to be the superior race of the universe, why don't you try climbing after us?" The Daleks generally make up for their lack of mobility with overwhelming firepower; a joke among "Doctor Who" fans is that "Real Daleks don't climb stairs; they level the building." Dalek mobility has improved over the history of the series: in their first appearance, in "The Daleks", they were capable of movement only on the conductive metal floors of their city; in "The Dalek Invasion of Earth" a Dalek emerges from the waters of the River Thames, indicating not only that they had become freely mobile, but that they are amphibious; "Planet of the Daleks" showed that they could ascend a vertical shaft by means of an external anti-gravity mat placed on the floor; "Revelation of the Daleks" showed Davros in his life-support chair and one of his Daleks hovering and "Remembrance of the Daleks" depicted them as capable of hovering up a flight of stairs. Despite this, journalists covering the series frequently refer to the Daleks' supposed inability to climb stairs; characters escaping up a flight of stairs in the 2005 episode "Dalek" made the same joke and were shocked when the Dalek began to hover up the stairs after uttering the phrase "ELEVATE", in a similar manner to their normal phrase "EXTERMINATE". The new series depicts the Daleks as fully capable of flight, even space flight.

The non-humanoid shape of the Dalek did much to enhance the creatures' sense of menace. A lack of familiar reference points differentiated them from the traditional "bug-eyed monster" of science fiction, which "Doctor Who" creator Sydney Newman had wanted the show to avoid. The unsettling Dalek form, coupled with their alien voices, made many believe that the props were wholly mechanical and operated by remote control.

The Daleks were actually controlled from inside by short operators, who had to manipulate their eyestalks, domes and arms, as well as flashing the lights on their heads in sync with the actors supplying their voices. The Dalek cases were built in two pieces; an operator would step into the lower section and then the top would be secured. The operators looked out between the cylindrical louvres just beneath the dome, which were lined with mesh to conceal their faces.

In addition to being hot and cramped, the Dalek casings also muffled external sounds, making it difficult for operators to hear the director or dialogue. John Scott Martin, a Dalek operator from the original series, said that Dalek operation was a challenge: "You had to have about six hands: one to do the eyestalk, one to do the lights, one for the gun, another for the smoke canister underneath, yet another for the sink plunger. If you were related to an octopus then it helped."

For "Doctor Who"'s 21st-century revival the Dalek casings retain the same overall shape and dimensional proportions of previous Daleks, although many details have been redesigned to give the Dalek a heavier and more solid look. Changes include a larger, more pointed base; a glowing eyepiece; an all-over metallic-brass finish (specified by Davies); thicker, nailed strips on the "neck" section; a housing for the eyestalk pivot; and significantly larger dome lights. The new prop made its on-screen debut in the 2005 episode "Dalek". These Dalek casings use a short operator inside the housing while the 'head' and eyestalk are operated via remote control. A third person, Nicholas Briggs, supplies the voice in their various appearances. In the 2010 season, a new, larger model appeared in several colours representing different parts of the Dalek command hierarchy.

Terry Nation's original plan was for the Daleks to glide across the floor. Early versions of the Daleks rolled on nylon castors, propelled by the operator's feet. Although castors were adequate for the Daleks' debut serial, which was shot entirely at the BBC's Lime Grove Studios, for "The Dalek Invasion of Earth" Terry Nation wanted the Daleks to be filmed on the streets of London. To enable the Daleks to travel smoothly on location, designer Spencer Chapman built the new Dalek shells around miniature tricycles with sturdier wheels, which were hidden by enlarged fenders fitted below the original base. The uneven flagstones of Central London caused the Daleks to rattle as they moved and it was not possible to remove this noise from the final soundtrack. A small parabolic dish was added to the rear of the prop's casing to explain why these Daleks, unlike the ones in their first serial, were not dependent on static electricity drawn up from the floors of the Dalek city for their motive power.

Later versions of the prop had more efficient wheels and were once again simply propelled by the seated operators' feet, but they remained so heavy that when going up ramps they often had to be pushed by stagehands out of camera shot. The difficulty of operating all the prop's parts at once contributed to the occasionally jerky Dalek movements. This problem has largely been eradicated with the advent of the "new series" version, as its remotely controlled dome and eyestalk allow the operator to concentrate on the smooth movement of the Dalek and its arms.

The staccato delivery, harsh tone and rising inflection of the Dalek voice were initially developed by two voice actors, Peter Hawkins and David Graham, who varied the pitch and speed of the lines according to the emotion needed. Their voices were further processed electronically by Brian Hodgson at the BBC Radiophonic Workshop. The sound-processing devices used have varied over the decades. In 1963 Hodgson and his colleagues used equalisation to boost the mid-range of the actor's voice, then subjected it to ring modulation with a 30 Hz sine wave. The distinctive harsh, grating vocal timbre this produced has remained the pattern for all Dalek voices since (with the exception of those in the 1985 serial "Revelation of the Daleks", for which the director, Graeme Harper, deliberately used less distortion).

Besides Hawkins and Graham, other voice actors for the Daleks have included Roy Skelton, who first voiced the Daleks in the 1967 story "The Evil of the Daleks" and provided voices for five additional Dalek serials including "Planet of the Daleks", and for the one-off anniversary special "The Five Doctors". Michael Wisher, the actor who originated the role of Dalek creator Davros in "Genesis of the Daleks", provided Dalek voices for that same story, as well as for "Frontier in Space", "Planet of the Daleks", and "Death to the Daleks". Other Dalek voice actors include Royce Mills (three stories), Brian Miller (two stories), and Oliver Gilbert and Peter Messaline (one story). John Leeson, who performed the voice of K9 in several "Doctor Who" stories, and Davros actors Terry Molloy and David Gooderson also contributed supporting voices for various Dalek serials.

Since 2005 the Dalek voice in the television series has been provided by Nicholas Briggs, speaking into a microphone connected to a voice modulator. Briggs had previously provided Dalek and other alien voices for Big Finish Productions audio plays, and continues to do so. In a 2006 BBC Radio interview, Briggs said that when the BBC asked him to do the voice for the new television series, they instructed him to bring his own analogue ring modulator that he had used in the audio plays. The BBC's sound department had changed to a digital platform and could not adequately create the distinctive Dalek sound with their modern equipment. Briggs went as far as to bring the voice modulator to the actors' readings of the scripts.

Manufacturing the props was expensive. In scenes where many Daleks had to appear, some of them would be represented by wooden replicas ("Destiny of the Daleks") or life-size photographic enlargements in the early black-and-white episodes ("The Daleks", "The Dalek Invasion of Earth", and "The Power of the Daleks"). In stories involving armies of Daleks, the BBC effects team even turned to using commercially available toy Daleks, manufactured by Louis Marx & Co and Herts Plastic Moulders Ltd. Examples of this can be observed in the serials "The Power of the Daleks", "The Evil of the Daleks", and "Planet of the Daleks". Judicious editing techniques also gave the impression that there were more Daleks than were actually available, such as using a split screen in "The Parting of the Ways".

Four fully functioning props were commissioned for the first serial "The Daleks" in 1963, and were constructed from BBC plans by Shawcraft Engineering. These became known in fan circles as "Mk I Daleks". Shawcraft were also commissioned to construct approximately 20 Daleks for the two Dalek movies in 1965 and 1966 (see below). Some of these movie props filtered back to the BBC and were seen in the televised serials, notably "The Chase", which was aired before the first movie's debut. The remaining props not bought by the BBC were either donated to charity or given away as prizes in competitions.

The BBC's own Dalek props were reused many times, with components of the original Shawcraft "Mk I Daleks" surviving right through to their final classic series appearance in 1988. But years of storage and repainting took their toll. By the time of the Sixth Doctor's "Revelation of the Daleks" new props were being manufactured out of fibreglass. These models were lighter and more affordable to construct than their predecessors. These newer models were slightly bulkier in appearance around the mid-shoulder section, and also had a redesigned skirt section which was more vertical at the back. Other minor changes were made to the design due to these new construction methods, including altering the fender and incorporating the arm boxes, collars, and slats into a single fibreglass moulding. These props were repainted in grey for the Seventh Doctor serial "Remembrance of the Daleks" and designated as "Renegade Daleks"; another redesign, painted in cream and gold, became the "Imperial Dalek" faction.

New Dalek props were built for the 21st-century version of "Doctor Who". The first, which appeared alone in the 2005 episode "Dalek", was built by modelmaker Mike Tucker. Additional Dalek props based on Tucker's master were subsequently built out of fibreglass by Cardiff-based Specialist Models.

Wishing to create an alien creature that did not look like a "man in a suit", Terry Nation stated in his script for the first Dalek serial that they should have no legs. He was also inspired by a performance by the Georgian National Ballet, in which dancers in long skirts appeared to glide across the stage. For many of the shows the Daleks were operated by retired ballet dancers wearing black socks while sitting inside the Dalek. Raymond Cusick was given the task of designing the Daleks when Ridley Scott, then a designer for the BBC, proved unavailable after having been initially assigned to their debut serial. According to Jeremy Bentham's "Doctor Who—The Early Years" (1986), after Nation wrote the script, Cusick was given only an hour to come up with the design for the Daleks and was inspired in his initial sketches by a pepper pot on a table. Cusick himself, however, states that he based it on a man seated in a chair, and used the pepper pot only to demonstrate how it might move. 

In 1964, Nation told a "Daily Mirror" reporter that the Dalek name came from a dictionary or encyclopaedia volume, the spine of which read "Dal – Lek" (or, according to another version, "Dal – Eks"). He later admitted that this book and the associated origin of the Dalek name were completely fictitious, and that anyone bothering to check out his story would have found him out. The name had simply rolled off his typewriter. Later, Nation was pleasantly surprised to discover that in Serbo-Croatian the word "dalek" means "far" or "distant".

Nation grew up during the Second World War and remembered the fear caused by German bombings. He consciously based the Daleks on the Nazis, conceiving the species as faceless, authoritarian figures dedicated to conquest, racial purity and complete conformity. The allusion is most obvious in the Dalek stories written by Nation, in particular "The Dalek Invasion of Earth" (1964) and "Genesis of the Daleks" (1975).

Before he wrote the first Dalek serial, Nation was a scriptwriter for the comedian Tony Hancock. The two men had a falling out and Nation either resigned or was fired. Hancock worked on several series proposals, one of which was called "From Plip to Plop", a comedic history of the world that would have ended with a nuclear apocalypse, the survivors being reduced to living in dustbin-like robot casings and eating radiation to stay alive. According to Hancock's biographer Cliff Goodwin, when Hancock saw the Daleks he allegedly shouted at the screen, "That bloody Nation — he's stolen my robots!"

The titling of early "Doctor Who" stories is complex and sometimes controversial. The first Dalek serial is called, variously, "The Survivors" (the pre-production title and on-screen title used for the serial's second episode), "The Mutants" (its official title at the time of production and broadcast, later taken by another unrelated story), "Beyond the Sun" (used on some production documentation), "The Dead Planet" (the on-screen title of the serial's first episode), or simply "The Daleks".

The instant appeal of the Daleks caught the BBC off-guard, and transformed "Doctor Who" into a national phenomenon. Children were both frightened and fascinated by the alien look of the monsters, and the idea of "hiding behind the sofa" became a popular, if inaccurate or exaggerated, meme. The "Doctor Who" production office was inundated with letters and calls asking about the creatures. Newspaper articles focused attention on the series and the Daleks, further enhancing their popularity.

Nation jointly owned the intellectual property rights to the Daleks with the BBC, and the money-making concept proved nearly impossible to sell to anyone else, so he was dependent on the BBC wanting to produce stories featuring the creatures. Several attempts to market the Daleks outside the series were unsuccessful. Since Nation's death in 1997, his share of the rights is now administered by his former agent, Tim Hancock.

Early plans for what eventually became the 1996 "Doctor Who" television movie included radically redesigned Daleks whose cases unfolded like spiders' legs. The concept for these "Spider Daleks" was abandoned, but it was picked up again in several "Doctor Who" spin-offs.

When the new series was announced, many fans hoped that the Daleks would return once more to the programme. The Nation estate, however, demanded levels of creative control over the Daleks' appearances and scripts that were unacceptable to the BBC. Eventually the Daleks were cleared to appear in the first series. In 2014, "Doctor Who" showrunner Steven Moffat denied their numerous appearances since was as a result of a contractual obligation.

Dalek in-universe history has seen many retroactive changes, which have caused continuity problems. When the Daleks first appeared, they were presented as the descendants of the Dals, mutated after a brief nuclear war between the Dal and Thal races 500 years ago. This race of Daleks is destroyed when their power supply is wrecked. However, when they reappear in "The Dalek Invasion of Earth", they have conquered Earth in the 22nd century. Later stories saw them develop time travel and a space empire. In 1975, Terry Nation revised the Daleks' origins in "Genesis of the Daleks", where the Dals were now called Kaleds (of which "Daleks" is an anagram), and the Dalek design was attributed to one man, the paralyzed Kaled chief scientist and evil genius, Davros. Later Big Finish Productions audio plays attempted to explain this retcon by saying that the Skaro word "dal" simply means warrior, which is how the Kaleds described themselves, while "dal-ek" means "god." According to "Genesis of the Daleks", instead of a short nuclear exchange, the Kaled-Thal war was a thousand-year-long war of attrition, fought with nuclear, biological and chemical weapons which caused widespread mutations among the life forms of Skaro. Davros experimented on living Kaled cells to find the ultimate mutated form of the Kaled species, believing his own people had become weak and needed to be replaced by a greater life form. He placed his new Dalek creations in tank-like "travel machines" of advanced technology whose design was based on his own life-support chair.

"Genesis of the Daleks" marked a new era for the depiction of the species, with most of their previous history either forgotten or barely referred to again. Future stories in the original "Doctor Who" series, which followed a rough story arc, would also focus more on Davros, much to the dissatisfaction of some fans who felt that the Daleks should take centre stage rather than merely becoming minions of their creator. Davros made his last televised appearance for 20 years in "Remembrance of the Daleks", which depicted a civil war between two factions of Daleks. One faction, the "Imperial Daleks", were loyal to Davros, who had become their Emperor, whilst the other, the "Renegade Daleks", followed a black Supreme Dalek. By the end of the story, armies of both factions have been wiped out and the Doctor has tricked them into destroying Skaro. However, Davros escapes and based on the fact that Daleks possess time travel and were spread throughout the universe, there was still a possibility that many had survived these events.

The original "classic" "Doctor Who" series ended in 1989. In the 1996 "Doctor Who" TV-movie (which introduced the Eighth Doctor), Skaro has seemingly been recreated and the Daleks are shown to still rule it. Though the aliens are never seen on-screen, the story shows the Time Lord villain the Master being executed on Skaro as Dalek voices chant "Exterminate." In Eighth Doctor audio plays produced by Big Finish from 2000–2005, Paul McGann reprised his role. The audio play "The Time of the Daleks" featured the Daleks without Davros and nearly removing William Shakespeare from history. In "Terror Firma", the Eighth Doctor met a Dalek faction led by Davros who was devolving more into a Dalek-like life form himself while attempting to create new Daleks from mutated humans of Earth. The audio dramas "The Apocalypse Element" and "Dalek Empire" also depicted the alien villains invading Gallifrey and then creating their own version of the Time Lord power source known as the Eye of Harmony, allowing the Daleks to rebuild an empire and become a greater threat against the Time Lords and other races that possess time travel.

A new "Doctor Who" series premiered in 2005, introducing the Ninth Doctor and revealing that the "Last Great Time War" had just ended, resulting in the seeming destruction of the Time Lord society. The episode "Dalek", written by Robert Shearman, was broadcast on BBC One on 30 April 2005 and confirmed that the Time War had mainly involved the Daleks fighting the Time Lords, with the Doctor ending the conflict by seemingly destroying both sides, remarking that his own survival was "not by choice." The episode featured a single Dalek who appeared to be the sole survivor of his race from the Time War. Later audio plays by Big Finish Productions expanded on the Time War in different audio drama series such as "Gallifrey: Time War, The Eighth Doctor: Time War, The War Doctor," and "The War Master."

A Dalek Emperor returned at the end of the 2005 series, having survived the Time War and then rebuilt the Dalek race with genetic material harvested from human subjects. It saw itself as a god, and the new human-based Daleks were shown worshipping it. The Emperor and this Dalek fleet were destroyed in "The Parting of the Ways". The 2006 season finale "Army of Ghosts"/"Doomsday" featured a squad of four pure-bred Dalek survivors from the old Empire, known as the Cult of Skaro, composed of Daleks who were tasked with developing imagination to better predict and combat enemies. These Daleks took on names: Jast, Thay, Caan, and their black Dalek leader Sec. The Cult had survived the Time War by escaping into the Void between dimensions. They emerged along with the Genesis Ark, a Time Lord prison vessel containing millions of pure Daleks, at Canary Wharf due to the actions of the Torchwood Institute and Cybermen from a parallel world. This resulted in a Cyberman-Dalek clash in London, which was resolved when the Tenth Doctor caused both groups to be suckedunprotectedinto the Void. The Cult of Skaro survived by utilising an "emergency temporal shift" to escape.

The four-Dalek Cult of Skaro returned in the two-part story "Daleks in Manhattan"/"Evolution of the Daleks", in which whilst stranded in 1930s New York, they set up a base in the partially built Empire State Building and attempt to rebuild the Dalek race. To this end, Dalek Sec merges with a human being to become a Human/Dalek hybrid. The Cult then set about creating "Human Daleks" by "formatting" the brains of a few thousand captured humans so they can have Dalek minds. Dalek Sec, however, becomes more human in personality and alters the plan so the hybrids will be more human like him. The rest of the Cult mutinies. Sec is killed, while Thay and Jast are later wiped out with the hybrids. Dalek Caan, believing it may be the last of its kind now, escapes once more via an emergency temporal shift.

The Daleks returned in the 2008 season's two-part finale, "The Stolen Earth"/"Journey's End", accompanied once again by their creator Davros. The story reveals that Caan's temporal shift sent him into the Time War, despite the War being "Time-Locked". The experience of piercing the Time-Lock resulted in Caan seeing parts of several futures, destroying his sanity in the process. Caan rescued many pure-bred Time War era Daleks and Davros, who created new pure Dalek troops using his own body's cells (his Kaled DNA, as all pure Daleks were originally Kaleds). A red Supreme Dalek leads the new army while keeping Caan and Davros imprisoned on the Dalek flagship, the "Crucible". Davros and the Daleks plan to destroy reality itself with a "reality bomb". The plan fails due to the interference of Donna Noble, a companion of the Doctor, and Caan, who has been manipulating events to destroy the Daleks after realising the severity of the atrocities they have committed.

The Daleks returned in the 2010 episode "Victory of the Daleks", wherein it is revealed that some Daleks survived the destruction of their army in "Journey's End" and retrieved the "Progenitor", a tiny apparatus containing 'original' Dalek DNA. The activation of the Progenitor results in the creation of New Paradigm Daleks who deem the Time War era Daleks to be inferior. The new Daleks are organised into different roles (drone, scientist, strategists, supreme and eternal), which are identifiable with colour-coded armour instead of the identification plates under the eyestalk used by their predecessors. They escape the Doctor at the end of the episode via time travel with the intent to rebuild their Empire.

The Daleks appeared, only briefly, in subsequent finales "The Pandorica Opens"/"The Big Bang" (2010) and The Wedding of River Song (2011) as Steven Moffat decided to "give them a rest" and stated, "There's a problem with the Daleks. They are the most famous of the Doctor's adversaries and the most frequent, which means they are the most reliably defeatable enemies in the universe." These episodes also reveal that Skaro has been recreated yet again. They next appear in "Asylum of the Daleks" (2012), where the Daleks are shown to have greatly increased numbers and now have a Parliament; in addition to the traditional "modern" Daleks, several designs from both the original and new series appear, all co-existing rather than judging each other as inferior or outdated (except for those Daleks whose personalities deem them "insane" or can no longer battle). All record of the Doctor is removed from their collective consciousness at the end of the episode.

The Daleks then appear in the 50th Anniversary special "The Day of the Doctor" (2013), where they are seen being defeated in the Time War. The same special reveals that many Time Lords survived the war since the Doctor found a way to transfer planet Gallifrey out of phase with reality and into a pocket dimension. In "The Time of the Doctor" (2013), the Daleks are one of the races that besieges Trenzalore in an attempt to stop the Doctor from releasing the Time Lords from the pocket dimension. After converting Tasha Lem into a Dalek puppet, they regain knowledge of the Doctor.

The Twelfth Doctor's first encounter with the Daleks is in his second full episode, "Into the Dalek" (2014), where he encounters a damaged Dalek he names 'Rusty.' Connecting to the Doctor's love of the universe and his hatred of the Daleks, Rusty assumes a mission to destroy other Daleks. In "The Magician's Apprentice"/"The Witch's Familiar" (2015), the Doctor is summoned to Skaro where he learns Davros has rebuilt the Dalek Empire. In "The Pilot" (2017), the Doctor briefly visits a battle during the Dalek-Movellan war.

The Thirteenth Doctor encountered a Dalek in a New Year's Day episode, "Resolution" (2019), when a Dalek mutant, separated from its armoured casing, takes control of a human in order to build a new travel device for itself and summon more Daleks to conquer Earth. This Dalek is cloned by a scientist in "Revolution of the Daleks" (2021), and attempts to take over Earth using further clones, but they are killed by other Daleks for perceived genetic impurity. The Dalek army is later sent by the Doctor into the "void" between worlds to be destroyed, using a spare TARDIS she recently acquired on Gallifrey. After cameo appearances depicting them as one of several villains trying to take advantage of "the Flux" event tearing through space-time in series 13, the Daleks returned in the first 2022 special, "Eve of the Daleks". In the episode, a team of Dalek Executioners are dispatched by High Command to avenge the Dalek War Fleet destroyed by the Doctor in the series 13 finale "The Vanquishers", only for a time loop established by the TARDIS to save the Doctor's life and give her a chance to destroy the executioners instead. The Daleks later appeared alongside the Cybermen as allies to the Master in "The Power of the Doctor" as part of a plot to finally destroy their nemesis, but the alliance is defeated by the Doctor and new and old companions.

In a video short for the 2023 "Children in Need" telethon, the origin of the iconic plunger-like appendages used by Daleks was retroactively established as being from the Fourteenth Doctor's TARDIS, while also establishing an unintentional hint by that Doctor, given to a Kaled military officer, for the creation of the name "Dalek".

Daleks have little, if any, individual personality, ostensibly no emotions other than hatred and anger, and a strict command structure in which they are conditioned to obey superiors' orders without question. Dalek speech is characterised by repeated phrases, and by orders given to themselves and to others. Unlike the stereotypical emotionless robots often found in science fiction, Daleks are often angry; author Kim Newman has described the Daleks as behaving "like toddlers in perpetual hissy fits", gloating when in power and flying into a rage when thwarted. They tend to be excitable and will repeat the same word or phrase over and over again in heightened emotional states, most famously "Exterminate! Exterminate!"

Daleks are extremely aggressive, and seem driven by an instinct to attack. This instinct is so strong that Daleks have been depicted fighting the urge to kill or even attacking when unarmed. The Fifth Doctor characterises this impulse by saying, "However you respond [to Daleks] is seen as an act of provocation." The fundamental feature of Dalek culture and psychology is an unquestioned belief in the superiority of the Dalek race, and their default directive is to destroy all non-Dalek life-forms. Other species are either to be exterminated immediately or enslaved and then exterminated once they are no longer useful.

The Dalek obsession with their own superiority is illustrated by the schism between the Renegade and Imperial Daleks seen in "Revelation of the Daleks" and "Remembrance of the Daleks": the two factions each consider the other to be a perversion despite the relatively minor differences between them. This intolerance of any "contamination" within themselves is also shown in "Dalek", "The Evil of the Daleks" and in the Big Finish Productions audio play "The Mutant Phase". This superiority complex is the basis of the Daleks' ruthlessness and lack of compassion. This is shown in extreme in "Victory of the Daleks", where the new, pure Daleks destroy their creators, impure Daleks, with the latter's consent. It is nearly impossible to negotiate or reason with a Dalek, a single-mindedness that makes them dangerous and not to be underestimated. The Eleventh Doctor (Matt Smith) is later puzzled in the "Asylum of the Daleks" as to why the Daleks don't just kill the sequestered ones that have "gone wrong". Although the Asylum is subsequently obliterated, the Prime Minister of the Daleks explains that "it is offensive to us to destroy such divine hatred", and the Doctor is sickened at the revelation that hatred is actually considered beautiful by the Daleks.

Dalek society is depicted as one of extreme scientific and technological advancement; the Third Doctor states that "it was their inventive genius that made them one of the greatest powers in the universe." However, their reliance on logic and machinery is also a strategic weakness which they recognise, and thus use more emotion-driven species as agents to compensate for these shortcomings.

Although the Daleks are not known for their regard for due process, they have taken at least two enemies back to Skaro for a "trial", rather than killing them immediately. The first was their creator, Davros, in "Revelation of the Daleks", and the second was the renegade Time Lord known as the Master in the 1996 television movie. The reasons for the Master's trial, and why the Doctor would be allowed to retrieve the Master's remains, have never been explained on screen. The "Doctor Who Annual 2006" implies that the trial may have been due to a treaty signed between the Time Lords and the Daleks. The framing device for the "" audio plays is a Dalek trial to determine if Davros should be the Daleks' leader once more.

Spin-off novels contain several tongue-in-cheek mentions of Dalek poetry, and an anecdote about an opera based upon it, which was lost to posterity when the entire cast was exterminated on the opening night. Two stanzas are given in the novel "The Also People" by Ben Aaronovitch. In an alternative timeline portrayed in the Big Finish Productions audio adventure "The Time of the Daleks", the Daleks show a fondness for the works of Shakespeare. A similar idea was satirised by comedian Frankie Boyle in the BBC comedy quiz programme "Mock the Week"; he gave the fictional Dalek poem "Daffodils; EXTERMINATE DAFFODILS!" as an "unlikely line to hear in "Doctor Who"".

Because the Doctor has defeated the Daleks so often, he has become their collective arch-enemy and they have standing orders to capture or exterminate him on sight. In later fiction, the Daleks know the Doctor as ""Ka Faraq Gatri"" ("Bringer of Darkness" or "Destroyer of Worlds"), and "The Oncoming Storm". Both the Ninth Doctor (Christopher Eccleston) and Rose Tyler (Billie Piper) suggest that the Doctor is one of the few beings the Daleks fear. In "Doomsday", Rose notes that while the Daleks see the extermination of five million Cybermen as "pest control", "one Doctor" visibly un-nerves them (to the point they physically recoil). To his indignant surprise, in "Asylum of the Daleks", the Eleventh Doctor (Matt Smith) learns that the Daleks have designated him as "The Predator".

A "rel" is a Dalek and Kaled unit of measurement. It was usually a measurement of time, with a duration of slightly more than one second, as mentioned in "Doomsday", "Evolution of the Daleks" and "Journey's End", counting down to the ignition of the reality bomb. (One earth minute most likely equals about 50 "rels".) However, in some comic books it was also used as a unit of velocity. Finally, in some cases it was used as a unit of hydroelectric energy (not to be confused with a vep, the unit used to measure artificial sunlight).

The "rel" was first used in the non-canonical feature film "", soon after appearing in early Doctor Who comic books.

Two "Doctor Who" movies starring Peter Cushing featured the Daleks as the main villains: "Dr. Who and the Daleks", and "Daleks - Invasion Earth 2150 AD", based on the television serials "The Daleks" and "The Dalek Invasion of Earth", respectively. The movies were not direct remakes; for example, the Doctor in the Cushing films was a human inventor called "Dr. Who" who built a time-travelling device named "Tardis", instead of a mysterious alien who stole a device called "the TARDIS".

Four books focusing on the Daleks were published in the 1960s. "The Dalek Book" (1964, written by Terry Nation and David Whitaker), "The Dalek World" (1965, written by Nation and Whitaker) and "The Dalek Outer Space Book" (1966, by Nation and Brad Ashton) were all hardcover books formatted like annuals, containing text stories and comics about the Daleks, along with fictional information (sometimes based on the television serials, other times made up for the books). Nation also published "The Dalek Pocketbook and Space-Travellers Guide", which collected articles and features treating the Daleks as if they were real. Four more annuals were published in the 1970s by World Distributors under the title "Terry Nation's Dalek Annual" (with cover dates 1976–1979, but published 1975–1978). Two original novels by John Peel, "War of the Daleks" (1997) and "Legacy of the Daleks" (1998), were released as part of the Eighth Doctor Adventures series of "Doctor Who" novels. A novella, "The Dalek Factor" by Simon Clark, was published in 2004, and two books featuring the Daleks and the Tenth Doctor ("I am a Dalek" by Gareth Roberts, 2006, and "Prisoner of the Daleks" by Trevor Baxendale, 2009) have been released as part of the New Series Adventures.

Nation authorised the publication of the comic strip "The Daleks" in the comic "TV Century 21" in 1965. The weekly one-page strip, written by Whitaker but credited to Nation, featured the Daleks as protagonists and "heroes", and continued for two years, from their creation of the mechanised Daleks by the humanoid Dalek scientist, Yarvelling, to their eventual discovery in the ruins of a crashed space-liner of the co-ordinates for Earth, which they proposed to invade. Although much of the material in these strips was directly contradicted by what was later shown on television, some concepts like the Daleks using humanoid duplicates and the design of the Dalek Emperor did show up later on in the programme.

At the same time, a "Doctor Who" strip was also being published in "TV Comic". Initially, the strip did not have the rights to use the Daleks, so the First Doctor battled the "Trods" instead, cone-shaped robotic creatures that ran on static electricity. By the time the Second Doctor appeared in the strip in 1967 the rights issues had been resolved, and the Daleks began making appearances starting in "The Trodos Ambush" (TVC #788-#791), where they massacred the Trods. The Daleks also made appearances in the Third Doctor-era "Dr. Who" comic strip that featured in the combined "Countdown/TV Action" comic during the early 1970s.

An animated series called "Daleks!", which consists of five 10-minute long episodes, was released on the official "Doctor Who" YouTube channel in 2020.

Other licensed appearances have included a number of stage plays (see Stage plays below) and television adverts for Wall's "Sky Ray" ice lollies (1966), Weetabix breakfast cereal (1977), Kit Kat chocolate bars (2001), and the ANZ Bank (2005). In 2003, Daleks also appeared in UK billboard ads for Energizer batteries, alongside the slogan "Are You Power Mad?"




Daleks have made cameo appearances in television programmes and films unrelated to "Doctor Who" from the 1960s to the present day.



Daleks have been referred to or associated in many musical compositions.


Licensed "Doctor Who" games featuring Daleks include 1984's "The Key to Time", a text adventure game for the ZX Spectrum. The first graphical game to feature daleks was the eponymous, turn-based title released by Johan Strandberg for the Macintosh in the same year. Daleks also appeared in minor roles or as thinly disguised versions in other, minor games throughout the 80s, but did not feature as central adversaries in a licensed game until 1992, when Admiral Software published "Dalek Attack". The game allowed the player to play various Doctors or companions, running them through several environments to defeat the Daleks. In 1997 the BBC released a PC game entitled "Destiny of the Doctors" which also featured the Daleks, among other adversaries.

One authorised online game is "The Last Dalek", a Flash game created by New Media Collective for the BBC. It is based on the 2005 episode "Dalek" and can be played at the official BBC "Doctor Who" website. The "Doctor Who" website also features another game, "Daleks vs Cybermen" (also known as "Cyber Troop Control Interface"), based on the 2006 episode "Doomsday"; in this game, the player controls troops of Cybermen which must fight Daleks as well as Torchwood Institute members.

On 5 June 2010, the BBC released the first of four official computer games on its website, "Doctor Who: The Adventure Games", which are intended as part of the official TV series adventures. In the first of these, 'The City of the Daleks', the Doctor in his 11th incarnation and Amy Pond must stop the Daleks re-writing time and reviving Skaro, their homeland.

They also appear in the Nintendo DS and Wii games "" and "".

Several Daleks appear in the iOS game "" as rare enemies the player faces, appearing only in the first and final levels.

The Daleks also appear in "Lego Dimensions" where they ally themselves with Lord Vortech and possess the size-altering scale keystone. When Batman, Gandalf, and Wyldstyle encounter them, they assume that they are allies of the Doctor and attack the trio. The main characters continue to fight the Daleks until they call the Doctor to save them. A Dalek saucer also appears in the level based on Metropolis, in which the top of it serves as the stage for the boss battle against Sauron and includes Daleks among the various enemies summoned to attack the player. A Dalek is also among the elements summoned by the player to deal with the obstacles in the "Portal 2" story level.

The Daleks also appear in Doctor Who: The Edge of Time, a Virtual Reality Game for the PlayStation VR, Oculus Rift, Oculus Quest, HTC Vive, and Vive Cosmos, which was released in September 2019.

The Daleks are a licensed costume in Fall Guys.

At the 1966 Conservative Party conference in Blackpool, delegate Hugh Dykes publicly compared the Labour government's Defence Secretary Denis Healey to the creatures. "Mr. Healey is the Dalek of defence, pointing a metal finger at the armed forces and saying 'I will eliminate you'."

In a British Government Parliamentary Debate in the House of Commons on 12 February 1968, the then Minister of Technology Tony Benn mentioned the Daleks during a reply to a question from the Labour MP Hugh Jenkins concerning the Concorde aircraft project. In the context of the dangers of solar flares, he said, "Because we are exploring the frontiers of technology, some people think Concorde will be avoiding solar flares like Dr. Who avoiding Daleks. It is not like this at all."

Australian Labor Party luminary Robert Ray described his right wing Labor Unity faction successor, Victorian Senator Stephen Conroy, and his Socialist Left faction counterpart, Kim Carr, as "factional Daleks" during a 2006 Australian Fabian Society lunch in Sydney.

During a 2021 House of Commons debate about the retention of dentists in rural areas of the United Kingdom during the COVID-19 pandemic, the voice of Conservative MP Scott Mann of North Cornwall, while on a video link, became distorted due to a malfunction with his audio feed. Deputy Speaker of the House Nigel Evans interrupted his broadcast, amidst the chuckles from other MPs; by saying, "Scott, you sound like a Dalek and I don't mean that unkindly. There's clearly a communications problem." Mann later returned to apologise.

Daleks have been used in political cartoons to caricature: Douglas Hurd, as the 'Douglek', in Private Eye's Dan Dire – Pilot of the Future; Tony Benn, John Birt, Tony Blair (also portrayed as Davros), Alec Douglas-Home, Charles de Gaulle, Mark Thompson.

Daleks have appeared on magazine covers promoting "Doctor Who" since the "Dalekmania" fad of the 1960s. "Radio Times" has featured the Daleks on its cover several times, beginning with the 21–27 November 1964 issue which promoted "The Dalek Invasion of Earth". Other magazines also used Daleks to attract readers' attention, including "Girl Illustrated".

In April 2005, "Radio Times" created a special cover to commemorate both the return of the Daleks to the screen in "Dalek" and the forthcoming general election. This cover recreated a scene from "The Dalek Invasion of Earth" in which the Daleks were seen crossing Westminster Bridge, with the Houses of Parliament in the background. The cover text read "VOTE DALEK!" In a 2008 contest sponsored by the Periodical Publishers Association, this cover was voted the best British magazine cover of all time. In 2013 it was voted "Cover of the century" by the Professional Publishers Association. The 2010 United Kingdom general election campaign also prompted a collector's set of three near-identical covers of the "Radio Times" on 17 April with exactly the same headline but with the newly redesigned Daleks in their primary colours representing the three main political parties, Red being Labour, Blue as Conservative and Yellow as Liberal Democrats.

Daleks have been the subject of many parodies, including Spike Milligan's "Pakistani Dalek" sketch in his comedy series "Q", and Victor Lewis-Smith's "Gay Daleks". Occasionally the BBC has used the Daleks to parody other subjects: in 2002, BBC Worldwide published the "Dalek Survival Guide", a parody of "The Worst-Case Scenario Survival Handbooks". Comedian Eddie Izzard has an extended stand-up routine about Daleks, which was included in her 1993 stand-up show "Live at the Ambassadors". The Daleks made two brief appearances in a pantomime version of "Aladdin" at the Birmingham Hippodrome which starred "Torchwood" star John Barrowman in the lead role. A joke-telling robot, possessing a Dalek-like boom, and loosely modelled after the Dalek, also appeared in the "South Park" episode "Funnybot", even spouting out "exterminate". A Dalek can also be seen in the background at timepoints 1:13 and 1:17 in the Sam & Max episode "The Trouble with Gary". In the "Community" parody of "Doctor Who" called "Inspector Spacetime", they are referred to as Blorgons.

The BBC approached Walter Tuckwell, a New Zealand-born entrepreneur who was handling product merchandising for other BBC shows, and asked him to do the same for the Daleks and "Doctor Who". Tuckwell created a glossy sales brochure that sparked off a Dalek craze, dubbed "Dalekmania" by the press, which peaked in 1965.

The first Dalek toys were released in 1965 as part of the "Dalekmania" craze. These included battery-operated, friction drive and "Rolykins" Daleks from Louis Marx & Co., as well as models from Cherilea, Herts Plastic Moulders Ltd and Cowan, de Groot Ltd, and "Bendy" Daleks made by Newfeld Ltd. At the height of the Daleks' popularity, in addition to toy replicas, there were Dalek board games and activity sets, slide projectors for children and even Dalek playsuits made from PVC. Collectible cards, stickers, toy guns, music singles, punching bags and many other items were also produced in this period. Dalek toys released in the 1970s included a new version of Louis Marx's battery-operated Dalek (1974), a "talking Dalek" from Palitoy (1975) and a Dalek board game (1975) and Dalek action figure (1977), both from Denys Fisher. From 1988 to 2002, Dapol released a line of Dalek toys in conjunction with its "Doctor Who" action figure series.

In 1984, Sevans Models released a self-assembly model kit for a one-fifth scale Dalek, which "Doctor Who" historian David Howe has described as "the most accurate model of a Dalek ever to be released". Comet Miniatures released two Dalek self-assembly model kits in the 1990s.

In 1992, Bally released a Doctor Who pinball machine which prominently featured the Daleks both as a primary playfield feature and as a motorised toy in the topper.

Bluebird Toys produced a Dalek-themed "Doctor Who" playset in 1998.

Beginning in 2000, Product Enterprise (who later operated under the names "Iconic Replicas" and "Sixteen 12 Collectibles") produced various Dalek toys. These included Dalek "Rolykins" (based on the Louis Marx toy from 1965); push-along "talking" Daleks; Dalek "Rollamatics" with a pull back and release mechanism; and a remote control Dalek.

In 2005 Character Options was granted the "Master Toy License" for the revived "Doctor Who" series, including the Daleks. Their product lines have included static/push-along and radio controlled Daleks, radio controlled versions and radio controlled / 1:3 scale variants. The 12-inch remote control Dalek won the 2005 award for Best Electronic Toy of the Year from the Toy Retailers Association. Some versions of the 18-inch model included semi-autonomous and voice command-features. In 2008, the company acquired a license to produce Daleks of the various "classic series" variants. For the fifth revived series, both Ironside (Post-Time war Daleks in camouflage khaki), Drone (new, red) and, later, Strategist Daleks (new, blue) were released as both RC Infrared Battle Daleks and action figures.

A pair of Lego based Daleks were included in the Lego Ideas "Doctor Who" set, and another appeared in the "Lego Dimensions" Cyberman Fun-Pack.

Dalek fans have been building life-size reproduction Daleks for many years. The BBC and Terry Nation estate officially disapprove of self-built Daleks, but usually intervene only if attempts are made to trade unlicensed Daleks and Dalek components commercially, or if it is considered that actual or intended use may damage the BBC's reputation or the Doctor Who/Dalek brand. The Crewe, Cheshire-based company "This Planet Earth" is the only business which has been licensed by the BBC and the Terry Nation Estate to produce full-size TV Dalek replicas, and by Canal+ Image UK Ltd. to produce full size Movie Dalek replicas commercially.




Davy Jones (musician)

David Thomas Jones (30 December 1945 – 29 February 2012) was an English actor and singer. Best known as a member of the band The Monkees and a co-star of the TV series "The Monkees" (1966–1968), Jones was considered a teen idol.

Aside from his work on "The Monkees" TV show, Jones's acting credits include a Tony-nominated performance as the Artful Dodger in the original London and Broadway productions of "Oliver!" and a guest-starring role in a hallmark episode of "The Brady Bunch" television show and a later reprised parody film.

David Thomas Jones was born on 30 December 1945 in Openshaw, England, to Harry and Doris Jones. He had three sisters: Hazel, Lynda and Beryl. Jones' mother died from emphysema when he was 14 years of age.

Jones' television acting debut was in the British television soap opera "Coronation Street", in which he appeared as Colin Lomax, grandson of the regular character Ena Sharples, for one episode on 6 March 1961. He also appeared in the BBC police series "Z-Cars". Following the death of his mother, Jones rejected acting in favour of becoming a jockey, commencing an apprenticeship with Newmarket trainer Basil Foster. He dropped out of secondary school to begin working in that field, but this career was short-lived. Even though Foster believed Jones would be successful as a jockey, he encouraged his young protégé to take a role as the Artful Dodger in a production of "Oliver!" in London's West End. When approached by a friend who worked in a West End theatre during the show's casting, Foster replied, "I've got the kid." Jones's portrayal brought him great acclaim. He played the role in London and then on Broadway, and was nominated for a Tony Award.

On 9 February 1964, Jones appeared on "The Ed Sullivan Show" with Georgia Brown, who was playing Nancy in the Broadway production of "Oliver!". This was the same episode of the show in which the Beatles made their first appearance on U.S. television. Jones said of that night, "I watched the Beatles from the side of the stage, I saw the girls going crazy, and I said to myself, this is it, I want a piece of that."

Following his "Ed Sullivan" appearance, Jones signed a contract with Ward Sylvester of Screen Gems (at that time the television division of Columbia Pictures). A pair of U.S. television appearances followed, as Jones received screen time in episodes of "Ben Casey" and "The Farmer's Daughter".

Jones debuted on the Billboard Hot 100 in the week of 14 August 1965, with the single "What Are We Going To Do?", which peaked at number 93. The 19-year-old singer was signed to Colpix Records, a label owned by Columbia. His debut album, "David Jones", on the same label, followed soon afterward (CP493).

From 1966 to 1970, Jones was a member of the Monkees, a pop-rock band formed expressly for a television show of the same name. With Screen Gems producing the series, Jones was shortlisted for auditions, as he was the only Monkee who was signed to a deal with the studio, but still had to meet the standards of producers Bob Rafelson and Bert Schneider. Jones sang lead vocals on many of the Monkees' recordings, including "I Wanna Be Free" and "Daydream Believer". The DVD release of the first season of the show contained commentary from the various bandmates. In Peter Tork's commentary, he stated that Jones was a good drummer and had the live performance line-up been based solely on playing ability, it ought to have been Tork on guitar, Mike Nesmith on bass, and Jones on drums, with Micky Dolenz taking the fronting role, rather than as it was done (with Nesmith on guitar, Tork on bass, and Dolenz on drums). Like Peter Tork, Jones, despite playing mostly tambourine or maracas, was a multi-instrumentalist and would fill in for Tork on bass when he played keyboards and vice versa and for Dolenz on drums when the Monkees performed live concerts.
The Monkees officially disbanded in 1970. The NBC television series "The Monkees" was popular and remained in syndication.

Bell Records, then having a string of hits with "The Partridge Family", signed Jones to a somewhat inflexible solo record contract in 1971. Jones was not allowed to choose his songs or producer, resulting in several lacklustre and aimless records. His second solo album, "Davy Jones" (1971) was notable for the song "Rainy Jane", which reached No. 52 in the "Billboard" charts. To promote the album, Jones performed "Girl" on an episode of "The Brady Bunch" entitled "Getting Davy Jones". Although the single sold poorly, the popularity of Jones' appearance on the show resulted in "Girl" becoming his best-remembered solo hit, even though it was not included in the album. The final single, "I'll Believe In You"/"Road to Love", was poorly received.

Thanks in part to reruns of "The Monkees" on Saturday mornings and in syndication, "The Monkees Greatest Hits" charted in 1976. The LP, issued by Arista (a subsidiary of Screen Gems), was actually a repackaging of a 1972 compilation LP called "Refocus" that had been issued by Arista's previous label imprint, Bell Records, also owned by Screen Gems.

Dolenz and Jones took advantage of this, joining ex-Monkees songwriters Tommy Boyce and Bobby Hart to tour the United States. From 1975 to 1977, as the "Golden Hits of The Monkees" show ("The Guys who Wrote 'Em and the Guys who Sang 'Em!"), they successfully performed in smaller venues such as state fairs and amusement parks as well as making stops in Japan, Thailand, and Singapore (although they were forbidden from using the "Monkees" name, as it was owned by Screen Gems at the time). They also released an album of new material appropriately as "Dolenz, Jones, Boyce & Hart"; a live album entitled "Concert in Japan" was also recorded in 1976, but was not released until 1996.

Despite his initial high-profile after the Monkees disbanded, Jones struggled to establish himself as a solo music artist. Glenn A. Baker, author of "Monkeemania: The True Story of the Monkees", commented in 1986 that "for an artist as versatile and confident as (Davy) Jones, the relative failure of his post-Monkees activities is puzzling. For all his cocky predictions to the press about his future plans, Davy fell into a directionless heap when left to his own devices."

Jones returned to theatre several times after the Monkees disbanded. In 1977, he performed with former bandmate Micky Dolenz in a stage production of the Harry Nilsson musical "The Point!" in London at the Mermaid Theatre, playing and singing the starring role of "Oblio" to Dolenz' roles as the "Count's Kid" and the "Leafman", (according to the CD booklet). An original cast recording was made and released. The comedic chemistry of Jones and Dolenz proved so strong that the show was revived in 1978 with Nilsson inserting additional comedy for the two, plus two more songs, with one of them ("Gotta Get Up") being sung by Jones and Dolenz. The show was considered so good that it was planned to be revived again in 1979 but it proved cost prohibitive (source CD booklet "Harry Nilsson's The Point"). Jones also appeared in several productions of "Oliver!" as the Artful Dodger, and in 1989 toured the US portraying "Fagin".

Jones appeared in two episodes each of "Love, American Style" and "My Two Dads". Jones also appeared in animated form as himself in 1972 in an hour-long episode of "The New Scooby-Doo Movies".

A "Monkees" television show marathon ("Pleasant Valley Sunday") broadcast on 23 February 1986 by MTV resulted in a wave of Monkeemania not seen since the band's heyday. Jones reunited with Dolenz and Peter Tork from 1986 to 1989 to celebrate the band's renewed success and promote the 20th anniversary of the band. A new top 20 hit, "That Was Then, This Is Now" was released (though Jones did not perform on the song) as well as an album, "Pool It!"

In 1996, Jones reunited with Dolenz, Tork and Michael Nesmith to celebrate the 30th anniversary of the Monkees. The band released a new album entitled "Justus", the first album since 1967's "Headquarters" that featured the band members performing all instrumental duties. It was the last time all four Monkees performed together.

Other television appearances include "Sledge Hammer!", "Boy Meets World", "Hey Arnold!", "The Single Guy" (where he is mistaken for Dudley Moore) and "Sabrina, the Teenage Witch" in which he sang "Daydream Believer" to Sabrina Spellman (played by Melissa Joan Hart) as well as (I'll) Love You Forever.
In 1995, Jones acted in a notable episode of the sitcom "Boy Meets World".

The continued popularity of Jones' 1971 "Brady Bunch" appearance led to his being cast as himself in "The Brady Bunch Movie" (1995). Jones sang his signature solo hit "Girl", with a grunge band providing backing, this time with middle-aged women swooning over him. Micky Dolenz and Peter Tork also appeared alongside Jones as judges.

On 2 August 1996, while The Monkees were on their 30th-anniversary tour in New England, Jones was interviewed on the "Sports Break" radio show on WBPS 890-AM in Boston by host Roland Regan about his early days as a jockey and amateur boxer back in England as a youth, and now how he stays in shape by jogging and playing in celebrity tennis tournaments.

On 21 June 1997, during a concert at the Los Angeles Coliseum, Jones joined U2's The Edge onstage for a karaoke performance of "Daydream Believer", which had become a fixture of the band's set during that year's PopMart Tour.

In 2001, Jones released "Just Me", an album of his own songs, some written for the album and others originally on Monkees releases. In the early 2000s he was performing in the Flower Power Concert Series during Epcot's Flower and Garden Festival, a yearly gig he would continue until his death.

In April 2006, Jones recorded the single "Your Personal Penguin", written by children's author Sandra Boynton, as a companion piece to her new board book of the same title.

In 2007, Jones performed the theme song for the film "Sexina: Popstar P.I.". On 1 November 2007, the Boynton book and CD titled "Blue Moo" was released and Jones is featured in both the book and CD, singing "Your Personal Penguin". In 2009, Jones released a collection of classics and standards from the 1940s through the 1970s entitled "She".

In December 2008, "Yahoo! Music" named Jones the "Number 1 teen idol of all time". In 2009, Jones was rated second in a list of 10 best teen idols compiled by Fox News.

In 2009, Jones made a cameo appearance as himself in the "SpongeBob SquarePants" episode "SpongeBob SquarePants vs. The Big One" (his appearance was meant as a pun on the phrase "Davy Jones' Locker").

In February 2011, Jones confirmed rumours of another Monkees reunion. "There's even talk of putting the Monkees back together again in the next year or so for a U.S. and UK tour," he told Disney's Backstage Pass newsletter. "You're always hearing all those great songs on the radio, in commercials, movies, almost everywhere." The tour (Jones' last) came to fruition and was entitled "."

In 1967, Jones opened his first store, called Zilch, at 217 Thompson Street in the Greenwich Village section of New York City. The store sold "hip" clothing and accessories, and also allowed customers to design their own clothes.

After the Monkees disbanded in 1970, Jones kept himself busy by establishing a New York City-style street market in Los Angeles, called "The Street", which cost approximately $40,000. He also collaborated with musical director Doug Trevor on a one-hour ABC television special titled "Pop Goes Davy Jones", which featured new artists The Jackson 5 and the Osmonds.

In addition to his career as an entertainer, Jones' other great love was horses. Having trained as a jockey in his teens in the UK, he had at first intended to pursue a career as a professional race jockey. He held an amateur rider's licence, and rode in his first race at Newbury in Berkshire for renowned trainer Toby Balding.

On 1 February 1996, Jones won his first race, on Digpast, in the one-mile Ontario Amateur Riders Handicap at Lingfield in Surrey. Jones also had horse ownership interests in both the US and the UK, and served as a commercial spokesman for Colonial Downs racetrack in Virginia. Following Jones' death, Lingfield announced that the first two races on the racecard for 3 March 2012 would be renamed the "Hey Hey We're The Monkees Handicap" and the "In Memory of Davy Jones Selling Stakes", with successful horses in those races accompanied into the winners' enclosure by some of the Monkees' biggest hits. Plans were also announced to erect a plaque to commemorate Jones next to a Monkey Puzzle tree on the course.

Jones was married three times and had four children. In December 1967, he married Dixie Linda Haines, with whom he had been living. Their relationship had been kept out of the public eye until after the birth of their first child in October 1968. It caused a considerable backlash for Jones from his fans when it was finally made public. Jones later stated in "Tiger Beat" magazine, "I kept my marriage a secret because I believe stars should be allowed a private life." Jones and Haines had two daughters, Talia Elizabeth Jones (2 October 1968) and Sarah Lee Jones (3 July 1971). The marriage ended in 1975.

Jones married his second wife, Anita Pollinger, on 24 January 1981, and also had two daughters. Jessica Lillian Jones (4 September 1981) and Annabel Charlotte Jones (26 June 1988). The couple divorced in 1996 during the Monkees' 30th-anniversary reunion tour.

Jones married Jessica Pacheco in 2009. Jones and his wife appeared on the "Dr. Phil" show in April 2011. On 28 July 2011, Pacheco filed to divorce Jones in Miami-Dade County, Florida, but dropped the suit in October. They were still married when he died in February 2012. Pacheco was omitted from Jones' will, which he had made before their marriage. His oldest daughter, whom he named his executrix, was granted by the court the unusual request that her father's will be sealed, on the basis that "planning documents and financial affairs as public opinion could have a material effect on his copyrights, royalties and ongoing goodwill"."

On the morning of 29 February 2012, Jones went to tend his 14 horses at a farm in Indiantown, Florida. After riding one of his favourite horses around the track, he complained of chest pains and difficulty breathing and was given antacid pills. He got in his car to go home. Just after 8:00 a.m., a ranch-hand found him unconscious and an ambulance was called but Jones could not be revived. He was taken to Martin Memorial South Hospital in Stuart, Florida, where he died of a heart attack resulting from arteriosclerosis. He was 66.

On 7 March, a private funeral service was held at Holy Cross Catholic parish church in Indiantown. To avoid drawing attention to the grieving family, the three surviving Monkees did not attend. Instead, the bandmates attended memorial services in New York City and organised their own private memorial in Los Angeles along with Jones' family and close friends. A public memorial service was held on 10 March in Beavertown, Pennsylvania, near a church Jones had purchased for future renovation.

On 12 March, a private memorial service was held in Jones' hometown of Openshaw, Manchester, at Lees Street Congregational Church, where Jones performed as a child in church plays. Jones' wife and daughters travelled to England to join his relatives based there for the service, and placed his ashes on his parents' graves for a time.

The news of Jones' death triggered a surge of Internet traffic, causing sales of the Monkees' music to increase dramatically.

Guitarist Michael Nesmith stated that Jones' "spirit and soul live well in my heart, among all the lovely people, who remember with me the good times, and the healing times, that were created for so many, including us. I have fond memories. I wish him safe travels." In an 8 March 2012 interview with "Rolling Stone" magazine, Nesmith commented, "For me, David was the Monkees. They were his band. We were his side men." Bassist Peter Tork said, "Adios to the Manchester Cowboy", and speaking to CNN, drummer/singer Micky Dolenz said, "He was the brother I never had and this leaves a gigantic hole in my heart." Dolenz claimed that he knew that something bad was about to happen and said "Can't believe it.. Still in shock.. had bad dreams all night long." Dolenz was gratified by the public affection expressed for both Jones and the Monkees in the wake of his bandmate's death. "He was a very well-known and well-loved character and person. There are a lot of people who are grieving pretty hard. The Monkees obviously had a following, and so did (Jones) on his own. So I'm not surprised, but I was flattered and honored to be considered one of his friends and a cohort in Monkee business."

"The Monkees" co-creator Bob Rafelson commented that Jones "deserves a lot of credit, let me tell you. He may not have lived as long as we wanted him to, but he survived about seven lifetimes, including being perhaps the biggest rock star of his time."

"Brady Bunch" co-star Maureen McCormick commented that "Davy was a beautiful soul," and that he "spread love and goodness around the world. He filled our lives with happiness, music, and joy. He will live on in our hearts forever. May he rest in peace."

Yahoo! Music commented that Jones' death "hit so many people so hard" because "Monkees nostalgia cuts across generations: from the people who discovered the band during their original 1960s run; to the kids who came of age watching 1970s reruns; to the 20- and 30-somethings who discovered the Monkees when MTV (a network that owes much to the Monkees' influence) began airing old episodes in 1986."

"Time" contributor James Poniewozik praised the Monkees' classic sitcom, and Jones in particular, saying, "even if the show never meant to be more than entertainment and a hit-single generator, we shouldn't sell "The Monkees" short. It was far better television than it had to be; during an era of formulaic domestic sitcoms and wacky comedies, it was a stylistically ambitious show, with a distinctive visual style, absurdist sense of humor and unusual story structure. Whatever Jones and the Monkees were meant to be, they became creative artists in their own right, and Jones' chipper Brit-pop presence was a big reason they were able to produce work that was commercial, wholesome, and yet impressively weird."

"Mediaite" columnist Paul Levinson noted, "The Monkees were the first example of something created in a medium – in this case, a rock band on television – that jumped off the screen to have big impact in the real world."




Discharge

Discharge may refer to






Dolly (sheep)

Dolly (5 July 1996 – 14 February 2003) was a female Finn-Dorset sheep and the first mammal that was cloned from an adult somatic cell. She was cloned by associates of the Roslin Institute in Scotland, using the process of nuclear transfer from a cell taken from a mammary gland. Her cloning proved that a cloned organism could be produced from a mature cell from a specific body part. Contrary to popular belief, she was not the first animal to be cloned.

The employment of adult somatic cells in lieu of embryonic stem cells for cloning emerged from the foundational work of John Gurdon, who cloned African clawed frogs in 1958 with this approach. The successful cloning of Dolly led to widespread advancements within stem cell research, including the discovery of induced pluripotent stem cells.

Dolly lived at the Roslin Institute throughout her life and produced several lambs. She was euthanized at the age of six years due to a progressive lung disease. No cause which linked the disease to her cloning was found.

Dolly's body was preserved and donated by the Roslin Institute in Scotland to the National Museum of Scotland, where it has been regularly exhibited since 2003.

Dolly was cloned by Keith Campbell, Ian Wilmut and colleagues at the Roslin Institute, part of the University of Edinburgh, Scotland, and the biotechnology company PPL Therapeutics, based near Edinburgh. The funding for Dolly's cloning was provided by PPL Therapeutics and the Ministry of Agriculture. She was born on 5 July 1996 and died on 14 February 2003 from a progressive lung disease that was considered unrelated to her being a clone. She has been called "the world's most famous sheep" by sources including BBC News and "Scientific American".

The cell used as the donor for the cloning of Dolly was taken from a mammary gland, and the production of a healthy clone, therefore, proved that a cell taken from a specific part of the body could recreate a whole individual. On Dolly's name, Wilmut stated "Dolly is derived from a mammary gland cell and we couldn't think of a more impressive pair of glands than Dolly Parton's."

Dolly was born on 5 July 1996 and had three mothers: one provided the egg, another the DNA, and a third carried the cloned embryo to term. She was created using the technique of somatic cell nuclear transfer, where the cell nucleus from an adult cell is transferred into an unfertilized oocyte (developing egg cell) that has had its cell nucleus removed. The hybrid cell is then stimulated to divide by an electric shock, and when it develops into a blastocyst it is implanted in a surrogate mother. Dolly was the first clone produced from a cell taken from an adult mammal. The production of Dolly showed that genes in the nucleus of such a mature differentiated somatic cell are still capable of reverting to an embryonic totipotent state, creating a cell that can then go on to develop into any part of an animal.

Dolly's existence was announced to the public on 22 February 1997. It gained much attention in the media. A commercial with Scottish scientists playing with sheep was aired on TV, and a special report in "Time" magazine featured Dolly. "Science" featured Dolly as the breakthrough of the year. Even though Dolly was not the first animal cloned, she received media attention because she was the first cloned from an adult cell.

Dolly lived her entire life at the Roslin Institute in Midlothian. There she was bred with a Welsh Mountain ram and produced six lambs in total. Her first lamb, named Bonnie, was born in April 1998. The next year Dolly produced twin lambs Sally and Rosie, and she gave birth to triplets Lucy, Darcy and Cotton in 2000. In late 2001, at the age of four, Dolly developed arthritis and began to walk stiffly. This was treated with anti-inflammatory drugs.

On 14 February 2003, Dolly was euthanised because she had a progressive lung disease and severe arthritis. A Finn Dorset such as Dolly has a life expectancy of around 11 to 12 years, but Dolly lived 6.5 years. A post-mortem examination showed she had a form of lung cancer called ovine pulmonary adenocarcinoma, also known as Jaagsiekte, which is a fairly common disease of sheep and is caused by the retrovirus JSRV. Roslin scientists stated that they did not think there was a connection with Dolly being a clone, and that other sheep in the same flock had died of the same disease. Such lung diseases are a particular danger for sheep kept indoors, and Dolly had to sleep inside for security reasons.

Some in the press speculated that a contributing factor to Dolly's death was that she could have been born with a genetic age of six years, the same age as the sheep from which she was cloned. One basis for this idea was the finding that Dolly's telomeres were short, which is typically a result of the aging process. The Roslin Institute stated that intensive health screening did not reveal any abnormalities in Dolly that could have come from advanced aging.

In 2016, scientists reported no defects in thirteen cloned sheep, including four from the same cell line as Dolly. The first study to review the long-term health outcomes of cloning, the authors found no evidence of late-onset, non-communicable diseases other than some minor examples of osteoarthritis and concluded "We could find no evidence, therefore, of a detrimental long-term effect of cloning by SCNT on the health of aged offspring among our cohort."

After her death Dolly's body was preserved via taxidermy and is currently on display at the National Museum of Scotland in Edinburgh.

After cloning was successfully demonstrated through the production of Dolly, many other large mammals were cloned, including pigs, deer, horses and bulls. The attempt to clone argali (mountain sheep) did not produce viable embryos. The attempt to clone a banteng bull was more successful, as were the attempts to clone mouflon (a form of wild sheep), both resulting in viable offspring. The reprogramming process that cells need to go through during cloning is not perfect and embryos produced by nuclear transfer often show abnormal development. Making cloned mammals was highly inefficientin 1996 Dolly was the only lamb that survived to adulthood from 277 attempts. By 2014 Chinese scientists were reported to have 70–80% success rates cloning pigs, and in 2016, a Korean company, Sooam Biotech, was producing 500 cloned embryos a day. Wilmut, who led the team that created Dolly, announced in 2007 that the nuclear transfer technique may never be sufficiently efficient for use in humans.

Cloning may have uses in preserving endangered species, and may become a viable tool for reviving extinct species. In January 2009, scientists from the Centre of Food Technology and Research of Aragon in northern Spain announced the cloning of the Pyrenean ibex, a form of wild mountain goat, which was officially declared extinct in 2000. Although the newborn ibex died shortly after birth due to physical defects in its lungs, it is the first time an extinct animal has been cloned, and may open doors for saving endangered and newly extinct species by resurrecting them from frozen tissue.

In July 2016, four identical clones of Dolly (Daisy, Debbie, Dianna, and Denise) were alive and healthy at nine years old.

"Scientific American" concluded in 2016 that the main legacy of Dolly has not been cloning of animals but in advances into stem cell research. After Dolly, researchers realised that ordinary cells could be reprogrammed to induced pluripotent stem cells, which can be grown into any tissue.

The first successful cloning of a primate species was reported in January 2018, using the same method which produced Dolly. Two identical clones of a macaque monkey, Zhong Zhong and Hua Hua, were created by researchers in China and were born in late 2017.

In January 2019, scientists in China reported the creation of five identical cloned gene-edited monkeys, again using this method, and the gene-editing CRISPR-Cas9 technique allegedly used by He Jiankui in creating the first ever gene-modified human babies Lulu and Nana. The monkey clones were made in order to study several medical diseases.



Dolores Fuller

Dolores Agnes Fuller ( Eble, later Chamberlin; March 10, 1923 – May 9, 2011) was an American actress and songwriter known as the one-time girlfriend of the low-budget film director Ed Wood. She played the protagonist's girlfriend in "Glen or Glenda", co-starred in Wood's "Jail Bait", and had a minor role in his "Bride of the Monster". After she broke up with Wood in 1955, she relocated to New York and had a very successful career there as a songwriter. Elvis Presley recorded a number of her songs written for his films.

Her first screen appearance was at the age of 10, when she appeared briefly in Frank Capra's "It Happened One Night". According to Fuller, the female lead in "Bride of the Monster" was written for her but Wood gave it to Loretta King instead.

In August 1954, Fuller was cast in Wood's "The Vampire's Tomb", intended to star Bela Lugosi. Frank Yaconelli was named as her co-star and 'comic killer'. The film was never made. She ended up making an appearance in "Bride of the Monster" (1956), also with Lugosi. Fuller hosted a benefit for Lugosi which preceded the showing of "Bride of the Atom" (early working title of "Bride of the Monster") on May 11, 1955. A cocktail party was held at the Gardens Restaurant at 4311 Magnolia Avenue in Burbank, California. Vampira attended and was escorted by Paul Marco. A single screening of the film was presented at the Hollywood Paramount.

According to Fuller, as quoted in Wood biography "Nightmare of Ecstasy" (1992), she first met Ed Wood when she attended a casting call with a friend for a movie he was supposed to direct called "Behind Locked Doors" (which he did not go on to direct); it has also been stated that they met in a restaurant.

She became his girlfriend shortly thereafter and began acting in his films. Her movie career included a bit part in "It Happened One Night" (1934) and roles in "Outlaw Women" (1952), "Glen or Glenda" (1953), "Body Beautiful" (1953), "The Blue Gardenia" (1953), "Count the Hours" (1953), "Mesa of Lost Women" (1953), "College Capers" (1954), "Jail Bait" (1954), "The Raid" (1954), "This Is My Love" (1954), "The Opposite Sex" (1956), and many years later appearances in "The Ironbound Vampire" (1997) and "Dimensions in Fear" (1998).

Fuller had already had earlier experience on television in "Queen for a Day" and "The Dinah Shore Show".

She also appeared on an episode of "It's a Great Life" as "the blonde in the mink coat."

Fuller's ability as a songwriter manifested itself through the intervention of her friend, producer Hal Wallis; Fuller had wanted to get an acting role in the Elvis Presley movie "Blue Hawaii", which Wallis was producing, but instead he put her in touch with Hill & Range, the publisher that provided Presley with songs. Fuller went into a collaborative partnership with composer Ben Weisman and co-wrote one song, "Rock-A-Hula Baby", for the film. Over time, this led to Presley recording a dozen of her songs, including "I Got Lucky" and "Spinout", primarily for his film soundtracks, though he also recorded "Cindy, Cindy" for his 1971 album "Love Letters From Elvis". Fuller's music was also recorded by Nat 'King' Cole, Peggy Lee, and other leading talents of the time. Toward the end of her life, Dolores helped edit and score a short western film Ed Wood had begun, but never completed, in the 1940s called "Crossroads of Laredo"

Dolores married Donald Fuller in 1941, with whom she had two children. At the time she met Ed Wood, she was in the process of divorcing her husband (they finally divorced in 1955). She and Wood shared an apartment together for several years. Wood biographer Rudolph Grey quotes Fuller as saying of the period before her success, He [Ed Wood] begged me to marry him. I loved him in a way, but I couldn't handle the transvestism. I'm a very normal person. It's hard for me to deviate! I wanted a man that was all man. After we broke up, he would stand outside my home in Burbank and cry. "Let me in, I love you!" What good would I have done if I had married him? We would have starved together. I bettered myself. I had to uplift myself. She has also been quoted as saying that "His dressing up didn't bother me—we all have our little queer habits" and giving Wood's drinking as the reason for their breakup.

Dolores remarried in 1988 at age 65, to Philip Chamberlin, and they remained married until her death in 2011. Fuller's autobiography, "A Fuller Life: Hollywood, Ed Wood and Me", co-authored by Winnipeg writer Stone Wallace and her husband Philip Chamberlin, was published in 2008.

Fuller was portrayed by Sarah Jessica Parker in Tim Burton's 1994 Wood biographical film "Ed Wood", a portrayal of which she disapproved due to the fact that she was depicted smoking in the film, while Fuller said she herself was a lifelong non-smoker. She also complained that she was only portrayed as "sort of as an actress" and did not feel she was given credit for her other accomplishments and contributions towards Wood's career. However, she stated that she liked the film overall, praising Johnny Depp's performance in the title role.

Songs recorded by Elvis Presley with lyrics by Dolores Fuller:

According to AllMusic, other songs co-written by her include "I'll Touch a Star" by Terry Stafford, "Lost Summer Love" by Shelley Fabares and "Someone to Tell It To" by Nat King Cole.

De jure

In law and government, de jure (, ; ) describes practices that are legally recognized, regardless of whether the practice exists in reality. In contrast, ('in fact') describes situations that exist in reality, even if not formally recognized.

Between 1805 and 1914, the ruling dynasty of Egypt were subject to the rulers of the Ottoman Empire, but acted as de facto independent rulers who maintained a polite fiction of Ottoman suzerainty. However, starting from around 1882, the rulers had only de jure rule over Egypt, as it had by then become a British puppet state. Thus, by Ottoman law, Egypt was de jure a province of the Ottoman Empire, but de facto was part of the British Empire.

In U.S. law, particularly after "Brown v. Board of Education" (1954), the difference between de facto segregation (segregation that existed because of the voluntary associations and neighborhoods) and de jure segregation (segregation that existed because of local laws that mandated the segregation) became important distinctions for court-mandated remedial purposes.


Des Moines, Iowa

Des Moines () is the capital and most populous city in the U.S. state of Iowa. It is the county seat of Polk County with parts extending into Warren County. It was incorporated on September 22, 1851, as Fort Des Moines, which was shortened to "Des Moines" in 1857. It is located on, and named after, the Des Moines River, which likely was adapted from the early French name, "Rivière des Moines," meaning "River of the Monks". The city's population was 214,133 as of the 2020 census. The six-county metropolitan area is ranked 81st in terms of population in the United States, with 709,466 residents according to the 2020 census by the United States Census Bureau, and is the largest metropolitan area fully located within the state.

Des Moines is a major center of the US insurance industry and has a sizable financial-services and publishing business base. The city was credited as the "number one spot for U.S. insurance companies" in a "Business Wire" article and named the third-largest "insurance capital" of the world. The city is the headquarters for the Principal Financial Group, Ruan Transportation, TMC Transportation, EMC Insurance Companies, and Wellmark Blue Cross Blue Shield. Other major corporations such as Wells Fargo, Cognizant, Voya Financial, Nationwide Mutual Insurance Company, ACE Limited, Marsh, Monsanto, and Corteva have large operations in or near the metropolitan area. In recent years, Microsoft, Hewlett-Packard, and Facebook have built data-processing and logistical facilities in the Des Moines area.

Des Moines is an important city in U.S. presidential politics; as the state's capital, it is the site of the first caucuses of the presidential primary cycle. Many presidential candidates set up campaign headquarters in Des Moines. A 2007 article in "The New York Times" said, "If you have any desire to witness presidential candidates in the most close-up and intimate of settings, there is arguably no better place to go than Des Moines."

Des Moines takes its name from Fort Des Moines (1843–46), which was named for the Des Moines River. This was adopted from the name given by French colonists. "Des Moines" (; formerly ) translates literally to either "from the monks" or "of the monks".

One popular interpretation of "Des Moines" concludes that it refers to a group of French Trappist monks, who in the 17th century lived in huts built on top of what is now known as the ancient Monks Mound at Cahokia, the major center of Mississippian culture, which developed in what is present-day Illinois, east of the Mississippi River and the city of St. Louis. This was some from the Des Moines River.

Based on archaeological evidence, the junction of the Des Moines and Raccoon Rivers has attracted humans for at least 7,000 years. Several prehistoric occupation areas have been identified by archaeologists in downtown Des Moines. Discovered in December 2010, the "Palace" is an expansive, 7,000-year-old site found during excavations prior to construction of the new wastewater treatment plant in southeastern Des Moines. It contains well-preserved house deposits and numerous graves. More than 6,000 artifacts were found at this site. State of Iowa archaeologist John Doershuk was assisted by University of Iowa archaeologists at this dig.

At least three Late Prehistoric villages, dating from about AD 1300 to 1700, stood in or near what developed later as downtown Des Moines. In addition, 15 to 18 prehistoric American Indian mounds were observed in this area by early settlers. All have been destroyed during development of the city.

Des Moines traces its origins to May 1843, when Captain James Allen supervised the construction of a fort on the site where the Des Moines and Raccoon Rivers merge. Allen wanted to use the name Fort Raccoon; however, the U.S. War Department preferred Fort Des Moines. The fort was built to control the Sauk and Meskwaki tribes, whom the government had moved to the area from their traditional lands in eastern Iowa. The fort was abandoned in 1846 after the Sauk and Meskwaki were removed from the state and shifted to the Indian Territory.

The Sauk and Meskwaki did not fare well in Des Moines. The illegal whiskey trade, combined with the destruction of traditional lifeways, led to severe problems for their society. One newspaper reported:
"It is a fact that the location of Fort Des Moines among the Sac and Fox Indians (under its present commander) for the last two years, had corrupted them more and lowered them deeper in the scale of vice and degradation, than all their intercourse with the whites for the ten years previous".After official removal, the Meskwaki continued to return to Des Moines until around 1857.

Archaeological excavations have shown that many fort-related features survived under what is now Martin Luther King Jr. Parkway and First Street. Soldiers stationed at Fort Des Moines opened the first coal mines in the area, mining coal from the riverbank for the fort's blacksmith.

Settlers occupied the abandoned fort and nearby areas. On May 25, 1846, the state legislature designated Fort Des Moines as the seat of Polk County. Arozina Perkins, a school teacher who spent the winter of 1850–1851 in the town of Fort Des Moines, was not favorably impressed:

This is one of the strangest looking "cities" I ever saw... This town is at the juncture of the Des Moines and Raccoon Rivers. It is mostly a level prairie with a few swells or hills around it. We have a court house of "brick" and one church, a plain, framed building belonging to the Methodists. There are two taverns here, one of which has a most important little bell that rings together some fifty boarders. I cannot tell you how many dwellings there are, for I have not counted them; some are of logs, some of brick, some framed, and some are the remains of the old dragoon houses... The people support two papers and there are several dry goods shops. I have been into but four of them... Society is as varied as the buildings are. There are people from nearly every state, and Dutch, Swedes, etc.

In May 1851, much of the town was destroyed during the Flood of 1851. "The Des Moines and Raccoon Rivers rose to an unprecedented height, inundating the entire country east of the Des Moines River. Crops were utterly destroyed, houses and fences swept away." The city started to rebuild from scratch.

On September 22, 1851, Des Moines was incorporated as a city; the charter was approved by voters on October 18. In 1857, the name "Fort Des Moines" was shortened to "Des Moines", and it was designated as the second state capital, previously at Iowa City. Growth was slow during the Civil War period, but the city exploded in size and importance after a railroad link was completed in 1866.

In 1864, the Des Moines Coal Company was organized to begin the first systematic mining in the region. Its first mine, north of town on the river's west side, was exhausted by 1873. The Black Diamond mine, near the south end of the West Seventh Street Bridge, sank a mine shaft to reach a coal bed. By 1876, this mine employed 150 men and shipped 20 carloads of coal per day. By 1885, numerous mine shafts were within the city limits, and mining began to spread into the surrounding countryside. By 1893, 23 mines were in the region. By 1908, Des Moines' coal resources were largely exhausted. In 1912, Des Moines still had eight locals of the United Mine Workers union, representing 1,410 miners. This was about 1.7% of the city's population in 1910.

By 1880, Des Moines had a population of 22,408, making it Iowa's largest city. It displaced the three Mississippi River ports: Burlington, Dubuque, and Davenport, that had alternated holding the position since the territorial period. Des Moines has remained Iowa's most populous city. In 1910, the Census Bureau reported Des Moines' population as 97.3% white and 2.7% black, reflecting its early settlement pattern primarily by ethnic Europeans.

At the turn of the 20th century, encouraged by the Civic Committee of the Des Moines Women's Club, Des Moines undertook a "City Beautiful" project in which large Beaux Arts public buildings and fountains were constructed along the Des Moines River. The former Des Moines Public Library building (now the home of the World Food Prize); the United States central Post Office, built by the federal government (now the Polk County Administrative Building, with a newer addition); and the City Hall are surviving examples of the 1900–1910 buildings. They form the Civic Center Historic District.

The ornate riverfront balustrades that line the Des Moines and Raccoon Rivers were built by the federal Civilian Conservation Corps in the mid-1930s, during the Great Depression under Democratic President Franklin D. Roosevelt, as a project to provide local employment and improve infrastructure. The ornamental fountains that stood along the riverbank were buried in the 1950s when the city began a postindustrial decline that lasted until the late 1980s. The city has since rebounded, transforming from a blue-collar industrial city to a white-collar professional city.
In 1907, the city adopted a city commission government known as the Des Moines Plan, comprising an elected mayor and four commissioners, all elected at-large, who were responsible for public works, public property, public safety, and finance. Considered progressive at the time, it diluted the votes of ethnic and national minorities, who generally could not command the majority to elect a candidate of their choice.

That form of government was scrapped in 1950 in favor of a council-manager government, with the council members elected at-large. In 1967, the city changed its government to elect four of the seven city council members from single-member districts or wards, rather than at-large. This enabled a broader representation of voters. As with many major urban areas, the city core began losing population to the suburbs in the 1960s (the peak population of 208,982 was recorded in 1960), as highway construction led to new residential construction outside the city. The population was 198,682 in 2000 and grew slightly to 200,538 in 2009. The growth of the outlying suburbs has continued, and the overall metropolitan-area population is over 700,000 today.

During the Great Flood of 1993, heavy rains throughout June and early July caused the Des Moines and Raccoon Rivers to rise above flood stage levels. The Des Moines Water Works was submerged by floodwaters during the early morning hours of July 11, 1993, leaving an estimated 250,000 people without running water for 12 days and without drinking water for 20 days. Des Moines suffered major flooding again in June 2008 with a major levee breach. The Des Moines river is controlled upstream by Saylorville Reservoir. In both 1993 and 2008, the flooding river overtopped the reservoir spillway.

Today, Des Moines is a member of ICLEI Local Governments for Sustainability USA. Through ICLEI, Des Moines has implemented "The Tomorrow Plan", a regional plan focused on developing central Iowa in a sustainable fashion, centrally-planned growth, and resource consumption to manage the local population.

According to the United States Census Bureau, the city has an area of , of which is land and is covered by water. It is above sea level at the confluence of the Raccoon and Des Moines Rivers.

In November 2005, Des Moines voters approved a measure that allowed the city to annex parcels of land in the northeast, southeast, and southern corners of Des Moines without agreement by local residents, particularly areas bordering the Iowa Highway 5/U.S. 65 bypass. The annexations became official on June 26, 2009, as and around 868 new residents were added to the city of Des Moines. An additional were voluntarily annexed to the city over that same period.

The skyline of Des Moines changed in the 1970s and the 1980s, when several new skyscrapers were built. Additional skyscrapers were built in the 1990s, including Iowa's tallest. Before then, the 19-story Equitable Building, from 1924, was the tallest building in the city and the tallest building in Iowa. The 25-story Financial Center was completed in 1973 and the 36-story Ruan Center was completed in 1974. They were later joined by the 33-story Des Moines Marriott Hotel (1981), the 25-story HUB Tower and 25-story Plaza Building (1985). Iowa's tallest building, Principal Financial Group's 45-story tower at 801 Grand was built in 1991, and the 19-story EMC Insurance Building was erected in 1997.

During this time period, the Civic Center of Greater Des Moines (1979) was developed; it hosts Broadway shows and special events. Also constructed were the Greater Des Moines Botanical Garden (1979), a large city botanical garden/greenhouse on the east side of the river; the Polk County Convention Complex (1985), and the State of Iowa Historical Museum (1987). The Des Moines skywalk also began to take shape during the 1980s. The skywalk system is long and connects many downtown buildings.

In the early 21st century, the city has had more major construction in the downtown area. The new Science Center of Iowa and Blank IMAX Dome Theater and the Iowa Events Center opened in 2005. The new central branch of the Des Moines Public Library, designed by renowned architect David Chipperfield of London, opened on April 8, 2006.

The World Food Prize Foundation, which is based in Des Moines, completed adaptation and restoration of the former Des Moines Public Library building in October 2011. The former library now serves as the home and headquarters of the Norman Borlaug/World Food Prize Hall of Laureates.

At the center of North America and far removed from large bodies of water, the Des Moines area has a hot summer type humid continental climate (Köppen "Dfa"), with warm to hot, humid summers and cold, dry winters. Summer temperatures can often climb into the range, occasionally reaching . Humidity can be high in spring and summer, with frequent afternoon thunderstorms. Fall brings pleasant temperatures and colorful fall foliage. Winters vary from moderately cold to bitterly cold, with low temperatures venturing below quite often. Snowfall averages per season, and annual precipitation averages , with a peak in the warmer months. Winters are slightly colder than Chicago, but still warmer than Minneapolis, with summer temperatures being very similar between the Upper Midwest metropolitan areas.

The city has the largest African American population in Iowa.

The 2020 United States census counted 214,133 people, 87,958 households, and 48,599 families in Des Moines. The population density was 2,428.4 per square mile (937.6/km). There were 95,082 housing units at an average density of 1,078.3 per square mile (416.3/km). The racial makeup was 64.54% (138,200) white or European American (60.99% non-Hispanic white), 11.68% (25,011) black or African-American, 0.69% (1,474) Native American or Alaska Native, 6.76% (14,474) Asian, 0.06% (135) Pacific Islander or Native Hawaiian, 6.62% (14,178) from other races, and 9.65% (20,661) from two or more races. Hispanic or Latino of any race was 15.64% (33,480) of the population.

The 2020 census population of the city included 252 people incarcerated in adult correctional facilities and 2,378 people in student housing.

Of the 87,958 households, 28.0% had children under the age of 18; 35.5% were married couples living together; 31.3% had a female householder with no spouse or partner present. 35.3% of households consisted of individuals and 11.0% had someone living alone who was 65 years of age or older. The average household size was 2.5 and the average family size was 3.3. The percent of those with a bachelor's degree or higher was estimated to be 19.9% of the population. Of the population age 25 and over, 86.7% were high school graduates or higher and 27.9% had a bachelor's degree or higher.

23.5% of the population was under the age of 18, 10.4% from 18 to 24, 29.6% from 25 to 44, 23.1% from 45 to 64, and 13.5% who were 65 years of age or older. The median age was 34.8 years. For every 100 females, there were 102.7 males. For every 100 females ages 18 and older, there were 104.4 males.

The 2016-2020 5-year American Community Survey estimates show that the median household income was $54,843 (with a margin of error of +/- $1,544) and the median family income was $66,420 (+/- $1,919). Males had a median income of $38,326 (+/- $1,405) versus $29,855 (+/- $1,327) for females. The median income for those above 16 years old was $33,699 (+/- $740). Approximately, 12.1% of families and 16.0% of the population were below the poverty line, including 24.3% of those under the age of 18 and 9.8% of those ages 65 or over.

As of the census of 2010, there were 203,433 people, 81,369 households, and 47,491 families residing in the city. Population density was . There were 88,729 housing units at an average density of . The racial makeup of the city for Unincorporated areas not merged with the city proper was 66.2% White, 15.5% African Americans, 0.5% Native American, 4.0% Asian, and 2.6% from Two or more races. People of Hispanic or Latino origin, of any race, made up 12.1% of the population. The city's racial make up during the 2010 census was 76.4% White, 10.2% African American, 0.5% Native American, 4.4% Asian (1.2% Vietnamese, 0.9% Laotian, 0.4% Burmese, 0.3% Asian Indian, 0.3% Thai, 0.2% Chinese, 0.2% Cambodian, 0.2% Filipino, 0.1% Hmong, 0.1% Korean, 0.1% Nepalese), 0.1% Pacific Islander, 5.0% from other races, and 3.4% from two or more races. People of Hispanic or Latino origin, of any race, formed 12.0% of the population (9.4% Mexican, 0.7% Salvadoran, 0.3% Guatemalan, 0.3% Puerto Rican, 0.1% Honduran, 0.1% Ecuadorian, 0.1% Cuban, 0.1% Spaniard, 0.1% Spanish). Non-Hispanic Whites were 70.5% of the population in 2010. Des Moines also has a sizeable South Sudanese community.

There were 81,369 households, of which 31.6% had children under the age of 18 living with them, 38.9% were married couples living together, 14.2% had a female householder with no husband present, 5.3% had a male householder with no wife present, and 41.6% were non-families. 32.5% of all households were made up of individuals, and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.11.

The median age in the city was 33.5 years. 24.8% of residents were under the age of 18; 10.9% were between the ages of 18 and 24; 29.4% were from 25 to 44; 23.9% were from 45 to 64; and 11% were 65 years of age or older. The gender makeup of the city was 48.9% male and 51.1% female.

As of the 2000 census, there were 198,682 people, 80,504 households, and 48,704 families in the city. The population density was . There were 85,067 housing units at an average density of . The racial makeup of the city was 82.3% white, 8.07% Black, 0.35% American Indian, 3.50% Asian, 0.05% Pacific Islander, 3.52% from other races, and 2.23% from two or more races. 6.61% of the population were Hispanic or Latino of any race. 20.9% were of German, 10.3% Irish, 9.1% "American" and 8.0% English ancestry, according to Census 2000.

There were 80,504 households, out of which 29.5% had children under the age of 18 living with them, 43.7% were married couples living together, 12.6% had a female householder with no husband present, and 39.5% were non-families. 31.9% of all households were made up of individuals, and 10.2% had someone living alone who was 65 years of age or older. The average household size was 2.39 and the average family size was 3.04.

The age distribution was 24.8% under the age of 18, 10.6% from 18 to 24, 31.8% from 25 to 44, 20.4% from 45 to 64, and 12.4% who were 65 years of age or older. The median age was 34 years. For every 100 females, there were 93.8 males. For every 100 females age 18 and over, there were 90.5 males.

The median income for a household in the city was $38,408, and the median income for a family was $46,590. Males had a median income of $31,712 versus $25,832 for females. The per capita income for the city was $19,467. About 7.9% of families and 11.4% of the population were below the poverty line, including 14.9% of those under age 18 and 7.6% of those ages 65 or over.

Many insurance companies are headquartered in Des Moines, including the Principal Financial Group, EMC Insurance Group, Fidelity & Guaranty Life, Allied Insurance, GuideOne Insurance, Wellmark Blue Cross Blue Shield of Iowa, FBL Financial Group, and American Republic Insurance Company. Iowa has one of the lowest insurance premium taxes in the nation at 1%, and does not charge any premium taxes on qualified life insurance plans, making the state attractive to insurance business. Des Moines has been referred to as the "Hartford of the West" and "Insurance Capital" because of this. Principal is one of two Fortune 500 companies with headquarters in Iowa (the other being Casey's General Stores), ranking 201st on the magazine's list in 2020.

As a center of financial and insurance services, other major corporations headquartered outside of Iowa have a presence in the Des Moines Metro area, including Wells Fargo, Voya Financial, and Electronic Data Systems (EDS). The Meredith Corporation, a leading publishing and marketing company, was also based in Des Moines prior to its acquisition by IAC and merger with Dotdash in 2021. Meredith published "Better Homes and Gardens", one of the most widely circulated publications in the United States. Des Moines was also the headquarters of "Golf Digest" magazine.

Other major employers in Des Moines include UnityPoint Health, Mercy Medical Center, MidAmerican Energy Company, CDS Global, UPS, Firestone, Lumen Technologies, Drake University, Titan Tire, "The Des Moines Register", Anderson Erickson, Dee Zee and EMCO.

In 2017, Kemin Industries opened a state-of-the-art worldwide headquarters building in Des Moines.

The City of Des Moines is a cultural center for Iowa and home to several art and history museums and performing arts groups. The Des Moines Performing Arts routinely hosts touring Broadway shows and other live professional theater. Its president and CEO, Jeff Chelsvig, is a member of the League of American Theatres and Producers, Inc. The Temple for Performing Arts and Des Moines Playhouse are other venues for live theater, comedy, and performance arts.

The Des Moines Metro Opera has been a cultural resource in Des Moines since 1973. The Opera offers educational and outreach programs and is one of the largest performing arts organizations in the state. Ballet Des Moines was established in 2002. Performing three productions each year, the Ballet also provides opportunities for education and outreach.

The Des Moines Symphony performs frequently at different venues. In addition to performing seven pairs of classical concerts each season, the Symphony also entertains with New Year's Eve Pops and its annual Yankee Doodle Pops concerts.

"Jazz in July" is an annual event founded in 1969 that performs free jazz shows daily at venues throughout the city during July.
Wells Fargo Arena is the Des Moines area's primary venue for sporting events and concerts since its opening in 2005. Named for title sponsor Wells Fargo Financial Services, Wells Fargo Arena holds 16,980 and books large, national touring acts for arena concert performances, while several smaller venues host local, regional, and national bands. It is the home of the Iowa Wolves of the NBA G League, the Iowa Wild of the American Hockey League, and the Iowa Barnstormers of the Indoor Football League.

The Simon Estes Riverfront Amphitheater is an outdoor concert venue on the east bank of the Des Moines River which hosts music events such as the Alive Concert Series.
The Des Moines Art Center, with a wing designed by architect I. M. Pei, presents art exhibitions and educational programs as well as studio art classes. The Center houses a collection of artwork from the 19th century to the present. An extension of the art center is downtown in an urban museum space, featuring three or four exhibitions each year.
The Pappajohn Sculpture Park was established in 2009. It showcases a collection of 24 sculptures donated by Des Moines philanthropists John and Mary Pappajohn. Nearby is the Temple for Performing Arts, a cultural center for the city. Next to the Temple is the Central Library, designed by renowned English architect David Chipperfield.

Salisbury House and Gardens is a 42-room historic house museum on of woodlands in the South of Grand neighborhood of Des Moines. It is named after—and loosely inspired by—King's House in Salisbury, England. Built in the 1920s by cosmetics magnate Carl Weeks and his wife, Edith, the Salisbury House contains authentic 16th-century English oak and rafters dating to Shakespeare's days, numerous other architectural features re-purposed from other historic English homes, and an internationally significant collection of original fine art, tapestries, decorative art, furniture, musical instruments, and rare books and documents. The Salisbury House is listed on the National Register of Historic Places, and has been featured on A&E's "America's Castles" and PBS's "Antiques Roadshow". Prominent artists in the Salisbury House collection include Joseph Stella, Lillian Genth, Anthony van Dyck and Lawrence Alma-Tadema.

Built in 1877 by prominent pioneer businessman Hoyt Sherman, Hoyt Sherman Place mansion was Des Moines' first public art gallery and houses a distinctive collection of 19th and 20th century artwork. Its restored 1,250-seat theater features an intricate rococo plaster ceiling and excellent acoustics and is used for a variety of cultural performances and entertainment.

Arising in the east and facing westward toward downtown, the Iowa State Capitol building with its , 23-karat gold leafed dome towering above the city is a favorite of sightseers. Four smaller domes flank the main dome. The Capitol houses the governor's offices, legislature, and the old Supreme Court Chambers. The ornate interior also features a grand staircase, mural "Westward", five-story law library, scale model of the USS "Iowa", and collection of first lady dolls. Guided tours are available.

The Capitol grounds include a World War II memorial with sculpture and Wall of Memories, the 1894 Soldiers and Sailors Monument of the Civil War and memorials honoring those who served in the Spanish–American, Korean, and Vietnam Wars.
The West Capitol Terrace provides the entrance from the west to the state's grandest building, the State Capitol Building. The "people's park" at the foot of the Capitol complex includes a promenade and landscaped gardens, in addition to providing public space for rallies and special events. A granite map of Iowa depicting all 99 counties rests at the base of the terrace and has become an attraction for in-state visitors, many of whom walk over the map to find their home county.
Iowa's history lives on in the State of Iowa Historical Museum. This modern granite and glass structure at the foot of the State Capitol Building houses permanent and temporary exhibits exploring the people, places, events, and issues of Iowa's past. The showcase includes native wildlife, American Indian and pioneer artifacts, and political and military items. The museum features a genealogy and Iowa history library, museum gift shop, and cafe.

Terrace Hill, a National Historic Landmark and Iowa Governor's Residence, is among the best examples of American Victorian Second Empire architecture. This opulent 1869 home was built by Iowa's first millionaire, Benjamin F. Allen, and restored to the late 19th century period. It overlooks downtown Des Moines and is situated on with a re-created Victorian formal garden. Tours are conducted Tuesdays through Saturdays from March through December.

The Science Center of Iowa and Blank IMAX Dome Theater offers seven interactive learning areas, live programs, and hands-on activities encouraging learning and fun for all ages. Among its three theaters include the 216-seat Blank IMAX Dome Theater, 175-seat John Deere Adventure Theater featuring live performances, and a domed Star Theater.
The Greater Des Moines Botanical Garden, an indoor conservatory of over 15,000 exotic plants, is one of the largest collections of tropical, subtropical, and desert-growing plants in the Midwest. The Center blooms with thousands of flowers year-round. Nearby are the Robert D. Ray Asian Gardens and Pavilion, named in honor of the former governor whose influence helped relocate thousands of Vietnamese refugees to Iowa homes in the 1970s and 1980s. Developed by the city's Asian community, the Gardens include a three-story Chinese pavilion, bonsai landscaping, and granite sculptures to highlight the importance of diversity and recognize Asian American contributions in Iowa.

Blank Park Zoo is a landscaped zoological park on the south side. Among the exhibits include a tropical rain forest, Australian Outback, and Africa. The Zoo offers education classes, tours, and rental facilities.

The Iowa Primate Learning Sanctuary was established as a scientific research facility with a campus housing bonobos and orangutans for the noninvasive interdisciplinary study of their cognitive and communicative capabilities.
The East Village, on the east side of the Des Moines River, begins at the river and extends about five blocks east to the State Capitol Building, offering an eclectic blend of historic buildings, hip eateries, boutiques, art galleries, and a wide variety of other retail establishments mixed with residences.

Adventureland Park is an amusement park in neighboring Altoona, just northeast of Des Moines. The park boasts more than 100 rides, shows, and attractions, including six rollercoasters. A hotel and campground is just outside the park. Also in Altoona is Prairie Meadows Racetrack and Casino, an entertainment venue for gambling and horse racing. Open 24 hours a day, year-round, the racetrack and casino features live racing, plus over 1,750 slot machines, table games, and concert and show entertainment. The racetrack hosts two Grade III races annually, the Iowa Oaks and the Cornhusker Handicap.

Living History Farms in suburban Urbandale tells the story of Midwestern agriculture and rural life in a open-air museum with interpreters dressed in period costume who recreate the daily routines of early Iowans. Open daily from May through October, the Living History Farms include a 1700 Ioway Indian village, 1850 pioneer farm, 1875 frontier town, 1900 horse-powered farm, and a modern crop center.

Wallace House was the home of the first Henry Wallace, a national leader in agriculture and conservation and the first editor of "Wallaces' Farmer" farm journal. This restored 1883 Italianate Victorian houses exhibits, artifacts, and information covering four generations of Henry Wallaces and other family members.

Historic Jordan House in West Des Moines is a stately Victorian home built in 1850 and added to in 1870 by the first white settler in West Des Moines, James C. Jordan. Completely refurbished, this mansion was part of the Underground Railroad and today houses 16 period rooms, a railroad museum, West Des Moines community history, and a museum dedicated to the Underground Railroad in Iowa. In 1893 Jordan's daughter Eda was sliding down the banister when she fell off and broke her neck. She died two days later, and her ghost is reputed to haunt the house.

The "Chicago Tribune" wrote that Iowa's capital city has "walker-friendly downtown streets and enough outdoor sculpture, sleek buildings, storefronts and cafes to delight the most jaded stroller".

Des Moines plays host to a growing number of nationally acclaimed cultural events, including the annual Des Moines Arts Festival in June, Metro Arts Jazz in July, Iowa State Fair in August, and the World Food & Music Festival in September. On Saturdays from May through October, the Downtown Farmers' Market draws visitors from across the state. Local parades include Saint Patrick's Day Parade, Drake Relays Parade, Capitol City Pride Parade, Iowa State Fair Parade, Labor Day Parade, and Beaverdale Fall Festival Parade.

Other annual festivals and events include: Des Moines Beer Week, 80/35 Music Festival, 515 Alive Music Festival, ArtFest Midwest, Blue Ribbon Bacon Fest, CelebrAsian Heritage Festival, Des Moines Pride Festival, Des Moines Renaissance Faire, Festa Italiana, Festival of Trees and Lights, World Food & Music Festival, I'll Make Me a World Iowa, Latino Heritage Festival, Oktoberfest, Winefest, ImaginEve!, Iowa's Premier Beer, Wine & Food Show, and Wild Rose Film Festival.


Des Moines hosts professional minor league teams in several sports — baseball, basketball, hockey, indoor football, and soccer — and is home to the sports teams of Drake University which play in NCAA Division I.

The Des Moines Menace soccer club, a member of USL League Two, play their home games at Valley Stadium in West Des Moines. Des Moines United FC of the National Premier Soccer League also utilize Valley Stadium.

Des Moines is home to the Iowa Cubs baseball team of the Triple-A East. The I-Cubs, which are the Triple-A affiliate of the major league Chicago Cubs, play their home games at Principal Park near the confluence of the Des Moines and Raccoon Rivers.

Wells Fargo Arena of the Iowa Events Center is home to the Iowa Barnstormers of the Indoor Football League, the Iowa Wild of the American Hockey League, and the Iowa Wolves of the NBA G League. The Barnstormers relaunched as an af2 club in 2008 before joining a relaunched Arena Football League in 2010 and the Indoor Football League in 2015; the Barnstormers had previously played in the Arena Football League from 1994 to 2000 (featuring future NFL Hall of Famer and Super Bowl MVP quarterback Kurt Warner) before relocating to New York. The Iowa Energy, a D-League team, began play in 2007. They were bought by the Minnesota Timberwolves in 2017 and were renamed the Iowa Wolves to reflect the new ownership. The Wild, the AHL affiliate of the National Hockey League's Minnesota Wild have played at Wells Fargo Arena since 2013; previously, the Iowa Chops played four seasons in Des Moines (known as the Iowa Stars for three of those seasons.)

Additionally, the Des Moines Buccaneers of the United States Hockey League play at Buccaneer Arena in suburban Urbandale.

Des Moines is also home to the Drake University Bulldogs, an NCAA Division I member of the Missouri Valley Conference, primarily playing northwest of downtown at the on-campus Drake Stadium and Knapp Center. Drake Stadium is home to the famed Drake Relays each April. In addition to the Drake Relays, Drake Stadium has hosted multiple NCAA Outdoor Track and Field Championships and USA Outdoor Track and Field Championships.

The Vikings of Grand View University also compete in intercollegiate athletics in Des Moines. A member of the Heart of America Athletic Conference, within the NAIA, they field 21 varsity athletic teams. They were NAIA National Champions in football in 2013.

The Principal Charity Classic, a Champions Tour golf event, is held at Wakonda Club in late May or early June. The IMT Des Moines Marathon is held throughout the city each October.

Des Moines has 76 city parks and three golf courses, as well as three family aquatic centers, five community centers and three swimming pools. The city has of trails. The first major park was Greenwood Park. The park commissioners purchased the land on April 21, 1894.

The Principal Riverwalk is a riverwalk park district being constructed along the banks of the Des Moines River in the downtown. Primarily funded by the Principal Financial Group, the Riverwalk is a multi-year jointly funded project also funded by the city and state. Upon completion, it will feature a recreational trail connecting the east and west sides of downtown via two pedestrian bridges. A landscaped promenade along the street level is planned. The Riverwalk includes the downtown Brenton Skating Plaza, open from November through March.

Gray's Lake, part of the of Gray's Lake Park, features a boat rental facility, fishing pier, floating boardwalks, and a park resource center. Located just south of the downtown, the centerpiece of the park is a lighted Kruidenier Trail, encircling it entirely.

From downtown Des Moines primarily along the east bank of the Des Moines River, the Neil Smith and John Pat Dorrian Trails are paved recreational trails that connect Gray's Lake northward to the east shore of Saylorville Lake, Big Creek State Park, and the recreational trails of Ankeny including the High Trestle Trail. These trails are near several recreational facilities including the Pete Crivaro Park, Principal Park, the Principal Riverwalk, the Greater Des Moines Botanical Garden, Union Park and its Heritage Carousel of Des Moines, Birdland Park and the Birdland Marina/Boatramp on the Des Moines River, Riverview Park, McHenry Park, and River Drive Park. Although outside of Des Moines, Jester Park has of land along the western shore of Saylorville Lake and can be reached from the Neil Smith Trail over the Saylorville Dam.

Just west of Gray's Lake are the of the Des Moines Water Works Park. The Water Works Park is along the banks of the Raccoon River immediately upstream from where the Raccoon River empties into the Des Moines River. The Des Moines Water Works Facility, which obtains the city's drinking water from the Raccoon River, is entirely within the Water Works Park. A bridge in the park crosses the Raccoon River. The Water Works Park recreational trails link to downtown Des Moines by travelling past Gray's Lake and back across the Raccoon River via either along the Meredith Trail near Principal Park, or along the Martin Luther King Jr. Parkway. The Water Works Park trails connect westward to Valley Junction and the recreational trails of the western suburbs: Windsor Heights, Urbandale, Clive, and Waukee. Also originating from Water Works Park, the Great Western Trail is an journey southward from Des Moines to Martensdale through the Willow Creek Golf Course, Orilla, and Cumming. Often, the location for summer music festivals and concerts, Water Works Park was the overnight campground for thousands of bicyclists on Tuesday, July 23, 2013, during RAGBRAI XLI.

Des Moines operates under a council–manager form of government. The council consists of a mayor who is elected in citywide vote, two at-large members, and four members representing each of the city's four wards. In 2014, Jonathan Gano was appointed as the new Public Works Director. In 2015, Dana Wingert was appointed as Police Chief. In 2018, Steven L. Naber was appointed as the new City Engineer.

The council members include:

A plan to merge the governments of Des Moines and Polk County was rejected by voters during the November 2, 2004, election. The consolidated city-county government would have had a full-time mayor and a 15-member council that would have been divided among the city and its suburbs. Each suburb would still have retained its individual government but with the option to join the consolidated government at any time. Although a full merger was soundly rejected, many city and county departments and programs have been consolidated.

The Des Moines Public Schools district is the largest community school district in Iowa with 32,062 enrolled students as of the 2012–2013 school year. The district consists of 63 schools: 38 elementary schools, eleven middle schools, five high schools (East, Hoover, Lincoln, North, and Roosevelt), and ten special schools and programs. Small parts of the city are instead served by Carlisle Community Schools, Johnston Community School District, the Southeast Polk Community School District and the Saydel School District Grand View Christian School is the only private school in the city, although Des Moines Christian School (in Des Moines from 1947 to 2006) in Urbandale, Dowling Catholic High School in West Des Moines, and Ankeny Christian Academy on the north side of the metro area serve some city residents.

Des Moines is also home to the main campuses of three four-year private colleges: Drake University, Grand View University, and Mercy College of Health Sciences. The University of Iowa has a satellite facility in the city's Western Gateway Park, while Iowa State University hosts Master of Business Administration classes downtown. Des Moines Area Community College is the area's community college with campuses in Ankeny, Des Moines, and West Des Moines. The city is also home to Des Moines University, an osteopathic medical school.

The Des Moines market, which originally consisted of Polk, Dallas, Story, and Warren counties, was ranked 91st by Arbitron as of the fall of 2007 with a population of 512,000 aged 12 and older. But in June 2011 it was moved up to 72nd with the addition of Boone, Clarke, Greene, Guthrie, Jasper, Lucas, Madison and Marion counties.

iHeartMedia owns five radio stations in the area, including WHO 1040 AM, a 50,000-watt AM news/talk station that has the highest ratings in the area and once employed future President Ronald Reagan as a sportscaster. In addition to WHO, iHeartMedia owns KDRB 100.3 FM (adult hits), KKDM 107.5 FM (contemporary hits), KXNO-FM 106.3, and KXNO 1460 AM (sports radio). They also own news/talk station KASI 1430 AM and hot adult contemporary station KCYZ 105.1 FM, both of which broadcast from Ames.

Cumulus Media owns five stations that broadcast from facilities in Urbandale: KBGG 1700 AM (sports), KGGO 94.9 FM (classic rock), KHKI 97.3 FM (country music), KJJY 92.5 FM (country music), and KWQW 98.3 FM (contemporary hits).

Saga Communications owns nine stations in the area: KAZR 103.3 FM (rock), KAZR-HD2 (oldies), KIOA 93.3 FM (oldies), KIOA-HD2 99.9FM & 93.3 HD2 (Rhythmic Top 40), KOEZ 104.1 FM (soft adult contemporary), KPSZ 940 AM (contemporary Christian music, religious teaching, and conservative talk), KRNT 1350 AM (ESPN Radio), KSTZ 102.5 FM (adult contemporary hits), and KSTZ-HD2 (classic country).

Other stations in the Des Moines area include religious stations KWKY 1150 AM, and KPUL 101.7 FM.

Non-commercial radio stations in the Des Moines area include KDPS 88.1 FM, a station operated by the Des Moines Public Schools; KWDM 88.7 FM, a station operated by Valley High School; KJMC 89.3 FM, an urban contemporary station; K213DV 90.5 FM, the contemporary Christian K-Love affiliate for the area; and KDFR 91.3 FM, operated by Family Radio. Iowa Public Radio broadcasts several stations in the Des Moines area, all of which are owned by Iowa State University and operated on campus. WOI 640 am, the networks flagship station, and WOI-FM 90.1, the networks flagship "Studio One" station, are both based out of Ames and serve as the area's National Public Radio outlets. The network also operates classical stations KICG, KICJ, KICL and KICP. The University of Northwestern – St. Paul operates Contemporary Christian simulcasts of KNWI-FM at 107.1 Osceola/Des Moines, KNWM-FM at 96.1 Madrid/Ames/Des Moines, and K264CD at 100.7 in downtown Des Moines.
Low-power FM stations include KFMG-LP 99.1, a community radio station broadcasting from the Hotel Fort Des Moines and also webstreamed.

The Des Moines-Ames media market consists of 35 central Iowa counties: Adair, Adams, Appanoose, Audubon, Boone, Calhoun, Carroll, Clarke, Dallas, Decatur, Franklin, Greene, Guthrie, Hamilton, Hardin, Humboldt, Jasper, Kossuth, Lucas, Madison, Mahaska, Marion, Marshall, Monroe, Pocahontas, Polk, Poweshiek, Ringgold, Story, Taylor, Union, Warren, Wayne, Webster, and Wright. It was ranked 71st by Nielsen Media Research for the 2008–2009 television season with 432,410 television households.

Commercial television stations serving Des Moines include CBS affiliate KCCI channel 8, NBC affiliate WHO-DT channel 13, and Fox affiliate KDSM-TV channel 17. ABC affiliate WOI-TV channel 5 and CW affiliate KCWI-TV channel 23 are both licensed to Ames and broadcast from studios in West Des Moines. KFPX-TV channel 39, the local ION affiliate, is licensed to Newton. Two non-commercial stations are also licensed to Des Moines: KDIN channel 11, the local PBS member station and flagship of the Iowa Public Television network, and KDMI channel 19, a TCT affiliate. Mediacom is the Des Moines area's cable television provider. Television sports listings for Des Moines and Iowa can be found on the Des Moines Register website.

"The Des Moines Register" is the city's primary daily newspaper. As of March 31, 2007, the "Register" ranked 71st in circulation among daily newspapers in the United States according to the Audit Bureau of Circulations with 146,050 daily and 233,229 Sunday subscribers. Weekly newspapers include "Juice", a publication aimed at the 25–34 demographic published by the "Register" on Wednesdays; "Cityview", an alternative weekly published on Thursdays; and the "Des Moines Business Record", a business journal published on Sundays, along with the West Des Moines Register, the Johnston Register, and the Waukee Register on Tuesdays, Wednesdays, or Thursdays depending on the address of the subscriber. Additionally, magazine publisher Meredith Corporation was based in Des Moines prior to its acquisition by IAC and merger with Dotdash in 2021.

Des Moines has an extensive skywalk system within its downtown core. With over four miles of enclosed walkway, it is one of the largest of such systems in the United States. The Des Moines Skywalk System has been criticized for hurting street-level business, though a recent initiative has been made to make street-level Skywalk entrances more visible.
Interstate 235 (I-235) cuts through the city, and I-35 and I-80 both pass through the Des Moines metropolitan area, as well as the city of Des Moines. On the northern side of the city of Des Moines and passing through the cities of Altoona, Clive, Johnston, Urbandale and West Des Moines, I-35 and I-80 converge into a long concurrency while I-235 takes a direct route through Des Moines, Windsor Heights, and West Des Moines before meeting up with I-35 and I-80 on the western edge of the metro. The Des Moines Bypass passes south and east of the city. Other routes in and around the city include US 6, US 69, Iowa 28, Iowa 141, Iowa 163, Iowa 330, Iowa 415, and Iowa 160.

Des Moines's public transit system, operated by DART (Des Moines Area Regional Transit), which was the Des Moines Metropolitan Transit Authority until October 2006, consists entirely of buses, including regular in-city routes and express and commuter buses to outlying suburban areas.

Characteristics of household ownership of cars in Des Moines are similar to national averages. In 2015, 8.5 percent of Des Moines households lacked a car, and increased to 9.6 percent in 2016. The national average was 8.7 percent in 2016. Des Moines averaged 1.71 cars per household in 2016, compared to a national average of 1.8.

Burlington Trailways, and Jefferson Lines run long-distance, intercity bus routes through Des Moines. The bus station is located north of downtown. 

Although Des Moines was historically a train hub, it does not have direct passenger train service. For east–west traffic it was served at the Rock Island Depot by the "Corn Belt Rocket" express from Omaha to the west, to Chicago in the east. The Rock Island also offered the "Rocky Mountain Rocket" from Colorado Springs in the west, to Chicago, and the "Twin Star Rocket" to Minneapolis to the north and Dallas and Houston to the south. The last train was an unnamed service ending at Council Bluffs, and it was discontinued on May 31, 1970. Today, this line constitutes the mainline of the Iowa Interstate Railroad.

Other railroads used the East Des Moines Union Station. Northward and northwest bound, there were Chicago and North Western trains to destinations including Minneapolis. The Wabash Railroad ran service to the southeast to St. Louis. These lines remain in use but are now operated by Union Pacific and BNSF.

The nearest Amtrak station is in Osceola, about south of Des Moines. The Osceola station is served by the Chicago–San Francisco "California Zephyr"; there is no Osceola–Des Moines Amtrak Thruway connecting service. There have been proposals to extend Amtrak's planned Chicago–Moline "Quad City Rocket" to Des Moines via the Iowa Interstate Railroad.

The Des Moines International Airport (DSM), on Fleur Drive in the southern part of Des Moines, offers nonstop service to destinations within the United States. The only international service is cargo service, but there have been discussions about adding an international terminal.

The Greater Des Moines Sister City Commission, with members from the City of Des Moines and the suburbs of Cumming, Norwalk, Windsor Heights, Johnston, Urbandale, and Ankeny, maintains sister city relationships with:




Donald Campbell

Donald Malcolm Campbell, (23 March 1921 – 4 January 1967) was a British speed record breaker who broke eight absolute world speed records on water and on land in the 1950s and 1960s. He remains the only person to set both world land and water speed records in the same year (1964). He died during a water speed record attempt at Coniston Water in the Lake District, England.

Donald Campbell was born at Canbury House, Kingston upon Thames, Surrey, the son of Malcolm, later Sir Malcolm Campbell, holder of 13 world speed records in the 1920s and 1930s in the "Bluebird" cars and boats, and his second wife, Dorothy Evelyn (née Whittall).

Campbell attended St Peter's School, Seaford and Uppingham School. At the outbreak of the Second World War he volunteered for the Royal Air Force, but was unable to serve because of a case of childhood rheumatic fever. He joined Briggs Motor Bodies Ltd in West Thurrock, where he became a maintenance engineer. Subsequently, he was a shareholder in a small engineering company called Kine Engineering, producing machine tools. Following his father's death on 31 December 1948 and aided by Malcolm's chief engineer, Leo Villa, the younger Campbell strove to set speed records first on water and then land.

He married three times — to Daphne Harvey in 1945, producing daughter Georgina (Gina) Campbell, born on 19 September 1946; to Dorothy McKegg (1928–2008) in 1952; and to Tonia Bern (1928–2021) in December 1958, which union lasted until his death in 1967. Campbell was intensely superstitious, hating the colour green, the number thirteen and believing nothing good ever happened on a Friday. He also had some interest in the paranormal, which he nurtured as a member of the Ghost Club.

Campbell began his speed record attempts in the summer of 1949, using his father's old boat, "Blue Bird K4", which he renamed "Bluebird K4". His initial attempts that summer were unsuccessful, although he did come close to raising his father's existing record. The team returned to Coniston Water, Lancashire in 1950 for further trials. While there, they heard that an American, Stanley Sayres, had raised the record from , beyond K4's capabilities without substantial modification.

In late 1950 and 1951, "Bluebird K4" was modified to make it a "prop-rider" as opposed to her original immersed propeller configuration. This greatly reduced hydrodynamic drag: The third planing point would now be the propeller hub, meaning one of the two propeller blades was always out of the water at high speed. She now sported two cockpits, the second one being for Leo Villa.

"Bluebird K4" now had a chance of exceeding Sayres' record and also enjoyed success as a circuit racer, winning the Oltranza Cup in Italy in the spring of that year. Returning to Coniston in September, they finally got "Bluebird" up to 170 mph after further trials, only to suffer a structural failure at which wrecked the boat. Sayres raised the record the following year to in Slo-Mo-Shun IV.

Along with Campbell, Britain had another potential contender for water speed record honours — John Cobb. He had commissioned the world's first purpose-built turbojet Hydroplane, "Crusader", with a target speed of over , and began trials on Loch Ness in autumn 1952. Cobb was killed later that year, when Crusader broke up, during an attempt on the record. Campbell was devastated at Cobb's loss, but he resolved to build a new "Bluebird" boat to bring the water speed record back to Britain.

In early 1953, Campbell began development of his own advanced all-metal jet-powered "Bluebird K7" hydroplane to challenge the record, by now held by the American prop rider hydroplane Slo-Mo-Shun IV.[1] Designed by Ken and Lew Norris, the K7 was a steel-framed, aluminium-bodied, three-point hydroplane with a Metropolitan-Vickers Beryl axial-flow turbojet engine, producing 3,500-pound-force (16 kN) of thrust.

Like Slo-Mo-Shun, but unlike Cobb's tricycle Crusader, the three planing points were arranged with two forward, on outrigged sponsons and one aft, in a "pickle-fork" layout, prompting "Bluebird"s early comparison to a blue lobster. K7 was of very advanced design and construction, and its load bearing steel space frame ultra rigid and stressed to 25 g (exceeding contemporary military jet aircraft). It had a design speed of and remained the only successful jet-boat in the world until the late 1960s.

The designation "K7" was derived from its Lloyd's unlimited rating registration. It was carried on a prominent white roundel on each sponson, underneath an infinity symbol. "Bluebird K7" was the seventh boat registered at Lloyds in the "Unlimited" series.

Campbell set seven world water speed records in K7 between July 1955 and December 1964. The first of these marks was set at Ullswater on 23 July 1955, where he achieved a speed of but only after many months of trials and a major redesign of "Bluebird"s forward sponson attachments points. Campbell achieved a steady series of subsequent speed-record increases with the boat during the rest of the decade, beginning with a mark of in 1955 on Lake Mead in Nevada. Subsequently, four new marks were registered on Coniston Water, where Campbell and "Bluebird" became an annual fixture in the latter half of the 1950s, enjoying significant sponsorship from the Mobil oil company and then subsequently BP.

Campbell also made an attempt in the summer of 1957 at Canandaigua, New York, which failed due to lack of suitable calm water conditions. "Bluebird K7" became a well known and popular attraction, and as well as her annual Coniston appearances, "K7" was displayed extensively in the UK, United States, Canada and Europe, and then subsequently in Australia during Campbell's prolonged attempt on the land speed record in 1963–1964.

To extract more speed, and endow the boat with greater high-speed stability, in both pitch and yaw, "K7" was subtly modified in the second half of the 1950s to incorporate more effective streamlining with a blown Perspex cockpit canopy and fluting to the lower part of the main hull. In 1958, a small wedge shaped tail fin, housing an arrester parachute, modified sponson fairings, that gave a significant reduction in forward aerodynamic lift, and a fixed hydrodynamic stabilising fin, attached to the transom to aid directional stability, and exert a marginal down-force on the nose were incorporated into the design to increase the safe operating envelope of the hydroplane. Thus she reached in 1956, where an unprecedented peak speed of was achieved on one run, in 1957, in 1958 and in 1959.

Campbell was awarded the Order of the British Empire (CBE) in January 1957 for his water speed record breaking, and in particular his record at Lake Mead in the United States which earned him and Britain very positive acclaim.

On 23 November 1964, Campbell achieved the Australian water speed record of on Lake Bonney Riverland in South Australia, although he was unable to break the world record on that attempt.

It was after the Lake Mead water speed record success in 1955 that the seeds of Campbell's ambition to hold the land speed record as well were planted. The following year, the serious planning was under way — to build a car to break the land speed record, which then stood at set by John Cobb in 1947. The Norris brothers designed "Bluebird-Proteus CN7" with in mind.

The British motor industry, in the guise of Dunlop, BP, Smiths Industries, Lucas Automotive, Rubery Owen as well as many others, became heavily involved in the project to build the most advanced car the world had yet seen. CN7 was powered by a specially modified Bristol-Siddeley Proteus free-turbine engine of driving all four wheels. "Bluebird CN7" was designed to achieve 475–500 mph and was completed by the spring of 1960.

Following low-speed tests conducted at the Goodwood motor racing circuit in Sussex, in July, the "CN7" was taken to the Bonneville Salt Flats in Utah, United States, scene of his father's last land speed record triumph, some 25 years earlier in September 1935. The trials initially went well, and various adjustments were made to the car. On the sixth run in CN7, Campbell lost control at over 360 mph and crashed. It was the car's tremendous structural integrity that saved his life. He was hospitalised with a fractured skull and a burst eardrum, as well as minor cuts and bruises, but "CN7" was a write-off. Almost immediately, Campbell announced he was determined to have another go. Sir Alfred Owen, whose Rubery Owen industrial group had built CN7, offered to rebuild it for him. That single decision was to have a profound influence on the rest of Campbell's life. His original plan had been to break the land speed record at over 400 mph in 1960, return to Bonneville the following year to really bump up the speed to something near to 500 mph, get his seventh water speed record with K7 and then retire.

Campbell decided not to go back to Utah for the new trials. He felt the Bonneville course was too short at and the salt surface was in poor condition. BP offered to find another venue and eventually after a long search, Lake Eyre, in South Australia, was chosen. It hadn't rained there for nine years and the vast dry bed of the salt lake offered a course of up to . By the summer of 1962, "Bluebird CN7" was rebuilt, some nine months later than Campbell had hoped. It was essentially the same car, but with the addition of a large stabilising tail fin and a reinforced fibreglass cockpit cover. At the end of 1962, "CN7" was shipped out to Australia ready for the new attempt. Low-speed runs had just started when the rains came. The course was compromised and further rain meant, that by May 1963, Lake Eyre was flooded to a depth of 3 inches, causing the attempt to be abandoned. Campbell was heavily criticised in the press for alleged time wasting and mismanagement of the project, despite the fact that he could hardly be held responsible for the unprecedented weather.

To make matters worse for Campbell, American Craig Breedlove drove his pure thrust jet car "Spirit of America" to a speed of at Bonneville in July 1963. Although the "car" did not conform to FIA (Federation Internationale de L'Automobile) regulations, that stipulated it had to be wheel-driven and have a minimum of four wheels, in the eyes of the world, Breedlove was now the fastest man on Earth.

Campbell returned to Australia in March 1964, but the Lake Eyre course failed to fulfil the early promise it had shown in 1962 and there were further spells of rain. BP pulled out as his main sponsor after a dispute, but he was able to secure backing from Australian oil company Ampol.

The track never properly dried out and Campbell was forced to make the best of the conditions. Finally, in July 1964, he was able to post some speeds that approached the record. On the 17th of that month, he took advantage of a break in the weather and made two courageous runs along the shortened and still damp track, posting a new land speed record of . The surreal moment was captured in a number of well-known images by photographers, including Australia's Jeff Carter. 

Campbell was bitterly disappointed with the record as the vehicle had been designed for much higher speeds. "CN7" covered the final third of the measured mile at an average of , peaking as it left the measured distance at over . He resented the fact that it had all been so difficult. "We've made it — we got the bastard at last," was his reaction to the success. Campbell's 403.1 mph represented the official land speed record.

In 1969, after Campbell's fatal accident, his widow, Tonia Bern-Campbell negotiated a deal with Lynn Garrison, president of Craig Breedlove and Associates, that would see Craig Breedlove run "Bluebird" on Bonneville's Salt Flats. This concept was cancelled when the parallel Spirit of America supersonic car project failed to find support.

Campbell now planned to go after the water speed record one more time with "Bluebird K7" — to do what he had aimed for so many years earlier, during the initial planning stages of CN7 — break both records in the same year. After more delays, he finally achieved his seventh water speed record at Lake Dumbleyung near Perth, Western Australia, on the last day of 1964, at a speed of . He had become the first, and so far only, person to set both land and water speed records in the same year.

Campbell's land speed record was short-lived, because FIA rule changes meant that pure jet cars would be eligible to set records from October 1964. Campbell's speed on his final Lake Eyre run remained the highest speed achieved by a wheel-driven car until 2001; "Bluebird CN7" is now on display at the National Motor Museum at Beaulieu in Hampshire, England, its potential only partly realised.

Campbell decided a massive jump in speed was called for following his successful 1964 land speed record attempt in "Bluebird CN7". His vision was of a supersonic rocket car with a potential maximum speed of . Norris Brothers were requested to undertake a design study. "Bluebird Mach 1.1" was a design for a rocket-powered supersonic land speed record car. Campbell chose a lucky date to hold a press conference at the Charing Cross Hotel on 7 July 1965 to announce his future record breaking plans:

"Bluebird Mach 1.1" was to be rocket-powered. Ken Norris had calculated using rocket motors would result in a vehicle with very low frontal area, greater density, and lighter weight than if he were to employ a jet engine. "Bluebird Mach 1.1" would also be a relatively compact and simple design. Norris specified two off-the-shelf Bristol Siddeley BS.605 rocket engines. The 605 had been developed as a rocket-assisted take-off engine for military aircraft and was fuelled with kerosene, using hydrogen peroxide as the oxidiser. Each engine was rated at thrust. In "Bluebird Mach 1.1" application, the combined thrust would be equivalent of 36,000 bhp (27,000 kW; 36,000 PS) at .

To increase publicity for his rocket car venture, in the spring of 1966, Campbell decided to try once more for a water speed record. This time the target was . "Bluebird K7" was fitted with a lighter and more powerful Bristol Orpheus engine, taken from a Folland Gnat jet aircraft, which developed of thrust. The modified boat was taken back to Coniston in the first week of November 1966. The trials did not go well. The weather was very poor, and "K7" suffered an engine failure when her air intakes collapsed and debris was drawn into the engine. By the middle of December, some high-speed runs were made, in excess of but still well below Campbell's existing record. Problems with "Bluebird"s fuel system meant that the engine could not reach full speed, and so would not develop maximum power. Eventually, by the end of December, after further modifications to her fuel system, and the replacement of a fuel pump, the fuel starvation problem was fixed, and Campbell awaited better weather to mount an attempt.

On 4 January 1967, weather conditions were finally suitable for an attempt. Campbell commenced the first run of his last record attempt at just after 8:45 am. "Bluebird" moved slowly out towards the middle of the lake, where she paused briefly as Campbell lined her up. With a deafening blast of power, Campbell now applied full throttle and "Bluebird" began to surge forward. Clouds of spray issued from the jet-pipe, water poured over the rear spar and after a few hundred yards, at , "Bluebird" unstuck from the surface and rocketed off towards the southern end of the lake, producing her characteristic comet's tail of spray. She entered the measured kilometre at 8:46 am. Leo Villa witnessed her passing the first marker buoy at about in perfect steady planing trim, her nose slightly down, still accelerating. 7.525 seconds later, Keith Harrison saw her leave the measured kilometre at a speed of over . The average speed for the first run was . Campbell lifted his foot from the throttle about 3/10 of a second before passing the southern kilometre marker. As "Bluebird" left the measured kilometre, Keith Harrison and Eric Shaw in a course boat at the southern end of the measured kilometre both noticed that she was very light around the bows, riding on her front stabilising fins. Her planing trim was no worse than she had exhibited when equipped with the Beryl engine, but it was markedly different from that observed by Leo Villa at the northern end of the kilometre, when she was under full acceleration. Campbell had made his usual commentary throughout the run.

Campbell's words on his first run were, via radio intercom:
Instead of refuelling and waiting for the wash of this run to subside, Campbell decided to make the return run immediately. This was not an unprecedented diversion from normal practice, as Campbell had used the advantage presented; i.e., no encroachment of water disturbances on the measured kilometre by the quick turnaround in many previous runs. The second run was even faster once severe tramping subsided on the run-up from Peel Island (caused by the water-brake disturbance). Once smooth water was reached some or so from the start of the kilometre, K7 demonstrated cycles of ground effect hovering before accelerating hard at 0.63 g to a peak speed of some 200 metres or so from the southern marker buoy. "Bluebird" was now experiencing bouncing episodes of the starboard sponson with increasing ferocity. At the peak speed, the most intense and long-lasting bounce precipitated a severe decelerating episode — to , -1.86g — as "K7" dropped back onto the water. Engine flame-out then occurred and, shorn of thrust nose-down momentum, K7 experienced a gliding episode in strong ground effect with increasing angle-of-attack, before completely leaving the water at her static stability pitch-up limit of 5.2°. "Bluebird" then executed an almost complete backflip (~ 320° and slightly off-axis) before plunging into the water (port sponson marginally in advance of the starboard), approximately 230 metres from the end of the measured kilometre. The boat then cartwheeled across the water before coming to rest. The impact broke "K7" forward of the air intakes (where Campbell was sitting) and the main hull sank shortly afterwards.

Mr Whoppit, Campbell's teddy bear mascot, was found among the floating debris and the pilot's helmet was recovered. Royal Navy divers made efforts to find and recover the body but, although the wreck of "K7" was found, they called off the search, after two weeks, without locating his body. Campbell's body was finally located in 2001.

Campbell's last words, during a 31-second transmission, on his final run were, via radio intercom:

The cause of the crash has been variously attributed to several possible causes (or a combination of these causes):

On 28 January 1967, Campbell was posthumously awarded the Queen's Commendation for Brave Conduct "for courage and determination in attacking the world water speed record."

The wreckage of Campbell's craft was recovered by the Bluebird Project between October 2000, when the first sections were raised, and May 2001, when Campbell's body was recovered. The largest section, comprising approximately two-thirds of the centre hull, was raised on 8 March 2001. The project began when diver Bill Smith was inspired to look for the wreck after hearing the Marillion song "Out of This World" (from the album "Afraid of Sunlight"), which was written about Campbell and "Bluebird".

The recovered wreck revealed that the water brake had deployed after the accident as a result of stored accumulator pressure; Campbell would not have had time to deploy the relatively slow-moving brake as the boat flipped out of control. The boat still contained fuel in the engine fuel lines, discounting the fuel-starvation theory. The wreckage all evidenced an impact from left to right, wiping the whole front of the boat off in that direction. Campbell's lower harness mounts had failed and were found to be effectively useless. Further dives recovered various parts of "K7", which had separated from the main hull when it broke up on impact.

Part of Campbell's body was finally located just over two months later and recovered from the lake on 28 May 2001, still wearing his blue nylon overalls. On the night before his death, while playing cards he had drawn the queen and the ace of spades. Reflecting upon the fact that Mary, Queen of Scots had drawn the same two cards the night before she was beheaded, he told his mechanics, who were playing cards with him, that he had a fearful premonition that he was going to "get the chop". It was not possible to determine the cause of Campbell's death, though a consultant engineer giving evidence to the inquest said that the force of the impact could have caused him to be decapitated. When his remains were found, his skull was not present and is still missing.

Campbell was buried in Coniston Cemetery on 12 September 2001 after his coffin was carried down the lake, and through the measured kilometre, on a launch, one last time. A funeral service was then held at St Andrew's Church in Coniston, after an earlier, and positive DNA examination had been carried out. The funeral was attended by his widow, Tonia, daughter Gina, other members of his family, members of his former team and admirers. The funeral was overshadowed in the media by coverage of the 9/11 attacks in the United States.

Campbell's sister, Jean Wales, had been against the recovery of her brother's body out of respect for his stated wish that, in the event of something going wrong, "Skipper and boat stay together". Jean Wales did, however, remain in daily telephone contact with project leader Bill Smith during the recovery operation in anticipation of any news of her brother's remains. 

When Campbell was buried in Coniston Cemetery on 12 September 2001 she did not attend the service. Steve Hogarth, lead singer for Marillion, was present at the funeral and performed the song "Out of This World" solo.

Between them, Campbell and his father had set 11 speed records on water and 10 on land.

The story of Campbell's last attempt at the water speed record on Coniston Water was told in the BBC television film "Across the Lake" in 1988, with Anthony Hopkins as Campbell. Nine years earlier, Robert Hardy had played Campbell's father, Sir Malcolm Campbell, in the "BBC2 Playhouse" television drama "Speed King"; both were written by Roger Milner and produced by Innes Lloyd. In 2003, the BBC showed a documentary reconstruction of Campbell's fateful water-speed record attempt in an episode of "Days That Shook the World". It featured a mixture of modern reconstruction and original film footage. All of the original colour clips were taken from a film capturing the event, "Campbell at Coniston" by John Lomax, a local amateur filmmaker from Wallasey, England. Lomax's film won awards worldwide in the late 1960s for recording the final weeks of Campbell's life.

In 1956, Campbell was surprised by Eamonn Andrews for the seventh episode of the new television show "This Is Your Life".

An English Heritage blue plaque commemorates Campbell and his father at Canbury School, Kingston Hill, Kingston upon Thames, where they lived.

In the village of Coniston, the Ruskin Museum has a display of Campbell memorabilia, and the Bristol Orpheus engine recovered in 2001 is also displayed. The engine's casing is mostly missing, having acted as a sacrificial anode in its time underwater, but the internals are preserved. Campbell's helmet from the ill-fated run is also on display.

On 23 March 2021, organised by the Ruskin Museum, two Hawk jets of the Royal Air Force staged a fly past over the Lake District to mark the 100th anniversary of Campbell's birth. As they flew over Coniston Water, the jets dipped their wings in salute, in a repeat of a gesture carried out by an Avro Vulcan on the day after his death. Campbell's daughter, Gina, laid flowers on the surface of the lake as the jets flew overhead.

On 7 December 2006, Campbell's daughter, Gina Campbell, formally gifted "Bluebird K7" to the Ruskin Museum in Coniston on behalf of the Campbell Family Heritage Trust. In agreement with the trust and the museum, Bill Smith was to organise the restoration of the boat back to running order circa 4 January 1967. Smith said that this would take an undisclosed number of years to accomplish. Gina Campbell commented: "I've decided to secure the future of Bluebird for the people of Coniston, the Ruskin Museum and the people of the world". Museum Director Vicky Slowe spoke of Gina Campbell's generosity and said that: "Bill Smith has assured us he can get Bluebird fully conserved and reconfigured at no cost to the museum. As of 2008, K7 is being fully restored by The Bluebird Project, to a very high standard of working condition in North Shields, Tyne and Wear, using a significant proportion of her original fabric, but with a replacement BS Orpheus engine of the same type albeit incorporating many original components."

As of May 2009, permission had been given for a one-off set of proving trials of "Bluebird" on Coniston Water, where she would be tested to a safe speed for demonstration purposes only. There was no fixed date given for completion of "Bluebird K7" or the trials. Upon restoration, it was planned that "K7" would be housed in her own purpose-built wing at the Ruskin Museum in Coniston.

On 20 March 2018 the restoration was featured on the BBC's "The One Show", when it was announced that "Bluebird K7" would return to the water on Loch Fad, on the Isle of Bute in Scotland, in August 2018 for handling trials.

In August 2018, initial restoration work on "Bluebird" was completed. She was transported to Loch Fad where she was refloated on 4 August 2018. Following initial engine trials on 5 August, "Bluebird" completed a series of test runs on the loch, reaching speeds of about . For safety reasons, there are no plans to attempt to reach any higher speeds.


Directed set

In mathematics, a directed set (or a directed preorder or a filtered set) is a nonempty set formula_1 together with a reflexive and transitive binary relation formula_2 (that is, a preorder), with the additional property that every pair of elements has an upper bound. In other words, for any formula_3 and formula_4 in formula_1 there must exist formula_6 in formula_1 with formula_8 and formula_9 A directed set's preorder is called a direction.

The notion defined above is sometimes called an '. A ' is defined analogously, meaning that every pair of elements is bounded below. 
Some authors (and this article) assume that a directed set is directed upward, unless otherwise stated. Other authors call a set directed if and only if it is directed both upward and downward.

Directed sets are a generalization of nonempty totally ordered sets. That is, all totally ordered sets are directed sets (contrast ordered sets, which need not be directed). Join-semilattices (which are partially ordered sets) are directed sets as well, but not conversely. Likewise, lattices are directed sets both upward and downward.

In topology, directed sets are used to define nets, which generalize sequences and unite the various notions of limit used in analysis. Directed sets also give rise to direct limits in abstract algebra and (more generally) category theory.

In addition to the definition above, there is an equivalent definition. A directed set is a set formula_1 with a preorder such that every finite subset of formula_1 has an upper bound. In this definition, the existence of an upper bound of the empty subset implies that formula_1 is nonempty.

The set of natural numbers formula_13 with the ordinary order formula_2 is one of the most important examples of a directed set. Every totally ordered set is a directed set, including formula_15 formula_16 formula_17 and formula_18

A (trivial) example of a partially ordered set that is directed is the set formula_19 in which the only order relations are formula_20 and formula_21 A less trivial example is like the following example of the "reals directed towards formula_22" but in which the ordering rule only applies to pairs of elements on the same side of formula_22 (that is, if one takes an element formula_3 to the left of formula_25 and formula_4 to its right, then formula_3 and formula_4 are not comparable, and the subset formula_29 has no upper bound).

Let formula_30 and formula_31 be directed sets. Then the Cartesian product set formula_32 can be made into a directed set by defining formula_33 if and only if formula_34 and formula_35 In analogy to the product order this is the product direction on the Cartesian product. For example, the set formula_36 of pairs of natural numbers can be made into a directed set by defining formula_37 if and only if formula_38 and formula_39

If formula_22 is a real number then the set formula_41 can be turned into a directed set by defining formula_42 if formula_43 (so "greater" elements are closer to formula_22). We then say that the reals have been directed towards formula_45 This is an example of a directed set that is partially ordered nor totally ordered. This is because antisymmetry breaks down for every pair formula_3 and formula_4 equidistant from formula_25 where formula_3 and formula_4 are on opposite sides of formula_45 Explicitly, this happens when formula_52 for some real formula_53 in which case formula_42 and formula_55 even though formula_56 Had this preorder been defined on formula_57 instead of formula_58 then it would still form a directed set but it would now have a (unique) greatest element, specifically formula_22; however, it still wouldn't be partially ordered. This example can be generalized to a metric space formula_60 by defining on formula_61 or formula_62 the preorder formula_63 if and only if formula_64

An element formula_65 of a preordered set formula_66 is a "maximal element" if for every formula_67 formula_68 implies formula_69
It is a "greatest element" if for every formula_67 formula_69

Any preordered set with a greatest element is a directed set with the same preorder. 
For instance, in a poset formula_72 every lower closure of an element; that is, every subset of the form formula_73 where formula_74 is a fixed element from formula_72 is directed.

Every maximal element of a directed preordered set is a greatest element. Indeed, a directed preordered set is characterized by equality of the (possibly empty) sets of maximal and of greatest elements.

The subset inclusion relation formula_76 along with its dual formula_77 define partial orders on any given family of sets. 
A non-empty family of sets is a directed set with respect to the partial order formula_78 (respectively, formula_79) if and only if the intersection (respectively, union) of any two of its members contains as a subset (respectively, is contained as a subset of) some third member. 
In symbols, a family formula_80 of sets is directed with respect to formula_78 (respectively, formula_79) if and only if 
or equivalently, 

Many important examples of directed sets can be defined using these partial orders. 
For example, by definition, a or is a non-empty family of sets that is a directed set with respect to the partial order formula_78 and that also does not contain the empty set (this condition prevents triviality because otherwise, the empty set would then be a greatest element with respect to formula_78). 
Every -system, which is a non-empty family of sets that is closed under the intersection of any two of its members, is a directed set with respect to formula_95 Every λ-system is a directed set with respect to formula_96 Every filter, topology, and σ-algebra is a directed set with respect to both formula_78 and formula_96

By definition, a is a function from a directed set and a sequence is a function from the natural numbers formula_99 Every sequence canonically becomes a net by endowing formula_13 with formula_101

If formula_102 is any net from a directed set formula_66 then for any index formula_104 the set formula_105 is called the tail of formula_66 starting at formula_107 The family formula_108 of all tails is a directed set with respect to formula_109 in fact, it is even a prefilter.

If formula_110 is a topological space and formula_22 is a point in formula_112 set of all neighbourhoods of formula_22 can be turned into a directed set by writing formula_114 if and only if formula_115 contains formula_116 For every formula_117 formula_118 and formula_119:

The set formula_133 of all finite subsets of a set formula_80 is directed with respect to formula_79 since given any two formula_136 their union formula_137 is an upper bound of formula_1 and formula_139 in formula_140 This particular directed set is used to define the sum formula_141 of a generalized series of an formula_80-indexed collection of numbers formula_143 (or more generally, the sum of elements in an abelian topological group, such as vectors in a topological vector space) as the limit of the net of partial sums formula_144 that is:
formula_145

Let formula_146 be a formal theory, which is a set of sentences with certain properties (details of which can be found in the article on the subject). For instance, formula_146 could be a first-order theory (like Zermelo–Fraenkel set theory) or a simpler zeroth-order theory. The preordered set formula_148 is a directed set because if formula_149 and if formula_150 denotes the sentence formed by logical conjunction formula_151 then formula_152 and formula_153 where formula_154 
If formula_155 is the Lindenbaum–Tarski algebra associated with formula_146 then formula_157 is a partially ordered set that is also a directed set.

Directed set is a more general concept than (join) semilattice: every join semilattice is a directed set, as the join or least upper bound of two elements is the desired formula_158 The converse does not hold however, witness the directed set {1000,0001,1101,1011,1111} ordered bitwise (e.g. formula_159 holds, but formula_160 does not, since in the last bit 1 > 0), where {1000,0001} has three upper bounds but no upper bound, cf. picture. (Also note that without 1111, the set is not directed.)

The order relation in a directed set is not required to be antisymmetric, and therefore directed sets are not always partial orders. However, the term is also used frequently in the context of posets. In this setting, a subset formula_1 of a partially ordered set formula_162 is called a directed subset if it is a directed set according to the same partial order: in other words, it is not the empty set, and every pair of elements has an upper bound. Here the order relation on the elements of formula_1 is inherited from formula_164; for this reason, reflexivity and transitivity need not be required explicitly.

A directed subset of a poset is not required to be downward closed; a subset of a poset is directed if and only if its downward closure is an ideal. While the definition of a directed set is for an "upward-directed" set (every pair of elements has an upper bound), it is also possible to define a downward-directed set in which every pair of elements has a common lower bound. A subset of a poset is downward-directed if and only if its upper closure is a filter.

Directed subsets are used in domain theory, which studies directed-complete partial orders. These are posets in which every upward-directed set is required to have a least upper bound. In this context, directed subsets again provide a generalization of convergent sequences.



Edward Bellamy

Edward Bellamy (March 26, 1850 – May 22, 1898) was an American author, journalist, and political activist most famous for his utopian novel "Looking Backward". Bellamy's vision of a harmonious future world inspired the formation of numerous "Nationalist Clubs" dedicated to the propagation of his political ideas.

After working as a journalist and writing several unremarkable novels, Bellamy published "Looking Backward" in 1888. It was one of the most commercially successful books published in the United States in the 19th century, and it especially appealed to a generation of intellectuals alienated from the alleged dark side of the Gilded Age. In the early 1890s, Bellamy established a newspaper known as "The New Nation" and began to promote united action between the various Nationalist Clubs and the emerging Populist Party. He published "Equality", a sequel to "Looking Backward", in 1897, and died the following year.

Edward Bellamy was born in Chicopee, Massachusetts. His father was Rufus King Bellamy (1816–1886), a Baptist minister and a descendant of Joseph Bellamy. His mother, Maria Louisa Putnam Bellamy, was a Calvinist. She was the daughter of a Baptist minister named Benjamin Putnam, who was forced to withdraw from the ministry in Salem, Massachusetts, following objections to his becoming a Freemason.

Bellamy attended public school at Chicopee Falls before leaving for Union College of Schenectady, New York, where he studied for just two semesters. Upon leaving school, he made his way to Europe for a year, spending extensive time in Germany. He briefly studied law but abandoned that field without ever having practiced as a lawyer, instead entering the world of journalism. In this capacity Bellamy briefly served on the staff of the "New York Post" before returning to his native Massachusetts to take a position at the "Springfield Union".

At the age of 25, Bellamy developed tuberculosis, the disease that would ultimately kill him. He suffered with its effects throughout his adult life. In an effort to regain his health, Bellamy spent a year in the Hawaiian Islands (1877 to 1878). Returning to the United States, he decided to abandon the daily grind of journalism in favor of literary work, which put fewer demands upon his time and his health.

Bellamy married Emma Augusta Sanderson in 1882. The couple had two children.

Bellamy's early novels, including "Six to One" (1878), "Dr. Heidenhoff's Process" (1880), and "Miss Ludington's Sister" (1885), were unremarkable works, making use of standard psychological plots.

A turn to utopian science fiction with "Looking Backward, 2000–1887," published in January 1888, captured the public imagination and catapulted Bellamy to literary fame. Its publisher could scarcely keep up with demand. Within a year it had sold some 200,000 copies, and by the end of the 19th century had sold more copies than any other book published in America up to that time except for "Uncle Tom's Cabin" by Harriet Beecher Stowe and "" by Lew Wallace. The book gained an extensive readership in the United Kingdom as well, more than 235,000 copies being sold there between 1890 and 1935.

In "Looking Backward", a non-violent revolution had transformed the American economy and thereby society; private property had been abolished in favor of state ownership of capital and the elimination of social classes and the ills of society that he thought inevitably followed from them. In the new world of the year 2000, there was no longer war, poverty, crime, prostitution, corruption, money, or taxes. Neither did there exist such occupations seen by Bellamy as of dubious worth to society, such as politicians, lawyers, merchants, or soldiers. Instead, Bellamy's utopian society of the future was based upon the voluntary employment of all citizens between the ages of 21 and 45, after which time all would retire. Work was simple, aided by machine production, working hours short and vacation time long. The new economic basis of society effectively remade human nature itself in Bellamy's idyllic vision, with greed, maliciousness, untruthfulness, and insanity all relegated to the past.

Bellamy's book inspired legions of inspired readers to establish so-called Nationalist Clubs, beginning in Boston late in 1888. His vision of a country relieved of its social ills through abandonment of the principle of competition and establishment of state ownership of industry proved an appealing panacea to a generation of intellectuals alienated from the dark side of Gilded Age America. By 1891 it was reported that no fewer than 162 Nationalist Clubs were in existence.

Bellamy's use of the term "Nationalism" rather than "socialism" as a descriptor of his governmental vision was calculated, as he did not want to limit either sales of his novel or the potential influence of its political ideas. In an 1888 letter to literary critic William Dean Howells, Bellamy wrote:

Bellamy himself came to actively participate in the political movement which emerged around his book, particularly after 1891 when he founded his own magazine, "The New Nation," and began to promote united action between the various Nationalist Clubs and the emerging People's Party. For the next three and a half years, Bellamy gave his all to politics, publishing his magazine, working to influence the platform of the People's Party, and publicizing the Nationalist movement in the popular press. This phase of his life came to an end in 1894, when "The New Nation" was forced to suspend publication owing to financial difficulties.

With the key activists of the Nationalist Clubs largely absorbed into the apparatus of the People's Party (although a Nationalist Party did run three candidates for office in Wisconsin as late as 1896), Bellamy abandoned politics for a return to literature. He set to work on a sequel to "Looking Backward" titled "Equality," attempting to deal with the ideal society of the post-revolutionary future in greater detail. In this final work, he addressed the question of feminism, dealing with the taboo subject of female reproductive rights in a future, post-revolutionary America. Other subjects overlooked in "Looking Backward," such as animal rights and wilderness preservation, were dealt with in a similar context. The book saw print in 1897 and would prove to be Bellamy's final creation.

Several short stories of Bellamy's were published in 1898, and "The Duke of Stockbridge; a Romance of Shays' Rebellion" was published in 1900.

Edward Bellamy died of tuberculosis in Chicopee Falls, Massachusetts ten years after the publication of his most famous book. He was 48 years old.

His lifelong home in Chicopee Falls, built by his father, was designated a National Historic Landmark in 1971.

Bellamy was the cousin of Francis Bellamy, famous for writing the original version of the Pledge of Allegiance.

Bellamy Road, a residential road in Toronto, is named for the author.





E

E, or e, is the fifth letter and the second vowel letter in the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. Its name in English is "e" (pronounced ); plural "es", "Es" or "E's". It is the most commonly used letter in many languages, including Czech, Danish, Dutch, English, French, German, Hungarian, Latin, Latvian, Norwegian, Spanish, and Swedish.

The Latin letter 'E' differs little from its source, the Greek letter epsilon, 'Ε'. This in turn comes from the Semitic letter "hê", which has been suggested to have started as a praying or calling human figure ("hillul" 'jubilation'), and was most likely based on a similar Egyptian hieroglyph that indicated a different pronunciation. In Semitic, the letter represented (and in foreign words); in Greek, "hê" became the letter epsilon, used to represent . The various forms of the Old Italic script and the Latin alphabet followed this usage.

Although Middle English spelling used to represent long and short , the Great Vowel Shift changed long (as in "me" or "bee") to while short (as in "met" or "bed") remained a mid vowel. In unstressed syllables, this letter is usually pronounced either as or . In other cases, the letter is silent, generally at the end of words like "queue".

In the orthography of many languages it represents either , , , or some variation (such as a nasalized version) of these sounds, often with diacritics (as: ) to indicate contrasts. Less commonly, as in French, German, or Saanich, represents a mid-central vowel . Digraphs with are common to indicate either diphthongs or monophthongs, such as or for or in English, for in German, and for in French or in German.

The International Phonetic Alphabet uses for the close-mid front unrounded vowel or the mid front unrounded vowel.

'E' is the most common (or highest-frequency) letter in the English language alphabet and several other European languages, which has implications in both cryptography and data compression. In the story "The Gold-Bug" by Edgar Allan Poe, a character figures out a random character code by remembering that the most used letter in English is E. This makes it a hard and popular letter to use when writing lipograms. Ernest Vincent Wright's "Gadsby" (1939) is considered a "dreadful" novel, and supposedly "at least part of Wright's narrative issues were caused by language limitations imposed by the lack of "E"." Both Georges Perec's novel "A Void" ("La Disparition") (1969) and its English translation by Gilbert Adair omit 'e' and are considered better works.




In British Sign Language (BSL), the letter 'e' is signed by extending the index finger of the right hand touching the tip of index on the left hand, with all fingers of left hand open.




Economics

Economics () is a social science that studies the production, distribution, and consumption of goods and services.

Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes what's viewed as basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the economy as a system where production, consumption, saving, and investment interact, and factors affecting it: employment of the resources of labour, capital, and land, currency inflation, economic growth, and public policies that have impact on these elements.

Other broad distinctions within economics include those between positive economics, describing "what is", and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.

Economic analysis can be applied throughout society, including business, finance, cybersecurity, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science and the environment.

The earlier term for the discipline was 'political economy', but since the late 19th century, it has commonly been called 'economics'. The term is ultimately derived from Ancient Greek ("oikonomia") which is a term for the "way (nomos) to run a household (oikos)", or in other words the know-how of an ("oikonomikos"), or "household or homestead manager". Derived terms such as "economy" can therefore often mean "frugal" or "thrifty". By extension then, "political economy" was the way to manage a polis or state.

There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as "an inquiry into the nature and causes of the wealth of nations", in particular as:

Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science "of" production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined "the dismal science" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) delimited the subject matter further:

Alfred Marshall provided a still widely cited definition in his textbook "Principles of Economics" (1890) that extended analysis beyond wealth and from the societal to the microeconomic level:

Lionel Robbins (1932) developed implications of what has been termed "[p]erhaps the most commonly accepted current definition of the subject":

Robbins described the definition as not "classificatory" in "pick[ing] out certain "kinds" of behaviour" but rather "analytical" in "focus[ing] attention on a particular "aspect" of behaviour, the form imposed by the influence of scarcity." He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after "end"), generates both cost and benefits; and, "resources" (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding "actors" (assuming they are rational) may never go to war (a "decision") but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).

Some subsequent comments criticized the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.

Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as "combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly." One commentary characterizes the remark as making economics an approach rather than a subject matter but with great specificity as to the "choice process and the type of social interaction that [such] analysis involves." The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.

Many economists including nobel prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter. Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, they do not say that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will do DNA analysis, others might do anatomy, and still others might build game theoretic models of animal behavior. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is very peculiar.

Questions regarding distribution of resources are found throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod himself as the "first economist". However, the word Oikos, the Greek word from which the word economy derives, was used for issues regarding how to manage a household (which was understood to be the landowner, his family, and his slaves) rather than to refer to some normative societal system of distribution of resources, which is a much more recent phenomenon. Xenophon, the author of the Oeconomicus, is credited by philologues for being the source of the word economy. Other notable writers from Antiquity through to the Renaissance which wrote on include Aristotle, Chanakya (also known as Kautilya), Qin Shi Huang, Ibn Khaldun, and Thomas Aquinas. Joseph Schumpeter described 16th and 17th century scholastic writers, including Tomás de Mercado, Luis de Molina, and Juan de Lugo, as "coming nearer than any other group to being the 'founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.

Two groups, who later were called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.

Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of "laissez-faire", which called for minimal government intervention in the economy.

Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject.

The publication of Adam Smith's "The Wealth of Nations" in 1776, has been described as "the effective birth of economics as a separate discipline." The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive.

Smith discusses potential benefits of specialization by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His "theorem" that "the division of labor is limited by the extent of the market" has been described as the "core of a theory of the functions of firm and industry" and a "fundamental principle of economic organization." To Smith has also been ascribed "the most important substantive proposition in all of economics" and foundation of resource-allocation theory—that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).

In an argument that includes "one of the most famous passages in all economics," Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this:

The Rev. Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Lincoln Simon has criticized Malthus's conclusions.

While Adam Smith emphasized production and income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first to state and prove the principle of comparative advantage, according to which each country should specialize in producing and exporting goods in that it has a lower "relative" cost of production, rather relying only on its own production. It has been termed a "fundamental analytical explanation" for gains from trade.

Coming at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.

Value theory was important in classical theory. Smith wrote that the "real price of every thing ... is the toil and trouble of acquiring it". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size.

Marxist (later, Marxian) economics descends from classical economics and it derives from the work of Karl Marx. The first volume of Marx's major work, "Das Kapital", was published in 1867. Marx focused on the labour theory of value and theory of surplus value which, he believed, explained the exploitation of labour by capital. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production, and the theory of surplus value demonstrated how workers were only paid a proportion of the value their work had created.

Marxian economics was further developed by Karl Kautsky (1854–1938)'s "The Economic Doctrines of Karl Marx" and "The Class Struggle (Erfurt Program)", Rudolf Hilferding's (1877–1941) "Finance Capital", Vladimir Lenin (1870–1924)'s "The Development of Capitalism in Russia" and "Imperialism, the Highest Stage of Capitalism", and Rosa Luxemburg (1871–1919)'s "The Accumulation of Capital".

At its inception as a social science, "economics" was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his "Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth" (1803). These three items were considered only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has survived in part up to the present, modified by substituting the word "wealth" for "goods and services" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his "Essay on the Nature and Significance of Economic Science", he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity, which forces people to choose, allocate scarce resources to competing ends, and economize (seeking the greatest welfare while avoiding the wasting of scarce resources). According to Robbins: "Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses". Robbins' definition eventually became widely accepted by mainstream economists, and found its way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition.

A body of theory later termed "neoclassical economics" formed from about 1870 to 1910. The term "economics" was popularized by such neoclassical economists as Alfred Marshall and Mary Paley Marshall as a concise synonym for "economic science" and a substitute for the earlier "political economy". This corresponded to the influence on the subject of mathematical methods used in the natural sciences.

Neoclassical economics systematically integrated supply and demand as joint determinants of both price and quantity in market equilibrium, influencing the allocation of output and income distribution. It rejected the classical economics' labour theory of value in favor of a marginal utility theory of value on the demand side and a more comprehensive theory of costs on the supply side. In the 20th century, neoclassical theorists departed from an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behavior-based relations across individuals.

In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.

Neoclassical economics is occasionally referred as "orthodox economics" whether by its critics or sympathizers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalize earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income.

Neoclassical economics studies the behaviour of individuals, households, and organizations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more players to attain the best possible outcome.

Keynesian economics derives from John Maynard Keynes, in particular his book "The General Theory of Employment, Interest and Money" (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low "effective demand" and why even price flexibility and monetary policy might be unavailing. The term "revolutionary" has been applied to the book in its impact on economic analysis. 

During the following decades, many economists followed Keynes' ideas and expanded on his works. John Hicks and Alvin Hansen developed the IS–LM model which was a simple formalisation of some of Keynes' insights on the economy's short-run equilibrium. Franco Modigliani and James Tobin developed important theories of private consumption and investment, respectively, two major components of aggregate demand. Lawrence Klein built the first large-scale macroeconometric model, applying the Keynesian thinking systematically to the US economy.

Immediately after World War II, Keynesian was the dominant economic view of the United States establishment and its allies, Marxian economics was the dominant economic view of the Soviet Union nomenklatura and its allies. 

Monetarism appeared in the 1950s and 1960s, its intellectual leader being Milton Friedman. Monetarists contended that monetary policy and other monetary shocks, as represented by the growth in the money stock, was an important cause of economic fluctuations, and consequently that monetary policy was more important than fiscal policy for purposes of stabilization. Friedman was also skeptical about the ability of central banks to conduct a sensible active monetary policy in practice, advocating instead using simple rules such as a steady rate of money growth.

Monetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned again because the results turned out to be unsatisfactory.

A more fundamental challenge to the prevailing Keynesian paradigm came in the 1970s from new classical economists like Robert Lucas, Thomas Sargent and Edward Prescott. They introduced the notion of rational expectations in economics, which had profound implications for many economic discussions, among which were the socalled Lucas critique and the presentation of real business cycle models.

During the 1980s a group of researchers appeared being called New Keynesian economists, including among others George Akerlof, Janet Yellen, Gregory Mankiw and Olivier Blanchard. They adopted the principle of rational expectations and other monetarist or new classical ideas such as building upon models employing micro foundations and optimizing behaviour, but simultaneously emphasized the importance of various market failures for the functioning of the economy, as had Keynes. Not least, they proposed various reasons that potentially explained the empirically observed features of price and wage rigidity, usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones.

After decades of often heated discussions between Keynesians, monetarists, new classical and new Keynesian economists, a synthesis emerged by the 2000s, often given the name "the new neoclassical synthesis". It integrated the rational expectations and optimizing framework of the new classical theory with a new Keynesian role for nominal rigidities and other market imperfections like imperfect information in goods, labour and credit markets. The monetarist importance of monetary policy in stabilizing the economy and in particular controlling inflation was recognized as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis led to a new class of applied models, known as dynamic stochastic general equilibrium or DSGE models, descending from real business cycles models, but extended with several new Keynesian and other features. These models proved very useful and influential in the design of modern monetary policy and are now standard workhorses in most central banks.

After the 2007–2008 financial crisis, macroeconomic research has put greater emphasis on understanding and integrating the financial system into models of the general economy and shedding light on the ways in which problems in the financial sector can turn into major macroeconomic recessions. In this and other research branches, inspiration from behavioral economics has started playing a more important role in mainstream economic theory. Also, heterogeneity among the economic agents, e.g. differences in income, plays an increasing role in recent economic research.

Other schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Freiburg School, the School of Lausanne, the Stockholm school and the Chicago school of economics. During the 1970s and 1980s mainstream economics was sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago school approach.

Within macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics and the new neoclassical synthesis. 

Beside the mainstream development of economic thought, various alternative or heterodox economic theories have evolved over time, positioning themselves in contrast to mainstream theory. These include:




Additionally, alternative developments include Marxian economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, econodynamics, feminist economics and biophysical economics.

Feminist economics emphasizes the role that gender plays in economies, challenging analyses that render gender invisible or support gender-oppressive economic systems. The goal is to create economic research and policy analysis that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalized groups.

Mainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories. While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories.

In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models with microfoundations, in which microeconomic concepts play a major part. 

Sometimes an economic hypothesis is only "qualitative", not "quantitative".

Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics. Paul Samuelson's treatise "Foundations of Economic Analysis" (1947) exemplifies the method, particularly as to maximizing behavioral relations of agents reaching equilibrium. The book focused on examining the class of statements called "operationally meaningful theorems" in economics, which are theorems that can conceivably be refuted by empirical data.

Economic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.

Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance ("signal strength") of the hypothesized relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.

Experimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct.

In behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a "genuine science".

Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.

Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a "price taker" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.

Forms of imperfect competition include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Firms under imperfect competition have the potential to be "price makers", which means that they can influence the prices of their products.

In partial equilibrium method of analysis, it is assumed that activity in the market being analysed does not affect other markets. This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across "all" markets. This method studies both changes in markets and their interactions leading towards equilibrium.

In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods (new computers, bananas, etc.), and "guns" vs "butter".

Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.

Economic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.

The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say "guns" and "butter"). The PPF is a table or graph (as at the right) showing the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.

Scarcity is represented in the figure by people being willing but unable in the aggregate to consume "beyond the PPF" (such as at "X") and by the negative slope of the curve. If production of one good "increases" along the curve, production of the other good "decreases", an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.

The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a "real opportunity cost". Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. "Along the PPF", scarcity implies that choosing "more" of one good in the aggregate entails doing with "less" of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.

By construction, each point on the curve shows "productive efficiency" in maximizing output for given total inputs. A point "inside" the curve (as at "A"), is feasible but represents "production inefficiency" (wasteful use of inputs), in that output of "one or both goods" could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organization of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.

Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organize society for the most efficient use of resources has been described as the "essence of economics", where the subject "makes its unique contribution."

Specialization is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus "relatively" cheaper, input.

Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialize in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.

It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialization in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.

The general theory of specialization applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding "division of labour" with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.

An example that combines features above is a country that specializes in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.

Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialization of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the "increased income levels" that trade may facilitate.

Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.

For a given market of a commodity, "demand" is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is "constrained utility maximization" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.

The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.

"Supply" is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be "profit maximizers", meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.

That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The "Law of Supply" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.

Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.

People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through "firms". The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organize their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.

In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organization generalizes from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.

Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimize business decisions, including unit-cost minimization and profit maximization, given the firm's objectives and constraints imposed by technology and market conditions.

Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.

Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organization, discussed above, to model different types of firm behaviour, for example in a solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own.

In this, it generalizes maximization approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic "Theory of Games and Economic Behavior" by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology.

Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation.

Some market organizations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's "Market for Lemons" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a "lemon" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving).

Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market ("incomplete markets"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure.

The term "market failure" encompasses several problems which may undermine standard economic assumptions. Although economists categorize market failures differently, the following categories emerge in the main texts.

Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.

Natural monopoly, or the overlapping concepts of "practical" and "technical" monopoly, is an extreme case of "failure of competition" as a restraint on producers. Extreme economies of scale are one possible cause.

Public goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.

Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidize or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.

In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesized long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.

Some specialized fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or "public bads".

Policy options include regulations that reflect cost–benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights.

Welfare economics uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyzes "social welfare", however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no "social welfare" apart from the "welfare" associated with its individual units.

Macroeconomics, another branch of economics, examines the economy as a whole to explain broad aggregates and their interactions "top down", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.

Since at least the 1960s, macroeconomics has been characterized by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject.

Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth.

"Growth economics" studies factors that explain economic growth – the increase in output "per capita" of a country over a long period of time. The same factors are used to explain differences in the "level" of output "per capita" "between" countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.

Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting.

The economics of a depression were the spur for the creation of "macroeconomics" as a separate discipline. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled "The General Theory of Employment, Interest and Money" outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.

He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilize output over the business cycle.
Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of "The General Theory".

Over the years, understanding of the business cycle has branched into various research programmes, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with classical economics, stating that Keynesianism is correct in the short run but qualified by classical-like considerations in the intermediate and long run.

New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and "rational expectations" theory, led by Robert Lucas, and real business cycle theory.

In contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are "sticky", which means they do not adjust instantaneously to changes in economic conditions.

Thus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the "long run" may be very long.

The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes.

Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.

Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.

While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.

Money is a "means of final payment" for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, "Money is what money does" ("Money is "that" money does" in the original).

As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialized producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.

Monetary policy is the policy that central banks conduct to accomplish their broader objectives. Most central banks in developed countries follow inflation targeting, whereas the main objective for many central banks in development countries is to uphold a fixed exchange rate system. The primary monetary tool is normally the adjustment of interest rates, either directly via administratively changing the central bank's own interest rates or indirectly via open market operations. Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and inflation.

Governments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.

For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.

The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.

Sceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes.

Economic inequality includes income inequality, measured using the distribution of income (the amount of money people receive), and wealth inequality measured using the distribution of wealth (the amount of wealth people own), and other measures such as consumption, land ownership, and human capital. Inequality exists at different extents between countries or states, groups of people, and individuals. There are many methods for measuring inequality, the Gini coefficient being widely used for income differences among individuals. An example measure of inequality between countries is the Inequality-adjusted Human Development Index, a composite index that takes inequality into account. Important concepts of equality include equity, equality of outcome, and equality of opportunity.

Research has linked economic inequality to political and social instability, including revolution, democratic breakdown and civil conflict. Research suggests that greater inequality hinders economic growth and macroeconomic stability, and that land and human capital inequality reduce growth more than inequality of income. Inequality is at the center stage of economic policy debate across the globe, as government tax and spending policies have significant effects on income distribution. In advanced economies, taxes and transfers decrease income inequality by one-third, with most of this being achieved via public social spending (such as pensions and family benefits.)

Public economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost–benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.

Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like.

Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society.

International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalization.

Labor economics seeks to understand the functioning and dynamics of the markets for wage labor. "Labor markets" function through the interaction of workers and employers. Labor economics looks at the suppliers of labor services (workers), the demands of labor services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, "labor" is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms.

Development economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.

Economics has been subject to criticism that it relies on unrealistic, unverifiable, or highly simplified assumptions, in some cases because these assumptions simplify the proofs of desired conclusions. For example, the economist Friedrich Hayek claimed that economics (at least historically) used a scientistic approach which he claimed was ""decidedly unscientific in the true sense of the word, since it involves a mechanical and uncritical application of habits of thought to fields different from those in which they have been formed"". Latter-day examples of such assumptions include perfect information, profit maximization and rational choices, axioms of neoclassical economics. Such criticisms often conflate neoclassical economics with all of contemporary economics. The field of information economics includes both mathematical-economical research and also behavioural economics, akin to studies in behavioural psychology, and confounding factors to the neoclassical assumptions are the subject of substantial study in many areas of economics.

Prominent historical mainstream economists such as Keynes and Joskow observed that much of the economics of their time was conceptual rather than quantitative, and difficult to model and formalize quantitatively. In a discussion on oligopoly research, Paul Joskow pointed out in 1975 that in practice, serious students of actual economies tended to use "informal models" based upon qualitative factors specific to particular industries. Joskow had a strong feeling that the important work in oligopoly was done through informal observations while formal models were "trotted out "ex post"". He argued that formal models were largely not important in the empirical work, either, and that the fundamental factor behind the theory of the firm, behaviour, was neglected. Deirdre McCloskey has argued that many empirical economic studies are poorly reported, and she and Stephen Ziliak argue that although her critique has been well-received, practice has not improved. The extent to which practice has improved since the early 2000s is contested: although economists have noted the discipline's adoption of increasingly rigorous modeling, other have criticized the field's focus on creating computer simulations detached from reality, as well as noting the loss of prestige suffered by the field for failing to anticipate the Great Recession.

Economics has been derogatorily dubbed "the dismal science", first coined by the Victorian historian Thomas Carlyle in the 19th century. It is often stated that Carlyle gave it this nickname as a response to the work of Thomas Robert Malthus, who predicted widespread starvation resulting from projections that population growth would exceed the rate of increase in the food supply. However, the actual phrase was coined by Carlyle in the context of a debate with John Stuart Mill on slavery, in which Carlyle argued for slavery; the "dismal" nature of economics in Carlyle's view was that it "[found] the secret of this Universe in 'supply and demand', and reduc[ed] the duty of human governors to that of letting men alone"."

Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, , family economics and institutional economics.

Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.

Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed "political economy" to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.

Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.

The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's "The Protestant Ethic and the Spirit of Capitalism" (1905) and Georg Simmel's "The Philosophy of Money" (1900). More recently, the works of James S. Coleman, Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field.

Gary Becker in 1974 presented an economic theory of social interactions, whose applications included the family, charity, merit goods and multiperson interactions, and envy and hatred.

The professionalization of economics, reflected in the growth of graduate programmes on the subject, has been described as "the main change in economics since around 1900". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study.
See Bachelor of Economics and Master of Economics.

In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national treasury, central bank or National Bureau of Statistics. See Economic analyst.

There are dozens of prizes awarded to economists each year for outstanding intellectual contributions to the field, the most prominent of which is the Nobel Memorial Prize in Economic Sciences, though it is not a Nobel Prize.

Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialize in econometrics and mathematical methods.

Harriet Martineau (1802–1876) was a widely-read populariser of classical economic thought. Mary Paley Marshall (1850–1944), the first women lecturer at a British economics faculty, wrote "The Economics of Industry" with her husband Alfred Marshall. Joan Robinson (1903–1983) was an important post-Keynesian economist. The economic historian Anna Schwartz (1915–2012) coauthored "A Monetary History of the United States, 1867–1960" with Milton Friedman. Three women have received the Nobel Prize in Economics: Elinor Ostrom (2009), Esther Duflo (2019) and Claudia Goldin (2023). Five have received the John Bates Clark Medal: Susan Athey (2007), Esther Duflo (2010), Amy Finkelstein (2012), Emi Nakamura (2019) and Melissa Dell (2020).

Women's authorship share in prominent economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship. Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation.




Electronic paper

Electronic paper, also known as electronic ink (e-ink) or intelligent paper, is a display device that mimics the appearance of ordinary ink on paper. Unlike conventional flat panel displays that emit light, an electronic paper display reflects ambient light, like paper. This may make them more comfortable to read, and provide a wider viewing angle than most light-emitting displays. The contrast ratio in electronic displays available as of 2008 approaches newspaper, and newly developed displays are slightly better. An ideal e-paper display can be read in direct sunlight without the image appearing to fade.

Technologies include Gyricon, electrophoretics, electrowetting, interferometry, and plasmonics.
Many electronic paper technologies hold static text and images indefinitely without electricity. Flexible electronic paper uses plastic substrates and plastic electronics for the display backplane. Applications of e-paper include electronic shelf labels and digital signage, bus station time tables, electronic billboards, smartphone displays, and e-readers able to display digital versions of books and magazines.

Electronic paper was first developed in the 1970s by Nick Sheridon at Xerox's Palo Alto Research Center. The first electronic paper, called Gyricon, consisted of polyethylene spheres between 75 and 106 micrometers across. Each sphere is a Janus particle composed of negatively charged black plastic on one side and positively charged white plastic on the other (each bead is thus a dipole). The spheres are embedded in a transparent silicone sheet, with each sphere suspended in a bubble of oil so that it can rotate freely. The polarity of the voltage applied to each pair of electrodes then determines whether the white or black side is face-up, thus giving the pixel a white or black appearance.
At the FPD 2008 exhibition, Japanese company Soken demonstrated a wall with electronic wall-paper using this technology. In 2007, the Estonian company Visitret Displays was developing this kind of display using polyvinylidene fluoride (PVDF) as the material for the spheres, dramatically improving the video speed and decreasing the control voltage needed.

An electrophoretic display (EPD) forms images by rearranging charged pigment particles with an applied electric field.
In the simplest implementation of an EPD, titanium dioxide (titania) particles approximately one micrometer in diameter are dispersed in a hydrocarbon oil. A dark-colored dye is also added to the oil, along with surfactants and charging agents that cause the particles to take on an electric charge. This mixture is placed between two parallel, conductive plates separated by a gap of 10 to 100 micrometres. When a voltage is applied across the two plates, the particles migrate electrophoretically to the plate that bears the opposite charge from that on the particles. When the particles are located at the front (viewing) side of the display, it appears white, because the light is scattered back to the viewer by the high-index titania particles. When the particles are located at the rear side of the display, it appears dark, because the light is absorbed by the colored dye. If the rear electrode is divided into a number of small picture elements (pixels), then an image can be formed by applying the appropriate voltage to each region of the display to create a pattern of reflecting and absorbing regions.

EPDs are typically addressed using MOSFET-based thin-film transistor (TFT) technology. TFTs are often used to form a high-density image in an EPD.
A common application for TFT-based EPDs are e-readers. Electrophoretic displays are considered prime examples of the electronic paper category, because of their paper-like appearance and low power consumption. Examples of commercial electrophoretic displays include the high-resolution active matrix displays used in the Amazon Kindle, Barnes & Noble Nook, Sony Reader, Kobo eReader, and iRex iLiad e-readers. These displays are constructed from an electrophoretic imaging film manufactured by E Ink Corporation. A mobile phone that used the technology is the Motorola Fone.

Electrophoretic Display technology has also been developed by SiPix and Bridgestone/Delta. SiPix is now part of E Ink Corporation. The SiPix design uses a flexible 0.15 mm Microcup architecture, instead of E Ink's 0.04 mm diameter microcapsules. Bridgestone Corp.'s Advanced Materials Division cooperated with Delta Optoelectronics Inc. in developing Quick Response Liquid Powder Display technology.

Electrophoretic displays can be manufactured using the Electronics on Plastic by Laser Release (EPLaR) process, developed by Philips Research, to enable existing AM-LCD manufacturing plants to create flexible plastic displays.

]

In the 1990s another type of electronic ink based on a microencapsulated electrophoretic display was conceived and prototyped by a team of undergraduates at MIT as described in their Nature paper. J.D. Albert, Barrett Comiskey, Joseph Jacobson, Jeremy Rubin and Russ Wilcox co-founded E Ink Corporation in 1997 to commercialize the technology. E Ink subsequently formed a partnership with Philips Components two years later to develop and market the technology. In 2005, Philips sold the electronic paper business as well as its related patents to Prime View International. "It has for many years been an ambition of researchers in display media to create a flexible low-cost system that is the electronic analog of paper. In this context, microparticle-based displays have long intrigued researchers. Switchable contrast in such displays is achieved by the electromigration of highly scattering or absorbing microparticles (in the size range 0.1–5 μm), quite distinct from the molecular-scale properties that govern the behavior of the more familiar liquid-crystal displays. Micro-particle-based displays possess intrinsic bistability, exhibit extremely low power d.c. field addressing and have demonstrated high contrast and reflectivity. These features, combined with a near-lambertian viewing characteristic, result in an 'ink on paper' look. But such displays have to date suffered from short lifetimes and difficulty in manufacture. Here we report the synthesis of an electrophoretic ink based on the microencapsulation of an electrophoretic dispersion. The use of a microencapsulated electrophoretic medium solves the lifetime issues and permits the fabrication of a bistable electronic display solely by means of printing. This system may satisfy the practical requirements of electronic paper."

This used tiny microcapsules filled with electrically charged white particles suspended in a colored oil. In early versions, the underlying circuitry controlled whether the white particles were at the top of the capsule (so it looked white to the viewer) or at the bottom of the capsule (so the viewer saw the color of the oil). This was essentially a reintroduction of the well-known electrophoretic display technology, but microcapsules meant the display could be made on flexible plastic sheets instead of glass.
One early version of the electronic paper consists of a sheet of very small transparent capsules, each about 40 micrometers across. Each capsule contains an oily solution containing black dye (the electronic ink), with numerous white titanium dioxide particles suspended within. The particles are slightly negatively charged, and each one is naturally white.
The screen holds microcapsules in a layer of liquid polymer, sandwiched between two arrays of electrodes, the upper of which is transparent. The two arrays are aligned to divide the sheet into pixels, and each pixel corresponds to a pair of electrodes situated on either side of the sheet. The sheet is laminated with transparent plastic for protection, resulting in an overall thickness of 80 micrometers, or twice that of ordinary paper.
The network of electrodes connects to display circuitry, which turns the electronic ink 'on' and 'off' at specific pixels by applying a voltage to specific electrode pairs. A negative charge to the surface electrode repels the particles to the bottom of local capsules, forcing the black dye to the surface and turning the pixel black. Reversing the voltage has the opposite effect. It forces the particles to the surface, turning the pixel white. A more recent implementation of this concept requires only one layer of electrodes beneath the microcapsules. These are commercially referred to as Active Matrix Electrophoretic Displays (AMEPD).

Electrowetting display (EWD) is based on controlling the shape of a confined water/oil interface by an applied voltage. With no voltage applied, the (colored) oil forms a flat film between the water and a hydrophobic (water-repellent) insulating coating of an electrode, resulting in a colored pixel. When a voltage is applied between the electrode and the water, the interfacial tension between the water and the coating changes. As a result, the stacked state is no longer stable, causing the water to move the oil aside. This makes a partly transparent pixel, or, if a reflective white surface is under the switchable element, a white pixel. Because of the small pixel size, the user only experiences the average reflection, which provides a high-brightness, high-contrast switchable element.

Displays based on electrowetting provide several attractive features. The switching between white and colored reflection is fast enough to display video content. It is a low-power, low-voltage technology, and displays based on the effect can be made flat and thin. The reflectivity and contrast are better than or equal to other reflective display types and approach the visual qualities of paper. In addition, the technology offers a unique path toward high-brightness full-color displays, leading to displays that are four times brighter than reflective LCDs and twice as bright as other emerging technologies. Instead of using red, green, and blue (RGB) filters or alternating segments of the three primary colors, which effectively result in only one-third of the display reflecting light in the desired color, electrowetting allows for a system in which one sub-pixel can switch two different colors independently.

This results in the availability of two-thirds of the display area to reflect light in any desired color. This is achieved by building up a pixel with a stack of two independently controllable colored oil films plus a color filter.

The colors are cyan, magenta, and yellow, which is a subtractive system, comparable to the principle used in inkjet printing. Compared to LCD, brightness is gained because no polarisers are required.

Electrofluidic display is a variation of an electrowetting display that place an aqueous pigment dispersion inside a tiny reservoir. The reservoir comprises less than 5-10% of the viewable pixel area and therefore the pigment is substantially hidden from view. Voltage is used to electromechanically pull the pigment out of the reservoir and spread it as a film directly behind the viewing substrate. As a result, the display takes on color and brightness similar to that of conventional pigments printed on paper. When voltage is removed liquid surface tension causes the pigment dispersion to rapidly recoil into the reservoir. The technology can potentially provide greater than 85% white state reflectance for electronic paper.

The core technology was invented at the Novel Devices Laboratory at the University of Cincinnati and there are working prototypes developed by collaboration with Sun Chemical, Polymer Vision and Gamma Dynamics.

It has a wide margin in critical aspects such as brightness, color saturation and response time.
Because the optically active layer can be less than 15 micrometres thick, there is strong potential for rollable displays.

The technology used in electronic visual displays that can create various colors via interference of reflected light. The color is selected with an electrically switched light modulator comprising a microscopic cavity that is switched on and off using driver integrated circuits similar to those used to address liquid-crystal displays (LCD).

Plasmonic nanostructures with conductive polymers have also been suggested as one kind of electronic paper. The material has two parts. The first part is a highly reflective metasurface made by metal-insulator-metal films tens of nanometers in thickness including nanoscale holes. The metasurfaces can reflect different colors depending on the thickness of the insulator. The standard RGB color schema can be used as pixels for full-color displays. The second part is a polymer with optical absorption controllable by an electrochemical potential. After growing the polymer on the plasmonic metasurfaces, the reflection of the metasurfaces can be modulated by the applied voltage. This technology presents broad range colors, high polarization-independent reflection (>50 %), strong contrast (>30 %), the fast response time (hundreds of ms), and long-term stability. In addition, it has ultralow power consumption (< 0.5 mW/cm2) and potential for high resolution (>10000 dpi). Since the ultrathin metasurfaces are flexible and the polymer is soft, the whole system can be bent. Desired future improvements for this technology include bistability, cheaper materials and implementation with TFT arrays.

Other research efforts into e-paper have involved using organic transistors embedded into flexible substrates, including attempts to build them into conventional paper.
Simple color e-paper consists of a thin colored optical filter added to the monochrome technology described above. The array of pixels is divided into triads, typically consisting of the standard cyan, magenta and yellow, in the same way as CRT monitors (although using subtractive primary colors as opposed to additive primary colors). The display is then controlled like any other electronic color display.

E Ink Corporation of E Ink Holdings Inc. released the first colored E Ink displays to be used in a marketed product. The Ectaco jetBook Color was released in 2012 as the first colored electronic ink device, which used E Ink's Triton display technology. E Ink in early 2015 also announced another color electronic ink technology called Prism. This new technology is a color changing film that can be used for e-readers, but Prism is also marketed as a film that can be integrated into architectural design such as "wall, ceiling panel, or entire room instantly." The disadvantage of these current color displays is that they are considerably more expensive than standard E Ink displays. The jetBook Color costs roughly nine times more than other popular e-readers such as the Amazon Kindle. As of January 2015, Prism had not been announced to be used in the plans for any e-reader devices.

Several companies are simultaneously developing electronic paper and ink. While the technologies used by each company provide many of the same features, each has its own distinct technological advantages. All electronic paper technologies face the following general challenges:

Electronic ink can be applied to flexible or rigid materials. For flexible displays, the base requires a thin, flexible material tough enough to withstand considerable wear, such as extremely thin plastic. The method of how the inks are encapsulated and then applied to the substrate is what distinguishes each company from others. These processes are complex and are carefully guarded industry secrets. Nevertheless, making electronic paper is less complex and costly than LCDs.

There are many approaches to electronic paper, with many companies developing technology in this area. Other technologies being applied to electronic paper include modifications of liquid-crystal displays, electrochromic displays, and the electronic equivalent of an Etch A Sketch at Kyushu University. Advantages of electronic paper include low power usage (power is only drawn when the display is updated), flexibility and better readability than most displays. Electronic ink can be printed on any surface, including walls, billboards, product labels and T-shirts. The ink's flexibility would also make it possible to develop rollable displays for electronic devices.

In December 2005, Seiko released the first electronic ink based watch called the Spectrum SVRD001 wristwatch, which has a flexible electrophoretic display and in March 2010 Seiko released a second generation of this famous electronic ink watch with an active matrix display. The Pebble smart watch (2013) uses a low-power memory LCD manufactured by Sharp for its e-paper display.

In 2019, Fossil launched a hybrid smartwatch called the Hybrid HR, integrating an always on electronic ink display with physical hands and dial to simulate the look of a traditional analog watch.

In 2004, Sony released the Librié in Japan, the first e-book reader with an electronic paper E Ink display. In September 2006, Sony released the PRS-500 Sony Reader e-book reader in the USA. On October 2, 2007, Sony announced the PRS-505, an updated version of the Reader. In November 2008, Sony released the PRS-700BC, which incorporated a backlight and a touchscreen.

In late 2007, Amazon began producing and marketing the Amazon Kindle, an e-book reader with an e-paper display. In February 2009, Amazon released the Kindle 2 and in May 2009 the larger Kindle DX was announced. In July 2010 the third-generation Kindle was announced, with notable design changes. The fourth generation of Kindle, called Touch, was announced in September 2011 that was the Kindle's first departure from keyboards and page turn buttons in favor of touchscreens. In September 2012, Amazon announced the fifth generation of the Kindle called the Paperwhite, which incorporates a LED frontlight and a higher contrast display.

In November 2009, Barnes and Noble launched the Barnes & Noble Nook, running an Android operating system. It differs from other e-readers in having a replaceable battery, and a separate touch-screen color LCD below the main electronic paper reading screen.

In 2017, Sony and reMarkable offered e-books tailored for writing with a smart stylus.

In 2020, Onyx released the first frontlit 13.3 inch electronic paper Android tablet, the Boox Max Lumi. At the end of the same year, Bigme released the first 10.3 inch color electronic paper Android tablet, the Bigme B1 Pro. This was also the first large electronic paper tablet to support 4g cellular data.

In February 2006, the Flemish daily "De Tijd" distributed an electronic version of the paper to select subscribers in a limited marketing study, using a pre-release version of the iRex iLiad. This was the first recorded application of electronic ink to newspaper publishing.

The French daily "Les Échos" announced the official launch of an electronic version of the paper on a subscription basis in September 2007. Two offers were available, combining a one-year subscription and a reading device. The offer included either a light (176g) reading device (adapted for Les Echos by Ganaxa) or the iRex iLiad. Two different processing platforms were used to deliver readable information of the daily, one based on the newly developed GPP electronic ink platform from "Ganaxa", and the other one developed internally by Les Echos.

Flexible display cards enable financial payment cardholders to generate a one-time password to reduce online banking and transaction fraud. Electronic paper offers a flat and thin alternative to existing key fob tokens for data security. The world's first ISO compliant smart card with an embedded display was developed by Innovative Card Technologies and nCryptone in 2005. The cards were manufactured by Nagra ID.

Some devices, like USB flash drives, have used electronic paper to display status information, such as available storage space. Once the image on the electronic paper has been set, it requires no power to maintain, so the readout can be seen even when the flash drive is not plugged in.

Motorola's low-cost mobile phone, the Motorola F3, uses an alphanumeric black-and-white electrophoretic display.

The Samsung Alias 2 mobile phone incorporates electronic ink from E Ink into the keypad, which allows the keypad to change character sets and orientation while in different display modes.

On December 12, 2012, Yota Devices announced the first "YotaPhone" prototype and was later released in December 2013, a unique double-display smartphone. It has a 4.3-inch, HD LCD on the front and an electronic ink display on the back.

On May and June 2020, Hisense released the Hisense A5c and A5 pro cc, the first color electronic ink smartphones. With a single color display, with a togglable front light running android 9 and Android 10.

E-paper based electronic shelf labels (ESL) are used to digitally display the prices of goods at retail stores. Electronic-paper-based labels are updated via two-way infrared or radio technology and powered by a rechargeable coin cell.
Some variants use ZBD (zenithal bistable display) which is more similar to LCD but does not need power to retain an image.
E-paper displays at bus or trams stops can be remotely updated. Compared to LED or liquid-crystal displays (LCDs), they consume lower energy and the text or graphics stays visible during a power failure. Compared to LCDs, it easily visible under full sunshine.

Because of its energy-saving properties, electronic paper has proved a technology suited to digital signage applications.

Electronic paper is used on computer monitors like the 13.3 inch Dasung Paperlike 3 HD and 25.3 inch Paperlike 253.

Some laptops like Lenovo ThinkBook Plus use e-paper as a secondary screen.

Typically, e-paper electronic tags integrate e-ink technology with wireless interfaces like NFC or UHF. They are most commonly used as employees' ID cards or as production labels to track manufacturing changes and status. E-paper tags are also increasingly being used as shipping labels, especially in the case of reusable boxes. 
An interesting feature provided by some e-paper Tags manufacturers is batteryless design. This means that the power needed for a display's content update is provided wirelessly and the module itself doesn't contain any battery.

Other proposed applications include clothes, digital photo frames, information boards, and keyboards. Keyboards with dynamically changeable keys are useful for less represented languages, non-standard keyboard layouts such as Dvorak, or for special non-alphabetical applications such as video editing or games.
The reMarkable is a writer tablet for reading and taking notes.




Earth

Earth is the third planet from the Sun and the only astronomical object known to harbor life. This is enabled by Earth being a water world, the only one in the Solar System sustaining liquid surface water. Almost all of Earth's water is contained in its global ocean, covering 70.8% of Earth's crust. The remaining 29.2% of Earth's crust is land, most of which is located in the form of continental landmasses within one hemisphere, Earth's land hemisphere. Most of Earth's land is somewhat humid and covered by vegetation, while large sheets of ice at Earth's polar deserts retain more water than Earth's groundwater, lakes, rivers and atmospheric water combined. Earth's crust consists of slowly moving tectonic plates, which interact to produce mountain ranges, volcanoes, and earthquakes. Earth has a liquid outer core that generates a magnetosphere capable of deflecting most of the destructive solar winds and cosmic radiation.

Earth has a dynamic atmosphere, which sustains Earth's surface conditions and protects it from most meteoroids and UV-light at entry. It has a composition of primarily nitrogen and oxygen. Water vapor is widely present in the atmosphere, forming clouds that cover most of the planet. The water vapor acts as a greenhouse gas and, together with other greenhouse gases in the atmosphere, particularly carbon dioxide (CO), creates the conditions for both liquid surface water and water vapor to persist via the capturing of energy from the Sun's light. This process maintains the current average surface temperature of 14.76 °C, at which water is liquid under atmospheric pressure. Differences in the amount of captured energy between geographic regions (as with the equatorial region receiving more sunlight than the polar regions) drive atmospheric and ocean currents, producing a global climate system with different climate regions, and a range of weather phenomena such as precipitation, allowing components such as nitrogen to cycle.

Earth is rounded into an ellipsoid with a circumference of about 40,000 km. It is the densest planet in the Solar System. Of the four rocky planets, it is the largest and most massive. Earth is about eight light-minutes away from the Sun and orbits it, taking a year (about 365.25 days) to complete one revolution. Earth rotates around its own axis in slightly less than a day (in about 23 hours and 56 minutes). Earth's axis of rotation is tilted with respect to the perpendicular to its orbital plane around the Sun, producing seasons. Earth is orbited by one permanent natural satellite, the Moon, which orbits Earth at 384,400 km (1.28 light seconds) and is roughly a quarter as wide as Earth. The Moon's gravity helps stabilize Earth's axis, causes tides and gradually slows Earth's rotation. Tidal locking has made the Moon always face Earth with the same side.

Earth, like most other bodies in the Solar System, formed 4.5 billion years ago from gas in the early Solar System. During the first billion years of Earth's history, the ocean formed and then life developed within it. Life spread globally and has been altering Earth's atmosphere and surface, leading to the Great Oxidation Event two billion years ago. Humans emerged 300,000 years ago in Africa and have spread across every continent on Earth. Humans depend on Earth's biosphere and natural resources for their survival, but have increasingly impacted the planet's environment. Humanity's current impact on Earth's climate and biosphere is unsustainable, threatening the livelihood of humans and many other forms of life, and causing widespread extinctions.

The Modern English word "Earth" developed, via Middle English, from an Old English noun most often spelled '. It has cognates in every Germanic language, and their ancestral root has been reconstructed as *"erþō". In its earliest attestation, the word "eorðe" was used to translate the many senses of Latin ' and Greek γῆ "gē": the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Roman Terra/Tellūs and Greek Gaia, Earth may have been a personified goddess in Germanic paganism: late Norse mythology included Jörð ("Earth"), a giantess often given as the mother of Thor.

Historically, "Earth" has been written in lowercase. Beginning with the use of Early Middle English, its definite sense as "the globe" was expressed as "the earth". By the era of Early Modern English, capitalization of nouns began to prevail, and "the earth" was also written "the Earth", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as "Earth", by analogy with the names of the other planets, though "earth" and forms with "the earth" remain common. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes "Earth" when appearing as a name, such as a description of the "Earth's atmosphere", but employs the lowercase when it is preceded by "the", such as "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"

The name "Terra" occasionally is used in scientific writing and especially in science fiction to distinguish humanity's inhabited planet from others, while in poetry "Tellus" has been used to denote personification of the Earth. "Terra" is also the name of the planet in some Romance languages, languages that evolved from Latin, like Italian and Portuguese, while in other Romance languages the word gave rise to names with slightly altered spellings, like the Spanish "Tierra" and the French "Terre". The Latinate form "Gæa" or "Gaea" () of the Greek poetic name "Gaia" (; or ) is rare, though the alternative spelling "Gaia" has become common due to the Gaia hypothesis, in which case its pronunciation is rather than the more classical English .

There are a number of adjectives for the planet Earth. The word "earthly" is derived from "Earth". From the Latin "Terra" comes "terran" , "terrestrial" , and (via French) "terrene" , and from the Latin "Tellus" comes "tellurian" and "telluric".

The oldest material found in the Solar System is dated to Ga (billion years) ago. By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth being estimated as likely taking anywhere from 70 to 100 million years to form.

Estimates of the age of the Moon range from 4.5 Ga to significantly younger. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object with about 10% of Earth's mass, named Theia, collided with Earth. It hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.

Earth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. Sufficient water to fill the oceans may have been on Earth since it formed. In this model, atmospheric greenhouse gases kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.

As the molten outer layer of Earth cooled it formed the first solid crust, which is thought to have been mafic in composition. The first continental crust, which was more felsic in composition, formed by the partial melting of this mafic crust. The presence of grains of the mineral zircon of Hadean age in Eoarchean sedimentary rocks suggests that at least some felsic crust existed as early as , only after Earth's formation. There are two main models of how this initial small volume of continental crust evolved to reach its current abundance: (1) a relatively steady growth up to the present day, which is supported by the radiometric dating of continental crust globally and (2) an initial rapid growth in the volume of continental crust during the Archean, forming the bulk of the continental crust that now exists, which is supported by isotopic evidence from hafnium in zircons and neodymium in sedimentary rocks. The two models and the data that support them can be reconciled by large-scale recycling of the continental crust, particularly during the early stages of Earth's history.

New continental crust forms as a result of plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, tectonic forces have caused areas of continental crust to group together to form supercontinents that have subsequently broken apart. At approximately , one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia at , then finally Pangaea, which also began to break apart at .

The most recent pattern of ice ages began about , and then intensified during the Pleistocene about . High- and middle-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every 21,000, 41,000 and 100,000 years. The Last Glacial Period, colloquially called the "last ice age", covered large parts of the continents, to the middle latitudes, in ice and ended about 11,700 years ago.

Chemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.

During the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed "Snowball Earth", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been at least five major mass extinctions and many minor ones. Apart from the proposed current Holocene extinction event, the most recent was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but largely spared small animals such as insects, mammals, lizards and birds. Mammalian life has diversified over the past , and several million years ago an African ape species gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.

Earth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. Due to the increased luminosity, Earth's mean temperature may reach in 1.5 billion years, and all ocean water will evaporate and be lost to space, which may trigger a runaway greenhouse effect, within an estimated 1.6 to 3 billion years. Even if the Sun were stable, a fraction of the water in the modern oceans will descend to the mantle, due to reduced steam venting from mid-ocean ridges.

The Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius, otherwise, with tidal effects, it may enter the Sun's atmosphere and be vaporized.

Earth has a rounded shape, through hydrostatic equilibrium, with an average diameter of , making it the fifth largest planetary sized and largest terrestrial object of the Solar System.

Due to Earth's rotation it has the shape of an ellipsoid, bulging at its Equator; its diameter is longer there than at its poles.
Earth's shape furthermore has local topographic variations. Though the largest local variations, like the Mariana Trench ( below local sea level), only shortens Earth's average radius by 0.17% and Mount Everest ( above local sea level) lengthens it by only 0.14%. Since Earth's surface is farthest out from Earth's center of mass at its equatorial bulge, the summit of the volcano Chimborazo in Ecuador () is its farthest point out.
Parallel to the rigid land topography the Ocean exhibits a more dynamic topography.

To measure the local variation of Earth's topography, geodesy employs an idealized Earth producing a shape called a geoid. Such a geoid shape is gained if the ocean is idealized, covering Earth completely and without any perturbations such as tides and winds. The result is a smooth but gravitational irregular geoid surface, providing a mean sea level (MSL) as a reference level for topographic measurements.

Earth's surface is the boundary between the atmosphere, and the solid Earth and oceans. Defined in this way, it has an area of about . Earth can be divided into two hemispheres: by latitude into the polar Northern and Southern hemispheres; or by longitude into the continental Eastern and Western hemispheres.

Most of Earth's surface is ocean water: 70.8% or . This vast pool of salty water is often called the "world ocean", and makes Earth with its dynamic hydrosphere a water world or ocean world. Indeed, in Earth's early history the ocean may have covered Earth completely. The world ocean is commonly divided into the Pacific Ocean, Atlantic Ocean, Indian Ocean, Antarctic or Southern Ocean, and Arctic Ocean, from largest to smallest. The ocean covers Earth's oceanic crust, but to a lesser extent with shelf seas also shelves of the continental crust. The oceanic crust forms large oceanic basins with features like abyssal plains, seamounts, submarine volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, and a globe-spanning mid-ocean ridge system.

At Earth's polar regions, the ocean surface is covered by seasonally variable amounts of sea ice that often connects with polar land, permafrost and ice sheets, forming polar ice caps.

Earth's land covers 29.2%, or of Earth's surface. The land surface includes many islands around the globe, but most of the land surface is taken by the four continental landmasses, which are (in descending order): Africa-Eurasia, America (landmass), Antarctica, and Australia (landmass). These landmasses are further broken down and grouped into the continents. The terrain of the land surface varies greatly and consists of mountains, deserts, plains, plateaus, and other landforms. The elevation of the land surface varies from a low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .

Land can be covered by surface water, snow, ice, artificial structures or vegetation. Most of Earth's land hosts vegetation, but ice sheets (10%, not including the equally large land under permafrost) or cold as well as hot deserts (33%) occupy also considerable amounts of it.

The pedosphere is the outermost layer of Earth's land surface and is composed of soil and subject to soil formation processes. Soil is crucial for land to be arable. Earth's total arable land is 10.7% of the land surface, with 1.3% being permanent cropland. Earth has an estimated of cropland and of pastureland.

The land surface and the ocean floor form the top of Earth's crust, which together with parts of the upper mantle form Earth's lithosphere. Earth's crust may be divided into oceanic and continental crust. Beneath the ocean-floor sediments, the oceanic crust is predominantly basaltic, while the continental crust may include lower density materials such as granite, sediments and metamorphic rocks. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the mass of the crust.

Earth's surface topography comprises both the topography of the ocean surface, and the shape of Earth's land surface. The submarine terrain of the ocean floor has an average bathymetric depth of 4 km, and is as varied as the terrain above sea level.

Earth's surface is continually being shaped by internal plate tectonic processes including earthquakes and volcanism; by weathering and erosion driven by ice, water, wind and temperature; and by biological processes including the growth and decomposition of biomass into soil.

Earth's mechanically rigid outer layer of Earth's crust and upper mantle, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: at convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.

As the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is , although zircons have been found preserved as clasts within Eoarchean sedimentary rocks that give ages up to , indicating that at least some continental crust existed at that time.

The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the South American Plate, progressing at a typical rate of .

Earth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, which is divided into independently moving tectonic plates.

Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. Earth's inner core may be rotating at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year, although both somewhat higher and much lower rates have also been proposed. The radius of the inner core is about one-fifth of that of Earth.
Density increases with depth, as described in the table on the right.

Among the Solar System's planetary-sized objects Earth is the object with the highest density.

Earth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1% by mass), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminum (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to gravitational separation, the core is primarily composed of the denser elements: iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements. The most common rock constituents of the crust are oxides. Over 99% of the crust is composed of various oxides of eleven elements, principally oxides containing silicon (the silicate minerals), aluminum, iron, calcium, magnesium, potassium, or sodium.

The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.

The mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.

The gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within Earth. Near Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad regional differences in Earth's gravitational field, known as gravity anomalies.

The main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with a magnetic dipole moment of at epoch 2000, decreasing nearly 6% per century (although it still remains stronger than its long time average). The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.

The extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bow shock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates. The ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belts are formed by high-energy particles whose motion is essentially random, but contained in the magnetosphere.

During magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.

Earth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer than the mean solar day.

Earth's rotation period relative to the fixed stars, called its "stellar day" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean March equinox (when the Sun is at 90° on the equator), is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms.

Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.

Earth orbits the Sun, making Earth the third-closest planet to the Sun and part of the inner Solar System. Earth's average orbital distance is about , which is the basis for the Astronomical Unit and is equal to roughly 8.3 light minutes or 380 times Earth's distance to the Moon.

Earth orbits the Sun every 365.2564 mean solar days, or one sidereal year. With an apparent movement of the Sun in Earth's sky at a rate of about 1°/day eastward, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian.

The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.

The Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the Sun and Earth's north poles, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth-Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.

The Hill sphere, or the sphere of gravitational influence, of Earth is about in radius. This is the maximum distance at which Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun. Earth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.

The axial tilt of Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and in the Southern Hemisphere when the Tropic of Capricorn faces the Sun. In each instance, winter occurs simultaneously in the opposite hemisphere.

During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. Above the Arctic Circle and below the Antarctic Circle there is no daylight at all for part of the year, causing a polar night, and this night extends for several months at the poles themselves. These same latitudes also experience a midnight sun, where the sun remains visible all day.

By astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when Earth's rotational axis is aligned with its orbital axis. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.

The angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800-year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.

In modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.8% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.

The Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as "moons", after Earth's. The most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.

The gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases. Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Ediacaran period, for example, (approximately ) there were 400±7 days in a year, with each day lasting 21.9±0.4 hours.

The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting large changes over millions of years, as is the case for Mars, though this is disputed.

Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.

On 1 November 2023, scientists reported that, according to computer simulations, remnants of a protoplanet, named Theia, could be inside the Earth, left over from a collision with the Earth in ancient times, and afterwards becoming the Moon.

Earth's co-orbital asteroids population consists of quasi-satellites, objects with a horseshoe orbit and trojans. There are at least five quasi-satellites, including 469219 Kamoʻoalewa. A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in Earth's orbit around the Sun. The tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.

, there are 4,550 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.

Earth's hydrosphere is the sum of Earth's water and its distribution. Most of Earth's hydrosphere consists of Earth's global ocean. Earth's hydrosphere also consists of water in the atmosphere and on land, including clouds, inland seas, lakes, rivers, and underground waters down to a depth of .

The mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be . About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers. The remaining 30% is ground water, 1% surface water (covering only 2.8% of Earth's land) and other small forms of fresh water deposits such as permafrost, water vapor in the atmosphere, biological binding, etc. .

In Earth's coldest regions, snow survives over the summer and changes into ice. This accumulated snow and ice eventually forms into glaciers, bodies of ice that flow under the influence of their own gravity. Alpine glaciers form in mountainous areas, whereas vast ice sheets form over land in polar regions. The flow of glaciers erodes the surface changing it dramatically, with the formation of U-shaped valleys and other landforms. Sea ice in the Arctic covers an area about as big as the United States, although it is quickly retreating as a consequence of climate change.

The average salinity of Earth's oceans is about 35 grams of salt per kilogram of seawater (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.

The abundance of water, particularly liquid water, on Earth's surface is a unique feature that distinguishes it from other planets in the Solar System. Solar System planets with considerable atmospheres do partly host atmospheric water vapor, but they lack surface conditions for stable surface water. Despite some moons showing signs of large reservoirs of extraterrestrial liquid water, with possibly even more volume than Earth's ocean, all of them are large bodies of water under a kilometers thick frozen surface layer.

The atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. Clouds cover around two-thirds of Earth's surface, more so over oceans than land. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.

Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the surface, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form.

Earth's atmosphere has no definite boundary, gradually becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface; this lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.

The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean heat content and currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.

Earth receives 1361 W/m of solar irradiance. The amount of solar energy that reaches Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.

Further factors that affect a location's climates are its proximity to oceans, the oceanic and atmospheric circulation, and topology. Places close to oceans typically have colder summers and warmer winters, due to the fact that oceans can store large amounts of heat. The wind transports the cold or the heat of the ocean to the land. Atmospheric circulation also plays an important role: San Francisco and Washington DC are both coastal cities at about the same latitude. San Francisco's climate is significantly more moderate as the prevailing wind direction is from sea to land. Finally, temperatures decrease with height causing mountainous areas to be colder than low-lying areas.

Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.

The commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions based on observed temperature and precipitation. Surface air temperature can rise to around in hot deserts, such as Death Valley, and can fall as low as in Antarctica.

The upper atmosphere, the atmosphere above the troposphere, is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.

Thermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.

Earth is the only known place that has ever been habitable for life. Earth's life developed in Earth's early bodies of water some hundred million years after Earth formed.

Earth's life has been shaping and inhabiting many particular ecosystems on Earth and has eventually expanded globally forming an overarching biosphere. Therefore, life has impacted Earth, significantly altering Earth's atmosphere and surface over long periods of time, causing changes like the Great Oxidation Event.

Earth's life has over time greatly diversified, allowing the biosphere to have different biomes, which are inhabited by comparatively similar plants and animals. The different biomes developed at distinct elevations or water depths, planetary temperature latitudes and on land also with different humidity. Earth's species diversity and biomass reaches a peak in shallow waters and with forests, particularly in equatorial, warm and humid conditions. While freezing polar regions and high altitudes, or extremely arid areas are relatively barren of plant and animal life.

Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain a metabolism. Plants and other organisms take up nutrients from water, soils and the atmosphere. These nutrients are constantly recycled between different species.

Extreme weather, such as tropical cyclones (including hurricanes and typhoons), occurs over most of Earth's surface and has a large impact on life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, blizzards, floods, droughts, wildfires, and other calamities and disasters. Human impact is felt in many areas due to pollution of the air and water, acid rain, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion. Human activities release greenhouse gases into the atmosphere which cause global warming. This is driving changes such as the melting of glaciers and ice sheets, a global rise in average sea levels, increased risk of drought and wildfires, and migration of species to colder areas.

Originating from earlier primates in Eastern Africa 300,000 years ago humans have since been migrating and with the advent of agriculture in the 10th millennium BC increasingly settling Earth's land. In the 20th century Antarctica had been the last continent to see a first and until today limited human presence.

Human population has since the 19th century grown exponentially to seven billion in the early 2010s, and is projected to peak at around ten billion in the second half of the 21st century. Most of the growth is expected to take place in sub-Saharan Africa.

Distribution and density of human population varies greatly around the world with the majority living in south to eastern Asia and 90% inhabiting only the Northern Hemisphere of Earth, partly due to the hemispherical predominance of the world's land mass, with 68% of the world's land mass being in the Northern Hemisphere. Furthermore, since the 19th century humans have increasingly converged into urban areas with the majority living in urban areas by the 21st century.

Beyond Earth's surface humans have lived on a temporary basis, with only special purpose deep underground and underwater presence, and a few space stations. Human population virtually completely remains on Earth's surface, fully depending on Earth and the environment it sustains. Since the second half of the 20th century, some hundreds of humans have temporarily stayed beyond Earth, a tiny fraction of whom have reached another celestial body, the Moon.

Earth has been subject to extensive human settlement, and humans have developed diverse societies and cultures. Most of Earth's land has been territorially claimed since the 19th century by sovereign states (countries) separated by political borders, and 205 such states exist today, with only parts of Antarctica and a few small regions remaining unclaimed. Most of these states together form the United Nations, the leading worldwide intergovernmental organization, which extends human governance over the ocean and Antarctica, and therefore all of Earth.

Earth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, are only replenished over geological timescales. Large deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These metals and other elements are extracted by mining, a process which often brings environmental and health damage.

Earth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of organic waste. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends on dissolved nutrients washed down from the land. In 2019, of Earth's land surface consisted of forest and woodlands, was shrub and grassland, were used for animal feed production and grazing, and were cultivated as croplands. Of the 1214% of ice-free land that is used for croplands, 2 percentage points were irrigated in 2015. Humans use building materials to construct shelters.

Human activities have impacted Earth's environments. Through activities such as the burning of fossil fuels, humans have been increasing the amount of greenhouse gases in the atmosphere, altering Earth's energy budget and climate. It is estimated that global temperatures in the year 2020 were warmer than the preindustrial baseline. This increase in temperature, known as global warming, has contributed to the melting of glaciers, rising sea levels, increased risk of drought and wildfires, and migration of species to colder areas.

The concept of planetary boundaries was introduced to quantify humanity's impact on Earth. Of the nine identified boundaries, five have been crossed: Biosphere integrity, climate change, chemical pollution, destruction of wild habitats and the nitrogen cycle are thought to have passed the safe threshold. As of 2018, no country meets the basic needs of its population without transgressing planetary boundaries. It is thought possible to provide all basic physical needs globally within sustainable levels of resource use.

Human cultures have developed many views of the planet. The standard astronomical symbols of Earth are a quartered circle, , representing the four corners of the world, and a globus cruciger, . Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities. The Gaia hypothesis, developed in the mid-20th century, compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability.

Images of Earth taken from space, particularly during the Apollo program, have been credited with altering the way that people viewed the planet that they lived on, called the overview effect, emphasizing its beauty, uniqueness and apparent fragility. In particular, this caused a realization of the scope of effects from human activity on Earth's environment. Enabled by science, particularly Earth observation, humans have started to take action on environmental issues globally, acknowledging the impact of humans and the interconnectedness of Earth's environments.

Scientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in Ancient Greece by the idea of a spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. Earth was generally believed to be the center of the universe until the 16th century, when scientists first concluded that it was a moving object, one of the planets of the Solar System.

It was only during the 19th century that geologists realized Earth's age was at least many millions of years. Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old.

</math>, where "m" is the mass of Earth, "a" is an astronomical unit, and "M" is the mass of the Sun. So the radius in AU is about formula_1.</ref>


English Channel

The English Channel, also known as the Channel, is an arm of the Atlantic Ocean that separates Southern England from northern France. It links to the southern part of the North Sea by the Strait of Dover at its northeastern end. It is the busiest shipping area in the world.

It is about long and varies in width from at its widest to at its narrowest in the Strait of Dover. It is the smallest of the shallow seas around the continental shelf of Europe, covering an area of some .

The Channel was a key factor in Britain becoming a naval superpower and has been utilised by Britain as a natural defence mechanism to halt attempted invasions, such as in the Napoleonic Wars and in the Second World War.

The population around the English Channel is predominantly located on the English coast and the major languages spoken in this region are English and French.

The name first appears in Roman sources as (or , meaning the British Ocean or British Sea). Variations of this term were used by influential writers such as Ptolemy, and remained popular with British and continental authors well into the modern era. Other Latin names for the sea include (the Gaulish Ocean) which was used by Isidore of Seville in the sixth century.

The term "British Sea" is still used by speakers of Cornish and Breton, with the sea known to them as and respectively. While it is likely that these names derive from the Latin term, it is possible that they predate the arrival of the Romans in the area. The modern Welsh is often given as (the Lord's or Prince's Sea); however, this name originally described both the Channel and the North Sea combined.

Anglo-Saxon texts make reference to the sea as (South Sea), but this term fell out of favour, as later English authors followed the same conventions as their Latin and Norman contemporaries. One English name that did persist was the "Narrow Seas", a collective term for the channel and North Sea. As England (followed by Great Britain and the United Kingdom) claimed sovereignty over the sea, a Royal Navy Admiral was appointed with maintaining duties in the two seas. The office was maintained until 1822, when several European nations (including the United Kingdom) adopted a limit to territorial waters.

The word "channel" was first recorded in Middle English in the 13th century and was borrowed from the Old French word (a variant form of 'canal'). By the middle of the fifteenth century, an Italian map based on Ptolemy's description named the sea as "Britanicus Oceanus nunc Canalites Anglie" (British Ocean but now English Channel). The map is possibly the first recorded use of the term "English Channel" and the description suggests the name had recently been adopted.

In the sixteenth century, Dutch maps referred to the sea as the (English Channel) and by the 1590s, William Shakespeare used the word "Channel" in his history plays of Henry VI, suggesting that by that time, the name was popularly understood by English people.

By the eighteenth century, the name "English Channel" was in common usage in England. Following the Acts of Union 1707, this was replaced in official maps and documents with "British Channel" or "British Sea" for much of the next century. However, the term English Channel remained popular and was finally in official usage by the nineteenth century.

The French name has been used since at least the 17th century. The name is usually said to refer to the sleeve () shape of the Channel. Folk etymology has derived it from a Celtic word meaning 'channel' that is also the source of the name for the Minch in Scotland, but this name is not attested before the 17th century, and French and British sources of that time are clear about its etymology. The name in French has been directly adapted in other languages as either a calque, such as in Italian, or a direct borrowing, such as in Spanish.

The International Hydrographic Organization defines the limits of the English Channel as:

The Strait of Dover (), at the Channel's eastern end, is its narrowest point, while its widest point lies between Lyme Bay and the Gulf of Saint Malo, near its midpoint. Well on the continental shelf, it has an average depth of about at its widest; yet averages about between Dover and Calais, its notable sandbank hazard being Goodwin Sands. Eastwards from there the adjoining North Sea reduces to about across the Broad Fourteens (14 fathoms) where it lies over the southern cusp of the former land bridge between East Anglia and the Low Countries. The North Sea reaches much greater depths east of northern Britain. The Channel descends briefly to in the submerged valley of Hurd's Deep, west-northwest of Guernsey.
There are several major islands in the Channel, the most notable being the Isle of Wight off the English coast, and the Channel Islands, British Crown Dependencies off the coast of France. The coastline, particularly on the French shore, is deeply indented, with several small islands close to the coastline, including Chausey and Mont Saint-Michel. The Cotentin Peninsula on the French coast juts out into the Channel, with the wide Bay of the Seine () to its east. On the English side there is a small parallel strait, the Solent, between the Isle of Wight and the mainland. The Celtic Sea is to the west of the Channel.

The Channel acts as a funnel that amplifies the tidal range from less than a metre at sea in eastern places to more than 6 metres at in the Channel Islands the west coast of the Cotentin Peninsula and the north coast of Brittany in monthly spring tides. The time difference of about six hours between high water at the eastern and western limits of the Channel is indicative of the tidal range being amplified further by resonance. Amphidromic points are the Bay of Biscay and varying more in precise location in the far south of the North Sea, meaning both those associated eastern coasts repel the tides effectively, leaving the Strait of Dover as every six hours the natural bottleneck short of its consequent gravity-induced repulsion of the southward tide (surge) of the North Sea (equally from the Atlantic). The Channel does not experience, but its existence is necessary to explain the extent of North Sea storm surges, such as necessitate the Thames Barrier, Delta Works, Zuiderzee works (Afsluitdijk and other dams).

In the UK Shipping Forecast the Channel is divided into the following areas, from the east:

The Channel is of geologically recent origin, having been land for most of the Pleistocene period. Before the Devensian glaciation (the most recent glacial period, which ended around 10,000 years ago), Britain and Ireland were part of continental Europe, linked by an unbroken Weald–Artois anticline, a ridge that acted as a natural dam holding back a large freshwater pro-glacial lake in the Doggerland region, now submerged under the North Sea. During this period, the North Sea and almost all of the British Isles were covered by ice. The lake was fed by meltwater from the Baltic and from the Caledonian and Scandinavian ice sheets that joined to the north, blocking its exit. The sea level was about lower than it is today. Then, between 450,000 and 180,000 years ago, at least two catastrophic glacial lake outburst floods breached the Weald–Artois anticline. These contributed to creating some of the deepest part's of the channel such as Hurd's Deep.

The first flood of 450 thousand years ago would have lasted for several months, releasing as much as one million cubic metres of water per second. The flood started with large but localised waterfalls over the ridge, which excavated depressions now known as the "Fosses Dangeard". The flow eroded the retaining ridge, causing the rock dam to fail and releasing lake water into the Atlantic. After multiple episodes of changing sea level, during which the "Fosses Dangeard" were largely infilled by various layers of sediment, another catastrophic flood some 180,000 years ago carved a large bedrock-floored valley, the Lobourg Channel, some 500 m wide and 25 m deep, from the southern North Sea basin through the centre of the Straits of Dover and into the English Channel. It left streamlined islands, longitudinal erosional grooves, and other features characteristic of catastrophic megaflood events, still present on the sea floor and now revealed by high-resolution sonar. Through the scoured channel passed a river, the Channel River, which drained the combined Rhine and Thames westwards to the Atlantic.

The flooding destroyed the ridge that connected Britain to continental Europe, although a land connection across the southern North Sea would have existed intermittently at later times when periods of glaciation resulted in lowering of sea levels. At the end of the last glacial period, rising sea levels finally severed the last land connection.

As a busy shipping lane, the Channel experiences environmental problems following accidents involving ships with toxic cargo and oil spills. Indeed, over 40% of the UK incidents threatening pollution occur in or very near the Channel. One occurrence was the MSC "Napoli", which on 18 January 2007 was beached with nearly 1700 tonnes of dangerous cargo in Lyme Bay, a protected World Heritage Site coastline. The ship had been damaged and was en route to Portland Harbour.

The English Channel, despite being a busy shipping lane, remains in part a haven for wildlife. Atlantic oceanic species are more common in the westernmost parts of the channel, particularly to the west of Start Point, Devon, but can sometimes be found further east towards Dorset and the Isle of Wight. Seal sightings are becoming more common along the English Channel, with both Grey Seal and Harbour Seal recorded frequently.

The Channel, which delayed human reoccupation of Great Britain for more than 100,000 years, has in historic times been both an easy entry for seafaring people and a key natural defence, halting invading armies while in conjunction with control of the North Sea allowing Britain to blockade the continent. The most significant failed invasion threats came when the Dutch and Belgian ports were held by a major continental power, e.g. from the Spanish Armada in 1588, Napoleon during the Napoleonic Wars, and Nazi Germany during World War II. Successful invasions include the Roman conquest of Britain, the Norman Conquest in 1066 and the Glorious Revolution of 1688, while the concentration of excellent harbours in the Western Channel on Britain's south coast made possible the largest amphibious invasion in history, the Normandy Landings in 1944. Channel naval battles include the Battle of the Downs (1639), Battle of Dover (1652), the Battle of Portland (1653) and the Battle of La Hougue (1692).

In more peaceful times, the Channel served as a link joining shared cultures and political structures, particularly the huge Angevin Empire from 1135 to 1217. For nearly a thousand years, the Channel also provided a link between the Modern Celtic regions and languages of Cornwall and Brittany. Brittany was founded by Britons who fled Cornwall and Devon after Anglo-Saxon encroachment. In Brittany, there is a region known as "Cornouaille" (Cornwall) in French and "Kernev" in Breton. In ancient times there was also a "Domnonia" (Devon) in Brittany as well.

In February 1684, ice formed on the sea in a belt wide off the coast of Kent and wide on the French side.

Remnants of a mesolithic boatyard have been found on the Isle of Wight. Wheat was traded across the Channel about 8,000 years ago. "... Sophisticated social networks linked the Neolithic front in southern Europe to the Mesolithic peoples of northern Europe." The Ferriby Boats, Hanson Log Boats and the later Dover Bronze Age Boat could carry a substantial cross-Channel cargo.

Diodorus Siculus and Pliny both suggest trade between the rebel Celtic tribes of Armorica and Iron Age Britain flourished. In 55 BC Julius Caesar invaded, claiming that the Britons had aided the Veneti against him the previous year. He was more successful in 54 BC, but Britain was not fully established as part of the Roman Empire until completion of the invasion by Aulus Plautius in 43 AD. A brisk and regular trade began between ports in Roman Gaul and those in Britain. This traffic continued until the end of Roman rule in Britain in 410 AD, after which the early Anglo-Saxons left less clear historical records.

In the power vacuum left by the retreating Romans, the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea. Having already been used as mercenaries in Britain by the Romans, many people from these tribes crossed during the Migration Period, conquering and perhaps displacing the native Celtic populations.

The attack on Lindisfarne in 793 is generally considered the beginning of the Viking Age. For the next 250 years the Scandinavian raiders of Norway, Sweden, and Denmark dominated the North Sea, raiding monasteries, homes, and towns along the coast and along the rivers that ran inland. According to the "Anglo-Saxon Chronicle" they began to settle in Britain in 851. They continued to settle in the British Isles and the continent until around 1050, with some raids recorded along the channel coast of England, including at Wareham, Portland, near Weymouth and along the river Teign in Devon.

The fiefdom of Normandy was created for the Viking leader Rollo (also known as Robert of Normandy). Rollo had besieged Paris but in 911 entered vassalage to the king of the West Franks Charles the Simple through the Treaty of St.-Claire-sur-Epte. In exchange for his homage and fealty, Rollo legally gained the territory he and his Viking allies had previously conquered. The name "Normandy" reflects Rollo's Viking (i.e. "Northman") origins.

The descendants of Rollo and his followers adopted the local Gallo-Romance language and intermarried with the area's inhabitants and became the Normans – a Norman French-speaking mixture of Scandinavians, Hiberno-Norse, Orcadians, Anglo-Danish, and indigenous Franks and Gauls.

Rollo's descendant William, Duke of Normandy became king of England in 1066 in the Norman Conquest beginning with the Battle of Hastings, while retaining the fiefdom of Normandy for himself and his descendants. In 1204, during the reign of King John, mainland Normandy was taken from England by France under Philip II, while insular Normandy (the Channel Islands) remained under English control. In 1259, Henry III of England recognised the legality of French possession of mainland Normandy under the Treaty of Paris. His successors, however, often fought to regain control of mainland Normandy.

With the rise of William the Conqueror, the North Sea and Channel began to lose some of their importance. The new order oriented most of England and Scandinavia's trade south, toward the Mediterranean and the Orient.

Although the British surrendered claims to mainland Normandy and other French possessions in 1801, the monarch of the United Kingdom retains the title Duke of Normandy in respect to the Channel Islands. The Channel Islands (except for Chausey) are Crown Dependencies of the British Crown. Thus the Loyal toast in the Channel Islands is "Le roi, notre Duc" ("The King, our Duke"). The British monarch is understood to "not" be the Duke of Normandy in regards of the French region of Normandy described herein, by virtue of the Treaty of Paris of 1259, the surrender of French possessions in 1801, and the belief that the rights of succession to that title are subject to Salic Law which excludes inheritance through female heirs.

French Normandy was occupied by English forces during the Hundred Years' War in 1346–1360 and again in 1415–1450.

From the reign of Elizabeth I, English foreign policy concentrated on preventing invasion across the Channel by ensuring no major European power controlled the potential Dutch and Flemish invasion ports. Her climb to the pre-eminent sea power of the world began in 1588 as the attempted invasion of the Spanish Armada was defeated by the combination of outstanding naval tactics by the English and the Dutch under command of Charles Howard, 1st Earl of Nottingham with Sir Francis Drake second in command, and the following stormy weather. Over the centuries the Royal Navy slowly grew to be the most powerful in the world.
The building of the British Empire was possible only because the Royal Navy eventually managed to exercise unquestioned control over the seas around Europe, especially the Channel and the North Sea. During the Seven Years' War, France attempted to launch an invasion of Britain. To achieve this France needed to gain control of the Channel for several weeks, but was thwarted following the British naval victory at the Battle of Quiberon Bay in 1759 and was unsuccessful (The last French landing on English soil being in 1690 with a raid on Teignmouth, although the last French raid on British soil was a raid on Fishguard, Wales in 1797).

Another significant challenge to British domination of the seas came during the Napoleonic Wars. The Battle of Trafalgar took place off the coast of Spain against a combined French and Spanish fleet and was won by Admiral Horatio Nelson, ending Napoleon's plans for a cross-Channel invasion and securing British dominance of the seas for over a century.

The exceptional strategic importance of the Channel as a tool for blockading was recognised by the First Sea Lord Admiral Fisher in the years before World War I. "Five keys lock up the world! Singapore, the Cape, Alexandria, Gibraltar, Dover." However, on 25 July 1909 Louis Blériot made the first Channel crossing from Calais to Dover in an aeroplane. Blériot's crossing signalled a change in the function of the Channel as a barrier-moat for England against foreign enemies.

Because the "Kaiserliche Marine" surface fleet could not match the British Grand Fleet, the Germans developed submarine warfare, which was to become a far greater threat to Britain. The Dover Patrol, set up just before the war started, escorted cross-Channel troopships and prevented submarines from sailing in the Channel, obliging them to travel to the Atlantic via the much longer route around Scotland.

On land, the German army attempted to capture French Channel ports in the Race to the Sea but although the trenches are often said to have stretched "from the frontier of Switzerland to the English Channel", they reached the coast at the North Sea. Much of the British war effort in Flanders was a bloody but successful strategy to prevent the Germans reaching the Channel coast.

At the outset of the war, an attempt was made to block the path of U-boats through the Dover Strait with naval minefields. By February 1915, this had been augmented by a stretch of light steel netting called the Dover Barrage, which it was hoped would ensnare submerged submarines. After initial success, the Germans learned how to pass through the barrage, aided by the unreliability of British mines. On 31 January 1917, the Germans resumed unrestricted submarine warfare leading to dire Admiralty predictions that submarines would defeat Britain by November, the most dangerous situation Britain faced in either world war.

The Battle of Passchendaele in 1917 was fought to reduce the threat by capturing the submarine bases on the Belgian coast, though it was the introduction of convoys and not capture of the bases that averted defeat. In April 1918 the Dover Patrol carried out the Zeebrugge Raid against the U-boat bases. During 1917, the Dover Barrage was re-sited with improved mines and more effective nets, aided by regular patrols by small warships equipped with powerful searchlights. A German attack on these vessels resulted in the Battle of Dover Strait in 1917. A much more ambitious attempt to improve the barrage, by installing eight massive concrete towers across the strait was called the Admiralty M-N Scheme but only two towers were nearing completion at the end of the war and the project was abandoned.

The naval blockade in the Channel and North Sea was one of the decisive factors in the German defeat in 1918.

During the Second World War, naval activity in the European theatre was primarily limited to the Atlantic. During the Battle of France in May 1940, the German forces succeeded in capturing both Boulogne and Calais, thereby threatening the line of retreat for the British Expeditionary Force. By a combination of hard fighting and German indecision, the port of Dunkirk was kept open allowing 338,000 Allied troops to be evacuated in Operation Dynamo. More than 11,000 were evacuated from Le Havre during Operation Cycle and a further 192,000 were evacuated from ports further down the coast in Operation Aerial in June 1940. The early stages of the Battle of Britain featured German air attacks on Channel shipping and ports; despite these early successes against shipping the Germans did not win the air supremacy necessary for Operation Sealion, the projected cross-Channel invasion.

The Channel subsequently became the stage for an intensive coastal war, featuring submarines, minesweepers, and Fast Attack Craft.

The narrow waters of the Channel were considered too dangerous for major warships until the Normandy Landings with the exception, for the German Kriegsmarine, of the Channel Dash (Operation Cerberus) in February 1942, and this required the support of the Luftwaffe in Operation Thunderbolt.

Dieppe was the site of an ill-fated Dieppe Raid by Canadian and British armed forces. More successful was the later Operation Overlord (D-Day), a massive invasion of German-occupied France by Allied troops. Caen, Cherbourg, Carentan, Falaise and other Norman towns endured many casualties in the fight for the province, which continued until the closing of the so-called Falaise gap between Chambois and Montormel, then liberation of Le Havre.

The Channel Islands were the only part of the British Commonwealth occupied by Germany (excepting the part of Egypt occupied by the Afrika Korps at the time of the Second Battle of El Alamein, which was a protectorate and not part of the Commonwealth). The German occupation of 1940–1945 was harsh, with some island residents being taken for slave labour on the Continent; native Jews sent to concentration camps; partisan resistance and retribution; accusations of collaboration; and slave labour (primarily Russians and eastern Europeans) being brought to the islands to build fortifications. The Royal Navy blockaded the islands from time to time, particularly following the liberation of mainland Normandy in 1944. Intense negotiations resulted in some Red Cross humanitarian aid, but there was considerable hunger and privation during the occupation, particularly in the final months, when the population was close to starvation. The German troops on the islands surrendered on 9 May 1945, a day after the final surrender in mainland Europe.
There is significant public concern in the UK about illegal immigrants coming on small boats from France. Since 2018, the English Channel has seen a major increase in number of crossing.

The English Channel coast is far more densely populated on the English shore. The most significant towns and cities along both the English and French sides of the Channel (each with more than 20,000 inhabitants, ranked in descending order; populations are the urban area populations from the 1999 French census, 2001 UK census, and 2001 Jersey census) are as follows:




The two dominant cultures are English on the north shore of the Channel, French on the south. However, there are also a number of minority languages that are or were found on the shores and islands of the English Channel, which are listed here, with the Channel's name in the specific language following them.


Most other languages tend towards variants of the French and English forms, but notably Welsh has .

The Channel has traffic on both the UK–Europe and North Sea–Atlantic routes, and is the world's busiest seaway, with over 500 ships per day. Following an accident in January 1971 and a series of disastrous collisions with wreckage in February, the Dover TSS, the world's first radar-controlled traffic separation scheme, was set up by the International Maritime Organization. The scheme mandates that vessels travelling north must use the French side, travelling south the English side. There is a separation zone between the two lanes.

In December 2002 the MV "Tricolor", carrying £30m of luxury cars, sank northwest of Dunkirk after collision in fog with the container ship "Kariba". The cargo ship "Nicola" ran into the wreckage the next day. There was no loss of life.
The shore-based long-range traffic control system was updated in 2003 and there is a series of traffic separation systems in operation. Though the system is inherently incapable of reaching the levels of safety obtained from aviation systems such as the traffic collision avoidance system, it has reduced accidents to one or two per year.

Marine GPS systems allow ships to be preprogrammed to follow navigational channels accurately and automatically, further avoiding risk of running aground, but following the fatal collision between Dutch Aquamarine and Ash in October 2001, Britain's Marine Accident Investigation Branch (MAIB) issued a safety bulletin saying it believed that in these most unusual circumstances GPS use had actually contributed to the collision. The ships were maintaining a very precise automated course, one directly behind the other, rather than making use of the full width of the traffic lanes as a human navigator would.

A combination of radar difficulties in monitoring areas near cliffs, a failure of a CCTV system, incorrect operation of the anchor, the inability of the crew to follow standard procedures of using a GPS to provide early warning of the ship dragging the anchor and reluctance to admit the mistake and start the engine led to the MV "Willy" running aground in Cawsand Bay, Cornwall, in January 2002. The MAIB report makes it clear that the harbour controllers were informed of impending disaster by shore observers before the crew were themselves aware. The village of Kingsand was evacuated for three days because of the risk of explosion, and the ship was stranded for 11 days.

The ferry routes crossing the English Channel, include (have included):-

Many travellers cross beneath the Channel using the Channel Tunnel, first proposed in the early 19th century and finally opened in 1994, connecting the UK and France by rail. It is now routine to travel between Paris or Brussels and London on the Eurostar train. Freight trains also use the tunnel. Cars, coaches and lorries are carried on Eurotunnel Shuttle trains between Folkestone and Calais.

The coastal resorts of the Channel, such as Brighton and Deauville, inaugurated an era of aristocratic tourism in the early 19th century. Short trips across the Channel for leisure purposes are often referred to as Channel Hopping.

The Rampion Wind Farm is an offshore wind farm located in the Channel, off the coast of West Sussex. Other offshore wind farms planned on the French side of the Channel.

As one of the narrowest and most well-known international waterways lacking dangerous currents, the Channel has been the first objective of numerous innovative sea, air, and human powered crossing technologies.
Pre-historic people sailed from the mainland to England for millennia. At the end of the last Ice Age, lower sea levels even permitted walking across.

Pierre Andriel crossed the English Channel aboard the "Élise", ex the Scottish p.s. "Margery" in March 1816, one of the earliest seagoing voyages by steam ship.

The paddle steamer "Defiance", Captain William Wager, was the first steamer to cross the Channel to Holland, arriving there on 9 May 1816.

On 10 June 1821, English-built paddle steamer "Rob Roy" was the first passenger ferry to cross channel. The steamer was purchased subsequently by the French postal administration and renamed "Henri IV" and put into regular passenger service a year later. It was able to make the journey across the Straits of Dover in around three hours.

In June 1843, because of difficulties with Dover harbour, the South Eastern Railway company developed the Boulogne-sur-Mer-Folkestone route as an alternative to Calais-Dover. The first ferry crossed under the command of Captain Hayward.

In 1974 a Welsh coracle piloted by Bernard Thomas of Llechryd crossed the English Channel to France in 13 hours. The journey was undertaken to demonstrate how the Bull Boats of the Mandan Indians of North Dakota could have been copied from coracles introduced by Prince Madog in the 12th century.

The Mountbatten class hovercraft (MCH) entered commercial service in August 1968, initially between Dover and Boulogne but later also Ramsgate (Pegwell Bay) to Calais. The journey time Dover to Boulogne was roughly 35 minutes, with six trips per day at peak times. The fastest crossing of the English Channel by a commercial car-carrying hovercraft was 22 minutes, recorded by the "Princess Anne" MCH SR-N4 Mk3 on 14 September 1995,

The first aircraft to cross the Channel was a balloon in 1785, piloted by Jean Pierre François Blanchard (France) and John Jeffries (US).

Louis Blériot (France) piloted the first aeroplane to cross in 1909.

On 26 September 2008, Swiss Yves Rossy aka "Jetman" became the first person to cross the English Channel with a Jet Powered Wing,
He jumped from a Pilatus Porter over Calais, France, Rossy crossed the English Channel where he deployed his parachute and landed in Dover

The first flying car to have crossed the English Channel is a Pégase designed by the French company Vaylon on 14 June 2017. It was piloted by a Franco-Italian pilot Bruno Vezzoli. This crossing was carried out as part of the first road and air trip from Paris to London in a flying car. Pegase is a 2 seats road approved dune buggy and a powered paraglider. The takeoff was at 8:03 a.m. from Ambleteuse in the North of France and landing was at East Studdal, near Dover. The flight was completed in 1 hour and 15 minutes for a total distance covered of including over the English Channel at an altitude of .

On 12 June 1979, the first human-powered aircraft to cross the English Channel was the "Gossamer Albatross", built by American aeronautical engineer Dr. Paul B. MacCready's company AeroVironment, and piloted by Bryan Allen. The crossing was completed in 2 hours and 49 minutes.

On 4 August 2019, Frenchman Franky Zapata became the first person to cross the English Channel on a jet-powered Flyboard Air. The board was powered by a kerosene-filled backpack. Zapata made the journey in 22 minutes, having landed on a boat half-way across to refuel.

The sport of Channel swimming traces its origins to the latter part of the 19th century when Captain Matthew Webb made the first observed and unassisted swim across the Strait of Dover, swimming from England to France on 24–25 August 1875 in 21 hours 45 minutes.

Up to 1927, fewer than ten swimmers (including the first woman, Gertrude Ederle in 1926) had managed to successfully swim the English Channel, and many dubious claims had been made. The Channel Swimming Association (CSA) was founded to authenticate and ratify swimmers' claims to have swum the Channel and to verify crossing times. The CSA was dissolved in 1999 and was succeeded by two separate organisations: CSA Ltd (CSA) and the Channel Swimming and Piloting Federation (CSPF), both observe and authenticate cross-Channel swims in the Strait of Dover. The Channel Crossing Association was also set up to cater for unorthodox crossings.

The team with the most Channel swims to its credit is the Serpentine Swimming Club in London, followed by the international Sri Chinmoy Marathon Team.

As of 2023, 1,881 people had completed 2,428 verified solo crossings under the rules of the CSA and the CSPF. This includes 24 two-way crossings and three three-way crossings. 

The Strait of Dover is the busiest stretch of water in the world. It is governed by International Law as described in "Unorthodox Crossing of the Dover Strait Traffic Separation Scheme". It states: "[In] exceptional cases the French Maritime Authorities may grant authority for unorthodox craft to cross French territorial waters within the Traffic Separation Scheme when these craft set off from the British coast, on condition that the request for authorisation is sent to them with the opinion of the British Maritime Authorities."

The fastest verified swim of the Channel was by the Australian Trent Grimsey on 8 September 2012, in 6 hours 55 minutes, beating a swim of 2007. The female record is held by Yvetta Hlavacova of Czechia, on 7 hours, 25 minutes on 5 August 2006. Both records were from England to France.

There may have been some unreported swims of the Channel, by people intent on entering Britain in circumvention of immigration controls. A failed attempt to cross the Channel by two Syrian refugees in October 2014 came to light when their bodies were discovered on the shores of the North Sea in Norway and the Netherlands.

On 16 September 1965, two Amphicars crossed from Dover to Calais.

PLUTO was war-time fuel delivery project of "pipelines under the ocean" from England to France. Though plagued with technical difficulties during the Battle of Normandy, the pipelines delivered about 8% of the fuel requirements of the allied forces between D-Day and VE-Day.



Eiffel Tower

The Eiffel Tower ( ; ) is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower from 1887 to 1889.

Locally nicknamed ""La dame de fer"" (French for "Iron Lady"), it was constructed as the centerpiece of the 1889 World's Fair, and to crown the centennial anniversary of the French Revolution. Although initially criticised by some of France's leading artists and intellectuals for its design, it has since become a global cultural icon of France and one of the most recognisable structures in the world. The tower received 5,889,000 visitors in 2022. The Eiffel Tower is the most visited monument with an entrance fee in the world: 6.91 million people ascended it in 2015. It was designated a in 1964, and was named part of a UNESCO World Heritage Site ("Paris, Banks of the Seine") in 1991.

The tower is tall, about the same height as an 81- building, and the tallest structure in Paris. Its base is square, measuring on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest human-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure in the world to surpass both the 200-metre and 300-metre mark in height. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by . Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.

The tower has three levels for visitors, with restaurants on the first and second levels. The top level's upper platform is above the ground – the highest observation deck accessible to the public in the European Union. Tickets can be purchased to ascend by stairs or lift to the first and second levels. The climb from ground level to the first level is over 300 steps, as is the climb from the first level to the second, making the entire ascent a 600 step climb. Although there is a staircase to the top level, it is usually accessible only by lift. On this top, third level is a private apartment built for Gustave Eiffel's private use. He decorated it with furniture by Jean Lachaise and invited friends such as Thomas Edison.

The design of the Eiffel Tower is attributed to Maurice Koechlin and Émile Nouguier, two senior engineers working for the Compagnie des Établissements Eiffel. It was envisioned after discussion about a suitable centerpiece for the proposed 1889 Exposition Universelle, a world's fair to celebrate the centennial of the French Revolution. In May 1884, working at home, Koechlin made a sketch of their idea, described by him as "a great pylon, consisting of four lattice girders standing apart at the base and coming together at the top, joined together by metal trusses at regular intervals". Eiffel initially showed little enthusiasm, but he did approve further study, and the two engineers then asked Stephen Sauvestre, the head of the company's architectural department, to contribute to the design. Sauvestre added decorative arches to the base of the tower, a glass pavilion to the first level, and other embellishments.

The new version gained Eiffel's support: he bought the rights to the patent on the design which Koechlin, Nougier, and Sauvestre had taken out, and the design was put on display at the Exhibition of Decorative Arts in the autumn of 1884 under the company name. On 30 March 1885, Eiffel presented his plans to the ; after discussing the technical problems and emphasising the practical uses of the tower, he finished his talk by saying the tower would symbolise 

Little progress was made until 1886, when Jules Grévy was re-elected as president of France and Édouard Lockroy was appointed as minister for trade. A budget for the exposition was passed and, on 1 May, Lockroy announced an alteration to the terms of the open competition being held for a centrepiece to the exposition, which effectively made the selection of Eiffel's design a foregone conclusion, as entries had to include a study for a four-sided metal tower on the Champ de Mars. (A 300-metre tower was then considered a herculean engineering effort.) On 12 May, a commission was set up to examine Eiffel's scheme and its rivals, which, a month later, decided that all the proposals except Eiffel's were either impractical or lacking in details.

After some debate about the exact location of the tower, a contract was signed on 8 January 1887. Eiffel signed it acting in his own capacity rather than as the representative of his company, the contract granting him 1.5 million francs toward the construction costs: less than a quarter of the estimated 6.5 million francs. Eiffel was to receive all income from the commercial exploitation of the tower during the exhibition and for the next 20 years. He later established a separate company to manage the tower, putting up half the necessary capital himself.

A French bank, the "Crédit Industriel et Commercial" (CIC), helped finance the construction of the Eiffel Tower. During the period of the tower's construction, the CIC was acquiring funds from predatory loans to the National Bank of Haiti, some of which went towards the financing of the tower. These loans were connected to an indemnity controversy which saw France force Haiti's government to financially compensate French slaveowners for lost income as a result of the Haitian Revolution, and required Haiti to pay the CIC and its partner nearly half of all taxes collected on exports, "effectively choking off the nation's primary source of income". According to "The New York Times", "[at] a time when the [CIC] was helping finance one of the world's best-known landmarks, the Eiffel Tower, as a monument to French liberty, it was choking Haiti's economy, taking much of the young nation's income back to Paris and impairing its ability to start schools, hospitals and the other building blocks of an independent country."

The proposed tower had been a subject of controversy, drawing criticism from those who did not believe it was feasible and those who objected on artistic grounds. Prior to the Eiffel Tower's construction, no structure had ever been constructed to a height of 300 m, or even 200 m for that matter, and many people believed it was impossible. These objections were an expression of a long-standing debate in France about the relationship between architecture and engineering. It came to a head as work began at the Champ de Mars: a "Committee of Three Hundred" (one member for each metre of the tower's height) was formed, led by the prominent architect Charles Garnier and including some of the most important figures of the arts, such as William-Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet. A petition called "Artists against the Eiffel Tower" was sent to the Minister of Works and Commissioner for the Exposition, Adolphe Alphand, and it was published by "Le Temps" on 14 February 1887:

Gustave Eiffel responded to these criticisms by comparing his tower to the Egyptian pyramids: "My tower will be the tallest edifice ever erected by man. Will it not also be grandiose in its way? And why would something admirable in Egypt become hideous and ridiculous in Paris?" These criticisms were also dealt with by Édouard Lockroy in a letter of support written to Alphand, sardonically saying, "Judging by the stately swell of the rhythms, the beauty of the metaphors, the elegance of its delicate and precise style, one can tell this protest is the result of collaboration of the most famous writers and poets of our time", and he explained that the protest was irrelevant since the project had been decided upon months before, and construction on the tower was already under way.

Indeed, Garnier was a member of the Tower Commission that had examined the various proposals, and had raised no objection. Eiffel was similarly unworried, pointing out to a journalist that it was premature to judge the effect of the tower solely on the basis of the drawings, that the Champ de Mars was distant enough from the monuments mentioned in the protest for there to be little risk of the tower overwhelming them, and putting the aesthetic argument for the tower: "Do not the laws of natural forces always conform to the secret laws of harmony?"

Some of the protesters changed their minds when the tower was built; others remained unconvinced. Guy de Maupassant supposedly ate lunch in the tower's restaurant every day because it was the one place in Paris where the tower was not visible.

By 1918, it had become a symbol of Paris and of France after Guillaume Apollinaire wrote a nationalist poem in the shape of the tower (a calligram) to express his feelings about the war against Germany. Today, it is widely considered to be a remarkable piece of structural art, and is often featured in films and literature.

Work on the foundations started on 28 January 1887. Those for the east and south legs were straightforward, with each leg resting on four concrete slabs, one for each of the principal girders of each leg. The west and north legs, being closer to the river Seine, were more complicated: each slab needed two piles installed by using compressed-air caissons long and in diameter driven to a depth of to support the concrete slabs, which were thick. Each of these slabs supported a block of limestone with an inclined top to bear a supporting shoe for the ironwork.

Each shoe was anchored to the stonework by a pair of bolts in diameter and long. The foundations were completed on 30 June, and the erection of the ironwork began. The visible work on-site was complemented by the enormous amount of exacting preparatory work that took place behind the scenes: the drawing office produced 1,700 general drawings and 3,629 detailed drawings of the 18,038 different parts needed. The task of drawing the components was complicated by the complex angles involved in the design and the degree of precision required: the position of rivet holes was specified to within and angles worked out to one second of arc. The finished components, some already riveted together into sub-assemblies, arrived on horse-drawn carts from a factory in the nearby Parisian suburb of Levallois-Perret and were first bolted together, with the bolts being replaced with rivets as construction progressed. No drilling or shaping was done on site: if any part did not fit, it was sent back to the factory for alteration. In all, 18,038 pieces were joined using 2.5 million rivets.

At first, the legs were constructed as cantilevers, but about halfway to the first level construction was paused to create a substantial timber scaffold. This renewed concerns about the structural integrity of the tower, and sensational headlines such as "Eiffel Suicide!" and "Gustave Eiffel Has Gone Mad: He Has Been Confined in an Asylum" appeared in the tabloid press. Multiple famous artists of that time, Charles Garnier and Alexander Dumas, thought poorly of the newly made tower. Charles Garnier thought it was a "truly tragic street lamp". Alexander Dumas said that it was like "Odius shadow of the odious column built of rivets and iron plates extending like a black blot". There were multiple protests over the style and the reasoning of placing it in the middle of Paris. At this stage, a small "creeper" crane designed to move up the tower was installed in each leg. They made use of the guides for the lifts which were to be fitted in the four legs. The critical stage of joining the legs at the first level was completed by the end of March 1888. Although the metalwork had been prepared with the utmost attention to detail, provision had been made to carry out small adjustments to precisely align the legs; hydraulic jacks were fitted to the shoes at the base of each leg, capable of exerting a force of 800 tonnes, and the legs were intentionally constructed at a slightly steeper angle than necessary, being supported by sandboxes on the scaffold. Although construction involved 300 on-site employees, due to Eiffel's safety precautions and the use of movable gangways, guardrails and screens, only one person died.

The main structural work was completed at the end of March 1889 and, on 31 March, Eiffel celebrated by leading a group of government officials, accompanied by representatives of the press, to the top of the tower. Because the lifts were not yet in operation, the ascent was made by foot, and took over an hour, with Eiffel stopping frequently to explain various features. Most of the party chose to stop at the lower levels, but a few, including the structural engineer, Émile Nouguier, the head of construction, Jean Compagnon, the President of the City Council, and reporters from "Le Figaro" and "Le Monde Illustré", completed the ascent. At 2:35 pm, Eiffel hoisted a large Tricolour to the accompaniment of a 25-gun salute fired at the first level.

There was still work to be done, particularly on the lifts and facilities, and the tower was not opened to the public until nine days after the opening of the exposition on 6 May; even then, the lifts had not been completed. The tower was an instant success with the public, and nearly 30,000 visitors made the 1,710-step climb to the top before the lifts entered service on 26 May.
Tickets cost 2 francs for the first level, 3 for the second, and 5 for the top, with half-price admission on Sundays, and by the end of the exhibition there had been 1,896,987 visitors.

After dark, the tower was lit by hundreds of gas lamps, and a beacon sent out three beams of red, white and blue light. Two searchlights mounted on a circular rail were used to illuminate various buildings of the exposition. The daily opening and closing of the exposition were announced by a cannon at the top.

On the second level, the French newspaper "Le Figaro" had an office and a printing press, where a special souvenir edition, "Le Figaro de la Tour", was made. There was also a pâtisserie.

At the top, there was a post office where visitors could send letters and postcards as a memento of their visit. Graffitists were also catered for: sheets of paper were mounted on the walls each day for visitors to record their impressions of the tower. Gustave Eiffel described the collection of responses as "truly curious".

Famous visitors to the tower included the Prince of Wales, Sarah Bernhardt, "Buffalo Bill" Cody (his Wild West show was an attraction at the exposition) and Thomas Edison. Eiffel invited Edison to his private apartment at the top of the tower, where Edison presented him with one of his phonographs, a new invention and one of the many highlights of the exposition. Edison signed the guestbook with this message: 

Eiffel made use of his apartment at the top of the tower to carry out meteorological observations, and also used the tower to perform experiments on the action of air resistance on falling bodies.

Eiffel had a permit for the tower to stand for 20 years. It was to be dismantled in 1909, when its ownership would revert to the City of Paris. The city had planned to tear it down (part of the original contest rules for designing a tower was that it should be easy to dismantle) but as the tower proved to be valuable for many innovations in the early 20th century, particularly radio telegraphy, it was allowed to remain after the expiry of the permit, and from 1910 it also became part of the International Time Service.

For the 1900 "Exposition Universelle", the lifts in the east and west legs were replaced by lifts running as far as the second level constructed by the French firm Fives-Lille. These had a compensating mechanism to keep the floor level as the angle of ascent changed at the first level, and were driven by a similar hydraulic mechanism as the Otis lifts, although this was situated at the base of the tower. Hydraulic pressure was provided by pressurised accumulators located near this mechanism. At the same time the lift in the north pillar was removed and replaced by a staircase to the first level. The layout of both first and second levels was modified, with the space available for visitors on the second level. The original lift in the south pillar was removed 13 years later.

On 19 October 1901, Alberto Santos-Dumont, flying his No.6 airship, won a 100,000-franc prize offered by Henri Deutsch de la Meurthe for the first person to make a flight from St. Cloud to the Eiffel Tower and back in less than half an hour.

In 1910, Father Theodor Wulf measured radiant energy at the top and bottom of the tower. He found more at the top than expected, incidentally discovering what are known today as cosmic rays. Two years later, on 4 February 1912, Austrian tailor Franz Reichelt died after jumping from the first level of the tower (a height of 57 m) to demonstrate his parachute design. In 1914, at the outbreak of World War I, a radio transmitter located in the tower jammed German radio communications, seriously hindering their advance on Paris and contributing to the Allied victory at the First Battle of the Marne. From 1925 to 1934, illuminated signs for Citroën adorned three of the tower's sides, making it the tallest advertising space in the world at the time. In April 1935, the tower was used to make experimental low-resolution television transmissions, using a shortwave transmitter of 200 watts power. On 17 November, an improved 180-line transmitter was installed.

On two separate but related occasions in 1925, the con artist Victor Lustig "sold" the tower for scrap metal. A year later, in February 1926, pilot Leon Collet was killed trying to fly under the tower. His aircraft became entangled in an aerial belonging to a wireless station. A bust of Gustave Eiffel by Antoine Bourdelle was unveiled at the base of the north leg on 2 May 1929. In 1930, the tower lost the title of the world's tallest structure when the Chrysler Building in New York City was completed. In 1938, the decorative arcade around the first level was removed.

Upon the German occupation of Paris in 1940, the lift cables were cut by the French. The tower was closed to the public during the occupation and the lifts were not repaired until 1946. In 1940, German soldiers had to climb the tower to hoist a swastika-centered Reichskriegsflagge, but the flag was so large it blew away just a few hours later, and was replaced by a smaller one. When visiting Paris, Hitler chose to stay on the ground. When the Allies were nearing Paris in August 1944, Hitler ordered General Dietrich von Choltitz, the military governor of Paris, to demolish the tower along with the rest of the city. Von Choltitz disobeyed the order. On 25 August, before the Germans had been driven out of Paris, the German flag was replaced with a Tricolour by two men from the French Naval Museum, who narrowly beat three men led by Lucien Sarniguet, who had lowered the Tricolour on 13 June 1940 when Paris fell to the Germans.

A fire started in the television transmitter on 3 January 1956, damaging the top of the tower. Repairs took a year, and in 1957, the present radio aerial was added to the top. In 1964, the Eiffel Tower was officially declared to be a historical monument by the Minister of Cultural Affairs, André Malraux. A year later, an additional lift system was installed in the north pillar.

According to interviews, in 1967, Montreal Mayor Jean Drapeau negotiated a secret agreement with Charles de Gaulle for the tower to be dismantled and temporarily relocated to Montreal to serve as a landmark and tourist attraction during Expo 67. The plan was allegedly vetoed by the company operating the tower out of fear that the French government could refuse permission for the tower to be restored in its original location.

In 1982, the original lifts between the second and third levels were replaced after 97 years in service. These had been closed to the public between November and March because the water in the hydraulic drive tended to freeze. The new cars operate in pairs, with one counterbalancing the other, and perform the journey in one stage, reducing the journey time from eight minutes to less than two minutes. At the same time, two new emergency staircases were installed, replacing the original spiral staircases. In 1983, the south pillar was fitted with an electrically driven Otis lift to serve the Jules Verne restaurant. The Fives-Lille lifts in the east and west legs, fitted in 1899, were extensively refurbished in 1986. The cars were replaced, and a computer system was installed to completely automate the lifts. The motive power was moved from the water hydraulic system to a new electrically driven oil-filled hydraulic system, and the original water hydraulics were retained solely as a counterbalance system. A service lift was added to the south pillar for moving small loads and maintenance personnel three years later.

Robert Moriarty flew a Beechcraft Bonanza under the tower on 31 March 1984. In 1987, A. J. Hackett made one of his first bungee jumps from the top of the Eiffel Tower, using a special cord he had helped develop. Hackett was arrested by the police. On 27 October 1991, Thierry Devaux, along with mountain guide Hervé Calvayrac, performed a series of acrobatic figures while bungee jumping from the second floor of the tower. Facing the Champ de Mars, Devaux used an electric winch between figures to go back up to the second floor. When firemen arrived, he stopped after the sixth jump.

For its "Countdown to the Year 2000" celebration on 31 December 1999, flashing lights and high-powered searchlights were installed on the tower. During the last three minutes of the year, the lights were turned on starting from the base of the tower and continuing to the top to welcome 2000 with a huge fireworks show. An exhibition above a cafeteria on the first floor commemorates this event. The searchlights on top of the tower made it a beacon in Paris's night sky, and 20,000 flashing bulbs gave the tower a sparkly appearance for five minutes every hour on the hour.

The lights sparkled blue for several nights to herald the new millennium on 31 December 2000. The sparkly lighting continued for 18 months until July 2001. The sparkling lights were turned on again on 21 June 2003, and the display was planned to last for 10 years before they needed replacing.

The tower received its th guest on 28 November 2002. The tower has operated at its maximum capacity of about 7 million visitors per year since 2003. In 2004, the Eiffel Tower began hosting a seasonal ice rink on the first level. A glass floor was installed on the first level during the 2014 refurbishment.
The puddle iron (wrought iron) of the Eiffel Tower weighs 7,300 tonnes, and the addition of lifts, shops and antennae have brought the total weight to approximately 10,100 tonnes. As a demonstration of the economy of design, if the 7,300 tonnes of metal in the structure were melted down, it would fill the square base, on each side, to a depth of only assuming the density of the metal to be 7.8 tonnes per cubic metre. Additionally, a cubic box surrounding the tower (324 m × 125 m × 125 m) would contain  tonnes of air, weighing almost as much as the iron itself. Depending on the ambient temperature, the top of the tower may shift away from the sun by up to due to thermal expansion of the metal on the side facing the sun.

When it was built, many were shocked by the tower's daring form. Eiffel was accused of trying to create something artistic with no regard to the principles of engineering. However, Eiffel and his team – experienced bridge builders – understood the importance of wind forces, and knew that if they were going to build the tallest structure in the world, they had to be sure it could withstand them. In an interview with the newspaper "Le Temps" published on 14 February 1887, Eiffel said:

He used graphical methods to determine the strength of the tower and empirical evidence to account for the effects of wind, rather than a mathematical formula. Close examination of the tower reveals a basically exponential shape. All parts of the tower were overdesigned to ensure maximum resistance to wind forces. The top half was even assumed to have no gaps in the latticework. In the years since it was completed, engineers have put forward various mathematical hypotheses in an attempt to explain the success of the design. The most recent, devised in 2004 after letters sent by Eiffel to the French Society of Civil Engineers in 1885 were translated into English, is described as a non-linear integral equation based on counteracting the wind pressure on any point of the tower with the tension between the construction elements at that point.

The Eiffel Tower sways by up to in the wind.

The four columns of the tower each house access stairs and elevators to the first two floors, while at the south column only the elevator to the second floor restaurant is publicly accessible.

The first floor is publicly accessible by elevator or stairs. 
When originally built, the first level contained three restaurants – one French, one Russian and one Flemish — and an "Anglo-American Bar". After the exposition closed, the Flemish restaurant was converted to a 250-seat theatre. Today there is the restaurant and other facilities.

The second floor is publicly accessible by elevator or stairs and has a restaurant called , a gourmet restaurant with its own lift going up from the south column to the second level. This restaurant has one star in the Michelin Red Guide. It was run by the multi-Michelin star chef Alain Ducasse from 2007 to 2017. As of May 2019, it is managed by three-star chef Frédéric Anton. It owes its name to the famous science-fiction writer Jules Verne.

The third floor is the top floor, publicly accessible by elevator.

Originally there were laboratories for various experiments, and a small apartment reserved for Gustave Eiffel to entertain guests, which is now open to the public, complete with period decorations and lifelike mannequins of Eiffel and some of his notable guests.

From 1937 until 1981, there was a restaurant near the top of the tower. It was removed due to structural considerations; engineers had determined it was too heavy and was causing the tower to sag. This restaurant was sold to an American restaurateur and transported to New York and then New Orleans. It was rebuilt on the edge of New Orleans' Garden District as a restaurant and later event hall. Today there is a champagne bar.

The arrangement of the lifts has been changed several times during the tower's history. Given the elasticity of the cables and the time taken to align the cars with the landings, each lift, in normal service, takes an average of 8 minutes and 50 seconds to do the round trip, spending an average of 1 minute and 15 seconds at each level. The average journey time between levels is 1 minute. The original hydraulic mechanism is on public display in a small museum at the base of the east and west legs. Because the mechanism requires frequent lubrication and maintenance, public access is often restricted. The rope mechanism of the north tower can be seen as visitors exit the lift.

Equipping the tower with adequate and safe passenger lifts was a major concern of the government commission overseeing the Exposition. Although some visitors could be expected to climb to the first level, or even the second, lifts clearly had to be the main means of ascent.

Constructing lifts to reach the first level was relatively straightforward: the legs were wide enough at the bottom and so nearly straight that they could contain a straight track, and a contract was given to the French company Roux, Combaluzier & Lepape for two lifts to be fitted in the east and west legs. Roux, Combaluzier & Lepape used a pair of endless chains with rigid, articulated links to which the car was attached. Lead weights on some links of the upper or return sections of the chains counterbalanced most of the car's weight. The car was pushed up from below, not pulled up from above: to prevent the chain buckling, it was enclosed in a conduit. At the bottom of the run, the chains passed around diameter sprockets. Smaller sprockets at the top guided the chains.

Installing lifts to the second level was more of a challenge because a straight track was impossible. No French company wanted to undertake the work. The European branch of Otis Brothers & Company submitted a proposal but this was rejected: the fair's charter ruled out the use of any foreign material in the construction of the tower. The deadline for bids was extended but still no French companies put themselves forward, and eventually the contract was given to Otis in July 1887. Otis were confident they would eventually be given the contract and had already started creating designs.

The car was divided into two superimposed compartments, each holding 25 passengers, with the lift operator occupying an exterior platform on the first level. Motive power was provided by an inclined hydraulic ram long and in diameter in the tower leg with a stroke of : this moved a carriage carrying six sheaves. Five fixed sheaves were mounted higher up the leg, producing an arrangement similar to a block and tackle but acting in reverse, multiplying the stroke of the piston rather than the force generated. The hydraulic pressure in the driving cylinder was produced by a large open reservoir on the second level. After being exhausted from the cylinder, the water was pumped back up to the reservoir by two pumps in the machinery room at the base of the south leg. This reservoir also provided power to the lifts to the first level.

The original lifts for the journey between the second and third levels were supplied by Léon Edoux. A pair of hydraulic rams were mounted on the second level, reaching nearly halfway up to the third level. One lift car was mounted on top of these rams: cables ran from the top of this car up to sheaves on the third level and back down to a second car. Each car travelled only half the distance between the second and third levels and passengers were required to change lifts halfway by means of a short gangway. The 10-ton cars each held 65 passengers.

Gustave Eiffel engraved on the tower the names of 72 French scientists, engineers and mathematicians in recognition of their contributions to the building of the tower. Eiffel chose this "invocation of science" because of his concern over the artists' protest. At the beginning of the 20th century, the engravings were painted over, but they were restored in 1986–87 by the , a company operating the tower.

The tower is painted in three shades: lighter at the top, getting progressively darker towards the bottom to complement the Parisian sky. It was originally reddish brown; this changed in 1968 to a bronze colour known as "Eiffel Tower Brown". In what is expected to be a temporary change, the tower is being painted gold in commemoration of the upcoming 2024 Summer Olympics in Paris.

The only non-structural elements are the four decorative grill-work arches, added in Sauvestre's sketches, which served to make the tower look more substantial and to make a more impressive entrance to the exposition.

A pop-culture movie cliché is that the view from a Parisian window always includes the tower. In reality, since zoning restrictions limit the height of most buildings in Paris to seven storeys, only a small number of tall buildings have a clear view of the tower.

Maintenance of the tower includes applying 60 tons of paint every seven years to prevent it from rusting. The tower has been completely repainted at least 19 times since it was built. Lead paint was still being used as recently as 2001 when the practice was stopped out of concern for the environment.

The tower has been used for making radio transmissions since the beginning of the 20th century. Until the 1950s, sets of aerial wires ran from the cupola to anchors on the Avenue de Suffren and Champ de Mars. These were connected to longwave transmitters in small bunkers. In 1909, a permanent underground radio centre was built near the south pillar, which still exists today. On 20 November 1913, the Paris Observatory, using the Eiffel Tower as an aerial, exchanged wireless signals with the United States Naval Observatory, which used an aerial in Arlington County, Virginia. The object of the transmissions was to measure the difference in longitude between Paris and Washington, D.C. Today, radio and digital television signals are transmitted from the Eiffel Tower.

A television antenna was first installed on the tower in 1957, increasing its height by . Work carried out in 2000 added a further , giving the current height of . Analogue television signals from the Eiffel Tower ceased on 8 March 2011.

The pinnacle height of the Eiffel Tower has changed multiple times over the years as described in the chart below.

The Eiffel Tower was the world's tallest structure when completed in 1889, a distinction it retained until 1929 when the Chrysler Building in New York City was topped out. The tower also lost its standing as the world's tallest tower to the Tokyo Tower in 1958 but retains its status as the tallest freestanding (non-guyed) structure in France.

The nearest Paris Métro station is Bir-Hakeim and the nearest RER station is Champ de Mars-Tour Eiffel. The tower itself is located at the intersection of the quai Branly and the Pont d'Iéna.
More than 300 million people have visited the tower since it was completed in 1889. In 2015, there were 6.91 million visitors. The tower is the most-visited paid monument in the world. An average of 25,000 people ascend the tower every day (which can result in long queues).

The tower and its image have been in the public domain since 1993, 70 years after Eiffel's death. In June 1990 a French court ruled that a special lighting display on the tower in 1989 to mark the tower's 100th anniversary was an "original visual creation" protected by copyright. The Court of Cassation, France's judicial court of last resort, upheld the ruling in March 1992. The (SETE) now considers any illumination of the tower to be a separate work of art that falls under copyright. As a result, the SNTE alleges that it is illegal to publish contemporary photographs of the lit tower at night without permission in France and some other countries for commercial use. For this reason, it is often rare to find images or videos of the lit tower at night on stock image sites, and media outlets rarely broadcast images or videos of it.

The imposition of copyright has been controversial. The Director of Documentation for what was then called the (SNTE), Stéphane Dieu, commented in 2005: "It is really just a way to manage commercial use of the image, so that it isn't used in ways [of which] we don't approve". SNTE made over €1 million from copyright fees in 2002. However, it could also be used to restrict the publication of tourist photographs of the tower at night, as well as hindering non-profit and semi-commercial publication of images of the illuminated tower.

The copyright claim itself has never been tested in courts to date, according to a 2014 article in the "Art Law Journal", and there has never been an attempt to track down millions of people who have posted and shared their images of the illuminated tower on the Internet worldwide. It added, however, that permissive situation may arise on commercial use of such images, like in a magazine, on a film poster, or on product packaging.

French doctrine and jurisprudence allows pictures incorporating a copyrighted work as long as their presence is incidental or accessory to the subject being represented, a reasoning akin to the "de minimis" rule. Therefore, SETE may be unable to claim copyright on photographs of Paris which happen to include the lit tower.

As one of the most famous landmarks in the world, the Eiffel Tower has been the inspiration for the creation of many replicas and similar towers. An early example is Blackpool Tower in England. The mayor of Blackpool, Sir John Bickerstaffe, was so impressed on seeing the Eiffel Tower at the 1889 exposition that he commissioned a similar tower to be built in his town. It opened in 1894 and is tall. Tokyo Tower in Japan, built as a communications tower in 1958, was also inspired by the Eiffel Tower.

There are various scale models of the tower in the United States, including a half-scale version at the Paris Las Vegas, Nevada, one in Paris, Texas built in 1993, and two 1:3 scale models at Kings Island, located in Mason, Ohio, and Kings Dominion, Virginia, amusement parks opened in 1972 and 1975 respectively. Two 1:3 scale models can be found in China, one in Durango, Mexico that was donated by the local French community, and several across Europe.

In 2011, the TV show "Pricing the Priceless" on the National Geographic Channel speculated that a full-size replica of the tower would cost approximately US$480 million to build. This would be more than ten times the cost of the original (nearly 8 million in 1890 Francs; ~US$40 million in 2018 dollars).




Ethical egoism

In ethical philosophy, ethical egoism is the normative position that moral agents "ought" to act in their own self-interest. It differs from psychological egoism, which claims that people "can only" act in their self-interest. Ethical egoism also differs from rational egoism, which holds that it is "rational" to act in one's self-interest.
Ethical egoism holds, therefore, that actions whose consequences will benefit the doer are ethical.

Ethical egoism contrasts with ethical altruism, which holds that moral agents have an obligation to help others. Egoism and altruism both contrast with ethical utilitarianism, which holds that a moral agent should treat one's self (also known as the subject) with no higher regard than one has for others (as egoism does, by elevating self-interests and "the self" to a status not granted to others). But it also holds that one is not obligated to sacrifice one's own interests (as altruism does) to help others' interests, so long as one's own interests (i.e., one's own desires or well-being) are substantially equivalent to the others' interests and well-being, but they have the choice to do so. Egoism, utilitarianism, and altruism are all forms of consequentialism, but egoism and altruism contrast with utilitarianism, in that egoism and altruism are both agent-focused forms of consequentialism (i.e., subject-focused or subjective). However, utilitarianism is held to be agent-neutral (i.e., objective and impartial): it does not treat the subject's (i.e., the self's, i.e., the moral "agent's") own interests as being more or less important than the interests, desires, or well-being of others.

Ethical egoism does not, however, require moral agents to harm the interests and well-being of others when making moral deliberation; e.g., what is in an agent's self-interest may be incidentally detrimental, beneficial, or neutral in its effect on others. Individualism allows for others' interest and well-being to be disregarded or not, as long as what is chosen is efficacious in satisfying the self-interest of the agent. Nor does ethical egoism necessarily entail that, in pursuing self-interest, one ought always to do what one wants to do; e.g., in the long term, the fulfillment of short-term desires may prove detrimental to the self. Fleeting pleasure, then, takes a back seat to protracted eudaimonia. In the words of James Rachels, "Ethical egoism ... endorses selfishness, but it doesn't endorse foolishness."

Ethical egoism is often used as the philosophical basis for support of right-libertarianism and individualist anarchism. These are political positions based partly on a belief that individuals should not coercively prevent others from exercising freedom of action.

Ethical egoism can be broadly divided into three categories: individual, personal, and universal. An "individual ethical egoist" would hold that all people should do whatever benefits "my" ("the individual's")" "self-interest; a "personal ethical egoist" would hold that they should act in "their" self-interest, but would make no claims about what anyone else ought to do; a "universal ethical egoist" would argue that everyone should act in ways that are in their self-interest.

Ethical egoism was introduced by the philosopher Henry Sidgwick in his book "The Methods of Ethics", written in 1874. Sidgwick compared egoism to the philosophy of utilitarianism, writing that whereas utilitarianism sought to maximize overall pleasure, egoism focused only on maximizing individual pleasure.

Philosophers before Sidgwick have also retroactively been identified as ethical egoists. One ancient example is the philosophy of Yang Zhu (4th century BC), Yangism, who views "wei wo", or "everything for myself", as the only virtue necessary for self-cultivation. Ancient Greek philosophers like Plato, Aristotle and the Stoics were exponents of virtue ethics, and "did not accept the formal principle that whatever the good is, we should seek only our own good, or prefer it to the good of others." However, the beliefs of the Cyrenaics have been referred to as a "form of egoistic hedonism", and while some refer to Epicurus' hedonism as a form of virtue ethics, others argue his ethics are more properly described as ethical egoism.

Philosopher James Rachels, in an essay that takes as its title the theory's name, outlines the three arguments most commonly touted in its favor:

It has been argued that extreme ethical egoism is self-defeating. Faced with a situation of limited resources, egoists would consume as much of the resource as they could, making the overall situation worse for everybody. Egoists may respond that if the situation becomes worse for everybody, that would include the egoist, so it is not, in fact, in their rational self-interest to take things to such extremes. However, the (unregulated) tragedy of the commons and the (one off) prisoner's dilemma are cases in which, on the one hand, it is rational for an individual to seek to take as much as possible "even though" that makes things worse for everybody, and on the other hand, those cases are not self-refuting since that behaviour remains rational "even though" it is ultimately self-defeating, i.e. self-defeating does not imply self-refuting. Egoists might respond that a tragedy of the commons, however, assumes some degree of public land. That is, a commons forbidding homesteading requires regulation. Thus, an argument against the tragedy of the commons, in this belief system, is fundamentally an argument for private property rights and the system that recognizes both property rights and rational self-interest—capitalism. More generally, egoists might say that an increasing respect for individual rights uniquely allows for increasing wealth creation and increasing usable resources despite a fixed amount of raw materials (e.g. the West pre-1776 versus post-1776, East versus West Germany, Hong Kong versus mainland China, North versus South Korea, etc.).

It is not clear how to apply a private ownership model to many examples of "commons", however. Examples include large fisheries, the atmosphere and the ocean.

Some perhaps decisive problems with ethical egoism have been pointed out.

One is that an ethical egoist would not want ethical egoism to be universalized: as it would be in the egoist's best self-interest if others acted altruistically towards them, they wouldn't want them to act egoistically; however, that is what they consider to be morally binding. Their moral principles would demand of others not to follow them, which can be considered self-defeating and leads to the question: "How can ethical egoism be considered morally binding if its advocates do not want it to be universally applied?"

Another objection (e.g. by James Rachels) states that the distinction ethical egoism makes between "yourself" and "the rest" – demanding to view the interests of "yourself" as more important – is arbitrary, as no justification for it can be offered; considering that the merits and desires of "the rest" are comparable to those of "yourself" while lacking a justifiable distinction, Rachels concludes that "the rest" should be given the same moral consideration as "yourself".

The term "ethical egoism" has been applied retroactively to philosophers such as Bernard de Mandeville and to many other materialists of his generation, although none of them declared themselves to be egoists. Note that materialism does not necessarily imply egoism, as indicated by Karl Marx, and the many other materialists who espoused forms of collectivism. It has been argued that ethical egoism can lend itself to individualist anarchism such as that of Benjamin Tucker, or the combined anarcho-communism and egoism of Emma Goldman, both of whom were proponents of many egoist ideas put forward by Max Stirner. In this context, egoism is another way of describing the sense that the common good should be enjoyed by all. However, most notable anarchists in history have been less radical, retaining altruism and a sense of the importance of the individual that is appreciable but does not go as far as egoism. Recent trends to greater appreciation of egoism within anarchism tend to come from less classical directions such as post-left anarchy or Situationism (e.g. Raoul Vaneigem). Egoism has also been referenced by anarcho-capitalists, such as Murray Rothbard.

Philosopher Max Stirner, in his book "The Ego and Its Own", was the first philosopher to call himself an egoist, though his writing makes clear that he desired not a new idea of morality (ethical egoism), but rather a rejection of morality (amoralism), as a nonexistent and limiting "spook"; for this, Stirner has been described as the first individualist anarchist. Other philosophers, such as Thomas Hobbes and David Gauthier, have argued that the conflicts which arise when people each pursue their own ends can be resolved for the best of each individual only if they all voluntarily forgo some of their aims—that is, one's self-interest is often best pursued by allowing others to pursue their self-interest as well so that liberty is equal among individuals. Sacrificing one's short-term self-interest to maximize one's long-term self-interest is one form of "rational self-interest" which is the idea behind most philosophers' advocacy of ethical egoism. Egoists have also argued that one's actual interests are not immediately obvious, and that the pursuit of self-interest involves more than merely the acquisition of some good, but the "maximizing" of one's chances of survival and/or happiness.

Philosopher Friedrich Nietzsche suggested that egoistic or "life-affirming" behavior stimulates jealousy or "ressentiment" in others, and that this is the psychological motive for the altruism in Christianity. Sociologist Helmut Schoeck similarly considered envy the motive of collective efforts by society to reduce the disproportionate gains of successful individuals through moral or legal constraints, with altruism being primary among these. In addition, Nietzsche (in "Beyond Good and Evil") and Alasdair MacIntyre (in "After Virtue") have pointed out that the ancient Greeks did not associate morality with altruism in the way that post-Christian Western civilization has done.
Aristotle's view is that we have duties to ourselves as well as to other people (e.g. friends) and to the "polis" as a whole. The same is true for Thomas Aquinas, Christian Wolff and Immanuel Kant, who claim that there are duties to ourselves as Aristotle did, although it has been argued that, for Aristotle, the duty to one's self is primary.

Ayn Rand argued that there is a positive harmony of interests among free, rational humans, such that no moral agent can rationally coerce another person consistently with their own long-term self-interest. Rand argued that other people are an enormous value to an individual's well-being (through education, trade and affection), but also that this value could be fully realized only under conditions of political and economic freedom. According to Rand, voluntary trade alone can assure that human interaction is "mutually" beneficial. Rand's student, Leonard Peikoff has argued that the identification of one's interests itself is impossible absent the use of principles, and that self-interest cannot be consistently pursued absent a consistent adherence to certain ethical principles. Recently, Rand's position has also been defended by such writers as Tara Smith, Tibor Machan, Allan Gotthelf, David Kelley, Douglas Rasmussen, Nathaniel Branden, Harry Binswanger, Andrew Bernstein, and Craig Biddle.

Philosopher David L. Norton identified himself as an "ethical individualist", and, like Rand, saw a harmony between an individual's fidelity to their own self-actualization, or "personal destiny", and the achievement of society's well-being.




Evolution

Evolution is the change in the heritable characteristics of biological populations over successive generations. Evolution occurs when evolutionary processes such as natural selection and genetic drift act on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations. The process of evolution has given rise to biodiversity at every level of biological organisation.

The theory of evolution by natural selection was conceived independently by Charles Darwin and Alfred Russel Wallace in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin's book "On the Origin of Species". Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2) traits vary among individuals with respect to their morphology, physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics for that environment.

In the early 20th century, competing ideas of evolution were refuted and evolution was combined with Mendelian inheritance and population genetics to give rise to modern evolutionary theory. In this synthesis the basis for heredity is in DNA molecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift, mutation, and gene flow.

All life on Earth—including humanity—shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits tend to be more similar among species that share a more recent common ancestor, which historically was used to reconstruct phylogenetic trees, although direct comparison of genetic sequences is a more common method today.

Evolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but also other fields including agriculture, medicine, and computer science.

Evolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the "brown-eye trait" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its "genotype".

The complete set of observable traits that make up the structure and behaviour of an organism is called its "phenotype". Some of these traits come from the interaction of its genotype with the environment while others are neutral. Some observable characteristics are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.

Heritable characteristics are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called a chromosome. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in a quantitative or epistatic manner.

Evolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species. However, discoveries in the field of evolutionary developmental biology have demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.

An individual organism's phenotype results from both its genotype and the influence of the environment it has lived in. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.

Mutations are changes in the DNA sequence of a cell's genome and are the ultimate source of genetic variation in all organisms. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. 

About half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit. Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.

Mutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.

New genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termed "de novo" gene birth.

The generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling). When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.

One example of mutation is wild boar piglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in the "melanocortin 1 receptor" ("MC1R") disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring.

In asexual organisms, genes are inherited together, or "linked", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.

The two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and many invertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment. Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial.

Gene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.

Gene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast "Saccharomyces cerevisiae" and the adzuki bean weevil "Callosobruchus chinensis" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.

Large-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.

Some heritable changes cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.

From a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms, for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias.

Evolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:


More offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Consequences of selection include nonrandom mating and genetic hitchhiking.

The central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.

If an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be "selected "for"." Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are "selected "against"." 

Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form. However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. "Throwbacks" such as these are known as atavisms.

Natural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.

Natural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. "Nature" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: "Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system..." Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.

Natural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation.

Genetic drift is the random fluctuation of allele frequencies within a population from one generation to the next. When selective forces are absent or relatively weak, allele frequencies are equally likely to "drift" upward or downward in each successive generation because the alleles are subject to sampling error. This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.

According to the neutral theory of molecular evolution most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift. This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature. A better-supported version of this model is the nearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft. Another concept is constructive neutral evolution (CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting, and it has been applied in areas ranging from the origins of the spliceosome to the complex interdependence of microbial communities.

The time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.

It is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.

Mutation bias is usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea of developmental bias. Haldane and Fisher argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution, until the molecular era prompted renewed interest in neutral evolution.

Noboru Sueoka and Ernst Freese proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biased "E. coli" mutator strain in 1967, along with the proposal of the neutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.

For instance, mutation biases are frequently invoked in models of codon usage. Such models also include effects of selection, following the mutation-selection-drift model, which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores. Different insertion vs. deletion biases in different taxa can lead to the evolution of different genome sizes. The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.

However, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals and (2) bacterial genomes frequently have AT-biased mutation.

Contemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work showed that the original "pressures" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmental biases in the introduction of variation (arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.
Several studies report that the mutations implicated in adaptation reflect common mutation biases though others dispute this interpretation.

Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.

A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.

Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction; whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation. Macroevolution the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.

A common misconception is that evolution has goals, long-term plans, or an innate tendency for "progress", as expressed in beliefs such as orthogenesis and evolutionism; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.

Adaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term "adaptation" for the evolutionary process and "adaptive trait" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:


Adaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria "Escherichia coli" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, "Flavobacterium" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium "Sphingobium" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).

Adaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.

During evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.

However, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard "Holaspis guentheri", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.

An area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.

Interactions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.

Not all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.

Coalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.

Such cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the "helping" individual contains alleles which promote the helping activity, it is likely that its kin will "also" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.

Speciation is the process where a species diverges into two or more descendant species.

There are multiple ways to define the concept of "species." The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that "species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups." Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction; this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.

Barriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.

Speciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.

The second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.

The third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass "Anthoxanthum odoratum", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.

Finally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.

One type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species "Arabidopsis thaliana" and "Arabidopsis arenosa" crossbred to give the new species "Arabidopsis suecica". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.

Speciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short "bursts" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.

Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future. Despite the estimated extinction of more than 99% of all species that ever lived on Earth, about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.

The role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous "low-level" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.
Concepts and models used in evolutionary biology, such as natural selection, have many applications.

Artificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.

Understanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.

Evolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as to pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.

In computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.

The Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as "remains of biotic life" found in 4.1 billion-year-old rocks in Western Australia. Commenting on the Australian findings, Stephen Blair Hedges wrote: "If life arose relatively quickly on Earth, then it could be common in the universe." In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.

More than 99% of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80% not yet described.

Highly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.

All organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.

Due to horizontal gene transfer, this "tree of life" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species. To solve this problem and others, some authors prefer to use the "Coral of life" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.

Past species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.

More recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.

Prokaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.

The history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.

Soon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.

About 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from "reptile"-like lineages), mammals around 129 million years ago, Homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.

The proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork "De rerum natura" ().

In contrast to these materialistic views, Aristotelianism had considered all natural things as actualisations of fixed natural possibilities, known as forms. This became part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.

A number of Arab Muslim scholars wrote about evolution, most notably Ibn Khaldun, who wrote the book "Muqaddimah" in 1377 AD, in which he asserted that humans developed from "the world of the monkeys", in a process by which "species become more numerous".

The "New Science" of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, "species", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.

Other naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or "filament"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's "transmutation" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the "Natural Theology or Evidences of the Existence and Attributes of the Deity" (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.

The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin and Alfred Wallace in terms of variable populations. Darwin used the expression "descent with modification" rather than "evolution". Partly influenced by "An Essay on the Principle of Population" (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of "natural selection" from 1838 onwards and was writing up his "big book" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his "abstract" as "On the Origin of Species" explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.

The mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.

In the 1920s and 1930s, the modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, through fossil transitions in palaeontology.

Since then, further syntheses have extended evolution's explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of the biological hierarchy from genes to populations.

The publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved understanding of the relationship between genotype and phenotype. Advances were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that "nothing in biology makes sense except in the light of evolution", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.

One extension, known as evolutionary developmental biology and informally called "evo-devo," emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development). Since the beginning of the 21st century, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability.

In the 19th century, particularly after the publication of "On the Origin of Species" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.

While various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of "Vestiges of the Natural History of Creation" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.

The teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 "Epperson v. Arkansas" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 "Kitzmiller v. Dover Area School District" case. The debate over Darwin's ideas did not generate significant controversy in China.






Ernst Mayr

Ernst Walter Mayr (; 5 July 1904 – 3 February 2005) was a German-American evolutionary biologist. He was also a renowned taxonomist, tropical explorer, ornithologist, philosopher of biology, and historian of science. His work contributed to the conceptual revolution that led to the modern evolutionary synthesis of Mendelian genetics, systematics, and Darwinian evolution, and to the development of the biological species concept.

Although Charles Darwin and others posited that multiple species could evolve from a single common ancestor, the mechanism by which this occurred was not understood, creating the "species problem". Ernst Mayr approached the problem with a new definition for species. In his book "Systematics and the Origin of Species" (1942) he wrote that a species is not just a group of morphologically similar individuals, but a group that can breed only among themselves, excluding all others. When populations within a species become isolated by geography, feeding strategy, mate choice, or other means, they may start to differ from other populations through genetic drift and natural selection, and over time may evolve into new species. The most significant and rapid genetic reorganization occurs in extremely small populations that have been isolated (as on islands).

His theory of peripatric speciation (a more precise form of allopatric speciation which he advanced), based on his work on birds, is still considered a leading mode of speciation, and was the theoretical underpinning for the theory of punctuated equilibrium, proposed by Niles Eldredge and Stephen Jay Gould. Mayr is sometimes credited with inventing modern philosophy of biology, particularly the part related to evolutionary biology, which he distinguished from physics due to its introduction of (natural) history into science.

Mayr was the second son of Helene Pusinelli and Otto Mayr. His father was a district prosecuting attorney at Würzburg but took an interest in natural history and took the children out on field trips. Mayr learnt all the local birds in Würzburg from his elder brother Otto. He also had access to a natural history magazine for amateurs, "Kosmos". His father died just before he was thirteen. The family then moved to Dresden, where he studied at the Staatsgymnasium in Dresden-Neustadt and completed his high school education. In April 1922, while still in high school, he joined the newly founded Saxony Ornithologists' Association. There he met Rudolf Zimmermann, who became his ornithological mentor. In February 1923, Mayr passed his high school examination (Abitur) and his mother rewarded him with a pair of binoculars.

On 23 March 1923 on one of the lakes of Moritzburg, the Frauenteich, he spotted what he identified as a red-crested pochard. The species had not been seen in Saxony since 1845 and the local club argued about the identity. Raimund Schelcher (1891–1979) of the club then suggested that Mayr visit his classmate Erwin Stresemann on his way to Greifswald, where Mayr was to begin his medical studies. After a tough interrogation, Stresemann accepted and published the sighting as authentic. Stresemann was very impressed and suggested that, between semesters, Mayr could work as a volunteer in the ornithological section of the museum. Mayr wrote about this event, "It was as if someone had given me the key to heaven." He entered the University of Greifswald in 1923 and, according to Mayr himself, "took the medical curriculum (to satisfy a family tradition) but after only a year, he decided to leave medicine and enrolled at the Faculty of Biological Sciences." Mayr was endlessly interested in ornithology and "chose Greifswald at the Baltic for my studies for no other reason than that ... it was situated in the ornithologically most interesting area." Although he ostensibly planned to become a physician, he was "first and foremost an ornithologist." During the first semester break Stresemann gave him a test to identify treecreepers and Mayr was able to identify most of the specimens correctly. Stresemann declared that Mayr "was a born systematist". In 1925, Stresemann suggested that he give up his medical studies, in fact he should leave the faculty of medicine and enrol into the faculty of Biology and then join the Berlin Museum with the prospect of bird-collecting trips to the tropics, on the condition that he completed his doctoral studies in 16 months. Mayr completed his doctorate in ornithology at the University of Berlin under Dr. Carl Zimmer, who was a full professor (Ordentlicher Professor), on 24 June 1926 at the age of 21. On 1 July he accepted the position offered to him at the museum for a monthly salary of 330.54 Reichsmark.

At the International Zoological Congress at Budapest in 1927, Mayr was introduced by Stresemann to banker and naturalist Walter Rothschild, who asked him to undertake an expedition to New Guinea on behalf of himself and the American Museum of Natural History in New York. In New Guinea, Mayr collected several thousand bird skins (he named 26 new bird species during his lifetime) and, in the process also named 38 new orchid species. During his stay in New Guinea, he was invited to accompany the Whitney South Sea Expedition to the Solomon Islands. Also, while in New Guinea, he visited the Lutheran missionaries Otto Thiele and Christian Keyser, in the Finschhafen district; there, while in conversation with his hosts, he uncovered the discrepancies in Hermann Detzner's popular book "Four Years among Cannibals: New Guinea", in which Detzner claimed to have seen the interior, discovered several species of flora and fauna, while remaining only steps ahead of the Australian patrols sent to capture him. He returned to Germany in 1930.

Mayr moved to the United States in 1931 to take up a curatorial position at the American Museum of Natural History, where he played the important role of brokering and acquiring the Walter Rothschild collection of bird skins, which was being sold in order to pay off a blackmailer. During his time at the museum he produced numerous publications on bird taxonomy, and in 1942 his first book "Systematics and the Origin of Species", which completed the evolutionary synthesis started by Darwin.

After Mayr was appointed at the American Museum of Natural History, he influenced American ornithological research by mentoring young birdwatchers. Mayr was surprised at the differences between American and German birding societies. He noted that the German society was "far more scientific, far more interested in life histories and breeding bird species, as well as in reports on recent literature."

Mayr organized a monthly seminar under the auspices of the Linnean Society of New York. Under the influence of J.A. Allen, Frank Chapman, and Jonathan Dwight, the society concentrated on taxonomy and later became a clearing house for bird banding and sight records.

Mayr encouraged his Linnaean Society seminar participants to take up a specific research project of their own. Under Mayr's influence one of them, Joseph Hickey, went on to write "A Guide to Birdwatching" (1943). Hickey remembered later, "Mayr was our age and invited on all our field trips. The heckling of this German foreigner was tremendous, but he gave tit for tat, and any modern picture of Dr E. Mayr as a very formal person does not square with my memory of the 1930s. He held his own." A group of eight young birdwatchers from The Bronx later became the Bronx County Bird Club, led by Ludlow Griscom. "Everyone should have a problem" was the way one Bronx County Bird Club member recalled Mayr's refrain. Mayr said of his own involvement with the local birdwatchers: "In those early years in New York when I was a stranger in a big city, it was the companionship and later friendship which I was offered in the Linnean Society that was the most important thing in my life."

Mayr also greatly influenced the American ornithologist Margaret Morse Nice. Mayr encouraged her to correspond with European ornithologists and helped her in her landmark study on song sparrows. Nice wrote to Joseph Grinnell in 1932, trying to get foreign literature reviewed in the "Condor": "Too many American ornithologists have despised the study of the living bird; the magazines and books that deal with the subject abound in careless statements, anthropomorphic interpretations, repetition of ancient errors, and sweeping conclusions from a pitiful array of facts.  ... in Europe the study of the living bird is taken seriously. We could learn a great deal from their writing." Mayr ensured that Nice could publish her two-volume "Studies in the Life History of the Song Sparrow". He found her a publisher, and her book was reviewed by Aldo Leopold, Joseph Grinnell, and Jean Delacour. Nice dedicated her book to "My Friend Ernst Mayr."

Mayr joined the faculty of Harvard University in 1953, where he also served as director of the Museum of Comparative Zoology from 1961 to 1970. He retired in 1975 as emeritus professor of zoology, showered with honors. Following his retirement, he went on to publish more than 200 articles, in a variety of journals—more than some reputable scientists publish in their entire careers; 14 of his 25 books were published after he was 65. Even as a centenarian, he continued to write books. On his 100th birthday, he was interviewed by "Scientific American" magazine.

Mayr died on 3 February 2005 in his retirement home in Bedford, Massachusetts, after a short illness. He had married fellow German Margarete "Gretel" Simon in May 1935 (they had met at a party in Manhattan in 1932), and she assisted Mayr in some of his work.

Margarete died in 1990. He was survived by two daughters (Christa Menzel and Susanne Harrison), five grandchildren and 10 great-grandchildren.

The awards that Mayr received include the National Medal of Science, the Balzan Prize, the Sarton Medal of the History of Science Society, the International Prize for Biology, the Loye and Alden Miller Research Award, and the Lewis Thomas Prize for Writing about Science. In 1939 he was elected a Corresponding Member of the Royal Australasian Ornithologists Union. He was awarded the 1946 Leidy Award from the Academy of Natural Sciences of Philadelphia. He was awarded the Linnean Society of London's prestigious Darwin-Wallace Medal in 1958 and the Linnaean Society of New York's inaugural Eisenmann Medal in 1983. For his work, "Animal Species and Evolution", he was awarded the Daniel Giraud Elliot Medal from the National Academy of Sciences in 1967. Mayr was elected a Foreign Member of the Royal Society (ForMemRS) in 1988. In 1995 he received the Benjamin Franklin Medal for Distinguished Achievement in the Sciences of the American Philosophical Society, of which he was already a member.
Mayr never won a Nobel Prize, but he noted that there is no prize for evolutionary biology and that Darwin would not have received one, either. (In fact, there is no Nobel Prize for biology.) Mayr did win a 1999 Crafoord Prize. It honors basic research in fields that do not qualify for Nobel Prizes and is administered by the same organization as the Nobel Prize. In 2001, Mayr received the Golden Plate Award of the American Academy of Achievement.

Mayr was co-author of six global reviews of bird species new to science (listed below).

Mayr said he was an atheist in regards to "the idea of a personal God" because "there is nothing that supports [it]".

As a traditionally-trained biologist, Mayr was often highly critical of early mathematical approaches to evolution, such as those of J.B.S. Haldane, and famously called such approaches "beanbag genetics" in 1959. He maintained that factors such as reproductive isolation had to be taken into account. In a similar fashion, Mayr was also quite critical of molecular evolution studies such as those of Carl Woese. Current molecular studies in evolution and speciation indicate that although allopatric speciation is the norm, there are numerous cases of sympatric speciation in groups with greater mobility, such as birds. The precise mechanisms of sympatric speciation, however, are usually a form of microallopatry enabled by variations in niche occupancy among individuals within a population.

In many of his writings, Mayr rejected reductionism in evolutionary biology, arguing that evolutionary pressures act on the whole organism, not on single genes, and that genes can have different effects depending on the other genes present. He advocated a study of the whole genome, rather than of only isolated genes. After articulating the biological species concept in 1942, Mayr played a central role in the species problem debate over what was the best species concept. He staunchly defended the biological species concept against the many definitions of "species" that others proposed.

Mayr was an outspoken defender of the scientific method and was known to critique sharply science on the edge. As a notable example, in 1995, he criticized the Search for Extra-Terrestrial Intelligence (SETI), as conducted by fellow Harvard professor Paul Horowitz, as being a waste of university and student resources for its inability to address and answer a scientific question. Over 60 eminent scientists, led by Carl Sagan, rebutted the criticism.

Mayr rejected the idea of a gene-centered view of evolution and starkly but politely criticised Richard Dawkins's ideas:

Mayr insisted that the entire genome should be considered as the target of selection, rather than individual genes:


Darwin's theory of evolution is based on key facts and the inferences drawn from them, which Mayr summarised as follows:

In relation to the publication of Darwin's "Origins of Species", Mayr identified philosophical implications of evolution:







Europe

Europe is a continent located entirely in the Northern Hemisphere and mostly in the Eastern Hemisphere. It is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, the Mediterranean Sea to the south, and Asia to the east. Europe shares the landmass of Eurasia with Asia, and of Afro-Eurasia with both Asia and Africa. Europe is commonly considered to be separated from Asia by the watershed of the Ural Mountains, the Ural River, the Caspian Sea, the Greater Caucasus, the Black Sea, and the waterways of the Turkish straits.

Europe covers about , or 2% of Earth's surface (6.8% of land area), making it the second-smallest continent (using the seven-continent model). Politically, Europe is divided into about fifty sovereign states, of which Russia is the largest and most populous, spanning 39% of the continent and comprising 15% of its population. Europe had a total population of about /1e6 round 0 million (about 10% of the world population) in ; the third-largest after Asia and Africa. The European climate is affected by warm Atlantic currents, such as the Gulf Stream, which produce a temperate climate, tempering winters and summers, on much of the continent. Further from the sea, seasonal differences are more noticeable producing more continental climates.

European culture consists of a range of national and regional cultures, which form the central roots of the wider Western civilisation, and together commonly reference ancient Greece and ancient Rome, particularly through their Christian successors, as crucial and shared roots. Beginning with the fall of the Western Roman Empire in 476 CE, Christian consolidation of Europe in the wake of the Migration Period marked the European post-classical Middle Ages. The Renaissance, radiating from Florence, spread to the rest of the continent a new humanist interest in art and science. Since the Age of Discovery, led by Spain and Portugal, Europe played a predominant role in global affairs with multiple explorations and conquests around the world.
Between the 16th and 20th centuries, European powers colonised at various times the Americas, almost all of Africa and Oceania, and the majority of Asia.

The Age of Enlightenment, the French Revolution, and the Napoleonic Wars shaped the continent culturally, politically and economically from the end of the 17th century until the first half of the 19th century. The Industrial Revolution, which began in Great Britain at the end of the 18th century, gave rise to radical economic, cultural and social change in Western Europe and eventually the wider world. Both world wars began and were fought to a great extent in Europe, contributing to a decline in Western European dominance in world affairs by the mid-20th century as the Soviet Union and the United States took prominence and competed over dominance in Europe and globally. The resulting Cold War divided Europe along the Iron Curtain, with NATO in the West and the Warsaw Pact in the East. This divide ended with the Revolutions of 1989, the fall of the Berlin Wall, and the dissolution of the Soviet Union, which allowed European integration to advance significantly.

European integration is being advanced institutionally since 1948 with the founding of the Council of Europe, and significantly through the realization of the European Union (EU), which represents today the majority of Europe. The European Union is a supranational political entity that lies between a confederation and a federation and is based on a system of European treaties. The EU originated in Western Europe but has been expanding eastward since the dissolution of the Soviet Union in 1991. A majority of its members have adopted a common currency, the euro, and participate in the European single market and a customs union. A large bloc of countries, the Schengen Area, have also abolished internal border and immigration controls. Regular popular elections take place every five years within the EU; they are considered to be the second-largest democratic elections in the world after India's. The EU is the third largest economy in the world.

The place name Evros was first used by the ancient Greeks to refer to their northernmost province, which bears the same name today. The principal river there – Evros (today's Maritsa) – flows through the fertile valleys of Thrace.

In classical Greek mythology, Europa (, ) was a Phoenician princess. One view is that her name derives from the Ancient Greek elements () 'wide, broad', and (, , ) 'eye, face, countenance', hence their composite would mean 'wide-gazing' or 'broad of aspect'. "Broad" has been an epithet of Earth herself in the reconstructed Proto-Indo-European religion and the poetry devoted to it. An alternative view is that of Robert Beekes, who has argued in favour of a Pre-Indo-European origin for the name, explaining that a derivation from would yield a different toponym than Europa. Beekes has located toponyms related to that of Europa in the territory of ancient Greece, and localities such as that of Europos in ancient Macedonia.

There have been attempts to connect to a Semitic term for "west", this being either Akkadian meaning 'to go down, set' (said of the sun) or Phoenician 'evening, west', which is at the origin of Arabic and Hebrew . Martin Litchfield West stated that "phonologically, the match between Europa's name and any form of the Semitic word is very poor", while Beekes considers a connection to Semitic languages improbable.

Most major world languages use words derived from or "Europa" to refer to the continent. Chinese, for example, uses the word (/), which is an abbreviation of the transliterated name () ( means "continent"); a similar Chinese-derived term is also sometimes used in Japanese such as in the Japanese name of the European Union, , despite the katakana being more commonly used. In some Turkic languages, the originally Persian name ('land of the Franks') is used casually in referring to much of Europe, besides official names such as or .

The prevalent definition of Europe as a geographical term has been in use since the mid-19th century.
Europe is taken to be bounded by large bodies of water to the north, west and south; Europe's limits to the east and north-east are usually taken to be the Ural Mountains, the Ural River, and the Caspian Sea; to the south-east, the Caucasus Mountains, the Black Sea, and the waterways connecting the Black Sea to the Mediterranean Sea.

Islands are generally grouped with the nearest continental landmass, hence Iceland is considered to be part of Europe, while the nearby island of Greenland is usually assigned to North America, although politically belonging to Denmark. Nevertheless, there are some exceptions based on sociopolitical and cultural differences. Cyprus is closest to Anatolia (or Asia Minor), but is considered part of Europe politically and it is a member state of the EU. Malta was considered an island of North-western Africa for centuries, but now it is considered to be part of Europe as well. "Europe", as used specifically in British English, may also refer to Continental Europe exclusively.

The term "continent" usually implies the physical geography of a large land mass completely or almost completely surrounded by water at its borders. Prior to the adoption of the current convention that includes mountain divides, the border between Europe and Asia had been redefined several times since its first conception in classical antiquity, but always as a series of rivers, seas and straits that were believed to extend an unknown distance east and north from the Mediterranean Sea without the inclusion of any mountain ranges. Cartographer Herman Moll suggested in 1715 Europe was bounded by a series of partly-joined waterways directed towards the Turkish straits, and the Irtysh River draining into the upper part of the Ob River and the Arctic Ocean. In contrast, the present eastern boundary of Europe partially adheres to the Ural and Caucasus Mountains, which is somewhat arbitrary and inconsistent compared to any clear-cut definition of the term "continent".

The current division of Eurasia into two continents now reflects East-West cultural, linguistic and ethnic differences which vary on a spectrum rather than with a sharp dividing line. The geographic border between Europe and Asia does not follow any state boundaries and now only follows a few bodies of water. Turkey is generally considered a transcontinental country divided entirely by water, while Russia and Kazakhstan are only partly divided by waterways. France, the Netherlands, Portugal and Spain are also transcontinental (or more properly, intercontinental, when oceans or large seas are involved) in that their main land areas are in Europe while pockets of their territories are located on other continents separated from Europe by large bodies of water. Spain, for example, has territories south of the Mediterranean Sea—namely, Ceuta and Melilla—which are parts of Africa and share a border with Morocco. According to the current convention, Georgia and Azerbaijan are transcontinental countries where waterways have been completely replaced by mountains as the divide between continents.

The first recorded usage of "Eurṓpē" as a geographic term is in the Homeric Hymn to Delian Apollo, in reference to the western shore of the Aegean Sea. As a name for a part of the known world, it is first used in the 6th century BCE by Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni River on the territory of Georgia) in the Caucasus, a convention still followed by Herodotus in the 5th century BCE. Herodotus mentioned that the world had been divided by unknown persons into three parts—Europe, Asia, and Libya (Africa)—with the Nile and the Phasis forming their boundaries—though he also states that some considered the River Don, rather than the Phasis, as the boundary between Europe and Asia. Europe's eastern frontier was defined in the 1st century by geographer Strabo at the River Don. The "Book of Jubilees" described the continents as the lands given by Noah to his three sons; Europe was defined as stretching from the Pillars of Hercules at the Strait of Gibraltar, separating it from Northwest Africa, to the Don, separating it from Asia.

The convention received by the Middle Ages and surviving into modern usage is that of the Roman era used by Roman-era authors such as Posidonius, Strabo and Ptolemy, who took the Tanais (the modern Don River) as the boundary.

The Roman Empire did not attach a strong identity to the concept of continental divisions. However, following the fall of the Western Roman Empire, the culture that developed in its place, linked to Latin and the Catholic church, began to associate itself with the concept of "Europe". The term "Europe" is first used for a cultural sphere in the Carolingian Renaissance of the 9th century. From that time, the term designated the sphere of influence of the Western Church, as opposed to both the Eastern Orthodox churches and to the Islamic world.

A cultural definition of Europe as the lands of Latin Christendom coalesced in the 8th century, signifying the new cultural condominium created through the confluence of Germanic traditions and Christian-Latin culture, defined partly in contrast with Byzantium and Islam, and limited to northern Iberia, the British Isles, France, Christianised western Germany, the Alpine regions and northern and central Italy. The concept is one of the lasting legacies of the Carolingian Renaissance: "Europa" often figures in the letters of Charlemagne's court scholar, Alcuin. The transition of Europe to being a cultural term as well as a geographic one led to the borders of Europe being affected by cultural considerations in the East, especially relating to areas under Byzantine, Ottoman, and Russian influence. Such questions were affected by the positive connotations associated with the term Europe by its users. Such cultural considerations were not applied to the Americas, despite their conquest and settlement by European states. Instead, the concept of "Western civilization" emerged as a way of grouping together Europe and these colonies.

The question of defining a precise eastern boundary of Europe arises in the Early Modern period, as the eastern extension of Muscovy began to include North Asia. Throughout the Middle Ages and into the 18th century, the traditional division of the landmass of Eurasia into two continents, Europe and Asia, followed Ptolemy, with the boundary following the Turkish Straits, the Black Sea, the Kerch Strait, the Sea of Azov and the Don (ancient Tanais). But maps produced during the 16th to 18th centuries tended to differ in how to continue the boundary beyond the Don bend at Kalach-na-Donu (where it is closest to the Volga, now joined with it by the Volga–Don Canal), into territory not described in any detail by the ancient geographers.

Around 1715, Herman Moll produced a map showing the northern part of the Ob River and the Irtysh River, a major tributary of the Ob, as components of a series of partly-joined waterways taking the boundary between Europe and Asia from the Turkish Straits, and the Don River all the way to the Arctic Ocean. In 1721, he produced a more up to date map that was easier to read. However, his proposal to adhere to major rivers as the line of demarcation was never taken up by other geographers who were beginning to move away from the idea of water boundaries as the only legitimate divides between Europe and Asia.

Four years later, in 1725, Philip Johan von Strahlenberg was the first to depart from the classical Don boundary. He drew a new line along the Volga, following the Volga north until the Samara Bend, along Obshchy Syrt (the drainage divide between the Volga and Ural Rivers), then north and east along the latter waterway to its source in the Ural Mountains. At this point he proposed that mountain ranges could be included as boundaries between continents as alternatives to nearby waterways. Accordingly, he drew the new boundary north along Ural Mountains rather than the nearby and parallel running Ob and Irtysh rivers. This was endorsed by the Russian Empire and introduced the convention that would eventually become commonly accepted. However, this did not come without criticism. Voltaire, writing in 1760 about Peter the Great's efforts to make Russia more European, ignored the whole boundary question with his claim that neither Russia, Scandinavia, northern Germany, nor Poland were fully part of Europe. Since then, many modern analytical geographers like Halford Mackinder have declared that they see little validity in the Ural Mountains as a boundary between continents.

The mapmakers continued to differ on the boundary between the lower Don and Samara well into the 19th century. The published by the Russian Academy of Sciences has the boundary follow the Don beyond Kalach as far as Serafimovich before cutting north towards Arkhangelsk, while other 18th- to 19th-century mapmakers such as John Cary followed Strahlenberg's prescription. To the south, the Kuma–Manych Depression was identified by a German naturalist, Peter Simon Pallas, as a valley that once connected the Black Sea and the Caspian Sea, and subsequently was proposed as a natural boundary between continents.

By the mid-19th century, there were three main conventions, one following the Don, the Volga–Don Canal and the Volga, the other following the Kuma–Manych Depression to the Caspian and then the Ural River, and the third abandoning the Don altogether, following the Greater Caucasus watershed to the Caspian. The question was still treated as a "controversy" in geographical literature of the 1860s, with Douglas Freshfield advocating the Caucasus crest boundary as the "best possible", citing support from various "modern geographers".

In Russia and the Soviet Union, the boundary along the Kuma–Manych Depression was the most commonly used as early as 1906. In 1958, the Soviet Geographical Society formally recommended that the boundary between the Europe and Asia be drawn in textbooks from Baydaratskaya Bay, on the Kara Sea, along the eastern foot of Ural Mountains, then following the Ural River until the Mugodzhar Hills, and then the Emba River; and Kuma–Manych Depression, thus placing the Caucasus entirely in Asia and the Urals entirely in Europe. The "Flora Europaea" adopted a boundary along the Terek and Kuban rivers, so southwards from the Kuma and the Manych, but still with the Caucasus entirely in Asia. However, most geographers in the Soviet Union favoured the boundary along the Caucasus crest, and this became the common convention in the later 20th century, although the Kuma–Manych boundary remained in use in some 20th-century maps.

Some view the separation of Eurasia into Asia and Europe as a residue of Eurocentrism: "In physical, cultural and historical diversity, China and India are comparable to the entire European landmass, not to a single European country. [...]."

During the 2.5 million years of the Pleistocene, numerous cold phases called glacials (Quaternary ice age), or significant advances of continental ice sheets, in Europe and North America, occurred at intervals of approximately 40,000 to 100,000 years. The long glacial periods were separated by more temperate and shorter interglacials which lasted about 10,000–15,000 years. The last cold episode of the last glacial period ended about 10,000 years ago. Earth is currently in an interglacial period of the Quaternary, called the Holocene.

"Homo erectus georgicus", which lived roughly 1.8 million years ago in Georgia, is the earliest hominin to have been discovered in Europe. Other hominin remains, dating back roughly 1 million years, have been discovered in Atapuerca, Spain. Neanderthal man (named after the Neandertal valley in Germany) appeared in Europe 150,000 years ago (115,000 years ago it is found already in the territory of present-day Poland) and disappeared from the fossil record about 40,000 years ago, with their final refuge being the Iberian Peninsula. The Neanderthals were supplanted by modern humans (Cro-Magnons), who appeared in Europe around 43,000 to 40,000 years ago. Homo sapiens arrived in Europe around 54,000 years ago, some 10,000 years earlier than previously thought. The earliest sites in Europe dated 48,000 years ago are Riparo Mochi (Italy), Geissenklösterle (Germany) and Isturitz (France).

The European Neolithic period—marked by the cultivation of crops and the raising of livestock, increased numbers of settlements and the widespread use of pottery—began around 7000 BCE in Greece and the Balkans, probably influenced by earlier farming practices in Anatolia and the Near East. It spread from the Balkans along the valleys of the Danube and the Rhine (Linear Pottery culture), and along the Mediterranean coast (Cardial culture). Between 4500 and 3000 BCE, these central European neolithic cultures developed further to the west and the north, transmitting newly acquired skills in producing copper artifacts. In Western Europe the Neolithic period was characterised not by large agricultural settlements but by field monuments, such as causewayed enclosures, burial mounds and megalithic tombs. The Corded Ware cultural horizon flourished at the transition from the Neolithic to the Chalcolithic. During this period giant megalithic monuments, such as the Megalithic Temples of Malta and Stonehenge, were constructed throughout Western and Southern Europe.

The modern native populations of Europe largely descend from three distinct lineages: Mesolithic hunter-gatherers, descended from populations associated with the Paleolithic Epigravettian culture; Neolithic Early European Farmers who migrated from Anatolia during the Neolithic Revolution 9,000 years ago; and Yamnaya Steppe herders who expanded into Europe from the Pontic–Caspian steppe of Ukraine and southern Russia in the context of Indo-European migrations 5,000 years ago. The European Bronze Age began c. 3200 BCE in Greece with the Minoan civilisation on Crete, the first advanced civilisation in Europe. The Minoans were followed by the Myceneans, who collapsed suddenly around 1200 BCE, ushering the European Iron Age. Iron Age colonisation by the Greeks and Phoenicians gave rise to early Mediterranean cities. Early Iron Age Italy and Greece from around the 8th century BCE gradually gave rise to historical Classical antiquity, whose beginning is sometimes dated to 776 BCE, the year of the first Olympic Games.

Ancient Greece was the founding culture of Western civilisation. Western democratic and rationalist culture are often attributed to Ancient Greece. The Greek city-state, the polis, was the fundamental political unit of classical Greece. In 508 BCE, Cleisthenes instituted the world's first democratic system of government in Athens. The Greek political ideals were rediscovered in the late 18th century by European philosophers and idealists. Greece also generated many cultural contributions: in philosophy, humanism and rationalism under Aristotle, Socrates and Plato; in history with Herodotus and Thucydides; in dramatic and narrative verse, starting with the epic poems of Homer; in drama with Sophocles and Euripides; in medicine with Hippocrates and Galen; and in science with Pythagoras, Euclid and Archimedes. In the course of the 5th century BCE, several of the Greek city states would ultimately check the Achaemenid Persian advance in Europe through the Greco-Persian Wars, considered a pivotal moment in world history, as the 50 years of peace that followed are known as Golden Age of Athens, the seminal period of ancient Greece that laid many of the foundations of Western civilisation.
Greece was followed by Rome, which left its mark on law, politics, language, engineering, architecture, government and many more key aspects in western civilisation. By 200 BCE, Rome had conquered Italy and over the following two centuries it conquered Greece and Hispania (Spain and Portugal), the North African coast, much of the Middle East, Gaul (France and Belgium) and Britannia (England and Wales).

Expanding from their base in central Italy beginning in the third century BCE, the Romans gradually expanded to eventually rule the entire Mediterranean Basin and Western Europe by the turn of the millennium. The Roman Republic ended in 27 BCE, when Augustus proclaimed the Roman Empire. The two centuries that followed are known as the "pax romana", a period of unprecedented peace, prosperity and political stability in most of Europe. The empire continued to expand under emperors such as Antoninus Pius and Marcus Aurelius, who spent time on the Empire's northern border fighting Germanic, Pictish and Scottish tribes. Christianity was legalised by Constantine I in 313 CE after three centuries of imperial persecution. Constantine also permanently moved the capital of the empire from Rome to the city of Byzantium (modern-day Istanbul) which was renamed Constantinople in his honour in 330 CE. Christianity became the sole official religion of the empire in 380 CE and in 391–392 CE, the emperor Theodosius outlawed pagan religions. This is sometimes considered to mark the end of antiquity; alternatively antiquity is considered to end with the fall of the Western Roman Empire in 476 CE; the closure of the pagan Platonic Academy of Athens in 529 CE; or the rise of Islam in the early 7th century CE. During most of its existence, the Byzantine Empire was one of the most powerful economic, cultural, and military forces in Europe.

During the decline of the Roman Empire, Europe entered a long period of change arising from what historians call the "Age of Migrations". There were numerous invasions and migrations amongst the Ostrogoths, Visigoths, Goths, Vandals, Huns, Franks, Angles, Saxons, Slavs, Avars, Bulgars and, later on, the Vikings, Pechenegs, Cumans and Magyars. Renaissance thinkers such as Petrarch would later refer to this as the "Dark Ages".

Isolated monastic communities were the only places to safeguard and compile written knowledge accumulated previously; apart from this very few written records survive and much literature, philosophy, mathematics and other thinking from the classical period disappeared from Western Europe, though they were preserved in the east, in the Byzantine Empire.

While the Roman empire in the west continued to decline, Roman traditions and the Roman state remained strong in the predominantly Greek-speaking Eastern Roman Empire, also known as the Byzantine Empire. During most of its existence, the Byzantine Empire was the most powerful economic, cultural and military force in Europe. Emperor Justinian I presided over Constantinople's first golden age: he established a legal code that forms the basis of many modern legal systems, funded the construction of the Hagia Sophia and brought the Christian church under state control.

From the 7th century onwards, as the Byzantines and neighbouring Sasanid Persians were severely weakened due to the protracted, centuries-lasting and frequent Byzantine–Sasanian wars, the Muslim Arabs began to make inroads into historically Roman territory, taking the Levant and North Africa and making inroads into Asia Minor. In the mid-7th century, following the Muslim conquest of Persia, Islam penetrated into the Caucasus region. Over the next centuries Muslim forces took Cyprus, Malta, Crete, Sicily and parts of southern Italy. Between 711 and 720, most of the lands of the Visigothic Kingdom of Iberia was brought under Muslim rule—save for small areas in the north-west (Asturias) and largely Basque regions in the Pyrenees. This territory, under the Arabic name Al-Andalus, became part of the expanding Umayyad Caliphate. The unsuccessful second siege of Constantinople (717) weakened the Umayyad dynasty and reduced their prestige. The Umayyads were then defeated by the Frankish leader Charles Martel at the Battle of Poitiers in 732, which ended their northward advance. In the remote regions of north-western Iberia and the middle Pyrenees the power of the Muslims in the south was scarcely felt. It was here that the foundations of the Christian kingdoms of Asturias, Leon and Galicia were laid and from where the reconquest of the Iberian Peninsula would start. However, no coordinated attempt would be made to drive the Moors out. The Christian kingdoms were mainly focused on their own internal power struggles. As a result, the Reconquista took the greater part of eight hundred years, in which period a long list of Alfonsos, Sanchos, Ordoños, Ramiros, Fernandos and Bermudos would be fighting their Christian rivals as much as the Muslim invaders.
During the Dark Ages, the Western Roman Empire fell under the control of various tribes. The Germanic and Slav tribes established their domains over Western and Eastern Europe, respectively. Eventually the Frankish tribes were united under Clovis I. Charlemagne, a Frankish king of the Carolingian dynasty who had conquered most of Western Europe, was anointed "Holy Roman Emperor" by the Pope in 800. This led in 962 to the founding of the Holy Roman Empire, which eventually became centred in the German principalities of central Europe.

East Central Europe saw the creation of the first Slavic states and the adoption of Christianity (. The powerful West Slavic state of Great Moravia spread its territory all the way south to the Balkans, reaching its largest territorial extent under Svatopluk I and causing a series of armed conflicts with East Francia. Further south, the first South Slavic states emerged in the late 7th and 8th century and adopted Christianity: the First Bulgarian Empire, the Serbian Principality (later Kingdom and Empire) and the Duchy of Croatia (later Kingdom of Croatia). To the East, Kievan Rus' expanded from its capital in Kiev to become the largest state in Europe by the 10th century. In 988, Vladimir the Great adopted Orthodox Christianity as the religion of state. Further East, Volga Bulgaria became an Islamic state in the 10th century, but was eventually absorbed into Russia several centuries later.

The period between the year 1000 and 1250 is known as the High Middle Ages, followed by the Late Middle Ages until c. 1500.

During the High Middle Ages the population of Europe experienced significant growth, culminating in the Renaissance of the 12th century. Economic growth, together with the lack of safety on the mainland trading routes, made possible the development of major commercial routes along the coast of the Mediterranean and Baltic Seas. The growing wealth and independence acquired by some coastal cities gave the Maritime Republics a leading role in the European scene.

The Middle Ages on the mainland were dominated by the two upper echelons of the social structure: the nobility and the clergy. Feudalism developed in France in the Early Middle Ages, and soon spread throughout Europe. A struggle for influence between the nobility and the monarchy in England led to the writing of the "Magna Carta" and the establishment of a parliament. The primary source of culture in this period came from the Roman Catholic Church. Through monasteries and cathedral schools, the Church was responsible for education in much of Europe.

The Papacy reached the height of its power during the High Middle Ages. An East-West Schism in 1054 split the former Roman Empire religiously, with the Eastern Orthodox Church in the Byzantine Empire and the Roman Catholic Church in the former Western Roman Empire. In 1095 Pope Urban II called for a crusade against Muslims occupying Jerusalem and the Holy Land. In Europe itself, the Church organised the Inquisition against heretics. In the Iberian Peninsula, the Reconquista concluded with the fall of Granada in 1492, ending over seven centuries of Islamic rule in the south-western peninsula.

In the east, a resurgent Byzantine Empire recaptured Crete and Cyprus from the Muslims, and reconquered the Balkans. Constantinople was the largest and wealthiest city in Europe from the 9th to the 12th centuries, with a population of approximately 400,000. The Empire was weakened following the defeat at Manzikert, and was weakened considerably by the sack of Constantinople in 1204, during the Fourth Crusade. Although it would recover Constantinople in 1261, Byzantium fell in 1453 when Constantinople was taken by the Ottoman Empire.

In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Pechenegs and the Cuman-Kipchaks, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north, and temporarily halted the expansion of the Rus' state to the south and east. Like many other parts of Eurasia, these territories were overrun by the Mongols. The invaders, who became known as Tatars, were mostly Turkic-speaking peoples under Mongol suzerainty. They established the state of the Golden Horde with headquarters in Crimea, which later adopted Islam as a religion, and ruled over modern-day southern and central Russia for more than three centuries. After the collapse of Mongol dominions, the first Romanian states (principalities) emerged in the 14th century: Moldavia and Walachia. Previously, these territories were under the successive control of Pechenegs and Cumans. From the 12th to the 15th centuries, the Grand Duchy of Moscow grew from a small principality under Mongol rule to the largest state in Europe, overthrowing the Mongols in 1480, and eventually becoming the Tsardom of Russia. The state was consolidated under Ivan III the Great and Ivan the Terrible, steadily expanding to the east and south over the next centuries.

The Great Famine of 1315–1317 was the first crisis that would strike Europe in the late Middle Ages. The period between 1348 and 1420 witnessed the heaviest loss. The population of France was reduced by half. Medieval Britain was afflicted by 95 famines, and France suffered the effects of 75 or more in the same period. Europe was devastated in the mid-14th century by the Black Death, one of the most deadly pandemics in human history which killed an estimated 25 million people in Europe alone—a third of the European population at the time.

The plague had a devastating effect on Europe's social structure; it induced people to live for the moment as illustrated by Giovanni Boccaccio in "The Decameron" (1353). It was a serious blow to the Roman Catholic Church and led to increased persecution of Jews, beggars and lepers. The plague is thought to have returned every generation with varying virulence and mortalities until the 18th century. During this period, more than 100 plague epidemics swept across Europe.

The Renaissance was a period of cultural change originating in Florence, and later spreading to the rest of Europe. The rise of a new humanism was accompanied by the recovery of forgotten classical Greek and Arabic knowledge from monastic libraries, often translated from Arabic into Latin. The Renaissance spread across Europe between the 14th and 16th centuries: it saw the flowering of art, philosophy, music, and the sciences, under the joint patronage of royalty, the nobility, the Roman Catholic Church and an emerging merchant class. Patrons in Italy, including the Medici family of Florentine bankers and the Popes in Rome, funded prolific quattrocento and cinquecento artists such as Raphael, Michelangelo and Leonardo da Vinci.

Political intrigue within the Church in the mid-14th century caused the Western Schism. During this forty-year period, two popes—one in Avignon and one in Rome—claimed rulership over the Church. Although the schism was eventually healed in 1417, the papacy's spiritual authority had suffered greatly. In the 15th century, Europe started to extend itself beyond its geographic frontiers. Spain and Portugal, the greatest naval powers of the time, took the lead in exploring the world. Exploration reached the Southern Hemisphere in the Atlantic and the Southern tip of Africa. Christopher Columbus reached the New World in 1492, and Vasco da Gama opened the ocean route to the East linking the Atlantic and Indian Oceans in 1498. The Portuguese-born explorer Ferdinand Magellan reached Asia westward across the Atlantic and the Pacific Oceans in a Spanish expedition, resulting in the first circumnavigation of the globe, completed by the Spaniard Juan Sebastián Elcano (1519–1522). Soon after, the Spanish and Portuguese began establishing large global empires in the Americas, Asia, Africa and Oceania. France, the Netherlands and England soon followed in building large colonial empires with vast holdings in Africa, the Americas and Asia. In 1588, a Spanish armada failed to invade England. A year later England tried unsuccessfully to invade Spain, allowing Philip II of Spain to maintain his dominant war capacity in Europe. This English disaster also allowed the Spanish fleet to retain its capability to wage war for the next decades. However, two more Spanish armadas failed to invade England (2nd Spanish Armada and 3rd Spanish Armada).
The Church's power was further weakened by the Protestant Reformation in 1517 when German theologian Martin Luther nailed his "Ninety-five Theses" criticising the selling of indulgences to the church door. He was subsequently excommunicated in the papal bull "Exsurge Domine" in 1520 and his followers were condemned in the 1521 Diet of Worms, which divided German princes between Protestant and Roman Catholic faiths. Religious fighting and warfare spread with Protestantism. The plunder of the empires of the Americas allowed Spain to finance religious persecution in Europe for over a century. The Thirty Years War (1618–1648) crippled the Holy Roman Empire and devastated much of Germany, killing between 25 and 40 percent of its population. In the aftermath of the Peace of Westphalia, France rose to predominance within Europe. The defeat of the Ottoman Turks at the Battle of Vienna in 1683 marked the historic end of Ottoman expansion into Europe.

The 17th century in Central and parts of Eastern Europe was a period of general decline; the region experienced more than 150 famines in a 200-year period between 1501 and 1700. From the Union of Krewo (1385) east-central Europe was dominated by the Kingdom of Poland and the Grand Duchy of Lithuania. The hegemony of the vast Polish–Lithuanian Commonwealth had ended with the devastation brought by the Second Northern War (Deluge) and subsequent conflicts; the state itself was partitioned and ceased to exist at the end of the 18th century.

From the 15th to 18th centuries, when the disintegrating khanates of the Golden Horde were conquered by Russia, Tatars from the Crimean Khanate frequently raided Eastern Slavic lands to capture slaves. Further east, the Nogai Horde and Kazakh Khanate frequently raided the Slavic-speaking areas of contemporary Russia and Ukraine for hundreds of years, until the Russian expansion and conquest of most of northern Eurasia (i.e. Eastern Europe, Central Asia and Siberia).

The Renaissance and the New Monarchs marked the start of an Age of Discovery, a period of exploration, invention and scientific development. Among the great figures of the Western scientific revolution of the 16th and 17th centuries were Copernicus, Kepler, Galileo and Isaac Newton. According to Peter Barrett, "It is widely accepted that 'modern science' arose in the Europe of the 17th century (towards the end of the Renaissance), introducing a new understanding of the natural world."

The Seven Years' War brought to an end the "Old System" of alliances in Europe. Consequently, when the American Revolutionary War turned into a global war between 1778 and 1783, Britain found itself opposed by a strong coalition of European powers, and lacking any substantial ally.

The Age of Enlightenment was a powerful intellectual movement during the 18th century promoting scientific and reason-based thoughts. Discontent with the aristocracy and clergy's monopoly on political power in France resulted in the French Revolution, and the establishment of the First Republic as a result of which the monarchy and many of the nobility perished during the initial reign of terror. Napoleon Bonaparte rose to power in the aftermath of the French Revolution, and established the First French Empire that, during the Napoleonic Wars, grew to encompass large parts of Europe before collapsing in 1815 with the Battle of Waterloo. Napoleonic rule resulted in the further dissemination of the ideals of the French Revolution, including that of the nation state, as well as the widespread adoption of the French models of administration, law and education. The Congress of Vienna, convened after Napoleon's downfall, established a new balance of power in Europe centred on the five "Great Powers": the UK, France, Prussia, Austria and Russia. This balance would remain in place until the Revolutions of 1848, during which liberal uprisings affected all of Europe except for Russia and the UK. These revolutions were eventually put down by conservative elements and few reforms resulted. The year 1859 saw the unification of Romania, as a nation state, from smaller principalities. In 1867, the Austro-Hungarian empire was formed; 1871 saw the unifications of both Italy and Germany as nation-states from smaller principalities.

In parallel, the Eastern Question grew more complex ever since the Ottoman defeat in the Russo-Turkish War (1768–1774). As the dissolution of the Ottoman Empire seemed imminent, the Great Powers struggled to safeguard their strategic and commercial interests in the Ottoman domains. The Russian Empire stood to benefit from the decline, whereas the Habsburg Empire and Britain perceived the preservation of the Ottoman Empire to be in their best interests. Meanwhile, the Serbian Revolution (1804) and Greek War of Independence (1821) marked the beginning of the end of Ottoman rule in the Balkans, which ended with the Balkan Wars in 1912–1913. Formal recognition of the "de facto" independent principalities of Montenegro, Serbia and Romania ensued at the Congress of Berlin in 1878.

The Industrial Revolution started in Great Britain in the last part of the 18th century and spread throughout Europe. The invention and implementation of new technologies resulted in rapid urban growth, mass employment and the rise of a new working class. Reforms in social and economic spheres followed, including the first laws on child labour, the legalisation of trade unions, and the abolition of slavery. In Britain, the Public Health Act of 1875 was passed, which significantly improved living conditions in many British cities. Europe's population increased from about 100 million in 1700 to 400 million by 1900. The last major famine recorded in Western Europe, the Great Famine of Ireland, caused death and mass emigration of millions of Irish people. In the 19th century, 70 million people left Europe in migrations to various European colonies abroad and to the United States. The industrial revolution also led to large population growth, and the reached a peak of slightly above 25% around the year 1913.

Two world wars and an economic depression dominated the first half of the 20th century. The First World War was fought between 1914 and 1918. It started when Archduke Franz Ferdinand of Austria was assassinated by the Yugoslav nationalist Gavrilo Princip. Most European nations were drawn into the war, which was fought between the Entente Powers (France, Belgium, Serbia, Portugal, Russia, the United Kingdom, and later Italy, Greece, Romania, and the United States) and the Central Powers (Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire). The war left more than 16 million civilians and military dead. Over 60 million European soldiers were mobilised from 1914 to 1918.
Russia was plunged into the Russian Revolution, which threw down the Tsarist monarchy and replaced it with the communist Soviet Union, leading also to the independence of many former Russian governorates, such as Finland, Estonia, Latvia and Lithuania, as new European countries. Austria-Hungary and the Ottoman Empire collapsed and broke up into separate nations, and many other nations had their borders redrawn. The Treaty of Versailles, which officially ended the First World War in 1919, was harsh towards Germany, upon whom it placed full responsibility for the war and imposed heavy sanctions. Excess deaths in Russia over the course of the First World War and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million. In 1932–1933, under Stalin's leadership, confiscations of grain by the Soviet authorities contributed to the second Soviet famine which caused millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Stalin was also responsible for the Great Purge of 1937–38 in which the NKVD executed 681,692 people; millions of people were deported and exiled to remote areas of the Soviet Union.

The social revolutions sweeping through Russia also affected other European nations following The Great War: in 1919, with the Weimar Republic in Germany and the First Austrian Republic; in 1922, with Mussolini's one-party fascist government in the Kingdom of Italy and in Atatürk's Turkish Republic, adopting the Western alphabet and state secularism.
Economic instability, caused in part by debts incurred in the First World War and 'loans' to Germany played havoc in Europe in the late 1920s and 1930s. This, and the Wall Street Crash of 1929, brought about the worldwide Great Depression. Helped by the economic crisis, social instability and the threat of communism, fascist movements developed throughout Europe placing Adolf Hitler in power of what became Nazi Germany.

In 1933, Hitler became the leader of Germany and began to work towards his goal of building Greater Germany. Germany re-expanded and took back the Saarland and Rhineland in 1935 and 1936. In 1938, Austria became a part of Germany following the Anschluss. Later that year, following the Munich Agreement signed by Germany, France, the United Kingdom, and Italy, Germany annexed the Sudetenland, which was a part of Czechoslovakia inhabited by ethnic Germans, and in early 1939, the remainder of Czechoslovakia was split into the Protectorate of Bohemia and Moravia, controlled by Germany and the Slovak Republic. At the time, the United Kingdom and France preferred a policy of appeasement.

With tensions mounting between Germany and Poland over the future of Danzig, the Germans turned to the Soviets and signed the Molotov–Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of the Second World War. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and, on 30 November, Finland, the latter of which was followed by the devastating Winter War for the Red Army. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle Germany and cut the Germans off from Scandinavian resources. Around the same time, Germany moved troops into Denmark. The Phoney War continued.

In May 1940, Germany attacked France through the Low Countries. France capitulated in June 1940. By August, Germany had begun a bombing offensive against the United Kingdom but failed to convince the Britons to give up. In 1941, Germany invaded the Soviet Union in Operation Barbarossa. On 7 December 1941 Japan's attack on Pearl Harbor drew the United States into the conflict as allies of the British Empire, and other allied forces. 

After the staggering Battle of Stalingrad in 1943, the German offensive in the Soviet Union turned into a continual fallback. The Battle of Kursk, which involved the largest tank battle in history, was the last major German offensive on the Eastern Front. In June 1944, British and American forces invaded France in the D-Day landings, opening a new front against Germany. Berlin finally fell in 1945, ending the Second World War in Europe. The war was the largest and most destructive in human history, with 60 million dead across the world. More than 40 million people in Europe had died as a result of the Second World War, including between 11 and 17 million people who perished during the Holocaust. The Soviet Union lost around 27 million people (mostly civilians) during the war, about half of all Second World War casualties. By the end of the Second World War, Europe had more than 40 million refugees. Several post-war expulsions in Central and Eastern Europe displaced a total of about 20 million people.

The First World War, and especially the Second World War, diminished the eminence of Western Europe in world affairs. After the Second World War the map of Europe was redrawn at the Yalta Conference and divided into two blocs, the Western countries and the communist Eastern bloc, separated by what was later called by Winston Churchill an "Iron Curtain". The United States and Western Europe established the NATO alliance and, later, the Soviet Union and Central Europe established the Warsaw Pact. Particular hot spots after the Second World War were Berlin and Trieste, whereby the Free Territory of Trieste, founded in 1947 with the UN, was dissolved in 1954 and 1975, respectively. The Berlin blockade in 1948 and 1949 and the construction of the Berlin Wall in 1961 were one of the great international crises of the Cold War.

The two new superpowers, the United States and the Soviet Union, became locked in a fifty-year-long Cold War, centred on nuclear proliferation. At the same time decolonisation, which had already started after the First World War, gradually resulted in the independence of most of the European colonies in Asia and Africa.

In the 1980s the reforms of Mikhail Gorbachev and the Solidarity movement in Poland weakened the previously rigid communist system. The opening of the Iron Curtain at the Pan-European Picnic then set in motion a peaceful chain reaction, at the end of which the Eastern bloc, the Warsaw Pact and other communist states collapsed, and the Cold War ended. Germany was reunited, after the symbolic fall of the Berlin Wall in 1989 and the maps of Central and Eastern Europe were redrawn once more. This made old previously interrupted cultural and economic relationships possible, and previously isolated cities such as Berlin, Prague, Vienna, Budapest and Trieste were now again in the centre of Europe.

European integration also grew after the Second World War. In 1949 the Council of Europe was founded, following a speech by Sir Winston Churchill, with the idea of unifying Europe to achieve common goals. It includes all European states except for Belarus, Russia, and Vatican City. The Treaty of Rome in 1957 established the European Economic Community between six Western European states with the goal of a unified economic policy and common market. In 1967 the EEC, European Coal and Steel Community, and Euratom formed the European Community, which in 1993 became the European Union. The EU established a parliament, court and central bank, and introduced the euro as a unified currency. Between 2004 and 2013, more Central European countries began joining, expanding the EU to 28 European countries and once more making Europe a major economical and political centre of power. However, the United Kingdom withdrew from the EU on 31 January 2020, as a result of a June 2016 referendum on EU membership. The Russo-Ukrainian conflict, which has been ongoing since 2014, steeply escalated when Russia launched a full-scale invasion of Ukraine on 24 February 2022, marking the largest humanitarian and refugee crisis in Europe since the Second World War and the Yugoslav Wars.

Europe makes up the western fifth of the Eurasian landmass. It has a higher ratio of coast to landmass than any other continent or subcontinent. Its maritime borders consist of the Arctic Ocean to the north, the Atlantic Ocean to the west and the Mediterranean, Black and Caspian Seas to the south.
Land relief in Europe shows great variation within relatively small areas. The southern regions are more mountainous, while moving north the terrain descends from the high Alps, Pyrenees and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. This extended lowland is known as the Great European Plain and at its heart lies the North German Plain. An arc of uplands also exists along the north-western seaboard, which begins in the western parts of the islands of Britain and Ireland, and then continues along the mountainous, fjord-cut spine of Norway.

This description is simplified. Subregions such as the Iberian Peninsula and the Italian Peninsula contain their own complex features, as does mainland Central Europe itself, where the relief contains many plateaus, river valleys and basins that complicate the general trend. Sub-regions like Iceland, Britain and Ireland are special cases. The former is a land unto itself in the northern ocean that is counted as part of Europe, while the latter are upland areas that were once joined to the mainland until rising sea levels cut them off.

Europe lies mainly in the temperate climate zone of the northern hemisphere, where the prevailing wind direction is from the west. The climate is milder in comparison to other areas of the same latitude around the globe due to the influence of the Gulf Stream, an ocean current which carries warm water from the Gulf of Mexico across the Atlantic ocean to Europe. The Gulf Stream is nicknamed "Europe's central heating", because it makes Europe's climate warmer and wetter than it would otherwise be. The Gulf Stream not only carries warm water to Europe's coast but also warms up the prevailing westerly winds that blow across the continent from the Atlantic Ocean.

Therefore, the average temperature throughout the year of Aveiro is , while it is only in New York City which is almost on the same latitude, bordering the same ocean. Berlin, Germany; Calgary, Canada; and Irkutsk, in far south-eastern Russia, lie on around the same latitude; January temperatures in Berlin average around higher than those in Calgary and they are almost higher than average temperatures in Irkutsk.

The large water masses of the Mediterranean Sea, which equalise the temperatures on an annual and daily average, are also of particular importance. The water of the Mediterranean extends from the Sahara desert to the Alpine arc in its northernmost part of the Adriatic Sea near Trieste.

In general, Europe is not just colder towards the north compared to the south, but it also gets colder from the west towards the east. The climate is more oceanic in the west and less so in the east. This can be illustrated by the following table of average temperatures at locations roughly following the 64th, 60th, 55th, 50th, 45th and 40th latitudes. None of them is located at high altitude; most of them are close to the sea.

It is notable how the average temperatures for the coldest month, as well as the annual average temperatures, drop from the west to the east. For instance, Edinburgh is warmer than Belgrade during the coldest month of the year, although Belgrade is around 10° of latitude farther south.

The geological history of Europe traces back to the formation of the Baltic Shield (Fennoscandia) and the Sarmatian craton, both around 2.25 billion years ago, followed by the Volgo–Uralia shield, the three together leading to the East European craton (≈ Baltica) which became a part of the supercontinent Columbia. Around 1.1 billion years ago, Baltica and Arctica (as part of the Laurentia block) became joined to Rodinia, later resplitting around 550 million years ago to reform as Baltica. Around 440 million years ago Euramerica was formed from Baltica and Laurentia; a further joining with Gondwana then leading to the formation of Pangea. Around 190 million years ago, Gondwana and Laurasia split apart due to the widening of the Atlantic Ocean. Finally and very soon afterwards, Laurasia itself split up again, into Laurentia (North America) and the Eurasian continent. The land connection between the two persisted for a considerable time, via Greenland, leading to interchange of animal species. From around 50 million years ago, rising and falling sea levels have determined the actual shape of Europe and its connections with continents such as Asia. Europe's present shape dates to the late Tertiary period about five million years ago.

The geology of Europe is hugely varied and complex and gives rise to the wide variety of landscapes found across the continent, from the Scottish Highlands to the rolling plains of Hungary. Europe's most significant feature is the dichotomy between highland and mountainous Southern Europe and a vast, partially underwater, northern plain ranging from Ireland in the west to the Ural Mountains in the east. These two halves are separated by the mountain chains of the Pyrenees and Alps/Carpathians. The northern plains are delimited in the west by the Scandinavian Mountains and the mountainous parts of the British Isles. Major shallow water bodies submerging parts of the northern plains are the Celtic Sea, the North Sea, the Baltic Sea complex and Barents Sea.

The northern plain contains the old geological continent of Baltica and so may be regarded geologically as the "main continent", while peripheral highlands and mountainous regions in the south and west constitute fragments from various other geological continents. Most of the older geology of western Europe existed as part of the ancient microcontinent Avalonia.

Having lived side by side with agricultural peoples for millennia, Europe's animals and plants have been profoundly affected by the presence and activities of humans. With the exception of Fennoscandia and northern Russia, few areas of untouched wilderness are currently found in Europe, except for various national parks.

The main natural vegetation cover in Europe is mixed forest. The conditions for growth are very favourable. In the north, the Gulf Stream and North Atlantic Drift warm the continent. Southern Europe has a warm but mild climate. There are frequent summer droughts in this region. Mountain ridges also affect the conditions. Some of these, such as the Alps and the Pyrenees, are oriented east–west and allow the wind to carry large masses of water from the ocean in the interior. Others are oriented south–north (Scandinavian Mountains, Dinarides, Carpathians, Apennines) and because the rain falls primarily on the side of mountains that is oriented towards the sea, forests grow well on this side, while on the other side, the conditions are much less favourable. Few corners of mainland Europe have not been grazed by livestock at some point in time, and the cutting down of the preagricultural forest habitat caused disruption to the original plant and animal ecosystems.

Possibly 80 to 90 percent of Europe was once covered by forest. It stretched from the Mediterranean Sea to the Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European forest dwelling species which require a mixture of tree species and diverse forest structure. The amount of natural forest in Western Europe is just 2–3% or less, while in its Western Russia its 5–10%. The European country with the smallest percentage of forested area is Iceland (1%), while the most forested country is Finland (77%).

In temperate Europe, mixed forest with both broadleaf and coniferous trees dominate. The most important species in central and western Europe are beech and oak. In the north, the taiga is a mixed spruce–pine–birch forest; further north within Russia and extreme northern Scandinavia, the taiga gives way to tundra as the Arctic is approached. In the Mediterranean, many olive trees have been planted, which are very well adapted to its arid climate; Mediterranean Cypress is also widely planted in southern Europe. The semi-arid Mediterranean region hosts much scrub forest. A narrow east–west tongue of Eurasian grassland (the steppe) extends westwards from Ukraine and southern Russia and ends in Hungary and traverses into taiga to the north.

Glaciation during the most recent ice age and the presence of humans affected the distribution of European fauna. As for the animals, in many parts of Europe most large animals and top predator species have been hunted to extinction. The woolly mammoth was extinct before the end of the Neolithic period. Today wolves (carnivores) and bears (omnivores) are endangered. Once they were found in most parts of Europe. However, deforestation and hunting caused these animals to withdraw further and further. By the Middle Ages the bears' habitats were limited to more or less inaccessible mountains with sufficient forest cover. Today, the brown bear lives primarily in the Balkan peninsula, Scandinavia and Russia; a small number also persist in other countries across Europe (Austria, Pyrenees etc.), but in these areas brown bear populations are fragmented and marginalised because of the destruction of their habitat. In addition, polar bears may be found on Svalbard, a Norwegian archipelago far north of Scandinavia. The wolf, the second-largest predator in Europe after the brown bear, can be found primarily in Central and Eastern Europe and in the Balkans, with a handful of packs in pockets of Western Europe (Scandinavia, Spain, etc.).

Other carnivores include the European wildcat, red fox and arctic fox, the golden jackal, different species of martens, the European hedgehog, different species of reptiles (like snakes such as vipers and grass snakes) and amphibians, as well as different birds (owls, hawks and other birds of prey).

Important European herbivores are snails, larvae, fish, different birds and mammals, like rodents, deer and roe deer, boars and living in the mountains, marmots, steinbocks, chamois among others. A number of insects, such as the small tortoiseshell butterfly, add to the biodiversity.

Sea creatures are also an important part of European flora and fauna. The sea flora is mainly phytoplankton. Important animals that live in European seas are zooplankton, molluscs, echinoderms, different crustaceans, squids and octopuses, fish, dolphins and whales.

Biodiversity is protected in Europe through the Council of Europe's Bern Convention, which has also been signed by the European Community as well as non-European states.

The political map of Europe is substantially derived from the re-organisation of Europe following the Napoleonic Wars in 1815. The prevalent form of government in Europe is parliamentary democracy, in most cases in the form of Republic; in 1815, the prevalent form of government was still the Monarchy. Europe's remaining eleven monarchies are constitutional.

European integration is the process of political, legal, economic (and in some cases social and cultural) integration of European states as it has been pursued by the powers sponsoring the Council of Europe since the end of the Second World War. The European Union has been the focus of economic integration on the continent since its foundation in 1993. More recently, the Eurasian Economic Union has been established as a counterpart comprising former Soviet states.

27 European states are members of the politico-economic European Union, 26 of the border-free Schengen Area and 20 of the monetary union Eurozone. Among the smaller European organisations are the Nordic Council, the Benelux, the Baltic Assembly and the Visegrád Group.

This list includes all internationally recognised sovereign countries falling even partially under any common geographical or political definitions of Europe.

Within the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:

Several dependencies and similar territories with broad autonomy are also found within or close to Europe. This includes Åland (an autonomous county of Finland), two autonomous territories of the Kingdom of Denmark (other than Denmark proper), three Crown Dependencies and two British Overseas Territories. Svalbard is also included due to its unique status within Norway, although it is not autonomous. Not included are the three countries of the United Kingdom with devolved powers and the two Autonomous Regions of Portugal, which despite having a unique degree of autonomy, are not largely self-governing in matters other than international affairs. Areas with little more than a unique tax status, such as the Canary Islands and Heligoland, are also not included for this reason.

As a continent, the economy of Europe is currently the largest on Earth and it is the richest region as measured by assets under management with over $32.7 trillion compared to North America's $27.1 trillion in 2008. In 2009 Europe remained the wealthiest region. Its $37.1 trillion in assets under management represented one-third of the world's wealth. It was one of several regions where wealth surpassed its precrisis year-end peak. As with other continents, Europe has a large wealth gap among its countries. The richer states tend to be in the Northwest and West in general, followed by Central Europe, while most economies of Eastern and Southeastern Europe are still reemerging from the collapse of the Soviet Union and the breakup of Yugoslavia.

The model of the Blue Banana was designed as an economic geographic representation of the respective economic power of the regions, which was further developed into the Golden Banana or Blue Star. The trade between East and West, as well as towards Asia, which had been disrupted for a long time by the two world wars, new borders and the Cold War, increased sharply after 1989. In addition, there is new impetus from the Chinese Belt and Road Initiative across the Suez Canal towards Africa and Asia.

The European Union, a political entity composed of 27 European states, comprises the largest single economic area in the world. Nineteen EU countries share the euro as a common currency.
Five European countries rank in the top ten of the world's largest national economies in GDP (PPP). This includes (ranks according to the CIA): Germany (6), Russia (7), the United Kingdom (10), France (11) and Italy (13).

Some European countries are much richer than others. The richest in terms of nominal GDP is Monaco with its US$185,829 per capita (2018) and the poorest is Ukraine with its US$3,659 per capita (2019).

As a whole, Europe's GDP per capita is US$21,767 according to a 2016 International Monetary Fund assessment.

Capitalism has been dominant in the Western world since the end of feudalism. From Britain, it gradually spread throughout Europe. The Industrial Revolution started in Europe, specifically the United Kingdom in the late 18th century, and the 19th century saw Western Europe industrialise. Economies were disrupted by the First World War, but by the beginning of the Second World War, they had recovered and were having to compete with the growing economic strength of the United States. The Second World War, again, damaged much of Europe's industries.

After the Second World War the economy of the UK was in a state of ruin, and continued to suffer relative economic decline in the following decades. Italy was also in a poor economic condition but regained a high level of growth by the 1950s. West Germany recovered quickly and had doubled production from pre-war levels by the 1950s. France also staged a remarkable comeback enjoying rapid growth and modernisation; later on Spain, under the leadership of Franco, also recovered and the nation recorded huge unprecedented economic growth beginning in the 1960s in what is called the Spanish miracle. The majority of Central and Eastern European states came under the control of the Soviet Union and thus were members of the Council for Mutual Economic Assistance (COMECON).

The states which retained a free-market system were given a large amount of aid by the United States under the Marshall Plan. The western states moved to link their economies together, providing the basis for the EU and increasing cross border trade. This helped them to enjoy rapidly improving economies, while those states in COMECON were struggling in a large part due to the cost of the Cold War. Until 1990, the European Community was expanded from 6 founding members to 12. The emphasis placed on resurrecting the West German economy led to it overtaking the UK as Europe's largest economy.


With the fall of communism in Central and Eastern Europe in 1991, the post-socialist states underwent shock therapy measures to liberalise their economies and implement free market reforms.

After East and West Germany were reunited in 1990, the economy of West Germany struggled as it had to support and largely rebuild the infrastructure of East Germany, while the latter experienced sudden mass unemployment and plummeting of industrial production.

By the millennium change, the EU dominated the economy of Europe, comprising the five largest European economies of the time: Germany, the United Kingdom, France, Italy, and Spain. In 1999, 12 of the 15 members of the EU joined the Eurozone, replacing their national currencies by the euro.

Figures released by Eurostat in 2009 confirmed that the Eurozone had gone into recession in 2008. It impacted much of the region. In 2010, fears of a sovereign debt crisis developed concerning some countries in Europe, especially Greece, Ireland, Spain and Portugal. As a result, measures were taken, especially for Greece, by the leading countries of the Eurozone. The EU-27 unemployment rate was 10.3% in 2012. For those aged 15–24 it was 22.4%.

The population of Europe was about 742 million in 2023 according to UN estimates. This is slightly more than one ninth of the world's population. The population density of Europe (the number of people per area) is the second highest of any continent, behind Asia. The population of Europe is currently slowly decreasing, by about 0.2% per year, because there are fewer births than deaths. This natural decrease in population is reduced by the fact that more people migrate to Europe from other continents than vice versa.

Southern Europe and Western Europe are the regions with the highest average number of elderly people in the world. In 2021, the percentage of people over 65 years old was 21% in Western Europe and Southern Europe, compared to 19% in all of Europe and 10% in the world. Projections suggest that by 2050 Europe will reach 30%. This is caused by the fact that the population has been having children below replacement level since the 1970s. The United Nations predicts that Europe will decline its population between 2022 and 2050 by −7 per cent, without changing immigration movements.

According to a population projection of the UN Population Division, Europe's population may fall to between 680 and 720 million people by 2050, which would be 7% of the world population at that time. Within this context, significant disparities exist between regions in relation to fertility rates. The average number of children per female of child-bearing age is 1.52, far below the replacement rate. The UN predicts a steady population decline in Central and Eastern Europe as a result of emigration and low birth rates.

Pan and Pfeil (2004) count 87 distinct "peoples of Europe", of which 33 form the majority population in at least one sovereign state, while the remaining 54 constitute ethnic minorities.

Europe is home to the highest number of migrants of all global regions at nearly 87 million people in 2020, according to the International Organisation for Migration. In 2005, the EU had an overall net gain from immigration of 1.8 million people. This accounted for almost 85% of Europe's total population growth. In 2021, 827,000 persons were given citizenship of an EU member state, an increase of about 14% compared with 2020. 2.3 million immigrants from non-EU countries entered the EU in 2021.

Early modern emigration from Europe began with Spanish and Portuguese settlers in the 16th century, and French and English settlers in the 17th century. But numbers remained relatively small until waves of mass emigration in the 19th century, when millions of poor families left Europe.

Today, large populations of European descent are found on every continent. European ancestry predominates in North America and to a lesser degree in South America (particularly in Uruguay, Argentina, Chile and Brazil, while most of the other Latin American countries also have a considerable population of European origins). Australia and New Zealand have large European-derived populations. Africa has no countries with European-derived majorities (or with the exception of Cape Verde and probably São Tomé and Príncipe, depending on context), but there are significant minorities, such as the White South Africans in South Africa. In Asia, European-derived populations, specifically Russians, predominate in North Asia and some parts of Northern Kazakhstan.

Europe has about 225 indigenous languages, mostly falling within three Indo-European language groups: the Romance languages, derived from the Latin of the Roman Empire; the Germanic languages, whose ancestor language came from southern Scandinavia; and the Slavic languages. Slavic languages are mostly spoken in Southern, Central and Eastern Europe. Romance languages are spoken primarily in Western and Southern Europe, as well as in Switzerland in Central Europe and Romania and Moldova in Eastern Europe. Germanic languages are spoken in Western, Northern and Central Europe as well as in Gibraltar and Malta in Southern Europe. Languages in adjacent areas show significant overlaps (such as in English, for example). Other Indo-European languages outside the three main groups include the Baltic group (Latvian and Lithuanian), the Celtic group (Irish, Scottish Gaelic, Manx, Welsh, Cornish and Breton), Greek, Armenian and Albanian.

A distinct non-Indo-European family of Uralic languages (Estonian, Finnish, Hungarian, Erzya, Komi, Mari, Moksha and Udmurt) is spoken mainly in Estonia, Finland, Hungary and parts of Russia. Turkic languages include Azerbaijani, Kazakh and Turkish, in addition to smaller languages in Eastern and Southeast Europe (Balkan Gagauz Turkish, Bashkir, Chuvash, Crimean Tatar, Karachay-Balkar, Kumyk, Nogai and Tatar). Kartvelian languages (Georgian, Mingrelian and Svan) are spoken primarily in Georgia. Two other language families reside in the North Caucasus (termed Northeast Caucasian, most notably including Chechen, Avar and Lezgin; and Northwest Caucasian, most notably including Adyghe). Maltese is the only Semitic language that is official within the EU, while Basque is the only European language isolate.

Multilingualism and the protection of regional and minority languages are recognised political goals in Europe today. The Council of Europe Framework Convention for the Protection of National Minorities and the Council of Europe's European Charter for Regional or Minority Languages set up a legal framework for language rights in Europe.

The largest religion in Europe is Christianity, with 76.2% of Europeans considering themselves Christians, including Catholic, Eastern Orthodox and various Protestant denominations. Among Protestants, the most popular are Lutheranism, Anglicanism and the Reformed faith. Smaller Protestant denominations include Anabaptists as well as denominations centered in the United States such as Pentecostalism, Methodism, and Evangelicalism. Although Christianity originated in the Middle East, its centre of mass shifted to Europe when it became the official religion of the Roman Empire in the late 4th century. Christianity played a prominent role in the development of the European culture and identity. Today, a bit over 25% of the world's Christians live in Europe.

Islam is the second most popular religion in Europe. Over 25 million, or roughly 5% of the population, adhere to it. In Albania and Bosnia and Herzegovina, two countries in the Balkan peninsula in Southeastern Europe, Islam instead of Christianity is the majority religion. This is also the case in Turkey and in certain parts of Russia, as well as in Azerbaijan and Kazakhstan, all of which are at the border to Asia. Many countries in Europe are home to a sizeable Muslim minority, and immigration to Europe has increased the number of Muslim people in Europe in recent years.

The Jewish population in Europe was about 1.4 million people in 2020 (about 0.2% of the population). There is a long history of Jewish life in Europe, beginning in antiquity. During the late 19th and early 20th centuries, the Russian Empire had the majority of the world's Jews living within its borders. In 1897, according to Russian census of 1897, the total Jewish population of Russia was 5.1 million people, which was 4.13% of total population. Of this total, the vast majority lived within the Pale of Settlement. In 1933, there were about 9.5 million Jewish people in Europe, representing 1.7% of the population, but most were killed, and most of the rest displaced, during The Holocaust. In the 21st century, France has the largest Jewish population in Europe, followed by the United Kingdom, Germany and Russia.

Other religions practiced in Europe include Hinduism and Buddhism, which are minority religions, except in Russia's Republic of Kalmykia, where Tibetan Buddhism is the majority religion.

A large and increasing number of people in Europe are irreligious, atheist and agnostic. They are estimated to make up about 18.3% of Europe's population currently.

The three largest urban areas of Europe are Moscow, London and Paris. All have over 10 million residents, and as such have been described as megacities. While Istanbul has the highest total city population, it lies partly in Asia. 64.9% of the residents live on the European side and 35.1% on the Asian side.
The next largest cities in order of population are Madrid, Saint Petersburg, Milan, Barcelona, Berlin, and Rome each having over three million residents.

When considering the commuter belts or metropolitan areas within Europe (for which comparable data is available), Moscow covers the largest population, followed in order by Istanbul, London, Paris, Madrid, Milan, Ruhr Area, Saint Petersburg, Rhein-Süd, Barcelona and Berlin.

"Europe" as a cultural concept is substantially derived from the shared heritage of ancient Greece and the Roman Empire and its cultures. The boundaries of Europe were historically understood as those of Christendom (or more specifically Latin Christendom), as established or defended throughout the medieval and early modern history of Europe, especially against Islam, as in the Reconquista and the Ottoman wars in Europe.

This shared cultural heritage is combined by overlapping indigenous national cultures and folklores, roughly divided into Slavic, Latin (Romance) and Germanic, but with several components not part of either of these groups (notably Greek, Basque and Celtic). Historically, special examples with overlapping cultures are Strasbourg with Latin (Romance) and Germanic, or Trieste with Latin, Slavic and Germanic roots.
Cultural contacts and mixtures shape a large part of the regional cultures of Europe. Europe is often described as "maximum cultural diversity with minimal geographical distances".

Different cultural events are organised in Europe, with the aim of bringing different cultures closer together and raising awareness of their importance, such as the European Capital of Culture, the European Region of Gastronomy, the European Youth Capital and the European Capital of Sport.

In Europe many people are unable to access basic social conditions, which makes it harder for them to thrive and flourish. Access to basic necessities can be compromised, for example 10% of Europeans spend at least 40% of household income on housing. 75 million Europeans feel socially isolated. From the 1980s income inequality has been rising and wage shares have been falling. In 2016, the richest 20% of households earned over five times more than the poorest 20%. Many workers experience stagnant real wages and precarious work is common even for essential workers.


Historical Maps

Europa

Europa may refer to:























Euglenozoa

Euglenozoa are a large group of flagellate Discoba. They include a variety of common free-living species, as well as a few important parasites, some of which infect humans. Euglenozoa are represented by four major groups, "i.e.," Kinetoplastea, Diplonemea, Euglenida, and Symbiontida. Euglenozoa are unicellular, mostly around in size, although some euglenids get up to long.

Most euglenozoa have two flagella, which are inserted parallel to one another in an apical or subapical pocket. In some these are associated with a cytostome or mouth, used to ingest bacteria or other small organisms. This is supported by one of three sets of microtubules that arise from the flagellar bases; the other two support the dorsal and ventral surfaces of the cell.

Some other euglenozoa feed through absorption, and many euglenids possess chloroplasts, the only eukaryotes outside Diaphoretickes to do so without performing kleptoplasty, and so obtain energy through photosynthesis. These chloroplasts are surrounded by three membranes and contain chlorophylls "A" and "B", along with other pigments, so are probably derived from a green alga, captured long ago in an endosymbiosis by a basal euglenozoan. Reproduction occurs exclusively through cell division. During mitosis, the nuclear membrane remains intact, and the spindle microtubules form inside of it.

The group is characterized by the ultrastructure of the flagella. In addition to the normal supporting microtubules or axoneme, each contains a rod (called "paraxonemal"), which has a tubular structure in one flagellum and a latticed structure in the other. Based on this, two smaller groups have been included here: the diplonemids and "Postgaardi".

Historically, euglenozoans have been treated as either plants or animals, depending on whether they belong to largely photosynthetic groups or not. Hence they have names based on either the International Code of Nomenclature for algae, fungi, and plants (ICNafp) or the International Code of Zoological Nomenclature (ICZN). For example, one family has the name Euglenaceae under the ICNafp and the name Euglenidae under the ICZN. As another example, the genus name "Dinema" is acceptable under the ICZN, but illegitimate under the ICNafp, as it is a later homonym of an orchid genus, so that the synonym "Dinematomonas" must be used instead.

The Euglenozoa are generally accepted as monophyletic. They are related to Percolozoa; the two share mitochondria with disk-shaped cristae, which only occurs in a few other groups.
Both probably belong to a larger group of eukaryotes called the Excavata. This grouping, though, has been challenged.

The phylogeny based on the work of Cavalier-Smith (2016):
A consensus phylogeny following the review by Kostygov "et al." (2021): 

The following classification of Euglenozoa is as described by Cavalier-Smith in 2016, modified to include the new subphylum Plicomonada according to Cavalier-Smith "et al" (2017).

Phylum Euglenozoa [Euglenobionta]



Phylum Euglenozoa 


Epistemology

Epistemology (; ) is the branch of philosophy concerned with knowledge. Epistemologists study the nature, origin, and scope of knowledge, epistemic justification, the rationality of belief, and various related issues. Debates in (contemporary) epistemology are generally clustered around four core areas:


In these debates and others, epistemology aims to answer questions such as "What do people know?", "What does it mean to say that people know something?", "What makes justified beliefs justified?", and "How do people know that they know?" Specialties in epistemology ask questions such as "How can people create formal models about issues related to knowledge?" (in formal epistemology), "What are the historical conditions of changes in different kinds of knowledge?" (in historical epistemology), "What are the methods, aims, and subject matter of epistemological inquiry?" (in metaepistemology), and "How do people know together?" (in social epistemology).

The etymology of the word "epistemology" is derived from the ancient Greek "epistēmē", meaning "knowledge, understanding, skill, scientific knowledge", and the English suffix "-ology", meaning "the study or discipline of (what is indicated by the first element)". The word "epistemology" first appeared in 1847, in a review in New York's "Eclectic Magazine" :

The word was first used to present a philosophy in English by Scottish philosopher James Frederick Ferrier in 1854. It was the title of the first section of his "Institutes of Metaphysics":

The entry "Knowledge How" of the Stanford Encyclopedia of Philosophy mentions that introductory classes to epistemology often start their analysis of knowledge by pointing out three different senses of "knowing" something: "knowing that" (knowing the truth of propositions), "knowing how" (understanding how to perform certain actions), and "knowing by acquaintance" (directly perceiving an object, being familiar with it, or otherwise coming into contact with it). Epistemology is primarily concerned with the first of these forms of knowledge, propositional knowledge. All three senses of "knowing" can be seen in our ordinary use of the word. In mathematics, you can know 2 + 2 = 4, but there is also knowing to add two numbers, and knowing a (e.g., knowing other persons, or knowing oneself), (e.g., one's hometown), (e.g., cars), or (e.g., addition). While these distinctions are not explicit in English, they are explicitly made in other languages, including French, Italian, Portuguese, Spanish, Romanian, German and Dutch (although some languages closely related to English have been said to retain these verbs, such as Scots).. In French, Portuguese, Spanish, Romanian, German, and Dutch 'to know (a person)' is translated using , , , and (both German and Dutch) respectively, whereas 'to know (how to do something)' is translated using , (both Portuguese and Spanish), , , and . Modern Greek has the verbs () and (). Italian has the verbs and and the nouns for 'knowledge' are and . German has the verbs and ; the former implies knowing a fact, the latter knowing in the sense of being acquainted with and having a working knowledge of; there is also a noun derived from , namely , which has been said to imply knowledge in the form of recognition or acknowledgment. The verb itself implies a process: you have to go from one state to another, from a state of "not-" to a state of true . This verb seems the most appropriate in terms of describing the "episteme" in one of the modern European languages, hence the German name "". The theoretical interpretation and significance of these linguistic issues remains controversial. The distinction is most pronounced in Polish, where means "to know", means "to know how" and means "to be familiar with" (to "know" a person).

In his paper "On Denoting" and his later book "Problems of Philosophy", Bertrand Russell brought a great deal of attention to the distinction between "knowledge by description" and "knowledge by acquaintance". Gilbert Ryle is similarly credited with bringing more attention to the distinction between knowing how and knowing that in "The Concept of Mind". In "Personal Knowledge", Michael Polanyi argues for the epistemological relevance of knowledge how and knowledge that; using the example of the act of balance involved in riding a bicycle, he suggests that the theoretical knowledge of the physics involved in maintaining a state of balance cannot substitute for the practical knowledge of how to ride, and that it is important to understand how both are established and grounded. This position is essentially Ryle's, who argued that a failure to acknowledge the distinction between "knowledge that" and "knowledge how" leads to infinite regress.

One of the most important distinctions in epistemology is between what can be known "a priori" (independently of experience) and what can be known "a posteriori" (through experience). The terms originate from the Analytic methods of Aristotle's Organon, and may be roughly defined as follows:

Views that emphasize the importance of "a priori" knowledge are generally classified as rationalist. Views that emphasize the importance of "a posteriori" knowledge are generally classified as empiricist.

One of the core concepts in epistemology is "belief". A belief is an attitude that a person holds regarding anything that they take to be true. For instance, to believe that snow is white is comparable to accepting the truth of the proposition "snow is white". Beliefs can be "occurrent" (e.g. a person actively thinking "snow is white"), or they can be "dispositional" (e.g. a person who if asked about the color of snow would assert "snow is white"). While there is not universal agreement about the nature of belief, most contemporary philosophers hold the view that a disposition to express belief "B" qualifies as holding the belief "B". There are various different ways that contemporary philosophers have tried to describe beliefs, including as representations of ways that the world could be (Jerry Fodor), as dispositions to act as if certain things are true (Roderick Chisholm), as interpretive schemes for making sense of someone's actions (Daniel Dennett and Donald Davidson), or as mental states that fill a particular function (Hilary Putnam). Some have also attempted to offer significant revisions to our notion of belief, including eliminativists about belief who argue that there is no phenomenon in the natural world which corresponds to our folk psychological concept of belief (Paul Churchland) and formal epistemologists who aim to replace our bivalent notion of belief ("either I have a belief or I don't have a belief") with the more permissive, probabilistic notion of credence ("there is an entire spectrum of degrees of belief, not a simple dichotomy between belief and non-belief").

While belief plays a significant role in epistemological debates surrounding knowledge and justification, it also has many other philosophical debates in its own right. Notable debates include: "What is the rational way to revise one's beliefs when presented with various sorts of evidence?"; "Is the content of our beliefs entirely determined by our mental states, or do the relevant facts have any bearing on our beliefs (e.g. if I believe that I'm holding a glass of water, is the non-mental fact that water is HO part of the content of that belief)?"; "How fine-grained or coarse-grained are our beliefs?"; and "Must it be possible for a belief to be expressible in language, or are there non-linguistic beliefs?"

Truth is the property or state of being in accordance with facts or reality. On most views, truth is the correspondence of language or thought to a mind-independent world. This is called the correspondence theory of truth. Among philosophers who think that it is possible to analyze the conditions necessary for knowledge, virtually all of them accept that truth is such a condition. There is much less agreement about the extent to which a knower must know "why" something is true in order to know. On such views, something being known implies that it is true. However, this should not be confused for the more contentious view that one must know that one knows in order to know (the KK principle).

Epistemologists disagree about whether belief is the only truth-bearer. Other common suggestions for things that can bear the property of being true include propositions, sentences, thoughts, utterances, and judgments. Plato, in his Gorgias, argues that belief is the most commonly invoked truth-bearer.

Many of the debates regarding truth are at the crossroads of epistemology and logic. Some contemporary debates regarding truth include: How do we define truth? Is it even possible to give an informative definition of truth? What things are truth-bearers and are therefore capable of being true or false? Are truth and falsity bivalent, or are there other truth values? What are the criteria of truth that allow us to identify it and to distinguish it from falsity? What role does truth play in constituting knowledge? And is truth absolute, or is it merely relative to one's perspective?

As the term "justification" is used in epistemology, a belief is justified if one has good reason for holding it. Loosely speaking, justification is the "reason" that someone holds a rationally admissible belief, on the assumption that it is a "good reason" for holding it. Sources of justification might include perceptual experience (the evidence of the senses), reason, and authoritative testimony, among others. Importantly however, a belief being justified does "not" guarantee that the belief is true, since a person could be justified in forming beliefs based on very convincing evidence that was nonetheless deceiving.

A central debate about the nature of justification is a debate between epistemological externalists on the one hand and epistemological internalists on the other. While epistemic externalism first arose in attempts to overcome the Gettier problem, it has flourished in the time since as an alternative way of conceiving of epistemic justification. The initial development of epistemic externalism is often attributed to Alvin Goldman, although numerous other philosophers have worked on the topic in the time since.

Externalists hold that factors deemed "external", meaning outside of the psychological states of those who gain knowledge, can be conditions of justification. For example, an externalist response to the Gettier problem is to say that for a justified true belief to count as knowledge, there must be a link or dependency between the belief and the state of the external world. Usually, this is understood to be a causal link. Such causation, to the extent that it is "outside" the mind, would count as an external, knowledge-yielding condition. Internalists, on the other hand, assert that all knowledge-yielding conditions are within the psychological states of those who gain knowledge.

Though unfamiliar with the internalist/externalist debate himself, many point to René Descartes as an early example of the internalist path to justification. He wrote that because the only method by which we perceive the external world is through our senses, and that, because the senses are not infallible, we should not consider our concept of knowledge infallible. The only way to find anything that could be described as "indubitably true", he advocates, would be to see things "clearly and distinctly". He argued that if there is an omnipotent, good being who made the world, then it is reasonable to believe that people are made with the ability to know. However, this does not mean that man's ability to know is perfect. God gave man the ability to know but not with omniscience. Descartes said that man must use his capacities for knowledge correctly and carefully through methodological doubt.

The dictum "Cogito ergo sum" (I think, therefore I am) is also commonly associated with Descartes's theory. In his own methodological doubt—doubting everything he previously knew so he could start from a blank slate—the first thing that he could not logically bring himself to doubt was his own existence: "I do not exist" would be a contradiction in terms. The act of saying that one does not exist assumes that someone must be making the statement in the first place. Descartes could doubt his senses, his body, and the world around him—but he could not deny his own existence, because he was able to doubt and must exist to manifest that doubt. Even if some "evil genius" were deceiving him, he would have to exist to be deceived. This one sure point provided him with what he called his Archimedean point, in order to further develop his foundation for knowledge. Simply put, Descartes's epistemological justification depended on his indubitable belief in his own existence and his clear and distinct knowledge of God.

A central issue in epistemology is the question of what the nature of knowledge is or how to define it. Sometimes the expressions "theory of knowledge" and "analysis of knowledge" are used specifically for this form of inquiry. The term "knowledge" has various meanings in natural language. It can refer to an awareness of facts, as in knowing that Mars is a planet, to a possession of skills, as in knowing how to swim, or to an experiential acquaintance, as in knowing Daniel Craig personally. Factual knowledge, also referred to as "propositional knowledge" or "descriptive knowledge", plays a special role in epistemology. On the linguistic level, it is distinguished from the other forms of knowledge since it can be expressed through a that-clause, i.e. using a formulation like "They know that..." followed by the known proposition.

Some features of factual knowledge are widely accepted: it is a form of cognitive success that establishes epistemic contact with reality. However, there are still various disagreements about its exact nature even though it has been studied intensely. Different factors are responsible for these disagreements. Some theorists try to furnish a practically useful definition by describing its most noteworthy and easily identifiable features. Others engage in an analysis of knowledge, which aims to provide a theoretically precise definition that identifies the set of essential features characteristic for all instances of knowledge and only for them. Differences in the methodology may also cause disagreements. In this regard, some epistemologists use abstract and general intuitions in order to arrive at their definitions. A different approach is to start from concrete individual cases of knowledge to determine what all of them have in common. Yet another method is to focus on linguistic evidence by studying how the term "knowledge" is commonly used. Different standards of knowledge are further sources of disagreement. A few theorists set these standards very high by demanding that absolute certainty or infallibility is necessary. On such a view, knowledge is a very rare thing. Theorists more in tune with ordinary language usually demand lower standards and see knowledge as something commonly found in everyday life.

The historically most influential definition, discussed since ancient Greek philosophy, characterizes knowledge in relation to three essential features: as (1) a belief that is (2) true and (3) justified. There is still wide acceptance that the first two features are correct, i.e. that knowledge is a mental state that affirms a true proposition. However, there is a lot of dispute about the third feature: justification. This feature is usually included to distinguish knowledge from true beliefs that rest on superstition, lucky guesses, or faulty reasoning. This expresses the idea that knowledge is not the same as being right about something. Traditionally, justification is understood as the possession of evidence: a belief is justified if the believer has good evidence supporting it. Such evidence could be a perceptual experience, a memory, or a second belief.

The justified-true-belief account of knowledge came under severe criticism in the second half of the 20th century, when Edmund Gettier proposed various counterexamples. In a famous example of what came to be known as a Gettier-case, a person is driving on a country road lined with multiple barn façades, only one of which is real barn, but it is not possible to tell the difference between them from the road. The person then stops by a fortuitous coincidence in front of the only real barn and forms the belief that it is a barn. The idea behind this thought experiment is that this is not knowledge even though the belief is both justified and true. The reason is that it is just a lucky accident since the person cannot tell the difference: They would have formed exactly the same justified belief if they had stopped at another site, in which case the belief would have been false.

Various additional examples were proposed along similar lines. Most of them involve a justified true belief that apparently fails to amount to knowledge because the belief's justification is in some sense not relevant to its truth. These counterexamples have provoked very diverse responses. Some theorists think that one only needs to modify one's conception of justification to avoid them. But the more common approach is to search for an additional criterion. On this view, all cases of knowledge involve a justified true belief but some justified true beliefs do not amount to knowledge since they lack this additional feature. There are diverse suggestions for this fourth criterion. Some epistemologists require that no false belief is involved in the justification or that no defeater of the belief is present. A different approach is to require that the belief tracks truth, i.e. that the person would not have the belief if it was false. Some even require that the justification has to be infallible, i.e. that it necessitates the belief's truth.

A quite different approach is to affirm that the justified-true-belief account of knowledge is deeply flawed and to seek a complete reconceptualization of knowledge. These reconceptualizations often do not require justification at all. One such approach is to require that the true belief was produced by a reliable process. Naturalized epistemologists often hold that the believed fact has to cause the belief. Virtue theorists are also interested in how the belief is produced. For them, the belief must be a manifestation of a cognitive virtue.

The primary value problem is to determine why knowledge should be more valuable than simply true belief. In Plato's "Meno", Socrates points out to Meno that a man who knew the way to Larissa could lead others there correctly. But so, too, could a man who had true beliefs about how to get there, even if he had not gone there or had any knowledge of Larissa. Socrates says that it seems that both knowledge and true opinion can guide action. Meno then wonders why knowledge is valued more than true belief and why knowledge and true belief are different. Socrates responds that knowledge, unlike belief, must be 'tied down' to the truth, like the mythical tethered statues of Daedalus.

More generally, the problem is to identify what (if anything) makes knowledge more valuable than a minimal conjunction of its components such as mere true belief or justified true belief. Other components considered besides belief, truth and justification are safety, sensitivity, statistical likelihood, and any anti-Gettier condition. This is done within analyses that conceive of knowledge as divided into components. Knowledge-first epistemological theories, which posit knowledge as fundamental, are notable exceptions to these kind of analyses. The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link to the concept of value in ethics.

In contemporary philosophy, epistemologists including Ernest Sosa, John Greco, Jonathan Kvanvig, Linda Zagzebski, and Duncan Pritchard have defended virtue epistemology as a solution to the value problem. They argue that epistemology should also evaluate the "properties" of people as epistemic agents (i.e. intellectual virtues), rather than merely the properties of propositions and propositional mental attitudes.

The value problem has been presented as an argument against epistemic reliabilism by Linda Zagzebski, Wayne Riggs, and Richard Swinburne, among others. Zagzebski analogizes the value of knowledge to the value of espresso produced by an espresso maker: "The liquid in this cup is not improved by the fact that it comes from a reliable espresso maker. If the espresso tastes good, it makes no difference if it comes from an unreliable machine." For Zagzebski, the value of knowledge deflates to the value of mere true belief. She assumes that reliability in itself has no value or disvalue, but Goldman and Olsson disagree. They point out that Zagzebski's conclusion rests on the assumption of veritism: all that matters is the acquisition of true belief. To the contrary, they argue that a reliable process for acquiring a true belief adds value to the mere true belief by making it more likely that future beliefs of a similar kind will be true. By analogy, having a reliable espresso maker that produced a good cup of espresso would be more valuable than having an unreliable one that luckily produced a good cup because the reliable one would more likely produce good future cups compared to the unreliable one.

The value problem is important to assessing the adequacy of theories of knowledge that conceive of knowledge as consisting of true belief and other components. According to Kvanvig, an adequate account of knowledge should resist counterexamples and allow an explanation of the value of knowledge over mere true belief. Should a theory of knowledge fail to do so, it would prove inadequate.

One of the more influential responses to the problem is that knowledge is not particularly valuable and is not what ought to be the main focus of epistemology. Instead, epistemologists ought to focus on other mental states, such as understanding. Advocates of virtue epistemology have argued that the value of knowledge comes from an internal relationship between the knower and the mental state of believing.

There are many proposed sources of knowledge and justified belief which we take to be actual sources of knowledge in our everyday lives. Some of the most commonly discussed include perception, reason, memory, and testimony.

As mentioned above, epistemologists draw a distinction between what can be known "a priori" (independently of experience) and what can only be known "a posteriori" (through experience). Much of what we call "a priori" knowledge is thought to be attained through reason alone, as featured prominently in rationalism. This might also include a non-rational faculty of intuition, as defended by proponents of innatism. In contrast, "a posteriori" knowledge is derived entirely through experience or as a result of experience, as emphasized in empiricism. This also includes cases where knowledge can be traced back to an earlier experience, as in memory or testimony.

A way to look at the difference between the two is through an example. Bruce Russell gives two propositions in which the reader decides which one he believes more. Option A: All crows are birds. Option B: All crows are black. If you believe option A, then you are a priori justified in believing it because you do not have to see a crow to know it is a bird. If you believe in option B, then you are posteriori justified to believe it because you have seen many crows therefore knowing they are black. He goes on to say that it does not matter if the statement is true or not, only that if you believe in one or the other that matters.

The idea of "a priori" knowledge is that it is based on intuition or rational insights. Laurence BonJour says in his article "The Structure of Empirical Knowledge", that a "rational insight is an immediate, non-inferential grasp, apprehension or 'seeing' that some proposition is necessarily true." (3) Going back to the crow example, by Laurence BonJour's definition the reason you would believe in option A is because you have an immediate knowledge that a crow is a bird, without ever experiencing one.

Evolutionary psychology takes a novel approach to the problem. It says that there is an innate predisposition for certain types of learning. "Only small parts of the brain resemble a tabula rasa; this is true even for human beings. The remainder is more like an exposed negative waiting to be dipped into a developer fluid".

Immanuel Kant, in his "Critique of Pure Reason", drew a distinction between "analytic" and "synthetic" propositions. He contended that some propositions are such that we can know they are true just by understanding their meaning. For example, consider, "My father's brother is my uncle." We can know it is true solely by virtue of our understanding in what its terms mean. Philosophers call such propositions "analytic". Synthetic propositions, on the other hand, have distinct subjects and predicates. An example would be, "My father's brother has black hair." Kant stated that all mathematical and scientific statements are synthetic a priori propositions because they are necessarily true but our knowledge about the attributes of the mathematical or physical subjects we can only get by logical inference.

While this distinction is first and foremost about meaning and is therefore most relevant to the philosophy of language, the distinction has significant epistemological consequences, seen most prominently in the works of the logical positivists. In particular, if the set of propositions which can only be known "a posteriori" is coextensive with the set of propositions which are synthetically true, and if the set of propositions which can be known "a priori" is coextensive with the set of propositions which are analytically true (or in other words, which are true by definition), then there can only be two kinds of successful inquiry: Logico-mathematical inquiry, which investigates what is true by definition, and empirical inquiry, which investigates what is true in the world. Most notably, this would exclude the possibility that branches of philosophy like metaphysics could ever provide informative accounts of what actually exists.

The American philosopher W. V. O. Quine, in his paper "Two Dogmas of Empiricism", famously challenged the analytic-synthetic distinction, arguing that the boundary between the two is too blurry to provide a clear division between propositions that are true by definition and propositions that are not. While some contemporary philosophers take themselves to have offered more sustainable accounts of the distinction that are not vulnerable to Quine's objections, there is no consensus about whether or not these succeed.

Science is often considered to be a refined, formalized, systematic, institutionalized form of the pursuit and acquisition of empirical knowledge. As such, the philosophy of science may be viewed as an application of the principles of epistemology and as a foundation for epistemological inquiry.

The regress problem (also known as Agrippa's Trilemma) is the problem of providing a complete logical foundation for human knowledge. The traditional way of supporting a rational argument is to appeal to other rational arguments, typically using chains of reason and rules of logic. A classic example that goes back to Aristotle is deducing that "Socrates is mortal". We have a logical rule that says "All humans are mortal" and an assertion that "Socrates is human" and we deduce that "Socrates is mortal". In this example how do we know that Socrates is human? Presumably we apply other rules such as: "All born from human females are human". Which then leaves open the question how do we know that all born from humans are human? This is the regress problem: how can we eventually terminate a logical argument with some statements that do not require further justification but can still be considered rational and justified? As John Pollock stated:

... to justify a belief one must appeal to a further justified belief. This means that one of two things can be the case. Either there are some beliefs that we can be justified for holding, without being able to justify them on the basis of any other belief, or else for each justified belief there is an infinite regress of (potential) justification [the nebula theory]. On this theory there is no rock bottom of justification. Justification just meanders in and out through our network of beliefs, stopping nowhere.

The apparent impossibility of completing an infinite chain of reasoning is thought by some to support skepticism. It is also the impetus for Descartes's famous dictum: "I think, therefore I am". Descartes was looking for some logical statement that could be true without appeal to other statements.

Many epistemologists studying justification have attempted to argue for various types of chains of reasoning that can escape the regress problem.

Foundationalists respond to the regress problem by asserting that certain "foundations" or "basic beliefs" support other beliefs but do not themselves require justification from other beliefs. These beliefs might be justified because they are self-evident, infallible, or derive from reliable cognitive mechanisms. Perception, memory, and a priori intuition are often considered possible examples of basic beliefs.

The chief criticism of foundationalism is that if a belief is not supported by other beliefs, accepting it may be arbitrary or unjustified.

Another response to the regress problem is coherentism, which is the rejection of the assumption that the regress proceeds according to a pattern of linear justification. To avoid the charge of circularity, coherentists hold that an individual belief is justified circularly by the way it fits together (coheres) with the rest of the belief system of which it is a part. This theory has the advantage of avoiding the infinite regress without claiming special, possibly arbitrary status for some particular class of beliefs. Yet, since a system can be coherent while also being wrong, coherentists face the difficulty of ensuring that the whole system corresponds to reality. Additionally, most logicians agree that any argument that is circular is, at best, only trivially valid. That is, to be illuminating, arguments must operate with information from multiple premises, not simply conclude by reiterating a premise.

Nigel Warburton writes in "Thinking from A to Z" that "[c]ircular arguments are not invalid; in other words, from a logical point of view there is nothing intrinsically wrong with them. However, they are, when viciously circular, spectacularly uninformative."

An alternative resolution to the regress problem is known as "infinitism". Infinitists take the infinite series to be merely potential, in the sense that an individual may have indefinitely many reasons available to them, without having consciously thought through all of these reasons when the need arises. This position is motivated in part by the desire to avoid what is seen as the arbitrariness and circularity of its chief competitors, foundationalism and coherentism. The most prominent defense of infinitism has been given by Peter Klein.

An intermediate position, known as "foundherentism", is advanced by Susan Haack. Foundherentism is meant to unify foundationalism and coherentism. Haack explains the view by using a crossword puzzle as an analogy. Whereas, for example, infinitists regard the regress of reasons as taking the form of a single line that continues indefinitely, Haack has argued that chains of properly justified beliefs look more like a crossword puzzle, with various different lines mutually supporting each other. Thus, Haack's view leaves room for both chains of beliefs that are "vertical" (terminating in foundational beliefs) and chains that are "horizontal" (deriving their justification from coherence with beliefs that are also members of foundationalist chains of belief).

Empiricism is a view in the theory of knowledge which focuses on the role of experience, especially experience based on perceptual observations by the senses, in the generation of knowledge. Certain forms exempt disciplines such as mathematics and logic from these requirements.

There are many variants of empiricism, including British empiricism, logical empiricism, phenomenalism, and some versions of common sense philosophy. Most forms of empiricism give epistemologically privileged status to sensory impressions or sense data, although this plays out very differently in different cases. Some of the most famous historical empiricists include John Locke, David Hume, George Berkeley, Francis Bacon, John Stuart Mill, Rudolf Carnap, and Bertrand Russell.

Rationalism is the epistemological view that reason is the chief source of knowledge and the main determinant of what constitutes knowledge. More broadly, it can also refer to any view which appeals to reason as a source of knowledge or justification. Rationalism is one of the two classical views in epistemology, the other being empiricism. Rationalists claim that the mind, through the use of reason, can directly grasp certain truths in various domains, including logic, mathematics, ethics, and metaphysics. Rationalist views can range from modest views in mathematics and logic (such as that of Gottlob Frege) to ambitious metaphysical systems (such as that of Baruch Spinoza).

Some of the most famous rationalists include Plato, René Descartes, Baruch Spinoza, and Gottfried Leibniz.

Skepticism is a position that questions the possibility of human knowledge, either in particular domains or on a general level. Skepticism does not refer to any one specific school of philosophy, but is rather a thread that runs through many epistemological debates. Ancient Greek skepticism began during the Hellenistic period in philosophy, which featured both Pyrrhonism (notably defended by Pyrrho, Sextus Empiricus, and Aenesidemus) and Academic skepticism (notably defended by Arcesilaus and Carneades). Among ancient Indian philosophers, skepticism was notably defended by the Ajñana school and in the Buddhist Madhyamika tradition. In modern philosophy, René Descartes' famous inquiry into mind and body began as an exercise in skepticism, in which he started by trying to doubt all purported cases of knowledge in order to search for something that was known with absolute certainty.

Epistemic skepticism questions whether knowledge is possible at all. Generally speaking, skeptics argue that knowledge requires certainty, and that most or all of our beliefs are fallible (meaning that our grounds for holding them always, or almost always, fall short of certainty), which would together entail that knowledge is always or almost always impossible for us. Characterizing knowledge as strong or weak is dependent on a person's viewpoint and their characterization of knowledge. Much of modern epistemology is derived from attempts to better understand and address philosophical skepticism.

One of the oldest forms of epistemic skepticism can be found in Agrippa's trilemma (named after the Pyrrhonist philosopher Agrippa the Skeptic) which demonstrates that certainty can not be achieved with regard to beliefs. Pyrrhonism dates back to Pyrrho of Elis from the 4th century BCE, although most of what we know about Pyrrhonism today is from the surviving works of Sextus Empiricus. Pyrrhonists claim that for any argument for a non-evident proposition, an equally convincing argument for a contradictory proposition can be produced. Pyrrhonists do not dogmatically deny the possibility of knowledge, but instead point out that beliefs about non-evident matters cannot be substantiated.

The Cartesian evil demon problem, first raised by René Descartes, supposes that our sensory impressions may be controlled by some external power rather than the result of ordinary veridical perception. In such a scenario, nothing we sense would actually exist, but would instead be mere illusion. As a result, we would never be able to know anything about the world, since we would be systematically deceived about everything. The conclusion often drawn from evil demon skepticism is that even if we are not completely deceived, all of the information provided by our senses is still "compatible" with skeptical scenarios in which we are completely deceived, and that we must therefore either be able to exclude the possibility of deception or else must deny the possibility of "infallible" knowledge (that is, knowledge which is completely certain) beyond our immediate sensory impressions. While the view that no beliefs are beyond doubt other than our immediate sensory impressions is often ascribed to Descartes, he in fact thought that we "can" exclude the possibility that we are systematically deceived, although his reasons for thinking this are based on a highly contentious ontological argument for the existence of a benevolent God who would not allow such deception to occur.

Epistemological skepticism can be classified as either "mitigated" or "unmitigated" skepticism. Mitigated skepticism rejects "strong" or "strict" knowledge claims but does approve weaker ones, which can be considered "virtual knowledge", but only with regard to justified beliefs. Unmitigated skepticism rejects claims of both virtual and strong knowledge. Characterizing knowledge as strong, weak, virtual or genuine can be determined differently depending on a person's viewpoint as well as their characterization of knowledge. Some of the most notable attempts to respond to unmitigated skepticism include direct realism, disjunctivism, common sense philosophy, pragmatism, fideism, and fictionalism.

Pragmatism is a fallibilist epistemology that emphasizes the role of action in knowing. Different interpretations of pragmatism variously emphasize: truth as the final outcome of ideal scientific inquiry and experimentation, truth as closely related to usefulness, experience as transacting with (instead of representing) nature, and human practices as the foundation of language. Pragmatism's origins are often attributed to Charles Sanders Peirce, William James, and John Dewey. In 1878, Peirce formulated the maxim: "Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object."

William James suggested that through a pragmatist epistemology, theories "become instruments, not answers to enigmas in which we can rest". In James's pragmatic method, which he adapted from Peirce, metaphysical disputes can be settled by tracing the practical consequences of the different sides of the argument. If this process does not resolve the dispute, then "the dispute is idle".

Contemporary versions of pragmatism have been developed by thinkers such as Richard Rorty and Hilary Putnam. Rorty proposed that values were historically contingent and dependent upon their utility within a given historical period. Contemporary philosophers working in pragmatism are called neopragmatists, and also include Nicholas Rescher, Robert Brandom, Susan Haack, and Cornel West.

In certain respects an intellectual descendant of pragmatism, naturalized epistemology considers the evolutionary role of knowledge for agents living and evolving in the world. It de-emphasizes the questions around justification and truth, and instead asks, empirically, how reliable beliefs are formed and the role that evolution played in the development of such processes. It suggests a more empirical approach to the subject as a whole, leaving behind philosophical definitions and consistency arguments, and instead using psychological methods to study and understand how "knowledge" is actually formed and is used in the natural world. As such, it does not attempt to answer the analytic questions of traditional epistemology, but rather replace them with new empirical ones.

Naturalized epistemology was first proposed in "Epistemology Naturalized", a seminal paper by W.V.O. Quine. A less radical view has been defended by Hilary Kornblith in "Knowledge and its Place in Nature", in which he seeks to turn epistemology towards empirical investigation without completely abandoning traditional epistemic concepts.

Epistemic relativism is the view that what is true, rational, or justified for one person need not be true, rational, or justified for another person. Epistemic relativists therefore assert that while there are "relative" facts about truth, rationality, justification, and so on, there is no "perspective-independent" fact of the matter. Note that this is distinct from epistemic contextualism, which holds that the "meaning" of epistemic terms vary across contexts (e.g. "I know" might mean something different in everyday contexts and skeptical contexts). In contrast, epistemic relativism holds that the relevant "facts" vary, not just linguistic meaning. Relativism about truth may also be a form of ontological relativism, insofar as relativists about truth hold that facts about what "exists" vary based on perspective.

Constructivism is a view in philosophy according to which all "knowledge is a compilation of human-made constructions", "not the neutral discovery of an objective truth". Whereas objectivism is concerned with the "object of our knowledge", constructivism emphasizes "how we construct knowledge". Constructivism proposes new definitions for knowledge and truth, which emphasize intersubjectivity rather than objectivity, and viability rather than truth. The constructivist point of view is in many ways comparable to certain forms of pragmatism.

Bayesian epistemology is a formal approach to various topics in epistemology that has its roots in Thomas Bayes' work in the field of probability theory. One advantage of its formal method in contrast to traditional epistemology is that its concepts and theorems can be defined with a high degree of precision. It is based on the idea that beliefs can be interpreted as subjective probabilities. As such, they are subject to the laws of probability theory, which act as the norms of rationality. These norms can be divided into static constraints, governing the rationality of beliefs at any moment, and dynamic constraints, governing how rational agents should change their beliefs upon receiving new evidence. The most characteristic Bayesian expression of these principles is found in the form of Dutch books, which illustrate irrationality in agents through a series of bets that lead to a loss for the agent no matter which of the probabilistic events occurs. Bayesians have applied these fundamental principles to various epistemological topics but Bayesianism does not cover all topics of traditional epistemology.

Feminist epistemology is a subfield of epistemology which applies feminist theory to epistemological questions. It began to emerge as a distinct subfield in the 20th century. Prominent feminist epistemologists include Miranda Fricker (who developed the concept of epistemic injustice), Donna Haraway (who first proposed the concept of situated knowledge), Sandra Harding, and Elizabeth Anderson. Harding proposes that feminist epistemology can be broken into three distinct categories: feminist empiricism, standpoint epistemology, and postmodern epistemology.

Feminist epistemology has also played a significant role in the development of many debates in social epistemology.

Epistemicide is a term used in decolonisation studies that describes the killing of knowledge systems under systemic oppression such as colonisation and slavery. The term was coined by Boaventura de Sousa Santos, who presented the significance of such physical violence creating the centering of Western knowledge in the current world. This term challenges the thought of what is seen as knowledge in academia today.

Indian schools of philosophy, such as the Hindu Nyaya and Carvaka schools, and the Jain and Buddhist philosophical schools, developed an epistemological tradition independently of the Western philosophical tradition called "pramana". Pramana can be translated as "instrument of knowledge" and refers to various means or sources of knowledge that Indian philosophers held to be reliable. Each school of Indian philosophy had their own theories about which pramanas were valid means to knowledge and which were unreliable (and why). A Vedic text, Taittirīya Āraṇyaka (–6th centuries BCE), lists "four means of attaining correct knowledge": "smṛti" ("tradition" or "scripture"), "pratyakṣa" ("perception"), "aitihya" ("communication by one who is expert", or "tradition"), and "anumāna" ("reasoning" or "inference").

In the Indian traditions, the most widely discussed pramanas are: "Pratyakṣa" (perception), "Anumāṇa" (inference), "Upamāṇa" (comparison and analogy), "Arthāpatti" (postulation, derivation from circumstances), "Anupalabdi" (non-perception, negative/cognitive proof) and "Śabda" (word, testimony of past or present reliable experts). While the Nyaya school (beginning with the Nyāya Sūtras of Gotama, between 6th-century BCE and 2nd-century CE) were a proponent of realism and supported four pramanas (perception, inference, comparison/analogy and testimony), the Buddhist epistemologists (Dignaga and Dharmakirti) generally accepted only perception and inference. The Carvaka school of materialists only accepted the pramana of perception, and hence were among the first empiricists in the Indian traditions. Another school, the Ajñana, included notable proponents of philosophical skepticism.

The theory of knowledge of the Buddha in the early Buddhist texts has been interpreted as a form of pragmatism as well as a form of correspondence theory. Likewise, the Buddhist philosopher Dharmakirti has been interpreted both as holding a form of pragmatism or correspondence theory for his view that what is true is what has effective power ("arthakriya"). The Buddhist Madhyamika school's theory of emptiness (shunyata) meanwhile has been interpreted as a form of philosophical skepticism.

The main contribution to epistemology by the Jains has been their theory of "many sided-ness" or "multi-perspectivism" (Anekantavada), which says that since the world is multifaceted, any single viewpoint is limited ("naya" – a partial standpoint). This has been interpreted as a kind of pluralism or perspectivism. According to Jain epistemology, none of the pramanas gives absolute or perfect knowledge since they are each limited points of view.

Formal epistemology uses formal tools and methods from decision theory, logic, probability theory and computability theory to model and reason about issues of epistemological interest. Work in this area spans several academic fields, including philosophy, computer science, economics, and statistics. The focus of formal epistemology has tended to differ somewhat from that of traditional epistemology, with topics like uncertainty, induction, and belief revision garnering more attention than the analysis of knowledge, skepticism, and issues with justification.

Historical epistemology is the study of the historical conditions of, and changes in, different kinds of knowledge. There are many versions of or approaches to historical epistemology, which is different from history of epistemology. Twentieth-century French historical epistemologists like Abel Rey, Gaston Bachelard, Jean Cavaillès, and Georges Canguilhem focused specifically on changes in scientific discourse.

Metaepistemology is the metaphilosophical study of the methods, aims, and subject matter of epistemology. In general, metaepistemology aims to better understand our first-order epistemological inquiry. Some goals of metaepistemology are identifying inaccurate assumptions made in epistemological debates and determining whether the questions asked in mainline epistemology are the "right" epistemological questions to be asking.

Social epistemology deals with questions about knowledge in contexts where our knowledge attributions cannot be explained by simply examining individuals in isolation from one another, meaning that the scope of our knowledge attributions must be widened to include broader social contexts. It also explores the ways in which interpersonal beliefs can be justified in social contexts. The most common topics discussed in contemporary social epistemology are testimony, which deals with the conditions under which a belief "x is true" which resulted from being told "x is true" constitutes knowledge; peer disagreement, which deals with when and how I should revise my beliefs in light of other people holding beliefs that contradict mine; and group epistemology, which deals with what it means to attribute knowledge to groups rather than individuals, and when group knowledge attributions are appropriate.

Contemporary philosophers consider that epistemology is a major subfield of philosophy, along with ethics, logic, and metaphysics, which are more ancient subdivisions of philosophy. But in the early and mid 20th century, epistemology was not seen as an independent field on its own. Quine viewed epistemology as a chapter of psychology. Russell viewed it as a mix of psychology and logic. William Alston presents a similar contemporary perspective, but in a historically 'oriented' manner: for him, epistemology has historically always been a part of cognitive psychology. 

The claim that psychology is a background for epistemology is often called its naturalization. The epistemology of Russell and Quine in the 20th century were naturalised in that way. More recently, Laurence Bonjour rejects that there is a need for that kind of psychologism in contemporary epistemology. His argument is that, nowadays, the required part of psychology, which he refers as "minimal psychologism", "conceptual psychologism" and "meliorative psychologism," are self evident within contemporary (traditional) epistemology, "involves at most a quite minor departure from traditional, nonnaturalized epistemology" or "poses no real threat to traditional epistemology". In this view point, epistemology has integrated all required psychological aspects, which are considered non controversial, and can be severed from psychologism.

For Luciano Floridi, "at the turn of the [20th] century there had been a resurgence of interest in epistemology through an anti-metaphysical, naturalist, reaction against the nineteenth-century development of Neo-Kantian and Neo-Hegelian idealism." In that perspective, contemporary epistemology, which in Bonjour's perspective does not need to be "naturalized" anymore, emerged after a naturalisation that rejected meta-physical perspectives associated with Kant and Hegel.

Historians of philosophy traditionally divide the modern period into a dispute between empiricists (including Francis Bacon, John Locke, David Hume, and George Berkeley) and rationalists (including René Descartes, Baruch Spinoza, and Gottfried Leibniz). The debate between them has often been framed using the question of whether knowledge comes primarily from sensory experience (empiricism), or whether a significant portion of our knowledge is derived entirely from our faculty of reason (rationalism). According to some scholars, this dispute was resolved in the late 18th century by Immanuel Kant, whose transcendental idealism famously made room for the view that "though all our knowledge begins with experience, it by no means follows that all [knowledge] arises out of experience".

In "Meno", the definition of knowledge as justified true belief appears for the first time. In other words, belief is required to have an explanation in order to be correct, beyond just happening to be right. A number of important epistemological concerns also appeared in the works of Aristotle.
During the subsequent Hellenistic period, philosophical schools began to appear which had a greater focus on epistemological questions, often in the form of philosophical skepticism. For instance, the Hellenistic Sceptics, especially Sextus Empiricus of the Pyrrhonian school rejected justification on the basis of Agrippa's trilemma and so, in the view of , rejected the possibility of knowledge as well. The Pyrrhonian school of Pyrrho and Sextus Empiricus held that eudaimonia (flourishing, happiness, or "the good life") could be attained through the application of epoché (suspension of judgment) regarding all non-evident matters. Pyrrhonism was particularly concerned with undermining the epistemological dogmas of Stoicism and Epicureanism. The other major school of Hellenistic skepticism was Academic skepticism, most notably defended by Carneades and Arcesilaus, which predominated in the Platonic Academy for almost two centuries.

In ancient India the Ajñana school of ancient Indian philosophy promoted skepticism. Ajñana was a Śramaṇa movement and a major rival of early Buddhism, Jainism and the Ājīvika school. They held that it was impossible to obtain knowledge of metaphysical nature or to ascertain the truth value of philosophical propositions; and even if knowledge was possible, it was useless and disadvantageous for final salvation. They were specialized in refutation without propagating any positive doctrine of their own.

During the Islamic Golden Age, one of the most prominent and influential philosophers, theologians, jurists, logicians and mystics in Islamic epistemology was Al-Ghazali. During his life, he wrote over 70 books on science, Islamic reasoning and Sufism. Al-Ghazali distributed his book "The Incoherence of Philosophers", set apart as a defining moment in Islamic epistemology. He shaped a conviction that all occasions and connections are not the result of material conjunctions but are the present and prompt will of God.

After the ancient philosophical era but before the modern philosophical era, a number of (non Islamic) medieval philosophers also engaged with epistemological questions at length. Most notable among the Medievals for their contributions to epistemology were Thomas Aquinas, John Duns Scotus, and William of Ockham. 

According to historian of philosophy Jan Woleński, the development of philosophy divides, with some exceptions, into the pre-Cartesian "ontologically oriented" and the post-Cartesian "epistemologically oriented". 
There are a number of different methods that contemporary scholars use when trying to understand the relationship between past epistemology and contemporary epistemology. One of the most contentious questions is this: "Should we assume that the problems of epistemology are perennial, and that trying to reconstruct and evaluate Plato's or Hume's or Kant's arguments is meaningful for current debates, too?" Similarly, there is also a question of whether contemporary philosophers should aim to "rationally reconstruct and evaluate" historical views in epistemology, or to "merely describe" them.


"Stanford Encyclopedia of Philosophy" articles

"Internet Encyclopedia of Philosophy" articles

Encyclopædia Britannica

Other links

Esperanto

Esperanto (, ) is the world's most widely spoken constructed international auxiliary language. Created by the Warsaw-based ophthalmologist L. L. Zamenhof in 1887, it is intended to be a universal second language for international communication, or "the international language" (). Zamenhof first described the language in "Dr. Esperanto's International Language" (), which he published under the pseudonym . Early adopters of the language liked the name "Esperanto" and soon used it to describe his language. The word translates into English as "one who hopes".

Within the range of constructed languages, Esperanto occupies a middle ground between "naturalistic" (imitating existing natural languages) and "a""priori" (where features are not based on existing languages). Esperanto's vocabulary, syntax and semantics derive predominantly from languages of the Indo-European group. A substantial majority of its vocabulary (approximately 80%) derives from Romance languages, but it also contains elements derived from Germanic, Greek, and Slavic languages. One of the language's most notable features is its extensive system of derivation, where prefixes and suffixes may be freely combined with roots to generate words, making it possible to communicate effectively with a smaller set of words.

Esperanto is the most successful constructed international auxiliary language, and the only such language with a sizeable population of native speakers, of which there are perhaps several thousand. Usage estimates are difficult, but two estimates put the number of people who know how to speak Esperanto at around 100,000. Concentration of speakers is highest in Europe, East Asia, and South America. Although no country has adopted Esperanto officially, ("Esperanto-land") is used as a name for the collection of places where it is spoken. The language has also gained a noticeable presence on the internet in recent years, as it became increasingly accessible on platforms such as Duolingo, Wikipedia, Amikumu and Google Translate. Esperanto speakers are often called "Esperantists" ().

Esperanto has not been a secondary official language of any recognized country, but it entered the education systems of several countries, such as Hungary and China.

There were plans at the beginning of the 20th century to establish Neutral Moresnet, in central-western Europe, as the world's first Esperanto state; any such plans came to an end when the Treaty of Versailles awarded the disputed territory to Belgium, effective January 10, 1920. In addition, the self-proclaimed artificial island micronation of Rose Island, near Italy in the Adriatic Sea, used Esperanto as its official language in 1968, and another micronation, the extant Republic of Molossia, near Dayton, Nevada, uses Esperanto as an official language alongside English.

The Chinese government has used Esperanto since 2001 for daily news on china.org.cn. China also uses Esperanto in China Radio International, and for the internet magazine "El Popola Ĉinio".

The Vatican Radio has an Esperanto version of its podcasts and its website.

The United States Army has published military phrase books in Esperanto, to be used from the 1950s until the 1970s in war games by mock enemy forces. A field reference manual, FM 30-101-1 Feb. 1962, contained the grammar, English-Esperanto-English dictionary, and common phrases. In the 1970s Esperanto was used as the basis for Defense Language Aptitude Tests.

Esperanto is the working language of several non-profit international organizations such as the , a left-wing cultural association which had 724 members in over 85 countries in 2006. There is also Education@Internet, which has developed from an Esperanto organization; most others are specifically Esperanto organizations. The largest of these, the Universal Esperanto Association, has an official consultative relationship with the United Nations and UNESCO, which recognized Esperanto as a medium for international understanding in 1954. The Universal Esperanto Association collaborated in 2017 with UNESCO to deliver an Esperanto translation of its magazine "UNESCO Courier" ("Unesko Kuriero en Esperanto"). The World Health Organization offers an Esperanto version of the coronavirus pandemic (COVID-19, ) occupational safety and health education course.

Esperanto was also the first language of teaching and administration of the International Academy of Sciences San Marino.

The League of Nations made attempts to promote teaching Esperanto in member countries, but the resolutions were defeated mainly by French delegates, who did not feel there was a need for it.

In the summer of 1924, the American Radio Relay League adopted Esperanto as its official international auxiliary language, and hoped that the language would be used by radio amateurs in international communications, but its actual use for radio communications was negligible.

All the personal documents sold by the World Service Authority, including the World Passport, are written in Esperanto, together with English, French, Spanish, Russian, Arabic, and Chinese (the official languages of the United Nations).

Esperanto was created in the late 1870s and early 1880s by L. L. Zamenhof, a Polish-Jewish ophthalmologist from Białystok, then part of the Russian Empire, but now part of Poland. In the 1870s, just a few years before Zamenhof created Esperanto, Polish was banned in public places in Białystok.

According to Zamenhof, he created the language to reduce the "time and labor we spend in learning foreign tongues", and to foster harmony between people from different countries: "Were there but an international language, all translations would be made into it alone ... and all nations would be united in a common brotherhood." His feelings and the situation in Białystok may be gleaned from an extract from his letter to Nikolai Borovko:

Zamenhof's goal was to create an easy and flexible language that would serve as a universal second language, to foster world peace and international understanding, and to build a "community of speakers".

His original title for the language was simply "the international language" (), but early speakers grew fond of the name "Esperanto," and began to use it as the name for the language just two years after its creation. The name quickly gained prominence, and has been used as an official name ever since.

In 1905, Zamenhof published the "Fundamento de Esperanto" as a definitive guide to the language. Later that year, French Esperantists organized with his participation the first World Esperanto Congress, an ongoing annual conference, in Boulogne-sur-Mer, France. Zamenhof also proposed to the first congress that an independent body of linguistic scholars should steward the future evolution of Esperanto, foreshadowing the founding of the Akademio de Esperanto (in part modeled after the Académie Française), which was established soon thereafter. Since then, world congresses have been held in different countries every year, except during the two World Wars, and the 2020 COVID-19 pandemic (when it was moved to an online-only event). Since the Second World War, they have been attended by an average of more than 2,000 people, and up to 6,000 people at the most.

Zamenhof wrote that he wanted mankind to "learn and use ... en masse ... the proposed language as a living one". The goal for Esperanto to become a global auxiliary language was not Zamenhof's only goal; he also wanted to "enable the learner to make direct use of his knowledge with persons of any nationality, whether the language be universally accepted or not; in other words, the language is to be directly a means of international communication".

After some ten years of development, which Zamenhof spent translating literature into Esperanto, as well as writing original prose and verse, the first book of Esperanto grammar was published in Warsaw on July 26, 1887. The number of speakers grew rapidly over the next few decades; at first, primarily in the Russian Empire and Central Europe, then in other parts of Europe, the Americas, China, and Japan. In the early years before the world congresses, speakers of Esperanto kept in contact primarily through correspondence and periodicals.

Zamenhof's name for the language was simply ("International Language"). December 15, Zamenhof's birthday, is now regarded as Zamenhof Day or Esperanto Book Day.

The autonomous territory of Neutral Moresnet, between what is today Belgium and Germany, had a sizable proportion of Esperanto-speaking citizens among its small, diverse population. There was a proposal to make Esperanto its official language.

However, neither Belgium nor Germany had surrendered their claims to the region, with the latter having adopted a more aggressive stance towards pursuing its claim around the turn of the century, even being accused of sabotage and administrative obstruction to force the issue. The outbreak of World War I would bring about the end of neutrality, with Moresnet initially left as "an oasis in a desert of destruction" following the German invasion of Belgium. The territory was formally annexed by Prussia in 1915, though without international recognition.

After the war, a great opportunity for Esperanto seemingly presented itself, when the Iranian delegation to the League of Nations proposed that the language be adopted for use in international relations following a report by a Japanese delegate to the League named Nitobe Inazō, in the context of the 13th World Congress of Esperanto, held in Prague. Ten delegates accepted the proposal with only one voice against, the French delegate, Gabriel Hanotaux. Hanotaux opposed all recognition of Esperanto at the League, from the first resolution on December 18, 1920, and subsequently through all efforts during the next three years. Hanotaux did not approve of how the French language was losing its position as the international language and saw Esperanto as a threat, effectively wielding his veto power to block the decision. However, two years later, the League recommended that its member states include Esperanto in their educational curricula. The French government retaliated by banning all instruction in Esperanto in France's schools and universities. The French Ministry of Public Instruction said that "French and English would perish and the literary standard of the world would be debased". Nonetheless, many people see the 1920s as the heyday of the Esperanto movement. During this time, Anarchism as a political movement was very supportive of both anationalism and the Esperanto language.

Fran Novljan was one of the chief promoters of Esperanto in the former Kingdom of Yugoslavia. He was among the founders of the Croatian (Educational Alliance), of which he was the first secretary, and organized Esperanto institutions in Zagreb. Novljan collaborated with Esperanto newspapers and magazines, and was the author of the Esperanto textbook "Internacia lingvo esperanto i Esperanto en tridek lecionoj".

In 1920s Korea, socialist thinkers pushed for the use of Esperanto through a series of columns in The Dong-a Ilbo as resistance to both Japanese occupation as well as a counter to the growing nationalist movement for Korean language standardization. This lasted until the Mukden Incident in 1931, when changing colonial policy led to an outright ban on Esperanto education in Korea.

Esperanto attracted the suspicion of many states. Repression was especially pronounced in Nazi Germany, Francoist Spain up until the 1950s, and the Soviet Union under Stalin, from 1937 to 1956.

In Nazi Germany, there was a motivation to ban Esperanto because Zamenhof was Jewish, and due to the internationalist nature of Esperanto, which was perceived as "Bolshevist". In his work, "Mein Kampf", Adolf Hitler specifically mentioned Esperanto as an example of a language that could be used by an international Jewish conspiracy once they achieved world domination. Esperantists were killed during the Holocaust, with Zamenhof's family in particular singled out to be killed. The efforts of a minority of German Esperantists to expel their Jewish colleagues and overtly align themselves with the Reich were futile, and Esperanto was legally forbidden in 1935. Esperantists in German concentration camps did, however, teach Esperanto to fellow prisoners, telling guards they were teaching Italian, the language of one of Germany's Axis allies.

In Imperial Japan, the left wing of the Japanese Esperanto movement was forbidden, but its leaders were careful enough not to give the impression to the government that the Esperantists were socialist revolutionaries, which proved a successful strategy.

After the October Revolution of 1917, Esperanto was given a measure of government support by the new communist states in the former Russian Empire and later by the Soviet Union government, with the Soviet Esperantist Union being established as an organization that, temporarily, was officially recognized. In his biography on Joseph Stalin, Leon Trotsky mentions that Stalin had studied Esperanto. However, in 1937, at the height of the Great Purge, Stalin completely reversed the Soviet government's policies on Esperanto; many Esperanto speakers were executed, exiled or held in captivity in the Gulag labour camps. Quite often the accusation was: "You are an active member of an international spy organization which hides itself under the name of 'Association of Soviet Esperantists' on the territory of the Soviet Union." Until the end of the Stalin era, it was dangerous to use Esperanto in the Soviet Union, even though it was never officially forbidden to speak Esperanto.

Fascist Italy allowed the use of Esperanto, finding its phonology similar to that of Italian and publishing some tourist material in the language.

During and after the Spanish Civil War, Francoist Spain suppressed anarchists, socialists and Catalan nationalists for many years, among whom the use of Esperanto was extensive, but in the 1950s the Esperanto movement was again tolerated.

In 1954, the United Nations — through UNESCO — granted official support to Esperanto as an international auxiliary language in the Montevideo Resolution. However, Esperanto is still not one of the official languages of the UN.

The development of Esperanto has continued unabated into the 21st century. The advent of the Internet has had a significant impact on the language, as learning it has become increasingly accessible on platforms such as Duolingo, and as speakers have increasingly networked on platforms such as Amikumu. With up to two million speakers, it is the most widely spoken constructed language in the world. Although no country has adopted Esperanto officially, "Esperantujo" ("Esperanto-land") is the name given to the collection of places where it is spoken.

While many of its advocates continue to hope for the day that Esperanto becomes officially recognized as the international auxiliary language, some (including raŭmistoj) have stopped focusing on this goal and instead view the Esperanto community as a stateless diasporic linguistic group based on freedom of association.

On May 28, 2015, the language learning platform Duolingo launched a free Esperanto course for English speakers On March 25, 2016, when the first Duolingo Esperanto course completed its beta-testing phase, that course had 350,000 people registered to learn Esperanto through the medium of English. By July 2018, the number of learners had risen to 1.36 million. On July 20, 2018, Duolingo changed from recording users cumulatively to reporting only the number of "active learners" (i.e., those who are studying at the time and have not yet completed the course), which as of October 2022 stands at 299,000 learners.

On October 26, 2016, a second Duolingo Esperanto course, for which the language of instruction is Spanish, appeared on the same platform and which as of April 2021 has a further 176,000 students. A third Esperanto course, taught in Brazilian Portuguese, began its beta-testing phase on May 14, 2018, and as of April 2021, 220,000 people are using this course and 155,000 people in May 2022. A fourth Esperanto course, taught in French, began its beta-testing phase in July 2020, and as of March 2021 has 72,500 students and 101,000 students in May 2022.

As of October 2018, , another online learning platform for Esperanto, has 320,000 registered users, and nearly 75,000 monthly visits. 50,000 users possess at least a basic understanding of Esperanto.

On February 22, 2012, Google Translate added Esperanto as its 64th language. On July 25, 2016, Yandex Translate added Esperanto as a language.

With about articles, Esperanto Wikipedia (Vikipedio) is the 36th-largest Wikipedia, as measured by the number of articles, and is the largest Wikipedia in a constructed language. About 150,000 users consult the Vikipedio regularly, as attested by Wikipedia's automatically aggregated log-in data, which showed that in October 2019 the website has 117,366 unique individual visitors per month, plus 33,572 who view the site on a mobile device instead.

Esperanto has been described as "a language lexically predominantly Romanic, morphologically intensively agglutinative, and to a certain degree isolating in character". Approximately 80% of Esperanto's vocabulary is derived from Romance languages. Typologically, Esperanto has prepositions and a pragmatic word order that by default is "subject–verb–object" (SVO). Adjectives can be freely placed before or after the nouns they modify, though placing them before the noun is more common. New words are formed through extensive use of affixes and compounds.

Esperanto's phonology, grammar, vocabulary, and semantics are based on the Indo-European languages spoken in Europe. Some evidence has shown that Zamenhof studied German, English, Spanish, Lithuanian, Italian and French and knew 13 different languages, which had an influence on Esperanto's linguistic properties. Esperantist and linguist Ilona Koutny notes that Esperanto's vocabulary, phrase structure, agreement systems, and semantic typology are considered to be similar to Indo-European languages spoken in Europe. However, Koutny and Esperantist Humphrey Tonkin also note that Esperanto has features that are atypical of Indo-European languages spoken in Europe, such as its agglutinative morphology.

Esperanto typically has 22 to 24 consonants (depending on the phonemic analysis and individual speaker), five vowels, and two semivowels that combine with the vowels to form six diphthongs. (The consonant and semivowel are both written ⟨j⟩, and the uncommon consonant is written with the digraph ⟨dz⟩, which is the only consonant that does not have its own letter.) Tone is not used to distinguish meanings of words. Stress is always on the second-to-last vowel in proper Esperanto words, unless a final vowel is elided, a phenomenon mostly occurring in poetry. For example, "" "family" is , with the stress on the second "i", but when the word is used without the final " ()," the stress remains on the second : .

The 23 consonants are:

There is some degree of allophony:

A large number of consonant clusters can occur, up to three in initial position (as in ', "strange") and five in medial position (as in "", "former slave"). Final clusters are uncommon except in unassimilated names, poetic elision of final "," and a very few basic words such as ' "hundred" and "" "after".

Esperanto has the five vowels found in such languages as Spanish, Modern Hebrew, and Modern Greek.
Since there are only five vowels, a good deal of variation in pronunciation is tolerated. For instance, "e" commonly ranges from (French ) to (French ). These details often depend on the speaker's native language. A glottal stop may occur between adjacent vowels in some people's speech, especially when the two vowels are the same, as in ' "hero" ( or ) and ' "great-grandfather" ( or ).

The Esperanto alphabet is based on the Latin script, using a one-sound-one-letter principle, with the exception of [d͡z]. It includes six letters with diacritics: five with circumflexes (⟨ĉ⟩, ⟨ĝ⟩, ⟨ĥ⟩, ⟨ĵ⟩, and ⟨ŝ⟩), and one with a breve (⟨ŭ⟩). The alphabet does not include the letters ⟨q⟩, ⟨w⟩, ⟨x⟩, or ⟨y⟩, which are only used in the writing of proper names and unassimilated borrowings.

The 28-letter alphabet is:
All letters lacking diacritics are pronounced approximately as their respective IPA symbols, with the exception of ⟨c⟩.

The letters ⟨j⟩ and ⟨c⟩ are used in a way that is familiar to speakers of many Central and Eastern European languages, but may be unfamiliar to English speakers. ⟨j⟩ has the sound of English ⟨y⟩, as in yellow" and "boy (Esperanto "jes" has the same pronunciation as its English cognate "yes"), and ⟨c⟩ has a ""ts"" sound, as in "hits or the ⟨zz⟩ in "pizza". In addition, the ⟨g⟩ in Esperanto is always 'hard', as in gift". Esperanto makes use of the five-vowel system, essentially identical to the vowels of Spanish and Modern Greek.

The accented letters are:

According to one of Zamenhof's entries in the "Lingvaj respondoj", the letter ⟨n⟩ ought to be pronounced as [n] in all cases, but a rendering as [ŋ] is admissible before ⟨g⟩, ⟨k⟩, and ⟨ĥ⟩.

Even with the widespread adoption of Unicode, the letters with diacritics (found in the "Latin-Extended A" section of the Unicode Standard) can cause problems with printing and computing, because they are not found on most physical keyboards and are left out of certain fonts.

There are two principal workarounds to this problem, which substitute digraphs for the accented letters. Zamenhof, the inventor of Esperanto, created an "h-convention", which replaces ⟨ĉ⟩, ⟨ĝ⟩, ⟨ĥ⟩, ⟨ĵ⟩, ⟨ŝ⟩, and ⟨ŭ⟩ with ⟨ch⟩, ⟨gh⟩, ⟨hh⟩, ⟨jh⟩, ⟨sh⟩, and ⟨u⟩, respectively. The main issue with this convention is its ambiguity: If used in a database, a program could not easily determine whether to render, for example, ⟨ch⟩ as /c/ followed by /h/ or as /ĉ/. Such words do exist in Esperanto: could not be rendered unambiguously, unless its component parts were intentionally separated, as in "senc·hava". A more recent "x-convention""" has also gained prominence with the advent of computing, utilizing an otherwise absent ⟨x⟩ to produce the digraphs ⟨cx⟩, ⟨gx⟩, ⟨hx⟩, ⟨jx⟩, ⟨sx⟩, and ⟨ux⟩; this has the incidental advantage of alphabetizing correctly in most cases, since the only letter after ⟨x⟩ is ⟨z⟩.

There are computer keyboard layouts that support the Esperanto alphabet, and some systems use software that automatically replaces x- or h-convention digraphs with the corresponding diacritic letters (for example, for Microsoft Windows, Mac OS X, and Linux, for Windows Phone, and Gboard and AnySoftKeyboard for Android).

On Linux, the GNOME, Cinnamon, and KDE desktop environments support the entry of characters with Esperanto diacritics.

Criticisms are levied against the letters with circumflex diacritics, which some find odd or cumbersome, along with their being invented specifically for Esperanto rather than borrowed from existing languages. Additionally, some of them are arguably unnecessary — for example, the use of "ĥ" instead of "x" and "ŭ" instead of "w". However, Zamenhof did not choose these letters arbitrarily: In fact, they were inspired by Czech letters with the caron diacritic but replaced the caron with a circumflex for the ease of those who had access to a French typewriter (with a circumflex dead-key). The Czech letter "ž" was replaced with "ĵ" to match the French letter "j" with the same sound. The letter "ŭ" on the other hand comes from the u-breve used in Latin prosody, and is also speculated to be inspired by the Belarusian Cyrillic letter "ў"; French typewriters can render it approximately as the French letter "ù".

Esperanto words are mostly derived by stringing together roots, grammatical endings, and at times prefixes and suffixes. This process is regular so that people can create new words as they speak and be understood. Compound words are formed with a modifier-first, head-final order, as in English (compare "birdsong" and "songbird", and likewise, and ). Speakers may optionally insert an "o" between the words in a compound noun if placing them together directly without the "o" would make the resulting word hard to say or understand.

The different parts of speech are marked by their own suffixes: all common nouns are marked with the suffix , all adjectives with , all derived adverbs with , and all verbs except the jussive (or imperative) and infinitive end in , specifically in one of six tense and mood suffixes, such as the present tense ; the jussive mood, which is tenseless, ends in . Nouns and adjectives have two cases: nominative for grammatical subjects and in general, and accusative for direct objects and (after a preposition) to indicate direction of movement.

Singular nouns used as grammatical subjects end in , plural subject nouns in (pronounced [oi̯] like English "oy"). Singular direct object forms end in , and plural direct objects with the combination ([oi̯n]; rhymes with "coin"): indicates that the word is a noun, indicates the plural, and indicates the accusative (direct object) case. Adjectives agree with their nouns; their endings are singular subject ([a]; rhymes with "ha!"), plural subject ([ai̯], pronounced "eye"), singular object , and plural object ([ai̯n]; rhymes with "fine").

The suffix , besides indicating the direct object, is used to indicate movement and a few other things as well.

The six verb inflections consist of three tenses and three moods. They are present tense , future tense , past tense , infinitive mood , conditional mood and jussive mood (used for wishes and commands). Verbs are not marked for person or number. Thus, means "to sing", means "I sing", means "you sing", and means "they sing".

Word order is comparatively free. Adjectives may precede or follow nouns; subjects, verbs and objects may occur in any order. However, the article "the", demonstratives such as "that" and prepositions (such as "at") must come before their related nouns. Similarly, the negative "not" and conjunctions such as "and" and "that" must precede the phrase or clause that they introduce. In copular (A = B) clauses, word order is just as important as in English: "people are animals" is distinguished from "animals are people".

The core vocabulary of Esperanto was defined by , published by Zamenhof in 1887. This book listed 917 roots; these could be expanded into tens of thousands of words using prefixes, suffixes, and compounding. In 1894, Zamenhof published the first Esperanto dictionary, , which had a larger set of roots. The rules of the language allowed speakers to borrow new roots as needed; it was recommended, however, that speakers use most international forms and then derive related meanings from these.

Since then, many words have been borrowed, primarily (but not solely) from the European languages. Not all proposed borrowings become widespread, but many do, especially technical and scientific terms. Terms for everyday use, on the other hand, are more likely to be derived from existing roots; "computer", for instance, is formed from the verb "compute" and the suffix "tool". Words are also calqued; that is, words acquire new meanings based on usage in other languages. For example, the word "mouse" has acquired the meaning of a computer mouse from its usage in many languages (English "mouse", French "souris", Dutch "muis", Spanish "ratón", etc.). Esperanto speakers often debate about whether a particular borrowing is justified or whether meaning can be expressed by deriving from or extending the meaning of existing words.

Some compounds and formed words in Esperanto are not entirely straightforward; for example, , literally "give out", means "publish", paralleling the usage of certain European languages (such as German , Dutch , Russian ). In addition, the suffix "-um-" has no defined meaning; words using the suffix must be learned separately (such as "to the right" and "clockwise").

There are not many idiomatic or slang words in Esperanto, as these forms of speech tend to make international communication difficult—working against Esperanto's main goal. The language contains several calques of Polish expressions.

Instead of derivations of Esperanto roots, new roots are taken from European languages in the endeavor to create an international language.

The following short extract gives an idea of the character of Esperanto.

Listed below are some useful Esperanto words and phrases along with transcriptions:

Esperanto speakers learn the language through self-directed study, online tutorials, and correspondence courses taught by volunteers. More recently, free teaching websites like and have become available.

Esperanto instruction is rarely available at schools, including four primary schools in a pilot project under the supervision of the University of Manchester, and by one count at a few universities. However, outside China and Hungary, these mostly involve informal arrangements, rather than dedicated departments or state sponsorship. Eötvös Loránd University in Budapest had a department of Interlinguistics and Esperanto from 1966 to 2004, after which time instruction moved to vocational colleges; there are state examinations for Esperanto instructors. Additionally, Adam Mickiewicz University in Poland offers a diploma in Interlinguistics. The Senate of Brazil passed a bill in 2009 that would make Esperanto an optional part of the curriculum in public schools, although mandatory if there is demand for it. , the bill is still under consideration by the Chamber of Deputies.

In the United States, Esperanto is notably offered as a weekly evening course at Stanford University's Bechtel International Center. "Conversational Esperanto, The International Language", is a free drop-in class that is open to Stanford students and the general public on campus during the academic year. With administrative permission, Stanford Students can take the class for two credits a quarter through the Linguistics Department. "Even four lessons are enough to get more than just the basics," the Esperanto at Stanford website reads.

Esperanto-USA suggests that Esperanto can be learned in, at most, one quarter of the amount of time required for other languages.

The Zagreb method is an Esperanto teaching method that was developed in Zagreb, Yugoslavia (present-day capital city of Croatia), in the late 1970s to early 1980s as a response to the unsatisfactory learning outcomes of traditional natural-language teaching techniques when used for Esperanto. Its goal was to streamline the material in order to equip learners with practical knowledge that could be put to use in a short of a time frame as possible. It is now implemented and available on some of the well-known learning websites in the community.

From 2006 to 2011, four primary schools in Britain, with 230 pupils, followed a course in "propaedeutic Esperanto"—that is, instruction in Esperanto to raise language awareness, and to accelerate subsequent learning of foreign languages—under the supervision of the University of Manchester. As they put it,

Many schools used to teach children the recorder, not to produce a nation of recorder players, but as a preparation for learning other instruments. [We teach] Esperanto, not to produce a nation of Esperanto-speakers, but as a preparation for learning other languages.

The results showed that the pupils achieved enhanced metalinguistic awareness, though the study did not indicate whether a course in a language other than Esperanto would have led to similar results. Similar studies have been conducted in New Zealand, the United States, England, and Germany. Many of these experiments' findings were compromised by unclear objectives, brief or anecdotal reporting, and a lack of methodological rigor. However, the results of these studies were consistently favorable, and suggested that studying Esperanto before another foreign language expedites the acquisition of the other, natural language.

Esperanto is by far the most widely spoken constructed language in the world. Speakers are most numerous in Europe and East Asia, especially in urban areas, where they often form Esperanto clubs. Esperanto is particularly prevalent in the northern and central countries of Europe; in China, Korea, Japan, and Iran within Asia; in Brazil, and the United States in the Americas; and in Togo in Africa.

Countering a common criticism against Esperanto, the statistician Svend Nielsen has found no significant correlation between the number of Esperanto speakers and the similarity of a given national native language to Esperanto. He concludes that Esperanto tends to be more popular in rich countries with widespread Internet access and a tendency to contribute more to science and culture. Linguistic diversity within a country was found to have no, or perhaps a slightly reductive, correlation with Esperanto popularity.

An estimate of the number of Esperanto speakers was made by Sidney S. Culbert, a retired psychology professor at the University of Washington and a longtime Esperantist, who tracked down and tested Esperanto speakers in sample areas in dozens of countries over a period of twenty years. Culbert concluded that between one and two million people speak Esperanto at Foreign Service Level 3, "professionally proficient" (able to communicate moderately complex ideas without hesitation, and to follow speeches, radio broadcasts, etc.). Culbert's estimate was not made for Esperanto alone, but formed part of his listing of estimates for all languages of more than one million speakers, published annually in the World Almanac and Book of Facts. Culbert's most detailed account of his methodology is found in a 1989 letter to David Wolff. Since Culbert never published detailed intermediate results for particular countries and regions, it is difficult to independently gauge the accuracy of his results.

In the Almanac, his estimates for numbers of language speakers were rounded to the nearest million, thus the number of Esperanto speakers is shown as two million. This latter figure appears in "Ethnologue". Assuming that this figure is accurate, that means that about 0.03% of the world's population speaks the language. Although it does not meet Zamenhof's goal of a universal language, it still represents a level of popularity unmatched by any other constructed language.

Marcus Sikosek (now Ziko van Dijk) has challenged this figure of 1.6 million as exaggerated. He estimated that even if Esperanto speakers were evenly distributed, assuming one million Esperanto speakers worldwide would lead one to expect about 180 in the city of Cologne. Van Dijk finds only 30 fluent speakers in that city, and similarly smaller-than-expected figures in several other places thought to have a larger-than-average concentration of Esperanto speakers. He also notes that there are a total of about 20,000 members of the various Esperanto organizations (other estimates are higher). Though there are undoubtedly many Esperanto speakers who are not members of any Esperanto organization, he thinks it unlikely that there are fifty times more speakers than organization members.

In 1996, Finnish linguist Jouko Lindstedt, an expert on native-born Esperanto speakers, presented the following scheme to show the overall proportions of language capabilities within the Esperanto community:

In 2017, doctoral student Svend Nielsen estimated around 63,000 Esperanto speakers worldwide, taking into account association memberships, user-generated data from Esperanto websites and census statistics. This number, however, was disputed by statistician Sten Johansson, who questioned the reliability of the source data and highlighted a wide margin of error, the latter point with which Nielsen agrees. Both have stated, however, that this new number is likely more realistic than some earlier projections.

In the absence of Culbert's detailed sampling data, or any other census data, it is impossible to state the number of speakers with certainty. According to the website of the Universal Esperanto Association:

Numbers of textbooks sold and membership of local societies put "the number of people with some knowledge of the language in the hundreds of thousands and possibly millions".

Native Esperanto speakers, , have learned the language from birth from Esperanto-speaking parents. This usually happens when Esperanto is the chief or only common language in an international family, but sometimes occurs in a family of Esperanto speakers who often use the language. As of 1996, according to Corsetti, there were approximately 350 attested cases of families with native Esperanto speakers (which means there were around 700 Esperanto speaking natives in these families, not accounting for older native speakers). The 2022 edition of "Ethnologue" gives 1,000 L1 users citing Corsetti et al 2004.

However, native speakers do not occupy an authoritative position in the Esperanto community, as they would in other language communities. This presents a challenge to linguists, whose usual source of grammaticality and meanings are native speakers.

Esperantists can access an international culture, including a large body of original as well as translated literature. There are more than 25,000 Esperanto books, both originals and translations, as well as several regularly distributed Esperanto magazines. In 2013 a museum about Esperanto opened in China. Esperantists use the language for free accommodations with Esperantists in 92 countries using the or to develop pen pals through "".

Every year, Esperantists meet for the World Congress of Esperanto "()".

Historically, much music has been written in the language such as , has been in various folk traditions. There is also a variety of classical and semi-classical choral music, both original and translated, as well as large ensemble music that includes voices singing Esperanto texts. Lou Harrison, who incorporated styles and instruments from many world cultures in his music, used Esperanto titles and/or texts in several of his works, most notably (1973). David Gaines used Esperanto poems as well as an excerpt from a speech by Zamenhof for his "Symphony No. One (Esperanto)" for mezzo-soprano and orchestra (1994–98). He wrote original Esperanto text for his ("I Can Cry No Longer") for unaccompanied SATB choir (1994).

There are also shared traditions, such as Zamenhof Day, celebrated on December 15. Esperantists speak primarily in Esperanto at special conventions, such as the World Esperanto Congress.

Proponents of Esperanto, such a Humphrey Tonkin, a professor at the University of Hartford, argue that Esperanto is "culturally neutral by design, as it was intended to be a facilitator between cultures, not to be the carrier of any one national culture". The late Scottish Esperanto author William Auld wrote extensively on the subject, arguing that Esperanto is "the expression of a common human culture, unencumbered by national frontiers. Thus it is considered a culture on its own." Critics have argued that the language is eurocentric, as it draws much of its vocabulary from European languages.

Several Esperanto associations also advance Esperanto education, and aim to preserve its culture and heritage. Poland added Esperanto to its list of intangible cultural heritage in 2014.

In the futuristic novel "Lord of the World" by Robert Hugh Benson, Esperanto is presented as the predominant language of the world, much as Latin is the language of the Church. A reference to Esperanto appears in the science-fiction story "War with the Newts" by Karel Čapek, published in 1936. As part of a passage on what language the salamander-looking creatures with human cognitive ability should learn, it is noted that "...in the Reform schools, Esperanto was taught as the medium of communication." (P. 206).
Esperanto has been used in many films and novels. Typically, this is done either to add the exotic flavour of a foreign language without representing any particular ethnicity, or to avoid going to the trouble of inventing a new language. The Charlie Chaplin film "The Great Dictator" (1940) showed Jewish ghetto shop signs in Esperanto. Two full-length feature films have been produced with dialogue entirely in Esperanto: "," in 1964, and "Incubus," a 1965 B-movie horror film which is also notable for starring William Shatner shortly before he began working on "". In "Captain Fantastic" (2016) there is a dialogue in Esperanto. The 1994 film "Street Fighter" contains Esperanto dialogue spoken by the character Sagat. Finally, Mexican film director Alfonso Cuarón has publicly shown his fascination for Esperanto, going as far as naming his film production company Esperanto Filmoj ("Esperanto Films").

In 1921 the French Academy of Sciences recommended using Esperanto for international scientific communication. A few scientists and mathematicians, such as Maurice Fréchet (mathematics), John C. Wells (linguistics), Helmar Frank (pedagogy and cybernetics), and Nobel laureate Reinhard Selten (economics) have published part of their work in Esperanto. Frank and Selten were among the founders of the International Academy of Sciences in San Marino, sometimes called the "Esperanto University", where Esperanto is the primary language of teaching and administration.

A message in Esperanto was recorded and included in "Voyager 1"s Golden Record.

Esperanto business groups have been active for many years. Research conducted in the 1920s by the French Chamber of Commerce and reported in "The New York Times" suggested that Esperanto seemed to be the best business language.

The privacy-oriented cryptocurrency, Monero, takes its name from the Esperanto word for "coin".

Zamenhof had three goals, as he wrote already in 1887: to create an easy language, to create a language ready to use "whether the language be universally accepted or not" and to find some means to get many people to learn the language. So Zamenhof's intention was not only to create an easy-to-learn language to foster peace and international understanding as a general language, but also to create a language for immediate use by a (small) language community. Esperanto was to serve as an international auxiliary language, that is, as a universal second language, not to replace ethnic languages. This goal was shared by Zamenhof among Esperanto speakers at the beginning of the movement. Later, Esperanto speakers began to see the language and the culture that had grown up around it as ends in themselves, even if Esperanto is never adopted by the United Nations or other international organizations.

Esperanto speakers who want to see Esperanto adopted officially or on a large scale worldwide are commonly called , from , meaning "final victory".
There are two kinds of "finvenkismo": "desubismo" aims to spread Esperanto between ordinary people ("desube", from below) to form a steadily growing community of Esperanto speakers, while "desuprismo" aims to act from above ("desupre"), beginning with politicians.
Zamenhof considered the first way more plausible, as "for such affairs as ours, governments come with their approval and help usually only when everything is completely ready".

Those who focus on the intrinsic value of the language are commonly called , from Rauma, Finland, where a declaration on the short-term improbability of the and the value of Esperanto culture was made at the International Youth Congress in 1980. However the "Manifesto de Raŭmo" clearly mentions the intention to further spread the language: "We want to spread Esperanto to put into effect its positive values more and more, step by step".

In 1996 the Prague Manifesto was adopted at the annual congress of the Universal Esperanto Association (UEA); it was subscribed by individual participants and later by other Esperanto speakers. More recently, language-learning apps like Duolingo and Amikumu have helped to increase the amount of fluent speakers of Esperanto, and find others in their area to speak the language with.

The earliest flag, and the one most commonly used today, features a green five-pointed star against a white canton, upon a field of green. It was proposed to Zamenhof by Richard Geoghegan, author of the first Esperanto textbook for English speakers, in 1887. The flag was approved in 1905 by delegates to the first conference of Esperantists at Boulogne-sur-Mer.

The green star on white ("") is also used by itself as a round (buttonhole, etc.) emblem by many esperantists, among other reasons to enhance their visibility outside the Esperanto world.

A version with an "E" superimposed over the green star is sometimes seen.
Other variants include that for Christian Esperantists, with a white Christian cross superimposed upon the green star, and that for Leftists, with the color of the field changed from green to red.

In 1987, a second flag design was chosen in a contest organized by the UEA celebrating the first centennial of the language.
It featured a white background with two stylised curved "E"s facing each other.
Dubbed the (jubilee symbol), it attracted criticism from some Esperantists, who dubbed it the (melon) for its elliptical shape.
It is still in use, though to a lesser degree than the traditional symbol, known as the (green star).

Esperanto has been placed in many proposed political situations.
The most popular of these is the Europe–Democracy–Esperanto, which aims to establish Esperanto as the official language of the European Union.
Grin's Report, published in 2005 by François Grin, found that the use of English as the lingua franca within the European Union costs billions annually and significantly benefits English-speaking countries financially.
The report considered a scenario where Esperanto would be the lingua franca, and found that it would have many advantages, particularly economically speaking, as well as ideologically.

Left-wing currents exist in the wider Esperanto world, mostly organized through the Sennacieca Asocio Tutmonda founded by French theorist Eugène Lanti.
Other notable Esperanto socialists include Nikolai Nekrasov and Vladimir Varankin, both of whom were put to death in October 1938 during the Stalinist repressions. Nekrasov was accused of being "an organizer and leader of a fascist, espionage, terrorist organization of Esperantists."

The Oomoto religion encourages the use of Esperanto among its followers and includes Zamenhof as one of its deified spirits.

The Baháʼí Faith encourages the use of an auxiliary international language. ʻAbdu'l-Bahá praised the ideal of Esperanto, and there was an affinity between Esperantists and Baháʼís during the late 19th century and early 20th century.

On February 12, 1913, ʻAbdu'l-Bahá gave a talk to the Paris Esperanto Society, stating:

Now, praise be to God that Dr. Zamenhof has invented the Esperanto language. It has all the potential qualities of becoming the international means of communication. All of us must be grateful and thankful to him for this noble effort; for in this way he has served his fellowmen well. With untiring effort and self-sacrifice on the part of its devotees Esperanto will become universal. Therefore every one of us must study this language and spread it as far as possible so that day by day it may receive a broader recognition, be accepted by all nations and governments of the world, and become a part of the curriculum in all the public schools. I hope that Esperanto will be adopted as the language of all the future international conferences and congresses, so that all people need acquire only two languages—one their own tongue and the other the international language. Then perfect union will be established between all the people of the world. Consider how difficult it is today to communicate with various nations. If one studies fifty languages one may yet travel through a country and not know the language. Therefore I hope that you will make the utmost effort, so that this language of Esperanto may be widely spread.

Lidia Zamenhof, daughter of L. L. Zamenhof, became a Baháʼí around 1925. James Ferdinand Morton Jr., an early member of the Baháʼí Faith in Greater Boston, was vice-president of the Esperanto League for North America. Ehsan Yarshater, the founding editor of "Encyclopædia Iranica", notes how as a child in Iran he learned Esperanto and that when his mother was visiting Haifa on a Baháʼí pilgrimage he wrote her a letter in Persian as well as Esperanto. At the request of ʻAbdu'l-Bahá, Agnes Baldwin Alexander became an early advocate of Esperanto and used it to spread the Baháʼí teachings at meetings and conferences in Japan.

Today there exists an active sub-community of Baháʼí Esperantists and various volumes of Baháʼí literature have been translated into Esperanto. In 1973, the Baháʼí Esperanto-League for active Baháʼí supporters of Esperanto was founded.

In 1908, spiritist Camilo Chaigneau wrote an article named "Spiritism and Esperanto" in the periodic "La Vie d'Outre-Tombe" recommending the use of Esperanto in a "central magazine" for all spiritists and Esperantists. Esperanto then became actively promoted by spiritists, at least in Brazil, initially by Ismael Gomes Braga and František Lorenz; the latter is known in Brazil as Francisco Valdomiro Lorenz, and was a pioneer of both spiritist and Esperantist movements in this country. The Brazilian Spiritist Federation publishes Esperanto coursebooks, translations of Spiritism's basic books, and encourages Spiritists to become Esperantists.

William T. Stead, a famous spiritualist and occultist in the United Kingdom, co-founded the first Esperanto club in the U.K.

The Teozofia Esperanta Ligo (Theosophical Esperantist League) was formed in 1911, and the organization's journal, "Espero Teozofia", was published from 1913 to 1928.

The first translation of the Bible into Esperanto was a translation of the Tanakh (or Old Testament) done by L. L. Zamenhof. The translation was reviewed and compared with other languages' translations by a group of British clergy and scholars before its publication at the British and Foreign Bible Society in 1910. In 1926 this was published along with a New Testament translation, in an edition commonly called the "". In the 1960s, the tried to organize a new, ecumenical Esperanto Bible version. Since then, the Dutch Remonstrant pastor Gerrit Berveling has translated the Deuterocanonical or apocryphal books, in addition to new translations of the Gospels, some of the New Testament epistles, and some books of the Tanakh. These have been published in various separate booklets, or serialized in , but the Deuterocanonical books have appeared in recent editions of the "Londona Biblio".

Christian Esperanto organizations and publications include:



Ayatollah Khomeini of Iran called on Muslims to learn Esperanto and praised its use as a medium for better understanding among peoples of different religious backgrounds. After he suggested that Esperanto replace English as an international "lingua franca", it began to be used in the seminaries of Qom. An Esperanto translation of the Qur'an was published by the state shortly thereafter.

Though Esperanto itself has changed little since the publication of ("Foundation of Esperanto"), a number of reform projects have been proposed over the years, starting with Zamenhof's proposals in 1894 and in 1907. Several later constructed languages, such as Universal, Saussure, Romániço, Internasia, Esperanto sen Fleksio, and , were all based on Esperanto.

In modern times, conscious attempts have been made to eliminate perceived sexism in the language, such as Riism. Many words with now have alternative spellings with and occasionally , so that may also be spelled ; see Esperanto phonology for further details of replacement. Reforms aimed at altering country names have also resulted in a number of different options, either due to disputes over suffixes or Eurocentrism in naming various countries.

J. R. R. Tolkien wrote in support of the language in a 1932 "British Esperantist" article, but criticised those who sought to adapt or "tinker" with the language, which, in his opinion, harmed unanimity and the goal of achieving wide acceptance.

There have been numerous objections to Esperanto over the years. For example, there has been criticism that Esperanto is not neutral enough, but also that it should convey a specific culture, which would make it less neutral; that Esperanto does not draw on a wide enough selection of the world's languages, but also that it should be more narrowly European.

Esperantists often argue for Esperanto as a culturally neutral means of communication. However, it is often accused of being Eurocentric. This is most often noted in regard to the vocabulary. The vocabulary, for example, draws about three-quarters from Romance languages, and the remainder primarily from German, Greek and Slavic languages. Supporters have argued that the agglutinative grammar and verb regularity of Esperanto has more in common with Asian languages than with European ones. A 2010 linguistic typological study concluded that "Esperanto is indeed somewhat European in character, but considerably less so than the European languages themselves."

Esperanto is sometimes accused of being inherently sexist, because the default form of some nouns is used for descriptions of men while a derived form is used for the women. This is said to retain traces of the male-dominated society of late 19th-century Europe of which Esperanto is a product. These nouns are primarily titles, such as "baron/baroness", and kinship terms, such as "sinjoro" "Mr, sir" vs. "sinjorino" "Ms, lady" and "patro" "father" vs. "patrino" "mother". Before the movement toward equal rights for women, this also applied to professional roles assumed to be predominantly male, such as "doktoro," a PhD doctor (male or unspecified), versus "doktorino," a female PhD. This was analogous to the situation with the English suffix "-ess," as in the words "waiter/waitress", etc.

On the other hand, the pronoun "ĝi" ("it") may be used generically to mean he/she/they; the pronoun "li" ("he") is always masculine and "ŝi" ("she") is always female, despite some authors' arguments. 
A gender-neutral singular pronoun "ri" has gradually become more widely used in recent years, although it is not currently universal.
The plural pronoun "ili" ("they") is always neutral, as are nouns with the prefix "ge–" such as "gesinjoroj" (equivalent to "sinjoro kaj sinjorino" "Mr.and Ms."), "gepatroj" "parents" (equivalent to "patro kaj patrino" "mother and father").

Speakers of languages without grammatical case or adjectival agreement frequently criticise these aspects of Esperanto. In addition, in the past some people found the Classical Greek forms of the plural (nouns in "-oj," adjectives in "-aj)" to be awkward, proposing instead that Italian "-i" be used for nouns, and that no plural be used for adjectives. These suggestions were adopted by the Ido reform. A reply to that criticism is that the presence of an accusative case allows much freedom in word order, e.g. for emphasis ("Johano batis Petron", John hit Peter; "Petron batis Johano", it is Peter whom John hit), that its absence in the "predicate of the object" avoids ambiguity ("Mi vidis la blankan domon", I saw the white house; "Mi vidis la domon blanka", the house seemed white to me) and that adjective agreement allows, among others, the use of hyperbaton in poetry (as in Latin, cf. Virgil's Eclogue 1:1 "Tityre, tu patulæ recubans sub tegmine fagi..." where "patulæ" (spread out) is epithet to "fagi" (beech) and their agreement in the genitive feminine binds them notwithstanding their distance in the verse).

The Esperanto alphabet uses two diacritics: the circumflex and the breve. The alphabet was designed with a French typewriter in mind, and although modern computers support Unicode, entering the letters with diacritic marks can be more or less problematic with certain operating systems or hardware. One of the first reform proposals (for Esperanto 1894) sought to do away with these marks and the language Ido went back to the basic Latin alphabet.

One common criticism is that Esperanto has failed to live up to the hopes of its creator, who dreamed of it becoming a universal second language. Because people were reluctant to learn a new language which hardly anyone spoke, Zamenhof asked people to sign a promise to start learning Esperanto once ten million people made the same promise. He "was disappointed to receive only a thousand responses".

However, Zamenhof had the goal to "enable the learner to make direct use of his knowledge with persons of any nationality, whether the language be universally accepted or not", as he wrote in 1887. The language is currently spoken by people living in more than 100 countries; there are about 2,000 native Esperanto speakers and probably up to 100,000 people who use the language regularly.

In this regard, Zamenhof was well aware that it might take much time for Esperanto to achieve his desired goals. In his speech at the 1907 World Esperanto Congress in Cambridge he said, "we hope that earlier or later, maybe after many centuries, on a neutral language foundation, understanding one another, the nations will build ... a big family circle."

The poet Wisława Szymborska expressed doubt that Esperanto could "produce works of lasting value", saying it is "an artificial language without variety or dialects". Esperantists have replied that "lasting value" is a statement of opinion, and that Esperanto grew "naturally" by the actions of its speakers on Zamenhof's intentionally elementary "Fundamento".

There are some geographical and astronomical features named after Esperanto, or after its creator L. L. Zamenhof. These include Esperanto Island in Antarctica, and the asteroids 1421 Esperanto and 1462 Zamenhof discovered by Finnish astronomer and Esperantist Yrjö Väisälä.




Engineering

Engineering is the practice of using natural science, mathematics, and the engineering design process to solve technical problems, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.

The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.

The term "engineering" is derived from the Latin , meaning "cleverness" and "ingeniare", meaning "to contrive, devise".

The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET) has defined "engineering" as:

Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.

The term "engineering" is derived from the word "engineer", which itself dates back to the 14th century when an "engine'er" (literally, one who builds or operates a "siege engine") referred to "a constructor of military engines". In this context, now obsolete, an "engine" referred to a military machine, "i.e.", a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, "e.g.", the U.S. Army Corps of Engineers.

The word "engine" itself is of even older origin, ultimately deriving from the Latin (), meaning "innate quality, especially mental power, hence a clever invention."

Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.

The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.

The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia , and then in ancient Egyptian technology . The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC, and ancient Egypt during the Twelfth Dynasty (1991–1802 BC). The screw, the last of the simple machines to be invented, first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC. The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.

The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC. The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.

Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation. Sappers were employed to build causeways during military campaigns. Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.

Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer, and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.

Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC, the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.

The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD. The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.

The cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.

The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their "Book of Ingenious Devices", in the 9th century. In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.

Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.

A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise "De re metallica" (1556), which also contains sections on geology, mining, and chemistry. "De re metallica" was the standard chemistry reference for the next 180 years.

The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering. With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.

Canal building was an important engineering work during the early phases of the Industrial Revolution.

John Smeaton was the first self-proclaimed civil engineer and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency. Smeaton introduced iron axles and gears to water wheels. Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.

Applied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called "The Miner's Friend". It employed both vacuum and pressure. Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.
The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke. These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible. New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.

One of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.
The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool. Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.

The United States Census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000. There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.

There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.

The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.
Chemical engineering developed in the late nineteenth century. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.

Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.

The first PhD in engineering (technically, "applied science and engineering") awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.

Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.

Engineering is a broad discipline that is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches: chemical engineering, civil engineering, electrical engineering, and mechanical engineering.

Chemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.

Civil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings. Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.

Electrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, control systems, and electronics.

Mechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, robotics, turbines, audio equipments, and mechatronics.

Bioengineering is the engineering of biological systems for a useful purpose. Examples of bioengineering research include bacteria engineered to produce chemicals, new medical imaging technology, portable and rapid disease diagnostic devices, prosthetics, biopharmaceuticals, and tissue-engineered organs.

Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, information engineering, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical, geological, textile, industrial, materials, and nuclear engineering. These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.

New specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.

Aerospace engineering covers the design, development, manufacture and operational behaviour of aircraft, satellites and rockets.

Marine engineering covers the design, development, manufacture and operational behaviour of watercraft and stationary structures like oil platforms and ports.

Computer engineering (CE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering.

Geological engineering is associated with anything constructed on or within the Earth. This discipline applies geological sciences and engineering principles to direct or support the work of other disciplines such as civil engineering, environmental engineering, and mining engineering. Geological engineers are involved with impact studies for facilities and operations that affect surface and subsurface environments, such as rock excavations (e.g. tunnels), building foundation consolidation, slope and fill stabilization, landslide risk assessment, groundwater monitoring, groundwater remediation, mining excavations, and natural resource exploration.

One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.

In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers.

If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.

Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.

Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.

More than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.

Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product.

Engineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.

The study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams.

As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.
One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.

These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.

There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.

In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).

The engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are "pro bono", and open-design engineering.

Engineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety.

Engineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies.

Engineering is a key driver of innovation and human development. Sub-Saharan Africa, in particular, has a small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.

Overseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development:

Engineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status. There are negative economic and political issues that this can cause, as well as ethical issues. It is agreed the engineering profession faces an "image crisis". The UK holds the compared to other European countries, together with the United States.

Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:
In Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.

There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.

Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists".
In the book "What Engineers Know and How They Know It", Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.

There is a "real and important" difference between engineering and physics as similar to any science field has to do with technology. Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology. For technology, physics is an auxiliary and in a way technology is considered as applied physics. Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training. Physicists and engineers engage in different lines of work. But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.

An example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.

As stated by Fung "et al." in the revision to the classic engineering text "Foundations of Solid Mechanics":

Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.

Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.

The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.
Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.

Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.

Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.

Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.

The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.

Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.

There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).

The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.

Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.

Business engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or "Management engineering" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.

In political science, the term "engineering" has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term.








Education

Education is the transmission of knowledge, skills, and character traits and comes in many forms. Formal education happens in a complex institutional framework, like public schools. Non-formal education is also structured but takes place outside the formal schooling system, while informal education is unstructured learning through daily experiences. Formal and non-formal education are divided into levels that include early childhood education, primary education, secondary education, and tertiary education. Other classifications focus on the teaching method, like teacher-centered and student-centered education, and on the subject, like science education, language education, and physical education. The term "education" can also refer to the mental states and qualities of educated people and the academic field studying educational phenomena.

The precise definition of education is disputed, and there are disagreements about what the aims of education are and to what extent education is different from indoctrination by fostering critical thinking. These disagreements affect how to identify, measure, and improve forms of education. Fundamentally, education socializes children into society by teaching cultural values and norms. It equips them with the skills needed to become productive members of society. This way, it stimulates economic growth and raises awareness of local and global problems. Organized institutions affect many aspects of education. For example, governments set education policies to determine when school classes happen, what is taught, and who can or must attend. International organizations, like UNESCO, have been influential in promoting primary education for all children.

Many factors influence whether education is successful. Psychological factors include motivation, intelligence, and personality. Social factors, like socioeconomic status, ethnicity, and gender, are often linked to discrimination. Further factors include access to educational technology, teacher quality, and parent involvement.

The main academic field investigating education is called education studies. It examines what education is, what aims and effects it has, and how to improve it. Education studies has many subfields, like philosophy, psychology, sociology, and economics of education. It also discusses comparative education, pedagogy, and the history of education.

In prehistory, education happened informally through oral communication and imitation. With the rise of ancient civilizations, writing was invented, and the amount of knowledge grew. This caused a shift from informal to formal education. Initially, formal education was mainly available to elites and religious groups. The invention of the printing press in the 15th century made books more widely available. This increased general literacy. Beginning in the 18th and 19th centuries, public education became more important. This development led to the worldwide process of making primary education available to all, free of charge, and compulsory up to a certain age. Today, over 90% of all primary-school-age children worldwide attend primary school.

The term "education" is derived from the Latin words , meaning "to bring up" and , meaning "to bring forth". The definition of education has been explored by theorists from various fields. Many agree that education is a purposeful activity aimed at achieving goals like the transmission of knowledge, skills, and character traits. Extensive debate surrounds its exact nature beyond these general features. One approach views education as a process that occurs during events such as schooling, teaching, and learning. Another outlook understands education not as a process but as the mental states and dispositions of educated persons that result from this process. Additionally, the term may also refer to the academic field that studies the methods, processes, and social institutions involved in teaching and learning. Having a clear idea of what the term means matters when trying to identify educational phenomena, measure educational success, and improve educational practices.

Some theorists provide precise definitions by identifying the specific features that are exclusive to all forms of education. Education theorist R. S. Peters, for instance, outlines three essential features of education, which include that knowledge and understanding are imparted to the student and that this process is beneficial and done in a morally appropriate manner. Such precise definitions often succeed at characterizing the most typical forms of education. However, they often face criticism because less common types of education occasionally fall outside their parameters. The difficulty of dealing with counterexamples not covered by precise definitions can be avoided by offering less exact definitions based on family resemblance instead. This means that all the forms of education are similar to each other, but they need not share a set of essential features that all of them have in common. Some education theorists, such as Keira Sewell and Stephen Newman, hold that the term "education" is context-dependent.

Evaluative or thick conceptions of education state that it is part of the nature of education to lead to some kind of improvement. They contrast with thin conceptions, which provide a value-neutral explanation. Some theorists provide a descriptive conception of education by observing how the term is commonly used in ordinary language. Prescriptive conceptions, by contrast, define what good education is or how education should be practiced. Many thick and prescriptive conceptions see education as an activity that tries to achieve certain aims, which may range from acquiring knowledge and learning to think rationally to nurturing character traits like kindness and honesty.

Various scholars stress the role of critical thinking to distinguish education from indoctrination. They state that mere indoctrination is only interested in instilling beliefs in the student, independent of whether the beliefs are rational; whereas education also fosters the rational ability to critically reflect on and question those beliefs. It is not universally accepted that these two phenomena can be clearly distinguished since some forms of indoctrination may be necessary in the early stages of education while the child's mind is not yet sufficiently developed. This applies to cases in which young children need to learn something without being able to understand the underlying reasons, like certain safety rules and hygiene practices.

Education can be characterized from the teacher's or the student's perspective. Teacher-centered definitions focus on the perspective and role of the teacher in the transmission of knowledge and skills in a morally appropriate way. Student-centered definitions analyze education from the student's involvement in the learning process and hold that this process transforms and enriches their subsequent experiences. Definitions taking both perspectives into account are also possible. This can take the form of describing education as a process of a shared experience of discovering a common world and solving problems.

There are many classifications of education. One of them depends on the institutional framework and distinguishes between formal, non-formal, and informal education. Another classification includes distinct levels of education based on factors like the student's age and the complexity of the content. Further categories focus on the topic, the teaching method, the medium used, and the funding.

The most common division is between formal, non-formal, and informal education. Formal education happens in a complex institutional framework. Such frameworks have a chronological and hierarchical order: the modern schooling system has classes based on the student's age and progress, extending from primary school to university. Formal education is usually controlled and guided by the government. It tends to be compulsory up to a certain age.

Non-formal and informal education take place outside the formal schooling system. Non-formal education is a middle ground. Like formal education, it is organized, systematic, and carried out with a clear purpose, as in the case of tutoring, fitness classes, and the scouting movement. Informal education happens in an unsystematic way through daily experiences and exposure to the environment. Unlike formal and non-formal education, there is usually no designated authority figure responsible for teaching. Informal education takes place in many different settings and situations throughout one's life, usually in a spontaneous way. This is how children learn their first language from their parents and how people learn to prepare a dish by cooking together.

Some theorists distinguish the three types based on the location of learning: formal education takes place in school, non-formal education happens in places that are not regularly visited, like museums, and informal education occurs in places of everyday routines. There are also differences in the source of motivation. Formal education tends to be driven by extrinsic motivation for external rewards. In non-formal and informal education, enjoyment of the learning process usually provides intrinsic motivation. The distinction between the three types is normally clear, but some forms of education do not easily fall into one category.

In primitive cultures, most education happened on the informal level and there was mostly no distinction between activities focused on education and other activities. Instead, the whole environment acted as a form of school, and most adults acted as teachers. Informal education is often not efficient enough to teach large quantities of knowledge. To do so, a formal setting and well-trained teachers are usually required. This was one of the reasons why in the course of history, formal education became more and more important. In this process, the experience of education and the discussed topics became more abstract and removed from daily life while more emphasis was put on grasping general patterns and concepts instead of observing and imitating particular forms of behavior.

Types of education are often divided into levels or stages. The most influential framework is the International Standard Classification of Education, maintained by the United Nations Educational, Scientific and Cultural Organization (UNESCO). It covers both formal and non-formal education and distinguishes levels based on the student's age, the duration of learning, and the complexity of the discussed content. Further criteria include entry requirements, teacher qualifications, and the intended outcome of successful completion. The levels are grouped into early childhood education (level 0), primary education (level 1), secondary education (levels 2–3), post-secondary non-tertiary education (level 4), and tertiary education (levels 5–8).

Early childhood education, also known as preschool education or nursery education, begins with birth and lasts until the start of primary school. It follows the holistic aim of fostering early child development at the physical, mental, and social levels. It plays a key role in socialization and personality development and includes various basic skills in the areas of communication, learning, and problem-solving. This way, it aims to prepare children for their entry into primary education. Preschool education is usually optional, but in some countries, such as Brazil, it is mandatory starting from the age of four.
Primary (or elementary) education usually starts within the ages of five to seven and lasts for four to seven years. It does not have any further entry requirements, and its main goal is to teach the basic skills in the fields of reading, writing, and mathematics. It also covers the core knowledge in other fields, like history, geography, the sciences, music, and art. A further aim is to foster personal development. Today, primary education is compulsory in almost all countries, and over 90% of all primary-school-age children worldwide attend primary school.

Secondary education follows primary education and usually covers the ages of 12 to 18 years. It is commonly divided into lower secondary education (middle school or junior high school) and upper secondary education (high school, senior high school, or college depending on the country). Lower secondary education normally has the completion of primary school as its entry requirement. It aims to extend and deepen the learning outcomes and is more focused on subject-specific curricula and teachers are specialized in only one or a few specific subjects. One of its aims is to familiarize students with the basic theoretical concepts in the different subjects. This helps create a solid basis for lifelong learning. In some cases, it also includes basic forms of vocational training. Lower secondary education is compulsory in many countries in Central and East Asia, Europe, and America. In some countries, it is the last stage of compulsory education. Mandatory lower secondary education is not as prevalent in Arab states, sub-Saharan Africa, and South and West Asia.
Upper secondary education starts roughly at the age of 15 and aims to provide students with the skills and knowledge needed for employment or tertiary education. Its requirement is usually the completion of lower secondary education. Its subjects are more varied and complex and students can often choose between a few subjects. Its successful completion is commonly tied to a formal qualification in the form of a high school diploma. Some types of education after secondary education do not belong to tertiary education and are categorized as post-secondary non-tertiary education. They are similar in complexity to secondary education but tend to focus more on vocational training to prepare students for the job market.

In some countries, tertiary education is used as a synonym of higher education, while in others, tertiary education is the wider term. Tertiary education expands upon the foundations of secondary education but has a more narrow and in-depth focus on a specific field or subject. Its completion leads to an academic degree. It can be divided into four levels: short-cycle tertiary, Bachelor's, Master's, and doctoral level education. These levels often form a hierarchical structure with later levels depending on the completion of previous levels. Short-cycle tertiary education focuses on practical matters. It includes advanced vocational and professional training to prepare students for the job market in specialized professions. Bachelor's level education, also referred to as undergraduate education, tends to be longer than short-cycle tertiary education. It is usually offered by universities and results in an intermediary academic certification in the form of a bachelor's degree. Master's level education is more specialized than undergraduate education. Many programs require independent research in the form of a master's thesis as a requirement for successful completion. Doctoral level education leads to an advanced research qualification, normally in the form of a doctor's degree, such as a Doctor of Philosophy (PhD). It usually requires the submission of a substantial academic work, such as a dissertation. More advanced levels include post-doctoral studies and habilitation.

Successful participation in formal education usually results in a form of certification that is required for higher levels of education and certain professions. Undetected cheating in exams, for example, by using a cheat sheet, threatens to undermine this system if unqualified students are certified.

In most countries, primary and secondary education are free of charge. There are significant global differences in the cost of tertiary education. A few countries, like Sweden, Finland, Poland, and Mexico, offer tertiary education for free or at a low cost. In some countries, like the United States and Singapore, tertiary school tuition fees are high and students often have to take substantial loans to afford their studies. High costs of education can constitute a significant barrier to students in developing countries whose families may be unable to afford school fees, uniforms, and textbooks.

The academic literature discusses many other types of education and distinguishes between traditional and alternative education. Traditional education concerns long-established and mainstream schooling practices. It uses teacher-centered education and takes place in a well-regulated school environment. Regulations cover many aspects of education, such as the curriculum and the timeframe when classes start and end.
Alternative education is an umbrella term for forms of schooling that differ from the mainstream traditional approach. Differences may include learning environment, subjects, or the teacher-student relationship. Alternative schooling is characterized by voluntary participation, relatively small class and school sizes, and personalized instruction. This often results in a more welcoming and emotionally safe atmosphere. Alternative education encompasses many types like charter schools and special programs for problematic or gifted children. It also includes homeschooling and unschooling. There are many alternative schooling traditions, like Montessori schools, Waldorf schools, Round Square schools, Escuela Nueva schools, free schools, and democratic schools. Alternative education also includes indigenous education, which focuses on the transmission of knowledge and skills from an indigenous heritage and employs methods like narration and storytelling. Further types of alternative schools include gurukul schools in India, madrasa schools in the Middle East, and yeshivas in Jewish tradition.

Some distinctions focus on who receives education. Categories by the age of the learner are childhood education, adolescent education, adult education, and elderly education. Categories by biological sex of the students include single-sex education and mixed-sex education. Special education is education that is specifically adapted to meet the unique needs of students with disabilities. It covers various forms of impairments on the intellectual, social, communicative, and physical levels. It aims to overcome the challenges posed by these impairments. This way, it provides the affected students with access to an appropriate educational structure. When understood in the broadest sense, special education also includes education for very gifted children who need adjusted curricula to reach their fullest potential.

Classifications based on the teaching method include teacher-centered education, in which the teacher takes center stage in providing students with information, and student-centered education, in which students take on a more active and responsible role in shaping classroom activities. For conscious education, learning and teaching happen with a clear purpose in mind. Unconscious education occurs on its own without being consciously planned or guided. This may happen in part through the personality of teachers and adults, which can have indirect effects on the development of the student's personality. Evidence-based education uses scientific studies to determine which methods of education work best. Its goal is to maximize the effectiveness of educational practices and policies by ensuring that they are informed by the best available empirical evidence. It includes evidence-based teaching, evidence-based learning, and school effectiveness research.

Autodidacticism, or self-education, happens without the guidance of teachers and institutions. It mainly occurs in adult education and is characterized by the freedom to choose what and when to study, which is why it can be a more fulfilling learning experience. The lack of structure and guidance can result in aimless learning, and the absence of external feedback may lead autodidacts to develop false ideas and inaccurately assess their learning progress. Autodidacticism is closely related to lifelong education, which is an ongoing learning process throughout a person's entire life.

Categories of education based on the subject include science education, language education, art education, religious education, physical education, and sex education. Special mediums, such as radio or websites, are used in distance education. Examples include e-learning (use of computers), m-learning (use of mobile devices), and online education. They often take the form of open education, in which the courses and materials are made available with a minimal amount of barriers. They contrast with regular classroom or onsite education. Some forms of online education are not open education, such as full online degree programs offered by some universities.

State education, also referred to as public education, is funded and controlled by the government and available to the general public. It normally does not require tuition fees and is thus a form of free education. Private education, by contrast, is funded and managed by private institutions. Private schools often have a more selective admission process and offer paid education by charging tuition fees. A more detailed classification focuses on the social institution responsible for education, like family, school, civil society, state, and church.

Compulsory education is education that people are legally required to receive. It concerns mainly children who need to visit school up to a certain age. It contrasts with voluntary education, which people pursue by personal choice without a legal requirement.

Education plays various roles in society, including in social, economic, and personal fields. On a social level, education makes it possible to establish and sustain a stable society. It helps people acquire the basic skills needed to interact with their environment and fulfill their needs and desires. In modern society, this involves a wide range of skills like being able to speak, read, write, solve arithmetic problems, and handle information and communications technology. Socialization also includes learning the dominant social and cultural norms and what kinds of behavior are considered appropriate in different contexts. Education enables the social cohesion, stability, and peace needed for people to productively engage in daily business. Socialization happens throughout life but is of special relevance to early childhood education. Education plays a key role in democracies by increasing civic participation in the form of voting and organizing, and through its tendency to promote equal opportunity for all.

On an economic level, people become productive members of society through education by acquiring the technical and analytical skills needed to pursue their profession, produce goods, and provide services to others. In early societies, there was little specialization, and each child would generally learn most of the skills that the community required to function. Modern societies are increasingly complex and many professions are only mastered by relatively few people who receive specialized training in addition to general education. Some of the skills and tendencies learned to function in society may conflict with each other, and their value depends on the context of their usage. For example, cultivating the tendency to be inquisitive and question established teachings promotes critical thinking and innovation, but in some cases, obedience to an authority is required to ensure social stability.

By helping people become productive members of society, education stimulates economic growth and reduces poverty. It helps workers become more skilled and thereby increases the quality of the produced goods and services, which in turn leads to prosperity and increased competitiveness. Public education is often understood as a long-term investment to benefit society as a whole. The rate of return is especially high for investments in primary education. Besides increasing economic prosperity, it can also lead to technological and scientific advances as well as decrease unemployment while promoting social equity. Increased education is associated with lower birth rates, in part because education augments the awareness of family planning, creates new opportunities for women, and tends to raise the age of marriage.

Education can prepare a country to adapt to changes and successfully face new challenges. It can help raise awareness and contribute to the solution of contemporary global problems, such as climate change, sustainability, and the widening inequalities between the rich and the poor. By making students aware of how their lives and actions affect others, it may inspire some to work toward realizing a more sustainable and fair world. This way, education serves not just the purpose of maintaining the societal status quo, but can also be an instrument of social development. That applies also to changing circumstances in the economic sector. For example, technological advances, particularly increased automation, are accompanied by new demands on the workforce, which education can help address. Changing circumstances may render currently taught skills and knowledge redundant while shifting the importance to other areas. Education can be used to prepare people for such changes by adjusting the curriculum, introducing subjects like digital literacy, promoting skills in handling new technologies, and including new forms of education such as massive open online courses.

On a more individual level, education promotes personal development. This can include factors such as learning new skills, developing talents, fostering creativity, and increasing self-knowledge as well as improving problem-solving and decision-making abilities. Education also has positive effects on health and well-being. Key factors responsible for these effects are that educated individuals tend to be better informed about health issues and adjust their behavior accordingly, have a better social support network and coping strategies, and have a higher income, which allows them access to high-quality healthcare services. The social importance of education is recognized by the annual International Day of Education on January 24. The United Nations declared the year 1970 the "International Education Year".

Organized institutions play a key role in various aspects of education. Institutions like schools, universities, teacher training institutions, and ministries of education make up the education sector. They interact both with each other and with other stakeholders, such as parents, local communities, religious groups, non-governmental organizations, professionals in healthcare, law enforcement, media platforms, and political leaders. Many people are directly involved in the education sector, like students, teachers, and school principals as well as school nurses and curriculum developers.

Various aspects of formal education are regulated by the policies of governmental institutions. These policies determine at what age children need to attend school and at what times classes are held as well as issues pertaining to the school environment, like infrastructure. Regulations also cover the exact qualifications and requirements that teachers need to fulfill. An important aspect of education policy concerns the curriculum used for teaching at schools, colleges, and universities. A curriculum is a plan of instruction or a program of learning that guides students to achieve their educational goals. The topics are usually selected based on their importance and depend on the type of school. The goals of public school curricula are usually to offer a comprehensive and well-rounded education, while vocational training focuses more on specific practical skills within a field. The curricula also cover various aspects besides the topic to be discussed, such as the teaching method, the objectives to be reached, and the standards for assessing progress. By determining the curricula, governmental institutions have a strong impact on what knowledge and skills are transmitted to the students. Examples of governmental institutions include the Ministry of Education in India, the Department of Basic Education in South Africa, and the Secretariat of Public Education in Mexico.

International organizations also play a key role in education. For instance, UNESCO is an intergovernmental organization that promotes education in many ways. One of its activities is to advocate education policies, like the treaty Convention on the Rights of the Child, which states that education is a human right of all children and young people. The Education for All initiative aimed to offer basic education to all children, adolescents, and adults by the year 2015 and was later replaced by the initiative Sustainable Development Goals as goal 4. Related policies include the Convention against Discrimination in Education and the Futures of Education initiative.

Some influential organizations are not intergovernmental, but non-governmental. For example, the International Association of Universities promotes collaboration and the exchange of knowledge between colleges and universities around the world, while the International Baccalaureate offers international diploma programs. Institutions like the Erasmus Programme facilitate student exchanges between countries, while initiatives such as the Fulbright Program provide a similar service for teachers.

Educational success, also called student and academic achievement, refers to the extent to which educational aims are reached, for example, the amount of knowledge and abilities that students acquire. For practical purposes, it is often measured primarily in terms of official exam scores, but there are many additional indicators, such as attendance rates, graduation rates, dropout rates, student attitudes, and post-school indicators like later income and incarceration rates. Several factors influence educational achievement, including psychological factors, which concern the student as an individual, and sociological factors, which pertain to the student's social environment. Further factors are access to educational technology, teacher quality, and parent involvement. Many of these factors overlap and influence each other.

On a psychological level, relevant factors include motivation, intelligence, and personality. Motivation is the internal force propelling people to engage in learning. Motivated students are more likely to interact with the content to be learned by participating in classroom activities like discussions, which often results in a deeper understanding of the subject. Motivation can also help students overcome difficulties and setbacks. An important distinction is between intrinsic and extrinsic motivation. Intrinsically motivated students are driven by an interest in the subject and the learning experience itself. Extrinsically motivated students seek external rewards like good grades and recognition from peers. Intrinsic motivation tends to be more beneficial by leading to increased creativity and engagement as well as long-term commitment. Educational psychologists try to discover how to increase motivation. This can be achieved, for instance, by encouraging some competition among students while ensuring a balance of positive and negative feedback in the form of praise and criticism.

Intelligence influences how people respond to education. It is a mental quality linked to the ability to learn from experience, to understand, and to employ knowledge and skills to solve problems. Those who have higher scores in intelligence metrics tend to perform better at school and go on to higher levels of education. Intelligence is often primarily associated with the so-called IQ, a standardized numerical metric for assessing intelligence by focusing on mathematical-logical and verbal skills. However, it has been argued that there are more types of intelligence. According to the psychologist Howard Gardner, there are distinct forms of intelligence belonging to fields like mathematics, logic, spatial cognition, language, and music. Further types affect how a person interacts with other people and with themselves. These types of intelligence are largely independent of each other, meaning that someone may excel at one type while scoring low on another.

A closely related factor concerns learning styles, which are preferred forms of acquiring knowledge and skills. According to proponents of learning style theory, students with an auditory learning style find it easy to follow spoken lectures and discussions, while visual learners benefit if information is presented visually in diagrams and videos. For efficient learning, it may be beneficial to include a wide variety of learning modalities. The learner's personality may also affect educational achievement. For example, the features of conscientiousness and openness to experience from the Big Five personality traits are linked to academic success. Further mental factors include self-efficacy, self-esteem, and metacognitive abilities.

Sociological factors focus not on psychological attributes of learners but on their environment and position in society. They include socioeconomic status, ethnicity, cultural background, and gender. They are of interest to researchers since they are associated with inequality and discrimination. For this reason, they play a key role in policy-making in attempts to mitigate their effects.

Socioeconomic status depends on income but includes other factors, such as financial security, social status, social class, and quality of life attributes. Low socioeconomic status affects educational success in various ways. It is linked to slower cognitive developments in language and memory and higher dropout rates. Poor families may not have enough money to meet basic the nutritional needs of their children, causing poor development. They may also lack the means to invest in educational resources like stimulating toys, books, and computers. Additionally, they may be unable to afford tuition at prestigious schools and are more likely to attend schools in poorer areas. Such schools tend to offer lower standards of teaching because of teacher shortages or because they lack educational materials and facilities, like libraries. Poor parents may also be unable to afford private lessons if their children fall behind. In some cases, students from an economically disadvantaged background are forced to dropout from school to provide income to their families. They also have less access to information on higher education and may face additional difficulties in securing and repaying student loans. Low socioeconomic status also has many indirect negative effects by being linked to lower physical and mental health. Due to these factors, social inequalities on the level of the parents are often reproduced in the children.

Ethnic background is linked to cultural differences and language barriers, which make it more difficult for students to adapt to the school environment and follow classes. Additional factors are explicit and implicit biases and discrimination toward ethnic minorities. This may affect the students' self-esteem and motivation as well as their access to educational opportunities. For example, teachers may hold stereotypical views even if they are not overtly racist, which can lead them to grade comparable performances differently based on the child's ethnicity.

Historically, gender has been a central factor in education since the roles of men and women were defined differently in many societies. Education tended to strongly favor men, who were expected to provide for the family. Women, by contrast, were expected to manage the household and rear children, which barred most educational opportunities available to them. While these inequalities have improved in most modern societies, there are still gender differences in education. Among other things, this concerns biases and stereotypes linked to the role of gender in education. They affect subjects like science, technology, engineering, and mathematics, which are often presented as male fields. This discourages female students from following them. In various cases, discrimination based on gender and social factors happens openly as part of official educational policy, such as the severe restrictions on female education instituted by the Taliban in Afghanistan and the school segregation of migrants and locals in urban China under the hukou system.

One aspect of many social factors is given by the expectations associated with stereotypes. They work both on an external level, based on how other people react to a person belonging to a certain group, and on an internal level, based on how the person internalizes them and acts accordingly. In this sense, the expectations may turn into self-fulfilling prophecies by causing the educational outcomes they anticipate. This can happen both for positive and negative stereotypes.

Technology plays another significant role in educational success. Educational technology is commonly associated with the use of modern digital devices, like computers. But understood in the broadest sense, it involves a wide range of resources and tools for learning, including basic aids that do not involve the use of machines, like regular books and worksheets.
Educational technology can benefit learning in various ways. In the form of media, it often takes the role of the primary supplier of information in the classroom. This means that the teacher can focus their time and energy on other tasks, like planning the lesson and guiding students as well as assessing educational performance. Educational technology can also make information easier to understand by presenting it using graphics, audio, and video rather than through mere text. In this regard, interactive elements may be used to make the learning experience more engaging in the form of educational games. Technology can be employed to make educational materials accessible to many people, like when using online resources. It additionally facilitates collaboration between students and communication with teachers. The use of artificial intelligence in education holds various potentials, such as providing new learning experiences to students and assisting teachers in their work, but also poses new risks associated with data privacy, false information, and manipulation. Various organizations promote student access to educational technologies, such as the One Laptop per Child initiative, the African Library Project, and Pratham.

School infrastructure also influences educational success. It includes physical aspects of the school, like its location and size as well as the available school facilities and equipment. A healthy and safe environment, well-maintained classrooms, and suitable classroom furniture as well as the availability of a library and a canteen tend to contribute to educational success. The quality of the teacher also has an important impact on student achievement. Skilled teachers know how to motivate and inspire students and are able to adjust their instructions to the students' abilities and needs. Important in this regard are the teacher's own education and training as well as their past teaching experience. A meta-analysis by Engin Karadağ et al. concludes that, compared to other influences, factors related to the school and the teacher have the biggest impact on educational success.

Parent involvement also boosts achievement and can make children more motivated and invested if they are aware that their parents care about their educational efforts. This tends to lead to increased self-esteem, better attendance rates, and more constructive behavior at school. Parent involvement also includes communication with teachers and other school staff to make other parties aware of current issues and how they may be resolved. Further relevant factors sometimes discussed in the academic literature include historical, political, demographic, religious, and legal aspects.

The main discipline investigating education is called education studies, also referred to as education sciences. It tries to determine how people transmit and acquire knowledge by studying the methods and forms of education. It is interested in its aims, effects, and value as well as the cultural, societal, governmental, and historical contexts that shape education. Education theorists integrate insights from many other fields of inquiry, including philosophy, psychology, sociology, economics, history, politics, and international relations. Because of these influences, some theorists claim that education studies is not an independent academic discipline like physics or history since its method and subject are not as clearly defined. Education studies differs from regular training programs, such as teacher training, since its focus on academic analysis and critical reflection goes beyond the skills needed to be a good teacher. It is not restricted to the topic of formal education but examines all forms and aspects of education.

Various research methods are used to study educational phenomena. They roughly divide into quantitative, qualitative, and mixed-methods approaches. Quantitative research emulates the methods found in the natural sciences by using precise numerical measurements to gather data from many observations and employs statistical tools to analyze it. It aims to arrive at an objective and impersonal understanding. Qualitative research usually has a much smaller sample size and tries to get an in-depth insight into more subjective and personal factors, like how different actors experience the process of education. Mixed-methods research aims to combine data gathered from both approaches to arrive at a balanced and comprehensive understanding. Data can be collected in various ways, like using direct observation or test scores as well as interviews and questionnaires. Some research projects study basic factors affecting all forms of education, while others concentrate on one specific application, look for solutions to concrete problems, or examine the effectiveness of educational projects and policies.

Education studies encompasses various subfields like pedagogy, comparative education, and the philosophy, psychology, sociology, economics, and history of education. The philosophy of education is the branch of applied philosophy that examines many of the basic assumptions underlying the theory and practice of education. It studies education both as a process and as a discipline while trying to provide exact definitions of its nature and how it differs from other phenomena. It further examines the purpose of education, its different types, and how to conceptualize teachers, students, and their relation. It includes educational ethics, which investigates the moral implications of education; for example, what ethical principles direct it and how teachers should apply them to specific cases. The philosophy of education has a long history and was discussed in ancient Greek philosophy.

The term "pedagogy" is sometimes used as a synonym for education studies, but when understood in a more restricted sense, it refers to the subfield interested in teaching methods. It studies how the aims of education, like the transmission of knowledge or fostering skills and character traits, can be realized. It is interested in the methods and practices used for teaching in regular schools. Some definitions restrict it to this domain, but in a wider sense, it covers all types of education, including forms of teaching outside schools. In this general sense, it explores how teachers can bring about experiences in learners to advance their understanding of the studied topic and how the learning itself takes place.

The psychology of education studies how education happens on the mental level, specifically how new knowledge and skills are acquired as well as how personal growth takes place. It examines what factors influence educational success, how they may differ between individuals, and to what extent nature or nurture is responsible. Influential psychological theories of education are behaviorism, cognitivism, and constructivism. Closely related fields are the neurology of education and educational neuroscience, which are interested in the neuropsychological processes and changes brought about through learning.

The sociology of education is concerned with how education leads to socialization. It examines how social factors and ideologies affect what kind of education is available to a person and how successful they are. Closely related questions include how education affects different groups in society and how educational experiences can form someone's personal identity. The sociology of education is specifically interested in the causes of inequalities, and its insights are relevant to education policy by trying to identify and mitigate factors that cause inequality. Two influential schools of thought are consensus theory and conflict theory. Consensus theorists hold that education benefits society as a whole by preparing people for their roles. Conflict theories have a more negative outlook on the resulting inequalities and see education as a force used by the ruling class to promote their own agenda.

The economics of education is the field of inquiry studying how education is produced, distributed, and consumed. It tries to determine how resources should be used to improve education, for example, by examining to what extent the quality of teachers is increased by raising their salary. Other questions are how smaller class sizes affect educational success and how to invest in new educational technologies. This way, the economics of education helps policy-makers decide how to distribute the limited resources most efficiently to benefit society as a whole. It also tries to understand what long-term role education plays for the economy of a country by providing a highly skilled labor force and increasing its competitiveness. A closely related issue concerns the economic advantages and disadvantages of different systems of education.

Comparative education is the discipline that examines and contrasts systems of education. Comparisons can happen from a general perspective or focus on specific factors, like social, political, or economic aspects. Comparative education is often applied to different countries to assess the similarities and differences of their educational institutions and practices as well as to evaluate the consequences of the distinct approaches. It can be used to learn from other countries which education policies work and how one's own system of education may be improved. This practice is known as policy borrowing and comes with many difficulties since the success of policies can depend to a large degree on the social and cultural context of students and teachers. A closely related and controversial topic concerns the question of whether the educational systems of developed countries are superior and should be exported to less developed countries. Other key topics are the internationalization of education and the role of education in transitioning from an authoritarian regime to a democracy.

The history of education examines the evolution of educational practices, systems, and institutions. It discusses various key processes, their possible causes and effects, and their relations to each other.

A central topic in education studies concerns the question of how people should be educated and what goals should guide this process. Many aims of education have been suggested, such as the acquisition of knowledge and skills as well as personal development and fostering of character traits. Common suggestions encompass features like curiosity, creativity, rationality, and critical thinking as well as the tendency to think, feel, and act morally. Some scholars focus on liberal values linked to freedom, autonomy, and open-mindedness, while others prioritize qualities like obedience to authority, ideological purity, piety, and religious faith. 

Some education theorists focus a single overarching purpose of education and see the more specific aims as means to this end. On a personal level, this purpose is often identified with helping the student lead a good life. On a societal level, education makes people productive members of society. It is controversial whether the primary aim of education is to benefit the educated person or society as a whole. 

Educational ideologies are systems of basic philosophical assumptions and principles that can be used to interpret, understand, and evaluate existing educational practicies and policies. They cover various additional issues besides the aims of education, like what topics are learned and how the learning activity is structured. Other themes include the role of the teacher, how educational progress should be assessed, and how institutional frameworks and policies should be structured. There are many ideologies, and they often overlap in various ways. Teacher-centered ideologies place the main emphasis on the teacher's role in transmitting knowledge to students, while student-centered ideologies give a more active role to the students in the process. Process-based ideologies focus on what the processes of teaching and learning should be like and contrast with product-based ideologies, which discuss education from the perspective of the result to be achieved. Conservative ideologies rely on traditional and well-established practices while Progressive ideologies emphasize innovation and creativity. Further categories are humanism, romanticism, essentialism, encyclopaedism, and pragmatism as well as authoritarian and democratic ideologies.

Learning theories try to explain how learning happens. Influential theories are behaviorism, cognitivism, and constructivism. Behaviorism understands learning as a change in behavior in response to environmental stimuli. This happens by presenting the learner with a stimulus, associating this stimulus with the desired response, and solidifying this stimulus-response pair. Cognitivism sees learning as a change in cognitive structures and focuses on the mental processes involved in storing, retrieving, and processing information. Constructivism holds that learning is based on the personal experience of each individual and puts more emphasis on social interactions and how they are interpreted by the learner. These theories have important implications for how to teach. For example, behaviorists tend to focus on drills, while cognitivists may advocate the use of mnemonics, and constructivists tend to employ collaborative learning strategies.

Various theories suggest that learning is more efficient when it is based on personal experience. An additional factor is to aim at a deeper understanding by connecting new to pre-existing knowledge rather than merely memorizing a list of unrelated facts. An influential developmental theory of learning is proposed by psychologist Jean Piaget, who outlines four stages of learning through which children pass on their way to adulthood: the sensorimotor, the pre-operational, the concrete operational, and the formal operational stage. They correspond to different levels of abstraction with early stages focusing more on simple sensory and motor activities, while later stages include more complex internal representations and information processing in the form of logical reasoning.

The teaching method concerns the way the content is presented by the teacher, for example, whether group work is used instead of a focus on individual learning. There are many teaching methods available and which one is most efficient in a case depends on factors like the subject matter and the learner's age and competence level. This is reflected in the fact that modern school systems organize students by age, competence, specialization, and native language into different classes to ensure a productive learning process. Different subjects frequently use different approaches; for instance, language education often focuses on verbal learning, while mathematical education is about abstract and symbolic thinking together with deductive reasoning. One central requirement for teaching methodologies is to ensure that the learner remains motivated because of interest and curiosity or through external rewards.

Teaching method also encompasses the use of instructional media used, such as books, worksheets, and audio-visual recordings, and having some form of test or assessment to evaluate the learning progress. An important pedagogical aspect in many forms of modern education is that each lesson is part of a larger educational enterprise governed by a syllabus, which often covers several months or years. According to Herbartianism, teaching is divided into phases. The initial phase consists of preparing the student's mind for new information. Next, new ideas are first presented to the learner and then associated with ideas with which the learner is already familiar. In later phases, the understanding shifts to a more general level behind the specific instances, and the ideas are then put into concrete practice.

The history of education studies the processes, methods, and institutions involved in teaching and learning. It tries to explain how they have interacted with each other and shaped educational practice until the present day. Education in prehistory took place as a form of enculturation and focused on practical knowledge and skills relevant to everyday concerns, for example, in relation to food, clothing, shelter, and protection. There were no formal schools or specialized teachers, and most adults in the community performed that role and learning happened informally during everyday activities, for example, when children observed and imitated their elders. For these oral societies, storytelling played a key role in transferring cultural and religious ideas from one generation to the next. Beginning with the emergence of agriculture around 9000 BCE, a slow educational change towards more specialization began to occur as people formed larger groups and more complex artisanal and technical skills were needed.

Starting in the 4th millennium BCE and continuing through the following millennia, a major shift in educational practices started to take place with the invention of writing in regions such as Mesopotamia, ancient Egypt, the Indus Valley, and ancient China. This development had a significant influence on the history of education as a whole. Through writing, it was possible to store, preserve, and communicate information. This facilitated various subsequent developments; for example, the creation of educational tools, like textbooks, and the formation of institutions, like schools.
Another key aspect of ancient education was the establishment of formal education. This became necessary since the amount of knowledge grew as civilizations evolved, and informal education proved insufficient to transmit all requisite knowledge between generations. Teachers would act as specialists to impart knowledge, and education became more abstract and further removed from daily life. Formal education was still quite rare in ancient societies and was restricted to the intellectual elites. It covered fields like reading and writing, record keeping, leadership, civic and political life, religion, and technical skills associated with specific professions. Formal education introduced a new way of teaching that gave more emphasis to discipline and drills than the earlier informal modes of education. Two often-discussed achievements of ancient education are the establishment of Plato's Academy in Ancient Greece, which is sometimes considered the first institute of higher learning, and the creation of the Great Library of Alexandria in Ancient Egypt as one of the most prestigious libraries of the ancient world.

Many aspects of education in the medieval period were shaped by religious traditions. In Europe, the Catholic Church wielded a significant influence over formal education. In the Arab world, the newly founded religion of Islam spread rapidly and led to various educational developments during the Islamic Golden Age, for example, by integrating classical and religious knowledge and by establishing madrasa schools. In Jewish communities, yeshivas were established as institutions dedicated to the study of religious texts and Jewish law. In China, an expansive state educational and exam system influenced by Confucian teachings was established. New complex societies began to evolve in other regions, such as Africa, the Americas, Northern Europe, and Japan. Some incorporated preexisting educational practices, while others developed new traditions. Additionally, this period saw the establishment of various institutes of higher education and research. The first universities in Europe were the University of Bologna, the University of Paris, and Oxford University. Other influential centers of higher learning were the Al-Qarawiyyin University in Morocco, the Al-Azhar University in Egypt, and the House of Wisdom in Iraq. Another key development was the creation of guilds, which were associations of skilled craftsmen and merchants who controlled the practice of their trades. They were responsible for vocational education, and new members had to pass through different stages on their way to masterhood.
Starting in the early modern period, education in Europe during the Renaissance slowly began to shift from a religious approach towards one which was more secular. This development was tied to an increased appreciation of the importance of education and a broadened range of topics, including a revived interest in ancient literary texts and educational programs. The turn toward secularization was accelerated during the Age of Enlightenment starting in the 17th century, which emphasized the role of reason and the empirical sciences. European colonization affected education in the Americas through Christian missionary initiatives. In China, the state educational system was further expanded and focused more on the teachings of neo-Confucianism. In the Islamic world, the outreach of formal education increased and remained under the influence of religion. A key development in the early modern period was the invention and popularization of the printing press in the middle of the 15th century, which had a profound impact on general education. It significantly reduced the cost of producing books, which were hand-written before, and thereby augmented the dissemination of written documents, including new forms like newspapers and pamphlets. The increased availability of written media had a major influence on the general literacy of the population.

These changes prepared the rise of public education in the 18th and 19th centuries. This period saw the establishment of publicly funded schools with the aim of providing education for all. This contrasts with earlier periods when formal education was primarily provided by private schools, religious institutions, and individual tutors. Aztec civilization was an exception in this regard since formal education was mandatory for the youth regardless of social class as early as the 14th century. Closely related changes were to make education compulsory and free of charge for all children up to a certain age. Initiatives to promote public education and universal access to education made significant progress in the 20th and the 21st centuries and were promoted by intergovernmental organizations like the UN. Examples include the Universal Declaration of Human Rights, the Convention on the Rights of the Child, the Education for All initiative, the Millennium Development Goals, and the Sustainable Development Goals. These efforts resulted in a steady rise of all forms of education but affected primary education in particular. In 1970, 28% of all primary-school-age children worldwide did not attend school; by 2015, this number dropped to 9%.

The establishment of public education was accompanied by the introduction of standardized curricula for public schools as well as standardized tests to assess the student's progress. Contemporary examples include the Test of English as a Foreign Language, which is a globally used test to assess English language proficiency of non-native English speakers, and the Programme for International Student Assessment, which evaluates education systems worldwide based on how 15-year-old students perform in the fields of reading, mathematics, and science. Similar changes also affected teachers by setting in place institutions and norms to guide and oversee teacher training, like certification requirements for teaching at public schools.

Emerging educational technologies have shaped contemporary education. The widespread availability of computers and the internet dramatically increased access to educational resources and made new types of education possible, such as online education. This was of particular relevance during the COVID-19 pandemic when schools globally closed for extended periods and many offered remote learning through video conferencing or pre-recorded video lessons to continue instruction. Contemporary education is also shaped by the increased globalization and internationalization of education.



Encyclopedia

An encyclopedia (American English) or encyclopaedia (British English) is a reference work or compendium providing summaries of knowledge, either general or special, to a particular field or discipline. Encyclopedias are divided into articles or entries that are arranged alphabetically by article name or by thematic categories, or else are hyperlinked and searchable. Encyclopedia entries are longer and more detailed than those in most dictionaries. Generally speaking, encyclopedia articles focus on "factual information" concerning the subject named in the article's title; this is unlike dictionary entries, which focus on linguistic information about words, such as their etymology, meaning, pronunciation, use, and grammatical forms.

Encyclopedias have existed for around 2,000 years and have evolved considerably during that time as regards language (written in a major international or a vernacular language), size (few or many volumes), intent (presentation of a global or a limited range of knowledge), cultural perspective (authoritative, ideological, didactic, utilitarian), authorship (qualifications, style), readership (education level, background, interests, capabilities), and the technologies available for their production and distribution (hand-written manuscripts, small or large print runs, Internet). As a valued source of reliable information compiled by experts, printed versions found a prominent place in libraries, schools and other educational institutions.

The appearance of digital and open-source versions in the 21st century, such as Wikipedia, has vastly expanded the accessibility, authorship, readership, and variety of encyclopedia entries.

The word "encyclopedia" ("encyclo"|"pedia") comes from the Koine Greek , transliterated , meaning 'general education' from (), meaning 'circular, recurrent, required regularly, general' and (), meaning 'education, rearing of a child'; together, the phrase literally translates as 'complete instruction' or 'complete knowledge'. However, the two separate words were reduced to a single word due to a scribal error by copyists of a Latin manuscript edition of Quintillian in 1470. The copyists took this phrase to be a single Greek word, , with the same meaning, and this spurious Greek word became the Neo-Latin word , which in turn came into English. Because of this compounded word, fifteenth-century readers and since have often, and incorrectly, thought that the Roman authors Quintillian and Pliny described an ancient genre.

The modern encyclopedia evolved from the dictionary in the 18th century; this lineage can be seen in the alphabetical order of print encyclopedias. Historically, both encyclopedias and dictionaries have been compiled by well-educated, well-informed content experts, but they are significantly different in structure. A dictionary is a linguistic work which primarily focuses on alphabetical listing of words and their definitions. Synonymous words and those related by the subject matter are to be found scattered around the dictionary, giving no obvious place for in-depth treatment. Thus, a dictionary typically provides limited information, analysis or background for the word defined. While it may offer a definition, it may leave the reader lacking in understanding the meaning, significance or limitations of a term, and how the term relates to a broader field of knowledge.

To address those needs, an encyclopedia article is typically not limited to simple definitions, and is not limited to defining an individual word, but provides a more extensive meaning for a "subject or discipline". In addition to defining and listing synonymous terms for the topic, the article is able to treat the topic's more extensive meaning in more depth and convey the most relevant accumulated knowledge on that subject. An encyclopedia article also often includes many maps and illustrations, as well as bibliography and statistics. An encyclopedia is, theoretically, not written in order to convince, although one of its goals is indeed to convince its reader of its own veracity.

Wikipedia co-founder Jimmy Wales has said that the goal of an encyclopedia should be to provide "the sum of all human knowledge, but sum meaning summary."

In addition, sometimes books or reading lists are compiled from a compendium of articles (either wholly or partially taken) from a specific encyclopedia.

There are four major elements that define an encyclopedia: its subject matter, its scope, its method of organization, and its method of production:


Some works entitled "dictionaries" are actually similar to encyclopedias, especially those concerned with a particular field (such as the "Dictionary of the Middle Ages", the "Dictionary of American Naval Fighting Ships", and "Black's Law Dictionary"). The "Macquarie Dictionary," Australia's national dictionary, became an encyclopedic dictionary after its first edition in recognition of the use of proper nouns in common communication, and the words derived from such proper nouns.

There are some broad differences between encyclopedias and dictionaries. Most noticeably, encyclopedia articles are longer, fuller and more thorough than entries in most general-purpose dictionaries. There are differences in content as well. Generally speaking, dictionaries provide linguistic information about words themselves, while encyclopedias focus more on the things for which those words stand. Thus, while dictionary entries are inextricably fixed to the word described, encyclopedia articles can be given a different entry name. As such, dictionary entries are not fully translatable into other languages, but encyclopedia articles can be.

In practice, however, the distinction is not concrete, as there is no clear-cut difference between factual, "encyclopedic" information and linguistic information such as appears in dictionaries. Thus encyclopedias may contain material that is also found in dictionaries, and vice versa. In particular, dictionary entries often contain factual information about the thing named by the word.

The earliest encyclopedic work to have survived to modern times is the of Pliny the Elder, a Roman statesman living in the 1st century AD. He compiled a work of 37 chapters covering natural history, architecture, medicine, geography, geology, and all aspects of the world around him. This work became very popular in Antiquity, was one of the first classical manuscripts to be printed in 1470, and has remained popular ever since as a source of information on the Roman world, and especially Roman art, Roman technology and Roman engineering.
The Spanish scholar Isidore of Seville was the first Christian writer to try to compile a "summa" of universal knowledge, the "Etymologiae" (), also known by classicists as the "Origines" (abbreviated "Orig".). This encyclopedia—the first such Christian epitome—formed a huge compilation of 448 chapters in 20 books based on hundreds of classical sources, including the . Of the "Etymologiae" in its time it was said "quaecunque fere sciri debentur", "practically everything that it is necessary to know". Among the areas covered were: grammar, rhetoric, mathematics, geometry, music, astronomy, medicine, law, the Catholic Church and heretical sects, pagan philosophers, languages, cities, animals and birds, the physical world, geography, public buildings, roads, metals, rocks, agriculture, ships, clothes, food, and tools.

Another Christian encyclopedia was the "Institutiones divinarum et saecularium litterarum" of Cassiodorus (543–560) dedicated to the Christian divinity and to the seven liberal arts. The encyclopedia of Suda, a massive 10th-century Byzantine encyclopedia, had 30,000 entries, many drawing from ancient sources that have since been lost, and often derived from medieval Christian compilers. The text was arranged alphabetically with some slight deviations from common vowel order and place in the Greek alphabet.

From India, the Siribhoovalaya (Kannada: ಸಿರಿಭೂವಲಯ), dated between 800 A.D. to 15th century, is a work of Kannada literature written by Kumudendu Muni, a Jain monk. It is unique because rather than employing alphabets, it is composed entirely in Kannada numerals. Many philosophies which existed in the Jain classics are eloquently and skillfully interpreted in the work.

The enormous encyclopedic work in China of the "Four Great Books of Song", compiled by the 11th century during the early Song dynasty (960–1279), was a massive literary undertaking for the time. The last encyclopedia of the four, the "Prime Tortoise of the Record Bureau", amounted to 9.4 million Chinese characters in 1,000 written volumes.

There were many great encyclopedists throughout Chinese history, including the scientist and statesman Shen Kuo (1031–1095) with his "Dream Pool Essays" of 1088; the statesman, inventor, and agronomist Wang Zhen (active 1290–1333) with his "Nong Shu" of 1313; and Song Yingxing (1587–1666) with his "Tiangong Kaiwu". Song Yingxing was termed the "Diderot of China" by British historian Joseph Needham.

Before the advent of the printing press, encyclopedic works were all hand copied and thus rarely available, beyond wealthy patrons or monastic men of learning: they were expensive, and usually written for those extending knowledge rather than those using it.
During the Renaissance, the creation of printing allowed a wider diffusion of encyclopedias and every scholar could have his or her own copy. The "De expetendis et fugiendis rebus" by Giorgio Valla was posthumously printed in 1501 by Aldo Manuzio in Venice. This work followed the traditional scheme of liberal arts. However, Valla added the translation of ancient Greek works on mathematics (firstly by Archimedes), newly discovered and translated. The "Margarita Philosophica" by Gregor Reisch, printed in 1503, was a complete encyclopedia explaining the seven liberal arts.

Financial, commercial, legal, and intellectual factors changed the size of encyclopedias. Middle classes had more time to read and encyclopedias helped them to learn more. Publishers wanted to increase their output so some countries like Germany started selling books missing alphabetical sections, to publish faster. Also, publishers could not afford all the resources by themselves, so multiple publishers would come together with their resources to create better encyclopedias. Later, rivalry grew, causing copyright to occur due to weak underdeveloped laws.
John Harris is often credited with introducing the now-familiar alphabetic format in 1704 with his English "Lexicon Technicum: Or, A Universal English Dictionary of Arts and Sciences: Explaining not only the Terms of Art, but the Arts Themselves" – to give its full title. Organized alphabetically, its content does indeed contain explanation not merely of the terms used in the arts and sciences, but of the arts and sciences themselves. Sir Isaac Newton contributed his only published work on chemistry to the second volume of 1710.

In the United States, the 1950s and 1960s saw the introduction of several large popular encyclopedias, often sold on installment plans. The best known of these were "World Book" and "Funk and Wagnalls". As many as 90% were sold door to door. Jack Lynch says in his book "You Could Look It Up" that encyclopedia salespeople were so common that they became the butt of jokes. He describes their sales pitch saying, "They were selling not books but a lifestyle, a future, a promise of social mobility." A 1961 "World Book" ad said, "You are holding your family's future in your hands right now," while showing a feminine hand holding an order form. As of the 1990s, two of the most prominent encyclopedias published in the United States were "Collier's Encyclopedia" and "Encyclopedia Americana".

By the late 20th century, encyclopedias were being published on CD-ROMs for use with personal computers. This was the usual way computer users accessed encyclopedic knowledge from the 1980s and 1990s. Later, DVD discs replaced CD-ROMs, and by the mid-2000s, internet encyclopedias were dominant and replaced disc-based software encyclopedias. 

CD-ROM encyclopedias were usually a macOS or Microsoft Windows (3.0, 3.1 or 95/98) application on a CD-ROM disc. The user would execute the encyclopedia's software program to see a menu that allowed them to start browsing the encyclopedia's articles, and most encyclopedias also supported a way to search the contents of the encyclopedia. The article text was usually hyperlinked and also included photographs, audio clips (for example in articles about historical speeches or musical instruments), and video clips. In the CD-ROM age the video clips had usually a low resolution, often 160x120 or 320x240 pixels. Such encyclopedias which made use of photos, audio and video were also called multimedia encyclopedias. However, because of the online encyclopedia, CD-ROM encyclopedias have been declared obsolete.

Microsoft's "Encarta", launched in 1993, was a landmark example as it had no printed equivalent. Articles were supplemented with video and audio files as well as numerous high-quality images. After sixteen years, Microsoft discontinued the Encarta line of products in 2009. Other examples of CD-ROM encyclopedia are Grolier Multimedia Encyclopedia and "Britannica".

Digital encyclopedias enable "Encyclopedia Services" (such as Wikimedia Enterprise) to facilitate programmatic access to the content.

The concept of a free encyclopedia began with the Interpedia proposal on Usenet in 1993, which outlined an Internet-based online encyclopedia to which anyone could submit content that would be freely accessible. Early projects in this vein included Everything2 and Open Site. In 1999, Richard Stallman proposed the GNUPedia, an online encyclopedia which, similar to the GNU operating system, would be a "generic" resource. The concept was very similar to Interpedia, but more in line with Stallman's GNU philosophy.

It was not until Nupedia and later Wikipedia that a stable free encyclopedia project was able to be established on the Internet.

The English Wikipedia, which was started in 2001, became the world's largest encyclopedia in 2004 at the 300,000 article stage. By late 2005, Wikipedia had produced over two million articles in more than 80 languages with content licensed under the copyleft GNU Free Documentation License. Wikipedia had over 3 million articles in English and well over 10 million combined articles in over 250 languages. Today, Wikipedia has articles in English, over 60 million combined articles in over 300 languages, and over 250 million combined pages including project and discussion pages.

Since 2002, other free encyclopedias appeared, including Hudong (2005–) and Baidu Baike (2006–) in Chinese, and Google's Knol (2008–2012) in English. Some MediaWiki-based encyclopedias have appeared, usually under a license compatible with Wikipedia, including Enciclopedia Libre (2002–2021) in Spanish and Conservapedia (2006–), Scholarpedia (2006–), and Citizendium (2007–) in English, the latter of which had become inactive by 2014.



Enigma machine

The Enigma machine is a cipher device developed and used in the early- to mid-20th century to protect commercial, diplomatic, and military communication. It was employed extensively by Nazi Germany during World War II, in all branches of the German military. The Enigma machine was considered so secure that it was used to encipher the most top-secret messages.

The Enigma has an electromechanical rotor mechanism that scrambles the 26 letters of the alphabet. In typical use, one person enters text on the Enigma's keyboard and another person writes down which of the 26 lights above the keyboard illuminated at each key press. If plain text is entered, the illuminated letters are the ciphertext. Entering ciphertext transforms it back into readable plaintext. The rotor mechanism changes the electrical connections between the keys and the lights with each keypress.

The security of the system depends on machine settings that were generally changed daily, based on secret key lists distributed in advance, and on other settings that were changed for each message. The receiving station would have to know and use the exact settings employed by the transmitting station to successfully decrypt a message.

While Nazi Germany introduced a series of improvements to the Enigma over the years, and these hampered decryption efforts, they did not prevent Poland from cracking the machine as early as December 1932 and reading messages prior to and into the war. Poland's sharing of their achievements enabled the Allies to exploit Enigma-enciphered messages as a major source of intelligence. Many commentators say the flow of Ultra communications intelligence from the decrypting of Enigma, Lorenz, and other ciphers shortened the war substantially and may even have altered its outcome.
The Enigma machine was invented by German engineer Arthur Scherbius at the end of World War I. The German firm Scherbius & Ritter, co-founded by Scherbius, patented ideas for a cipher machine in 1918 and began marketing the finished product under the brand name "Enigma" in 1923, initially targeted at commercial markets. Early models were used commercially from the early 1920s, and adopted by military and government services of several countries, most notably Nazi Germany before and during World War II.

Several different Enigma models were produced, but the German military models, having a plugboard, were the most complex. Japanese and Italian models were also in use. With its adoption (in slightly modified form) by the German Navy in 1926 and the German Army and Air Force soon after, the name "Enigma" became widely known in military circles. Pre-war German military planning emphasized fast, mobile forces and tactics, later known as blitzkrieg, which depend on radio communication for command and coordination. Since adversaries would likely intercept radio signals, messages had to be protected with secure encipherment. Compact and easily portable, the Enigma machine filled that need.

Around December 1932 Marian Rejewski, a Polish mathematician and cryptologist at the Polish Cipher Bureau, used the theory of permutations, and flaws in the German military-message encipherment procedures, to break message keys of the plugboard Enigma machine. France's spy Hans-Thilo Schmidt obtained access to German cipher materials that included the daily keys used in September and October 1932. Those keys included the plugboard settings. The French passed the material to the Poles, and Rejewski used some of that material and the message traffic in September and October to solve for the unknown rotor wiring. Consequently the Polish mathematicians were able to build their own Enigma machines, dubbed "Enigma doubles". Rejewski was aided by fellow mathematician-cryptologists Jerzy Różycki and Henryk Zygalski, both of whom had been recruited with Rejewski from Poznań University, which had been selected for its students' knowledge of the German language, since that area was held by Germany prior to World War I. The Polish Cipher Bureau developed techniques to defeat the plugboard and find all components of the daily key, which enabled the Cipher Bureau to read German Enigma messages starting from January 1933.

Over time, the German cryptographic procedures improved, and the Cipher Bureau developed techniques and designed mechanical devices to continue reading Enigma traffic. As part of that effort, the Poles exploited quirks of the rotors, compiled catalogues, built a cyclometer (invented by Rejewski) to help make a catalogue with 100,000 entries, invented and produced Zygalski sheets, and built the electromechanical cryptologic "bomba" (invented by Rejewski) to search for rotor settings. In 1938 the Poles had six "bomby" (plural of "bomba"), but when that year the Germans added two more rotors, ten times as many "bomby" would have been needed to read the traffic.

On 26 and 27 July 1939, in Pyry, just south of Warsaw, the Poles initiated French and British military intelligence representatives into the Polish Enigma-decryption techniques and equipment, including Zygalski sheets and the cryptologic bomb, and promised each delegation a Polish-reconstructed Enigma (the devices were soon delivered).

In September 1939, British Military Mission 4, which included Colin Gubbins and Vera Atkins, went to Poland, intending to evacuate cipher-breakers Marian Rejewski, Jerzy Różycki, and Henryk Zygalski from the country. The cryptologists, however, had been evacuated by their own superiors into Romania, at the time a Polish-allied country. On the way, for security reasons, the Polish Cipher Bureau personnel had deliberately destroyed their records and equipment. From Romania they traveled on to France, where they resumed their cryptological work, collaborating by teletype with the British, who began work on decrypting German Enigma messages, using the Polish equipment and techniques.

Gordon Welchman, who became head of Hut 6 at Bletchley Park, wrote: "Hut 6 Ultra would never have got off the ground if we had not learned from the Poles, in the nick of time, the details both of the German military version of the commercial Enigma machine, and of the operating procedures that were in use." The Polish transfer of theory and technology at Pyry formed the crucial basis for the subsequent World War II British Enigma-decryption effort at Bletchley Park, where Welchman worked.

During the war, British cryptologists decrypted a vast number of messages enciphered on Enigma. The intelligence gleaned from this source, codenamed "Ultra" by the British, was a substantial aid to the Allied war effort.

Though Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed.

The Abwehr used a different version of Enigma machines. In November 1942, during Operation Torch, a machine was captured which had no plugboard and the three rotors had been changed to rotate 11, 15, and 19 times rather than once every 26 letters, plus a plate on the left acted as a fourth rotor. From October 1944, the German Abwehr used the Schlüsselgerät 41. 

The Abwehr code had been broken on 8 December 1941 by Dilly Knox. Agents sent messages to the Abwehr in a simple code which was then sent on using an Enigma machine. The simple codes were broken and helped break the daily Enigma cipher. This breaking of the code enabled the Double-Cross System to operate.

Like other rotor machines, the Enigma machine is a combination of mechanical and electrical subsystems. The mechanical subsystem consists of a keyboard; a set of rotating disks called "rotors" arranged adjacently along a spindle; one of various stepping components to turn at least one rotor with each key press, and a series of lamps, one for each letter. These design features are the reason that the Enigma machine was originally referred to as the rotor-based cipher machine during its intellectual inception in 1915.

An electrical pathway is a route for current to travel. By manipulating this phenomenon the Enigma machine was able to scramble messages. The mechanical parts act by forming a varying electrical circuit. When a key is pressed, one or more rotors rotate on the spindle. On the sides of the rotors are a series of electrical contacts that, after rotation, line up with contacts on the other rotors or fixed wiring on either end of the spindle. When the rotors are properly aligned, each key on the keyboard is connected to a unique electrical pathway through the series of contacts and internal wiring. Current, typically from a battery, flows through the pressed key, into the newly configured set of circuits and back out again, ultimately lighting one display lamp, which shows the output letter. For example, when encrypting a message starting "ANX...", the operator would first press the "A" key, and the "Z" lamp might light, so "Z" would be the first letter of the ciphertext. The operator would next press "N", and then "X" in the same fashion, and so on.
Current flows from the battery (1) through a depressed bi-directional keyboard switch (2) to the plugboard (3). Next, it passes through the (unused in this instance, so shown closed) plug "A" (3) via the entry wheel (4), through the wiring of the three (Wehrmacht Enigma) or four ("Kriegsmarine" M4 and "Abwehr" variants) installed rotors (5), and enters the reflector (6). The reflector returns the current, via an entirely different path, back through the rotors (5) and entry wheel (4), proceeding through plug "S" (7) connected with a cable (8) to plug "D", and another bi-directional switch (9) to light the appropriate lamp.

The repeated changes of electrical path through an Enigma scrambler implement a polyalphabetic substitution cipher that provides Enigma's security. The diagram on the right shows how the electrical pathway changes with each key depression, which causes rotation of at least the right-hand rotor. Current passes into the set of rotors, into and back out of the reflector, and out through the rotors again. The greyed-out lines are other possible paths within each rotor; these are hard-wired from one side of each rotor to the other. The letter "A" encrypts differently with consecutive key presses, first to "G", and then to "C". This is because the right-hand rotor steps (rotates one position) on each key press, sending the signal on a completely different route. Eventually other rotors step with a key press.

The rotors (alternatively "wheels" or "drums", "Walzen" in German) form the heart of an Enigma machine. Each rotor is a disc approximately in diameter made from Ebonite or Bakelite with 26 brass, spring-loaded, electrical contact pins arranged in a circle on one face, with the other face housing 26 corresponding electrical contacts in the form of circular plates. The pins and contacts represent the alphabet — typically the 26 letters A–Z, as will be assumed for the rest of this description. When the rotors are mounted side by side on the spindle, the pins of one rotor rest against the plate contacts of the neighbouring rotor, forming an electrical connection. Inside the body of the rotor, 26 wires connect each pin on one side to a contact on the other in a complex pattern. Most of the rotors are identified by Roman numerals, and each issued copy of rotor I, for instance, is wired identically to all others. The same is true for the special thin beta and gamma rotors used in the M4 naval variant.
By itself, a rotor performs only a very simple type of encryption, a simple substitution cipher. For example, the pin corresponding to the letter "E" might be wired to the contact for letter "T" on the opposite face, and so on. Enigma's security comes from using several rotors in series (usually three or four) and the regular stepping movement of the rotors, thus implementing a polyalphabetic substitution cipher.

Each rotor can be set to one of 26 possible starting positions when placed in an Enigma machine. After insertion, a rotor can be turned to the correct position by hand, using the grooved finger-wheel which protrudes from the internal Enigma cover when closed. In order for the operator to know the rotor's position, each has an "alphabet tyre" (or letter ring) attached to the outside of the rotor disc, with 26 characters (typically letters); one of these is visible through the window for that slot in the cover, thus indicating the rotational position of the rotor. In early models, the alphabet ring was fixed to the rotor disc. A later improvement was the ability to adjust the alphabet ring relative to the rotor disc. The position of the ring was known as the "Ringstellung" ("ring setting"), and that setting was a part of the initial setup needed prior to an operating session. In modern terms it was a part of the initialization vector.
Each rotor contains one or more notches that control rotor stepping. In the military variants, the notches are located on the alphabet ring.

The Army and Air Force Enigmas were used with several rotors, initially three. On 15 December 1938, this changed to five, from which three were chosen for a given session. Rotors were marked with Roman numerals to distinguish them: I, II, III, IV and V, all with single turnover notches located at different points on the alphabet ring. This variation was probably intended as a security measure, but ultimately allowed the Polish Clock Method and British Banburismus attacks.

The Naval version of the "Wehrmacht" Enigma had always been issued with more rotors than the other services: At first six, then seven, and finally eight. The additional rotors were marked VI, VII and VIII, all with different wiring, and had two notches, resulting in more frequent turnover. The four-rotor Naval Enigma (M4) machine accommodated an extra rotor in the same space as the three-rotor version. This was accomplished by replacing the original reflector with a thinner one and by adding a thin fourth rotor. That fourth rotor was one of two types, "Beta" or "Gamma", and never stepped, but could be manually set to any of 26 positions. One of the 26 made the machine perform identically to the three-rotor machine.

To avoid merely implementing a simple (solvable) substitution cipher, every key press caused one or more rotors to step by one twenty-sixth of a full rotation, before the electrical connections were made. This changed the substitution alphabet used for encryption, ensuring that the cryptographic substitution was different at each new rotor position, producing a more formidable polyalphabetic substitution cipher. The stepping mechanism varied slightly from model to model. The right-hand rotor stepped once with each keystroke, and other rotors stepped less frequently.

The advancement of a rotor other than the left-hand one was called a "turnover" by the British. This was achieved by a ratchet and pawl mechanism. Each rotor had a ratchet with 26 teeth and every time a key was pressed, the set of spring-loaded pawls moved forward in unison, trying to engage with a ratchet. The alphabet ring of the rotor to the right normally prevented this. As this ring rotated with its rotor, a notch machined into it would eventually align itself with the pawl, allowing it to engage with the ratchet, and advance the rotor on its left. The right-hand pawl, having no rotor and ring to its right, stepped its rotor with every key depression. For a single-notch rotor in the right-hand position, the middle rotor stepped once for every 26 steps of the right-hand rotor. Similarly for rotors two and three. For a two-notch rotor, the rotor to its left would turn over twice for each rotation.

The first five rotors to be introduced (I–V) contained one notch each, while the additional naval rotors VI, VII and VIII each had two notches. The position of the notch on each rotor was determined by the letter ring which could be adjusted in relation to the core containing the interconnections. The points on the rings at which they caused the next wheel to move were as follows.

The design also included a feature known as "double-stepping". This occurred when each pawl aligned with both the ratchet of its rotor and the rotating notched ring of the neighbouring rotor. If a pawl engaged with a ratchet through alignment with a notch, as it moved forward it pushed against both the ratchet and the notch, advancing both rotors. In a three-rotor machine, double-stepping affected rotor two only. If, in moving forward, the ratchet of rotor three was engaged, rotor two would move again on the subsequent keystroke, resulting in two consecutive steps. Rotor two also pushes rotor one forward after 26 steps, but since rotor one moves forward with every keystroke anyway, there is no double-stepping. This double-stepping caused the rotors to deviate from odometer-style regular motion.
With three wheels and only single notches in the first and second wheels, the machine had a period of 26×25×26 = 16,900 (not 26×26×26, because of double-stepping). Historically, messages were limited to a few hundred letters, and so there was no chance of repeating any combined rotor position during a single session, denying cryptanalysts valuable clues.

To make room for the Naval fourth rotors, the reflector was made much thinner. The fourth rotor fitted into the space made available. No other changes were made, which eased the changeover. Since there were only three pawls, the fourth rotor never stepped, but could be manually set into one of 26 possible positions.

A device that was designed, but not implemented before the war's end, was the "Lückenfüllerwalze" (gap-fill wheel) that implemented irregular stepping. It allowed field configuration of notches in all 26 positions. If the number of notches was a relative prime of 26 and the number of notches were different for each wheel, the stepping would be more unpredictable. Like the Umkehrwalze-D it also allowed the internal wiring to be reconfigured.

The current entry wheel ("Eintrittswalze" in German), or entry stator, connects the plugboard to the rotor assembly. If the plugboard is not present, the entry wheel instead connects the keyboard and lampboard to the rotor assembly. While the exact wiring used is of comparatively little importance to security, it proved an obstacle to Rejewski's progress during his study of the rotor wirings. The commercial Enigma connects the keys in the order of their sequence on a QWERTZ keyboard: "Q"→"A", "W"→"B", "E"→"C" and so on. The military Enigma connects them in straight alphabetical order: "A"→"A", "B"→"B", "C"→"C", and so on. It took inspired guesswork for Rejewski to penetrate the modification.

With the exception of models "A" and "B", the last rotor came before a 'reflector' (German: "Umkehrwalze", meaning 'reversal rotor'), a patented feature unique to Enigma among the period's various rotor machines. The reflector connected outputs of the last rotor in pairs, redirecting current back through the rotors by a different route. The reflector ensured that Enigma would be self-reciprocal; thus, with two identically configured machines, a message could be encrypted on one and decrypted on the other, without the need for a bulky mechanism to switch between encryption and decryption modes. The reflector allowed a more compact design, but it also gave Enigma the property that no letter ever encrypted to itself. This was a severe cryptological flaw that was subsequently exploited by codebreakers.

In Model 'C', the reflector could be inserted in one of two different positions. In Model 'D', the reflector could be set in 26 possible positions, although it did not move during encryption. In the "Abwehr" Enigma, the reflector stepped during encryption in a manner similar to the other wheels.

In the German Army and Air Force Enigma, the reflector was fixed and did not rotate; there were four versions. The original version was marked 'A', and was replaced by "Umkehrwalze B" on 1 November 1937. A third version, "Umkehrwalze C" was used briefly in 1940, possibly by mistake, and was solved by Hut 6. The fourth version, first observed on 2 January 1944, had a rewireable reflector, called "Umkehrwalze D", nick-named Uncle Dick by the British, allowing the Enigma operator to alter the connections as part of the key settings.

The plugboard ("Steckerbrett" in German) permitted variable wiring that could be reconfigured by the operator (visible on the front panel of Figure 1; some of the patch cords can be seen in the lid). It was introduced on German Army versions in 1928, and was soon adopted by the "Reichsmarine" (German Navy). The plugboard contributed more cryptographic strength than an extra rotor, as it had 150 trillion possible settings (see below). Enigma without a plugboard (known as "unsteckered Enigma") could be solved relatively straightforwardly using hand methods; these techniques were generally defeated by the plugboard, driving Allied cryptanalysts to develop special machines to solve it.

A cable placed onto the plugboard connected letters in pairs; for example, "E" and "Q" might be a steckered pair. The effect was to swap those letters before and after the main rotor scrambling unit. For example, when an operator pressed "E", the signal was diverted to "Q" before entering the rotors. Up to 13 steckered pairs might be used at one time, although only 10 were normally used.

Current flowed from the keyboard through the plugboard, and proceeded to the entry-rotor or "Eintrittswalze". Each letter on the plugboard had two jacks. Inserting a plug disconnected the upper jack (from the keyboard) and the lower jack (to the entry-rotor) of that letter. The plug at the other end of the crosswired cable was inserted into another letter's jacks, thus switching the connections of the two letters.

Other features made various Enigma machines more secure or more convenient.

Some M4 Enigmas used the "Schreibmax", a small printer that could print the 26 letters on a narrow paper ribbon. This eliminated the need for a second operator to read the lamps and transcribe the letters. The "Schreibmax" was placed on top of the Enigma machine and was connected to the lamp panel. To install the printer, the lamp cover and light bulbs had to be removed. It improved both convenience and operational security; the printer could be installed remotely such that the signal officer operating the machine no longer had to see the decrypted plaintext.

Another accessory was the remote lamp panel "Fernlesegerät". For machines equipped with the extra panel, the wooden case of the Enigma was wider and could store the extra panel. A lamp panel version could be connected afterwards, but that required, as with the "Schreibmax", that the lamp panel and light bulbs be removed. The remote panel made it possible for a person to read the decrypted plaintext without the operator seeing it.

In 1944, the "Luftwaffe" introduced a plugboard switch, called the "Uhr" (clock), a small box containing a switch with 40 positions. It replaced the standard plugs. After connecting the plugs, as determined in the daily key sheet, the operator turned the switch into one of the 40 positions, each producing a different combination of plug wiring. Most of these plug connections were, unlike the default plugs, not pair-wise. In one switch position, the "Uhr" did not swap letters, but simply emulated the 13 stecker wires with plugs.

The Enigma transformation for each letter can be specified mathematically as a product of permutations. Assuming a three-rotor German Army/Air Force Enigma, let denote the plugboard transformation, denote that of the reflector (formula_1), and , , denote those of the left, middle and right rotors respectively. Then the encryption can be expressed as

After each key press, the rotors turn, changing the transformation. For example, if the right-hand rotor is rotated positions, the transformation becomes

where is the cyclic permutation mapping A to B, B to C, and so forth. Similarly, the middle and left-hand rotors can be represented as and rotations of and . The encryption transformation can then be described as

Combining three rotors from a set of five, each of the 3 rotor settings with 26 positions, and the plugboard with ten pairs of letters connected, the military Enigma has 158,962,555,217,826,360,000 different settings (nearly 159 quintillion or about 67 bits).

A German Enigma operator would be given a plaintext message to encrypt. After setting up his machine, he would type the message on the Enigma keyboard. For each letter pressed, one lamp lit indicating a different letter according to a pseudo-random substitution determined by the electrical pathways inside the machine. The letter indicated by the lamp would be recorded, typically by a second operator, as the cyphertext letter. The action of pressing a key also moved one or more rotors so that the next key press used a different electrical pathway, and thus a different substitution would occur even if the same plaintext letter were entered again. For each key press there was rotation of at least the right hand rotor and less often the other two, resulting in a different substitution alphabet being used for every letter in the message. This process continued until the message was completed. The cyphertext recorded by the second operator would then be transmitted, usually by radio in Morse code, to an operator of another Enigma machine. This operator would type in the cyphertext and — as long as all the settings of the deciphering machine were identical to those of the enciphering machine — for every key press the reverse substitution would occur and the plaintext message would emerge.

In use, the Enigma required a list of daily key settings and auxiliary documents. In German military practice, communications were divided into separate networks, each using different settings. These communication nets were termed "keys" at Bletchley Park, and were assigned code names, such as "Red", "Chaffinch", and "Shark". Each unit operating in a network was given the same settings list for its Enigma, valid for a period of time. The procedures for German Naval Enigma were more elaborate and more secure than those in other services and employed auxiliary codebooks. Navy codebooks were printed in red, water-soluble ink on pink paper so that they could easily be destroyed if they were endangered or if the vessel was sunk.

An Enigma machine's setting (its cryptographic key in modern terms; "Schlüssel" in German) specified each operator-adjustable aspect of the machine:

For a message to be correctly encrypted and decrypted, both sender and receiver had to configure their Enigma in the same way; rotor selection and order, ring positions, plugboard connections and starting rotor positions must be identical. Except for the starting positions, these settings were established beforehand, distributed in key lists and changed daily. For example, the settings for the 18th day of the month in the German Luftwaffe Enigma key list number 649 (see image) were as follows:

Enigma was designed to be secure even if the rotor wiring was known to an opponent, although in practice considerable effort protected the wiring configuration. If the wiring is secret, the total number of possible configurations has been calculated to be around (approximately 380 bits); with known wiring and other operational constraints, this is reduced to around (76 bits). Because of the large number of possibilities, users of Enigma were confident of its security; it was not then feasible for an adversary to even begin to try a brute-force attack.

Most of the key was kept constant for a set time period, typically a day. A different initial rotor position was used for each message, a concept similar to an initialisation vector in modern cryptography. The reason is that encrypting many messages with identical or near-identical settings (termed in cryptanalysis as being "in depth"), would enable an attack using a statistical procedure such as Friedman's Index of coincidence. The starting position for the rotors was transmitted just before the ciphertext, usually after having been enciphered. The exact method used was termed the "indicator procedure". Design weakness and operator sloppiness in these indicator procedures were two of the main weaknesses that made cracking Enigma possible.
One of the earliest "indicator procedures" for the Enigma was cryptographically flawed and allowed Polish cryptanalysts to make the initial breaks into the plugboard Enigma. The procedure had the operator set his machine in accordance with the secret settings that all operators on the net shared. The settings included an initial position for the rotors (the "Grundstellung"), say, "AOH". The operator turned his rotors until "AOH" was visible through the rotor windows. At that point, the operator chose his own arbitrary starting position for the message he would send. An operator might select "EIN", and that became the "message setting" for that encryption session. The operator then typed "EIN" into the machine twice, this producing the encrypted indicator, for example "XHTLOA". This was then transmitted, at which point the operator would turn the rotors to his message settings, "EIN" in this example, and then type the plaintext of the message.

At the receiving end, the operator set the machine to the initial settings ("AOH") and typed in the first six letters of the message ("XHTLOA"). In this example, "EINEIN" emerged on the lamps, so the operator would learn the "message setting" that the sender used to encrypt this message. The receiving operator would set his rotors to "EIN", type in the rest of the ciphertext, and get the deciphered message.

This indicator scheme had two weaknesses. First, the use of a global initial position ("Grundstellung") meant all message keys used the same polyalphabetic substitution. In later indicator procedures, the operator selected his initial position for encrypting the indicator and sent that initial position in the clear. The second problem was the repetition of the indicator, which was a serious security flaw. The message setting was encoded twice, resulting in a relation between first and fourth, second and fifth, and third and sixth character. These security flaws enabled the Polish Cipher Bureau to break into the pre-war Enigma system as early as 1932. The early indicator procedure was subsequently described by German cryptanalysts as the "faulty indicator technique".

During World War II, codebooks were only used each day to set up the rotors, their ring settings and the plugboard. For each message, the operator selected a random start position, let's say "WZA", and a random message key, perhaps "SXT". He moved the rotors to the "WZA" start position and encoded the message key "SXT". Assume the result was "UHL". He then set up the message key, "SXT", as the start position and encrypted the message. Next, he transmitted the start position, "WZA", the encoded message key, "UHL", and then the ciphertext. The receiver set up the start position according to the first trigram, "WZA", and decoded the second trigram, "UHL", to obtain the "SXT" message setting. Next, he used this "SXT" message setting as the start position to decrypt the message. This way, each ground setting was different and the new procedure avoided the security flaw of double encoded message settings.

This procedure was used by "Wehrmacht" and "Luftwaffe" only. The "Kriegsmarine" procedures on sending messages with the Enigma were far more complex and elaborate. Prior to encryption the message was encoded using the "Kurzsignalheft" code book. The "Kurzsignalheft" contained tables to convert sentences into four-letter groups. A great many choices were included, for example, logistic matters such as refuelling and rendezvous with supply ships, positions and grid lists, harbour names, countries, weapons, weather conditions, enemy positions and ships, date and time tables. Another codebook contained the "Kenngruppen" and "Spruchschlüssel": the key identification and message key.

The Army Enigma machine used only the 26 alphabet characters. Punctuation was replaced with rare character combinations. A space was omitted or replaced with an X. The X was generally used as full-stop.

Some punctuation marks were different in other parts of the armed forces. The "Wehrmacht" replaced a comma with ZZ and the question mark with FRAGE or FRAQ.

The "Kriegsmarine" replaced the comma with Y and the question mark with UD. The combination CH, as in ""Acht"" (eight) or ""Richtung"" (direction), was replaced with Q (AQT, RIQTUNG). Two, three and four zeros were replaced with CENTA, MILLE and MYRIA.

The "Wehrmacht" and the "Luftwaffe" transmitted messages in groups of five characters and counted the letters.

The "Kriegsmarine" used four-character groups and counted those groups. 
Frequently used names or words were varied as much as possible. Words like "Minensuchboot" (minesweeper) could be written as MINENSUCHBOOT, MINBOOT or MMMBOOT. To make cryptanalysis harder, messages were limited to 250 characters. Longer messages were divided into several parts, each using a different message key.

The character substitutions by the Enigma machine as a whole can be expressed as a string of letters with each position occupied by the character that will replace the character at the corresponding position in the alphabet. For example, a given machine configuration that enciphered A to L, B to U, C to S, ..., and Z to J could be represented compactly as

and the enciphering of a particular character by that configuration could be represented by highlighting the enciphered character as in

Since the operation of an Enigma machine enciphering a message is a series of such configurations, each associated with a single character being enciphered, a sequence of such representations can be used to represent the operation of the machine as it enciphers a message. For example, the process of enciphering the first sentence of the main body of the famous "Dönitz message" to

can be represented as

where the letters following each mapping are the letters that appear at the windows at that stage (the only state changes visible to the operator) and the numbers show the underlying physical position of each rotor.

The character mappings for a given configuration of the machine are in turn the result of a series of such mappings applied by each pass through a component of the machine: the enciphering of a character resulting from the application of a given component's mapping serves as the input to the mapping of the subsequent component. For example, the 4th step in the enciphering above can be expanded to show each of these stages using the same representation of mappings and highlighting for the enciphered character:
  P EFMQAB(G)UINKXCJORDPZTHWVLYS         AE.BF.CM.DQ.HU.JN.LX.PR.SZ.VW
  1 OFRJVM(A)ZHQNBXPYKCULGSWETDI  N  03  VIII
  2 (N)UKCHVSMDGTZQFYEWPIALOXRJB  U  17  VI
  3 XJMIYVCARQOWH(L)NDSUFKGBEPZT  D  15  V
  4 QUNGALXEPKZ(Y)RDSOFTVCMBIHWJ  C  25  β
  R RDOBJNTKVEHMLFCWZAXGYIPS(U)Q         c
  4 EVTNHQDXWZJFUCPIAMOR(B)SYGLK         β
  3 H(V)GPWSUMDBTNCOKXJIQZRFLAEY         V
  2 TZDIPNJESYCUHAVRMXGKB(F)QWOL         VI
  1 GLQYW(B)TIZDPSFKANJCUXREVMOH         VIII
  P E(F)MQABGUINKXCJORDPZTHWVLYS         AE.BF.CM.DQ.HU.JN.LX.PR.SZ.VW

Here the enciphering begins trivially with the first "mapping" representing the keyboard (which has no effect), followed by the plugboard, configured as AE.BF.CM.DQ.HU.JN.LX.PR.SZ.VW which has no effect on 'G', followed by the VIII rotor in the 03 position, which maps G to A, then the VI rotor in the 17 position, which maps A to N, ..., and finally the plugboard again, which maps B to F, producing the overall mapping indicated at the final step: G to F.

Note that this model has 4 rotors (lines 1 through 4) and that the reflector (line R) also permutes (garbles) letters.

The Enigma family included multiple designs. The earliest were commercial models dating from the early 1920s. Starting in the mid-1920s, the German military began to use Enigma, making a number of security-related changes. Various nations either adopted or adapted the design for their own cipher machines.

An estimated 40,000 Enigma machines were constructed. After the end of World War II, the Allies sold captured Enigma machines, still widely considered secure, to developing countries.

On 23 February 1918, Arthur Scherbius applied for a patent for a ciphering machine that used rotors. Scherbius and E. Richard Ritter founded the firm of Scherbius & Ritter. They approached the German Navy and Foreign Office with their design, but neither agency was interested. Scherbius & Ritter then assigned the patent rights to Gewerkschaft Securitas, who founded the "Chiffriermaschinen Aktien-Gesellschaft" (Cipher Machines Stock Corporation) on 9 July 1923; Scherbius and Ritter were on the board of directors.

Chiffriermaschinen AG began advertising a rotor machine, "Enigma Handelsmaschine", which was exhibited at the Congress of the International Postal Union in 1924. The machine was heavy and bulky, incorporating a typewriter. It measured 65×45×38 cm and weighed about .

This was also a model with a type writer. There were a number of problems associated with the printer and the construction was not stable until 1926. Both early versions of Enigma lacked the reflector and had to be switched between enciphering and deciphering.

The reflector, suggested by Scherbius' colleague Willi Korn, was introduced with the glow lamp version.

The machine was also known as the military Enigma. It had two rotors and a manually rotatable reflector. The typewriter was omitted and glow lamps were used for output. The operation was somewhat different from later models. Before the next key pressure, the operator had to press a button to advance the right rotor one step.

Enigma "model B" was introduced late in 1924, and was of a similar construction. While bearing the Enigma name, both models "A" and "B" were quite unlike later versions: They differed in physical size and shape, but also cryptographically, in that they lacked the reflector. This model of Enigma machine was referred to as the Glowlamp Enigma or "Glühlampenmaschine" since it produced its output on a lamp panel rather than paper. This method of output was much more reliable and cost effective. Hence this machine was 1/8th the price of its predecessor.

"Model C" was the third model of the so-called ″glowlamp Enigmas″ (after A and B) and it again lacked a typewriter.

The "Enigma C" quickly gave way to "Enigma D" (1927). This version was widely used, with shipments to Sweden, the Netherlands, United Kingdom, Japan, Italy, Spain, United States and Poland. In 1927 Hugh Foss at the British Government Code and Cypher School was able to show that commercial Enigma machines could be broken, provided suitable cribs were available. Soon, the Enigma D would pioneer the use of a standard keyboard layout to be used in German computing. This "QWERTZ" layout is very similar to the American QWERTY keyboard format used in many languages.

Other countries used Enigma machines. The Italian Navy adopted the commercial Enigma as "Navy Cipher D". The Spanish also used commercial Enigma machines during their Civil War. British codebreakers succeeded in breaking these machines, which lacked a plugboard. Enigma machines were also used by diplomatic services.

There was also a large, eight-rotor printing model, the "Enigma H", called "Enigma II" by the "Reichswehr". In 1933 the Polish Cipher Bureau detected that it was in use for high-level military communication, but it was soon withdrawn, as it was unreliable and jammed frequently.

The Swiss used a version of Enigma called "Model K" or "Swiss K" for military and diplomatic use, which was very similar to commercial Enigma D. The machine's code was cracked by Poland, France, the United Kingdom and the United States; the latter code-named it INDIGO. An "Enigma T" model, code-named "Tirpitz", was used by Japan.

The various services of the Wehrmacht used various Enigma versions, and replaced them frequently, sometimes with ones adapted from other services. Enigma seldom carried high-level strategic messages, which when not urgent went by courier, and when urgent went by other cryptographic systems including the Geheimschreiber.

The Reichsmarine was the first military branch to adopt Enigma. This version, named "Funkschlüssel C" ("Radio cipher C"), had been put into production by 1925 and was introduced into service in 1926.

The keyboard and lampboard contained 29 letters — A-Z, Ä, Ö and Ü — that were arranged alphabetically, as opposed to the QWERTZUI ordering. The rotors had 28 contacts, with the letter "X" wired to bypass the rotors unencrypted. Three rotors were chosen from a set of five and the reflector could be inserted in one of four different positions, denoted α, β, γ and δ. The machine was revised slightly in July 1933.

By 15 July 1928, the German Army ("Reichswehr") had introduced their own exclusive version of the Enigma machine, the "Enigma G".

The "Abwehr" used the "Enigma G" (the "Abwehr" Enigma). This Enigma variant was a four-wheel unsteckered machine with multiple notches on the rotors. This model was equipped with a counter that incremented upon each key press, and so is also known as the "counter machine" or the "Zählwerk" Enigma.

Enigma machine G was modified to the "Enigma I" by June 1930. Enigma I is also known as the "Wehrmacht", or "Services" Enigma, and was used extensively by German military services and other government organisations (such as the railways) before and during World War II.
The major difference between "Enigma I" (German Army version from 1930), and commercial Enigma models was the addition of a plugboard to swap pairs of letters, greatly increasing cryptographic strength.

Other differences included the use of a fixed reflector and the relocation of the stepping notches from the rotor body to the movable letter rings. The machine measured and weighed around .

In August 1935, the Air Force introduced the Wehrmacht Enigma for their communications.

By 1930, the Reichswehr had suggested that the Navy adopt their machine, citing the benefits of increased security (with the plugboard) and easier interservice communications. The Reichsmarine eventually agreed and in 1934 brought into service the Navy version of the Army Enigma, designated "Funkschlüssel" ' or "M3". While the Army used only three rotors at that time, the Navy specified a choice of three from a possible five.
In December 1938, the Army issued two extra rotors so that the three rotors were chosen from a set of five. In 1938, the Navy added two more rotors, and then another in 1939 to allow a choice of three rotors from a set of eight.

A four-rotor Enigma was introduced by the Navy for U-boat traffic on 1 February 1942, called "M4" (the network was known as "Triton", or "Shark" to the Allies). The extra rotor was fitted in the same space by splitting the reflector into a combination of a thin reflector and a thin fourth rotor.

The effort to break the Enigma was not disclosed until the 1970s. Since then, interest in the Enigma machine has grown. Enigmas are on public display in museums around the world, and several are in the hands of private collectors and computer history enthusiasts.

The "Deutsches Museum" in Munich has both the three- and four-rotor German military variants, as well as several civilian versions. The "Deutsches Spionagemuseum" in Berlin also showcases two military variants. Enigma machines are also exhibited at the National Codes Centre in Bletchley Park, the Government Communications Headquarters, the Science Museum in London, Discovery Park of America in Tennessee, the Polish Army Museum in Warsaw, the Swedish Army Museum ("Armémuseum") in Stockholm, the Military Museum of A Coruña in Spain, the Nordland Red Cross War Memorial Museum in Narvik, Norway, The Artillery, Engineers and Signals Museum in Hämeenlinna, Finland the Technical University of Denmark in Lyngby, Denmark, in Skanderborg Bunkerne at Skanderborg, Denmark, and at the Australian War Memorial and in the foyer of the Australian Signals Directorate, both in Canberra, Australia. The Jozef Pilsudski Institute in London exhibited a rare Polish Enigma double assembled in France in 1940. In 2020, thanks to the support of the Ministry of Culture and National Heritage, it became the property of the Polish History Museum.
In the United States, Enigma machines can be seen at the Computer History Museum in Mountain View, California, and at the National Security Agency's National Cryptologic Museum in Fort Meade, Maryland, where visitors can try their hand at enciphering and deciphering messages. Two machines that were acquired after the capture of during World War II are on display alongside the submarine at the Museum of Science and Industry in Chicago, Illinois. A three-rotor Enigma is on display at Discovery Park of America in Union City, Tennessee. A four-rotor device is on display in the ANZUS Corridor of the Pentagon on the second floor, A ring, between corridors 8 and 9. This machine is on loan from Australia. The United States Air Force Academy in Colorado Springs has a machine on display in the Computer Science Department. There is also a machine located at The National WWII Museum in New Orleans. The International Museum of World War II near Boston has seven Enigma machines on display, including a U-boat four-rotor model, one of three surviving examples of an Enigma machine with a printer, one of fewer than ten surviving ten-rotor code machines, an example blown up by a retreating German Army unit, and two three-rotor Enigmas that visitors can operate to encode and decode messages. Computer Museum of America in Roswell, Georgia has a three-rotor model with two additional rotors. The machine is fully restored and CMoA has the original paperwork for the purchase on 7 March 1936 by the German Army. The National Museum of Computing also contains surviving Enigma machines in Bletchley, England.
In Canada, a Swiss Army issue Enigma-K, is in Calgary, Alberta. It is on permanent display at the Naval Museum of Alberta inside the Military Museums of Calgary. A four-rotor Enigma machine is on display at the Military Communications and Electronics Museum at Canadian Forces Base (CFB) Kingston in Kingston, Ontario.

Occasionally, Enigma machines are sold at auction; prices have in recent years ranged from US$40,000 to US$547,500 in 2017. Replicas are available in various forms, including an exact reconstructed copy of the Naval M4 model, an Enigma implemented in electronics (Enigma-E), various simulators and paper-and-scissors analogues.

A rare "Abwehr" Enigma machine, designated G312, was stolen from the Bletchley Park museum on 1 April 2000. In September, a man identifying himself as "The Master" sent a note demanding £25,000 and threatening to destroy the machine if the ransom was not paid. In early October 2000, Bletchley Park officials announced that they would pay the ransom, but the stated deadline passed with no word from the blackmailer. Shortly afterward, the machine was sent anonymously to BBC journalist Jeremy Paxman, missing three rotors.

In November 2000, an antiques dealer named Dennis Yates was arrested after telephoning "The Sunday Times" to arrange the return of the missing parts. The Enigma machine was returned to Bletchley Park after the incident. In October 2001, Yates was sentenced to ten months in prison and served three months.

In October 2008, the Spanish daily newspaper "El País" reported that 28 Enigma machines had been discovered by chance in an attic of Army headquarters in Madrid. These four-rotor commercial machines had helped Franco's Nationalists win the Spanish Civil War, because, though the British cryptologist Alfred Dilwyn Knox in 1937 broke the cipher generated by Franco's Enigma machines, this was not disclosed to the Republicans, who failed to break the cipher. The Nationalist government continued using its 50 Enigmas into the 1950s. Some machines have gone on display in Spanish military museums, including one at the National Museum of Science and Technology (MUNCYT) in La Coruña and one at the Spanish Army Museum. Two have been given to Britain's GCHQ.

The Bulgarian military used Enigma machines with a Cyrillic keyboard; one is on display in the National Museum of Military History in Sofia.

On 3 December 2020, German divers working on behalf of the World Wide Fund for Nature discovered a destroyed Enigma machine in Flensburg Firth (part of the Baltic Sea) which is believed to be from a scuttled U-boat. This Enigma machine will be restored by and be the property of the Archaeology Museum of Schleswig Holstein.

An M4 Enigma was salvaged in the 1980s from the German minesweeper R15, which was sunk off the Istrian coast in 1945. The machine was put on display in the Pivka Park of Military History in Slovenia on 13 April 2023.

The Enigma was influential in the field of cipher machine design, spinning off other rotor machines. Once the British discovered Enigma's principle of operation, they created the Typex rotor cipher, which the Germans believed to be unsolvable. Typex was originally derived from the Enigma patents; Typex even includes features from the patent descriptions that were omitted from the actual Enigma machine. The British paid no royalties for the use of the patents. In the United States, cryptologist William Friedman designed the M-325 machine, starting in 1936, that is logically similar.

Machines like the SIGABA, NEMA, Typex, and so forth, are not considered to be Enigma derivatives as their internal ciphering functions are not mathematically identical to the Enigma transform.

A unique rotor machine called Cryptograph was constructed in 2002 by Netherlands-based Tatjana van Vark. This device makes use of 40-point rotors, allowing letters, numbers and some punctuation to be used; each rotor contains 509 parts.



Enzyme

Enzymes () are proteins that act as biological catalysts by accelerating chemical reactions. The molecules upon which enzymes may act are called substrates, and the enzyme converts the substrates into different molecules known as products. Almost all metabolic processes in the cell need enzyme catalysis in order to occur at rates fast enough to sustain life. Metabolic pathways depend upon enzymes to catalyze individual steps. The study of enzymes is called "enzymology" and the field of pseudoenzyme analysis recognizes that during evolution, some enzymes have lost the ability to carry out biological catalysis, which is often reflected in their amino acid sequences and unusual 'pseudocatalytic' properties.

Enzymes are known to catalyze more than 5,000 biochemical reaction types. Other biocatalysts are catalytic RNA molecules, called ribozymes. An enzyme's specificity comes from its unique three-dimensional structure.

Like all catalysts, enzymes increase the reaction rate by lowering its activation energy. Some enzymes can make their conversion of substrate to product occur many millions of times faster. An extreme example is orotidine 5'-phosphate decarboxylase, which allows a reaction that would otherwise take millions of years to occur in milliseconds. Chemically, enzymes are like any catalyst and are not consumed in chemical reactions, nor do they alter the equilibrium of a reaction. Enzymes differ from most other catalysts by being much more specific. Enzyme activity can be affected by other molecules: inhibitors are molecules that decrease enzyme activity, and activators are molecules that increase activity. Many therapeutic drugs and poisons are enzyme inhibitors. An enzyme's activity decreases markedly outside its optimal temperature and pH, and many enzymes are (permanently) denatured when exposed to excessive heat, losing their structure and catalytic properties.

Some enzymes are used commercially, for example, in the synthesis of antibiotics. Some household products use enzymes to speed up chemical reactions: enzymes in biological washing powders break down protein, starch or fat stains on clothes, and enzymes in meat tenderizer break down proteins into smaller molecules, making the meat easier to chew.
By the late 17th and early 18th centuries, the digestion of meat by stomach secretions and the conversion of starch to sugars by plant extracts and saliva were known but the mechanisms by which these occurred had not been identified.

French chemist Anselme Payen was the first to discover an enzyme, diastase, in 1833. A few decades later, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur concluded that this fermentation was caused by a vital force contained within the yeast cells called "ferments", which were thought to function only within living organisms. He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells."

In 1877, German physiologist Wilhelm Kühne (1837–1900) first used the term "enzyme", which comes , to describe this process. The word "enzyme" was used later to refer to nonliving substances such as pepsin, and the word "ferment" was used to refer to chemical activity produced by living organisms.

Eduard Buchner submitted his first paper on the study of yeast extracts in 1897. In a series of experiments at the University of Berlin, he found that sugar was fermented by yeast extracts even when there were no living yeast cells in the mixture. He named the enzyme that brought about the fermentation of sucrose "zymase". In 1907, he received the Nobel Prize in Chemistry for "his discovery of cell-free fermentation". Following Buchner's example, enzymes are usually named according to the reaction they carry out: the suffix "-ase" is combined with the name of the substrate (e.g., lactase is the enzyme that cleaves lactose) or to the type of reaction (e.g., DNA polymerase forms DNA polymers).

The biochemical identity of enzymes was still unknown in the early 1900s. Many scientists observed that enzymatic activity was associated with proteins, but others (such as Nobel laureate Richard Willstätter) argued that proteins were merely carriers for the true enzymes and that proteins "per se" were incapable of catalysis. In 1926, James B. Sumner showed that the enzyme urease was a pure protein and crystallized it; he did likewise for the enzyme catalase in 1937. The conclusion that pure proteins can be enzymes was definitively demonstrated by John Howard Northrop and Wendell Meredith Stanley, who worked on the digestive enzymes pepsin (1930), trypsin and chymotrypsin. These three scientists were awarded the 1946 Nobel Prize in Chemistry.

The discovery that enzymes could be crystallized eventually allowed their structures to be solved by x-ray crystallography. This was first done for lysozyme, an enzyme found in tears, saliva and egg whites that digests the coating of some bacteria; the structure was solved by a group led by David Chilton Phillips and published in 1965. This high-resolution structure of lysozyme marked the beginning of the field of structural biology and the effort to understand how enzymes work at an atomic level of detail.

Enzymes can be classified by two main criteria: either amino acid sequence similarity (and thus evolutionary relationship) or enzymatic activity.

Enzyme activity. An enzyme's name is often derived from its substrate or the chemical reaction it catalyzes, with the word ending in "-ase". Examples are lactase, alcohol dehydrogenase and DNA polymerase. Different enzymes that catalyze the same chemical reaction are called isozymes.

The International Union of Biochemistry and Molecular Biology have developed a nomenclature for enzymes, the EC numbers (for "Enzyme Commission"). Each enzyme is described by "EC" followed by a sequence of four numbers which represent the hierarchy of enzymatic activity (from very general to very specific). That is, the first number broadly classifies the enzyme based on its mechanism while the other digits add more and more specificity.

The top-level classification is:

These sections are subdivided by other features such as the substrate, products, and chemical mechanism. An enzyme is fully specified by four numerical designations. For example, hexokinase (EC 2.7.1.1) is a transferase (EC 2) that adds a phosphate group (EC 2.7) to a hexose sugar, a molecule containing an alcohol group (EC 2.7.1).

Sequence similarity. EC categories do not reflect sequence similarity. For instance, two ligases of the same EC number that catalyze exactly the same reaction can have completely different sequences. Independent of their function, enzymes, like any other proteins, have been classified by their sequence similarity into numerous families. These families have been documented in dozens of different protein and protein family databases such as Pfam.

Non-homologous isofunctional enzymes. Unrelated enzymes that have the same enzymatic activity have been called "non-homologous isofunctional enzymes". Horizontal gene transfer may spread these genes to unrelated species, especially bacteria where they can replace endogenous genes of the same function, leading to hon-homologous gene displacement.

Enzymes are generally globular proteins, acting alone or in larger complexes. The sequence of the amino acids specifies the structure which in turn determines the catalytic activity of the enzyme. Although structure determines function, a novel enzymatic activity cannot yet be predicted from structure alone. Enzyme structures unfold (denature) when heated or exposed to chemical denaturants and this disruption to the structure typically causes a loss of activity. Enzyme denaturation is normally linked to temperatures above a species' normal level; as a result, enzymes from bacteria living in volcanic environments such as hot springs are prized by industrial users for their ability to function at high temperatures, allowing enzyme-catalysed reactions to be operated at a very high rate.

Enzymes are usually much larger than their substrates. Sizes range from just 62 amino acid residues, for the monomer of 4-oxalocrotonate tautomerase, to over 2,500 residues in the animal fatty acid synthase. Only a small portion of their structure (around 2–4 amino acids) is directly involved in catalysis: the catalytic site. This catalytic site is located next to one or more binding sites where residues orient the substrates. The catalytic site and binding site together compose the enzyme's active site. The remaining majority of the enzyme structure serves to maintain the precise orientation and dynamics of the active site.

In some enzymes, no amino acids are directly involved in catalysis; instead, the enzyme contains sites to bind and orient catalytic cofactors. Enzyme structures may also contain allosteric sites where the binding of a small molecule causes a conformational change that increases or decreases activity.

A small number of RNA-based biological catalysts called ribozymes exist, which again can act alone or in complex with proteins. The most common of these is the ribosome which is a complex of protein and catalytic RNA components.

Enzymes must bind their substrates before they can catalyse any chemical reaction. Enzymes are usually very specific as to what substrates they bind and then the chemical reaction catalysed. Specificity is achieved by binding pockets with complementary shape, charge and hydrophilic/hydrophobic characteristics to the substrates. Enzymes can therefore distinguish between very similar substrate molecules to be chemoselective, regioselective and stereospecific.

Some of the enzymes showing the highest specificity and accuracy are involved in the copying and expression of the genome. Some of these enzymes have "proof-reading" mechanisms. Here, an enzyme such as DNA polymerase catalyzes a reaction in a first step and then checks that the product is correct in a second step. This two-step process results in average error rates of less than 1 error in 100 million reactions in high-fidelity mammalian polymerases. Similar proofreading mechanisms are also found in RNA polymerase, aminoacyl tRNA synthetases and ribosomes.

Conversely, some enzymes display enzyme promiscuity, having broad specificity and acting on a range of different physiologically relevant substrates. Many enzymes possess small side activities which arose fortuitously (i.e. neutrally), which may be the starting point for the evolutionary selection of a new function.

To explain the observed specificity of enzymes, in 1894 Emil Fischer proposed that both the enzyme and the substrate possess specific complementary geometric shapes that fit exactly into one another. This is often referred to as "the lock and key" model. This early model explains enzyme specificity, but fails to explain the stabilization of the transition state that enzymes achieve.

In 1958, Daniel Koshland suggested a modification to the lock and key model: since enzymes are rather flexible structures, the active site is continuously reshaped by interactions with the substrate as the substrate interacts with the enzyme. As a result, the substrate does not simply bind to a rigid active site; the amino acid side-chains that make up the active site are molded into the precise positions that enable the enzyme to perform its catalytic function. In some cases, such as glycosidases, the substrate molecule also changes shape slightly as it enters the active site. The active site continues to change until the substrate is completely bound, at which point the final shape and charge distribution is determined.
Induced fit may enhance the fidelity of molecular recognition in the presence of competition and noise via the conformational proofreading mechanism.

Enzymes can accelerate reactions in several ways, all of which lower the activation energy (ΔG, Gibbs free energy)
Enzymes may use several of these mechanisms simultaneously. For example, proteases such as trypsin perform covalent catalysis using a catalytic triad, stabilise charge build-up on the transition states using an oxyanion hole, complete hydrolysis using an oriented water substrate.

Enzymes are not rigid, static structures; instead they have complex internal dynamic motions – that is, movements of parts of the enzyme's structure such as individual amino acid residues, groups of residues forming a protein loop or unit of secondary structure, or even an entire protein domain. These motions give rise to a conformational ensemble of slightly different structures that interconvert with one another at equilibrium. Different states within this ensemble may be associated with different aspects of an enzyme's function. For example, different conformations of the enzyme dihydrofolate reductase are associated with the substrate binding, catalysis, cofactor release, and product release steps of the catalytic cycle, consistent with catalytic resonance theory.

Substrate presentation is a process where the enzyme is sequestered away from its substrate. Enzymes can be sequestered to the plasma membrane away from a substrate in the nucleus or cytosol. Or within the membrane, an enzyme can be sequestered into lipid rafts away from its substrate in the disordered region. When the enzyme is released it mixes with its substrate. Alternatively, the enzyme can be sequestered near its substrate to activate the enzyme. For example, the enzyme can be soluble and upon activation bind to a lipid in the plasma membrane and then act upon molecules in the plasma membrane.

Allosteric sites are pockets on the enzyme, distinct from the active site, that bind to molecules in the cellular environment. These molecules then cause a change in the conformation or dynamics of the enzyme that is transduced to the active site and thus affects the reaction rate of the enzyme. In this way, allosteric interactions can either inhibit or activate enzymes. Allosteric interactions with metabolites upstream or downstream in an enzyme's metabolic pathway cause feedback regulation, altering the activity of the enzyme according to the flux through the rest of the pathway.

Some enzymes do not need additional components to show full activity. Others require non-protein molecules called cofactors to be bound for activity. Cofactors can be either inorganic (e.g., metal ions and iron–sulfur clusters) or organic compounds (e.g., flavin and heme). These cofactors serve many purposes; for instance, metal ions can help in stabilizing nucleophilic species within the active site. Organic cofactors can be either coenzymes, which are released from the enzyme's active site during the reaction, or prosthetic groups, which are tightly bound to an enzyme. Organic prosthetic groups can be covalently bound (e.g., biotin in enzymes such as pyruvate carboxylase).

An example of an enzyme that contains a cofactor is carbonic anhydrase, which uses a zinc cofactor bound as part of its active site. These tightly bound ions or molecules are usually found in the active site and are involved in catalysis. For example, flavin and heme cofactors are often involved in redox reactions.

Enzymes that require a cofactor but do not have one bound are called "apoenzymes" or "apoproteins". An enzyme together with the cofactor(s) required for activity is called a "holoenzyme" (or haloenzyme). The term "holoenzyme" can also be applied to enzymes that contain multiple protein subunits, such as the DNA polymerases; here the holoenzyme is the complete complex containing all the subunits needed for activity.

Coenzymes are small organic molecules that can be loosely or tightly bound to an enzyme. Coenzymes transport chemical groups from one enzyme to another. Examples include NADH, NADPH and adenosine triphosphate (ATP). Some coenzymes, such as flavin mononucleotide (FMN), flavin adenine dinucleotide (FAD), thiamine pyrophosphate (TPP), and tetrahydrofolate (THF), are derived from vitamins. These coenzymes cannot be synthesized by the body "de novo" and closely related compounds (vitamins) must be acquired from the diet. The chemical groups carried include: 
Since coenzymes are chemically changed as a consequence of enzyme action, it is useful to consider coenzymes to be a special class of substrates, or second substrates, which are common to many different enzymes. For example, about 1000 enzymes are known to use the coenzyme NADH.

Coenzymes are usually continuously regenerated and their concentrations maintained at a steady level inside the cell. For example, NADPH is regenerated through the pentose phosphate pathway and "S"-adenosylmethionine by methionine adenosyltransferase. This continuous regeneration means that small amounts of coenzymes can be used very intensively. For example, the human body turns over its own weight in ATP each day.

As with all catalysts, enzymes do not alter the position of the chemical equilibrium of the reaction. In the presence of an enzyme, the reaction runs in the same direction as it would without the enzyme, just more quickly. For example, carbonic anhydrase catalyzes its reaction in either direction depending on the concentration of its reactants:

The rate of a reaction is dependent on the activation energy needed to form the transition state which then decays into products. Enzymes increase reaction rates by lowering the energy of the transition state. First, binding forms a low energy enzyme-substrate complex (ES). Second, the enzyme stabilises the transition state such that it requires less energy to achieve compared to the uncatalyzed reaction (ES). Finally the enzyme-product complex (EP) dissociates to release the products.

Enzymes can couple two or more reactions, so that a thermodynamically favorable reaction can be used to "drive" a thermodynamically unfavourable one so that the combined energy of the products is lower than the substrates. For example, the hydrolysis of ATP is often used to drive other chemical reactions.

Enzyme kinetics is the investigation of how enzymes bind substrates and turn them into products. The rate data used in kinetic analyses are commonly obtained from enzyme assays. In 1913 Leonor Michaelis and Maud Leonora Menten proposed a quantitative theory of enzyme kinetics, which is referred to as Michaelis–Menten kinetics. The major contribution of Michaelis and Menten was to think of enzyme reactions in two stages. In the first, the substrate binds reversibly to the enzyme, forming the enzyme-substrate complex. This is sometimes called the Michaelis–Menten complex in their honor. The enzyme then catalyzes the chemical step in the reaction and releases the product. This work was further developed by G. E. Briggs and J. B. S. Haldane, who derived kinetic equations that are still widely used today.

Enzyme rates depend on solution conditions and substrate concentration. To find the maximum speed of an enzymatic reaction, the substrate concentration is increased until a constant rate of product formation is seen. This is shown in the saturation curve on the right. Saturation happens because, as substrate concentration increases, more and more of the free enzyme is converted into the substrate-bound ES complex. At the maximum reaction rate ("V") of the enzyme, all the enzyme active sites are bound to substrate, and the amount of ES complex is the same as the total amount of enzyme.

"V" is only one of several important kinetic parameters. The amount of substrate needed to achieve a given rate of reaction is also important. This is given by the Michaelis–Menten constant ("K"), which is the substrate concentration required for an enzyme to reach one-half its maximum reaction rate; generally, each enzyme has a characteristic "K" for a given substrate. Another useful constant is "k", also called the "turnover number", which is the number of substrate molecules handled by one active site per second.

The efficiency of an enzyme can be expressed in terms of "k"/"K". This is also called the specificity constant and incorporates the rate constants for all steps in the reaction up to and including the first irreversible step. Because the specificity constant reflects both affinity and catalytic ability, it is useful for comparing different enzymes against each other, or the same enzyme with different substrates. The theoretical maximum for the specificity constant is called the diffusion limit and is about 10 to 10 (M s). At this point every collision of the enzyme with its substrate will result in catalysis, and the rate of product formation is not limited by the reaction rate but by the diffusion rate. Enzymes with this property are called "catalytically perfect" or "kinetically perfect". Example of such enzymes are triose-phosphate isomerase, carbonic anhydrase, acetylcholinesterase, catalase, fumarase, β-lactamase, and superoxide dismutase. The turnover of such enzymes can reach several million reactions per second. But most enzymes are far from perfect: the average values of formula_1 and formula_2 are about formula_3 and formula_4, respectively.

Michaelis–Menten kinetics relies on the law of mass action, which is derived from the assumptions of free diffusion and thermodynamically driven random collision. Many biochemical or cellular processes deviate significantly from these conditions, because of macromolecular crowding and constrained molecular movement. More recent, complex extensions of the model attempt to correct for these effects.

Enzyme reaction rates can be decreased by various types of enzyme inhibitors.

A competitive inhibitor and substrate cannot bind to the enzyme at the same time. Often competitive inhibitors strongly resemble the real substrate of the enzyme. For example, the drug methotrexate is a competitive inhibitor of the enzyme dihydrofolate reductase, which catalyzes the reduction of dihydrofolate to tetrahydrofolate. The similarity between the structures of dihydrofolate and this drug are shown in the accompanying figure. This type of inhibition can be overcome with high substrate concentration. In some cases, the inhibitor can bind to a site other than the binding-site of the usual substrate and exert an allosteric effect to change the shape of the usual binding-site.

A non-competitive inhibitor binds to a site other than where the substrate binds. The substrate still binds with its usual affinity and hence K remains the same. However the inhibitor reduces the catalytic efficiency of the enzyme so that V is reduced. In contrast to competitive inhibition, non-competitive inhibition cannot be overcome with high substrate concentration.

An uncompetitive inhibitor cannot bind to the free enzyme, only to the enzyme-substrate complex; hence, these types of inhibitors are most effective at high substrate concentration. In the presence of the inhibitor, the enzyme-substrate complex is inactive. This type of inhibition is rare.

A mixed inhibitor binds to an allosteric site and the binding of the substrate and the inhibitor affect each other. The enzyme's function is reduced but not eliminated when bound to the inhibitor. This type of inhibitor does not follow the Michaelis–Menten equation.

An irreversible inhibitor permanently inactivates the enzyme, usually by forming a covalent bond to the protein. Penicillin and aspirin are common drugs that act in this manner.

In many organisms, inhibitors may act as part of a feedback mechanism. If an enzyme produces too much of one substance in the organism, that substance may act as an inhibitor for the enzyme at the beginning of the pathway that produces it, causing production of the substance to slow down or stop when there is sufficient amount. This is a form of negative feedback. Major metabolic pathways such as the citric acid cycle make use of this mechanism.

Since inhibitors modulate the function of enzymes they are often used as drugs. Many such drugs are reversible competitive inhibitors that resemble the enzyme's native substrate, similar to methotrexate above; other well-known examples include statins used to treat high cholesterol, and protease inhibitors used to treat retroviral infections such as HIV. A common example of an irreversible inhibitor that is used as a drug is aspirin, which inhibits the COX-1 and COX-2 enzymes that produce the inflammation messenger prostaglandin. Other enzyme inhibitors are poisons. For example, the poison cyanide is an irreversible enzyme inhibitor that combines with the copper and iron in the active site of the enzyme cytochrome c oxidase and blocks cellular respiration.

As enzymes are made up of proteins, their actions are sensitive to change in many physio chemical factors such as pH, temperature, substrate concentration, etc.

The following table shows pH optima for various enzymes.

Enzymes serve a wide variety of functions inside living organisms. They are indispensable for signal transduction and cell regulation, often via kinases and phosphatases. They also generate movement, with myosin hydrolyzing adenosine triphosphate (ATP) to generate muscle contraction, and also transport cargo around the cell as part of the cytoskeleton. Other ATPases in the cell membrane are ion pumps involved in active transport. Enzymes are also involved in more exotic functions, such as luciferase generating light in fireflies. Viruses can also contain enzymes for infecting cells, such as the HIV integrase and reverse transcriptase, or for viral release from cells, like the influenza virus neuraminidase.

An important function of enzymes is in the digestive systems of animals. Enzymes such as amylases and proteases break down large molecules (starch or proteins, respectively) into smaller ones, so they can be absorbed by the intestines. Starch molecules, for example, are too large to be absorbed from the intestine, but enzymes hydrolyze the starch chains into smaller molecules such as maltose and eventually glucose, which can then be absorbed. Different enzymes digest different food substances. In ruminants, which have herbivorous diets, microorganisms in the gut produce another enzyme, cellulase, to break down the cellulose cell walls of plant fiber.

Several enzymes can work together in a specific order, creating metabolic pathways. In a metabolic pathway, one enzyme takes the product of another enzyme as a substrate. After the catalytic reaction, the product is then passed on to another enzyme. Sometimes more than one enzyme can catalyze the same reaction in parallel; this can allow more complex regulation: with, for example, a low constant activity provided by one enzyme but an inducible high activity from a second enzyme.

Enzymes determine what steps occur in these pathways. Without enzymes, metabolism would neither progress through the same steps and could not be regulated to serve the needs of the cell. Most central metabolic pathways are regulated at a few key steps, typically through enzymes whose activity involves the hydrolysis of ATP. Because this reaction releases so much energy, other reactions that are thermodynamically unfavorable can be coupled to ATP hydrolysis, driving the overall series of linked metabolic reactions.

There are five main ways that enzyme activity is controlled in the cell.

Enzymes can be either activated or inhibited by other molecules. For example, the end product(s) of a metabolic pathway are often inhibitors for one of the first enzymes of the pathway (usually the first irreversible step, called committed step), thus regulating the amount of end product made by the pathways. Such a regulatory mechanism is called a negative feedback mechanism, because the amount of the end product produced is regulated by its own concentration. Negative feedback mechanism can effectively adjust the rate of synthesis of intermediate metabolites according to the demands of the cells. This helps with effective allocations of materials and energy economy, and it prevents the excess manufacture of end products. Like other homeostatic devices, the control of enzymatic action helps to maintain a stable internal environment in living organisms.

Examples of post-translational modification include phosphorylation, myristoylation and glycosylation. For example, in the response to insulin, the phosphorylation of multiple enzymes, including glycogen synthase, helps control the synthesis or degradation of glycogen and allows the cell to respond to changes in blood sugar. Another example of post-translational modification is the cleavage of the polypeptide chain. Chymotrypsin, a digestive protease, is produced in inactive form as chymotrypsinogen in the pancreas and transported in this form to the stomach where it is activated. This stops the enzyme from digesting the pancreas or other tissues before it enters the gut. This type of inactive precursor to an enzyme is known as a zymogen or proenzyme.

Enzyme production (transcription and translation of enzyme genes) can be enhanced or diminished by a cell in response to changes in the cell's environment. This form of gene regulation is called enzyme induction. For example, bacteria may become resistant to antibiotics such as penicillin because enzymes called beta-lactamases are induced that hydrolyse the crucial beta-lactam ring within the penicillin molecule. Another example comes from enzymes in the liver called cytochrome P450 oxidases, which are important in drug metabolism. Induction or inhibition of these enzymes can cause drug interactions. Enzyme levels can also be regulated by changing the rate of enzyme degradation. The opposite of enzyme induction is enzyme repression.

Enzymes can be compartmentalized, with different metabolic pathways occurring in different cellular compartments. For example, fatty acids are synthesized by one set of enzymes in the cytosol, endoplasmic reticulum and Golgi and used by a different set of enzymes as a source of energy in the mitochondrion, through β-oxidation. In addition, trafficking of the enzyme to different compartments may change the degree of protonation (e.g., the neutral cytoplasm and the acidic lysosome) or oxidative state (e.g., oxidizing periplasm or reducing cytoplasm) which in turn affects enzyme activity. In contrast to partitioning into membrane bound organelles, enzyme subcellular localisation may also be altered through polymerisation of enzymes into macromolecular cytoplasmic filaments.

In multicellular eukaryotes, cells in different organs and tissues have different patterns of gene expression and therefore have different sets of enzymes (known as isozymes) available for metabolic reactions. This provides a mechanism for regulating the overall metabolism of the organism. For example, hexokinase, the first enzyme in the glycolysis pathway, has a specialized form called glucokinase expressed in the liver and pancreas that has a lower affinity for glucose yet is more sensitive to glucose concentration. This enzyme is involved in sensing blood sugar and regulating insulin production.

Since the tight control of enzyme activity is essential for homeostasis, any malfunction (mutation, overproduction, underproduction or deletion) of a single critical enzyme can lead to a genetic disease. The malfunction of just one type of enzyme out of the thousands of types present in the human body can be fatal. An example of a fatal genetic disease due to enzyme insufficiency is Tay–Sachs disease, in which patients lack the enzyme hexosaminidase.

One example of enzyme deficiency is the most common type of phenylketonuria. Many different single amino acid mutations in the enzyme phenylalanine hydroxylase, which catalyzes the first step in the degradation of phenylalanine, result in build-up of phenylalanine and related products. Some mutations are in the active site, directly disrupting binding and catalysis, but many are far from the active site and reduce activity by destabilising the protein structure, or affecting correct oligomerisation. This can lead to intellectual disability if the disease is untreated. Another example is pseudocholinesterase deficiency, in which the body's ability to break down choline ester drugs is impaired.
Oral administration of enzymes can be used to treat some functional enzyme deficiencies, such as pancreatic insufficiency and lactose intolerance.

Another way enzyme malfunctions can cause disease comes from germline mutations in genes coding for DNA repair enzymes. Defects in these enzymes cause cancer because cells are less able to repair mutations in their genomes. This causes a slow accumulation of mutations and results in the development of cancers. An example of such a hereditary cancer syndrome is xeroderma pigmentosum, which causes the development of skin cancers in response to even minimal exposure to ultraviolet light.

Similar to any other protein, enzymes change over time through mutations and sequence divergence. Given their central role in metabolism, enzyme evolution plays a critical role in adaptation. A key question is therefore whether and how enzymes can change their enzymatic activities alongside. It is generally accepted that many new enzyme activities have evolved through gene duplication and mutation of the duplicate copies although evolution can also happen without duplication. One example of an enzyme that has changed its activity is the ancestor of methionyl aminopeptidase (MAP) and creatine amidinohydrolase (creatinase) which are clearly homologous but catalyze very different reactions (MAP removes the amino-terminal methionine in new proteins while creatinase hydrolyses creatine to sarcosine and urea). In addition, MAP is metal-ion dependent while creatinase is not, hence this property was also lost over time. Small changes of enzymatic activity are extremely common among enzymes. In particular, substrate binding specificity (see above) can easily and quickly change with single amino acid changes in their substrate binding pockets. This is frequently seen in the main enzyme classes such as kinases.

Artificial (in vitro) evolution is now commonly used to modify enzyme activity or specificity for industrial applications (see below).

Enzymes are used in the chemical industry and other industrial applications when extremely specific catalysts are required. Enzymes in general are limited in the number of reactions they have evolved to catalyze and also by their lack of stability in organic solvents and at high temperatures. As a consequence, protein engineering is an active area of research and involves attempts to create new enzymes with novel properties, either through rational design or "in vitro" evolution. These efforts have begun to be successful, and a few enzymes have now been designed "from scratch" to catalyze reactions that do not occur in nature.







Ethics

Ethics or moral philosophy is the philosophical study of moral phenomena. It investigates normative questions about what people ought to do or which behavior is morally right. It is usually divided into three major fields: normative ethics, applied ethics, and metaethics.

Normative ethics tries to discover and justify universal principles that govern how people should act in any situation. According to consequentialists, an act is right if it leads to the best consequences. Deontologists hold that morality consists in fulfilling duties, like telling the truth and keeping promises. Virtue theorists see the manifestation of virtues, like courage and compassion, as the fundamental principle of morality. Applied ethics examines concrete ethical problems in real-life situations, for example, by exploring the moral implications of the universal principles discovered in normative ethics within a specific domain. Bioethics studies moral issues associated with living organisms including humans, animals, and plants. Business ethics investigates how ethical principles apply to corporations, while professional ethics focuses on what is morally required of members of different professions. Metaethics is a metatheory that examines the underlying assumptions and concepts of ethics. It asks whether moral facts have mind-independent existence, whether moral statements can be true, how it is possible to acquire moral knowledge, and how moral judgments motivate people.

Ethics is closely connected to value theory, which studies what value is and what types of value there are. Two related empirical fields are moral psychology, which investigates psychological moral processes, and descriptive ethics, which provides value-neutral descriptions of the dominant moral codes and beliefs in different societies.

The history of ethics started in the ancient period with the development of ethical principles and theories in ancient Egypt, India, China, and Greece. During the medieval period, ethical thought was strongly influenced by religious teachings. In the modern period, this focus shifted to a more secular approach concerned with moral experience, practical reason, and the consequences of actions. An influential development in the 20th century was the emergence of metaethics.

Ethics, also referred to as moral philosophy, is the study of moral phenomena. It is one of the main branches of philosophy and investigates the nature of morality and the principles that govern the moral evaluation of conduct, character traits, and institutions. It examines what obligations people have, what behavior is right and wrong, and how to lead a good life. Some of its key questions are "How should one live?" and "What gives meaning to life?".

The domain of morality is a normative field governing what people ought to do rather than what they actually do, what they want to do, or what social conventions require. As a rational and systematic field of inquiry, ethics studies practical reasons why people should act one way rather than another. Most ethical theories seek universal principles that express a general standpoint of what is objectively right and wrong. In a slightly different sense, the term "ethics" can also refer to individual ethical theories in the form of a rational system of moral principles, such as Aristotelian ethics, and to a moral code that certain societies, social groups, or professions follow, as in Protestant work ethic and medical ethics.

The terms "ethics" and "morality" are usually used interchangeably but some philosophers draw a distinction between the two. According to one view, morality is restricted to the question of what moral obligations people have while ethics is a wider term that takes additional considerations into account, such as what is good or how to lead a meaningful life. Another difference is that codes of conduct pertaining to specific areas, such as the business and environment, are usually termed "ethics" rather than morality, as in business ethics and environmental ethics.

As a philosophical discipline, ethics is usually divided into normative ethics, applied ethics, and metaethics. Normative ethics tries to find and justify universal principles of moral conduct. Applied ethics examines the consequences of those principles in specific domains of practical life. Metaethics is a metatheory that studies underlying assumptions and concepts, such as what the nature of morality is and whether moral judgments can be objectively true.

The English word "ethics" has its roots in the Ancient Greek word () meaning "character, personal disposition". This word gave rise to the Ancient Greek word (), which was translated into Latin as and entered the English language in the 15th century through the Old French term .

Normative ethics is the philosophical study of ethical conduct and investigates the fundamental principles of morality. It asks questions like "How should one live?" and "How should people act?". Its main goal is to discover and justify general answers to these questions. To do so, it usually seeks universal or domain-independent principles that determine whether an act is right or wrong. For example, given the particular impression that it is wrong to set a child on fire for fun, normative ethics aims to find more general principles that explain why this is the case, like the principle that one should not cause extreme suffering to the innocent, which may itself be explained in terms of a more general principle. Many theories of normative ethics try not only to provide principles to assess the moral value of actions but aim additionally to guide behavior by helping people make moral decisions.

Theories in normative ethics state how people should act or what kind of behavior is correct. They do not aim to describe how people normally act, what moral beliefs ordinary people have, how these beliefs change over time, or what ethical codes are upheld in certain social groups. These topics belong to descriptive ethics and are studied in fields like anthropology, sociology, and history rather than normative ethics. Another contrast is with applied ethics, which investigates right moral conduct within a specific domain rather than general moral principles studied by normative ethics.

Some systems of normative ethics arrive at a single principle that covers all possible cases while others encompass a small set of basic rules that address all or at least the most important moral considerations. One difficulty for systems with several basic principles is that these principles may in some cases conflict with each other and lead to ethical dilemmas.

Different theories in normative ethics suggest different principles as the foundation of morality. The three most influential schools of thought are consequentialism, deontology, and virtue ethics. These schools are usually presented as exclusive alternatives but depending on how they are defined, they can overlap and do not necessarily exclude one another. In some cases, they differ concerning which acts they see as right or wrong. In other cases, they recommend the same course of action but provide different justifications for why it is right.

Consequentialism, also referred to as teleological ethics, holds that morality depends on consequences. According to the most common view, an act is right if it brings about the best future. This means that there is no alternative course of action that has better consequences. A key aspect of consequentialist theories is that they provide a characterization of what is good and then define what is right in terms of what is good.

Consequentialists usually understand the consequences of an action in a very wide sense that includes the totality of its effects. This is based on the idea that actions make a difference to the world by bringing about a causal chain of events that would not have existed otherwise. A core intuition behind consequentialism is that what matters is not the past but the future and that it should be shaped to result in the best possible outcome.

The act itself is usually not seen as part of the consequences. This means that if an act has intrinsic value and disvalue, it is not included as a relevant factor. Some consequentialists try to avoid this complication by including the act itself as part of the consequences. A related approach is to characterize consequentialism not in terms of consequences but in terms of outcomes with outcome being defined as the act together with its consequences.

Most forms of consequentialism are agent-neutral. This means that the value of consequences is assessed from a neutral perspective, i.e., acts should have consequences that are good in general and not just good for the agent. It is controversial whether agent-relative moral theories, like ethical egoism, should be considered as types of consequentialism.

There are many different types of consequentialism. They differ from each other based on what type of entity they evaluate, how they determine whether a consequence is good, and what consequences they take into consideration. Most theories assess the moral value of acts. But consequentialism can also be used to evaluate motives, character traits, rules, and policies.

Many consequentialists assess the value of consequences based on whether they promote happiness or suffering. But there are also alternative evaluative principles, such as desire satisfaction, autonomy, freedom, knowledge, friendship, beauty, and self-perfection. Some forms of consequentialism hold that there is only a single source of value. The most prominent among them is utilitarianism, which states that the moral value of acts only depends on the pleasure they cause. An alternative approach is to hold that there are many different sources of value. According to this view, all sources of value contribute to one overall value. Traditionally, consequentialists were only concerned with the sum total of value or the aggregate good. A more recently developed view is that the distribution of value also matters. It states, for example, that an equal distribution of goods is overall better than an unequal distribution even if the aggregate good is the same.

There are various disagreements about what consequences should be assessed. An important distinction is between act and rule consequentialism. According to act consequentialism, the consequences of an act determine the moral value of this act. This means that there is a direct relation between the consequences of an act and its moral value. Rule consequentialism, by contrast, holds that an act is right if it follows a certain set of rules. Rule consequentialism uses considerations of consequences to determine which rules should be followed: people should follow the rules that have the best consequences in a community that accepts them. This implies that the relation between act and consequences is indirect. For example, if a prohibition to lie is part of the best rules then, according to rule consequentialism, a person should not lie even in a particular case where lying would result in the best possible consequences.

Another disagreement on the level of consequences is between actual and expected consequentialism. According to the traditional view, only the actual consequences of an act affect its moral value. One difficulty of this view is that many consequences cannot be known in advance. This means that in some cases, even well-planned and intentioned acts are morally wrong if they inadvertently lead to negative outcomes. An alternative perspective states that what matters are not the actual consequences but the expected consequences. This view takes into account that when deciding what to do, people have to rely on their very limited knowledge of the total consequences of their actions. According to this view, a course of action has positive moral value despite leading to an overall negative outcome if it had the highest expected value, for example, because the negative outcome could not be anticipated or was very unlikely.

Another difference is between maximizing and satisficing consequentialism. According to maximizing consequentialism, only the best possible act is morally permitted. This means that acts with positive consequences are wrong if there are alternatives with even better consequences. One criticism of maximizing consequentialism is that it demands too much by requiring that people do significantly more than they are socially expected to. For example, if the best action for someone with a good salary would be to donate 70% of their income to charity, it would be morally wrong for them to only donate 65%. Satisficing consequentialism, by contrast, only requires that an act is "good enough" even if it is not the best possible alternative. According to this view, it is possible to do more than one is morally required to do, a state known as supererogation.

One of the earliest forms of consequentialism is found in ancient Chinese philosophy where Mohists argued that political action should promote justice as a means to increase the welfare of the people.

The most well-known form of consequentialism is utilitarianism. In its classical form, it is an act consequentialism that sees happiness as the only source of intrinsic value. This means that an act is morally right if it produces "the greatest good for the greatest number" by increasing happiness and reducing suffering. Utilitarians do not deny that other things also have value, like health, friendship, and knowledge. However, they deny that these things have intrinsic value. Instead, they hold that they have extrinsic value because they affect happiness and suffering. In this regard, they are desirable as a means but, unlike happiness, not desirable as an end. The view that pleasure is the only thing with intrinsic value is called ethical or evaluative hedonism.

Utilitarianism was initially formulated by Jeremy Bentham and further developed by John Stuart Mill. Bentham introduced the hedonic calculus to assess the value of consequences. Two key aspects of the hedonic calculus are the intensity and the duration of pleasure. According to this view, a pleasurable experience has a high value if it has a high intensity and lasts for a long time. Some critics of Bentham's utilitarianism argued that it is a "philosophy of swine" whose focus on the intensity of pleasure promotes an immoral lifestyle centered around indulgence in sensory pleasures. Mill responded to this criticism by distinguishing between higher and lower pleasures. He stated that higher pleasures, like the intellectual pleasure of reading a book, are more valuable than lower pleasures, like the sensory pleasure of food and drink, even if their intensity and duration are the same. Today, there are many variations of utilitarianism, including the difference between act and rule utilitarianism and between maximizing and satisficing utilitarianism.

Deontology assesses the moral rightness of actions based on a set of norms or principles. These norms describe certain requirements or duties that all actions need to follow. Examples are that one should tell the truth, keep promises, and not intentionally harm others. Unlike consequentialists, deontologists hold that the validity of general moral principles does not depend on their consequences. They state that these principles should be followed in every case since they express how actions are inherently right or wrong. For example, according to David Ross, it is wrong to break a promise even if no harm comes from it. In this regard, deontologists often allow that there is a gap between what is right and what is good. Many tend to follow a negative approach by holding that certain acts are forbidden under any circumstances.

Agent-centered deontological theories focus on the role of moral agency and following one's duties. They are often interested in the motives and intentions for which people act and emphasize the importance of doing something for the right reasons. They are often agent-relative, meaning that the reasons for which people should act depend on personal circumstances. For example, a parent has a special obligation to their child while a stranger does not have this kind of obligation toward a child they do not know. Patient-centered theories, by contrast, emphasize the rights of the people affected by the action. An example is the requirement to treat other people as ends and not merely as a means to an end. This requirement can be used to argue, for example, that it is wrong to kill a person against their will even if this act would save the life of several others. Patient-centered deontological theories are usually agent-neutral, meaning that they apply equally to everyone in a situation, regardless of their specific role or position.

Immanuel Kant is one of the most well-known deontologists. He insists that moral action should not be guided by situation-dependent means-end reasoning to achieve some kind of fixed good, such as happiness. Instead, he argues that there are certain moral principles that apply to every situation independent of means-end relations. Kant uses the term categorical imperative for these principles and holds that they are non-empirical and universal laws that have their source in the structure of practical reason and apply to all rational agents. For Kant, to act morally is to act in accordance with reason as expressed by these principles. He sees immoral actions as irrational by going against the fundamental principles of practical reason.

Kant provided several formulations of the categorical imperative. One emphasizes the universal nature of reason and states that people should only follow maxims that could become universal laws applicable to everyone. This means that the person would want everyone else also to follow this maxim. Another formulation states that one should treat other people always as ends in themselves and never as mere means to an end. This formulation focuses on respecting and valuing other people for their own sake rather than using them in the pursuit of personal goals.

In either case, Kant holds that what matters is to have a good will. A person has a good will if they respect the moral law and form their intentions and motives in accordance with it. For Kant, actions motivated in such a way are unconditionally good, meaning that they are good even in cases where they result in undesirable consequences.

Divine command theory sees God as the source of morality. It states that moral laws are divine commands and that to act morally is to obey and follow God's will. While all divine command theorists agree that morality depends on God, there are disagreements about the precise content of the divine commands, and theorists belonging to different religions tend to propose different moral laws. For example, Christian and Jewish divine command theorists may argue that the Ten Commandments express God's will while Muslims may reserve this role for the teachings of the Quran.

Contractualists reject the reference to God as the source of morality and argue instead that morality is based on an explicit or implicit social contract between humans. They state that actual or hypothetical consent to this contract is the source of moral norms and duties. To determine which duties people have, contractualists often rely on a thought experiment about what rational people under ideal circumstances would agree on. For example, if they would agree that people should not lie then there is a moral obligation to refrain from lying. Because of its reliance on consent, contractualism is often understood as a patient-centered form of deontology.

Discourse ethics also focuses on social agreement on moral norms but holds that this agreement is based on communicative rationality. It aims to arrive at moral norms for pluralistic modern societies that encompass a diversity of viewpoints. A universal moral norm is seen as valid if all rational discourse participants do or would approve. This way, morality is not imposed by a single moral authority but arises from the moral discourse within society. This discourse should follow certain requirements characteristic of an ideal speech situation. One of its key aspects is that discourse participants are free to voice their different opinions without coercion but are at the same time required to justify them using rational argumentation.

The main concern of virtue ethics is how virtues are expressed in actions. As such, it is neither directly interested in the consequences of actions nor in universal moral duties. Virtues are positive character traits, like honesty, courage, kindness, and compassion. They are usually understood as dispositions to feel, decide, and act in a certain manner by being wholeheartedly committed to this manner. Virtues contrast with vices, which are their harmful counterparts.

Virtue theorists usually hold that the mere possession of virtues by itself is not sufficient. Instead, people should manifest virtues in their actions. An important factor in this regard is the practical wisdom, also referred to as phronesis, of knowing, when, how, and which virtue to express. For example, a lack of practical wisdom may lead courageous people to perform morally wrong actions by taking unnecessary risks that should better be avoided.

Different types of virtue ethics differ concerning how they understand virtues and their role in practical life. Eudaimonism is the classical view and draws a close relation between virtuous behavior and happiness. It states that people flourish by living a virtuous life. Eudaimonist theories often hold that virtues are positive potentials residing in human nature and that actualizing these potentials results in leading a good and happy life. Agent-based theories, by contrast, see happiness only as a side effect and focus instead on the motivational and dispositional characteristics that are expressed while acting. This is often combined with the idea that one can learn from exceptional individuals what those characteristics are. Feminist ethics of care constitute another form of virtue ethics. They emphasize the importance of interpersonal relationships and hold that benevolence by caring for the well-being of others is one of the key virtues.

Influential schools of virtue ethics in ancient philosophy were Aristotelianism and Stoicism. According to Aristotle, each virtue is a golden mean between two types of vices: excess and deficiency. For example, courage is a virtue that lies between the deficient state of cowardice and the excessive state of recklessness. Aristotle held that virtuous action leads to happiness and makes people flourish in life. The Stoics believed that people can achieve happiness through virtue alone. They stated that people are happy if they are in a peaceful state of mind that is free from emotional disturbances. They advocated rationality and self-mastery to achieve this state. In the 20th century, virtue ethics experienced a resurgence thanks to philosophers such as Elizabeth Anscombe, Philippa Foot, Alasdair MacIntyre, and Martha Nussbaum.

There are many other schools of normative ethics in addition to the three main traditions. Pragmatist ethics focuses on the role of practice and holds that one of the key tasks of ethics is to solve practical problems in concrete situations. It has certain similarities to utilitarianism and its focus on consequences but concentrates more on how morality is embedded in and relative to social and cultural contexts. Pragmatists tend to give more importance to habits than to conscious deliberation and understand morality as a habit that should be shaped in the right way.

Postmodern ethics agrees with pragmatist ethics about the cultural relativity of morality. It rejects the idea that there are objective moral principles that apply universally to all cultures and traditions. It asserts that there is no one coherent ethical code since morality itself is irrational and humans are morally ambivalent beings.

Ethical egoism is the view that people should act in their self-interest or that an action is morally right if the person acts for their own benefit. It differs from psychological egoism, which states that people actually follow their self-interest without claiming that they should do so. Ethical egoists may act in accordance with commonly accepted moral expectations and benefit other people, for example, by keeping promises, helping friends, and cooperating with others. However, they do so only as a means to promote their self-interest. Ethical egoism is often criticized as an immoral and contradictory position.

Normative ethics has a central place in most religions. Key aspects of Jewish ethics are to follow the 613 commandments of God according to the "Mitzvah" duty found in the Torah and to take responsibility for societal welfare. Christian ethics puts less emphasis on following precise laws and teaches instead the practice of self-less love, such as the Great Commandment to "love your neighbor as yourself". The Five Pillars of Islam constitute a basic framework of Muslim ethics and focus on the practice of faith, prayer, charity, fasting during Ramadan, and pilgrimage to Mecca. Buddhists emphasize the importance of compassion and loving-kindness towards all sentient entities. A similar outlook is found in Jainism, which has non-violence as its principal virtue. Duty is a central aspect of Hindu ethics and is about fulfilling social obligations, which may vary depending on a person's social class and stage of life. Confucianism places great emphasis on harmony in society and sees benevolence as a key virtue. Taoism extends the importance of living in harmony to the whole world and teaches that people should practice effortless action by following the natural flow of the universe.

Applied ethics, also known as practical ethics, is the branch of ethics and applied philosophy that examines concrete moral problems encountered in real-life situations. Unlike normative ethics, it is not concerned with discovering or justifying universal ethical principles. Instead, it studies how those principles can be applied to specific domains of practical life, what consequences they have in these fields, and whether other considerations are relevant.

One of the main challenges of applied ethics is to breach the gap between abstract universal theories and their application to concrete situations. For example, an in-depth understanding of Kantianism or utilitarianism is usually not sufficient to decide how to analyze the moral implications of a medical procedure. One reason is that it may not be clear how the procedure affects the Kantian requirement of respecting everyone's personhood and what the consequences of the procedure are in terms of the greatest good for the greatest number. This difficulty is particularly relevant to applied ethicists who employ a top-down methodology by starting from universal ethical principles and applying them to particular cases within a specific domain. A different approach is to use a bottom-up methodology, which relies on many observations of particular cases to arrive at an understanding of the moral principles relevant to this particular domain. In either case, inquiry into applied ethics is often triggered by ethical dilemmas to solve cases in which a person is subject to conflicting moral requirements.

Applied ethics covers issues pertaining to both the private sphere, like right conduct in the family and close relationships, and the public sphere, like moral problems posed by new technologies and international duties toward future generations. Major branches include bioethics, business ethics, and professional ethics. There are many other branches and their domains of inquiry often overlap.

Bioethics is a wide field that covers moral problems associated with living organisms and biological disciplines. A key problem in bioethics concerns the moral status of entities and to what extent this status depends on features such as consciousness, being able to feel pleasure and pain, rationality, and personhood. These differences concern, for example, how to treat non-living entities like rocks and non-sentient entities like plants in contrast to animals and whether humans have a different moral status than other animals. According to anthropocentrism, only humans have a basic moral status. This implies that all other entities only have a derivative moral status to the extent that they affect human life. Sentientism, by contrast, extends an inherent moral status to all sentient beings. Further positions include biocentrism, which also covers non-sentient lifeforms, and ecocentrism, which states that all of nature has a basic moral status.

Bioethics is relevant to various aspects of life and to many professions. It covers a wide range of moral problems associated with topics like abortion, cloning, stem cell research, euthanasia, suicide, animal testing, intensive animal farming, nuclear waste, and air pollution.

Bioethics can be divided into medical ethics, animal ethics, and environmental ethics based on whether the ethical problems relate to humans, other animals, or nature in general. Medical ethics is the oldest branch of bioethics and has its origins in the Hippocratic Oath, which establishes ethical guidelines for medical practitioners like a prohibition to harm the patient. A central topic in medical ethics concerns issues associated with the beginning and the end of life. One debate focuses on the question of whether a fetus is a full-fledged person with all the rights associated with this status. For example, some proponents of this view argue that abortion is a form of murder. In relation to the end of life, there are ethical dilemmas concerning whether a person has a right to end their own life in cases of terminal illness and whether a medical practitioner may assist them in doing so. Other topics in medical ethics include medical confidentiality, informed consent, research on human beings, organ transplantation, and access to healthcare.

Animal ethics examines how humans should treat other animals. An influential consideration in this field emphasizes the importance of animal welfare while arguing that humans should avoid or minimize the harm done to animals. There is wide agreement that it is wrong to torture animals for fun. The situation is more complicated in cases where harm is inflicted on animals as a side effect of the pursuit of human interests. This happens, for example, during factory farming, when using animals as food, and for research experiments on animals. A key topic in animal ethics is the formulation of animal rights. Animal rights theorists assert that animals have a certain moral status and that humans have an obligation to respect this status when interacting with them. Examples of suggested animal rights include the right to life, the right to be free from unnecessary suffering, and the right to natural behavior in a suitable environment.

Environmental ethics deals with moral problems relating to the natural environment including animals, plants, natural resources, and ecosystems. In its widest sense, it also covers the whole biosphere and the cosmos. In the domain of agriculture, this concerns questions like under what circumstances it is acceptable to clear the vegetation of an area to use it for farming and the implications of using genetically modified crops. On a wider scale, environmental ethics addresses the problem of global warming and how people are responsible for this both on an individual and a collective level. Environmental ethicists often promote sustainable practices and policies directed at protecting and conserving ecosystems and biodiversity.

Business ethics examines the moral implications of business conduct and investigates how ethical principles apply to corporations and organizations. A key topic is corporate social responsibility, which is the responsibility of corporations to act in a manner that benefits society at large. Corporate social responsibility is a complex issue since many stakeholders are directly and indirectly involved in corporate decisions, such as the CEO, the board of directors, and the shareholders. A closely related topic concerns the question of whether corporations themselves, and not just their stakeholders, have moral agency. Business ethics further examines the role of truthfulness, honesty, and fairness in business practices as well as the moral implications of bribery, conflict of interest, protection of investors and consumers, worker's rights, ethical leadership, and corporate philanthropy.

Professional ethics is a closely related field that studies ethical principles applying to members of a specific profession, like engineers, medical doctors, lawyers, and teachers. It is a diverse field since different professions often have different responsibilities. Principles applying to many professions include that the professional has the required expertise for the intended work and that they have personal integrity and are trustworthy. Further principles are to serve the interest of their target group, follow client confidentiality, and respect and uphold the client's rights, such as informed consent. More precise requirements often vary between professions. A cornerstone of engineering ethics is to protect the public's safety, health, and well-being. Legal ethics emphasizes the importance of respect for justice, personal integrity, and confidentiality. Key factors in journalism ethics include accuracy, truthfulness, independence, and impartiality as well as proper attribution to avoid plagiarism.

Many other fields of applied ethics are discussed in the academic literature. Communication ethics covers moral principles in relation to communicative conduct. Two key issues in it are freedom of speech and speech responsibility. Freedom of speech concerns the ability to articulate one's opinions and ideas without the threats of punishment and censorship. Speech responsibility is about being accountable for the consequences of communicative action and inaction. A closely related field is information ethics, which focuses on the moral implications of creating, controlling, disseminating, and using information.

The ethics of technology has implications for both communication ethics and information ethics in regard to communication and information technologies. In its widest sense, it examines the moral issues associated with any artifacts created and used for instrumental means, from simple artifacts like spears to high-tech computers and nanotechnology. Central topics in the ethics of technology include the risks associated with creating new technologies, their responsible use, and questions surrounding the issue of human enhancement through technological means, such as prosthetic limbs, performance-enhancing drugs, and genetic enhancement. Important subfields include computer ethics, ethics of artificial intelligence, machine ethics, ethics of nanotechnology, and nuclear ethics.
The ethics of war investigates moral problems in relation to war and violent conflicts. According to just war theory, waging war is morally justified if it fulfills certain conditions. They are commonly divided into requirements concerning the cause to initiate violent activities, such as self-defense, and the way those violent activities are conducted, such as avoiding excessive harm to civilians in the pursuit of legitimate military targets. Military ethics is a closely related field that is interested in the conduct of military personnel. It governs questions of the circumstances under which they are permitted to kill enemies, destroy infrastructure, and put the lives of their own troops at risk. Additional topics are recruitment, training, and discharge of military personnel as well as the procurement of military equipment.

Further fields of applied ethics include political ethics, which examines the moral dimensions of political decisions, educational ethics, which covers ethical issues related to proper teaching practices, and sexual ethics, which addresses the moral implications of sexual behavior.

Metaethics is the branch of ethics that examines the nature, foundations, and scope of moral judgments, concepts, and values. It is not interested in what actions are right or wrong but in what it means for an action to be right or wrong and whether moral judgments are objective and can be true at all. It further examines the meaning of morality and moral terms. Metaethics is a metatheory that operates on a higher level of abstraction than normative ethics by investigating its underlying background assumptions. Metaethical theories usually do not directly take substantive positions regarding normative ethical theories but they can influence them nonetheless by questioning the foundational principles on which they rest.

Metaethics overlaps with various branches of philosophy. On the level of ontology, it is concerned with the metaphysical status of moral values and principles. In relation to semantics, it asks what the meaning of moral terms is and whether moral statements have a truth value. The epistemological side of metaethics discusses whether and how people can acquire moral knowledge. Metaethics further covers psychological and anthropological considerations in regard to how moral judgments motivate people to act and how to explain cross-cultural differences in moral assessments.

Metaethics examines basic ethical concepts and their relations. Ethics is concerned with normative statements about what ought to be the case, in contrast to descriptive statements, which are about what is the case. Duties and obligations express requirements of what people ought to do. Duties are sometimes defined as counterparts of the rights that always accompany them. According to this view, someone has a duty to benefit another person if this other person has the right to receive that benefit. Obligation and permission are contrasting terms that can be defined through each other: to be obligated to do something means that one is not permitted not to do it and to be permitted to do something means that one is not obligated not to do it. Some theorists define obligations in terms of values, such as the good. When used in a general sense, good contrasts with bad. In relation to people and their intentions, the term evil rather than bad is often employed.

Obligations are used to assess the moral status of actions, motives, and character traits. An action is morally right if it is in tune with the obligations and morally wrong if it violates the obligations. Supererogation is a special moral status that applies to cases in which the agent does more than is morally required of them. To be morally responsible for an action usually means that the person possessed and exercised certain capacities or some form of control. People who are morally responsible deserve evaluative attitudes from others, such as praise or blame.

A key debate in metaethics concerns the ontological status of morality and encompasses the question of whether ethical values and principles form part of reality. It examines whether moral properties exist as objective features independent of the human mind and culture rather than as subjective constructs or expressions of personal preferences and cultural norms.

Moral realists accept the claim that there are objective moral facts. This view implies that moral values are mind-independent aspects of reality and that there is an absolute fact about whether a given action is right or wrong. A consequence of this view is that moral requirements have the same ontological status as non-moral facts: it is an objective fact whether there is an obligation to keep a promise just as there is an objective fact whether a thing has a black color. Moral realism is often associated with the claim that there are universal ethical principles that apply equally to everyone. It implies that if two people disagree about a moral evaluation then at least one of them is wrong. This observation is sometimes taken as an argument against moral realism since moral disagreement is widespread and concerns most fields.

Moral relativists reject the idea that morality is an objective feature of reality. They argue instead that moral principles are human inventions. This means that a behavior is not objectively right or wrong but only subjectively right or wrong relative to a certain standpoint. Moral standpoints may differ between persons, cultures, and historical periods. For example, moral statements like "slavery is wrong" or "suicide is permitted" may be true in one culture and false in another. This position can be understood in analogy to Einstein's theory of relativity, which states that the magnitude of physical properties like mass, length, and duration depends on the frame of reference of the observer. Some moral relativists hold that moral systems are constructed to serve certain goals such as social coordination. According to this view, different societies and different social groups within a society construct different moral systems based on their diverging purposes. A different explanation states that morality arises from moral emotions, which people project onto the external world.

Moral nihilists deny the existence of moral facts. They are opposed to both objective moral facts defended by moral realism and subjective moral facts defended by moral relativism. They believe that the basic assumptions underlying moral claims are misguided. Some moral nihilists, like Friedrich Nietzsche, conclude from this that anything is allowed. A slightly different view emphasizes that moral nihilism is not itself a moral position about what is allowed and prohibited but the rejection of any moral position. Moral nihilism agrees with moral relativism that there are different standpoints according to which people judge actions to be right or wrong. However, it disagrees that this practice involves a form of morality and understands it instead as one among many types of human practices.

An influential debate among moral realists is between naturalism and non-naturalism. Naturalism states that moral properties are natural properties and are in this respect similar to the natural properties accessible to empirical observation and investigated by the natural sciences, like color and shape. Some moral naturalists hold that moral properties are a unique and basic type of natural property. Another view states that moral properties are real but not a fundamental part of reality and can be reduced to other natural properties, for example, concerning what causes pleasure and pain.

Non-naturalism accepts that moral properties form part of reality and argues that moral features are not identical or reducible to natural properties. This view is usually motivated by the idea that moral properties are unique because they express normative features or what should be the case. Proponents of this position often emphasize this uniqueness by claiming that it is a fallacy to define ethics in terms of natural entities or to infer prescriptive from descriptive statements.

The metaethical debate between cognitivism and non-cognitivism belongs to the field of semantics and concerns the meaning of moral statements. According to cognitivism, moral statements like "Abortion is morally wrong" and "Going to war is never morally justified" are truth-apt. This means that they all have a truth value: they are either true or false. Cognitivism only claims that moral statements have a truth value but is not interested in which truth value they have. It is often seen as the default position since moral statements resemble other statements, like "Abortion is a medical procedure" or "Going to war is a political decision", which have a truth value.

The semantic position of cognitivism is closely related to the ontological position of moral realism and philosophers who accept one often accept the other as well. An exception is J. L. Mackie's error theory, which combines cognitivism with moral nihilism by claiming that all moral statements are false because there are no moral facts.

Non-cognitivism is the view that moral statements lack a truth value. According to this view, the statement "Murder is wrong" is neither true nor false. Some non-cognitivists claim that moral statements have no meaning at all. A different interpretation is that they express other types of meaning contents. Emotivism holds that they articulate emotional attitudes. According to this view, the statement "Murder is wrong" expresses that the speaker has negative moral attitudes towards murder or dislikes it. Prescriptivism, by contrast, understands moral statements as commands. According to this view, stating that "Murder is wrong" expresses a command like "Do not commit murder".

The epistemology of ethics studies whether or how one can know moral truths. Foundationalist views state that some moral beliefs are basic and do not require further justification. Ethical intuitionism is one foundationalist view that states that humans have a special cognitive faculty through which they can know right from wrong. Intuitionists often argue that general moral truths, like "lying is wrong", are self-evident and that it is possible to know them "a priori" without relying on empirical experience. A different foundationalist view relies not on general intuitions but on particular observations. It holds that if people are confronted with a concrete moral situation, they can perceive whether right or wrong conduct was involved.

In contrast to foundationalists, coherentists hold that there are no basic moral beliefs. They argue that beliefs form a complex network and mutually support and justify one another. According to this view, a moral belief can only amount to knowledge if it coheres with the rest of the beliefs in the network. Moral skeptics reject the idea that moral knowledge is possible by arguing that people are unable to distinguish between right and wrong behavior. Moral skepticism is often criticized based on the claim that it leads to immoral behavior.

Thought experiments are a common methodological device in ethics to decide between competing theories. They usually present an imagined situation involving an ethical dilemma and explore how moral intuitions about what behavior is right depend on particular factors in the imagined situation. For example, in the trolley problem, a person can flip a switch to redirect a trolley from one track to another, thereby sacrificing the life of one person in order to save five. This scenario explores how the difference between doing and allowing harm affects moral obligations. Another thought experiment examines the moral implications of abortion by imagining a situation in which a person gets connected without their consent to an ill violinist. It explores whether it would be morally permissible to sever the connection within the next nine months even if this would lead to the violinist's death.

On the level of psychology, metaethics is interested in how moral beliefs and experiences affect behavior. According to motivational internalists, there is a direct link between moral judgments and action. This means that every judgment about what is right motivates the person to act accordingly. For example, Socrates defends a strong form of motivational internalism by holding that a person can only perform an evil deed if they are unaware that it is evil. Weaker forms of motivational internalism allow that people can act against moral judgments, for example, because of weakness of the will. Motivational externalists accept that people can judge a behavior to be morally required without feeling a reason to engage in it. This means that moral judgments do not always provide motivational force. The debate between internalism and externalism is relevant for explaining the behavior of psychopaths or sociopaths, who fail either to judge that a behavior is wrong or to translate their judgment into action. A closely related question is whether moral judgments can provide motivation on their own or need to be accompanied by other mental states, such as a desire to act morally.

Value theory, also referred to as axiology, is the philosophical study of value. It aims to understand what value is and what types of value there are. Further questions include what kinds of things have value and how valuable they are. A central distinction is between intrinsic and instrumental value. An entity has intrinsic value if it is good in itself or good for its own sake. An entity has instrumental value if it is valuable as a means to something else, for example, by causing something that has intrinsic value. Another key topic is about what entities have intrinsic value, for example, whether pleasure has intrinsic value and whether there are other sources of intrinsic value besides pleasure.

There are disagreements about the exact relation between value theory and ethics. Some philosophers characterize value theory as a subdiscipline of ethics while others see value theory as the broader term that encompasses other fields besides ethics, such as aesthetics and political philosophy. A different characterization sees the two disciplines as overlapping but distinct fields. The term axiological ethics is sometimes used for the discipline studying this overlap, i.e., for the part of ethics that studies values. The two disciplines are sometimes distinguished based on their focus: ethics is about moral behavior or what is right while value theory is about value or what is good. Some ethical theories, like consequentialism, stand very close to value theory by defining what is right in terms of what is good. But this is not true for ethics in general and deontological theories tend to reject the idea that what is good can be used to define what is right.

Moral psychology explores the psychological foundations and processes involved in moral behavior. It is an empirical science that studies how humans think and act in moral contexts. It is interested in how moral reasoning and judgments take place, how moral character forms, what sensitivity people have to moral evaluations, and how people attribute and react to moral responsibility.

One of its key topics is moral development or the question of how morality develops on a psychological level from infancy to adulthood. According to Lawrence Kohlberg, for example, children go through different stages of moral development as they understand moral principles first as fixed rules governing reward and punishment, then as conventional social norms, and later as abstract principles of what is objectively right across societies. A closely related question is whether and how people can be taught to act morally.

Evolutionary ethics is a subfield of moral psychology and sociobiology. It explores how evolutionary processes have shaped ethics. One of its key ideas is that natural selection is responsible for moral behavior and moral sensitivity. It interprets morality as an adaptation to evolutionary pressure that augments fitness by offering a selective advantage. Altruism, for example, can provide benefits to group survival by improving cooperation.

Descriptive ethics, also called comparative ethics, studies actually existing moral codes, practices, and beliefs. It investigates and compares moral phenomena in different societies and different groups within a society. It aims to provide a value-neutral and empirical description without judging or justifying which practices are objectively right. For instance, the question of how nurses think about the ethical implications of abortion belongs to descriptive ethics. Another example is descriptive business ethics, which describes ethical standards in the context of business, including common practices, official policies, and employee opinions. Descriptive ethics also has a historical dimension by exploring how moral practices and beliefs have changed over time.

Descriptive ethics is a multidisciplinary field that is covered by disciplines such as anthropology, sociology, psychology, and history. Its empirical outlook contrasts with the philosophical inquiry into normative questions, such as which ethical principles are correct and how to justify them.

The history of ethics studies how moral philosophy has developed and evolved in the course of history. It has its origin in the ancient civilizations. In ancient Egypt, the concept of Maat was used as an ethical principle to guide behavior and maintain order by emphasizing the importance of truth, balance, and harmony. In ancient India, the Vedas and Upanishads were written as the foundational texts of Hindu philosophy and discussed the role of duty and the consequences of one's actions. Buddhist ethics also originated in ancient India and advocated compassion, non-violence, and the pursuit of enlightenment. Ancient China saw the emergence of Confucianism, which focuses on moral conduct and self-cultivation by acting in accordance with virtues, and Daoism, which teaches that human behavior should be in harmony with the natural order of the universe.

In ancient Greece, Socrates emphasized the importance of inquiry into what a good life is by critically questioning established ideas and exploring concepts like virtue, justice, courage, and wisdom. According to Plato, to lead a good life means that the different parts of the soul are in harmony with each other. For Aristotle, a good life is associated with being happy by cultivating virtues and flourishing. The close relation between right action and happiness was also explored by Hellenistic schools of Epicureanism, which recommended a simple lifestyle without indulging in sensory pleasures, and Stoicism, which advocated living in tune with reason and virtue while practicing self-mastery and becoming immune to disturbing emotions.

Ethical thought in the medieval period was strongly influenced by religious teachings. Christian philosophers interpreted moral principles as divine commands originating from God. Thomas Aquinas developed natural law ethics by claiming that ethical behavior consists in following the laws and order of nature, which he believed were created by God. In the Islamic world, philosophers like Al-Farabi and Avicenna synthesized ancient Greek philosophy with the ethical teachings of Islam while emphasizing the harmony between reason and faith. In medieval India, philosophers like Adi Shankara and Ramanuja saw the practice of spirituality to attain liberation as the highest goal of human behavior.

Moral philosophy in the modern period was characterized by a shift toward a secular approach to ethics. Thomas Hobbes identified self-interest as the primary drive of humans. He concluded that it would lead to "a war of every man against every man" unless a social contract is established to avoid this outcome. David Hume thought that only moral sentiments, like empathy, can motivate ethical actions while he saw reason not as a motivating factor but only as what anticipates the consequences of possible actions. Immanuel Kant, by contrast, saw reason as the source of morality. He formulated a deontological theory, according to which the ethical value of actions depends on their conformity with moral laws independent of their outcome. These laws take the form of categorical imperatives, which are universal requirements that apply to every situation. Another influential development in this period was the formulation of utilitarianism by Jeremy Bentham and John Stuart Mill. According to the utilitarian doctrine, actions should promote happiness while reducing suffering and the right action is the one that produces the greatest good for the greatest number of people.

An important development in 20th-century ethics in analytic philosophy was the emergence of metaethics. Significant early contributions to this field were made by G. E. Moore, who argued that moral values are essentially different from other properties found in the natural world. R. M. Hare followed this idea in formulating his prescriptivism, which states that moral statements are commands that, unlike regular judgments, are neither true nor false. An influential argument for moral realism was made by Derek Parfit, who argued that morality concerns objective features of reality that give people reasons to act in one way or another. Bernard Williams agreed with the close relation between reasons and ethics but defended a subjective view instead that sees reasons as internal mental states that may or may not reflect external reality. Another development in this period was the revival of ancient virtue ethics by philosophers like Philippa Foot. In the field of political philosophy, John Rawls relied on Kantian ethics to analyze social justice as a form of fairness. In continental philosophy, phenomenologists such as Max Scheler and Nicolai Hartmann built ethical systems based on the claim that values have objective reality that can be investigated using the phenomenological method. Existentialists like Jean-Paul Sartre, by contrast, held that values are created by humans and explored the consequences of this view in relation to individual freedom, responsibility, and authenticity. This period also saw the emergence of feminist ethics, which questions traditional ethical assumptions associated with a male perspective and puts alternative concepts, like care, at the center.


Equivalence relation

In mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. The equipollence relation between line segments in geometry is a common example of an equivalence relation. A simpler example is equality. Any number formula_1 is equal to itself (reflexive). If formula_2, then formula_3 (symmetric). If formula_2 and formula_5, then formula_6 (transitive).

Each equivalence relation provides a partition of the underlying set into disjoint equivalence classes. Two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class.

Various notations are used in the literature to denote that two elements formula_1 and formula_8 of a set are equivalent with respect to an equivalence relation formula_9 the most common are "formula_10" and "", which are used when formula_11 is implicit, and variations of "formula_12", "", or "formula_13" to specify formula_11 explicitly. Non-equivalence may be written "" or "formula_15".

A binary relation formula_16 on a set formula_17 is said to be an equivalence relation, if and only if it is reflexive, symmetric and transitive. That is, for all formula_18 and formula_19 in formula_20

formula_17 together with the relation formula_16 is called a setoid. The equivalence class of formula_1 under formula_30 denoted formula_31 is defined as formula_32

In relational algebra, if formula_33 and formula_34 are relations, then the composite relation formula_35 is defined so that formula_36 if and only if there is a formula_37 such that formula_38 and formula_39. This definition is a generalisation of the definition of functional composition. The defining properties of an equivalence relation formula_11 on a set formula_17 can then be reformulated as follows:

On the set formula_47, the relation formula_48 is an equivalence relation. The following sets are equivalence classes of this relation:
formula_49

The set of all equivalence classes for formula_11 is formula_51 This set is a partition of the set formula_17 with respect to formula_11.

The following relations are all equivalence relations:



If formula_16 is an equivalence relation on formula_70 and formula_71 is a property of elements of formula_70 such that whenever formula_73 formula_71 is true if formula_75 is true, then the property formula_76 is said to be well-defined or a under the relation formula_77 

A frequent particular case occurs when formula_59 is a function from formula_17 to another set formula_80 if formula_81 implies formula_82 then formula_59 is said to be a for formula_30 a formula_30 or simply formula_77 This occurs, e.g. in the character theory of finite groups. The latter case with the function formula_59 can be expressed by a commutative triangle. See also invariant. Some authors use "compatible with formula_88" or just "respects formula_88" instead of "invariant under formula_88".

More generally, a function may map equivalent arguments (under an equivalence relation formula_91) to equivalent values (under an equivalence relation formula_92). Such a function is known as a morphism from formula_91 to formula_94

Let formula_95, and formula_96 be an equivalence relation. Some key definitions and terminology follow:

A subset formula_97 of formula_17 such that formula_10 holds for all formula_1 and formula_8 in formula_97, and never for formula_1 in formula_97 and formula_8 outside formula_97, is called an equivalence class of formula_17 by formula_96. Let formula_109 denote the equivalence class to which formula_1 belongs. All elements of formula_17 equivalent to each other are also elements of the same equivalence class.

The set of all equivalence classes of formula_17 by formula_113 denoted formula_114 is the quotient set of formula_17 by formula_116 If formula_17 is a topological space, there is a natural way of transforming formula_118 into a topological space; see "Quotient space" for the details.

The projection of formula_16 is the function formula_120 defined by formula_121 which maps elements of formula_17 into their respective equivalence classes by formula_77

The equivalence kernel of a function formula_59 is the equivalence relation ~ defined by formula_133 The equivalence kernel of an injection is the identity relation.

A partition of "X" is a set "P" of nonempty subsets of "X", such that every element of "X" is an element of a single element of "P". Each element of "P" is a "cell" of the partition. Moreover, the elements of "P" are pairwise disjoint and their union is "X".

Let "X" be a finite set with "n" elements. Since every equivalence relation over "X" corresponds to a partition of "X", and vice versa, the number of equivalence relations on "X" equals the number of distinct partitions of "X", which is the "n"th Bell number "B":

A key result links equivalence relations and partitions:
In both cases, the cells of the partition of "X" are the equivalence classes of "X" by ~. Since each element of "X" belongs to a unique cell of any partition of "X", and since each cell of the partition is identical to an equivalence class of "X" by ~, each element of "X" belongs to a unique equivalence class of "X" by ~. Thus there is a natural bijection between the set of all equivalence relations on "X" and the set of all partitions of "X".

If formula_96 and formula_136 are two equivalence relations on the same set formula_137, and formula_10 implies formula_139 for all formula_140 then formula_136 is said to be a coarser relation than formula_96, and formula_96 is a finer relation than formula_136. Equivalently,

The equality equivalence relation is the finest equivalence relation on any set, while the universal relation, which relates all pairs of elements, is the coarsest.

The relation "formula_96 is finer than formula_136" on the collection of all equivalence relations on a fixed set is itself a partial order relation, which makes the collection a geometric lattice.


Much of mathematics is grounded in the study of equivalences, and order relations. Lattice theory captures the mathematical structure of order relations. Even though equivalence relations are as ubiquitous in mathematics as order relations, the algebraic structure of equivalences is not as well known as that of orders. The former structure draws primarily on group theory and, to a lesser extent, on the theory of lattices, categories, and groupoids.

Just as order relations are grounded in ordered sets, sets closed under pairwise supremum and infimum, equivalence relations are grounded in partitioned sets, which are sets closed under bijections that preserve partition structure. Since all such bijections map an equivalence class onto itself, such bijections are also known as permutations. Hence permutation groups (also known as transformation groups) and the related notion of orbit shed light on the mathematical structure of equivalence relations.

Let '~' denote an equivalence relation over some nonempty set "A", called the universe or underlying set. Let "G" denote the set of bijective functions over "A" that preserve the partition structure of "A", meaning that for all formula_181 and formula_182 Then the following three connected theorems hold:

In sum, given an equivalence relation ~ over "A", there exists a transformation group "G" over "A" whose orbits are the equivalence classes of "A" under ~.

This transformation group characterisation of equivalence relations differs fundamentally from the way lattices characterize order relations. The arguments of the lattice theory operations meet and join are elements of some universe "A". Meanwhile, the arguments of the transformation group operations composition and inverse are elements of a set of bijections, "A" → "A".

Moving to groups in general, let "H" be a subgroup of some group "G". Let ~ be an equivalence relation on "G", such that formula_183 The equivalence classes of ~—also called the orbits of the action of "H" on "G"—are the right cosets of "H" in "G". Interchanging "a" and "b" yields the left cosets.

Related thinking can be found in Rosen (2008: chpt. 10).

Let "G" be a set and let "~" denote an equivalence relation over "G". Then we can form a groupoid representing this equivalence relation as follows. The objects are the elements of "G", and for any two elements "x" and "y" of "G", there exists a unique morphism from "x" to "y" if and only if formula_184

The advantages of regarding an equivalence relation as a special case of a groupoid include:

The equivalence relations on any set "X", when ordered by set inclusion, form a complete lattice, called Con "X" by convention. The canonical map ker : "X"^"X" → Con "X", relates the monoid "X"^"X" of all functions on "X" and Con "X". ker is surjective but not injective. Less formally, the equivalence relation ker on "X", takes each function "f" : "X" → "X" to its kernel ker "f". Likewise, ker(ker) is an equivalence relation on "X"^"X".

Equivalence relations are a ready source of examples or counterexamples. For example, an equivalence relation with exactly two infinite equivalence classes is an easy example of a theory which is ω-categorical, but not categorical for any larger cardinal number.

An implication of model theory is that the properties defining a relation can be proved independent of each other (and hence necessary parts of the definition) if and only if, for each property, examples can be found of relations not satisfying the given property while satisfying all the other properties. Hence the three defining properties of equivalence relations can be proved mutually independent by the following three examples:

Properties definable in first-order logic that an equivalence relation may or may not possess include:




Equivalence class

In mathematics, when the elements of some set formula_1 have a notion of equivalence (formalized as an equivalence relation), then one may naturally split the set formula_1 into equivalence classes. These equivalence classes are constructed so that elements formula_3 and formula_4 belong to the same equivalence class if, and only if, they are equivalent.

Formally, given a set formula_1 and an equivalence relation formula_6 on formula_7 the of an element formula_3 in formula_1 is denoted formula_10 or, equivalently, formula_11 to emphasize its equivalence relation formula_12 The definition of equivalence relations implies that the equivalence classes form a partition of formula_13 meaning, that every element of the set belongs to exactly one equivalence class.
The set of the equivalence classes is sometimes called the quotient set or the quotient space of formula_1 by formula_15 and is denoted by formula_16

When the set formula_1 has some structure (such as a group operation or a topology) and the equivalence relation formula_6 is compatible with this structure, the quotient set often inherits a similar structure from its parent set. Examples include quotient spaces in linear algebra, quotient spaces in topology, quotient groups, homogeneous spaces, quotient rings, quotient monoids, and quotient categories.

An equivalence relation on a set formula_19 is a binary relation formula_6 on formula_19 satisfying the three properties:

The equivalence class of an element formula_3 is defined as
The word "class" in the term "equivalence class" may generally be considered as a synonym of "set", although some equivalence classes are not sets but proper classes. For example, "being isomorphic" is an equivalence relation on groups, and the equivalence classes, called isomorphism classes, are not sets.

The set of all equivalence classes in formula_19 with respect to an equivalence relation formula_34 is denoted as formula_35 and is called formula_19 modulo formula_34 (or the ' of formula_19 by formula_34). The surjective map formula_40 from formula_19 onto formula_35 which maps each element to its equivalence class, is called the ', or the canonical projection.

Every element of an equivalence class characterizes the class, and may be used to "represent" it. When such an element is chosen, it is called a representative of the class. The choice of a representative in each class defines an injection from formula_43 to . Since its composition with the canonical surjection is the identity of formula_35 such an injection is called a section, when using the terminology of category theory. 

Sometimes, there is a section that is more "natural" than the other ones. In this case, the representatives are called . For example, in modular arithmetic, for every integer greater than , the congruence modulo is an equivalence relation on the integers, for which two integers and are equivalent—in this case, one says "congruent"—if divides formula_45 this is denoted formula_46 Each class contains a unique non-negative integer smaller than formula_47 and these integers are the canonical representatives. 

The use of representatives for representing classes allows avoiding to consider explicitly classes as sets. In this case, the canonical surjection that maps an element to its class is replaced by the function that maps an element to the representative of its class. In the preceding example, this function is denoted formula_48 and produces the remainder of the Euclidean division of by .

Every element formula_49 of formula_19 is a member of the equivalence class formula_51 Every two equivalence classes formula_52 and formula_53 are either equal or disjoint. Therefore, the set of all equivalence classes of formula_19 forms a partition of formula_19: every element of formula_19 belongs to one and only one equivalence class. Conversely, every partition of formula_19 comes from an equivalence relation in this way, according to which formula_58 if and only if formula_49 and formula_60 belong to the same set of the partition.

It follows from the properties in the previous section that if formula_6 is an equivalence relation on a set formula_62 and formula_49 and formula_60 are two elements of formula_62 the following statements are equivalent:



An undirected graph may be associated to any symmetric relation on a set formula_62 where the vertices are the elements of formula_62 and two vertices formula_94 and formula_95 are joined if and only if formula_96 Among these graphs are the graphs of equivalence relations. These graphs, called cluster graphs, are characterized as the graphs such that the connected components are cliques.

If formula_6 is an equivalence relation on formula_62 and formula_99 is a property of elements of formula_19 such that whenever formula_101 formula_99 is true if formula_103 is true, then the property formula_104 is said to be an invariant of formula_15 or well-defined under the relation formula_106

A frequent particular case occurs when formula_107 is a function from formula_19 to another set formula_109; if formula_110 whenever formula_111 then formula_107 is said to be formula_15 or simply formula_106 This occurs, for example, in the character theory of finite groups. Some authors use "compatible with formula_6" or just "respects formula_6" instead of "invariant under formula_6".

Any function formula_118 is "class invariant under" formula_15 according to which formula_120 if and only if formula_121 The equivalence class of formula_49 is the set of all elements in formula_19 which get mapped to formula_124 that is, the class formula_52 is the inverse image of formula_126 This equivalence relation is known as the kernel of formula_127

More generally, a function may map equivalent arguments (under an equivalence relation formula_128 on formula_19) to equivalent values (under an equivalence relation formula_130 on formula_109). Such a function is a morphism of sets equipped with an equivalence relation.

In topology, a quotient space is a topological space formed on the set of equivalence classes of an equivalence relation on a topological space, using the original space's topology to create the topology on the set of equivalence classes.

In abstract algebra, congruence relations on the underlying set of an algebra allow the algebra to induce an algebra on the equivalence classes of the relation, called a quotient algebra. In linear algebra, a quotient space is a vector space formed by taking a quotient group, where the quotient homomorphism is a linear map. By extension, in abstract algebra, the term quotient space may be used for quotient modules, quotient rings, quotient groups, or any quotient algebra. However, the use of the term for the more general cases can as often be by analogy with the orbits of a group action.

The orbits of a group action on a set may be called the quotient space of the action on the set, particularly when the orbits of the group action are the right cosets of a subgroup of a group, which arise from the action of the subgroup on the group by left translations, or respectively the left cosets as orbits under right translation.

A normal subgroup of a topological group, acting on the group by translation action, is a quotient space in the senses of topology, abstract algebra, and group actions simultaneously.

Although the term can be used for any equivalence relation's set of equivalence classes, possibly with further structure, the intent of using the term is generally to compare that type of equivalence relation on a set formula_62 either to an equivalence relation that induces some structure on the set of equivalence classes from a structure of the same kind on formula_62 or to the orbits of a group action. Both the sense of a structure preserved by an equivalence relation, and the study of invariants under group actions, lead to the definition of invariants of equivalence relations given above.




Entertainment

Entertainment is a form of activity that holds the attention and interest of an audience or gives pleasure and delight. It can be an idea or a task, but it is more likely to be one of the activities or events that have developed over thousands of years specifically for the purpose of keeping an audience's attention.

Although people's attention is held by different things because individuals have different preferences, most forms of entertainment are recognisable and familiar. Storytelling, music, drama, dance, and different kinds of performance exist in all cultures, were supported in royal courts, and developed into sophisticated forms over time, becoming available to all citizens. The process has been accelerated in modern times by an entertainment industry that records and sells entertainment products. Entertainment evolves and can be adapted to suit any scale, ranging from an individual who chooses private entertainment from a now enormous array of pre-recorded products, to a banquet adapted for two, to any size or type of party with appropriate music and dance, to performances intended for thousands, and even for a global audience.

The experience of being entertained has come to be strongly associated with amusement, so that one common understanding of the idea is fun and laughter, although many entertainments have a serious purpose. This may be the case in various forms of ceremony, celebration, religious festival, or satire, for example. Hence, there is the possibility that what appears to be entertainment may also be a means of achieving insight or intellectual growth.

An important aspect of entertainment is the audience, which turns a private recreation or leisure activity into entertainment. The audience may have a passive role, as in the case of people watching a play, opera, television show, or film; or the audience role may be active, as in the case of games, where the participant and audience roles may be routinely reversed. Entertainment can be public or private, involving formal, scripted performances, as in the case of theatre or concerts, or unscripted and spontaneous, as in the case of children's games. Most forms of entertainment have persisted over many centuries, evolving due to changes in culture, technology, and fashion, as with stage magic. Films and video games, although they use newer media, continue to tell stories, present drama, and play music. Festivals devoted to music, film, or dance allow audiences to be entertained over a number of consecutive days.

Some entertainment, such as public executions, is now illegal in most countries. Activities such as fencing or archery, once used in hunting or war, have become spectator sports. In the same way, other activities, such as cooking, have developed into performances among professionals, staged as global competitions, and then broadcast for entertainment. What is entertainment for one group or individual may be regarded as work or an act of cruelty by another.

The familiar forms of entertainment have the capacity to cross over into different media and have demonstrated a seemingly unlimited potential for creative remix. This has ensured the continuity and longevity of many themes, images, and structures.

The Oxford English Dictionary gives Latin and French origins for the word "entertain", including "inter" (among) + "tenir" (to hold) as derivations, giving translations of "to hold mutually" or "to hold intertwined" and "to engage, keep occupied, the attention, thoughts, or time (of a person)". It also provides words like "merry-making", "pleasure", and "delight", as well as "to receive as a guest and show hospitality to". It cites a 1490 usage by William Caxton.

Entertainment can be distinguished from other activities such as education and marketing even though they have learned how to use the appeal of entertainment to achieve their different goals. Sometimes entertainment can be a mixture for both. The importance and impact of entertainment is recognised by scholars and its increasing sophistication has influenced practices in other fields such as museology.

Psychologists say the function of media entertainment is "the attainment of gratification". No other results or measurable benefits are usually expected from it (except perhaps the final score in a sporting entertainment). This is in contrast to education (which is designed with the purpose of developing understanding or helping people to learn) and marketing (which aims to encourage people to purchase commercial products). However, the distinctions become blurred when education seeks to be more "entertaining" and entertainment or marketing seek to be more "educational". Such mixtures are often known by the neologisms "edutainment" or "infotainment". The psychology of entertainment as well as of learning has been applied to all these fields. Some education-entertainment is a serious attempt to combine the best features of the two. Some people are entertained by others' pain or the idea of their unhappiness (schadenfreude).

An entertainment might go beyond gratification and produce some insight in its audience. Entertainment may skilfully consider universal philosophical questions such as: "What does it mean to be human?"; "What is the right thing to do?"; or "How do I know what I know?". "The meaning of life", for example, is the subject in a wide range of entertainment forms, including film, music and literature. Questions such as these drive many narratives and dramas, whether they are presented in the form of a story, film, play, poem, book, dance, comic, or game. Dramatic examples include Shakespeare's influential play "Hamlet", whose hero articulates these concerns in poetry; and films, such as "The Matrix", which explores the nature of knowledge and was released worldwide. Novels give great scope for investigating these themes while they entertain their readers. An example of a creative work that considers philosophical questions so entertainingly that it has been presented in a very wide range of forms is "The Hitchhiker's Guide to the Galaxy". Originally a radio comedy, this story became so popular that it has also appeared as a novel, film, television series, stage show, comic, audiobook, LP record, adventure game and online game, its ideas became popular references (see Phrases from The Hitchhiker's Guide to the Galaxy) and has been translated into many languages. Its themes encompass the meaning of life, as well as "the ethics of entertainment, artificial intelligence, multiple worlds, God, and philosophical method".

The "ancient craft of communicating events and experiences, using words, images, sounds and gestures" by telling a story is not only the means by which people passed on their cultural values and traditions and history from one generation to another, it has been an important part of most forms of entertainment ever since the earliest times. Stories are still told in the early forms, for example, around a fire while camping, or when listening to the stories of another culture as a tourist. "The earliest storytelling sequences we possess, now of course, committed to writing, were undoubtedly originally a speaking from mouth to ear and their force as entertainment derived from the very same elements we today enjoy in films and novels." Storytelling is an activity that has evolved and developed "toward variety". Many entertainments, including storytelling but especially music and drama, remain familiar but have developed into a wide variety of form to suit a very wide range of personal preferences and cultural expression. Many types are blended or supported by other forms. For example, drama, stories and banqueting (or dining) are commonly enhanced by music; sport and games are incorporated into other activities to increase appeal. Some may have evolved from serious or necessary activities (such as running and jumping) into competition and then become entertainment. It is said, for example, that pole vaulting "may have originated in the Netherlands, where people used long poles to vault over wide canals rather than wear out their clogs walking miles to the nearest bridge. Others maintain that pole vaulting was used in warfare to vault over fortress walls during battle." The equipment for such sports has become increasingly sophisticated. Vaulting poles, for example, were originally made from woods such as ash, hickory or hazel; in the 19th century bamboo was used and in the 21st century poles can be made of carbon fibre. Other activities, such as walking on stilts, are still seen in circus performances in the 21st century. Gladiatorial combats, also known as "gladiatorial games", popular during Roman times, provide a good example of an activity that is a combination of sport, punishment, and entertainment.

Changes to what is regarded as entertainment can occur in response to cultural or historical shifts. Hunting wild animals, for example, was introduced into the Roman Empire from Carthage and became a popular public entertainment and spectacle, supporting an international trade in wild animals.

Entertainment also evolved into different forms and expressions as a result of social upheavals such as wars and revolutions. During the Chinese Cultural Revolution, for example, Revolutionary opera was sanctioned by the Communist party and World War I, the Great Depression and the Russian Revolution all affected entertainment.

Relatively minor changes to the form and venue of an entertainment continue to come and go as they are affected by the period, fashion, culture, technology, and economics. For example, a story told in dramatic form can be presented in an open-air theatre, a music hall, a movie theatre, a multiplex, or as technological possibilities advanced, via a personal electronic device such as a tablet computer. Entertainment is provided for mass audiences in purpose-built structures such as a theatre, auditorium, or stadium. One of the most famous venues in the Western world, the Colosseum, "dedicated AD 80 with a hundred days of games, held fifty thousand spectators," and in it audiences "enjoyed blood sport with the trappings of stage shows". Spectacles, competitions, races, and sports were once presented in this purpose-built arena as public entertainment. New stadia continue to be built to suit the ever more sophisticated requirements of global audiences.

Imperial and royal courts have provided training grounds and support for professional entertainers, with different cultures using palaces, castles and forts in different ways. In the Maya city states, for example, "spectacles often took place in large plazas in front of palaces; the crowds gathered either there or in designated places from which they could watch at a distance." Court entertainments also crossed cultures. For example, the durbar was introduced to India by the Mughals, and passed onto the British Empire, which then followed Indian tradition: "institutions, titles, customs, ceremonies by which a Maharaja or Nawab were installed ... the exchange of official presents ... the order of precedence", for example, were "all inherited from ... the Emperors of Delhi". In Korea, the "court entertainment dance" was "originally performed in the palace for entertainment at court banquets."

Court entertainment often moved from being associated with the court to more general use among commoners. This was the case with "masked dance-dramas" in Korea, which "originated in conjunction with village shaman rituals and eventually became largely an entertainment form for commoners". Nautch dancers in the Mughal Empire performed in Indian courts and palaces. Another evolution, similar to that from courtly entertainment to common practice, was the transition from religious ritual to secular entertainment, such as happened during the Goryeo dynasty with the Narye festival. Originally "solely religious or ritualistic, a secular component was added at the conclusion". Former courtly entertainments, such as jousting, often also survived in children's games.

In some courts, such as those during the Byzantine Empire, the genders were segregated among the upper classes, so that "at least before the period of the Komnenoi" (1081–1185) men were separated from women at ceremonies where there was entertainment such as receptions and banquets.

Court ceremonies, palace banquets and the spectacles associated with them, have been used not only to entertain but also to demonstrate wealth and power. Such events reinforce the relationship between ruler and ruled; between those with power and those without, serving to "dramatise the differences between ordinary families and that of the ruler". This is the case as much as for traditional courts as it is for contemporary ceremonials, such as the Hong Kong handover ceremony in 1997, at which an array of entertainments (including a banquet, a parade, fireworks, a festival performance and an art spectacle) were put to the service of highlighting a change in political power. Court entertainments were typically performed for royalty and courtiers as well as "for the pleasure of local and visiting dignitaries". Royal courts, such as the Korean one, also supported traditional dances. In Sudan, musical instruments such as the so-called "slit" or "talking" drums, once "part of the court orchestra of a powerful chief", had multiple purposes: they were used to make music; "speak" at ceremonies; mark community events; send long-distance messages; and call men to hunt or war.

Courtly entertainments also demonstrate the complex relationship between entertainer and spectator: individuals may be either an entertainer or part of the audience, or they may swap roles even during the course of one entertainment. In the court at the Palace of Versailles, "thousands of courtiers, including men and women who inhabited its apartments, acted as both performers and spectators in daily rituals that reinforced the status hierarchy".

Like court entertainment, royal occasions such as coronations and weddings provided opportunities to entertain both the aristocracy and the people. For example, the splendid 1595 Accession Day celebrations of Queen Elizabeth I offered tournaments and jousting and other events performed "not only before the assembled court, in all their finery, but also before thousands of Londoners eager for a good day's entertainment. Entry for the day's events at the Tiltyard in Whitehall was set at 12d".

Although most forms of entertainment have evolved and continued over time, some once-popular forms are no longer as acceptable. For example, during earlier centuries in Europe, watching or participating in the punishment of criminals or social outcasts was an accepted and popular form of entertainment. Many forms of public humiliation also offered local entertainment in the past. Even capital punishment such as hanging and beheading, offered to the public as a warning, were also regarded partly as entertainment. Capital punishments that lasted longer, such as stoning and drawing and quartering, afforded a greater public spectacle. "A hanging was a carnival that diverted not merely the unemployed but the unemployable. Good bourgeois or curious aristocrats who could afford it watched it from a carriage or rented a room." Public punishment as entertainment lasted until the 19th century by which time "the awesome event of a public hanging aroused the[ir] loathing of writers and philosophers". Both Dickens and Thackeray wrote about a hanging in Newgate Prison in 1840, and "taught an even wider public that executions are obscene entertainments".

Children's entertainment is centred on play and is significant for their growth. It often mimics adult activities, such as watching performances (on television); prepares them for adult responsibilities, such as child rearing or social interaction (through dolls, pets and group games); or develops skills such as motor skills (such as a game of marbles), needed for sports and music. In the modern day, it often involves sedentary engagement with television or tablet computer.

Entertainment is also provided to children or taught to them by adults and many activities that appeal to them such as puppets, clowns, pantomimes and cartoons are also enjoyed by adults.

Children have always played games. It is accepted that as well as being entertaining, playing games helps children's development. One of the most famous visual accounts of children's games is a painting by Pieter Bruegel the Elder called "Children's Games", painted in 1560. It depicts children playing a range of games that presumably were typical of the time. Many of these games, such as marbles, hide-and-seek, blowing soap bubbles and piggyback riding continue to be played.
Most forms of entertainment can be or are modified to suit children's needs and interests. During the 20th century, starting with the often criticised but nonetheless important work of G. Stanley Hall, who "promoted the link between the study of development and the 'new' laboratory psychology", and especially with the work of Jean Piaget, who "saw cognitive development as being analogous to biological development", it became understood that the psychological development of children occurs in stages and that their capacities differ from adults. Hence, stories and activities, whether in books, film, or video games were developed specifically for child audiences. Countries have responded to the special needs of children and the rise of digital entertainment by developing systems such as television content rating systems, to guide the public and the entertainment industry.

In the 21st century, as with adult products, much entertainment is available for children on the internet for private use. This constitutes a significant change from earlier times. The amount of time expended by children indoors on screen-based entertainment and the "remarkable collapse of children's engagement with nature" has drawn criticism for its negative effects on imagination, adult cognition and psychological well-being.<ref name="http://www.guardian.co.uk/commentisfree/2012/nov/19/children-lose-contact-with-nature"></ref>

Banquets have been a venue for amusement, entertainment or pleasure since ancient times, continuing into the modern era. until the 21st century when they are still being used for many of their original purposesto impress visitors, especially important ones; to show hospitality; as an occasion to showcase supporting entertainments such as music or dancing, or both. They were an integral part of court entertainments and helped entertainers develop their skills. They are also important components of celebrations such as coronations, weddings, birthdays civic or political achievements, military engagements or victories as well as religious obligations, one of the most famous being the Banqueting House, Whitehall in London. In modern times, banquets are available privately, or commercially in restaurants, sometimes combined with a dramatic performance in dinner theatres. Cooking by professional chefs has also become a form of entertainment as part of global competitions such as the Bocuse d'Or.

Music is a supporting component of many kinds of entertainment and most kinds of performance. For example, it is used to enhance storytelling, it is indispensable in dance and opera, and is usually incorporated into dramatic film or theatre productions.

Music is also a universal and popular type of entertainment on its own, constituting an entire performance such as when concerts are given. Depending on the rhythm, instrument, performance and style, music is divided into many genres, such as classical, jazz, folk, rock, pop music or traditional. Since the 20th century, performed music, once available only to those who could pay for the performers, has been available cheaply to individuals by the entertainment industry, which broadcasts it or pre-records it for sale.

The wide variety of musical performances, whether or not they are artificially amplified, all provide entertainment irrespective of whether the performance is from soloists, choral or orchestral groups, or ensemble. Live performances use specialised venues, which might be small or large; indoors or outdoors; free or expensive. The audiences have different expectations of the performers as well as of their own role in the performance. For example, some audiences expect to listen silently and are entertained by the excellence of the music, its rendition or its interpretation. Other audiences of live performances are entertained by the ambience and the chance to participate. Even more listeners are entertained by pre-recorded music and listen privately.

The instruments used in musical entertainment are either solely the human voice or solely instrumental or some combination of the two. Whether the performance is given by vocalists or instrumentalists, the performers may be soloists or part of a small or large group, in turn entertaining an audience that might be individual, passing by, small or large. Singing is generally accompanied by instruments although some forms, notably a cappella and overtone singing, are unaccompanied. Modern concerts often use various special effects and other theatrics to accompany performances of singing and dancing.

Games are played for entertainmentsometimes purely for recreation, sometimes for achievement or reward as well. They can be played alone, in teams, or online; by amateurs or by professionals. The players may have an audience of non-players, such as when people are entertained by watching a chess championship. On the other hand, players in a game may constitute their own audience as they take their turn to play. Often, part of the entertainment for children playing a game is deciding who is part of their audience and who is a player.

Equipment varies with the game. Board games, such as Go, "Monopoly" or backgammon need a board and markers. One of the oldest known board games is Senet, a game played in Ancient Egypt, enjoyed by the pharaoh Tutankhamun. Card games, such as whist, poker and Bridge have long been played as evening entertainment among friends. For these games, all that is needed is a deck of playing cards. Other games, such as bingo, played with numerous strangers, have been organised to involve the participation of non-players via gambling. Many are geared for children, and can be played outdoors, including hopscotch, hide and seek, or Blind man's bluff. The list of ball games is quite extensive. It includes, for example, croquet, lawn bowling and paintball as well as many sports using various forms of balls. The options cater to a wide range of skill and fitness levels. Physical games can develop agility and competence in motor skills. Number games such as Sudoku and puzzle games like the Rubik's cube can develop mental prowess.

Video games are played using a controller to create results on a screen. They can also be played online with participants joining in remotely. In the second half of the 20th century and in the 21st century the number of such games increased enormously, providing a wide variety of entertainment to players around the world. Video games are popular across the world.

Reading has been a source of entertainment for a very long time, especially when other forms, such as performance entertainments, were (or are) either unavailable or too costly. Even when the primary purpose of the writing is to inform or instruct, reading is well known for its capacity to distract from everyday worries. Both stories and information have been passed on through the tradition of orality and oral traditions survive in the form of performance poetry for example. However, they have drastically declined. "Once literacy had arrived in strength, there was no return to the oral prerogative." The advent of printing, the reduction in costs of books and an increasing literacy all served to enhance the mass appeal of reading. Furthermore, as fonts were standardised and texts became clearer, "reading ceased being a painful process of decipherment and became an act of pure pleasure". By the 16th century in Europe, the appeal of reading for entertainment was well established.

Among literature's many genres are some designed, in whole or in part, purely for entertainment. Limericks, for example, use verse in a strict, predictable rhyme and rhythm to create humour and to amuse an audience of listeners or readers. Interactive books such as "choose your own adventure" can make literary entertainment more participatory.
Comics and editorial cartoons are literary genres that use drawings or graphics, usually in combination with text, to convey an entertaining narrative. Many contemporary comics have elements of fantasy and are produced by companies that are part of the entertainment industry. Others have unique authors who offer a more personal, philosophical view of the world and the problems people face. Comics about superheroes such as Superman are of the first type. Examples of the second sort include the individual work over 50 years of Charles M. Schulz who produced a popular comic called "Peanuts" about the relationships among a cast of child characters; and Michael Leunig who entertains by producing whimsical cartoons that also incorporate social criticism. The Japanese Manga style differs from the western approach in that it encompasses a wide range of genres and themes for a readership of all ages. Caricature uses a kind of graphic entertainment for purposes ranging from merely putting a smile on the viewer's face, to raising social awareness, to highlighting the moral characteristics of a person being caricatured.

Comedy is both a genre of entertainment and a component of it, providing laughter and amusement, whether the comedy is the sole purpose or used as a form of contrast in an otherwise serious piece. It is a valued contributor to many forms of entertainment, including in literature, theatre, opera, film and games. In royal courts, such as in the Byzantine court, and presumably, also in its wealthy households, "mimes were the focus of orchestrated humour, expected or obliged to make fun of all at court, not even excepting the emperor and members of the imperial family. This highly structured role of jester consisted of verbal humour, including teasing, jests, insult, ridicule, and obscenity and non-verbal humour such as slapstick and horseplay in the presence of an audience." In medieval times, all comic types the buffoon, jester, hunchback, dwarf, jokester, were all "considered to be essentially of one comic type: the fool", who while not necessarily funny, represented "the shortcomings of the individual".

Shakespeare wrote seventeen comedies that incorporate many techniques still used by performers and writers of comedysuch as jokes, puns, parody, wit, observational humor, or the unexpected effect of irony. One-liner jokes and satire are also used to comedic effect in literature. In farce, the comedy is a primary purpose.

The meaning of the word "comedy" and the audience's expectations of it have changed over time and vary according to culture. Simple physical comedy such as slapstick is entertaining to a broad range of people of all ages. However, as cultures become more sophisticated, national nuances appear in the style and references so that what is amusing in one culture may be unintelligible in another.

Live performances before an audience constitute a major form of entertainment, especially before the invention of audio and video recording. Performance takes a wide range of forms, including theatre, music and drama. In the 16th and 17th centuries, European royal courts presented masques that were complex theatrical entertainments involving dancing, singing and acting. Opera is a similarly demanding performance style that remains popular. It also encompass all three forms, demanding a high level of musical and dramatic skill, collaboration and like the masque, production expertise as well.
Audiences generally show their appreciation of an entertaining performance with applause. However, all performers run the risk of failing to hold their audience's attention and thus, failing to entertain. Audience dissatisfaction is often brutally honest and direct.

Storytelling is an ancient form of entertainment that has influenced almost all other forms. It is "not only entertainment, it is also thinking through human conflicts and contradictions". Hence, although stories may be delivered directly to a small listening audience, they are also presented as entertainment and used as a component of any piece that relies on a narrative, such as film, drama, ballet, and opera. Written stories have been enhanced by illustrations, often to a very high artistic standard, for example, on illuminated manuscripts and on ancient scrolls such as Japanese ones. Stories remain a common way of entertaining a group that is on a journey. Showing how stories are used to pass the time and entertain an audience of travellers, Chaucer used pilgrims in his literary work "The Canterbury Tales" in the 14th century, as did Wu Cheng'en in the 16th century in "Journey to the West". Even though journeys can now be completed much faster, stories are still told to passengers en route in cars and aeroplanes either orally or delivered by some form of technology.

The power of stories to entertain is evident in one of the most famous onesScheherazadea story in the Persian professional storytelling tradition, of a woman who saves her own life by telling stories. The connections between the different types of entertainment are shown by the way that stories like this inspire a retelling in another medium, such as music, film or games. For example, composers Rimsky-Korsakov, Ravel and Szymanowski have each been inspired by the Scheherazade story and turned it into an orchestral work; director Pasolini made a film adaptation; and there is an innovative video game based on the tale. Stories may be told wordlessly, in music, dance or puppetry for example, such as in the Javanese tradition of wayang, in which the performance is accompanied by a gamelan orchestra or the similarly traditional Punch and Judy show.

Epic narratives, poems, sagas and allegories from all cultures tell such gripping tales that they have inspired countless other stories in all forms of entertainment. Examples include the Hindu "Ramayana" and "Mahabharata"; Homer's "Odyssey" and "Iliad"; the first Arabic novel "Hayy ibn Yaqdhan"; the Persian epic "Shahnameh"; the Sagas of Icelanders and the celebrated "Tale of the Genji". Collections of stories, such as "Grimms' Fairy Tales" or those by Hans Christian Andersen, have been similarly influential. Originally published in the early 19th century, this collection of folk stories significantly influence modern popular culture, which subsequently used its themes, images, symbols, and structural elements to create new entertainment forms.

Some of the most powerful and long-lasting stories are the foundation stories, also called origin or creation myths such as the Dreamtime myths of the Australian aborigines, the Mesopotamian "Epic of Gilgamesh", or the Hawaiian stories of the origin of the world. These too are developed into books, films, music and games in a way that increases their longevity and enhances their entertainment value.

Theatre performances, typically dramatic or musical, are presented on a stage for an audience and have a history that goes back to Hellenistic times when "leading musicians and actors" performed widely at "poetical competitions", for example at "Delphi, Delos, Ephesus". Aristotle and his teacher Plato both wrote on the theory and purpose of theatre. Aristotle posed questions such as "What is the function of the arts in shaping character? Should a member of the ruling class merely watch performances or be a participant and perform? What kind of entertainment should be provided for those who do not belong to the elite?" The "Ptolemys in Egypt, the Seleucids in Pergamum" also had a strong theatrical tradition and later, wealthy patrons in Rome staged "far more lavish productions".

Expectations about the performance and their engagement with it have changed over time. For example, in England during the 18th century, "the prejudice against actresses had faded" and in Europe generally, going to the theatre, once a socially dubious activity, became "a more respectable middle-class pastime" in the late 19th and early 20th centuries, when the variety of popular entertainments increased. Operetta and music halls became available, and new drama theatres such as the Moscow Art Theatre and the Suvorin Theatre in Russia opened. At the same time, commercial newspapers "began to carry theatre columns and reviews" that helped make theatre "a legitimate subject of intellectual debate" in general discussions about art and culture. Audiences began to gather to "appreciate creative achievement, to marvel at, and be entertained by, the prominent 'stars'." Vaudeville and music halls, popular at this time in the United States, England, Canada, Australia and New Zealand, were themselves eventually superseded.

Plays, musicals, monologues, pantomimes, and performance poetry are part of the very long history of theatre, which is also the venue for the type of performance known as comedy. In the 20th century, radio and television, often broadcast live, extended the theatrical tradition that continued to exist alongside the new forms.

The stage and the spaces set out in front of it for an audience create a theatre. All types of stage are used with all types of seating for the audience, including the impromptu or improvised; the temporary; the elaborate; or the traditional and permanent. They are erected indoors or outdoors. The skill of managing, organising and preparing the stage for a performance is known as stagecraft. The audience's experience of the entertainment is affected by their expectations, the stagecraft, the type of stage, and the type and standard of seating provided.

Films are a major form of entertainment, although not all films have entertainment as their primary purpose: documentary film, for example, aims to create a record or inform, although the two purposes often work together. The medium was a global business from the beginning: "The Lumière brothers were the first to send cameramen throughout the world, instructing them to film everything which could be of interest for the public." In 1908, Pathé launched and distributed newsreels and by World War I, films were meeting an enormous need for mass entertainment. "In the first decade of the [20th] century cinematic programmes combined, at random, fictions and newsfilms." The Americans first "contrived a way of producing an illusion of motion through successive images," but "the French were able to transform a scientific principle into a commercially lucrative spectacle". Film therefore became a part of the entertainment industry from its early days. Increasingly sophisticated techniques have been used in the film medium to delight and entertain audiences. Animation, for example, which involves the display of rapid movement in an art work, is one of these techniques that particularly appeals to younger audiences. The advent of computer-generated imagery (CGI) in the 21st century made it "possible to do spectacle" more cheaply and "on a scale never dreamed of" by Cecil B. DeMille. From the 1930s to 1950s, movies and radio were the "only mass entertainment" but by the second decade of the 21st century, technological changes, economic decisions, risk aversion and globalisation reduced both the quality and range of films being produced. Sophisticated visual effects and CGI techniques, for example, rather than humans, were used not only to create realistic images of people, landscapes and events (both real and fantastic) but also to animate non-living items such as Lego normally used as entertainment as a game in physical form. Creators of "The Lego Movie" "wanted the audience to believe they were looking at actual Lego bricks on a tabletop that were shot with a real camera, not what we actually did, which was create vast environments with digital bricks inside the computer." The convergence of computers and film has allowed entertainment to be presented in a new way and the technology has also allowed for those with the personal resources to screen films in a home theatre, recreating in a private venue the quality and experience of a public theatre. This is similar to the way that the nobility in earlier times could stage private musical performances or the use of domestic theatres in large homes to perform private plays in earlier centuries.

Films also re-imagine entertainment from other forms, turning stories, books and plays, for example, into new entertainments. "", a documentary about the history of film, gives a survey of global achievements and innovations in the medium, as well as changes in the conception of film-making. It demonstrates that while some films, particularly those in the Hollywood tradition that combines "realism and melodramatic romanticism", are intended as a form of escapism, others require a deeper engagement or more thoughtful response from their audiences. For example, the award-winning Senegalese film "Xala" takes government corruption as its theme. Charlie Chaplin's film "The Great Dictator" was a brave and innovative parody, also on a political theme. Stories that are thousands of years old, such as "Noah", have been re-interpreted in film, applying familiar literary devices such as allegory and personification with new techniques such as CGI to explore big themes such as "human folly", good and evil, courage and despair, love, faith, and death themes that have been a main-stay of entertainment across all its forms.

As in other media, excellence and achievement in films is recognised through a range of awards, including ones from the American Academy of Motion Picture Arts and Sciences, the British Academy of Film and Television Arts, the Cannes International Film Festival in France and the Asia Pacific Screen Awards.

The many forms of dance provide entertainment for all age groups and cultures. Dance can be serious in tone, such as when it is used to express a culture's history or important stories; it may be provocative; or it may put in the service of comedy. Since it combines many forms of entertainment music, movement, storytelling, theatre it provides a good example of the various ways that these forms can be combined to create entertainment for different purposes and audiences.

Dance is "a form of cultural representation" that involves not just dancers, but "choreographers, audience members, patrons and impresarios ... coming from all over the globe and from vastly varied time periods." Whether from Africa, Asia or Europe, dance is constantly negotiating the realms of political, social, spiritual and artistic influence." Even though dance traditions may be limited to one cultural group, they all develop. For example, in Africa, there are "Dahomean dances, Hausa dances, Masai dances and so forth." Ballet is an example of a highly developed Western form of dance that moved to the theatres from the French court during the time of Louis XIV, the dancers becoming professional theatrical performers. Some dances, such as the quadrille, a square dance that "emerged during the Napoleonic years in France" and other country dances were once popular at social gatherings like balls, but are now rarely performed. On the other hand, many folk dances (such as Scottish Highland dancing and Irish dancing), have evolved into competitions, which by adding to their audiences, has increased their entertainment value. "Irish dance theatre, which sometimes features traditional Irish steps and music, has developed into a major dance form with an international reputation."

Since dance is often "associated with the female body and women's experiences", female dancers, who dance to entertain, have in some cases been regarded as distinct from "decent" women because they "use their bodies to make a living instead of hiding them as much as possible". Society's attitudes to female dancers depend on the culture, its history and the entertainment industry itself. For example, while some cultures regard any dancing by women as "the most shameful form of entertainment", other cultures have established venues such as strip clubs where deliberately erotic or sexually provocative dances such as striptease are performed in public by professional women dancers for mostly male audiences.

Various political regimes have sought to control or ban dancing or specific types of dancing, sometimes because of disapproval of the music or clothes associated with it. Nationalism, authoritarianism and racism have played a part in banning dances or dancing. For example, during the Nazi regime, American dances such as swing, regarded as "completely un-German", had "become a public offense and needed to be banned". Similarly, in Shanghai, China, in the 1930s, "dancing and nightclubs had come to symbolise the excess that plagued Chinese society" and officials wondered if "other forms of entertainment such as brothels" should also be banned. Banning had the effect of making "the dance craze" even greater. In Ireland, the Public Dance Hall Act of 1935 "banned but did not stop dancing at the crossroads and other popular dance forms such as house and barn dances." In the US, various dances were once banned, either because like burlesque, they were suggestive, or because, like the Twist, they were associated with African Americans. "African American dancers were typically banned from performing in minstrel shows until after the American Civil War."

Dances can be performed solo, in pairs, in groups, or by massed performers. They might be improvised or highly choreographed; spontaneous for personal entertainment (such as when children begin dancing for themselves); a private audience, a paying audience, a world audience, or an audience interested in a particular dance genre. They might be a part of a celebration, such as a wedding or New Year, or a cultural ritual with a specific purpose, such as a dance by warriors like a haka. Some dances, such as traditional dance and ballet, need a very high level of skill and training; others, such as the can-can, require a very high level of energy and physical fitness. Entertaining the audience is a normal part of dance but its physicality often also produces joy for the dancers themselves.

Animals have been used for the purposes of entertainment for millennia. They have been hunted for entertainment (as opposed to hunted for food); displayed while they hunt for prey; watched when they compete with each other; and watched while they perform a trained routine for human amusement. The Romans, for example, were entertained both by competitions involving wild animals and acts performed by trained animals. They watched as "lions and bears danced to the music of pipes and cymbals; horses were trained to kneel, bow, dance and prance ... acrobats turning handsprings over wild lions and vaulting over wild leopards." There were "violent confrontations with wild beasts" and "performances over time became more brutal and bloodier".

Animals that perform trained routines or "acts" for human entertainment include fleas in flea circuses, dolphins in dolphinaria, and monkeys doing tricks for an audience on behalf of the player of a street organ. Animals kept in zoos in ancient times were often kept there for later use in the arena as entertainment or for their entertainment value as exotica.

Many contests between animals are now regarded as sports for example, horse racing is regarded as both a sport and an important source of entertainment. Its economic impact means that it is also considered a global industry, one in which horses are carefully transported around the world to compete in races. In Australia, the horse race run on Melbourne Cup Day is a public holiday and the public regards the race as an important annual event. Like horse racing, camel racing requires human riders, while greyhound racing does not. People find it entertaining to watch animals race competitively, whether they are trained, like horses, camels or dogs, or untrained, like cockroaches.

The use of animals for entertainment is sometimes controversial, especially the hunting of wild animals. Some contests between animals, once popular entertainment for the public, have become illegal because of the cruelty involved. Among these are blood sports such as bear-baiting, dog fighting and cockfighting. Other contests involving animals remain controversial and have both supporters and detractors. For example, the conflict between opponents of pigeon shooting who view it as "a cruel and moronic exercise in marksmanship, and proponents, who view it as entertainment" has been tested in a court of law. Fox hunting, which involves the use of horses as well as hounds, and bullfighting, which has a strong theatrical component, are two entertainments that have a long and significant cultural history. They both involve animals and are variously regarded as sport, entertainment or cultural tradition. Among the organisations set up to advocate for the rights of animals are some whose concerns include the use of animals for entertainment. However, "in many cases of animal advocacy groups versus organisations accused of animal abuse, both sides have cultural claims."

A circus, described as "one of the most brazen of entertainment forms", is a special type of theatrical performance, involving a variety of physical skills such as acrobatics and juggling and sometimes performing animals. Usually thought of as a travelling show performed in a big top, circus was first performed in permanent venues. Philip Astley is regarded as the founder of the modern circus in the second half of the 18th century and Jules Léotard is the French performer credited with developing the art of the trapeze, considered synonymous with circuses. Astley brought together performances that were generally familiar in traditional British fairs "at least since the beginning of the 17th century": "tumbling, rope-dancing, juggling, animal tricks and so on". It has been claimed that "there is no direct link between the Roman circus and the circus of modern times. ... Between the demise of the Roman 'circus' and the foundation of Astley's Amphitheatre in London some 1300 years later, the nearest thing to a circus ring was the rough circle formed by the curious onlookers who gathered around the itinerant tumbler or juggler on a village green."

The form of entertainment known as stage magic or conjuring and recognisable as performance, is based on traditions and texts of magical rites and dogmas that have been a part of most cultural traditions since ancient times. (References to magic, for example, can be found in the Bible, in Hermeticism, in Zoroastrianism, in the Kabbalistic tradition, in mysticism and in the sources of Freemasonry.)

Stage magic is performed for an audience in a variety of media and locations: on stage, on television, in the street, and live at parties or events. It is often combined with other forms of entertainment, such as comedy or music and showmanship is often an essential part of magic performances. Performance magic relies on deception, psychological manipulation, sleight of hand and other forms of trickery to give an audience the illusion that a performer can achieve the impossible. Audiences amazed at the stunt performances and escape acts of Harry Houdini, for example, regarded him as a magician.

Fantasy magicians have held an important place in literature for centuries, offering entertainment to millions of readers. Famous wizards such as Merlin in the Arthurian legends have been written about since the 5th and 6th centuries, while in the 21st century, the young wizard Harry Potter became a global entertainment phenomenon when the book series about him sold about 450 million copies (as at June 2011), making it the best-selling book series in history.

Street entertainment, street performance, or "busking" are forms of performance that have been meeting the public's need for entertainment for centuries. It was "an integral aspect of London's life", for example, when the city in the early 19th century was "filled with spectacle and diversion". Minstrels or troubadours are part of the tradition. The art and practice of busking is still celebrated at annual busking festivals.

There are three basic forms of contemporary street performance. The first form is the "circle show". It tends to gather a crowd, usually has a distinct beginning and end, and is done in conjunction with street theatre, puppeteering, magicians, comedians, acrobats, jugglers and sometimes musicians. This type has the potential to be the most lucrative for the performer because there are likely to be more donations from larger audiences if they are entertained by the act. Good buskers control the crowd so patrons do not obstruct foot traffic. The second form, the "walk-by act", has no distinct beginning or end. Typically, the busker provides an entertaining ambience, often with an unusual instrument, and the audience may not stop to watch or form a crowd. Sometimes a walk-by act spontaneously turns into a circle show. The third form, "café busking", is performed mostly in restaurants, pubs, bars and cafés. This type of act occasionally uses public transport as a venue.

Parades are held for a range of purposes, often more than one. Whether their mood is sombre or festive, being public events that are designed to attract attention and activities that necessarily divert normal traffic, parades have a clear entertainment value to their audiences. Cavalcades and the modern variant, the motorcade, are examples of public processions. Some people watching the parade or procession may have made a special effort to attend, while others become part of the audience by happenstance. Whatever their mood or primary purpose, parades attract and entertain people who watch them pass by. Occasionally, a parade takes place in an improvised theatre space (such as the Trooping the Colour in ) and tickets are sold to the physical audience while the global audience participates via broadcast.

One of the earliest forms of parade were "triumphs" grand and sensational displays of foreign treasures and spoils, given by triumphant Roman generals to celebrate their victories. They presented conquered peoples and nations that exalted the prestige of the victor. "In the summer of 46 BCE Julius Caesar chose to celebrate four triumphs held on different days extending for about one month." In Europe from the Middle Ages to the Baroque the Royal Entry celebrated the formal visit of the monarch to the city with a parade through elaborately decorated streets, passing various shows and displays. The annual Lord Mayor's Show in London is an example of a civic parade that has survived since medieval times.

Many religious festivals (especially those that incorporate processions, such as Holy Week processions or the Indian festival of Holi) have some entertainment appeal in addition to their serious purpose. Sometimes, religious rituals have been adapted or evolved into secular entertainments, or like the Festa del Redentore in Venice, have managed to grow in popularity while holding both secular and sacred purposes in balance. However, pilgrimages, such as the Roman Catholic pilgrimage of the Way of St. James, the Muslim Hajj and the Hindu Kumbh Mela, which may appear to the outsider as an entertaining parade or procession, are not intended as entertainment: they are instead about an individual's spiritual journey. Hence, the relationship between spectator and participant, unlike entertainments proper, is different. The manner in which the Kumbh Mela, for example, "is divorced from its cultural context and repackaged for Western consumption renders the presence of voyeurs deeply problematic."

Parades generally impress and delight often by including unusual, colourful costumes. Sometimes they also commemorate or celebrate. Sometimes they have a serious purpose, such as when the context is military, when the intention is sometimes to intimidate; or religious, when the audience might participate or have a role to play. Even if a parade uses new technology and is some distance away, it is likely to have a strong appeal, draw the attention of onlookers and entertain them.

Fireworks are a part of many public entertainments and have retained an enduring popularity since they became a "crowning feature of elaborate celebrations" in the 17th century. First used in China, classical antiquity and Europe for military purposes, fireworks were most popular in the 18th century and high prices were paid for pyrotechnists, especially the skilled Italian ones, who were summoned to other countries to organise displays. Fire and water were important aspects of court spectacles because the displays "inspired by means of fire, sudden noise, smoke and general magnificence the sentiments thought fitting for the subject to entertain of his sovereign: awe fear and a vicarious sense of glory in his might. Birthdays, name-days, weddings and anniversaries provided the occasion for celebration." One of the most famous courtly uses of fireworks was one used to celebrate the end of the War of the Austrian Succession and while the fireworks themselves caused a fire, the accompanying Music for the Royal Fireworks written by Handel has been popular ever since. Aside from their contribution to entertainments related to military successes, courtly displays and personal celebrations, fireworks are also used as part of religious ceremony. For example, during the Indian Dashavatara Kala of Gomantaka "the temple deity is taken around in a procession with a lot of singing, dancing and display of fireworks".

The "fire, sudden noise and smoke" of fireworks is still a significant part of public celebration and entertainment. For example, fireworks were one of the primary forms of display chosen to celebrate the turn of the millennium around the world. As the clock struck midnight and 1999 became 2000, firework displays and open-air parties greeted the New Year as the time zones changed over to the next century. Fireworks, carefully planned and choreographed, were let off against the backdrop of many of the world's most famous buildings, including the Sydney Harbour Bridge, the Pyramids of Giza in Egypt, the Acropolis in Athens, Red Square in Moscow, Vatican City in Rome, the Brandenburg Gate in Berlin, the Eiffel Tower in Paris, and Elizabeth Tower in London.

Sporting competitions have always provided entertainment for crowds. To distinguish the players from the audience, the latter are often known as spectators. Developments in stadium and auditorium design, as well as in recording and broadcast technology, have allowed off-site spectators to watch sport, with the result that the size of the audience has grown ever larger and spectator sport has become increasingly popular. Two of the most popular sports with global appeal are association football and cricket. Their ultimate international competitions, the FIFA World Cup and the Cricket World Cup, are broadcast around the world. Beyond the very large numbers involved in playing these sports, they are notable for being a major source of entertainment for many millions of non-players worldwide. A comparable multi-stage, long-form sport with global appeal is the Tour de France, unusual in that it takes place outside of special stadia, being run instead in the countryside.

Aside from sports that have worldwide appeal and competitions, such as the Olympic Games, the entertainment value of a sport depends on the culture and country where people play it. For example, in the United States, baseball and basketball games are popular forms of entertainment; in Bhutan, the national sport is archery; in New Zealand, it is rugby union; in Iran, it is freestyle wrestling. Japan's unique sumo wrestling contains ritual elements that derive from its long history. In some cases, such as the international running group Hash House Harriers, participants create a blend of sport and entertainment for themselves, largely independent of spectator involvement, where the social component is more important than the competitive.

The evolution of an activity into a sport and then an entertainment is also affected by the local climate and conditions. For example, the modern sport of surfing is associated with Hawaii and that of snow skiing probably evolved in Scandinavia. While these sports and the entertainment they offer to spectators have spread around the world, people in the two originating countries remain well known for their prowess. Sometimes the climate offers a chance to adapt another sport such as in the case of ice hockeyan important entertainment in Canada.

Fairs and exhibitions have existed since ancient and medieval times, displaying wealth, innovations and objects for trade and offering specific entertainments as well as being places of entertainment in themselves. Whether in a medieval market or a small shop, "shopping always offered forms of exhilaration that took one away from the everyday". However, in the modern world, "merchandising has become entertainment: spinning signs, flashing signs, thumping music ... video screens, interactive computer kiosks, day care .. cafés".

By the 19th century, "expos" that encouraged arts, manufactures and commerce had become international. They were not only hugely popular but affected international ideas. For example, the 1878 Paris Exposition facilitated international cooperation about ideas, innovations and standards. From London 1851 to Paris 1900, "in excess of 200 million visitors had entered the turnstiles in London, Paris, Vienna, Philadelphia, Chicago and a myriad of smaller shows around the world." Since World War II "well over 500 million visits have been recorded through world expo turnstiles". As a form of spectacle and entertainment, expositions influenced "everything from architecture, to patterns of globalisation, to fundamental matters of human identity" and in the process established the close relationship between "fairs, the rise of department stores and art museums", the modern world of mass consumption and the entertainment industry.

Some entertainments, such as at large festivals (whether religious or secular), concerts, clubs, parties and celebrations, involve big crowds. From earliest times, crowds at an entertainment have associated hazards and dangers, especially when combined with the recreational consumption of intoxicants such as alcohol. The Ancient Greeks had Dionysian Mysteries, for example, and the Romans had Saturnalia. The consequence of excess and crowds can produce breaches of social norms of behaviour, sometimes causing injury or even death, such as for example, at the Altamont Free Concert, an outdoor rock festival. The list of serious incidents at nightclubs includes those caused by stampede; overcrowding; terrorism, such as the 2002 Bali bombings that targeted a nightclub; and especially fire. Investigations, such as that carried out in the US after The Station nightclub fire often demonstrate that lessons learned "regarding fire safety in nightclubs" from earlier events such as the Cocoanut Grove fire do "not necessarily result in lasting effective change". Efforts to prevent such incidents include appointing special officers, such as the medieval Lord of Misrule or, in modern times, security officers who control access; and also ongoing improvement of relevant standards such as those for building safety. The tourism industry now regards safety and security at entertainment venues as an important management task.

Entertainment is big business, especially in the United States, but ubiquitous in all cultures.
Although kings, rulers and powerful people have always been able to pay for entertainment to be provided for them and in many cases have paid for public entertainment, people generally have made their own entertainment or when possible, attended a live performance. Technological developments in the 20th century, especially in the area of mass media, meant that entertainment could be produced independently of the audience, packaged and sold on a commercial basis by an entertainment industry. Sometimes referred to as show business, the industry relies on business models to produce, market, broadcast or otherwise distribute many of its traditional forms, including performances of all types. The industry became so sophisticated that its economics became a separate area of academic study.

The film industry is a part of the entertainment industry. Components of it include the Hollywood and Bollywood film industries, as well as the cinema of the United Kingdom and all the cinemas of Europe, including France, Germany, Spain, Italy and others. The sex industry is another component of the entertainment industry, applying the same forms and media (for example, film, books, dance and other performances) to the development, marketing and sale of sex products on a commercial basis.

Amusement parks entertain paying guests with rides, such as roller coasters, ridable miniature railways, water rides, and dark rides, as well as other events and associated attractions. The parks are built on a large area subdivided into themed areas named "lands". Sometimes the whole amusement park is based on one theme, such as the various SeaWorld parks that focus on the theme of sea life.

One of the consequences of the development of the entertainment industry has been the creation of new types of employment. While jobs such as writer, musician and composer exist as they always have, people doing this work are likely to be employed by a company rather than a patron as they once would have been. New jobs have appeared, such as gaffer or special effects supervisor in the film industry, and attendants in an amusement park.

Prestigious awards are given by the industry for excellence in the various types of entertainment. For example, there are awards for music, games (including video games), comics, theatre, television, film, dance and magical arts. Sporting awards are made for the results and skill, rather than for the entertainment value.

Purpose-built structures as venues for entertainment that accommodate audiences have produced many famous and innovative buildings, among the most recognisable of which are theatre structures. For the ancient Greeks, "the architectural importance of the theatre is a reflection of their importance to the community, made apparent in their monumentality, in the effort put into their design, and in the care put into their detail." The Romans subsequently developed the stadium in an oval form known as a circus. In modern times, some of the grandest buildings for entertainment have brought fame to their cities as well as their designers. The Sydney Opera House, for example, is a World Heritage Site and The O₂ in London is an entertainment precinct that contains an indoor arena, a music club, a cinema and exhibition space. The Bayreuth Festspielhaus in Germany is a theatre designed and built for performances of one specific musical composition.

Two of the chief architectural concerns for the design of venues for mass audiences are speed of egress and safety. The speed at which the venue empty is important both for amenity and safety, because large crowds take a long time to disperse from a badly designed venue, which creates a safety risk. The Hillsborough disaster is an example of how poor aspects of building design can contribute to audience deaths. Sightlines and acoustics are also important design considerations in most theatrical venues.

In the 21st century, entertainment venues, especially stadia, are "likely to figure among the leading architectural genres". However, they require "a whole new approach" to design, because they need to be "sophisticated entertainment centres, multi-experience venues, capable of being enjoyed in many diverse ways". Hence, architects now have to design "with two distinct functions in mind, as sports and entertainment centres playing host to live audiences, and as sports and entertainment studios serving the viewing and listening requirements of the remote audience".

Architects who push the boundaries of design or construction sometimes create buildings that are entertaining because they exceed the expectations of the public and the client and are aesthetically outstanding. Buildings such as Guggenheim Museum Bilbao, designed by Frank Gehry, are of this type, becoming a tourist attraction as well as a significant international museum. Other apparently usable buildings are really follies, deliberately constructed for a decorative purpose and never intended to be practical.

On the other hand, sometimes architecture is entertainment, while pretending to be functional. The tourism industry, for example, creates or renovates buildings as "attractions" that have either never been used or can never be used for their ostensible purpose. They are instead re-purposed to entertain visitors often by simulating cultural experiences. Buildings, history and sacred spaces are thus made into commodities for purchase. Such intentional tourist attractions divorce buildings from the past so that "the difference between historical authenticity and contemporary entertainment venues/theme parks becomes hard to define". Examples include "the preservation of the Alcázar of Toledo, with its grim Civil War History, the conversion of slave dungeons into tourist attractions in Ghana, [such as, for example, Cape Coast Castle] and the presentation of indigenous culture in Libya". The specially constructed buildings in amusement parks represent the park's theme and are usually neither authentic nor completely functional.

By the second half of the 20th century, developments in electronic media made possible the delivery of entertainment products to mass audiences across the globe. The technology enabled people to see, hear and participate in all the familiar forms stories, theatre, music, dance wherever they live. The rapid development of entertainment technology was assisted by improvements in data storage devices such as cassette tapes or compact discs, along with increasing miniaturisation. Computerisation and the development of barcodes also made ticketing easier, faster and global.

In the 1940s, radio was the electronic medium for family entertainment and information. In the 1950s, it was television that was the new medium and it rapidly became global, bringing visual entertainment, first in black and white, then in colour, to the world. By the 1970s, games could be played electronically, then hand-held devices provided mobile entertainment, and by the last decade of the 20th century, via networked play. In combination with products from the entertainment industry, all the traditional forms of entertainment became available personally. People could not only select an entertainment product such as a piece of music, film or game, they could choose the time and place to use it. The "proliferation of portable media players and the emphasis on the computer as a site for film consumption" together have significantly changed how audiences encounter films. One of the most notable consequences of the rise of electronic entertainment has been the rapid obsolescence of the various recording and storage methods. As an example of speed of change driven by electronic media, over the course of one generation, television as a medium for receiving standardised entertainment products went from unknown, to novel, to ubiquitous and finally to superseded. One estimate was that by 2011 over 30 percent of households in the US would own a Wii console, "about the same percentage that owned a television in 1953". Some expected that halfway through the second decade of the 21st century, online entertainment would have completely replaced televisionwhich did not happen. The so-called "digital revolution" has produced an increasingly transnational marketplace that has caused difficulties for governments, business, industries, and individuals, as they all try to keep up. Even the sports stadium of the future will increasingly compete with television viewing "...in terms of comfort, safety and the constant flow of audio-visual information and entertainment available." Other flow on effects of the shift are likely to include those on public architecture such as hospitals and nursing homes, where television, regarded as an essential entertainment service for patients and residents, will need to be replaced by access to the internet. At the same time, the ongoing need for entertainers as "professional engagers" shows the continuity of traditional entertainment.

By the second decade of the 21st century, analogue recording was being replaced by digital recording and all forms of electronic entertainment began to converge. For example, convergence is challenging standard practices in the film industry: whereas "success or failure used to be determined by the first weekend of its run. Today, ... a series of exhibition 'windows', such as DVD, pay-per-view, and fibre-optic video-on-demand are used to maximise profits." Part of the industry's adjustment is its release of new commercial product directly via video hosting services. Media convergence is said to be more than technological: the convergence is cultural as well. It is also "the result of a deliberate effort to protect the interests of business entities, policy institutions and other groups". Globalisation and cultural imperialism are two of the cultural consequences of convergence. Others include fandom and interactive storytelling as well as the way that single franchises are distributed through and affect a range of delivery methods. The "greater diversity in the ways that signals may be received and packaged for the viewer, via terrestrial, satellite or cable television, and of course, via the Internet" also affects entertainment venues, such as sports stadia, which now need to be designed so that both live and remote audiences can interact in increasingly sophisticated ways for example, audiences can "watch highlights, call up statistics", "order tickets and merchandise" and generally "tap into the stadium's resources at any time of the day or night".

The introduction of television altered the availability, cost, variety and quality of entertainment products for the public and the convergence of online entertainment is having a similar effect. For example, the possibility and popularity of user-generated content, as distinct from commercial product, creates a "networked audience model [that] makes programming obsolete". Individuals and corporations use video hosting services to broadcast content that is equally accepted by the public as legitimate entertainment.

While technology increases demand for entertainment products and offers increased speed of delivery, the forms that make up the content are in themselves, relatively stable. Storytelling, music, theatre, dance and games are recognisably the same as in earlier centuries.


Ether

In organic chemistry, ethers are a class of compounds that contain an ether group—an oxygen atom connected to two organyl groups (e.g., alkyl or aryl). They have the general formula , where R and R′ represent organyl groups (e.g., alkyl or aryl). Ethers can again be classified into two varieties: if the organyl groups are the same on both sides of the oxygen atom, then it is a simple or symmetrical ether, whereas if they are different, the ethers are called mixed or unsymmetrical ethers. A typical example of the first group is the solvent and anaesthetic diethyl ether, commonly referred to simply as "ether" (). Ethers are common in organic chemistry and even more prevalent in biochemistry, as they are common linkages in carbohydrates and lignin.

Ethers feature bent linkages. In dimethyl ether, the bond angle is 111° and C–O distances are 141 pm. The barrier to rotation about the C–O bonds is low. The bonding of oxygen in ethers, alcohols, and water is similar. In the language of valence bond theory, the hybridization at oxygen is sp.

Oxygen is more electronegative than carbon, thus the alpha hydrogens of ethers are more acidic than those of simple hydrocarbons. They are far less acidic than alpha hydrogens of carbonyl groups (such as in ketones or aldehydes), however.

Ethers can be symmetrical of the type ROR or unsymmetrical of the type ROR'. Examples of the former are dimethyl ether, diethyl ether, dipropyl ether etc. Illustrative unsymmetrical ethers are anisole (methoxybenzene) and dimethoxyethane.

Vinyl- and acetylenic ethers are far less common than alkyl or aryl ethers. Vinylethers, often called enol ethers, are important intermediates in organic synthesis. Acetylenic ethers are especially rare. Di-tert-butoxyacetylene is the most common example of this rare class of compounds.

In the IUPAC Nomenclature system, ethers are named using the general formula ""alkoxyalkane"", for example CH–CH–O–CH is methoxyethane. If the ether is part of a more-complex molecule, it is described as an alkoxy substituent, so –OCH would be considered a ""methoxy-"" group. The simpler alkyl radical is written in front, so CH–O–CHCH would be given as "methoxy"(CHO)"ethane"(CHCH).

IUPAC rules are often not followed for simple ethers. The trivial names for simple ethers (i.e., those with none or few other functional groups) are a composite of the two substituents followed by "ether". For example, ethyl methyl ether (CHOCH), diphenylether (CHOCH). As for other organic compounds, very common ethers acquired names before rules for nomenclature were formalized. Diethyl ether is simply called ether, but was once called "sweet oil of vitriol". Methyl phenyl ether is anisole, because it was originally found in aniseed. The aromatic ethers include furans. Acetals (α-alkoxy ethers R–CH(–OR)–O–R) are another class of ethers with characteristic properties.

Polyethers are generally polymers containing ether linkages in their main chain. The term polyol generally refers to polyether polyols with one or more functional end-groups such as a hydroxyl group. The term "oxide" or other terms are used for high molar mass polymer when end-groups no longer affect polymer properties.

Crown ethers are cyclic polyethers. Some toxins produced by dinoflagellates such as brevetoxin and ciguatoxin are extremely large and are known as "cyclic" or "ladder" polyethers.

The phenyl ether polymers are a class of aromatic polyethers containing aromatic cycles in their main chain: polyphenyl ether (PPE) and poly("p"-phenylene oxide) (PPO).

Many classes of compounds with C–O–C linkages are not considered ethers: Esters (R–C(=O)–O–R′), hemiacetals (R–CH(–OH)–O–R′), carboxylic acid anhydrides (RC(=O)–O–C(=O)R′).

There are compounds which, instead of C in the linkage, contain heavier group 14 chemical elements (e.g., Si, Ge, Sn, Pb). Such compounds are considered ethers as well. Examples of such ethers are silyl enol ethers (containing the linkage), disiloxane (the other name of this compound is disilyl ether, containing the linkage) and stannoxanes (containing the linkage).

Ethers have boiling points similar to those of the analogous alkanes. Simple ethers are generally colorless.

The C-O bonds that comprise simple ethers are strong. They are unreactive toward all but the strongest bases. Although generally of low chemical reactivity, they are more reactive than alkanes. 

Specialized ethers such as epoxides, ketals, and acetals are unrepresentative classes of ethers and are discussed in separate articles. Important reactions are listed below.

Although ethers resist hydrolysis, they are cleaved by hydrobromic acid and hydroiodic acid. Hydrogen chloride cleaves ethers only slowly. Methyl ethers typically afford methyl halides:
These reactions proceed via onium intermediates, i.e. [RO(H)CH]Br.

Some ethers undergo rapid cleavage with boron tribromide (even aluminium chloride is used in some cases) to give the alkyl bromide. Depending on the substituents, some ethers can be cleaved with a variety of reagents, e.g. strong base.

Despite these difficulties the chemical paper pulping processes are based on cleavage of ether bonds in the lignin.

When stored in the presence of air or oxygen, ethers tend to form explosive peroxides, such as diethyl ether hydroperoxide. The reaction is accelerated by light, metal catalysts, and aldehydes. In addition to avoiding storage conditions likely to form peroxides, it is recommended, when an ether is used as a solvent, not to distill it to dryness, as any peroxides that may have formed, being less volatile than the original ether, will become concentrated in the last few drops of liquid. The presence of peroxide in old samples of ethers may be detected by shaking them with freshly prepared solution of a ferrous sulfate followed by addition of KSCN. Appearance of blood red color indicates presence of peroxides. The dangerous properties of ether peroxides are the reason that diethyl ether and other peroxide forming ethers like tetrahydrofuran (THF) or ethylene glycol dimethyl ether (1,2-dimethoxyethane) are avoided in industrial processes.

Ethers serve as Lewis bases. For instance, diethyl ether forms a complex with boron trifluoride, i.e. borane diethyl etherate (). Ethers also coordinate to the Mg center in Grignard reagents. Tetrahydrofuran is more basic than acyclic ethers. It forms with many complexes.

This reactivity is similar to the tendency of ethers with alpha hydrogen atoms to form peroxides. Reaction with chlorine produces alpha-chloroethers.

The dehydration of alcohols affords ethers:

This direct nucleophilic substitution reaction requires elevated temperatures (about 125 °C). The reaction is catalyzed by acids, usually sulfuric acid. The method is effective for generating symmetrical ethers, but not unsymmetrical ethers, since either OH can be protonated, which would give a mixture of products. Diethyl ether is produced from ethanol by this method. Cyclic ethers are readily generated by this approach. Elimination reactions compete with dehydration of the alcohol:

The dehydration route often requires conditions incompatible with delicate molecules. Several milder methods exist to produce ethers.

Alcohols add to electrophilically activated alkenes. The method is atom-economical:
Acid catalysis is required for this reaction. Commericially important ethers prepared in this way are derived from isobutene or isoamylene, which protonate to give relatively stable carbocations. Using ethanol and methanol with these two alkenes, four fuel-grade ethers are produced: methyl tert-butyl ether (MTBE), methyl tert-amyl ether (TAME), ethyl tert-butyl ether (ETBE), and ethyl tert-amyl ether (TAEE).

Solid acid catalysts are typically used to promote this reaction.

Epoxides are typically prepared by oxidation of alkenes. The most important epoxide in terms of industrial scale is ethylene oxide, which is produced by oxidation of ethylene with oxygen. Other epoxides are produced by one of two routes:

Many ethers, ethoxylates and crown ethers, are produced from epoxides.

Nucleophilic displacement of alkyl halides by alkoxides
This reaction, the Williamson ether synthesis, involves treatment of a parent alcohol with a strong base to form the alkoxide, followed by addition of an appropriate aliphatic compound bearing a suitable leaving group (R–X). Although popular in textbooks, the method is usually impractical on scale because it cogenerates significant waste.

Suitable leaving groups (X) include iodide, bromide, or sulfonates. This method usually does not work well for aryl halides (e.g. bromobenzene, see Ullmann condensation below). Likewise, this method only gives the best yields for primary halides. Secondary and tertiary halides are prone to undergo E2 elimination on exposure to the basic alkoxide anion used in the reaction due to steric hindrance from the large alkyl groups.

In a related reaction, alkyl halides undergo nucleophilic displacement by phenoxides. The R–X cannot be used to react with the alcohol. However phenols can be used to replace the alcohol while maintaining the alkyl halide. Since phenols are acidic, they readily react with a strong base like sodium hydroxide to form phenoxide ions. The phenoxide ion will then substitute the –X group in the alkyl halide, forming an ether with an aryl group attached to it in a reaction with an S2 mechanism.

The Ullmann condensation is similar to the Williamson method except that the substrate is an aryl halide. Such reactions generally require a catalyst, such as copper.


