Aberdeenshire

Aberdeenshire (; ) is one of the 32 council areas of Scotland.

It takes its name from the County of Aberdeen, which has substantially different boundaries. The Aberdeenshire Council area includes all of the area of the historic counties of Aberdeenshire and Kincardineshire (except the area making up Aberdeen City council area), as well as part of Banffshire. The county boundaries are officially used for a few purposes, namely land registration and lieutenancy.

Aberdeenshire Council is headquartered at Woodhill House, in Aberdeen, making it the only Scottish council whose headquarters are located outside its jurisdiction. Aberdeen itself forms a different council area (Aberdeen City). Aberdeenshire borders onto Angus and Perth and Kinross to the south, Highland and Moray to the west and Aberdeen City to the east.

Traditionally, it has depended economically on the primary sector (agriculture, fishing, and forestry) and related processing industries. Over the last 40 years, the development of the oil and gas industry and associated service sector has broadened Aberdeenshire's economic base, and contributed to a rapid population growth of some 50% since 1975. Its land represents 8% of Scotland's overall territory. It covers an area of .

Aberdeenshire has a rich prehistoric and historical heritage. It is the locus of a large number of Neolithic and Bronze Age archaeological sites, including Longman Hill, Kempstone Hill, Catto Long Barrow and Cairn Lee. The area was settled in the Bronze Age by the Beaker culture, who arrived from the south around 2000–1800 BC. Stone circles and cairns were constructed predominantly in this era. In the Iron Age, hill forts were built. Around the 1st century AD, the Taexali people, who have left little history, were believed to have resided along the coast. The Picts were the next documented inhabitants of the area and were no later than 800–900 AD. The Romans also were in the area during this period, as they left signs at Kintore. Christianity influenced the inhabitants early on, and there were Celtic monasteries at Old Deer and Monymusk.

Since medieval times, there have been many traditional paths that crossed the Mounth (a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven) through present-day Aberdeenshire from the Scottish Lowlands to the Highlands. Some of the most well-known and historically important trackways are the Causey Mounth and Elsick Mounth.

Aberdeenshire played an important role in the fighting between the Scottish clans. Clan MacBeth and the Clan Canmore were two of the larger clans. Macbeth fell at Lumphanan in 1057. During the Anglo-Norman penetration, other families arrive, such as House of Balliol, Clan Bruce, and Clan Cumming (Comyn). When the fighting amongst these newcomers resulted in the Scottish Wars of Independence, the English king Edward I travelled across the area twice, in 1296 and 1303. In 1307, Robert the Bruce was victorious near Inverurie. Along with his victory came new families, namely the Forbeses and the Gordons.

These new families set the stage for the upcoming rivalries during the 14th and 15th centuries. This rivalry grew worse during and after the Protestant Reformation when religion was another reason for conflict between the clans. The Gordon family adhered to Catholicism and the Forbeses to Protestantism. Aberdeenshire was the historic seat of the clan Dempster. Three universities were founded in the area prior to the 17th century, King's College in Old Aberdeen (1494), Marischal College in Aberdeen (1593), and the University of Fraserburgh (1597).

After the end of the Revolution of 1688, an extended peaceful period was interrupted only by fleeting events such as the Rising of 1715 and the Rising of 1745. The latter resulted in the end of the ascendancy of Episcopalianism and the feudal power of landowners. An era began of increased agricultural and industrial progress. During the 17th century, Aberdeenshire was the location of more fighting, centred on the Marquess of Montrose and the English Civil Wars. This period also saw increased wealth due to the increase in trade with Germany, Poland, and the Low Countries.

The present council area is named after the historic county of Aberdeenshire, which has different boundaries and was abandoned as an administrative area in 1975 under the Local Government (Scotland) Act 1973. It was replaced by Grampian Regional Council and five district councils: Banff and Buchan, Gordon, Kincardine and Deeside, Moray and the City of Aberdeen. Local government functions were shared between the two levels. In 1996, under the Local Government, etc. (Scotland) Act 1994, the Banff and Buchan District, Gordon District, and Kincardine and Deeside District were merged to form the present Aberdeenshire Council area. Moray and the City of Aberdeen were made their own council areas. The present Aberdeenshire Council area consists of all of the historic counties of Aberdeenshire and Kincardineshire (except the area of those two counties making up the City of Aberdeen), as well as the north-east portions of Banffshire.

The population of the council area has risen over 50% since 1971 to approximately , representing 4.7% of Scotland's total. Aberdeenshire's population has increased by 9.1% since 2001, while Scotland's total population grew by 3.8%.
The census lists a relatively high proportion of under 16s and slightly fewer working-age people compared with the Scottish average.

Aberdeenshire is one of the most homogeneous/indigenous regions of the UK. In 2011, 82.2% of residents identified as 'White Scottish', followed by 12.3% who are 'White British', whilst ethnic minorities constitute only 0.9% of the population. The largest ethnic minority group are Asian Scottish/British at 0.8%. In addition to the English language, 48.8% of residents reported being able to speak and understand the Scots language.

The largest settlements in Aberdeenshire are:
Aberdeenshire's Gross Domestic Product (GDP) is estimated at £3,496M (2011), representing 5.2% of the Scottish total. Aberdeenshire's economy is closely linked to Aberdeen City's (GDP £7,906M), and in 2011, the region as a whole was calculated to contribute 16.8% of Scotland's GDP. Between 2012 and 2014, the combined Aberdeenshire and Aberdeen City economic forecast GDP growth rate is 8.6%, the highest growth rate of any local council area in the UK and above the Scottish rate of 4.8%.

A significant proportion of Aberdeenshire's working residents commute to Aberdeen City for work, varying from 11.5% from Fraserburgh to 65% from Westhill.

Average Gross Weekly Earnings (for full-time employees employed in workplaces in Aberdeenshire in 2011) are £572.60. This is lower than the Scottish average by £2.10 and a fall of 2.6% on the 2010 figure. The average gross weekly pay of people resident in Aberdeenshire is much higher, at £741.90, as many people commute out
of Aberdeenshire, principally into Aberdeen City.

Total employment (excluding farm data) in Aberdeenshire is estimated at 93,700 employees (Business Register and
Employment Survey 2009). The majority of employees work within the service sector, predominantly in public administration, education and health. Almost 19% of employment is within the public sector. Aberdeenshire's economy remains closely linked to Aberdeen City's and the North Sea oil industry, with many employees in oil-related jobs.

The average monthly unemployment (claimant count) rate for Aberdeenshire in 2011 was 1.5%. This is lower than the average rate of Aberdeen City (2.3%), Scotland (4.2%) and the UK (3.8%).


The first election to Aberdeenshire Council was held in 1995, initially operating as a shadow authority alongside the outgoing authorities until the new system came into force on 1 April 1996. The council is the fifth largest Scottish council having 70 members.

The council has devolved power to six area committees: Banff and Buchan; Buchan; Formartine; Garioch; Marr; and Kincardine and Mearns. Each area committee takes decisions on local issues such as planning applications, and the split is meant to reflect the diverse circumstances of each area. (Boundary map)

In the 2014 Scottish independence referendum, 60.36% of voters in Aberdeenshire voted for the Union, while 39.64% opted for independence.

Aberdeenshire Council has been under no overall control since its creation:

The leaders of the council since 1996 have been:

Following the 2022 election and subsequent changes of allegiance up to June 2023, the composition of the council was:

Of the independent councillors, eight form the "Administration Independents" group, which forms part the council's administration in coalition with the Conservatives and Liberal Democrats. Two of the independents form the "Democratic Independent Group" which sits with the SNP as the "Opposition Coalition". The remaining independent councillor does not belong to any group. The next election is due in 2027.

The council is based at Woodhill House in Aberdeen, outside the council's own territory. The building was completed in 1977 for the former Grampian Regional Council.

The council has 70 councillors, elected by single transferable vote in 19 multi-member wards:

The following significant structures or places are within Aberdeenshire:


There are numerous rivers and burns in Aberdeenshire, including Cowie Water, Carron Water, Burn of Muchalls, River Dee, River Don, River Ury, River Ythan, Water of Feugh, Burn of Myrehouse, Laeca Burn and Luther Water. Numerous bays and estuaries are found along the seacoast of Aberdeenshire, including Banff Bay, Ythan Estuary, Stonehaven Bay and Thornyhive Bay. Aberdeenshire has a marine west coast climate on the Köppen climate classification. Aberdeenshire is in the rain shadow of the Grampians, therefore it has a generally dry climate for a maritime region, with portions of the coast receiving of moisture annually. Summers are mild, and winters are typically cold in Aberdeenshire; Coastal temperatures are moderated by the North Sea such that coastal areas are typically cooler in the summer and warmer in winter than inland locations. Coastal areas are also subject to haar, or coastal fog.



Aztlan Underground

Aztlan Underground is a band from Los Angeles, California that combines Hip-Hop, Punk Rock, Jazz, and electronic music with Chicano and Native American themes, and indigenous instrumentation. They are often cited as progenitors of Chicano rap.

The band traces its roots to the late-1980s hardcore scene in the Eastside of Los Angeles. They have played rapcore, with elements of punk, hip hop, rock, funk, jazz, indigenous music, and spoken word. Indigenous drums, flutes, and rattles are also commonly used in their music. Their lyrics often address the family and economic issues faced by the Chicano community, and they have been noted as activists for that community.

As an example of the politically active and culturally important artists in Los Angeles in the 1990s, Aztlan Underground appeared on "Culture Clash" on Fox in 1993; and was part of "Breaking Out", a concert on pay per view in 1998, The band was featured in the independent films "Algun Dia" and "Frontierland" in the 1990s, and on the upcoming "Studio 49". The band has been mentioned or featured in various newspapers and magazines: "the Vancouver Sun", "New Times", "BLU Magazine" (an underground hip hop magazine), "BAM Magazine", "La Banda Elastica Magazine", and the "Los Angeles Times" calendar section. The band is also the subject of a chapter in the book "It's Not About a Salary", by Brian Cross.

Aztlan Underground remains active in the community, lending their voice to annual events such as The Farce of July, and the recent movement to recognize Indigenous People's Day in Los Angeles and beyond.

In addition to forming their own label, Xicano Records and Film, Aztlan Underground were signed to the Basque record label Esan Ozenki in 1999 which enabled them to tour Spain extensively and perform in France and Portugal. Aztlan Underground have also performed in Canada, Australia, and Venezuela. The band has been recognized for their music with nominations in the "New Times" 1998 "Best Latin Influenced" category, the "BAM Magazine" 1999 "Best Rock en Español" category, and the "LA Weekly" 1999 "Best Hip Hop" category. The release of their eponymous third album on August 29, 2009 was met with positive reviews and earned the band four Native American Music Award (NAMMY) nominations in 2010.

Year:1995


Year:1998


Year: 2009



American Civil War

The American Civil War (April 12, 1861 – May 26, 1865; also known by other names) was a civil war in the United States between the Union ("the North") and the Confederacy ("the South"), which had been formed by states that had seceded from the Union. The central cause of the war was the dispute over whether slavery would be permitted to expand into the western territories, leading to more slave states, or be prevented from doing so, which many believed would place slavery on a course of ultimate extinction.

Decades of political controversy over slavery were brought to a head by the victory in the 1860 U.S. presidential election of Abraham Lincoln, who opposed slavery's expansion into the western territories. Seven southern slave states responded to Lincoln's victory by seceding from the United States and forming the Confederacy. The Confederacy seized U.S. forts and other federal assets within their borders. The war began when on April 12, 1861, Confederate troops fired on Fort Sumter in South Carolina's Charleston Harbor. A wave of enthusiasm for war swept over both North and South, as recruitment soared. The states in the undecided border region had to choose sides, although Kentucky declared it was neutral. Four more southern states seceded after the war began and, led by Confederate President Jefferson Davis, the Confederacy asserted control over about a third of the U.S. population in eleven states. Four years of intense combat, mostly in the South, ensued.

During 1861–1862 in the Western Theater, the Union made significant permanent gainsthough in the Eastern Theater the conflict was inconclusive. The abolition of slavery became a Union war goal on January 1, 1863, when Lincoln issued the Emancipation Proclamation, which declared all slaves in rebel states to be free, which applied to more than 3.5 million of the 4 million enslaved people in the country. To the west, the Union first destroyed the Confederacy's river navy by the summer of 1862, then much of its western armies, and later seized New Orleans. The successful 1863 Union siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Confederate General Robert E. Lee's incursion north ended at the Battle of Gettysburg. Western successes led to General Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled resources and manpower to attack the Confederacy from all directions. This led to the fall of Atlanta in 1864 to Union General William Tecumseh Sherman, followed by his March to the Sea. The last significant battles raged around the ten-month Siege of Petersburg, gateway to the Confederate capital of Richmond. The Confederates abandoned Richmond, and on April 9, 1865, Lee surrendered to Grant following the Battle of Appomattox Court House, setting in motion the end of the war. Lincoln lived to see this victory but on April 14, he was assassinated.

While the conclusion of the American Civil War arguably has several different dates, Appomattox is often referred to symbolically. It set off a wave of Confederate surrenders. On May 26, the last military department of the Confederacy, the Department of the Trans-Mississippi disbanded. A few small Confederate ground forces continued formal surrenders through June 23. By the end of the war, much of the South's infrastructure was destroyed. The Confederacy collapsed, slavery was abolished, and four million enslaved black people were freed. The war-torn nation then entered the Reconstruction era in an attempt to rebuild the country, bring the former Confederate states back into the United States, and grant civil rights to freed slaves.

The Civil War is one of the most extensively studied and written about episodes in U.S. history. It remains the subject of cultural and historiographical debate. Of particular interest is the persisting myth of the Lost Cause of the Confederacy. The American Civil War was among the first wars to use industrial warfare. Railroads, the telegraph, steamships, the ironclad warship, and mass-produced weapons were all widely used during the war. In total, the war left between 620,000 and 750,000 soldiers dead, along with an undetermined number of civilian casualties, making the Civil War the deadliest military conflict in American history. The technology and brutality of the Civil War foreshadowed the coming World Wars.

The reasons for the Southern states' decisions to secede have been historically controversial, but most scholars today identify preserving slavery as the central reason, in large part because the seceding states' secession documents say that it was. Although some historical revisionists have offered additional reasons for the war, slavery was the central source of escalating political tensions in the 1850s. The Republican Party was determined to prevent any spread of slavery to the territories, which, after they were admitted as free states, would give the free states greater representation in Congress and the Electoral College. Many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to enact pro-slavery laws and policies. In his second inaugural address, Lincoln said that: 

Disagreements among states about the future of slavery were the main cause of disunion and the war that followed. Slavery had been controversial during the framing of the Constitution, which, because of compromises, ended up with proslavery and antislavery features. The issue of slavery had confounded the nation since its inception and increasingly separated the United States into a slaveholding South and a free North. The issue was exacerbated by the rapid territorial expansion of the country, which repeatedly brought to the fore the question of whether new territory should be slaveholding or free. The issue had dominated politics for decades leading up to the war. Key attempts to resolve the matter included the Missouri Compromise and the Compromise of 1850, but these only postponed the showdown over slavery that would lead to the Civil War.

The motivations of the average person were not necessarily those of their faction; some Northern soldiers were indifferent on the subject of slavery, but a general pattern can be established. As the war dragged on, more and more Unionists came to support the abolition of slavery, whether on moral grounds or as a means to cripple the Confederacy. Confederate soldiers fought the war primarily to protect a Southern society of which slavery was an integral part. Opponents of slavery considered slavery an anachronistic evil incompatible with republicanism. The strategy of the anti-slavery forces was containment—to stop the expansion of slavery and thereby put it on a path to ultimate extinction. The slaveholding interests in the South denounced this strategy as infringing upon their constitutional rights. Southern whites believed that the emancipation of slaves would destroy the South's economy, because of the large amount of capital invested in slaves and fears of integrating the ex-slave black population. In particular, many Southerners feared a repeat of the 1804 Haiti massacre (referred to at the time as "the horrors of Santo Domingo"), in which former slaves systematically murdered most of what was left of the country's white population—including men, women, children, and even many sympathetic to abolition—after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea and proposes it contributed to the segregation in the Jim Crow era following emancipation. These fears were exacerbated by the 1859 attempt of John Brown to instigate an armed slave rebellion in the South.

The abolitionists—those advocating the end of slavery—were active in the decades leading up to the Civil War. They traced their philosophical roots back to some of the Puritans, who believed that slavery was morally wrong. One of the early Puritan writings on this subject was "The Selling of Joseph," by Samuel Sewall in 1700. In it, Sewall condemned slavery and the slave trade and refuted many of the era's typical justifications for slavery.

The American Revolution and the cause of liberty added tremendous impetus to the abolitionist cause. Even in Southern states, laws were changed to limit slavery and facilitate manumission. The amount of indentured servitude dropped dramatically throughout the country. An Act Prohibiting Importation of Slaves sailed through Congress with little opposition. President Thomas Jefferson supported it, and it went into effect on January 1, 1808, which was the first day that the Constitution (Article I, section 9, clause 1) permitted Congress to prohibit the importation of slaves. Benjamin Franklin and James Madison each helped found manumission societies. Influenced by the American Revolution, many slave owners freed their slaves, but some, such as George Washington, did so only in their wills. The number of free black people as a proportion of the black population in the upper South increased from less than one percent to nearly 10 percent between 1790 and 1810 as a result of these actions.

The establishment of the Northwest Territory as "free soil"—no slavery—by Manasseh Cutler and Rufus Putnam (who both came from Puritan New England) would also prove crucial. This territory (which became the states of Ohio, Michigan, Indiana, Illinois, Wisconsin and part of Minnesota) virtually doubled the size of the United States. If those states had been slave states and voted for Abraham Lincoln's chief opponent in 1860, Lincoln would not have become president.
In the decades leading up to the Civil War, abolitionists, such as Theodore Parker, Ralph Waldo Emerson, Henry David Thoreau, and Frederick Douglass, repeatedly used the Puritan heritage of the country to bolster their cause. The most radical anti-slavery newspaper, "The Liberator," invoked the Puritans and Puritan values over a thousand times. Parker, in urging New England congressmen to support the abolition of slavery, wrote, "The son of the Puritan ... is sent to Congress to stand up for Truth and Right." Literature served as a means to spread the message to common folks. Key works included "Twelve Years a Slave", the "Narrative of the Life of Frederick Douglass", "American Slavery as It Is", and the most important: "Uncle Tom's Cabin", the best-selling book of the 19th century aside from the Bible.

A more unusual abolitionist than those named above was Hinton Rowan Helper, whose 1857 book, "The Impending Crisis of the South: How to Meet It", "[e]ven more perhaps than "Uncle Tom's Cabin" ... fed the fires of sectional controversy leading up to the Civil War." A Southerner and a virulent racist, Helper was nevertheless an abolitionist because he believed, and showed with statistics, that slavery "impeded the progress and prosperity of the South, ... dwindled our commerce, and other similar pursuits, into the most contemptible insignificance; sunk a large majority of our people in galling poverty and ignorance, ... [and] entailed upon us a humiliating dependence on the Free States..."

By 1840 more than 15,000 people were members of abolitionist societies in the United States. Abolitionism in the United States became a popular expression of moralism, and led directly to the Civil War. In churches, conventions and newspapers, reformers promoted an absolute and immediate rejection of slavery. Support for abolition among the religious was not universal though. As the war approached, even the main denominations split along political lines, forming rival Southern and Northern churches. For example, in 1845 the Baptists split into the Northern Baptists and Southern Baptists over the issue of slavery.

Abolitionist sentiment was not strictly religious or moral in origin. The Whig Party became increasingly opposed to slavery because it saw it as inherently against the ideals of capitalism and the free market. Whig leader William H. Seward (who would serve as Lincoln's secretary of state) proclaimed that there was an "irrepressible conflict" between slavery and free labor, and that slavery had left the South backward and undeveloped. As the Whig party dissolved in the 1850s, the mantle of abolition fell to its newly formed successor, the Republican Party.

Manifest destiny heightened the conflict over slavery. Each new territory acquired had to face the thorny question of whether to allow or disallow the "peculiar institution". Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. Pro- and anti-slavery forces collided over the territories west of the Mississippi River.

The Mexican–American War and its aftermath was a key territorial event in the leadup to the war. As the Treaty of Guadalupe Hidalgo finalized the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well. Prophetically, Ralph Waldo Emerson wrote that "Mexico will poison us", referring to the ensuing divisions around whether the newly conquered lands would end up slave or free. Northern free-soil interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with a stronger federal fugitive slave law for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859), and Kansas (1861). In the Southern states, the question of the territorial expansion of slavery westward again became explosive. Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself." Soon after the Utah Territory legalized slavery in 1852, the Utah War of 1857 saw Mormon settlers in the Utah territory fighting the US government.

By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly. The first of these theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a constitutional mandate. The failed Crittenden Compromise of 1860 was an expression of this view.

The second doctrine of congressional preeminence was championed by Abraham Lincoln and the Republican Party. It insisted that the Constitution did not bind legislators to a policy of balance—that slavery could be excluded in a territory, as it was in the Northwest Ordinance of 1787, at the discretion of Congress. Thus Congress could restrict human bondage, but never establish it. The ill-fated Wilmot Proviso announced this position in 1846. The Proviso was a pivotal moment in national politics, as it was the first time slavery had become a major congressional issue based on sectionalism, instead of party lines. Its support by Northern Democrats and Whigs, and opposition by Southerners, was a dark omen of coming divisions.

Senator Stephen A. Douglas proclaimed the third doctrine: territorial or "popular" sovereignty, which asserted that the settlers in a territory had the same rights as states in the Union to allow or disallow slavery as a purely local matter. The Kansas–Nebraska Act of 1854 legislated this doctrine. In the Kansas Territory, political conflict spawned "Bleeding Kansas", a five-year paramilitary conflict between pro- and anti-slavery supporters. The U.S. House of Representatives voted to admit Kansas as a free state in early 1860, but its admission did not pass the Senate until January 1861, after the departure of Southern senators.

The fourth doctrine was advocated by Mississippi Senator (and soon to be Confederate President) Jefferson Davis. It was one of state sovereignty ("states' rights"), also known as the "Calhoun doctrine", named after the South Carolinian political theorist and statesman John C. Calhoun. Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution. These four doctrines comprised the dominant ideologies presented to the American public on the matters of slavery, the territories, and the U.S. Constitution before the 1860 presidential election.

A long-running dispute over the origin of the Civil War is to what extent states' rights triggered the conflict. The consensus among historians is that the Civil War was fought about states' rights. But the issue is frequently referenced in popular accounts of the war and has much traction among Southerners. Southerners advocating secession argued that just as each state had decided to join the Union, a state had the right to secede—leave the Union—at any time. Northerners (including pro-slavery President Buchanan) rejected that notion as opposed to the will of the Founding Fathers, who said they were setting up a perpetual union.

Historian James McPherson points out that even if Confederates genuinely fought over states' rights, it boiled down to states' right to slavery. McPherson writes concerning states' rights and other non-slavery explanations:

States' rights was an ideology formulated and applied as a means of advancing slave state interests through federal authority. As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of Federal power." Before the Civil War, slavery advocates supported the use of federal powers to enforce and extend slavery, as with the Fugitive Slave Act of 1850 and the "Dred Scott v. Sandford" decision. The faction that pushed for secession often infringed on states' rights. Because of the overrepresentation of pro-slavery factions in the federal government, many Northerners, even non-abolitionists, feared the Slave Power conspiracy. Some Northern states resisted the enforcement of the Fugitive Slave Act. Historian Eric Foner states that the act "could hardly have been designed to arouse greater opposition in the North. It overrode numerous state and local laws and legal procedures and 'commanded' individual citizens to assist, when called upon, in capturing runaways." He continues, "It certainly did not reveal, on the part of slaveholders, sensitivity to states' rights." According to historian Paul Finkelman, "the southern states mostly complained that the northern states were asserting their states' rights and that the national government was not powerful enough to counter these northern claims." The Confederate Constitution also "federally" required slavery to be legal in all Confederate states and claimed territories.

Sectionalism resulted from the different economies, social structure, customs, and political values of the North and South. Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention, which manifested Northern dissatisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence agriculture for poor whites. In the 1840s and 1850s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist, and Presbyterian churches) into separate Northern and Southern denominations.

Historians have debated whether economic differences between the mainly industrial North and the mainly agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s, and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.

Owners of slaves preferred low-cost manual labor with no mechanization. Northern manufacturing interests supported tariffs and protectionism while Southern planters demanded free trade. The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress. The tariff issue was a Northern grievance. However, neo-Confederate writers have claimed it as a Southern grievance. In 1860–61 none of the groups that proposed compromises to head off secession raised the tariff issue. Pamphleteers from the North and the South rarely mentioned the tariff.

Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entirety of the United States (called "Southern Unionists") and those loyal primarily to the Southern region and then the Confederacy.

Perceived insults to Southern collective honor included the enormous popularity of "Uncle Tom's Cabin" and abolitionist John Brown's attempt to incite a slave rebellion in 1859.

While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it. The South ignored the warnings; Southerners did not realize how ardently the North would fight to hold the Union together.

The election of Abraham Lincoln in November 1860 was the final trigger for secession. Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. However, Lincoln would not be inaugurated until five months after the election, which gave the South time to secede and prepare for war in the winter and spring of 1861.

According to Lincoln, the American people had shown that they had been successful in "establishing" and "administering" a republic, but a third challenge faced the nation: "maintaining" a republic based on the people's vote, in the face of an attempt to destroy it.

The election of Lincoln provoked the legislature of South Carolina to call a state convention to consider secession. Before the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws, and even to secede from the United States. The convention unanimously voted to secede on December 20, 1860, and adopted a secession declaration. It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.

Among the ordinances of secession passed by the individual states, those of three—Texas, Alabama, and Virginia—specifically mentioned the plight of the "slaveholding states" at the hands of Northern abolitionists. The rest make no mention of the slavery issue and are often brief announcements of the dissolution of ties by the legislatures. However, at least four states—South Carolina, Mississippi, Georgia, and Texas—also passed lengthy and detailed explanations of their reasons for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement's influence over the politics of the Northern states. The Southern states believed slaveholding was a constitutional right because of the Fugitive Slave Clause of the Constitution. These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861. They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress". One-quarter of the U.S. Army—the entire garrison in Texas—was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.

As Southerners resigned their seats in the Senate and the House, Republicans were able to pass projects that had been blocked by Southern senators before the war. These included the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railroad Acts), the National Bank Act, the authorization of United States Notes by the Legal Tender Act of 1862, and the ending of slavery in the District of Columbia. The Revenue Act of 1861 introduced the income tax to help finance the war.

In December 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of the Southern states, but Lincoln and the Republicans rejected it. Lincoln stated that any compromise that would extend slavery would in time bring down the Union. A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise; it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.

On March 4, 1861, Abraham Lincoln was sworn in as president. In his inaugural address, he argued that the Constitution was a "more perfect union" than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void". He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of federal property, including forts, arsenals, mints, and customhouses that had been seized by the Southern states. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify the armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.

The Davis government of the new Confederacy sent three delegates to Washington to negotiate a peace treaty with the United States of America. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government. Lincoln instead attempted to negotiate directly with the governors of individual seceded states, whose administrations he continued to recognize.

Complicating Lincoln's attempts to defuse the crisis were the actions of the new Secretary of State, William Seward. Seward had been Lincoln's main rival for the Republican presidential nomination. Shocked and embittered by this defeat, Seward agreed to support Lincoln's candidacy only after he was guaranteed the executive office that was considered at that time to be the most powerful and important after the presidency itself. Even in the early stages of Lincoln's presidency Seward still held little regard for the new chief executive due to his perceived inexperience, and therefore Seward viewed himself as the "de facto" head of government or "prime minister" behind the throne of Lincoln. In this role, Seward attempted to engage in unauthorized and indirect negotiations that failed. However, President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy: Fort Monroe in Virginia, Fort Pickens, Fort Jefferson, and Fort Taylor in Florida, and Fort Sumter in South Carolina.

The American Civil War began on April 12, 1861, when Confederate forces opened fire on the Union-held Fort Sumter. Fort Sumter is located in the middle of the harbor of Charleston, South Carolina. Its status had been contentious for months. Outgoing President Buchanan had dithered in reinforcing the Union garrison in the harbor, which was under command of Major Robert Anderson. Anderson took matters into his own hands and on December 26, 1860, under the cover of darkness, sailed the garrison from the poorly placed Fort Moultrie to the stalwart island Fort Sumter. Anderson's actions catapulted him to hero status in the North. An attempt to resupply the fort on January 9, 1861, failed and nearly started the war then and there. But an informal truce held. On March 5, the newly sworn in Lincoln was informed that the Fort was running low on supplies.

Fort Sumter proved to be one of the main challenges of the new Lincoln administration. Back-channel dealing by Secretary of State Seward with the Confederates undermined Lincoln's decision-making; Seward wanted to pull out of the fort. But a firm hand by Lincoln tamed Seward, and Seward became one of Lincoln's staunchest allies. Lincoln ultimately decided that holding the fort, which would require reinforcing it, was the only workable option. Thus, on April 6, Lincoln informed the Governor of South Carolina that a ship with food but no ammunition would attempt to supply the Fort. Historian McPherson describes this win-win approach as "the first sign of the mastery that would mark Lincoln's presidency"; the Union would win if it could resupply and hold onto the Fort, and the South would be the aggressor if it opened fire on an unarmed ship supplying starving men. An April 9 Confederate cabinet meeting resulted in President Davis's ordering General P. G. T. Beauregard to take the Fort before supplies could reach it.

At 4:30 am on April 12, Confederate forces fired the first of 4,000 shells at the Fort; it fell the next day. The loss of Fort Sumter lit a patriotic fire under the North. On April 15, Lincoln called on the states to field 75,000 volunteer troops for 90 days; impassioned Union states met the quotas quickly. On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years. Shortly after this, Virginia, Tennessee, Arkansas, and North Carolina seceded and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.

Maryland, Delaware, Missouri, and Kentucky were slave states whose people had divided loyalties to Northern and Southern businesses and family members. Some men enlisted in the Union Army and others in the Confederate Army. West Virginia separated from Virginia and was admitted to the Union on June 20, 1863.

Maryland's territory surrounded the United States' capital of Washington, D.C., and could cut it off from the North. It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53–13) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war. Lincoln responded by establishing martial law and unilaterally suspending habeas corpus in Maryland, along with sending in militia units from the North. Lincoln rapidly took control of Maryland and the District of Columbia by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened. All were held without trial, with Lincoln ignoring a ruling on June 1, 1861, by U.S. Supreme Court Chief Justice Roger Taney, not speaking for the Court, that only Congress (and not the president) could suspend habeas corpus ("Ex parte Merryman"). Federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring Taney's ruling.

In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state ("see also": Missouri secession). Early in the war the Confederacy controlled the southern portion of Missouri through the Confederate government of Missouri but was largely driven out of the state after 1862. In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.

Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status while maintaining slavery. During a brief invasion by Confederate forces in 1861, Confederate sympathizers and delegates from 68 Kentucky counties organized a secession convention at the Russellville Convention, formed the shadow Confederate Government of Kentucky, inaugurated a governor, and gained recognition from the Confederacy and Kentucky was formally admitted into the Confederacy on December 10, 1861. Its jurisdiction extended only as far as Confederate battle lines in the Commonwealth which at its greatest extent was over half the state, and it went into exile after October 1862.

After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving). Twenty-four secessionist counties were included in the new state, and the ensuing guerrilla war engaged about 40,000 federal troops for much of the war. Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000–22,000 soldiers to both the Confederacy and the Union.

A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.

The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book "The American Civil War", British historian John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". In many cases, without geographic objectives, the only target for each side was the enemy's soldier.

As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias. The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 soldiers for one year or the duration, and that was answered in kind by the U.S. Congress.

In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law—conscription—as a device to encourage or force volunteering; relatively few were drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt. The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.

When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their services conscripted.

In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war. At least 100,000 Southerners deserted, or about 10 percent; Southern desertion was high because, according to one historian writing in 1991, the highly localized Southern identity meant that many Southern men had little investment in the outcome of the war, with individual soldiers caring more about the fate of their local area than any grand ideal. In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.

From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. Some European observers at the time dismissed them as amateur and unprofessional, but historian John Keegan concluded that each outmatched the French, Prussian, and Russian armies of the time, and without the Atlantic, would have threatened any of them with defeat.

At the start of the Civil War, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their army. They were paid, but they were not allowed to perform any military duties. The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.

Historian Elizabeth D. Leonard writes that, according to various estimates, between five hundred and one thousand women enlisted as soldiers on both sides of the war, disguised as men. Women also served as spies, resistance activists, nurses, and hospital personnel. Women served on the Union hospital ship "Red Rover" and nursed Union and Confederate troops at field hospitals.

Mary Edwards Walker, the only woman ever to receive the Medal of Honor, served in the Union Army and was given the medal for her efforts to treat the wounded during the war. Her name was deleted from the Army Medal of Honor Roll in 1917 (along with over 900 other Medal of Honor recipients); however, it was restored in 1977.

The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 sailors in 1865, with 671 vessels, having a tonnage of 510,396. Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy. Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland. The U.S. Navy eventually gained control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers. In the East, the Navy shelled Confederate forts and provided support for coastal army operations.

The Civil War occurred during the early stages of the industrial revolution. Many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries. Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's ironclad warships, they were unsuccessful.

In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.

The Confederacy experimented with the submarine , which did not work satisfactorily, and with building an ironclad ship, , which was based on rebuilding a sunken Union ship, . On its first foray, on March 8, 1862, "Virginia" inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, , arrived to challenge it in the Chesapeake Bay. The resulting three-hour Battle of Hampton Roads was a draw, but it proved that ironclads were effective warships. Not long after the battle, the Confederacy was forced to scuttle the "Virginia" to prevent its capture, while the Union built many copies of the "Monitor". Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Great Britain. However, this failed, because Great Britain had no interest in selling warships to a nation that was at war with a stronger enemy, and doing so could sour relations with the U.S.

By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible, which called for blockading the Confederacy and slowly suffocating the South to surrender. Lincoln adopted parts of the plan, but chose to prosecute a more active vision of war. In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.

The Confederates began the war short on military supplies and in desperate need of large quantities of arms which the agrarian South could not provide. Arms manufactures in the industrial North were restricted by an arms embargo, keeping shipments of arms from going to the South, and ending all existing and future contracts. The Confederacy subsequently looked to foreign sources for their enormous military needs and sought out financiers and companies like S. Isaac, Campbell & Company and the London Armoury Company in Britain, who acted as purchasing agents for the Confederacy, connecting them with Britain's many arms manufactures, and ultimately becoming the Confederacy's main source of arms.

To get the arms safely to the Confederacy, British investors built small, fast, steam-driven blockade runners that traded arms and supplies brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were lightweight and designed for speed and could only carry a relatively small amount of cotton back to England. When the Union Navy seized a blockade runner, the ship and cargo were condemned as a prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British, and they were released.

The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.

Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well. The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade, so they stopped calling at Confederate ports.

To fight an offensive war, the Confederacy purchased arms in Britain and converted British-built ships into commerce raiders. Purchasing arms involved the smuggling of 600,000 arms (mostly British Enfield rifles) that enabled the Confederate Army to fight on for two more years and the commerce raiders were used in raiding U.S. Merchant Marine ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested. After the war ended, the U.S. government demanded that Britain compensate it for the damage done by blockade runners and raiders outfitted in British ports. Britain partly acquiesced to the demand, paying the U.S. $15 million in 1871 only for commerce raiding.

Dinçaslan argues that another outcome of the blockade was oil's rise to prominence as a widely used and traded commodity. The already declining whale oil industry took a blow as many old whaling ships were used in blockade efforts such as the Stone Fleet, and Confederate raiders harassing Union whalers aggravated the situation. Oil products that had been treated mostly as lubricants, especially kerosene, started to replace whale oil used in lamps and essentially became a fuel commodity. This increased the importance of oil as a commodity, long before its eventual use as fuel for combustion engines.

Although the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring the British and French governments in as mediators. The Union, under Lincoln and Seward, worked to block this and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe turned to Egypt and India for cotton, which they found superior, hindering the South's recovery after the war.

Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860–62 crop failures in Europe made the North's grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half. Meanwhile, the war created employment for arms makers, ironworkers, and ships to transport weapons.

Lincoln's administration initially failed to appeal to European public opinion. At first, diplomats explained that the United States was not committed to the ending of slavery, and instead repeated legalistic arguments about the unconstitutionality of secession. Confederate representatives, on the other hand, started off much more successful, by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. The European aristocracy was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic." However, there was still a European public with liberal sensibilities, that the U.S. sought to appeal to by building connections with the international press. As early as 1861, many Union diplomats such as Carl Schurz realized emphasizing the war against slavery was the Union's most effective moral asset in the struggle for public opinion in Europe. Seward was concerned that an overly radical case for reunification would distress the European merchants with cotton interests; even so, Seward supported a widespread campaign of public diplomacy.

U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to openly challenge the Union blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (, , , , , and some others). The most famous, "Alabama", did considerable damage and led to serious postwar disputes. However, public opinion against slavery in Britain created a political liability for British politicians, where the anti-slavery movement was powerful.

War loomed in late 1861 between the U.S. and Britain over the "Trent" affair, which began when U.S. Navy personnel boarded the British ship and seized two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two men. Prince Albert had left his deathbed to issue diplomatic instructions to Lord Lyons during the Trent affair. His request was honored, and, as a result, the British response to the United States was toned down and helped avert the British becoming involved in the war. In 1862, the British government considered mediating between the Union and Confederacy, though even such an offer would have risked war with the United States. British Prime Minister Lord Palmerston reportedly read "Uncle Tom's Cabin" three times when deciding on what his decision would be.

The Union victory in the Battle of Antietam caused the British to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Realizing that Washington could not intervene in Mexico as long as the Confederacy controlled Texas, France invaded Mexico in 1861 and installed the Habsburg Austrian archduke Maximilian I as emperor. Washington repeatedly protested France's violation of the Monroe Doctrine. Despite sympathy for the Confederacy, France's seizure of Mexico ultimately deterred it from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers and ensured that they would remain neutral.

Russia supported the Union, largely because it believed that the U.S. served as a counterbalance to its geopolitical rival, the United Kingdom. In 1863, the Russian Navy's Baltic and Pacific fleets wintered in the American ports of New York and San Francisco, respectively.

The Eastern theater refers to the military operations east of the Appalachian Mountains, including the states of Virginia, West Virginia, Maryland, and Pennsylvania, the District of Columbia, and the coastal fortifications and seaports of North Carolina.

Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26, 1861 (he was briefly general-in-chief of all the Union armies but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. The 1862 Union strategy called for simultaneous advances along four axes:


The primary Confederate force in the Eastern theater was the Army of Northern Virginia. The Army originated as the (Confederate) Army of the Potomac, which was organized on June 20, 1861, from all operational forces in Northern Virginia. On July 20 and 21, the Army of the Shenandoah and forces from the District of Harpers Ferry were added. Units from the Army of the Northwest were merged into the Army of the Potomac between March 14 and May 17, 1862. The Army of the Potomac was renamed "Army of Northern Virginia" on March 14. The Army of the Peninsula was merged into it on April 12, 1862.

When Virginia declared its secession in April 1861, Robert E. Lee chose to follow his home state, despite his desire for the country to remain intact and an offer of a senior Union command.

Lee's biographer, Douglas S. Freeman, asserts that the army received its final name from Lee when he issued orders assuming command on June 1, 1862. However, Freeman does admit that Lee corresponded with Brigadier General Joseph E. Johnston, his predecessor in army command, before that date and referred to Johnston's command as the Army of Northern Virginia. Part of the confusion results from the fact that Johnston commanded the Department of Northern Virginia (as of October 22, 1861) and the name Army of Northern Virginia can be seen as an informal consequence of its parent department's name. Jefferson Davis and Johnston did not adopt the name, but it is clear that the organization of units as of March 14 was the same organization that Lee received on June 1, and thus it is generally referred to today as the Army of Northern Virginia, even if that is correct only in retrospect.

On July 4 at Harper's Ferry, Colonel Thomas J. Jackson assigned Jeb Stuart to command all the cavalry companies of the Army of the Shenandoah. He eventually commanded the Army of Northern Virginia's cavalry.

In one of the first highly visible battles, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces led by Beauregard near Washington was repulsed at the First Battle of Bull Run (also known as First Manassas).

The Union had the upper hand at first, nearly pushing confederate forces holding a defensive position into a rout, but Confederate reinforcements under Joseph E. Johnston arrived from the Shenandoah Valley by railroad, and the course of the battle quickly changed. A brigade of Virginians under the relatively unknown brigadier general from the Virginia Military Institute, Thomas J. Jackson, stood its ground, which resulted in Jackson receiving his famous nickname, "Stonewall".

Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. McClellan's army reached the gates of Richmond in the Peninsula Campaign.

Also in the spring of 1862, in the Shenandoah Valley, Stonewall Jackson led his Valley Campaign. Employing audacity and rapid, unpredictable movements on interior lines, Jackson's 17,000 troops marched 646 miles (1,040 km) in 48 days and won several minor battles as they successfully engaged three Union armies (52,000 men), including those of Nathaniel P. Banks and John C. Fremont, preventing them from reinforcing the Union offensive against Richmond. The swiftness of Jackson's men earned them the nickname of "foot cavalry".

Johnston halted McClellan's advance at the Battle of Seven Pines, but he was wounded in the battle, and Robert E. Lee assumed his position of command. General Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.

The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South. McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.

Emboldened by Second Bull Run, the Confederacy made its first invasion of the North with the Maryland Campaign. General Lee led 45,000 troops of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history. Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.

When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.

Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, his Chancellorsville Campaign proved ineffective, and he was humiliated in the Battle of Chancellorsville in May 1863. Chancellorsville is known as Lee's "perfect battle" because his risky decision to divide his army in the presence of a much larger enemy force resulted in a significant Confederate victory. Gen. Stonewall Jackson was shot in the left arm and right hand by accidental friendly fire during the battle. The arm was amputated, but he died shortly thereafter of pneumonia. Lee famously said: "He has lost his left arm, but I have lost my right arm."

The fiercest fighting of the battle—and the second bloodiest day of the Civil War—occurred on May 3 as Lee launched multiple attacks against the Union position at Chancellorsville. That same day, John Sedgwick advanced across the Rappahannock River, defeated the small Confederate force at Marye's Heights in the Second Battle of Fredericksburg, and then moved to the west. The Confederates fought a successful delaying action at the Battle of Salem Church.

Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863). This was the bloodiest battle of the war and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties, versus Meade's 23,000.

The Western theater refers to military operations between the Appalachian Mountains and the Mississippi River, including the states of Alabama, Georgia, Florida, Mississippi, North Carolina, Kentucky, South Carolina, and Tennessee, as well as parts of Louisiana.

The primary Union forces in the Western theater were the Army of the Tennessee and the Army of the Cumberland, named for the two rivers, the Tennessee River and Cumberland River. After Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.

The primary Confederate force in the Western theater was the Army of Tennessee. The army was formed on November 20, 1862, when General Braxton Bragg renamed the former Army of Mississippi. While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West.

The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry (February 6, 1862) and Donelson (February 11 to 16, 1862), earning him the nickname of "Unconditional Surrender" Grant. With these victories the Union gained control of the Tennessee and Cumberland Rivers. Nathan Bedford Forrest rallied nearly 4,000 Confederate troops and led them to escape across the Cumberland. Nashville and central Tennessee thus fell to the Union, leading to attrition of local food supplies and livestock and a breakdown in social organization.

Leonidas Polk's invasion of Columbus ended Kentucky's policy of neutrality and turned it against the Confederacy. Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Although rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their gunboats, were forced to retreat and the Union took control of western Kentucky and opened Tennessee in March 1862.

At the Battle of Shiloh, in Shiloh, Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counterattacked. Grant and the Union won a decisive victory—the first battle with the high casualty rates that would repeat over and over. The Confederates lost Albert Sidney Johnston, considered their finest general before the emergence of Lee.

One of the early Union objectives in the war was to capture the Mississippi River in order to cut the Confederacy in half. The Mississippi River was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee.

In April 1862, the Union Navy captured New Orleans. "The key to the river was New Orleans, the South's largest port [and] greatest industrial center." U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South, which allowed Union forces to begin moving up the Mississippi. Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.

Bragg's second invasion of Kentucky in the Confederate Heartland Offensive included initial successes such as Kirby Smith's triumph at the Battle of Richmond and the capture of the Kentucky capital of Frankfort on September 3, 1862. However, the campaign ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville. Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of logistical support and lack of infantry recruits for the Confederacy in that state.

Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee, the culmination of the Stones River Campaign.

Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at the Battle of Vicksburg in July 1863, which cemented Union control of the Mississippi River and is considered one of the turning points of the war.

The one clear Confederate victory in the West was the Battle of Chickamauga. After Rosecrans' successful Tullahoma Campaign, Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas.

Rosecrans retreated to Chattanooga, which Bragg then besieged in the Chattanooga Campaign. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga, eventually causing Longstreet to abandon his Knoxville Campaign and driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.

The Trans-Mississippi theater refers to military operations west of the Mississippi River, encompassing most of Missouri, Arkansas, most of Louisiana, and Indian Territory (now Oklahoma). The Trans-Mississippi District was formed by the Confederate Army to better coordinate Ben McCulloch's command of troops in Arkansas and Louisiana, Sterling Price's Missouri State Guard, as well as the portion of Earl Van Dorn's command that included the Indian Territory and excluded the Army of the West. The Union's command was the Trans-Mississippi Division, or the Military Division of West Mississippi.

The first battle of the Trans-Mississippi theater was the Battle of Wilson's Creek (August 1861). The Confederates were driven from Missouri early in the war as a result of the Battle of Pea Ridge.

Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control. Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements. The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged. By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, but Lincoln took 70 percent of the vote for re-election.

Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Battle of Glorieta Pass was the decisive battle of the New Mexico Campaign. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy and smaller numbers for the Union. The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.

After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union, in turn, did not directly engage him. Its 1864 Red River Campaign to take Shreveport, Louisiana, was a failure and Texas remained in Confederate hands throughout the war.

The Lower Seaboard theater refers to military and naval operations that occurred near the coastal areas of the Southeast (Alabama, Florida, Louisiana, Mississippi, South Carolina, and Texas) as well as the southern part of the Mississippi River (Port Hudson and south). Union Naval activities were dictated by the Anaconda Plan.

One of the earliest battles of the war was fought at Port Royal Sound (November 1861), south of Charleston. Much of the war along the South Carolina coast concentrated on capturing Charleston. In attempting to capture Charleston, the Union military tried two approaches: by land over James or Morris Islands or through the harbor. However, the Confederates were able to drive back each Union attack. One of the most famous of the land attacks was the Second Battle of Fort Wagner, in which the 54th Massachusetts Infantry took part. The Union suffered a serious defeat in this battle, losing 1,515 soldiers while the Confederates lost only 174. However, the 54th was hailed for its valor in that battle, which encouraged the general acceptance of the recruitment of African American soldiers into the Union Army, which reinforced the Union's numerical advantage.

Fort Pulaski on the Georgia coast was an early target for the Union navy. Following the capture of Port Royal, an expedition was organized with engineer troops under the command of Captain Quincy A. Gillmore, forcing a Confederate surrender. The Union army occupied the fort for the rest of the war after repairing it.

In April 1862, a Union naval task force commanded by Commander David D. Porter attacked Forts Jackson and St. Philip, which guarded the river approach to New Orleans from the south. While part of the fleet bombarded the forts, other vessels forced a break in the obstructions in the river and enabled the rest of the fleet to steam upriver to the city. A Union army force commanded by Major General Benjamin Butler landed near the forts and forced their surrender. Butler's controversial command of New Orleans earned him the nickname "Beast".

The following year, the Union Army of the Gulf commanded by Major General Nathaniel P. Banks laid siege to Port Hudson for nearly eight weeks, the longest siege in US military history. The Confederates attempted to defend with the Bayou Teche Campaign but surrendered after Vicksburg. These two surrenders gave the Union control over the entire Mississippi.

Several small skirmishes were fought in Florida, but no major battles. The biggest was the Battle of Olustee in early 1864.

The Pacific Coast theater refers to military operations on the Pacific Ocean and in the states and Territories west of the Continental Divide.

At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war. This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end." Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.

Grant's army set out on the Overland Campaign intending to draw Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides and forced Lee's Confederates to fall back repeatedly. At the Battle of Yellow Tavern, the Confederates lost Jeb Stuart.

An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored those they had suffered under prior generals, though, unlike those prior generals, Grant chose to fight on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.

Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. vice president and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war and included a charge by teenage VMI cadets. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.

Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president. Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin–Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.

Leaving Atlanta, and his base of supplies, Sherman's army marched, with no destination set, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia, in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the march. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.

Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee's army and the Confederate government were forced to evacuate. The Confederate capital fell on April 2–3, to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler's Creek on April 6.

Initially, Lee did not intend to surrender but planned to regroup at Appomattox Station, where supplies were to be waiting and then continue the war. Grant chased Lee and got in front of him so that when Lee's army reached the village of Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia to Grant on April 9, 1865, during a conference at the McLean House In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller. His men were paroled, and a chain of Confederate surrenders began.

On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Confederate sympathizer. Lincoln died early the next morning. Lincoln's vice president, Andrew Johnson, was unharmed, because his would-be assassin, George Atzerodt, lost his nerve, so Johnson was immediately sworn in as president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them. On April 26, 1865, the same day Sergeant Boston Corbett killed Booth at a tobacco barn, Johnston surrendered nearly 90,000 troops of the Army of Tennessee to Sherman at Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces. On May 4, all remaining Confederate forces in Alabama, Louisiana east of the Mississippi River, and Mississippi under Lieutenant General Richard Taylor surrendered.

Davis was captured at Irwinville, Georgia on May 10, 1865.

On May 13, 1865, the last land battle of the war was fought at the Battle of Palmito Ranch in Texas.

On May 26, 1865, Confederate Lt. Gen. Simon B. Buckner, acting for Edmund Smith, signed a military convention surrendering the Confederate trans-Mississippi Department forces. This date is often cited by contemporaries and historians as the end date of the American Civil War. On June 2, 1865, with most of his troops having already gone home, technically deserted, a reluctant Kirby Smith had little choice but to sign the official surrender document. On June 23, 1865, Cherokee leader and Confederate Brig. Gen. Stand Watie became the last Confederate general to surrender his forces.

On June 19, 1865, Union Maj. Gen. Gordon Granger announced General Order No. 3, bringing the Emancipation Proclamation into effect in Texas and freeing the last slaves of the Confederacy. The anniversary of this date is now celebrated as Juneteenth.

The naval portion of the war ended more slowly. It had begun on April 11, 1865, two days after Lee's surrender, when President Lincoln proclaimed that foreign nations had no further "claim or pretense" to deny equality of maritime rights and hospitalities to U.S. warships and, in effect, that rights extended to Confederate ships to use neutral ports as safe havens from U.S. warships should end. Having no response to Lincoln's proclamation, President Andrew Johnson issued a similar proclamation dated May 10, 1865, more directly stating the premise that the war was almost at an end ("armed resistance...may be regarded as virtually at an end") and that insurgent cruisers still at sea and prepared to attack U.S. ships should not have rights to do so through use of safe foreign ports or waters and warned nations which continued to do so that their government vessels would be denied access to U.S. ports. He also "enjoined" U.S. officers to arrest the cruisers and their crews so "that they may be prevented from committing further depredations on commerce and that the persons on board of them may no longer enjoy impunity for their crimes". Britain finally responded on June 6, 1865, by transmitting a June 2, 1865 letter from Foreign Secretary John Russell, 1st Earl Russell to the Lords of the Admiralty withdrawing rights to Confederate warships to enter British ports and waters but with exceptions for a limited time to allow a captain to enter a port to "divest his vessel of her warlike character" and for U.S. ships to be detained in British ports or waters to allow Confederate cruisers twenty-four hours to leave first. U.S. Secretary of State Seward welcomed the withdrawal of concessions to the Confederates but objected to the exceptions. Finally, on October 18, 1865, Russell advised the Admiralty that the time specified in his June 2, 1865 message had elapsed and "all measures of a restrictive nature on vessels of war of the United States in British ports, harbors, and waters, are now to be considered as at an end". Nonetheless, the final Confederate surrender was in Liverpool, England where James Iredell Waddell, the captain of CSS "Shenandoah", surrendered the cruiser to British authorities on November 6, 1865.

Legally, the war did not end until August 20, 1866, when President Johnson issued a proclamation that declared "that the said insurrection is at an end and that peace, order, tranquillity, and civil authority now exist in and throughout the whole of the United States of America".

The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich Southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second-class citizenship of the freedmen and their poverty.

Historians have debated whether the Confederacy could have won the war. Most scholars, including James M. McPherson, argue that Confederate victory was at least possible. McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, it would have more easily been able to hold out long enough to exhaust the Union.

Confederates did not need to invade and hold enemy territory to win but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win. Lincoln was not a military dictator and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by outlasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads, who had wanted a negotiated peace with the Confederate States of America.
Some scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat. Civil War historian Shelby Foote expressed this view succinctly: 

A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win." However, most historians reject the argument. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864–65, he says most Confederate soldiers were fighting hard. Historian Gary Gallagher cites General Sherman, who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let-up—some few deserters—plenty tired of war, but the masses determined to fight it out."

Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers. The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Great Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95% effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.

Historian Don Doyle has argued that the Union victory had a major impact on the course of world history. The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:

Scholars have debated what the effects of the war were on political and economic power in the South. The prevailing view is that the southern planter elite retained its powerful position in the South. However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.

The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths—two-thirds by disease—and 50,000 civilians. Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000. A novel way of calculating casualties by looking at the deviation of the death rate of men of fighting age from the norm through analysis of census data found that at least 627,000 and at most 888,000 people, but most likely 761,000 people, died in the war. As historian McPherson notes, the war's "cost in American lives was as great as in all of the nation's other wars combined through Vietnam."

Based on 1860 census figures, 8 percent of all white men aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South. About 56,000 soldiers died in prison camps during the War. An estimated 60,000 soldiers lost limbs in the war.

Of the 359,528 Union Army dead, amounting to 15 percent of the over two million who served:

In addition, there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).

After the Emancipation Proclamation authorized freed slaves to "be received into the armed service of the United States", former slaves who escaped from plantations or were liberated by the Union Army were recruited into the United States Colored Troops regiments of the Union Army, as were black men who had not been slaves. The U.S. Colored Troops made up 10 percent of the Union death toll—15 percent of Union deaths from disease and less than 3 percent of those killed in battle. Losses among African Americans were high. In the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military died during the Civil War. Notably, their mortality rate was significantly higher than that of white soldiers. While 15.2% of United States Volunteers and just 8.6% of white Regular Army troops died, 20.5% of United States Colored Troops died.

The United States National Park Service uses the following figures in its official tally of war losses:

While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, superintendent of the 1870 census, used census and surgeon general data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 census's undercounting, it was later found that the census was only off by 6.5% and that the data Walker used would be roughly accurate.

Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war. This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.

Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in areas where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor Jim Downs states that tens of thousands to hundreds of thousands of slaves died during the war from disease, starvation, or exposure, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.

It is estimated that during the Civil War, of the equines killed, including horses, mules, donkeys and even confiscated children's ponies, over 32,600 of them belonged to the Union Army and 45,800 the Confederacy. However, there are other estimates that place the total equine loss at 1,000,000.

It is estimated that 544 Confederate flags were captured during the Civil War by the Union. The flags were sent to the War Department in Washington. The Union flags captured by the Confederates were sent to Richmond.

Losses were far higher than during the recent war with Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Minié balls, and (near the end of the war for the Union army) repeating firearms such as the Spencer repeating rifle and the Henry repeating rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.

Abolishing slavery was not a Union war goal from the outset, but it quickly became one. Lincoln's initial claims were that preserving the Union was the central goal of the war. In contrast, the South saw itself as fighting to preserve slavery. While not all Southerners saw themselves as fighting for slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery. However, as the war dragged on, and it became clear that slavery was central to the conflict, and that emancipation was (to quote from the Emancipation Proclamation) "a fit and necessary war measure for suppressing [the] rebellion," Lincoln and his cabinet made ending slavery a war goal, culminating in the Emancipation Proclamation. Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans. By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the Northern state of Ohio when they tried to resurrect anti-black sentiment.

Slavery for the Confederacy's 3.5 million blacks effectively ended in each area when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. The last Confederate slaves were freed on June 19, 1865, celebrated as the modern holiday of Juneteenth. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment. The Emancipation Proclamation enabled African Americans, both free blacks and escaped slaves, to join the Union Army. About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.

During the Civil War, sentiment concerning slaves, enslavement, and emancipation in the United States was divided. Lincoln's fears of making slavery a war issue were based on a harsh reality: abolition did not enjoy wide support in the west, the territories, and the border states. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game." Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of the total war needed to save the Union.

At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals Frémont (in Missouri) and David Hunter (in South Carolina, Georgia, and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his plan of gradual compensated emancipation and voluntary colonization was rejected. But compensated emancipation occurred only in the District of Columbia, where Congress had the power to enact it. When Lincoln told his cabinet about his proposed emancipation proclamation, which would apply to the states still in rebellion on January 1, 1863, Seward advised Lincoln to wait for a Union military victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat". Walter Stahr, however, writes, "There are contemporary sources, however, that suggest others were involved in the decision to delay", and Stahr quotes them.
Lincoln laid the groundwork for public support in an open letter published in response to Horace Greeley's "The Prayer of Twenty Millions". He also laid the groundwork at a meeting at the White House with five African American representatives on August 14, 1862. Arranging for a reporter to be present, he urged his visitors to agree to the voluntary colonization of black people, apparently to make his forthcoming preliminary Emancipation Proclamation more palatable to racist white people. A Union victory in the Battle of Antietam on September 17, 1862, provided Lincoln with an opportunity to issue the preliminary Emancipation Proclamation, and the subsequent War Governors' Conference added support for the proclamation.

Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862. It stated that the slaves in all states in rebellion on January 1, 1863, would be free. He issued his final Emancipation Proclamation on January 1, 1863, keeping his promise. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong ... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling ... I claim not to have controlled events, but confess plainly that events have controlled me."

Lincoln's moderate approach succeeded in inducing the border states to remain in the Union and War Democrats to support the Union. The border states, which included Kentucky, Missouri, Maryland, Delaware, and Union-controlled regions around New Orleans, Norfolk, Virginia, and elsewhere, were not covered by the Emancipation Proclamation. Nor was Tennessee, which had come under Union control. Missouri and Maryland abolished slavery on their own; Kentucky and Delaware did not. Still, the proclamation did not enjoy universal support. It caused much unrest in what were then considered western states, where racist sentiments led to a great fear of abolition. There was some concern that the proclamation would lead to the secession of western states, and its issuance prompted the stationing of Union troops in Illinois in case of rebellion.

Since the Emancipation Proclamation was based on the President's war powers, it applied only in territory held by Confederates at the time it was issued. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. The Emancipation Proclamation greatly reduced the Confederacy's hope of being recognized or otherwise aided by Britain or France. By late 1864, Lincoln was playing a leading role in getting the House of Representatives to vote for the Thirteenth Amendment to the United States Constitution, which mandated the ending of chattel slavery.

The war devastated the South and posed serious questions of how the South would be reintegrated into the Union. The war destroyed much of the wealth that had existed in the South, in part because wealth held in enslaved people (valued at a minimum of $1,000 each for a healthy adult prior to the war) was wiped off the books. All accumulated investment in Confederate bonds was forfeited; most banks and railroads were bankrupt. The income per capita in the South dropped to less than 40 percent of that of the North, an economic condition that lasted into the 20th century. Southern influence in the federal government, previously considerable, was greatly diminished until the latter half of the 20th century. Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877. It comprised multiple complex methods to resolve the outstanding issues of the war's aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution: the 13th outlawing slavery (1865), the 14th guaranteeing citizenship to former slaves (1868), and the 15th ensuring voting rights to former male slaves (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union, to guarantee a "republican form of government" for the ex-Confederate states, and to permanently end slavery—and prevent semi-slavery status.

President Johnson, who took office on April 15, 1865, took a lenient approach and saw the achievement of the main war goals as realized in 1865 when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They overrode Johnson's vetoes of civil rights legislation, and the House impeached him, although the Senate did not convict him. In 1868 and 1872, the Republican candidate Grant won the presidency. In 1872, the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They chose Horace Greeley to head a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed further reconstruction. The Compromise of 1877 closed with a national consensus, except perhaps on the part of former slaves, that the Civil War had finally ended. With the withdrawal of federal troops, however, whites retook control of every Southern legislature, and the Jim Crow era of disenfranchisement and legal segregation was ushered in.

The Civil War had a demonstrable impact on American politics in the years to come. Many veterans on both sides were subsequently elected to political office, including five U.S. Presidents: General Ulysses Grant, Rutherford B. Hayes, James Garfield, Benjamin Harrison, and William McKinley.

The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books, and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war. The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.

Professional historians have paid much more attention to the causes of the war than to the war itself. Military history has largely developed outside academia, leading to a proliferation of studies by non-scholars who nevertheless are familiar with the primary sources and pay close attention to battles and campaigns and who write for the general public. Bruce Catton and Shelby Foote are among the best known. Practically every major figure in the war, both North and South, has had a serious biographical study.

Even the name used for the conflict has been controversial, with many names for the American Civil War. During and immediately after the war, Northern historians often used a term like "War of the Rebellion". Writers in rebel states often referred to the "War for Southern Independence". More recently, some Southerners have described it as the "War of Northern Aggression".

The memory of the war in the white South crystallized in the myth of the "Lost Cause": that the Confederate cause was just and heroic. The myth shaped regional identity and race relations for generations. Alan T. Nolan notes that the Lost Cause was expressly a rationalization, a cover-up to vindicate the name and fame of those in rebellion. Some claims revolve around the insignificance of slavery as a cause of the war; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful. Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing black American progress to white man's reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance. The Lost Cause myth was formalized by Charles A. Beard and Mary R. Beard, whose "The Rise of American Civilization" (1927) spawned "Beardian historiography". The Beards downplayed slavery, abolitionism, and issues of morality. Though this interpretation was abandoned by the Beards in the 1940s, and by historians generally by the 1950s, Beardian themes still echo among Lost Cause writers.

The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861. The oldest surviving monument is the Hazen Brigade Monument near Murfreesboro in Central Tennessee, built in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead following the Battle of Stones River. 

In the 1890s, the U.S. government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park at Fort Oglethorpe, Georgia, and the Antietam National Battlefield in Sharpsburg, Maryland, in 1890. The Shiloh National Military Park was established in 1894 in Shiloh, Tennessee, followed by the Gettysburg National Military Park in Gettysburg, Pennsylvania in 1895, and Vicksburg National Military Park in Vicksburg, Mississippi, in 1899. 

In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service. Chief among modern efforts to preserve Civil War sites has been the American Battlefield Trust, with more than 130 battlefields in 24 states. The five major Civil War battlefield parks operated by the National Park Service (Gettysburg, Antietam, Shiloh, Chickamauga/Chattanooga and Vicksburg) had a combined 3.1 million visitors in 2018, down 70% from 10.2 million in 1970.

The American Civil War has been commemorated in many capacities, ranging from the reenactment of battles to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. These commemorations occurred in greater numbers on the 100th and 150th anniversaries of the war.
Hollywood's take on the war has been especially influential in shaping public memory, as in such film classics as "The Birth of a Nation" (1915), "Gone with the Wind" (1939), and "Lincoln" (2012). Ken Burns's PBS television series "The Civil War" (1990) is especially well-remembered, though criticized for its historical inaccuracy.

Numerous technological innovations during the Civil War had a great impact on 19th-century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war. New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel. It was also in this war that aerial warfare, in the form of reconnaissance balloons, was first used. It saw the first action involving steam-powered ironclad warships in naval warfare history. Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare. The war also saw the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.

The Civil War is one of the most studied events in American history, and the collection of cultural works around it is enormous. This section gives an abbreviated overview of the most notable works.

















Andy Warhol

Andy Warhol (; born Andrew Warhola Jr.; August 6, 1928 – February 22, 1987) was an American visual artist, film director, producer, and leading figure in the pop art movement. His works explore the relationship between artistic expression, advertising, and celebrity culture that flourished by the 1960s, and span a variety of media, including painting, silkscreening, photography, film, and sculpture. Some of his best-known works include the silkscreen paintings "Campbell's Soup Cans" (1962) and "Marilyn Diptych" (1962), the experimental films "Empire" (1964) and "Chelsea Girls" (1966), and the multimedia events known as the "Exploding Plastic Inevitable" (1966–67).

Born and raised in Pittsburgh, Warhol initially pursued a successful career as a commercial illustrator. After exhibiting his work in several galleries in the late 1950s, he began to receive recognition as an influential and controversial artist. His New York studio, The Factory, became a well-known gathering place that brought together distinguished intellectuals, drag queens, playwrights, Bohemian street people, Hollywood celebrities and wealthy patrons. He promoted a collection of personalities known as Warhol superstars, and is credited with inspiring the widely used expression "15 minutes of fame".

In the late 1960s, he managed and produced the experimental rock band the Velvet Underground and founded "Interview". He authored numerous books, including "The Philosophy of Andy Warhol" and "". He lived openly as a gay man before the gay liberation movement. In June 1968, he was almost killed by radical feminist Valerie Solanas, who shot him inside his studio. After gallbladder surgery, Warhol died of cardiac arrhythmia in February 1987 at the age of 58 in New York.

Warhol has been the subject of numerous retrospective exhibitions, books and feature and documentary films. The Andy Warhol Museum in his native city of Pittsburgh, which holds an extensive permanent collection of art and archives, is the largest museum in the United States dedicated to a single artist. Warhol has been described as the "bellwether of the art market". Many of his creations are very collectible and highly valuable. His works include some of the most expensive paintings ever sold. In 2013, a 1963 serigraph titled "Silver Car Crash (Double Disaster)" sold for $105 million. In 2022, "Shot Sage Blue Marilyn" (1964) sold for $195 million, which is the most expensive work of art sold at auction by an American artist.

Warhol was born on August 6, 1928, in Pittsburgh, Pennsylvania. He was the fourth child of Ondrej Warhola (Americanized as Andrew Warhola Sr.; 1889–1942) and Julia ("née" Zavacká, 1891–1972), whose first child was born in their homeland of Austria-Hungary and died before their move to the US.
His parents were working-class Lemko emigrants from Mikó, Austria-Hungary (now called Miková, located in today's northeastern Slovakia). Warhol's father emigrated to the United States in 1914, and his mother joined him in 1921, after the death of Warhol's grandparents. Warhol's father worked in a coal mine. The family lived at 55 Beelen Street and later at 3252 Dawson Street in the Oakland neighborhood of Pittsburgh. The family was Ruthenian Catholic and attended St. John Chrysostom Byzantine Catholic Church. Warhol had two elder brothers—Pavol (Paul), the eldest, was born before the family emigrated; John was born in Pittsburgh. Pavol's son, James Warhola, became a successful children's book illustrator.

In third grade, Warhol had Sydenham's chorea (also known as St. Vitus' Dance), the nervous system disease that causes involuntary movements of the extremities, which is believed to be a complication of scarlet fever which causes skin pigmentation blotchiness. At times when he was confined to bed, he drew, listened to the radio and collected pictures of movie stars around his bed. Warhol later described this period as very important in the development of his personality, skill-set and preferences. When Warhol was 13, his father died in an accident.

As a teenager, Warhol graduated from Schenley High School in 1945, and also won a Scholastic Art and Writing Award. After graduating from high school, he enrolled in the Carnegie Institute of Technology, now Carnegie Mellon University in Pittsburgh, where he studied commercial art. During his time there, Warhol joined the campus Modern Dance Club and Beaux Arts Society. He also served as art director of the student art magazine, "Cano", illustrating a cover in 1948 and a full-page interior illustration in 1949. These are believed to be his first two published artworks. Warhol earned a Bachelor of Fine Arts in pictorial design in 1949. Later that year, he moved to New York City and began a career in magazine illustration and advertising.

Warhol's early career was dedicated to commercial and advertising art, where his first commission had been to draw shoes for "Glamour" magazine in the late 1940s. In the 1950s, Warhol worked as a designer for shoe manufacturer Israel Miller. While working in the shoe industry, Warhol developed his "blotted line" technique, applying ink to paper and then blotting the ink while still wet, which was akin to a printmaking process on the most rudimentary scale. His use of tracing paper and ink allowed him to repeat the basic image and also to create endless variations on the theme. American photographer John Coplans recalled that "nobody drew shoes the way Andy did. He somehow gave each shoe a temperament of its own, a sort of sly, Toulouse-Lautrec kind of sophistication, but the shape and the style came through accurately and the buckle was always in the right place. The kids in the apartment [which Andy shared in New York – note by Coplans] noticed that the vamps on Andy's shoe drawings kept getting longer and longer but [Israel] Miller didn't mind. Miller loved them."

In 1952, Warhol had his first solo show at the Hugo Gallery in New York, and although that show was not well received, by 1956, he was included in his first group exhibition at the Museum of Modern Art, New York. Warhol's "whimsical" ink drawings of shoe advertisements figured in some of his earliest showings at the Bodley Gallery in New York in 1957.

Warhol habitually used the expedient of tracing photographs projected with an epidiascope. Using prints by Edward Wallowitch, his "first boyfriend", the photographs would undergo a subtle transformation during Warhol's often cursory tracing of contours and hatching of shadows. Warhol used Wallowitch's photograph "Young Man Smoking a Cigarette" (), for a 1958 design for a book cover he submitted to Simon and Schuster for the Walter Ross pulp novel "The Immortal", and later used others for his series of paintings.

With the rapid expansion of the record industry, RCA Records hired Warhol, along with another freelance artist, Sid Maurer, to design album covers and promotional materials.

Warhol was an early adopter of the silk screen printmaking process as a technique for making paintings. In 1961 and 1962, Warhol resided at a 1342 Lexington Avenue apartment/art studio. In 1962, Warhol was taught silk screen printmaking techniques by Max Arthur Cohn at his graphic arts business in Manhattan. In his book "", Warhol writes: "When you do something exactly wrong, you always turn up something".

In May 1962, Warhol was featured in an article in "Time" with his painting "Big Campbell's Soup Can with Can Opener (Vegetable)" (1962), which initiated his most sustained motif, the Campbell's soup can. That painting became Warhol's first to be shown in a museum when it was exhibited at the Wadsworth Atheneum in Hartford in July 1962. On July 9, 1962, Warhol's exhibition opened at the Ferus Gallery in Los Angeles with "Campbell's Soup Cans", marking his West Coast debut of pop art.

In November 1962, Warhol had an exhibition at Eleanor Ward's Stable Gallery in New York. The exhibit included the works "Gold Marilyn", eight of the classic "Marilyn" series also named ""Flavor Marilyns"", "Marilyn Diptych", "100 Soup Cans", "100 Coke Bottles", and "100 Dollar Bills". "Gold Marilyn", was bought by the architect Philip Johnson and donated to the Museum of Modern Art. At the exhibit, Warhol met poet John Giorno, who would star in Warhol's first film, "Sleep" (1964).
In December 1962, New York City's Museum of Modern Art hosted a symposium on pop art, during which artists such as Warhol were attacked for "capitulating" to consumerism. Critics were appalled by Warhol's open acceptance of market culture, which set the tone for his reception.

In early 1963, Warhol rented his first studio, an old firehouse at 159 East 87th Street. At this studio, he created his "Elvis" series, which included "Eight Elvises" (1963) and "Triple Elvis" (1963). These portraits along with a series of Elizabeth Taylor portraits were shown at his second exhibition at the Ferus Gallery in Los Angeles. Later that year, Warhol relocated his studio to East 47th Street, which would turn into The Factory. The Factory became a popular gathering spot for a wide range of artists, writers, musicians and underground celebrities.

Warhol had his second exhibition at the Stable Gallery in the spring of 1964, which featured sculptures of commercial boxes stacked and scattered throughout the space to resemble a warehouse. For the exhibition, Warhol custom ordered wooden boxes and silkscreened graphics onto them. The sculptures—"Brillo Box", "Del Monte Peach Box", "Heinz Tomato Ketchup Box", "Kellogg's Cornflakes Box", "Campbell's Tomato Juice Box" and "Mott's Apple Juice Box"—sold for $200 to $400 depending on the size of the box.

A pivotal event was "The American Supermarket" exhibition at Paul Bianchini's Upper East Side gallery in the fall of 1964. The show was presented as a typical small supermarket environment, except that everything in it—from the produce, canned goods, meat, posters on the wall, etc.—was created by prominent pop artists of the time, among them sculptor Claes Oldenburg, Mary Inman and Bob Watts. Warhol designed a $12 paper shopping bag—plain white with a red Campbell's soup can. His painting of a can of a Campbell's soup cost $1,500 while each autographed can sold for three for $18, $6.50 each. The exhibit was one of the first mass events that directly confronted the general public with both pop art and the perennial question of what art is.

In 1967 Warhol established Factory Additions for his printmaking and publishing enterprise.

As an advertisement illustrator in the 1950s, Warhol used assistants to increase his productivity. Collaboration would remain a defining (and controversial) aspect of his working methods throughout his career; this was particularly true in the 1960s. One of the most important collaborators during this period was Gerard Malanga. Malanga assisted the artist with the production of silkscreens, films, sculpture and other works at The Factory, Warhol's aluminum foil-and-silver-paint-lined studio on 47th Street (later moved to Broadway). Other members of Warhol's Factory crowd included Freddie Herko, Ondine, Ronald Tavel, Mary Woronov, Billy Name and Brigid Berlin (from whom he apparently got the idea to tape-record his phone conversations).

During the 1960s, Warhol also groomed a retinue of bohemian and counterculture eccentrics upon whom he bestowed the designation "superstars", including Nico, Joe Dallesandro, Edie Sedgwick, Viva, Ultra Violet, Holly Woodlawn, Jackie Curtis and Candy Darling. These people all participated in the Factory films, and some—like Berlin—remained friends with Warhol until his death. Important figures in the New York underground art/cinema world, such as writer John Giorno and film-maker Jack Smith, also appear in Warhol films (many premiering at the New Andy Warhol Garrick Theatre and 55th Street Playhouse) of the 1960s, revealing Warhol's connections to a diverse range of artistic scenes during this time. Less well known was his support and collaboration with several teenagers during this era, who would achieve prominence later in life, including writer David Dalton, photographer Stephen Shore and artist Bibbe Hansen (mother of pop musician Beck).

On June 3, 1968, radical feminist writer Valerie Solanas shot Warhol and Mario Amaya, art critic and curator, at The Factory. Before the shooting, Solanas had been a marginal figure in the Factory scene. She authored the "SCUM Manifesto", a separatist feminist tract that advocated the elimination of men; and appeared in the 1968 Warhol film "I, a Man". Earlier on the day of the attack, Solanas had been turned away from the Factory after asking for the return of a script she had given to Warhol. The script had apparently been misplaced.

Amaya received only minor injuries and was released from the hospital later the same day. Warhol was seriously wounded by the attack and barely survived; he was released after nearly two months. He had physical effects for the rest of his life, including being required to wear a surgical corset. The shooting had a profound effect on Warhol's life and art.

Solanas was arrested the day after the assault, after turning herself in to police. By way of explanation, she said that Warhol "had too much control over my life". She was subsequently diagnosed with paranoid schizophrenia and eventually sentenced to three years under the control of the Department of Corrections. After the shooting, the Factory scene heavily increased its security, and for many the "Factory 60s" ended ("The superstars from the old Factory days didn't come around to the new Factory much").

Warhol had this to say about the attack:

In 1969, Warhol and British journalist John Wilcock founded "Interview".

In 1970, screens and film matrixes that had been used to produce original Warhol works in the 1960s were taken to Europe for the production of Warhol screenprints under the name "Sunday B Morning". Warhol signed and numbered one edition of 250 before subsequent unauthorized unsigned versions were produced. The unauthorized works were the result of a falling out between Warhol and some of his New York City studio employees who went to Brussels where they produced work stamped with "Sunday B Morning" and "Add Your Own Signature Here". Since the works began as a collaboration, Warhol facilitated exact duplication by providing the photo negatives and precise color codes. Some of the unauthorized productions bore the markings "This is not by me, Andy Warhol". The most famous unauthorized reproductions are 1967 Marilyn Monroe portfolio screenprints. These "Sunday B Morning" Marilyn Monroe prints were among those still under production as of 2013. Art galleries and dealers also market Sunday B Morning reprint versions of several other screenprint works including "Flowers", "Campbell's Soup I", "Campbell's Soup Cans II","Gold Marilyn Monroe" Mao and Dollare bill prints. Although the original Sunday B Morning versions had black stamps on the back, by the 1980s, they switched to blue.

Warhol had a retrospective exhibition at the Whitney Museum of American Art in 1971. His famous portrait of Chinese Communist leader Mao Zedong was created in 1973. In 1975, he published "The Philosophy of Andy Warhol" (1975). An idea expressed in the book: "Making money is art, and working is art and good business is the best art."

Compared to the success and scandal of Warhol's work in the 1960s, the 1970s were a much quieter decade, as he became more entrepreneurial. He socialized at various nightspots in New York City, including Max's Kansas City and, later in the 1970s, Studio 54. He was generally regarded as quiet, shy and a meticulous observer. Art critic Robert Hughes called him "the white mole of Union Square". In 1977, Warhol was commissioned by art collector Richard Weisman to create "Athletes", ten portraits consisting of the leading athletes of the day.

According to Bob Colacello, Warhol devoted much of his time to rounding up new, rich patrons for portrait commissions—including Shah of Iran Mohammad Reza Pahlavi, his wife Empress Farah Pahlavi, his sister Princess Ashraf Pahlavi, Mick Jagger, Liza Minnelli, John Lennon, Diana Ross and Brigitte Bardot. In 1979, reviewers disliked his exhibits of portraits of 1970s personalities and celebrities, calling them superficial, facile and commercial, with no depth or indication of the significance of the subjects. In 1979, Warhol and his longtime friend Stuart Pivar founded the New York Academy of Art.

Warhol had a re-emergence of critical and financial success in the 1980s, partially due to his affiliation and friendships with a number of prolific younger artists, who were dominating the "bull market" of 1980s New York art: Jean-Michel Basquiat, Julian Schnabel, David Salle and other so-called Neo-Expressionists, as well as members of the Transavantgarde movement in Europe, including Francesco Clemente and Enzo Cucchi. Warhol also earned street credibility and graffiti artist Fab Five Freddy paid homage to Warhol by painting an entire train with Campbell soup cans.

Warhol was also being criticized for becoming merely a "business artist". Critics panned his 1980 exhibition "Ten Portraits of Jews of the Twentieth Century" at the Jewish Museum in Manhattan, which Warhol—who was uninterested in Judaism and Jews—had described in his diary as "They're going to sell." In hindsight, however, some critics have come to view Warhol's superficiality and commerciality as "the most brilliant mirror of our times", contending that "Warhol had captured something irresistible about the zeitgeist of American culture in the 1970s."

In 1981, Warhol worked on a project with Peter Sellars and Lewis Allen that would create a traveling stage show called, "A No Man Show", with a life-sized animatronic robot in the exact image of Warhol. The "Andy Warhol Robot" would then be able to read Warhol's diaries as a theatrical production. The play would be based on Warhol's books "The Philosophy of Andy Warhol" and "Exposures". Warhol was quoted as saying, "I'd like to be a machine, wouldn't you?"

Warhol also had an appreciation for intense Hollywood glamour. He once said: "I love Los Angeles. I love Hollywood. They're so beautiful. Everything's plastic, but I love plastic. I want to be plastic." Warhol occasionally walked the fashion runways and did product endorsements, represented by Zoli Agency and later Ford Models.

Before the 1984 Sarajevo Winter Olympics, he teamed with 15 other artists, including David Hockney and Cy Twombly, and contributed a Speed Skater print to the Art and Sport collection. The Speed Skater was used for the official Sarajevo Winter Olympics poster.

In 1984, "Vanity Fair" commissioned Warhol to produce a portrait of Prince, in order to accompany an article that celebrated the success of "Purple Rain" and its accompanying movie. Referencing the many celebrity portraits produced by Warhol across his career, "Orange Prince (1984)" was created using a similar composition to the Marilyn "Flavors" series from 1962, among some of Warhol's first celebrity portraits. Prince is depicted in a pop color palette commonly used by Warhol, in bright orange with highlights of bright green and blue. The facial features and hair are screen-printed in black over the orange background.

In September 1985, Warhol's joint exhibition with Basquiat, "Paintings", opened to negative reviews at the Tony Shafrazi Gallery. That month, despite apprehension from Warhol, his silkscreen series "Reigning Queens" was shown at the Leo Castelli Gallery. In the "Andy Warhol Diaries", Warhol wrote: "They were supposed to be only for Europe—nobody here cares about royalty and it'll be another bad review."

In January 1987, Warhol traveled to Milan for the opening of his last exhibition, "Last Supper", at the Palazzo delle Stelline. The next month, Warhol and jazz musician Miles Davis modeled for Koshin Satoh's fashion show at the Tunnel in New York City on February 17, 1987.

Warhol died in Manhattan at 6:32 a.m. on February 22, 1987, at age 58. According to news reports, he had been making a good recovery from gallbladder surgery at New York Hospital before dying in his sleep from a sudden post-operative irregular heartbeat. Prior to his diagnosis and operation, Warhol delayed having his recurring gallbladder problems checked, as he was afraid to enter hospitals and see doctors. His family sued the hospital for inadequate care, saying that the arrhythmia was caused by improper care and water intoxication. The malpractice case was quickly settled out of court; Warhol's family received an undisclosed sum of money.

Shortly before Warhol's death, doctors expected Warhol to survive the surgery, though a re-evaluation of the case about thirty years after his death showed many indications that Warhol's surgery was in fact riskier than originally thought. It was widely reported at the time that Warhol had died of a "routine" surgery, though when considering factors such as his age, a family history of gallbladder problems, his previous gunshot wound, and his medical state in the weeks leading up to the procedure, the potential risk of death following the surgery appeared to have been significant.

Warhol's brothers took his body back to Pittsburgh, where an open-coffin wake was held at the Thomas P. Kunsak Funeral Home. The solid bronze casket had gold-plated rails and white upholstery. Warhol was dressed in a black cashmere suit, a paisley tie, a platinum wig, and sunglasses. He was laid out holding a small prayer book and a red rose. The funeral liturgy was held at the Holy Ghost Byzantine Catholic Church on Pittsburgh's North Side on February 27, 1987. The eulogy was given by Monsignor Peter Tay. Yoko Ono and John Richardson were speakers. The coffin was covered with white roses and asparagus ferns.

After the liturgy, the coffin was driven to St. John the Baptist Byzantine Catholic Cemetery in Bethel Park, a south suburb of Pittsburgh, where Warhol was buried near his parents. The priest said a brief prayer at the graveside and sprinkled holy water on the casket. Before the coffin was lowered, Warhol's close friend and associate publisher of "Interview", Paige Powell, dropped a copy of the magazine and a bottle of Beautiful Eau de Parfum by Estée Lauder into the grave. A memorial service was held in Manhattan for Warhol at St. Patrick's Cathedral on April 1, 1987.

By the beginning of the 1960s, pop art was an experimental form that several artists were independently adopting; some of these pioneers, such as Roy Lichtenstein, would later become synonymous with the movement. Warhol, who would become famous as the "Pope of Pop", turned to this new style, where popular subjects could be part of the artist's palette. His early paintings show images taken from cartoons and advertisements, hand-painted with paint drips. Those drips emulated the style of successful abstract expressionists such as Willem de Kooning.

From these beginnings, he developed his later style and subjects. Instead of working on a signature subject matter, as he started out to do, he worked more and more on a signature style, slowly eliminating the handmade from the artistic process. Warhol frequently used silk-screening; his later drawings were traced from slide projections. At the height of his fame as a painter, Warhol had several assistants, including notable artist George Condo, who produced his silk-screen multiples, following his directions to make different versions and variations.

Warhol's first pop art paintings were displayed in April 1961, serving as the backdrop for New York Department Store Bonwit Teller's window display. This was the same stage his Pop Art contemporaries Jasper Johns, James Rosenquist and Robert Rauschenberg had also once graced. It was the gallerist Muriel Latow who came up with the ideas for both the soup cans and Warhol's dollar paintings. On November 23, 1961, Warhol wrote Latow a check for $50 which, according to the 2009 Warhol biography, "Pop, The Genius of Warhol", was payment for coming up with the idea of the soup cans as subject matter. For his first major exhibition, Warhol painted his famous cans of Campbell's soup, which he claimed to have had for lunch for most of his life.

It was during the 1960s that Warhol began to make paintings of iconic American objects such as dollar bills, mushroom clouds, electric chairs, Campbell's soup cans, Coca-Cola bottles, celebrities such as Marilyn Monroe, Elvis Presley and Elizabeth Taylor, as well as newspaper headlines or photographs of police dogs attacking African-American protesters during the Birmingham campaign in the civil rights movement. His work became popular and controversial. Warhol had this to say about Coca-Cola: In 1962, Warhol created his famous "Marilyn" series. The Flavor Marilyns were selected from a group of fourteen canvases in the sub-series, each measuring 20" x 16". Some of the canvases were named after various candy Life Savers flavors, including "Cherry Marilyn", "Lemon Marilyn" and "Licorice Marilyn". The others are identified by their background colors.

Warhol produced both comic and serious works; his subject could be a soup can or an electric chair. Warhol used the same techniques—silkscreens, reproduced serially, and often painted with bright colors—whether he painted celebrities, everyday objects, or images of suicide, car crashes and disasters, as in the 1962–63 "Death and Disaster" series.

In 1979, Warhol was commissioned to paint a BMW M1 Group 4 racing version for the fourth installment of the BMW Art Car project. He was initially asked to paint a BMW 320i in 1978, but the car model was changed and it didn't qualify for the race that year. Warhol was the first artist to paint directly onto the automobile himself instead of letting technicians transfer a scale-model design to the car. Reportedly, it took him only 23 minutes to paint the entire car. Racecar drivers Hervé Poulain, Manfred Winkelhock and Marcel Mignot drove the car at the 1979 24 Hours of Le Mans.

Some of Warhol's work, as well as his own personality, has been described as being Keatonesque. Warhol has been described as playing dumb to the media. He sometimes refused to explain his work. He has suggested that all one needs to know about his work is "already there 'on the surface.

His Rorschach inkblots are intended as pop comments on art and what art could be. His cow wallpaper (literally, wallpaper with a cow motif) and his oxidation paintings (canvases prepared with copper paint that was then oxidized with urine) are also noteworthy in this context. Equally noteworthy is the way these works—and their means of production—mirrored the atmosphere at Andy's New York "Factory". Biographer Bob Colacello provides some details on Andy's "piss paintings":

Warhol's 1982 portrait of Basquiat, "Jean-Michel Basquiat", is a silkscreen over an oxidized copper "piss painting". After many years of silkscreen, oxidation, photography, etc., Warhol returned to painting with a brush in hand. In 1983, Warhol began collaborating with Basquiat and Clemente. Warhol and Basquiat created a series of more than 50 large collaborative works between 1984 and 1985. Despite criticism when these were first shown, Warhol called some of them "masterpieces", and they were influential for his later work.

In 1984, Warhol was commissioned by collector and gallerist Alexander Iolas to produce work based on Leonardo da Vinci's "The Last Supper" for an exhibition at the old refectory of the Palazzo delle Stelline in Milan, opposite from the Santa Maria delle Grazie where Leonardo da Vinci's mural can be seen. Warhol exceeded the demands of the commission and produced nearly 100 variations on the theme, mostly silkscreens and paintings, and among them a collaborative sculpture with Basquiat, the "Ten Punching Bags (Last Supper)". The Milan exhibition that opened in January 1987 with a set of 22 silk-screens, was the last exhibition for both the artist and the gallerist. The series of "The Last Supper" was seen by some as "arguably his greatest", but by others as "wishy-washy, religiose" and "spiritless". It is the largest series of religious-themed works by any American artist.

Artist Maurizio Cattelan describes that it is difficult to separate daily encounters from the art of Andy Warhol: "That's probably the greatest thing about Warhol: the way he penetrated and summarized our world, to the point that distinguishing between him and our everyday life is basically impossible, and in any case useless." Warhol was an inspiration towards Cattelan's magazine and photography compilations, such as "Permanent Food, Charley", and "Toilet Paper".

In the period just before his death, Warhol was working on "Cars", a series of paintings for Mercedes-Benz.

"Though he is often associated with printmaking—specifically silkscreen—Warhol was also an incredibly talented illustrator and draughtsman, and drawing was an integral part of his practice throughout his career. His early drawings on paper bare a resemblance to both continuous line and blind contour drawing techniques, giving his work a sense of ease and immediacy. While working primarily within commercial advertisement, he pioneered the blotted line technique, which synthesized graphite drawing on paper with elements of printmaking. Warhol continued his practice of drawing through the last years of his life and career, and the work from this later period exemplifies a long and storied career's worth of honed skill and technique."

The value of Andy Warhol's work has been on an endless upward trajectory since his death in 1987. In 2014, his works accumulated $569 million at auction, which accounted for more than a sixth of the global art market. However, there have been some dips. According to art dealer Dominique Lévy: "The Warhol trade moves something like a seesaw being pulled uphill: it rises and falls, but each new high and low is above the last one." She attributes this to the consistent influx of new collectors intrigued by Warhol. "At different moments, you've had different groups of collectors entering the Warhol market, and that resulted in peaks in demand, then satisfaction and a slow down," before the process repeats another demographic or the next generation.

In 1998, "Orange Marilyn" (1964), a depiction of Marilyn Monroe, sold for $17.3 million, which at the time set a new record as the highest price paid for a Warhol artwork. In 2007, one of Warhol's 1963 paintings of Elizabeth Taylor, "Liz (Colored Liz)", which was owned by actor Hugh Grant, sold for $23.7 million at Christie's.

In 2007, Stefan Edlis and Gael Neeson sold Warhol's "Turquoise Marilyn" (1964) to financier Steven A. Cohen for $80 million. In May 2007, "Green Car Crash" (1963) sold for $71.1 million and "Lemon Marilyn" (1962) sold for $28 million at Christie's post-war and contemporary art auction. In 2007, "Large Campbell's Soup Can" (1964) was sold at a Sotheby's auction to a South American collector for 7.4 million. In November 2009, "200 One Dollar Bills" (1962) at Sotheby's for $43.8 million.

In 2008, "Eight Elvises" (1963) was sold by Annibale Berlingieri for $100 million to a private buyer. The work depicts Elvis Presley in a gunslinger pose. It was first exhibited in 1963 at the Ferus Gallery in Los Angeles. Warhol made 22 versions of the "Elvis" portraits, eleven of which are held in museums. In May 2012, "Double Elvis (Ferus Type)" sold at auction at Sotheby's for $37 million. In November 2014, "Triple Elvis (Ferus Type)" sold for $81.9 million at Christie's.

In May 2010, a purple self-portrait of Warhol from 1986 that was owned by fashion designer Tom Ford sold for $32.6 million at Sotheby's. In November 2010, "Men in Her Life" (1962), based on Elizabeth Taylor, sold for $63.4 million at Phillips de Pury and "Coca-Cola (4)" (1962) sold for $35.3 million at Sotheby's. In May 2011, Warhol's first self-portrait from 1963 to 1964 sold for $38.4 million and a red self-portrait from 1986 sold for $27.5 million at Christie's. In May 2011, "Liz #5 (Early Colored Liz)" sold for $26.9 million at Phillips.

In November 2013, Warhol's rarely seen 1963 diptych, "Silver Car Crash (Double Disaster)", sold at Sotheby's for $105.4 million, a new record for the artist. In November 2013, "Coca-Cola (3)" (1962) sold for $57.3 million at Christie's. In May 2014, "White Marilyn" (1962) sold for $41 million at Christie's. In November 2014, "Four Marlons" (1964), which depicts Marlon Brando, sold for $69.6 million at Christie's. In May 2015, "Silver Liz (diptych)", painted in 1963, sold for $28 million and "Colored Mona Lisa" (1963) sold for $56.2 million at Christie's. In May 2017, Warhol's 1962 painting "Big Campbell's Soup Can With Can Opener (Vegetable)" sold for $27.5 million at Christie's. In 2017, billionaire hedge-fund manager Ken Griffin purchased "Orange Marilyn" privately for around $200 million. In March 2022, "Silver Liz (Ferus Type)" sold for 2.3 billion yen ($18.9 million) at Shinwa Auction, which set a new record for the highest bid ever at auction in Japan. In May 2022, "Shot Sage Blue Marilyn" (1964) sold for $195 million at Christie's, becoming the most expensive American artwork sold at auction.

Among Warhol's early collectors and influential supporters were Emily and Burton Tremaine. Among the over 15 artworks purchased, "Marilyn Diptych" (now at Tate Modern, London) and "A boy for Meg" (now at the National Gallery of Art in Washington, DC), were purchased directly out of Warhol's studio in 1962. One Christmas, Warhol left a small "Head of Marilyn Monroe" by the Tremaine's door at their New York apartment in gratitude for their support and encouragement.

Warhol attended the 1962 premiere of the static composition by La Monte Young called "Trio for Strings" and subsequently created his famous series of static films. Filmmaker Jonas Mekas, who accompanied Warhol to the Trio premiere, claims Warhol's static films were directly inspired by the performance. Between 1963 and 1968, he made more than 60 films, plus some 500 short black-and-white "screen test" portraits of Factory visitors.

One of his most famous films, "Sleep", monitors poet John Giorno sleeping for six hours. The 35-minute film "Blow Job" is one continuous shot of the face of DeVeren Bookwalter supposedly receiving oral sex from filmmaker Willard Maas, although the camera never tilts down to see this. Another, "Empire" (1964), consists of eight hours of footage of the Empire State Building in New York City at dusk. The film "Eat" consists of a man eating a mushroom for 45 minutes.

"Batman Dracula" is a 1964 film that was produced and directed by Warhol, without the permission of DC Comics. It was screened only at his art exhibits. A fan of the "Batman" series, Warhol's movie was an "homage" to the series, and is considered the first appearance of a blatantly campy Batman. The film was until recently thought to have been lost, until scenes from the picture were shown at some length in the 2006 documentary "Jack Smith and the Destruction of Atlantis".

Warhol's 1965 film "Vinyl" is an adaptation of Anthony Burgess' popular dystopian novel "A Clockwork Orange". Others record improvised encounters between Factory regulars such as Brigid Berlin, Viva, Edie Sedgwick, Candy Darling, Holly Woodlawn, Ondine, Nico and Jackie Curtis. The underground artist Jack Smith appears in the film "Camp".

His most popular and critically successful film was "Chelsea Girls" (1966). The film was highly innovative in that it consisted of two 16 mm-films being projected simultaneously, with two different stories being shown in tandem. From the projection booth, the sound would be raised for one film to elucidate that "story" while it was lowered for the other. The multiplication of images evoked Warhol's seminal silk-screen works of the early 1960s.

Warhol was a fan of filmmaker Radley Metzger film work and commented that Metzger's film, "The Lickerish Quartet", was "an outrageously kinky masterpiece". "Blue Movie"—a film in which Warhol superstar Viva makes love in bed with Louis Waldon, another Warhol superstar—was Warhol's last film as director. The film, a seminal film in the Golden Age of Porn, was, at the time, controversial for its frank approach to a sexual encounter. "Blue Movie" was publicly screened in New York City in 2005, for the first time in more than 30 years.

In the wake of the 1968 shooting, a reclusive Warhol relinquished his personal involvement in film making. His acolyte and assistant director, Paul Morrissey, took over the film-making chores for the Factory collective, steering Warhol-branded cinema towards more mainstream, narrative-based, B-movie exploitation fare with "Flesh", "Trash" and "Heat". All of these films, including the later "Andy Warhol's Dracula" and "Andy Warhol's Frankenstein", were far more mainstream than anything Warhol as a director had attempted. These latter "Warhol" films starred Joe Dallesandro, more of a Morrissey star than a true Warhol superstar.

In the early 1970s, most of the films directed by Warhol were pulled out of circulation by Warhol and the people around him who ran his business. After Warhol's death, the films were slowly restored by the Whitney Museum and are occasionally projected at museums and film festivals. Few of the Warhol-directed films are available on video or DVD.

In the mid-1960s, Warhol adopted the band the Velvet Underground, making them a crucial element of the "Exploding Plastic Inevitable" multimedia performance art show. Warhol, with Paul Morrissey, acted as the band's manager, introducing them to Nico (who would perform with the band at Warhol's request). While managing The Velvet Underground, Andy would have them dressed in all black to perform in front of movies that he was also presenting. In 1966, he "produced" their first album "The Velvet Underground & Nico", as well as providing its album art. His actual participation in the album's production amounted to simply paying for the studio time.

After the band's first album, Warhol and band leader Lou Reed started to disagree more about the direction the band should take, and their artistic friendship ended. In 1989, after Warhol's death, Reed and John Cale re-united for the first time since 1972 to write, perform, record and release the concept album "Songs for Drella", a tribute to Warhol. In October 2019, an audio tape of publicly unknown music by Reed, based on Warhols' 1975 book, ""The Philosophy of Andy Warhol: From A to B and Back Again"", was reported to have been discovered in an archive at the Andy Warhol Museum in Pittsburgh.

Warhol designed many album covers for various artists starting with the photographic cover of John Wallowitch's debut album, "This Is John Wallowitch!!!" (1964). He designed the cover art for The Rolling Stones' albums "Sticky Fingers" (1971) and "Love You Live" (1977), and the John Cale albums "The Academy in Peril" (1972) and "Honi Soit" in 1981. One of Warhol's last works was a portrait of Aretha Franklin for the cover of her 1986 gold album "Aretha".

In 1984, Warhol co-directed the music video "Hello Again" by the Cars, and he appeared in the video as a bartender. In 1986, Warhol co-directed the music video "Misfit" by Curiosity Killed the Cat and he made a cameo in video.

Beginning in the early 1950s, Warhol produced several unbound portfolios of his work.

The first of several bound self-published books by Warhol was "25 Cats Name Sam and One Blue Pussy", printed in 1954 by Seymour Berlin on Arches brand watermarked paper using his blotted line technique for the lithographs. The original edition was limited to 190 numbered, hand-colored copies, using Dr. Martin's ink washes. Most of these were given by Warhol as gifts to clients and friends. Copy No. 4, inscribed "Jerry" on the front cover and given to Geraldine Stutz, was used for a facsimile printing in 1987, and the original was auctioned in May 2006 for US$35,000 by Doyle New York.

Other self-published books by Warhol include:

Warhol's book "A La Recherche du Shoe Perdu" (1955) marked his "transition from commercial to gallery artist". (The title is a play on words by Warhol on the title of French author Marcel Proust's "À la recherche du temps perdu".)

After gaining fame, Warhol "wrote" several books that were commercially published:

Warhol created the fashion magazine "Interview" that is still published. The loopy title script on the cover is thought to be either his own handwriting or that of his mother, Julia Warhola, who would often do text work for his early commercial pieces.

Although Andy Warhol is most known for his paintings and films, he authored works in many different media.



In 1980, he told an interviewer that he was still a virgin. Biographer Bob Colacello, who was present at the interview, felt it was probably true and that what little sex he had was probably "a mixture of voyeurism and masturbation—to use [Andy's] word "abstract"". Warhol's assertion of virginity would seem to be contradicted by his hospital treatment in 1960 for condylomata, a sexually transmitted disease. It has also been contradicted by his lovers, including Warhol muse BillyBoy, who has said they had sex to orgasm: "When he wasn't being Andy Warhol and when you were just alone with him he was an incredibly generous and very kind person. What seduced me was the Andy Warhol who I saw alone. In fact when I was with him in public he kind of got on my nerves...I'd say: 'You're just obnoxious, I can't bear you.'"

Billy Name also denied that Warhol was only a voyeur, saying: "He was the essence of sexuality. It permeated everything. Andy exuded it, along with his great artistic creativity...It brought a joy to the whole art world in New York." "But his personality was so vulnerable that it became a defense to put up the blank front." Warhol's lovers included John Giorno, Billy Name, Charles Lisanby, and Jon Gould. Gould was the most photographed subject of Warhol's later career and worked as an American film executive at Paramount Pictures where he was responsible for films including John Travolta's "Urban Cowboy". His boyfriend of 12 years was Jed Johnson, whom he met in 1968, and who later achieved fame as an interior designer.

The fact that Warhol's homosexuality influenced his work and shaped his relationship to the art world is a major subject of scholarship on the artist and is an issue that Warhol himself addressed in interviews, in conversation with his contemporaries, and in his publications ("e.g.", "Popism: The Warhol 1960s"). Throughout his career, Warhol produced erotic photography and drawings of male nudes. Many of his most famous works (portraits of Liza Minnelli, Judy Garland, and Elizabeth Taylor and films such as "Blow Job", "My Hustler" and "Lonesome Cowboys") draw from gay underground culture or openly explore the complexity of sexuality and desire. As has been addressed by a range of scholars, many of his films premiered in gay porn theaters, including the New Andy Warhol Garrick Theatre and 55th Street Playhouse, in the late 1960s.

The first works that Warhol submitted to a fine art gallery, homoerotic drawings of male nudes, were rejected for being too openly gay. In "Popism", furthermore, the artist recalls a conversation with the filmmaker Emile de Antonio about the difficulty Warhol had being accepted socially by the then-more-famous (but closeted) gay artists Jasper Johns and Robert Rauschenberg. De Antonio explained that Warhol was "too swish and that upsets them". In response to this, Warhol writes, "There was nothing I could say to that. It was all too true. So I decided I just wasn't going to care, because those were all the things that I didn't want to change anyway, that I didn't think I 'should' want to change ... Other people could change their attitudes but not me". In exploring Warhol's biography, many turn to this period—the late 1950s and early 1960s—as a key moment in the development of his persona.

Some have suggested that his frequent refusal to comment on his work, to speak about himself (confining himself in interviews to responses like "Um, no" and "Um, yes", and often allowing others to speak for him)—and even the evolution of his pop style—can be traced to the years when Warhol was first dismissed by the inner circles of the New York art world.

Warhol was a practising Ruthenian Catholic. He regularly volunteered at homeless shelters in New York City, particularly during the busier times of the year, and described himself as a religious person. Many of Warhol's later works depicted religious subjects, including two series, "Details of Renaissance Paintings" (1984) and "The Last Supper" (1986). In addition, a body of religious-themed works was found posthumously in his estate.

Warhol regularly attended Mass, and the priest at Warhol's church, Saint Vincent Ferrer, said that the artist went there almost daily, although he was not observed taking Communion or going to Confession and sat or knelt in the pews at the back. The priest thought he was afraid of being recognized; Warhol said he was self-conscious about being seen in a Latin Catholic church crossing himself "in the Orthodox way" (right to left instead of the reverse).

Warhol's art is noticeably influenced by the Eastern Christian tradition which was so evident in his places of worship. Warhol's brother has described the artist as "really religious, but he didn't want people to know about that because [it was] private". Despite the private nature of his faith, in Warhol's eulogy John Richardson depicted it as devout: "To my certain knowledge, he was responsible for at least one conversion. He took considerable pride in financing his nephew's studies for the priesthood".

Warhol was an avid collector. His friends referred to his numerous collections, which filled not only his four-story townhouse, but also a nearby storage unit, as "Andy's Stuff". The true extent of his collections was not discovered until after his death, when The Andy Warhol Museum in Pittsburgh took in 641 boxes of his "Stuff".

Warhol's collections included a Coca-Cola memorabilia sign, and 19th century paintings along with airplane menus, unpaid invoices, pizza dough, pornographic pulp novels, newspapers, stamps, supermarket flyers and cookie jars, among other eccentricities. It also included significant works of art, such as George Bellows's "Miss Bentham". One of his main collections was his wigs. Warhol owned more than 40 and felt very protective of his hairpieces, which were sewn by a New York wig-maker from hair imported from Italy. In 1985, a girl snatched Warhol's wig off his head. It was later discovered in Warhol's diary entry for that day that he wrote: "I don't know what held me back from pushing her over the balcony."

In 1960, he had bought a drawing of a light bulb by Jasper Johns. Another item found in Warhol's boxes at the museum in Pittsburgh was a mummified human foot from Ancient Egypt. The curator of anthropology at Carnegie Museum of Natural History felt that Warhol most likely found it at a flea market.

Warhol collected many books, with more than 1,200 titles in his collection. Of these, 139 titles have been publicly identified through a 1988 Sotheby's Auction catalog, "The Andy Warhol Collection" and can be viewed online. His book collection reflects his eclectic taste and interests, and includes books written by and about some of his acquaintances and friends. Some of the titles in his collection include "The Two Mrs. Grenvilles: A Novel" by Dominick Dunne, "Artists in Uniform" by Max Eastman, "Andrews' Diseases of the Skin: Clinical Dermatology" by George Clinton Andrews, "D.V." by Diana Vreeland, "Blood of a Poet" by Jean Cocteau, "Watercolours" by Francesco Clemente, "Little World, Hello!" by Jimmy Savo, "Hidden Faces" by Salvador Dalí and "The Dinah Shore Cookbook".

In 2002, the US Postal Service issued an 18-cent stamp commemorating Warhol. Designed by Richard Sheaff of Scottsdale, Arizona, the stamp was unveiled at a ceremony at The Andy Warhol Museum and features Warhol's painting "Self-Portrait, 1964". In March 2011, a chrome statue of Andy Warhol and his Polaroid camera was revealed at Union Square in New York City.

A crater on Mercury was named after Warhol in 2012.

In 2013, to honor the 85th anniversary of Warhol's birthday, The Andy Warhol Museum and EarthCam launched a collaborative project titled "Figment", a live feed of Warhol's gravesite.

Warhol's will dictated that his entire estate—with the exception of a few modest legacies to family members—would go to create a foundation dedicated to the "advancement of the visual arts". Warhol had so many possessions that it took Sotheby's nine days to auction his estate after his death; the auction grossed more than US$20 million.

In 1987, in accordance with Warhol's will, the Andy Warhol Foundation for the Visual Arts began. The foundation serves as the estate of Andy Warhol, but also has a mission "to foster innovative artistic expression and the creative process" and is "focused primarily on supporting work of a challenging and often experimental nature".

The Artists Rights Society is the US copyright representative for the Andy Warhol Foundation for the Visual Arts for all Warhol works with the exception of Warhol film stills. The US copyright representative for Warhol film stills is the Warhol Museum in Pittsburgh. Additionally, the Andy Warhol Foundation for the Visual Arts has agreements in place for its image archive. All digital images of Warhol are exclusively managed by Corbis, while all transparency images of Warhol are managed by Art Resource.

The Andy Warhol Foundation released its "20th Anniversary Annual Report" as a three-volume set in 2007: Vol. I, 1987–2007; Vol. II, Grants & Exhibitions; and Vol. III, Legacy Program.

The Foundation is in the process of compiling its catalogue raisonné of paintings and sculptures in volumes covering blocks of years of the artist's career. Volumes IV and V were released in 2019. The subsequent volumes are still in the process of being compiled.

The Foundation remains one of the largest grant-giving organizations for the visual arts in the US.

Many of Warhol's works and possessions are on display at the Andy Warhol Museum in Pittsburgh. The foundation donated more than 3,000 works of art to the museum.

From November 19, 2021 – June 19, 2022, the Brooklyn Museum displayed the "Andy Warhol: Revelation" exhibition. "Revelation" examines themes such as life and death, power and desire, the role and representation of women, Renaissance imagery, family and immigrant traditions and rituals, depictions and duplications of Christ and the Catholic body and queer desire. Among the more than one hundred objects on view were rare source materials and newly discovered items that provide a fresh and intimate look at Warhol's creative process, as well as major paintings from his epic "Last Supper" series (1986), the experimental film "The Chelsea Girls" (1966), an unfinished film depicting the setting sun commissioned by the de Menil family and funded by the Roman Catholic Church and drawings created by Warhol's mother, Julia Warhola, when she lived with her son in New York City.

Warhol founded "Interview", a stage for celebrities he "endorsed" and a business staffed by his friends. He collaborated with others on all of his books (some of which were written with Pat Hackett.) One might even say that he produced people (as in the Warholian "Superstar" and the Warholian portrait). Warhol endorsed products, appeared in commercials, and made frequent celebrity guest appearances on television shows and in films (he appeared in everything from "Love Boat" to "Saturday Night Live", as well as the Richard Pryor movie "Dynamite Chicken").

In this respect Warhol was a fan of "Art Business" and "Business Art"—he, in fact, wrote about his interest in thinking about art as business in "The Philosophy of Andy Warhol from A to B and Back Again".

Warhol appeared as himself in the film "Cocaine Cowboys" (1979) and in the film "Tootsie" (1982).

After his death, Warhol was portrayed by Crispin Glover in Oliver Stone's film "The Doors" (1991), by Jared Harris in Mary Harron's film "I Shot Andy Warhol" (1996) and by David Bowie in Julian Schnabel's film "Basquiat" (1996). Bowie recalled how meeting Warhol in real life helped him in the role, and recounted his early meetings with him:

Warhol appears as a character in Michael Daugherty's opera "Jackie O" (1997). Actor Mark Bringleson makes a brief cameo as Warhol in "" (1997). Many films by avant-garde cineast Jonas Mekas have caught the moments of Warhol's life. Sean Gregory Sullivan depicted Warhol in the film "54" (1998). Guy Pearce portrayed Warhol in the film "Factory Girl" (2007) about Edie Sedgwick's life. Actor Greg Travis portrays Warhol in a brief scene from the film "Watchmen" (2009). Comedian Conan O'Brien portrayed Warhol in the film "" (2022).

In the movie "Highway to Hell" a group of Andy Warhols are part of the "Good Intentions Paving Company" where good-intentioned souls are ground into pavement. In the film "Men in Black 3" (2012) Andy Warhol turns out to really be undercover MIB Agent W (played by Bill Hader). Warhol is throwing a party at The Factory in 1969, where he is looked up by MIB Agents K and J (J from the future). Agent W is desperate to end his undercover job ("I'm so out of ideas I'm painting soup cans and bananas, for Christ sakes!", "You gotta fake my death, okay? I can't listen to sitar music anymore." and "I can't tell the women from the men.").

Andy Warhol (portrayed by Tom Meeten) is one of main characters of the 2012 British television show "Noel Fielding's Luxury Comedy". The character is portrayed as having robot-like mannerisms. In the 2017 feature "The Billionaire Boys Club", Cary Elwes portrays Warhol in a film based on the true story about Ron Levin (portrayed by Kevin Spacey) a friend of Warhol's who was murdered in 1986. In September 2016, it was announced that Jared Leto would portray the title character in "Warhol", an upcoming American biographical drama film produced by Michael De Luca and written by Terence Winter, based on the book "Warhol: The Biography" by Victor Bockris.


Warhol appeared as a recurring character in TV series "Vinyl", played by John Cameron Mitchell. Warhol was portrayed by Evan Peters in the "" episode "". The episode depicts the attempted assassination of Warhol by Valerie Solanas (Lena Dunham).

In early 1969, Andy Warhol was commissioned by Braniff International to appear in two television commercials to promote the luxury airline's "When You Got It – Flaunt It" campaign. The campaign was created by the advertising agency Lois Holland Calloway, which was led by George Lois, creator of a famed series of "Esquire" covers. The first commercial series involved pairing unlikely people who shared the fact that they both flew Braniff Airways. Warhol was paired with boxing legend Sonny Liston. The odd commercial worked, as did the others that featured unlikely fellow travelers, such as painter Salvador Dalí and baseball legend Whitey Ford.

Two additional commercials for Braniff were created that featured famous persons entering a Braniff jet and being greeted by a Braniff hostess, while espousing their like for flying Braniff. Warhol was also featured in the first of these commercials that were also produced by Lois and were released in the summer of 1969. Lois has incorrectly stated that he was commissioned by Braniff in 1967 for representation during that year, but at that time Madison Avenue advertising doyenne Mary Wells Lawrence, who was married to Braniff's chairman and president Harding Lawrence, was representing the Dallas-based carrier at that time. Lois succeeded Wells Rich Greene Agency on December 1, 1968. The rights to Warhol's films for Braniff and his signed contracts are owned by a private trust and are administered by Braniff Airways Foundation in Dallas, Texas.

Warhol strongly influenced the new wave/punk rock band Devo, as well as David Bowie. Bowie recorded a song called "Andy Warhol" for his 1971 album "Hunky Dory". Lou Reed wrote the song "Andy's Chest", about Valerie Solanas, the woman who shot Warhol, in 1968. He recorded it with the Velvet Underground, and this version was released on the "VU" album in 1985. The band Triumph also wrote a song about Andy Warhol, "Stranger In A Strange Land" off their 1984 album "Thunder Seven".

A biography of Andy Warhol written by art critic Blake Gopnik was published in 2020 under the title "Warhol".

Warhol is featured as a character in the "Miracleman" series of comics. It is first mentioned that he was resurrected by the alien scientist Mors and subsequently convinces the latter to mass-produce copies of himself. Later on, 18 copies of Warhol are seen in the underworld beneath the pyramid structure Olympus, where they produce pop art relating to the new superhuman regime. One Warhol clone numbered 6 is assigned to and develop a friendship with a clone of Emil Gargunza (Miracleman's creator) before the latter's betrayal and attempted escape.

Warhol (played by Jeff Grace) makes a cameo appearance in the 2022 video game "Immortality".




Alp Arslan

Alp Arslan, born Muhammad bin Dawud Chaghri, was the second sultan of the Seljuk Empire and great-grandson of Seljuk, the eponymous founder of the dynasty. He greatly expanded the Seljuk territory and consolidated his power, defeating rivals to the south and northwest, and his victory over the Byzantines at the Battle of Manzikert, in 1071, ushered in the Turkmen settlement of Anatolia.

Muhammad bin Dawud Chaghri's military prowess and fighting skills earned him the nickname "Alp Arslan", which means "Heroic Lion" in Turkish.

Historical sources differ about his actual birth date. His birth year, which some early sources of medieval period mentioned 1032 and 1033 in khorasan -iran 1 while later sources gave 1030. However, the most authentic considered as "TDV Encyclopedia of Islam" mentions, is that recorded by Ibn al-Athir, a medieval historian, as 1 Muharram 420 AH equivalent to 20 January 1029 CE. He was the son of Chaghri and nephew of Tughril, the founding sultans of the Seljuk Empire. His grandfather was Mikail, who in turn was the son of the warlord Seljuk. He was the father of numerous children, including Malik-Shah I and Tutush I. It is unclear who the mother or mothers of his children were. He was known to have been married at least twice. His wives included the widow of his uncle Tughril, a Kara-Khanid princess known as Aka or Seferiye Khatun, and the daughter or niece of Bagrat IV of Georgia (who would later marry his vizier, Nizam al-Mulk). One of Seljuk's other sons was the Turkic chieftain Arslan Isra'il, whose son, Kutalmish, contested his nephew's succession to the sultanate. Alp Arslan's younger brothers Suleiman ibn Chaghri and Qavurt were his rivals. Kilij Arslan, the son and successor of Suleiman ibn Kutalmish (Kutalmish's son, who would later become Sultan of Rûm), was a major opponent of the Franks during the First Crusade and the Crusade of 1101.

Alp Arslan accompanied his uncle Tughril on campaigns in the south against the Fatimids while his father Chaghri remained in Khorasan. Upon Alp Arslan's return to Khorasan, he began his work in administration at his father's suggestion. While there, his father introduced him to Nizam al-Mulk, one of the most eminent statesmen in early Muslim history and Alp Arslan's future vizier.

After the death of his father, Alp Arslan succeeded him as governor of Khorasan in 1059. His uncle Tughril died in 1063 and designated his successor as Suleiman, Arslan's infant brother. Arslan and his uncle Kutalmish both contested this succession which was resolved at the battle of Damghan in 1063. Arslan defeated Kutalmish for the throne and succeeded on 27 April 1064 as sultan of the Seljuk Empire, thus becoming the sole monarch of Persia from the river Oxus to the Tigris. In 1064 he led a campaign in Georgia during which he captured the regions between Tbilisi and the Çoruh river, Akhalkalaki and Alaverdi. Bagrat IV submitted to paying jizya to the Seljuks but the Georgians broke the agreement in 1065. Alp Arslan invaded Georgia again in 1068. He captured Tbilisi after a short battle and obtained the submission of Bagrat IV; however, the Georgians freed themselves from Seljuk rule around 1073–1074.

In consolidating his empire and subduing contending factions, Arslan was ably assisted by Nizam al-Mulk, and the two are credited with helping to stabilize the empire after the death of Tughril. With peace and security established in his dominions, Arslan convoked an assembly of the states, and in 1066, he declared his son Malik Shah I his heir and successor. With the hope of capturing Caesarea Mazaca, the capital of Cappadocia, he placed himself at the head of the Turkoman cavalry, crossed the Euphrates, and entered and invaded the city. Along with Nizam al-Mulk, he then marched into Armenia and Georgia, which he conquered in 1064. After a siege of 25 days, the Seljuks captured Ani, the capital city of Armenia. An account of the sack and massacres in Ani is given by the historian Sibt ibn al-Jawzi, who quotes an eyewitness saying:

En route to fight the Fatimids in Syria in 1068, Alp Arslan invaded the Byzantine Empire. The Emperor Romanos IV Diogenes, assuming command in person, met the invaders in Cilicia. In three arduous campaigns, the Turks were defeated in detail and driven across the Euphrates in 1070. The first two campaigns were conducted by the emperor himself, while the third was directed by Manuel Comnenos, the great-uncle of Emperor Manuel Comnenos. During this time, Arslan gained the allegiance of Rashid al-Dawla Mahmud, the Mirdasid emir of Aleppo.

In 1071, Romanos again took the field and advanced into Armenia with possibly 30,000 men, including a contingent of Cuman Turks as well as contingents of Franks and Normans, under Ursel de Baieul. Alp Arslan, who had moved his troops south to fight the Fatimids, quickly reversed to meet the Byzantines. Alp Arslan handed control of his army to his eunuch slave general, Taranges, and commanded him to "Win or be beheaded." Taranges prepared for the battle by setting traps and organizing ambushes. The Seljuk and Byzantine armies met on Friday, 26 August 1071 at Manzikert on the Murat River, north of Lake Van, beginning the Battle of Manzikert. The Cuman mercenaries among the Byzantine forces immediately defected to the Turkic side. Seeing this, the Western mercenaries subsequently abandoned the battlefield as well. To be exact, Romanos was betrayed by general Andronikos Doukas, son of the Caesar (Romanos's stepson), who pronounced him dead and rode off with a large part of the Byzantine forces at a critical moment. The Byzantines were wholly routed.

Emperor Romanos himself was captured in battle and presented to Alp Arslan. It is reported that upon seeing the Roman emperor, the sultan leaped from his throne, commanded Romanos to kiss the ground, and stepped on his neck. He repeatedly berated the emperor, including for spurning his emissaries and offers of peace. Romanos remained unrepentant, asserting that he had merely done what was "possible for a man, and which kings are bound to do, and I have fallen short in nothing. But God has fulfilled his will. And now, do what you wish and abandon recriminations." Purportedly declaring Romanos "too trivial... to kill", Arslan then led him about the camp to sell the prisoner to one of his men. The Seljuk soldiers initially refused to spend any money on buying the emperor, until one man traded a dog for him. Next, wishing to test Romanos, Alp Arslan asked Romanos what he would do if their situation were reversed and Arslan was imprisoned by the Byzantines. Romanos bluntly answered "The worst!" His honesty impressed Arslan, who then decided to spare Romanos's life and instead ransom him back to his homeland. After agreeing on a ransom, Alp Arslan sent Romanos to Constantinople with a Turkish escort, carrying a banner above the disgraced emperor that read: "There is no god but Allah and Muhammad is his messenger".

The reason Alp Arslan spared Romanos was likely to avoid a two-front war. The Fatimids were launching devastating raids on the Seljuk domains during this period, Arslan may have worried that executing the Roman emperor might escalate his conflict with the Byzantines. Romanos himself had told the sultan that "killing me will not be of any use to you".

After hearing of the death of Byzantine Emperor Romanos IV Diogenes, Sultan Alp Arslan pledged: "The Byzantine nation has no God, so this day the oath of peace and friendship taken by both the Persians and Byzantines is nullified; henceforth I shall consume with the sword all those people who venerate the cross, and all the lands of the Christians shall be enslaved."

Alp Arslan and his successor Malik Shah urged Turkish tribes to invade and settle Anatolia where they would not only cease to be a problem for the Seljuk Sultanate but also extend its territory further. Alp Arslan commanded the Turks as follows:
Alp Arslan's victories changed the balance in western Asia completely in favor of the Seljuq Turks and Sunni Muslims. While the Byzantine Empire was to continue for nearly four more centuries, the victory at Manzikert signalled the beginning of Turkic ascendancy in Anatolia. The victory at Manzikert became so popular among the Turks that later every noble family in Anatolia claimed to have had an ancestor who had fought on that day.

Alp Arslan's strength lay in the military realm. Domestic affairs were handled by his able vizier, Nizam al-Mulk, the founder of the administrative organization that characterized and strengthened the sultanate during the reigns of Alp Arslan and his son, Malik Shah. Military Iqtas, governed by Seljuq princes, were established to provide support for the soldiery and to accommodate the nomadic Turks to the established Anatolian agricultural scene. This type of military fiefdom enabled the nomadic Turks to draw on the resources of the sedentary Persians, Turks, and other established cultures within the Seljuq realm, and allowed Alp Arslan to field a huge standing army without depending on tribute from conquest to pay his soldiers. He not only had enough food from his subjects to maintain his military, but the taxes collected from traders and merchants added to his coffers sufficiently to fund his continuous wars.

Suleiman ibn Qutalmish was the son of the contender for Arslan's throne; he was appointed governor of the north-western provinces and assigned to complete the invasion of Anatolia. An explanation for this choice can only be conjectured from Ibn al-Athir's account of the battle between Alp-Arslan and Kutalmish, in which he writes that Alp-Arslan wept for the latter's death and greatly mourned the loss of his kinsman.

Contemporary descriptions portray Alp Arslan as "very awe-inspiring, dominating," a "greatformed one, elegant of stature. He had long, thin whiskers, which he used to knot up when shooting arrows. And they say his arrow never went astray... From the top button of his hat to the end of his moustaches it was two yards"
Muslim sources show Alp Arslan as fanatically pious but just. Alp Arslan was so dedicated to the Hanafi madhhab that he always kept a qadi by his side, including in battles.
His vizier, Nizam al-Mulk, described the young sultan:
After Manzikert, the dominion of Alp Arslan extended over much of western Asia. He soon prepared to march for the conquest of Turkestan, the original seat of his ancestors. With a powerful army, he advanced to the banks of the Oxus. Before he could pass the river safely, however, it was necessary to subdue certain fortresses, one of which was for several days vigorously defended by the rebel, Yusuf al-Kharezmi or Yusuf al-Harani. Perhaps over-eager to press on against his Qarakhanid enemy, Alp Arslan gained the governor's submission by promising the rebel 'perpetual ownership of his lands'. When Yusuf al-Harani was brought before him, the Sultan ordered that he be shot, but before the archers could raise their bows Yusuf seized a knife and threw himself at Alp Arslan, striking three blows before being slain. Four days later on 24 November 1072, Alp Arslan died and was buried at Merv, having designated his 18-year-old son Malik Shah as his successor.

One of his wives was Safariyya Khatun. She had a daughter, Sifri Khatun, who in 1071–72, married Abbasid Caliph Al-Muqtadi. Safariyya died in Isfahan in 1073–74. Another of his wives was Akka Khatun. She had been formerly the wife of Sultan Tughril. Alp Arslan married her after Tughril's death in 1063. Another of his wives was Shah Khatun. She was the daughter of Qadir Khan Yusuf, and had been formerly married to Ghaznavid Mas'ud I. Another wife was Ummu Hifchaq also known as Ummu Qipchaq. Another of his wives was the daughter of King of Tashir Kiurike I, who was married to the sister of the Georgian king Bagrat IV. Alp Arslan divorced her, and married her to Nizam al-Mulk. His sons were Malik-Shah I, Tutush I, Arslan Shah, Tekish, Toghan-Shah, Ayaz and Buibars. One of his daughters married the son of Kurd Surkhab, son of Bard in 1068. Another daughter, Zulaikha Khatun, was married to a Muslim, son of Quraish in 1086–87. Another daughter, Aisha Khatun, married Shams al-Mulk Nasr, son of Ibrahim Khan Tamghach. Another daughter was married to Mas'ud III of Ghazni and was his first wife. Another daughter was Sara Khatun.

Alp Arslan's conquest of Anatolia from the Byzantines is also seen as one of the pivotal precursors to the launch of the Crusades.

From 2002 to July 2008 under Turkmen calendar reform, the month of August was named after Alp Arslan.

The 2nd Training Motorized Rifle Division of the Turkmen Ground Forces is named in his honor.


American Film Institute

The American Film Institute (AFI) is an American nonprofit film organization that educates filmmakers and honors the heritage of the motion picture arts in the United States. AFI is supported by private funding and public membership fees.

The institute is composed of leaders from the film, entertainment, business, and academic communities. The board of trustees is chaired by Kathleen Kennedy and the board of directors chaired by Robert A. Daly guide the organization, which is led by President and CEO, film historian Bob Gazzale. Prior leaders were founding director George Stevens Jr. (from the organization's inception in 1967 until 1980) and Jean Picker Firstenberg (from 1980 to 2007).

The American Film Institute was founded by a 1965 presidential mandate announced in the Rose Garden of the White House by Lyndon B. Johnson—to establish a national arts organization to preserve the legacy of American film heritage, educate the next generation of filmmakers, and honor the artists and their work. Two years later, in 1967, AFI was established, supported by the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation.

The original 22-member Board of Trustees included actor Gregory Peck as chairman and actor Sidney Poitier as vice-chairman, as well as director Francis Ford Coppola, film historian Arthur Schlesinger, Jr., lobbyist Jack Valenti, and other representatives from the arts and academia.

The institute established a training program for filmmakers known then as the Center for Advanced Film Studies. Also created in the early years were a repertory film exhibition program at the Kennedy Center for the Performing Arts and the AFI Catalog of Feature Films — a scholarly source for American film history. The institute moved to its current eight-acre Hollywood campus in 1981. The film training program grew into the AFI Conservatory, an accredited graduate school.

AFI moved its presentation of first-run and auteur films from the Kennedy Center to the historic AFI Silver Theatre and Cultural Center, which hosts the AFI DOCS film festival, making AFI the largest nonprofit film exhibitor in the world. AFI educates audiences and recognizes artistic excellence through its awards programs and 10 Top 10 Lists.
In 2017, then-aspiring filmmaker Ilana Bar-Din Giannini claimed that the AFI expelled her after she accused Dezso Magyar of sexually harassing her in the early 1980s.

AFI educational and cultural programs include:

In 1969, the institute established the AFI Conservatory for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, California. The first class included filmmakers Terrence Malick, Caleb Deschanel, and Paul Schrader. That program grew into the AFI Conservatory, an accredited graduate film school located in the hills above Hollywood, California, providing training in six filmmaking disciplines: cinematography, directing, editing, producing, production design, and screenwriting. Mirroring a professional production environment, Fellows collaborate to make more films than any other graduate level program. Admission to AFI Conservatory is highly selective, with a maximum of 140 graduates per year.

In 2013, Emmy and Oscar-winning director, producer, and screenwriter James L. Brooks ("As Good as It Gets", "Broadcast News", "Terms of Endearment") joined as the artistic director of the AFI Conservatory where he provides leadership for the film program. Brooks' artistic role at the AFI Conservatory has a rich legacy that includes Daniel Petrie, Jr., Robert Wise, and Frank Pierson. Award-winning director Bob Mandel served as dean of the AFI Conservatory for nine years. Jan Schuette took over as dean in 2014 and served until 2017. Film producer Richard Gladstein was dean from 2017 until 2019, when Susan Ruskin was appointed.

AFI Conservatory's alumni have careers in film, television and on the web. They have been recognized with all of the major industry awards—Academy Award, Emmy Award, guild awards, and the Tony Award.

The AFI Catalog, started in 1968, is a web-based filmographic database. A research tool for film historians, the catalog consists of entries on more than 60,000 feature films and 17,000 short films produced from 1893 to 2011, as well as AFI Awards Outstanding Movies of the Year from 2000 through 2010. Early print copies of this catalog may also be found at local libraries.

Created in 2000, the AFI Awards honor the ten outstanding films ("Movies of the Year") and ten outstanding television programs ("TV Programs of the Year"). The awards are a non-competitive acknowledgment of excellence.

The awards are announced in December, and a private luncheon for award honorees takes place the following January.

The AFI 100 Years... series, which ran from 1998 to 2008 and created jury-selected lists of America's best movies in categories such as Musicals, Laughs and Thrills, prompted new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics, and historians. "Citizen Kane" was voted the greatest American film twice.

AFI operates two film festivals: AFI Fest in Los Angeles, and AFI Docs (formally known as Silverdocs) in Silver Spring, Maryland, and Washington, D.C.

AFI Fest is the American Film Institute's annual celebration of artistic excellence. It is a showcase for the best festival films of the year and an opportunity for master filmmakers and emerging artists to come together with audiences in the movie capital of the world. It is the only festival of its stature that is free to the public. The Academy of Motion Picture Arts and Sciences recognizes AFI Fest as a qualifying festival for the Short Films category for the annual Academy Awards.

The festival has paid tribute to numerous influential filmmakers and artists over the years, including Agnès Varda, Pedro Almodóvar and David Lynch as guest artistic directors, and has screened scores of films that have produced Oscar nominations and wins.

Held annually in June, AFI Docs (formerly Silverdocs) is a documentary festival in Washington, D.C. The festival attracts over 27,000 documentary enthusiasts.

The AFI Silver Theatre and Cultural Center is a moving image exhibition, education and cultural center located in Silver Spring, Maryland. Anchored by the restoration of noted architect John Eberson's historic 1938 Silver Theatre, it features 32,000 square feet of new construction housing two stadium theatres, office and meeting space, and reception and exhibit areas.

The AFI Silver Theatre and Cultural Center presents film and video programming, augmented by filmmaker interviews, panels, discussions, and musical performances.

The Directing Workshop for Women is a training program committed to educating and mentoring participants in an effort to increase the number of women working professionally in screen directing. In this tuition-free program, each participant is required to complete a short film by the end of the year-long program.

Alumnae of the program include Maya Angelou, Anne Bancroft, Dyan Cannon, Ellen Burstyn, Jennifer Getzinger, Lesli Linka Glatter, Lily Tomlin, Susan Oliver and Nancy Malone.

AFI released a set of hour-long programs reviewing the career of acclaimed directors. The Directors Series content was copyrighted in 1997 by Media Entertainment Inc and The American Film Institute, and the VHS and DVDs were released between 1999 and 2001 on Winstar TV and Video.

Directors featured included:


Akira Kurosawa

Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director during World War II with the popular action film "Sanshiro Sugata" (1943). After the war, the critically acclaimed "Drunken Angel" (1948), in which Kurosawa cast the then little-known actor Toshiro Mifune in a starring role, cemented the director's reputation as one of the most important young filmmakers in Japan. The two men would go on to collaborate on another fifteen films.

"Rashomon" (1950), which premiered in Tokyo, became the surprise winner of the Golden Lion at the 1951 Venice Film Festival. The commercial and critical success of that film opened up Western film markets for the first time to the products of the Japanese film industry, which in turn led to international recognition for other Japanese filmmakers. Kurosawa directed approximately one film per year throughout the 1950s and early 1960s, including a number of highly regarded (and often adapted) films, such as (1952), "Seven Samurai" (1954), "Throne of Blood" (1957), "Yojimbo" (1961) and "High and Low" (1963). After the 1960s he became much less prolific; even so, his later work—including two of his final films, (1980) and (1985)—continued to receive great acclaim.

In 1990, he accepted the Academy Award for Lifetime Achievement. Posthumously, he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited there as being among the five people who most prominently contributed to the improvement of Asia in the 20th century. His career has been honored by many retrospectives, critical studies and biographies in both print and video, and by releases in many consumer media.

Kurosawa was born on March 23, 1910, in Ōimachi in the Ōmori district of Tokyo. His father Isamu (1864–1948), a member of a samurai family from Akita Prefecture, worked as the director of the Army's Physical Education Institute's lower secondary school, while his mother Shima (1870–1952) came from a merchant's family living in Osaka. Akira was the eighth and youngest child of the moderately wealthy family, with two of his siblings already grown up at the time of his birth and one deceased, leaving Kurosawa to grow up with three sisters and a brother.

In addition to promoting physical exercise, Isamu Kurosawa was open to Western traditions and considered theatre and motion pictures to have educational merit. He encouraged his children to watch films; young Akira viewed his first movies at the age of six. An important formative influence was his elementary school teacher Mr. Tachikawa, whose progressive educational practices ignited in his young pupil first a love of drawing and then an interest in education in general. During this time, Akira also studied calligraphy and Kendo swordsmanship.

Another major childhood influence was Heigo Kurosawa (1906-1933), Akira's older brother by four years. In the aftermath of the Great Kantō earthquake and the subsequent Kantō Massacre of 1923, Heigo took the thirteen-year-old Akira to view the devastation. When Akira wanted to look away from the corpses of humans and animals scattered everywhere, Heigo forbade him to do so, encouraging Akira instead to face his fears by confronting them directly. Some commentators have suggested that this incident would influence Kurosawa's later artistic career, as the director was seldom hesitant to confront unpleasant truths in his work.

Heigo was academically gifted, but soon after failing to secure a place in Tokyo's foremost high school, he began to detach himself from the rest of the family, preferring to concentrate on his interest in foreign literature. In the late 1920s, Heigo became a benshi (silent film narrator) for Tokyo theaters showing foreign films and quickly made a name for himself. Akira, who at this point planned to become a painter, moved in with him, and the two brothers became inseparable. With Heigo's guidance, Akira devoured not only films but also theater and circus performances, while exhibiting his paintings and working for the left-wing Proletarian Artists' League. However, he was never able to make a living with his art, and, as he began to perceive most of the proletarian movement as "putting unfulfilled political ideals directly onto the canvas", he lost his enthusiasm for painting.

With the increasing production of talking pictures in the early 1930s, film narrators like Heigo began to lose work, and Akira moved back in with his parents. In July 1933, Heigo died by suicide. Kurosawa has commented on the lasting sense of loss he felt at his brother's death and the chapter of his autobiography ("Something Like an Autobiography") that describes it—written nearly half a century after the event—is titled, "A Story I Don't Want to Tell". Only four months later, Kurosawa's eldest brother also died, leaving Akira, at age 23, the only one of the Kurosawa brothers still living, together with his three surviving sisters.

In 1935, the new film studio Photo Chemical Laboratories, known as P.C.L. (which later became the major studio Toho), advertised for assistant directors. Although he had demonstrated no previous interest in film as a profession, Kurosawa submitted the required essay, which asked applicants to discuss the fundamental deficiencies of Japanese films and find ways to overcome them. His half-mocking view was that if the deficiencies were fundamental, there was no way to correct them. Kurosawa's essay earned him a call to take the follow-up exams, and director Kajirō Yamamoto, who was among the examiners, took a liking to Kurosawa and insisted that the studio hire him. The 25-year-old Kurosawa joined P.C.L. in February 1936.

During his five years as an assistant director, Kurosawa worked under numerous directors, but by far the most important figure in his development was Yamamoto. Of his 24 films as A.D., he worked on 17 under Yamamoto, many of them comedies featuring the popular actor Ken'ichi Enomoto, known as "Enoken". Yamamoto nurtured Kurosawa's talent, promoting him directly from third assistant director to chief assistant director after a year. Kurosawa's responsibilities increased, and he worked at tasks ranging from stage construction and film development to location scouting, script polishing, rehearsals, lighting, dubbing, editing, and second-unit directing. In the last of Kurosawa's films as an assistant director for Yamamoto, "Horse" (1941), Kurosawa took over most of the production, as his mentor was occupied with the shooting of another film.

Yamamoto advised Kurosawa that a good director needed to master screenwriting. Kurosawa soon realized that the potential earnings from his scripts were much higher than what he was paid as an assistant director. He later wrote or co-wrote all his films, and frequently penned screenplays for other directors such as Satsuo Yamamoto's film, "A Triumph of Wings" ("Tsubasa no gaika", 1942). This outside scriptwriting would serve Kurosawa as a lucrative sideline lasting well into the 1960s, long after he became famous.

In the two years following the release of "Horse" in 1941, Kurosawa searched for a story he could use to launch his directing career. Towards the end of 1942, about a year after the Japanese attack on Pearl Harbor, novelist Tsuneo Tomita published his Musashi Miyamoto-inspired judo novel, "Sanshiro Sugata", the advertisements for which intrigued Kurosawa. He bought the book on its publication day, devoured it in one sitting, and immediately asked Toho to secure the film rights. Kurosawa's initial instinct proved correct as, within a few days, three other major Japanese studios also offered to buy the rights. Toho prevailed, and Kurosawa began pre-production on his debut work as director.

Shooting of "Sanshiro Sugata" began on location in Yokohama in December 1942. Production proceeded smoothly, but getting the completed film past the censors was an entirely different matter. The censorship office considered the work to be objectionably "British-American" by the standards of wartime Japan, and it was only through the intervention of director Yasujirō Ozu, who championed the film, that "Sanshiro Sugata" was finally accepted for release on March 25, 1943. (Kurosawa had just turned 33.) The movie became both a critical and commercial success. Nevertheless, the censorship office would later decide to cut out some 18 minutes of footage, much of which is now considered lost.

He next turned to the subject of wartime female factory workers in "The Most Beautiful", a propaganda film which he shot in a semi-documentary style in early 1944. To elicit realistic performances from his actresses, the director had them live in a real factory during the shoot, eat the factory food and call each other by their character names. He would use similar methods with his performers throughout his career.
During production, the actress playing the leader of the factory workers, Yōko Yaguchi, was chosen by her colleagues to present their demands to the director. She and Kurosawa were constantly at odds, and it was through these arguments that the two paradoxically became close. They married on May 21, 1945, with Yaguchi two months pregnant (she never resumed her acting career), and the couple would remain together until her death in 1985. They had two children, both surviving Kurosawa : a son, Hisao, born December 20, 1945, who served as producer on some of his father's last projects, and Kazuko, a daughter, born April 29, 1954, who became a costume designer.

Shortly before his marriage, Kurosawa was pressured by the studio against his will to direct a sequel to his debut film. The often blatantly propagandistic "Sanshiro Sugata Part II", which premiered in May 1945, is generally considered one of his weakest pictures.

Kurosawa decided to write the script for a film that would be both censor-friendly and less expensive to produce. "The Men Who Tread on the Tiger's Tail", based on the Kabuki play "Kanjinchō" and starring the comedian Enoken, with whom Kurosawa had often worked during his assistant director days, was completed in September 1945. By this time, Japan had surrendered and the occupation of Japan had begun. The new American censors interpreted the values allegedly promoted in the picture as overly "feudal" and banned the work. It was not released until 1952, the year another Kurosawa film, , was also released. Ironically, while in production, the film had already been savaged by Japanese wartime censors as too Western and "democratic" (they particularly disliked the comic porter played by Enoken), so the movie most probably would not have seen the light of day even if the war had continued beyond its completion.

After the war, Kurosawa, influenced by the democratic ideals of the Occupation, sought to make films that would establish a new respect towards the individual and the self. The first such film, "No Regrets for Our Youth" (1946), inspired by both the 1933 Takigawa incident and the Hotsumi Ozaki wartime spy case, criticized Japan's prewar regime for its political oppression. Atypically for the director, the heroic central character is a woman, Yukie (Setsuko Hara), who, born into upper-middle-class privilege, comes to question her values in a time of political crisis. The original script had to be extensively rewritten and, because of its controversial theme and gender of its protagonist, the completed work divided critics. Nevertheless, it managed to win the approval of audiences, who turned variations on the film's title into a postwar catchphrase.

His next film, "One Wonderful Sunday", premiered in July 1947 to mixed reviews. It is a relatively uncomplicated and sentimental love story dealing with an impoverished postwar couple trying to enjoy, within the devastation of postwar Tokyo, their one weekly day off. The movie bears the influence of Frank Capra, D. W. Griffith and F. W. Murnau, each of whom was among Kurosawa's favorite directors. Another film released in 1947 with Kurosawa's involvement was the action-adventure thriller, "Snow Trail", directed by Senkichi Taniguchi from Kurosawa's screenplay. It marked the debut of the intense young actor Toshiro Mifune. It was Kurosawa who, with his mentor Yamamoto, had intervened to persuade Toho to sign Mifune, during an audition in which the young man greatly impressed Kurosawa, but managed to alienate most of the other judges.
"Drunken Angel" is often considered the director's first major work. Although the script, like all of Kurosawa's occupation-era works, had to go through rewrites due to American censorship, Kurosawa felt that this was the first film in which he was able to express himself freely. A gritty story of a doctor who tries to save a gangster (yakuza) with tuberculosis, it was also the first time that Kurosawa directed Mifune, who went on to play major roles in all but one of the director's next 16 films (the exception being ). While Mifune was not cast as the protagonist in "Drunken Angel", his explosive performance as the gangster so dominates the drama that he shifted the focus from the title character, the alcoholic doctor played by Takashi Shimura, who had already appeared in several Kurosawa movies. However, Kurosawa did not want to smother the young actor's immense vitality, and Mifune's rebellious character electrified audiences in much the way that Marlon Brando's defiant stance would startle American film audiences a few years later. The film premiered in Tokyo in April 1948 to rave reviews and was chosen by the prestigious "Kinema Junpo" critics poll as the best film of its year, the first of three Kurosawa movies to be so honored.

After the completion of "Drunken Angel", Toho became embroiled in a months-long labor strike, in which the Toho union occupied the grounds of the studio. When Toho management ceased paying workers' salaries, Kurosawa formed a touring acting troupe to raise funds, directing Anton Chekhov's "The Proposal", and an adaptation of "Drunken Angel" starring Mifune and Shimura. Disillusioned by the division and violence between employees at Toho, the underhanded tactics of Toho leadership, and the breaking of the occupation by police and military standoff, Kurosawa left Toho, later recalling "I had come to understand that the studio I had thought was my home actually belonged to strangers".

Kurosawa, with producer Sōjirō Motoki and fellow directors and friends Kajiro Yamamoto, Mikio Naruse and Senkichi Taniguchi, formed a new independent production unit called Film Art Association (Eiga Geijutsu Kyōkai). For this organization's debut work, and first film for Daiei studios, Kurosawa turned to a contemporary play by Kazuo Kikuta and, together with Taniguchi, adapted it for the screen. "The Quiet Duel" starred Toshiro Mifune as an idealistic young doctor struggling with syphilis, a deliberate attempt by Kurosawa to break the actor away from being typecast as gangsters. Released in March 1949, it was a box office success, but is generally considered one of the director's lesser achievements.
His second film of 1949, also produced by Film Art Association and released by Shintoho, was "Stray Dog". It is a detective movie (perhaps the first important Japanese film in that genre) that explores the mood of Japan during its painful postwar recovery through the story of a young detective, played by Mifune, and his fixation on the recovery of his handgun, which was stolen by a penniless war veteran who proceeds to use it to rob and murder. Adapted from an unpublished novel by Kurosawa in the style of a favorite writer of his, Georges Simenon, it was the director's first collaboration with screenwriter Ryuzo Kikushima, who would later help to script eight other Kurosawa films. A famous, virtually wordless sequence, lasting over eight minutes, shows the detective, disguised as an impoverished veteran, wandering the streets in search of the gun thief; it employed actual documentary footage of war-ravaged Tokyo neighborhoods shot by Kurosawa's friend, Ishirō Honda, the future director of "Godzilla". The film is considered a precursor to the contemporary police procedural and buddy cop film genres.

"Scandal", released by Shochiku in April 1950, was inspired by the director's personal experiences with, and anger towards, Japanese yellow journalism. The work is an ambitious mixture of courtroom drama and social problem film about free speech and personal responsibility, but even Kurosawa regarded the finished product as dramatically unfocused and unsatisfactory, and almost all critics agree. However, it would be Kurosawa's second film of 1950, "Rashomon", that would ultimately win him, and Japanese cinema, a whole new international audience.

After finishing "Scandal", Kurosawa was approached by Daiei studios to make another film for them. Kurosawa picked a script by an aspiring young screenwriter, Shinobu Hashimoto, who would eventually work on nine of his films. Their first joint effort was based on Ryūnosuke Akutagawa's experimental short story "In a Grove", which recounts the murder of a samurai and the rape of his wife from various different and conflicting points-of-view. Kurosawa saw potential in the script, and with Hashimoto's help, polished and expanded it and then pitched it to Daiei, who were happy to accept the project due to its low budget.

The shooting of "Rashomon" began on July 7, 1950, and, after extensive location work in the primeval forest of Nara, wrapped on August 17. Just one week was spent in hurried post-production, hampered by a studio fire, and the finished film premiered at Tokyo's Imperial Theatre on August 25, expanding nationwide the following day. The movie was met by lukewarm reviews, with many critics puzzled by its unique theme and treatment, but it was nevertheless a moderate financial success for Daiei.
Kurosawa's next film, for Shochiku, was "The Idiot", an adaptation of the novel by the director's favorite writer, Fyodor Dostoevsky. The story is relocated from Russia to Hokkaido, but otherwise adheres closely to the original, a fact seen by many critics as detrimental to the work. A studio-mandated edit shortened it from Kurosawa's original cut of 265 minutes to just 166 minutes, making the resulting narrative exceedingly difficult to follow. The severely edited film version is widely considered to be one of the director's least successful works and the original full-length version no longer exists. Contemporary reviews of the much shortened edited version were very negative, but the film was a moderate success at the box office, largely because of the popularity of one of its stars, Setsuko Hara.

Meanwhile, unbeknownst to Kurosawa, "Rashomon" had been entered in the Venice Film Festival, due to the efforts of Giuliana Stramigioli, a Japan-based representative of an Italian film company, who had seen and admired the movie and convinced Daiei to submit it. On September 10, 1951, "Rashomon" was awarded the festival's highest prize, the Golden Lion, shocking not only Daiei but the international film world, which at the time was largely unaware of Japan's decades-old cinematic tradition.

After Daiei briefly exhibited a subtitled print of the film in Los Angeles, RKO purchased distribution rights to "Rashomon" in the United States. The company was taking a considerable gamble. It had put out only one prior subtitled film in the American market, and the only previous Japanese talkie commercially released in New York had been Mikio Naruse's comedy, "Wife! Be Like a Rose!", in 1937: a critical and box-office flop. However, "Rashomon"s commercial run, greatly helped by strong reviews from critics and even the columnist Ed Sullivan, earned $35,000 in its first three weeks at a single New York theatre, an almost unheard-of sum at the time.

This success in turn led to a vogue in America and the West for Japanese movies throughout the 1950s, replacing the enthusiasm for Italian neorealist cinema. By the end of 1952 "Rashomon" was released in Japan, the United States, and most of Europe. Among the Japanese film-makers whose work, as a result, began to win festival prizes and commercial release in the West were Kenji Mizoguchi ("The Life of Oharu", "Ugetsu", "Sansho the Bailiff") and, somewhat later, Yasujirō Ozu ("Tokyo Story", "An Autumn Afternoon")—artists highly respected in Japan but, before this period, almost totally unknown in the West. Kurosawa's growing reputation among Western audiences in the 1950s would make Western audiences more sympathetic to the reception of later generations of Japanese film-makers ranging from Kon Ichikawa, Masaki Kobayashi, Nagisa Oshima and Shohei Imamura to Juzo Itami, Takeshi Kitano and Takashi Miike.

His career boosted by his sudden international fame, Kurosawa, now reunited with his original film studio, Toho (which would go on to produce his next 11 films), set to work on his next project, . The movie stars Takashi Shimura as a cancer-ridden Tokyo bureaucrat, Watanabe, on a final quest for meaning before his death. For the screenplay, Kurosawa brought in Hashimoto as well as writer Hideo Oguni, who would go on to co-write twelve Kurosawa films. Despite the work's grim subject matter, the screenwriters took a satirical approach, which some have compared to the work of Brecht, to both the bureaucratic world of its hero and the U.S. cultural colonization of Japan. (American pop songs figure prominently in the film.) Because of this strategy, the filmmakers are usually credited with saving the picture from the kind of sentimentality common to dramas about characters with terminal illnesses. opened in October 1952 to rave reviews—it won Kurosawa his second Kinema Junpo "Best Film" award—and enormous box office success. It remains the most acclaimed of all the artist's films set in the modern era.

In December 1952, Kurosawa took his screenwriters, Shinobu Hashimoto and Hideo Oguni, for a forty-five-day secluded residence at an inn to create the screenplay for his next movie, "Seven Samurai". The ensemble work was Kurosawa's first proper samurai film, the genre for which he would become most famous. The simple story, about a poor farming village in Sengoku period Japan that hires a group of samurai to defend it against an impending attack by bandits, was given a full epic treatment, with a huge cast (largely consisting of veterans of previous Kurosawa productions) and meticulously detailed action, stretching out to almost three-and-a-half hours of screen time.

Three months were spent in pre-production and a month in rehearsals. Shooting took up 148 days spread over almost a year, interrupted by production and financing troubles and Kurosawa's health problems. The film finally opened in April 1954, half a year behind its original release date and about three times over budget, making it at the time the most expensive Japanese film ever made. (However, by Hollywood standards, it was a quite modestly budgeted production, even for that time.) The film received positive critical reaction and became a big hit, quickly making back the money invested in it and providing the studio with a product that they could, and did, market internationally—though with extensive edits. Over time—and with the theatrical and home video releases of the uncut version—its reputation has steadily grown. It is now regarded by some commentators as the greatest Japanese film ever made, and in 1999, a poll of Japanese film critics also voted it the best Japanese film ever made. In the most recent (2022) version of the widely respected British Film Institute (BFI) "Sight & Sound" "Greatest Films of All Time" poll, "Seven Samurai" placed 20th among all films from all countries in the critics' and tied at 14th in the directors' polls, receiving a place in the Top Ten lists of 48 critics and 22 directors.

In 1954, nuclear tests in the Pacific were causing radioactive rainstorms in Japan and one particular incident in March had exposed a Japanese fishing boat to nuclear fallout, with disastrous results. It is in this anxious atmosphere that Kurosawa's next film, "I Live in Fear", was conceived. The story concerned an elderly factory owner (Toshiro Mifune) so terrified of the prospect of a nuclear attack that he becomes determined to move his entire extended family (both legal and extra-marital) to what he imagines is the safety of a farm in Brazil. Production went much more smoothly than the director's previous film, but a few days before shooting ended, Kurosawa's composer, collaborator, and close friend Fumio Hayasaka died (of tuberculosis) at the age of 41. The film's score was finished by Hayasaka's student, Masaru Sato, who would go on to score all of Kurosawa's next eight films. "I Live in Fear" opened in November 1955 to mixed reviews and muted audience reaction, becoming the first Kurosawa film to lose money during its original theatrical run. Today, it is considered by many to be among the finest films dealing with the psychological effects of the global nuclear stalemate.

Kurosawa's next project, "Throne of Blood", an adaptation of William Shakespeare's "Macbeth"—set, like "Seven Samurai", in the Sengoku Era—represented an ambitious transposition of the English work into a Japanese context. Kurosawa instructed his leading actress, Isuzu Yamada, to regard the work as if it were a cinematic version of a "Japanese" rather than a European literary classic. Given Kurosawa's appreciation of traditional Japanese stage acting, the acting of the players, particularly Yamada, draws heavily on the stylized techniques of the Noh theater. It was filmed in 1956 and released in January 1957 to a slightly less negative domestic response than had been the case with the director's previous film. Abroad, "Throne of Blood", regardless of the liberties it takes with its source material, quickly earned a place among the most celebrated Shakespeare adaptations.

Another adaptation of a classic European theatrical work followed almost immediately, with production of "The Lower Depths", based on a play by Maxim Gorky, taking place in May and June 1957. In contrast to the Shakespearean sweep of "Throne of Blood", "The Lower Depths" was shot on only two confined sets, in order to emphasize the restricted nature of the characters' lives. Though faithful to the play, this adaptation of Russian material to a completely Japanese setting—in this case, the late Edo period—unlike his earlier "The Idiot", was regarded as artistically successful. The film premiered in September 1957, receiving a mixed response similar to that of "Throne of Blood". However, some critics rank it among the director's most underrated works.

Kurosawa's three next movies after "Seven Samurai" had not managed to capture Japanese audiences in the way that that film had. The mood of the director's work had been growing increasingly pessimistic and dark even as Japan entered a boom period of high-speed growth and rising standards of living. Out of step with the prevailing mood of the era, Kurosawa's films questioned the possibility of redemption through personal responsibility, particularly in "Throne of Blood" and "The Lower Depths". He recognized this, and deliberately aimed for a more light-hearted and entertaining film for his next production while switching to the new widescreen format that had been gaining popularity in Japan. The resulting film, "The Hidden Fortress", is an action-adventure comedy-drama about a medieval princess, her loyal general, and two peasants who all need to travel through enemy lines in order to reach their home region. Released in December 1958, "The Hidden Fortress" became an enormous box-office success in Japan and was warmly received by critics both in Japan and abroad. Today, the film is considered one of Kurosawa's most lightweight efforts, though it remains popular, not least because it is one of several major influences on George Lucas's 1977 space opera, "Star Wars".

Starting with "Rashomon", Kurosawa's productions had become increasingly large in scope and so had the director's budgets. Toho, concerned about this development, suggested that he might help finance his own works, therefore making the studio's potential losses smaller, while in turn allowing himself more artistic freedom as co-producer. Kurosawa agreed, and the Kurosawa Production Company was established in April 1959, with Toho as the majority shareholder.

Despite risking his own money, Kurosawa chose a story that was more directly critical of the Japanese business and political elites than any previous work. "The Bad Sleep Well", based on a script by Kurosawa's nephew Mike Inoue, is a revenge drama about a young man who is able to infiltrate the hierarchy of a corrupt Japanese company with the intention of exposing the men responsible for his father's death. Its theme proved topical: while the film was in production, the massive Anpo protests were held against the new U.S.–Japan Security treaty, which was seen by many Japanese, particularly the young, as threatening the country's democracy by giving too much power to corporations and politicians. The film opened in September 1960 to positive critical reaction and modest box office success. The 25-minute opening sequence depicting a corporate wedding reception is widely regarded as one of Kurosawa's most skillfully executed set pieces, but the remainder of the film is often perceived as disappointing by comparison. The movie has also been criticized for employing the conventional Kurosawan hero to combat a social evil that cannot be resolved through the actions of individuals, however courageous or cunning.

"Yojimbo" ("The Bodyguard"), Kurosawa Production's second film, centers on a masterless samurai, Sanjuro, who strolls into a 19th-century town ruled by two opposing violent factions and provokes them into destroying each other. The director used this work to play with many genre conventions, particularly the Western, while at the same time offering an unprecedentedly (for the Japanese screen) graphic portrayal of violence. Some commentators have seen the Sanjuro character in this film as a fantasy figure who magically reverses the historical triumph of the corrupt merchant class over the samurai class. Featuring Tatsuya Nakadai in his first major role in a Kurosawa movie, and with innovative photography by Kazuo Miyagawa (who shot "Rashomon") and Takao Saito, the film premiered in April 1961 and was a critically and commercially successful venture, earning more than any previous Kurosawa film. The movie and its blackly comic tone were also widely imitated abroad. Sergio Leone's "A Fistful of Dollars" was a virtual (unauthorized) scene-by-scene remake with Toho filing a lawsuit on Kurosawa's behalf and prevailing.
Following the success of "Yojimbo", Kurosawa found himself under pressure from Toho to create a sequel. Kurosawa turned to a script he had written before "Yojimbo", reworking it to include the hero of his previous film. "Sanjuro" was the first of three Kurosawa films to be adapted from the work of the writer Shūgorō Yamamoto (the others would be "Red Beard" and "Dodeskaden"). It is lighter in tone and closer to a conventional period film than "Yojimbo", though its story of a power struggle within a samurai clan is portrayed with strongly comic undertones. The film opened on January 1, 1962, quickly surpassing "Yojimbo"s box office success and garnering positive reviews.

Kurosawa had meanwhile instructed Toho to purchase the film rights to "King's Ransom", a novel about a kidnapping written by American author and screenwriter Evan Hunter, under his pseudonym of Ed McBain, as one of his 87th Precinct series of crime books. The director intended to create a work condemning kidnapping, which he considered one of the very worst crimes. The suspense film, titled "High and Low", was shot during the latter half of 1962 and released in March 1963. It broke Kurosawa's box office record (the third film in a row to do so), became the highest grossing Japanese film of the year, and won glowing reviews. However, his triumph was somewhat tarnished when, ironically, the film was blamed for a wave of kidnappings which occurred in Japan about this time (he himself received kidnapping threats directed at his young daughter, Kazuko). "High and Low" is considered by many commentators to be among the director's strongest works.

Kurosawa quickly moved on to his next project, "Red Beard". Based on a short story collection by Shūgorō Yamamoto and incorporating elements from Dostoevsky's novel "The Insulted and Injured", it is a period film, set in a mid-nineteenth century clinic for the poor, in which Kurosawa's humanist themes receive perhaps their fullest statement. A conceited and materialistic, foreign-trained young doctor, Yasumoto, is forced to become an intern at the clinic under the stern tutelage of Doctor Niide, known as "Akahige" ("Red Beard"), played by Mifune. Although he resists Red Beard initially, Yasumoto comes to admire his wisdom and courage and to perceive the patients at the clinic, whom he at first despised, as worthy of compassion and dignity.

Yūzō Kayama, who plays Yasumoto, was an extremely popular film and music star at the time, particularly for his "Young Guy" ("Wakadaishō") series of musical comedies, so signing him to appear in the film virtually guaranteed Kurosawa strong box-office. The shoot, the filmmaker's longest ever, lasted well over a year (after five months of pre-production), and wrapped in spring 1965, leaving the director, his crew and his actors exhausted. "Red Beard" premiered in April 1965, becoming the year's highest-grossing Japanese production and the third (and last) Kurosawa film to top the prestigious "Kinema Jumpo" yearly critics poll. It remains one of Kurosawa's best-known and most-loved works in his native country. Outside Japan, critics have been much more divided. Most commentators concede its technical merits and some praise it as among Kurosawa's best, while others insist that it lacks complexity and genuine narrative power, with still others claiming that it represents a retreat from the artist's previous commitment to social and political change.

The film marked something of an end of an era for its creator. The director himself recognized this at the time of its release, telling critic Donald Richie that a cycle of some kind had just come to an end and that his future films and production methods would be different. His prediction proved quite accurate. Beginning in the late 1950s, television began increasingly to dominate the leisure time of the formerly large and loyal Japanese cinema audience. And as film company revenues dropped, so did their appetite for risk—particularly the risk represented by Kurosawa's costly production methods.

"Red Beard" also marked the midway point, chronologically, in the artist's career. During his previous twenty-nine years in the film industry (which includes his five years as assistant director), he had directed twenty-three films, while during the remaining twenty-eight years, for many complex reasons, he would complete only seven more. Also, for reasons never adequately explained, "Red Beard" would be his final film starring Toshiro Mifune. Yū Fujiki, an actor who worked on "The Lower Depths", observed, regarding the closeness of the two men on the set, "Mr. Kurosawa's heart was in Mr. Mifune's body." Donald Richie has described the rapport between them as a unique "symbiosis".

When Kurosawa's exclusive contract with Toho came to an end in 1966, the 56-year-old director was seriously contemplating change. Observing the troubled state of the domestic film industry, and having already received dozens of offers from abroad, the idea of working outside Japan appealed to him as never before.

For his first foreign project, Kurosawa chose a story based on a "Life" magazine article. The Embassy Pictures action thriller, to be filmed in English and called simply "Runaway Train", would have been his first in color. But the language barrier proved a major problem, and the English version of the screenplay was not even finished by the time filming was to begin in autumn 1966. The shoot, which required snow, was moved to autumn 1967, then canceled in 1968. Almost two decades later, another foreign director working in Hollywood, Andrei Konchalovsky, finally made "Runaway Train" (1985), though from a new script loosely based on Kurosawa's.

The director meanwhile had become involved in a much more ambitious Hollywood project. "Tora! Tora! Tora!", produced by 20th Century Fox and Kurosawa Production, would be a portrayal of the Japanese attack on Pearl Harbor from both the American and the Japanese points of view, with Kurosawa helming the Japanese half and an Anglophonic film-maker directing the American half. He spent several months working on the script with Ryuzo Kikushima and Hideo Oguni, but very soon the project began to unravel. The director of the American sequences turned out not to be David Lean, as originally planned, but American Richard Fleischer. The budget was also cut, and the screen time allocated for the Japanese segment would now be no longer than 90 minutes—a major problem, considering that Kurosawa's script ran over four hours. After numerous revisions with the direct involvement of Darryl Zanuck, a more or less finalized cut screenplay was agreed upon in May 1968.

Shooting began in early December, but Kurosawa would last only a little over three weeks as director. He struggled to work with an unfamiliar crew and the requirements of a Hollywood production, while his working methods puzzled his American producers, who ultimately concluded that the director must be mentally ill. Kurosawa was examined at Kyoto University Hospital by a neuropsychologist, Dr. Murakami, whose diagnosis was forwarded to Darryl Zanuck and Richard Zanuck at Fox studios indicating a diagnosis of neurasthenia stating that, "He is suffering from disturbance of sleep, agitated with feelings of anxiety and in manic excitement caused by the above mentioned illness. It is necessary for him to have rest and medical treatment for more than two months." On Christmas Eve 1968, the Americans announced that Kurosawa had left the production due to "fatigue", effectively firing him. He was ultimately replaced, for the film's Japanese sequences, with two directors, Kinji Fukasaku and Toshio Masuda.

"Tora! Tora! Tora!", finally released to unenthusiastic reviews in September 1970, was, as Donald Richie put it, an "almost unmitigated tragedy" in Kurosawa's career. He had spent years of his life on a logistically nightmarish project to which he ultimately did not contribute a foot of film shot by himself. (He had his name removed from the credits, though the script used for the Japanese half was still his and his co-writers'.) He became estranged from his longtime collaborator, writer Ryuzo Kikushima, and never worked with him again. The project had inadvertently exposed corruption in his own production company (a situation reminiscent of his own movie, "The Bad Sleep Well"). His very sanity had been called into question. Worst of all, the Japanese film industry—and perhaps Kurosawa himself—began to suspect that he would never make another film.

Knowing that his reputation was at stake following the much publicised "Tora! Tora! Tora!" debacle, Kurosawa moved quickly to a new project to prove he was still viable. To his aid came friends and famed directors Keisuke Kinoshita, Masaki Kobayashi and Kon Ichikawa, who together with Kurosawa established in July 1969 a production company called the Club of the Four Knights (Yonki no kai). Although the plan was for the four directors to create a film each, it has been suggested that the real motivation for the other three directors was to make it easier for Kurosawa to successfully complete a film, and therefore find his way back into the business.

The first project proposed and worked on was a period film to be called "Dora-heita", but when this was deemed too expensive, attention shifted to "Dodesukaden", an adaptation of yet another Shūgorō Yamamoto work, again about the poor and destitute. The film was shot quickly (by Kurosawa's standards) in about nine weeks, with Kurosawa determined to show he was still capable of working quickly and efficiently within a limited budget. For his first work in color, the dynamic editing and complex compositions of his earlier pictures were set aside, with the artist focusing on the creation of a bold, almost surreal palette of primary colors, in order to reveal the toxic environment in which the characters live. It was released in Japan in October 1970, but though a minor critical success, it was greeted with audience indifference. The picture lost money and caused the Club of the Four Knights to dissolve. Initial reception abroad was somewhat more favorable, but "Dodesukaden" has since been typically considered an interesting experiment not comparable to the director's best work.

After struggling through the production of "Dodesukaden", Kurosawa turned to television work the following year for the only time in his career with "Song of the Horse", a documentary about thoroughbred race horses. It featured a voice-over narrated by a fictional man and a child (voiced by the same actors as the beggar and his son in "Dodesukaden"). It is the only documentary in Kurosawa's filmography; the small crew included his frequent collaborator Masaru Sato, who composed the music. "Song of the Horse" is also unique in Kurosawa's oeuvre in that it includes an editor's credit, suggesting that it is the only Kurosawa film that he did not cut himself.

Unable to secure funding for further work and allegedly having health problems, Kurosawa apparently reached the breaking point: on December 22, 1971, he slit his wrists and throat multiple times. The suicide attempt proved unsuccessful and the director's health recovered fairly quickly, with Kurosawa now taking refuge in domestic life, uncertain if he would ever direct another film.

In early 1973, the Soviet studio Mosfilm approached the film-maker to ask if he would be interested in working with them. Kurosawa proposed an adaptation of Russian explorer Vladimir Arsenyev's autobiographical work "Dersu Uzala". The book, about a Goldi hunter who lives in harmony with nature until destroyed by encroaching civilization, was one that he had wanted to make since the 1930s. In December 1973, the 63-year-old Kurosawa set off for the Soviet Union with four of his closest aides, beginning a year-and-a-half stay in the country. Shooting began in May 1974 in Siberia, with filming in exceedingly harsh natural conditions proving very difficult and demanding. The picture wrapped in April 1975, with a thoroughly exhausted and homesick Kurosawa returning to Japan and his family in June. "Dersu Uzala" had its world premiere in Japan on August 2, 1975, and did well at the box office. While critical reception in Japan was muted, the film was better reviewed abroad, winning the Golden Prize at the 9th Moscow International Film Festival, as well as an Academy Award for Best Foreign Language Film. Today, critics remain divided over the film: some see it as an example of Kurosawa's alleged artistic decline, while others count it among his finest works.

Although proposals for television projects were submitted to him, he had no interest in working outside the film world. Nevertheless, the hard-drinking director did agree to appear in a series of television ads for Suntory whiskey, which aired in 1976. While fearing that he might never be able to make another film, the director nevertheless continued working on various projects, writing scripts and creating detailed illustrations, intending to leave behind a visual record of his plans in case he would never be able to film his stories.

In 1977, American director George Lucas released "Star Wars", a wildly successful science fiction film influenced by Kurosawa's "The Hidden Fortress", among other works. Lucas, like many other New Hollywood directors, revered Kurosawa and considered him a role model, and was shocked to discover that the Japanese film-maker was unable to secure financing for any new work. The two met in San Francisco in July 1978 to discuss the project Kurosawa considered most financially viable: , the epic story of a thief hired as the double of a medieval Japanese lord of a great clan. Lucas, enthralled by the screenplay and Kurosawa's illustrations, leveraged his influence over 20th Century Fox to coerce the studio that had fired Kurosawa just ten years earlier to produce , then recruited fellow fan Francis Ford Coppola as co-producer.

Production began the following April, with Kurosawa in high spirits. Shooting lasted from June 1979 through March 1980 and was plagued with problems, not the least of which was the firing of the original lead actor, Shintaro Katsu—known for portraying the popular character Zatoichi—due to an incident in which the actor insisted, against the director's wishes, on videotaping his own performance. (He was replaced by Tatsuya Nakadai, in his first of two consecutive leading roles in a Kurosawa movie.) The film was completed only a few weeks behind schedule and opened in Tokyo in April 1980. It quickly became a massive hit in Japan. The film was also a critical and box office success abroad, winning the coveted at the 1980 Cannes Film Festival in May, though some critics, then and now, have faulted the film for its alleged coldness. Kurosawa spent much of the rest of the year in Europe and America promoting , collecting awards and accolades, and exhibiting as art the drawings he had made to serve as storyboards for the film.
The international success of allowed Kurosawa to proceed with his next project, , another epic in a similar vein. The script, partly based on William Shakespeare's "King Lear", depicted a ruthless, bloodthirsty "daimyō" (warlord), played by Tatsuya Nakadai, who, after foolishly banishing his one loyal son, surrenders his kingdom to his other two sons, who then betray him, thus plunging the entire kingdom into war. As Japanese studios still felt wary about producing another film that would rank among the most expensive ever made in the country, international help was again needed. This time it came from French producer Serge Silberman, who had produced Luis Buñuel's final movies. Filming did not begin until December 1983 and lasted more than a year.

In January 1985, production of was halted as Kurosawa's 64-year-old wife Yōko fell ill. She died on February 1. Kurosawa returned to finish his film and premiered at the Tokyo Film Festival on May 31, with a wide release the next day. The film was a moderate financial success in Japan, but a larger one abroad and, as he had done with , Kurosawa embarked on a trip to Europe and America, where he attended the film's premieres in September and October.

For his next movie, Kurosawa chose a subject very different from any that he had ever filmed before. While some of his previous pictures (for example, "Drunken Angel" and ) had included brief dream sequences, "Dreams" was to be entirely based upon the director's own dreams. Significantly, for the first time in over forty years, Kurosawa, for this deeply personal project, wrote the screenplay alone. Although its estimated budget was lower than the films immediately preceding it, Japanese studios were still unwilling to back one of his productions, so Kurosawa turned to another famous American fan, Steven Spielberg, who convinced Warner Bros. to buy the international rights to the completed film. This made it easier for Kurosawa's son, Hisao, as co-producer and soon-to-be head of Kurosawa Production, to negotiate a loan in Japan that would cover the film's production costs. Shooting took more than eight months to complete, and "Dreams" premiered at Cannes in May 1990 to a polite but muted reception, similar to the reaction the picture would generate elsewhere in the world. In 1990, he accepted the Academy Award for Lifetime Achievement. In his acceptance speech, he famously said "I'm a little worried because I don't feel that I understand cinema yet." At the time, Bob Thomas of "The Daily Spectrum" noted that Kurosawa was "considered by many critics as the greatest living filmmaker."
Kurosawa now turned to a more conventional story with "Rhapsody in August"—the director's first film fully produced in Japan since "Dodeskaden" over twenty years before—which explored the scars of the nuclear bombing which destroyed Nagasaki at the very end of World War II. It was adapted from a Kiyoko Murata novel, but the film's references to the Nagasaki bombing came from the director rather than from the book. This was his only movie to include a role for an American movie star: Richard Gere, who plays a small role as the nephew of the elderly heroine. Shooting took place in early 1991, with the film opening on May 25 that year to a largely negative critical reaction, especially in the United States, where the director was accused of promulgating naïvely anti-American sentiments, though Kurosawa rejected these accusations.

Kurosawa wasted no time moving onto his next project: "Madadayo", or "Not Yet". Based on autobiographical essays by Hyakken Uchida, the film follows the life of a Japanese professor of German through the Second World War and beyond. The narrative centers on yearly birthday celebrations with his former students, during which the protagonist declares his unwillingness to die just yet—a theme that was becoming increasingly relevant for the film's 81-year-old creator. Filming began in February 1992 and wrapped by the end of September. Its release on April 17, 1993, was greeted by an even more disappointed reaction than had been the case with his two preceding works.

Kurosawa nevertheless continued to work. He wrote the original screenplays "The Sea is Watching" in 1993 and "After the Rain" in 1995. While putting finishing touches on the latter work in 1995, Kurosawa slipped and broke the base of his spine. Following the accident, he would use a wheelchair for the rest of his life, putting an end to any hopes of him directing another film. His longtime wish—to die on the set while shooting a movie—was never to be fulfilled.

After his accident, Kurosawa's health began to deteriorate. While his mind remained sharp and lively, his body was giving up, and for the last half-year of his life, the director was largely confined to bed, listening to music and watching television at home. On September 6, 1998, Kurosawa died of a stroke in Setagaya, Tokyo, at the age of 88. At the time of his death, Kurosawa had two children, his son Hisao Kurosawa who married Hiroko Hayashi and his daughter Kazuko Kurosawa who married Harayuki Kato, along with several grandchildren. One of his grandchildren, the actor Takayuki Kato and , became a supporting actor in two films posthumously developed from screenplays written by Kurosawa which remained unproduced during his own lifetime, Takashi Koizumi's "After the Rain" (1999) and Kei Kumai's "The Sea is Watching" (2002).

Although Kurosawa is primarily known as a filmmaker, he also worked in theater and television and wrote books. A detailed list, including his complete filmography, can be found in the list of works by Akira Kurosawa.

Kurosawa displayed a bold, dynamic style, strongly influenced by Western cinema yet distinct from it; he was involved with all aspects of film production. He was a gifted screenwriter and worked closely with his co-writers from the film's development onward to ensure a high-quality script, which he considered the firm foundation of a good film. He frequently served as editor of his own films. His team, known as the , which included the cinematographer Asakazu Nakai, the production assistant Teruyo Nogami and the actor Takashi Shimura, was notable for its loyalty and dependability.

Kurosawa's style is marked by a number of devices and techniques. In his films of the 1940s and 1950s, he frequently employs the "axial cut", in which the camera moves toward or away from the subject through a series of matched jump cuts rather than tracking shots or dissolves. Another stylistic trait is "cut on motion", which displays the motion on the screen in two or more shots instead of one uninterrupted one. A form of cinematic punctuation strongly identified with Kurosawa is the wipe, an effect created through an optical printer: a line or bar appears to move across the screen, wiping away the end of a scene and revealing the first image of the next. As a transitional device, it is used as a substitute for the straight cut or the dissolve; in his mature work, the wipe became Kurosawa's signature.

In the film's soundtrack, Kurosawa favored the sound-image counterpoint, in which the music or sound effects appeared to comment ironically on the image rather than emphasizing it. Teruyo Nogami's memoir gives several such examples from "Drunken Angel" and "Stray Dog". Kurosawa was also involved with several of Japan's outstanding contemporary composers, including Fumio Hayasaka and Tōru Takemitsu.
Kurosawa employed a number of recurring themes in his films: the master-disciple relationship between a usually older mentor and one or more novices, which often involves spiritual as well as technical mastery and self-mastery; the heroic champion, the exceptional individual who emerges from the mass of people to produce something or right some wrong; the depiction of extremes of weather as both dramatic devices and symbols of human passion; and the recurrence of cycles of savage violence within history. According to Stephen Prince, the last theme, which he calls, "the countertradition to the committed, heroic mode of Kurosawa's cinema," began with "Throne of Blood" (1957), and recurred in the films of the 1980s.

Kurosawa is often cited as one of the greatest filmmakers of all time. In 1999, he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited as "one of the [five] people who contributed most to the betterment of Asia in the past 100 years". Kurosawa was ranked third in the directors' poll and fifth in the critics' poll in Sight & Sound's 2002 list of the greatest directors of all time. In commemoration of the 100th anniversary of Kurosawa's birth in 2010, a project called AK100 was launched in 2008. The AK100 Project aims to "expose young people who are the representatives of the next generation, and all people everywhere, to the light and spirit of Akira Kurosawa and the wonderful world he created".

Many filmmakers have been influenced by Kurosawa's work. Ingmar Bergman called his own film "The Virgin Spring" a "touristic... lousy imitation of Kurosawa", and added, "At that time my admiration for the Japanese cinema was at its height. I was almost a samurai myself!" Federico Fellini considered Kurosawa to be "the greatest living example of all that an author of the cinema should be". Steven Spielberg cited Kurosawa's cinematic vision as being important to shaping his own cinematic vision. Satyajit Ray, who was posthumously awarded the "Akira Kurosawa Award for Lifetime Achievement in Directing" at the San Francisco International Film Festival in 1992, had said earlier of "Rashomon": 

Roman Polanski considered Kurosawa to be among the three film-makers he favored most, along with Fellini and Orson Welles, and picked "Seven Samurai", "Throne of Blood" and "The Hidden Fortress" for praise. Bernardo Bertolucci considered Kurosawa's influence to be seminal: "Kurosawa's movies and "La Dolce Vita" of Fellini are the things that pushed me, sucked me into being a film director." Andrei Tarkovsky cited Kurosawa as one of his favorites and named "Seven Samurai" as one of his ten favorite films. Sidney Lumet called Kurosawa the "Beethoven of movie directors". Werner Herzog reflected on film-makers with whom he feels kinship and the movies that he admires:

According to an assistant, Stanley Kubrick considered Kurosawa to be "one of the great film directors" and spoke of him "consistently and admiringly", to the point that a letter from him "meant more than any Oscar" and caused him to agonize for months over drafting a reply. Robert Altman upon first seeing "Rashomon" was so impressed by the sequence of frames of the sun that he began to shoot the same sequences in his work the very next day, he claimed. George Lucas cited the movie "The Hidden Fortress" as the main inspiration for his film "Star Wars". He also cited other films of Kurosawa as his favorites including "Seven Samurai", "Yojimbo", and . He also said, "I had never seen anything that powerful or cinematographic. The emotions were so strong that it didn't matter that I did not understand the culture or the traditions. From that moment on, Kurosawa's films have served as one of my strongest sources of creative inspiration." Wes Anderson's animated film "Isle of Dogs" is partially inspired by Kurosawa's filming techniques. At the 64th Sydney Film Festival, there was a retrospective of Akira Kurosawa where films of his were screened to remember the great legacy he has created from his work. Zack Snyder cited him as one of his influences for his Netflix film "Rebel Moon".

Kenji Mizoguchi, the acclaimed director of "Ugetsu" (1953) and "Sansho the Bailiff" (1954), was eleven years Kurosawa's senior. After the mid-1950s, some critics of the French New Wave began to favor Mizoguchi over Kurosawa. New Wave critic and film-maker Jacques Rivette, in particular, thought Mizoguchi to be the only Japanese director whose work was at once entirely Japanese and truly universal; Kurosawa, by contrast, was thought to be more influenced by Western cinema and culture, a view that has been disputed.

In Japan, some critics and filmmakers considered Kurosawa to be elitist. They viewed him to center his effort and attention on exceptional or heroic characters. In her DVD commentary on "Seven Samurai", Joan Mellen argued that certain shots of the samurai characters Kambei and Kyuzo, which show Kurosawa to have accorded higher status or validity to them, constitutes evidence for this point of view. These Japanese critics argued that Kurosawa was not sufficiently progressive because the peasants were unable to find leaders from within their ranks. In an interview with Mellen, Kurosawa defended himself, saying, 

From the early 1950s, Kurosawa was also charged with catering to Western tastes due to his popularity in Europe and America. In the 1970s, the politically engaged, left-wing director Nagisa Ōshima, who was noted for his critical reaction to Kurosawa's work, accused Kurosawa of pandering to Western beliefs and ideologies. Author Audie Block, however, assessed Kurosawa to have never played up to a non-Japanese viewing public and to have denounced those directors who did.

Following Kurosawa's death, several posthumous works based on his unfilmed screenplays have been produced. "After the Rain", directed by Takashi Koizumi, was released in 1999, and "The Sea Is Watching", directed by Kei Kumai, premiered in 2002. A script created by the Yonki no Kai ("Club of the Four Knights") (Kurosawa, Keisuke Kinoshita, Masaki Kobayashi, and Kon Ichikawa), around the time that "Dodeskaden" was made, finally was filmed and released (in 2000) as "Dora-heita", by the only surviving founding member of the club, Kon Ichikawa. Huayi Brothers Media and CKF Pictures in China announced in 2017 plans to produce a film of Kurosawa's posthumous screenplay of "The Masque of the Red Death" by Edgar Allan Poe for 2020, to be entitled "The Mask of the Black Death". Patrick Frater writing for "Variety" magazine in May 2017 stated that another two unfinished films by Kurosawa were planned, with "Silvering Spear" to start filming in 2018.

In September 2011, it was reported that remake rights to most of Kurosawa's movies and unproduced screenplays were assigned by the Akira Kurosawa 100 Project to the L.A.-based company Splendent. Splendent's chief Sakiko Yamada, stated that he aimed to "help contemporary film-makers introduce a new generation of moviegoers to these unforgettable stories".

Kurosawa Production Co., established in 1959, continues to oversee many of the aspects of Kurosawa's legacy. The director's son, Hisao Kurosawa, is the current head of the company. Its American subsidiary, Kurosawa Enterprises, is located in Los Angeles. Rights to Kurosawa's works were then held by Kurosawa Production and the film studios under which he worked, most notably Toho. These rights were then assigned to the Akira Kurosawa 100 Project before being reassigned in 2011 to the L.A. based company Splendent. Kurosawa Production works closely with the Akira Kurosawa Foundation, established in December 2003 and also run by Hisao Kurosawa. The foundation organizes an annual short film competition and spearheads Kurosawa-related projects, including a recently shelved one to build a memorial museum for the director.

In 1981, the Kurosawa Film Studio was opened in Yokohama; two additional locations have since been launched in Japan. A large collection of archive material, including scanned screenplays, photos and news articles, has been made available through the Akira Kurosawa Digital Archive, a Japanese proprietary website maintained by Ryukoku University Digital Archives Research Center in collaboration with Kurosawa Production.

Anaheim University in collaboration with Kurosawa Production and the Kurosawa family established the Anaheim University Akira Kurosawa School of Film in spring 2009. The Anaheim University Akira Kurosawa School of Film offers an Online Master of Fine Arts (MFA) in Digital Filmmaking supported by many of the world's greatest filmmakers.

Kurosawa was known to be a connoisseur of Japanese cuisine and as such, the Kurosawa family foundation established the Kurosawa Restaurant Group after his passing in 1999, opening four restaurants in the Tokyo area bearing the family name. "Nagatacho Kurosawa" specializing in Shabu-shabu, "Teppanyaki Kurosawa" in Tsukiji specializing in Teppanyaki, "Keyaki Kurosawa" in Nishi-Azabu specializing in soba, and "Udon Kurosawa" specializing in udon in Roppongi. All four locations were designed to evoke the Meiji era machiya of Kurosawa's youth and feature memorabilia of Kurosawa's career. As of 2023, only the Tsukiji location is currently still operating. A number of entrepreneurs around the world have also opened restaurants and businesses in honor of Kurosawa without any connection to Akira or the estate.

Two film awards have also been named in Kurosawa's honor. The Akira Kurosawa Award for Lifetime Achievement in Film Directing is awarded during the San Francisco International Film Festival, while the Akira Kurosawa Award is given during the Tokyo International Film Festival.

Kurosawa has also been given a number of state honors, including being named as an officer of the French Légion d'honneur in 1984, a Knight Grand Cross of the Order of Merit of the Italian Republic in 1986, and was the first filmmaker to receive the Order of Culture from his native Japan in 1985. Posthumously, he was recognized with the Junior Third Court Rank, which would be the modern equivalent of a noble title under the Kazoku aristocracy.

A significant number of short and full-length documentaries concerning the life and work of Kurosawa were made both during his artistic heyday and after his death. "AK", by French video essay director Chris Marker, was filmed while Kurosawa was working on ; however, the documentary is more concerned about Kurosawa's distant yet polite personality than on the making of the film. Other documentaries concerning Kurosawa's life and works produced posthumously include:



Ancient Egypt

Ancient Egypt was a civilization of ancient Northeast Africa, concentrated along the lower reaches of the Nile River, situated in the place that is now the country Egypt. Ancient Egyptian civilization followed prehistoric Egypt and coalesced around 3100BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer). The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.

Egypt reached the pinnacle of its power during the New Kingdom, ruling much of Nubia and a sizable portion of the Levant, after which it entered a period of slow decline. During the course of its history, Egypt was invaded or conquered by a number of foreign powers, including the Hyksos, the Nubians, the Assyrians, the Achaemenid Persians, and the Macedonians under Alexander the Great. The Greek Ptolemaic Kingdom, formed in the aftermath of Alexander's death, ruled Egypt until 30BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province. Egypt remained under Roman control until the 640s AD, when it was conquered by the Rashidun Caliphate.

The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.

The many achievements of the ancient Egyptians include the quarrying, surveying, and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems, and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Ancient Egypt has left a lasting legacy. Its art and architecture were widely copied, and its antiquities were carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for millennia. A newfound respect for antiquities and excavations in the early modern period by Europeans and Egyptians has led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.

The Nile has been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization. Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120,000 years ago. By the late Paleolithic period, the arid climate of Northern Africa had become increasingly hot and dry, forcing the populations of the area to concentrate along the river region.

In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs, and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.

By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badarian culture, which probably originated in the Western Desert; it was known for its high-quality ceramics, stone tools, and its use of copper.

The Badari was followed by the Naqada culture: the Naqada I (Amratian), the Naqada II (Gerzeh), and Naqada III (Semainean). These brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. Mutual trade with the Levant was established during Naqada II (); this period was also the beginning of trade with Mesopotamia, which continued into the early dynastic period and beyond. Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Nekhen (in Greek, Hierakonpolis), and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east.

The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.

The Early Dynastic Period was approximately contemporary to the early Sumerian-Akkadian civilization of Mesopotamia and of ancient Elam. The third-centuryBC Egyptian priest Manetho grouped the long line of kings from Menes to his own time into 30 dynasties, a system still used today. He began his official history with the king named "Meni" (or Menes in Greek), who was believed to have united the two kingdoms of Upper and Lower Egypt.

The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the king Narmer, who is depicted wearing royal regalia on the ceremonial "Narmer Palette," in a symbolic act of unification. In the Early Dynastic Period, which began about 3000BC, the first of the Dynastic kings solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labor force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the kings during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified king after his death. The strong institution of kingship developed by the kings served to legitimize state control over the land, labor, and resources that were essential to the survival and growth of ancient Egyptian civilization.

Major advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration. Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. 
With the rising importance of central administration in Egypt, a new class of educated scribes and officials arose who were granted estates by the king in payment for their services. Kings also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the king after his death. Scholars believe that five centuries of these practices slowly eroded the economic vitality of Egypt, and that the economy could no longer afford to support a large centralized administration. As the power of the kings diminished, regional governors called nomarchs began to challenge the supremacy of the office of king. This, coupled with severe droughts between 2200 and 2150BC, is believed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.

After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the king, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.

Free from their loyalties to the king, local rulers began competing with each other for territorial control and political power. By 2160BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.

The kings of the Middle Kingdom restored the country's stability and prosperity, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming the kingship at the beginning of the Twelfth Dynasty around 1985BC, shifted the kingdom's capital to the city of Itjtawy, located in Faiyum. From Itjtawy, the kings of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls of the Ruler", to defend against foreign attack.

With the kings having secured the country militarily and politically and with vast agricultural and mineral wealth at their disposal, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom displayed an increase in expressions of personal piety. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style. The relief and portrait sculpture of the period captured subtle, individual details that reached new heights of technical sophistication.

The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the Delta region to provide a sufficient labor force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to assume greater control of the Delta region, eventually coming to power in Egypt as the Hyksos.

Around 1785BC, as the power of the Middle Kingdom kings weakened, a Western Asian people called the Hyksos, who had already settled in the Delta, seized control of Egypt and established their capital at Avaris, forcing the former central government to retreat to Thebes. The king was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") retained Egyptian models of government and identified as kings, thereby integrating Egyptian elements into their culture. They and other invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.

After retreating south, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555BC. The kings Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty and, in the New Kingdom that followed, the military became a central priority for the kings, who sought to expand Egypt's borders and attempted to gain mastery of the Near East.

The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Beginning with Merneptah the rulers of Egypt adopted the title of pharaoh.

Between their reigns, Hatshepsut, a queen who established herself as pharaoh, launched many building projects, including the restoration of temples damaged by the Hyksos, and sent trading expeditions to Punt and the Sinai. When Tuthmosis III died in 1425BC, Egypt had an empire extending from Niya in north west Syria to the Fourth Cataract of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.

The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built.

Around 1350BC, the stability of the New Kingdom was threatened when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and moved the capital to the new city of Akhetaten (modern-day Amarna). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned and the traditional religious order restored. The subsequent pharaohs, Tutankhamun, Ay, and Horemheb, worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.
Around 1279BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258BC.

Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a conjectured confederation of seafarers from the Aegean Sea. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Canaan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.

Following the death of Ramesses XI in 1078BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Libyans had been settling in the western delta, and chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945BC, founding the so-called Libyan or Bubastite dynasty that would rule for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions. Libyan control began to erode as a rival dynasty in the delta arose in Leontopolis, and Kushites threatened from the south.

Around 727BC the Kushite king Piye invaded northward, seizing control of Thebes and eventually the Delta, which established the 25th Dynasty. During the 25th Dynasty, Pharaoh Taharqa created an empire nearly as large as the New Kingdom's. Twenty-fifth Dynasty pharaohs built, or restored, temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, and Jebel Barkal. During this period, the Nile valley saw the first widespread construction of pyramids (many in modern Sudan) since the Middle Kingdom.

Egypt's far-reaching prestige declined considerably toward the end of the Third Intermediate Period. Its foreign allies had fallen under the Assyrian sphere of influence, and by 700BC war between the two states became inevitable. Between 671 and 667BC the Assyrians began the Assyrian conquest of Egypt. The reigns of both Taharqa and his successor, Tanutamun, were filled with constant conflict with the Assyrians, against whom Egypt enjoyed several victories. Ultimately, the Assyrians pushed the Kushites back into Nubia, occupied Memphis, and sacked the temples of Thebes.

The Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-Sixth Dynasty. By 653BC, the Saite king Psamtik I was able to oust the Assyrians with the help of Greek mercenaries, who were recruited to form Egypt's first navy. Greek influence expanded greatly as the city-state of Naucratis became the home of Greeks in the Nile Delta. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the Battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from Iran, leaving Egypt under the control of a satrap. A few successful revolts against the Persians marked the 5th centuryBC, but Egypt was never able to permanently overthrow the Persians.

Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-Seventh Dynasty, ended in 402BC, when Egypt regained independence under a series of native dynasties. The last of these dynasties, the Thirtieth, proved to be the last native royal house of ancient Egypt, ending with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-First Dynasty, began in 343BC, but shortly after, in 332BC, the Persian ruler Mazaces handed Egypt over to Alexander the Great without a fight.

In 332BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic Kingdom, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a centre of learning and culture, that included the famous Library of Alexandria as part of the Mouseion. The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.

Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.

Egypt became a province of the Roman Empire in 30BC, following the defeat of Mark Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.

Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.

From the mid-first century AD, Christianity took root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from the pagan Egyptian and Greco-Roman religions and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out. In 391, the Christian emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's native religious culture was continually in decline. While the native population continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.

In the fourth century, as the Roman Empire divided, Egypt found itself in the Eastern Empire with its capital at Constantinople. In the waning years of the Empire, Egypt fell to the Sasanian Persian army in the Sasanian conquest of Egypt (618–628). It was then recaptured by the Byzantine emperor Heraclius (629–639), and was finally captured by Muslim Rashidun army in 639–641, marking the end of both Byzantine rule and of the period typically considered Ancient Egypt.

The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they places of worship, but were also responsible for collecting and storing the kingdom's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.

Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn  sacks (200 kg or 400 lb) of grain per month, while a foreman might earn  sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140deben. Grain could be traded for other goods, according to the fixed price list. During the fifth centuryBC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.

Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. It is unclear whether slavery as understood today existed in ancient Egypt; there is difference of opinions among authors.

The ancient Egyptians viewed men and women, including people from all social classes, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Although slaves were mostly used as indentured servants, they were able to buy and sell their servitude, work their way to freedom or nobility, and were usually treated by doctors in the workplace. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices, legal rights, and opportunities for achievement. Women such as Hatshepsut and Cleopatra VII even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, aside from the royal high priestesses, apparently served only secondary roles in the temples (not much data for many dynasties), and were not so likely to be as educated as men.

The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.

Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgement by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.

A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.

Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.

The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.

The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry, such as ducks, geese, and pigeons, were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and provided both honey and wax.

The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period. Camels, although known from the New Kingdom, were not used as beasts of burden until the Late Period. There is also evidence to suggest that elephants were briefly used in the Late Period but largely abandoned due to lack of grazing land. Cats, dogs, and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as Sub-Saharan African lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Late Period, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were kept in large numbers for the purpose of ritual sacrifice.

Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the Eastern Desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.

The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were used in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the Eastern Desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the Eastern Desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.

The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.

By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil.

The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the longest known history of any language having been written from c. 3200BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.

Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian developed prefixal definite and indefinite articles, which replaced the older inflectional suffixes. There was a change from the older verb–subject–object word order to subject–verb–object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.

Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Late Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton is the semantic core of the word 'hear'; its basic conjugation is ', 'he hears'. If the subject is a noun, suffixes are not added to the verb: ', 'the woman hears'.

Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is in verbal and adjectival sentences, and in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).

Hieroglyphic writing dates from c. 3000BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.

Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in the 1820s, after the discovery of the Rosetta Stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs substantially deciphered.

Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300BC. Late Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.

The "Story of Sinuhe", written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of Near Eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Greco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.

Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mudbrick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread. Ceramics served as household wares for the storage, preparation, transport, and consumption of food, drink, and raw materials. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.

The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.

Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.

The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. "Hounds and Jackals" also known as 58 holes is another example of board games played in ancient Egypt. The first complete set of this game was discovered from a Theban tomb of the Egyptian pharaoh Amenemhat IV that dates to the 13th Dynasty. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting, fishing, and boating as well.

The excavation of the workers' village of Deir el-Medina has resulted in one of the most thoroughly documented accounts of community life in the ancient world, which spans almost four hundred years. There is no comparable site in which the organization, social interactions, and working and living conditions of a community have been studied in such detail.

Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.

The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the wide-ranging power of the pharaoh. The ancient Egyptians were skilled builders; using only simple but effective tools and sighting instruments, architects could build large stone structures with great accuracy and precision that is still envied today.

The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mudbricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite and the pharaoh were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of mudbricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.

The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs. The use of the pyramid form continued in private tomb chapels of the New Kingdom and in the royal pyramids of Nubia.

The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.
Ancient Egyptian artisans used stone as a medium for carving statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.

Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.

Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna Period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly abandoned after Akhenaten's death and replaced by the traditional forms.

Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.

Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.

The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth." If deemed worthy, the deceased could continue their existence on earth in spiritual form. If they were not deemed worthy, their heart was eaten by Ammit the Devourer and they were erased from the Universe.

The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.

By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.

Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Funerary texts were often included in the grave, and, beginning in the New Kingdom, so were shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.

The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.

Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops." Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.

In technology, medicine, and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.

Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper. The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.

The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.

The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).

The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.
Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.

Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection, while opium, thyme, and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey, and copper salts were also used to prevent infection from dirt in burns. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.

Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000BC. The Archaeological Institute of America reports that the oldest planked ships known are the Abydos boats. A group of 14 discovered ships in Abydos were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000BC was long and is now thought to perhaps have belonged to an earlier pharaoh, perhaps one as early as Hor-Aha.

Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.

Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especially Byblos (on the coast of modern-day Lebanon), and in several expeditions down the Red Sea to the Land of Punt. In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship", which originally defined a class of Egyptian seagoing ships used on the Byblos run; however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.

In 1977, an ancient north–south canal was discovered extending from Lake Timsah to the Ballah Lakes. It was dated to the Middle Kingdom of Egypt by extrapolating dates of ancient sites constructed along its course.

In 2011, archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut's Punt expedition onto the open ocean. Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles. In 2013, a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Khufu on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez).

The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, calculate the areas of rectangles, triangles, and circles and compute the volumes of boxes, columns and pyramids. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.

Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction "two-fifths" into the sum of "one-third" + "one-fifteenth". Standard tables of values facilitated this. Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.

Ancient Egyptian mathematicians knew the Pythagorean theorem as an empirical formula. They were aware, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio. They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:

a reasonable approximation of the formula .

Estimates of the size of the population range from 1–1.5 million in the 3rd millennium BC to possibly 2–3 million by the 1st millennium BC, before growing significantly towards the end of that millennium.

According to historian William Stiebling and archaeologist Susan N. Helft, conflicting DNA analysis on recent genetic samples such as the Amarna royal mummies has led to a lack of consensus on the genetic makeup of the ancient Egyptians and their geographic origins.

The genetic history of Ancient Egypt remains a developing field, and is relevant for the understanding of population demographic events connecting Africa and Eurasia. To date, the amount of genome-wide aDNA analyses on ancient specimens from Egypt and Sudan remain scarce, although studies on uniparental haplogroups in ancient individuals have been carried out several times, pointing broadly to affinities with other African and Eurasian groups.

The currently most advanced full genome analyses was made on three ancient specimens recovered from the Nile River Valley, Abusir el-Meleq, Egypt. Two of the individuals were dated to the Pre-Ptolemaic Period (New Kingdom to Late Period), and one individual to the Ptolemaic Period, spanning around 1300 years of Egyptian history. These results point to a genetic continuity of Ancient Egyptians with modern Egyptians. The results further point to a close genetic affinity between ancient Egyptians and Middle Eastern populations, especially ancient groups from the Levant (Natufian culture).

Ancient Egyptians also displayed affinities to Nubians to the south of Egypt, in modern day Sudan. Archaeological and historical evidence support interactions between Egyptian and Nubian populations more than 5000 years ago, with socio-political dynamics between Egyptians and Nubians ranging from peaceful coexistence to variably successful attempts of conquest. A study on sixty-six ancient Nubian individuals revealed significant contact with ancient Egyptians, characterized by the presence of c. 57% Neolithic/Bronze Age Levantine ancestry in these individuals. Such geneflow of Levantine-like ancestry corresponds with archaeological and botanic evidence, pointing to a Neolithic movement around 7,000 years ago.

Genetic data on other Northern African specimens, such as the c. 15,000 year old Iberomaurusian Taforalt man, but also specimens from the "last Green Sahara" and the Savanna Pastoral Neolithic, point to the widespread presence of an (Western) Eurasian ancestry component distributed throughout Northern Africa, the Sahara, and the Horn of Africa, having arrived via back migration(s) from the Middle East starting as early as c. 23,000 years ago.

Modern Egyptians, like modern Nubians, underwent subsequent admixture events, contributing both "Sub-Saharan" African-like and West Asian-like ancestries, since the Roman period, with significance on the African Slave Trade and the Spread of Islam.

Some scholars, such as Christopher Ehret, caution that a wider sampling area is needed and argue that the current data is inconclusive on the origin of ancient Egyptians. They also point out issues with the previously used methodology such as the sampling size, comparative approach and a "biased interpretation" of the genetic data. They argue in favor for a link between Ancient Egypt and the northern Horn of Africa. This latter view has been attributed to the corresponding archaeological, genetic, linguistic and biological anthropological sources of evidence which broadly indicate that the earliest Egyptians and Nubians were the descendants of populations in northeast Africa.

The culture and monuments of ancient Egypt have left a lasting legacy on the world. Egyptian civilization significantly influenced the Kingdom of Kush and Meroë with both adopting Egyptian religious and architectural norms (hundreds of pyramids (6–30 meters high) were built in Egypt/Sudan), as well as using Egyptian writing as the basis of the Meroitic script. Meroitic is the oldest written language in Africa, other than Egyptian, and was used from the 2nd century BC until the early 5th century AD. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect Egyptian-style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.

During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe, as evident in symbolism like the Eye of Providence and the Great Seal of the United States. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities. Napoleon arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Égypte".

In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. Since the 2010s, the Ministry of Tourism and Antiquities has overseen excavations and the recovery of artifacts.




Analog Brothers

Analog Brothers were an experimental hip hop band featuring Tracy "Ice-T" Marrow (Ice Oscillator) on keyboards, drums and vocals, Keith "Kool Keith" Thornton (Keith Korg) on bass, strings and vocals, Marc Live (Marc Moog) on drums, violins and vocals, Christopher "Black Silver" Rodgers (Silver Synth) on synthesizer, lazar bell and vocals, and Rex Colonel "Pimpin' Rex" Doby Jr. (Rex Roland JX3P) on keyboards, vocals and production. 

The group's only studio album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky Jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., Synth-A-Size Sisters and Teflon.

While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky Jasper to release two albums as KHM. Marc Live rapped with Ice-T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released.

In 2008, Ice-T and Black Silver toured together as Black Ice, and released an album together called "Urban Legends".

In 2013, Black Silver and newest member to Analog Brothers, Kiew Kurzweil (Kiew Nikon of Kinetic) collaborated on the joint album called "Slang Banging (Return to Analog)" with production by Junkadelic Music. In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.



Motor neuron diseases

Motor neuron diseases or motor neurone diseases (MNDs) are a group of rare neurodegenerative disorders that selectively affect motor neurons, the cells which control voluntary muscles of the body. They include amyotrophic lateral sclerosis (ALS), progressive bulbar palsy (PBP), pseudobulbar palsy, progressive muscular atrophy (PMA), primary lateral sclerosis (PLS), spinal muscular atrophy (SMA) and monomelic amyotrophy (MMA), as well as some rarer variants resembling ALS.

Motor neuron diseases affect both children and adults. While each motor neuron disease affects patients differently, they all cause movement-related symptoms, mainly muscle weakness. Most of these diseases seem to occur randomly without known causes, but some forms are inherited. Studies into these inherited forms have led to discoveries of various genes (e.g. "SOD1") that are thought to be important in understanding how the disease occurs.

Symptoms of motor neuron diseases can be first seen at birth or can come on slowly later in life. Most of these diseases worsen over time; while some, such as ALS, shorten one's life expectancy, others do not. Currently, there are no approved treatments for the majority of motor neuron disorders, and care is mostly symptomatic.

Signs and symptoms depend on the specific disease, but motor neuron diseases typically manifest as a group of movement-related symptoms. They come on slowly, and worsen over the course of more than three months. Various patterns of muscle weakness are seen, and muscle cramps and spasms may occur. One can have difficulty breathing with climbing stairs (exertion), difficulty breathing when lying down (orthopnea), or even respiratory failure if breathing muscles become involved. Bulbar symptoms, including difficulty speaking (dysarthria), difficulty swallowing (dysphagia), and excessive saliva production (sialorrhea), can also occur. Sensation, or the ability to feel, is typically not affected. Emotional disturbance (e.g. pseudobulbar affect) and cognitive and behavioural changes (e.g. problems in word fluency, decision-making, and memory) are also seen. There can be lower motor neuron findings (e.g. muscle wasting, muscle twitching), upper motor neuron findings (e.g. brisk reflexes, Babinski reflex, Hoffman's reflex, increased muscle tone), or both.

Motor neuron diseases are seen both in children and adults. Those that affect children tend to be inherited or familial, and their symptoms are either present at birth or appear before learning to walk. Those that affect adults tend to appear after age 40. The clinical course depends on the specific disease, but most progress or worsen over the course of months. Some are fatal (e.g. ALS), while others are not (e.g. PLS).

Various patterns of muscle weakness occur in different motor neuron diseases. Weakness can be symmetric or asymmetric, and it can occur in body parts that are distal, proximal, or both. According to Statland et al., there are three main weakness patterns that are seen in motor neuron diseases, which are:


Motor neuron diseases are on a spectrum in terms of upper and lower motor neuron involvement. Some have just lower or upper motor neuron findings, while others have a mix of both. Lower motor neuron (LMN) findings include muscle atrophy and fasciculations, and upper motor neuron (UMN) findings include hyperreflexia, spasticity, muscle spasm, and abnormal reflexes.

Pure upper motor neuron diseases, or those with just UMN findings, include PLS.

Pure lower motor neuron diseases, or those with just LMN findings, include PMA.

Motor neuron diseases with both UMN and LMN findings include both familial and sporadic ALS.

Most cases are sporadic and their causes are usually not known. It is thought that environmental, toxic, viral, or genetic factors may be involved.

TAR DNA-binding protein 43 (TDP-43), is a critical component of the non-homologous end joining (NHEJ) enzymatic pathway that repairs DNA double-strand breaks in pluripotent stem cell-derived motor neurons. TDP-43 is rapidly recruited to double-strand breaks where it acts as a scaffold for the recruitment of the XRCC4-DNA ligase protein complex that then acts to repair double-strand breaks. About 95% of ALS patients have abnormalities in the nucleus-cytoplasmic localization in spinal motor neurons of TDP43. In TDP-43 depleted human neural stem cell-derived motor neurons, as well as in sporadic ALS patients' spinal cord specimens there is significant double-strand break accumulation and reduced levels of NHEJ.

In adults, men are more commonly affected than women.

Differential diagnosis can be challenging due to the number of overlapping symptoms, shared between several motor neuron diseases. Frequently, the diagnosis is based on clinical findings (i.e. LMN vs. UMN signs and symptoms, patterns of weakness), family history of MND, and a variation of tests, many of which are used to rule out disease mimics, which can manifest with identical symptoms.

Motor neuron disease describes a collection of clinical disorders, characterized by progressive muscle weakness and the degeneration of the motor neuron on electrophysiological testing. The term "motor neuron disease" has varying meanings in different countries. Similarly, the literature inconsistently classifies which degenerative motor neuron disorders can be included under the umbrella term "motor neuron disease". The four main types of MND are marked (*) in the table below.

All types of MND can be differentiated by two defining characteristics:


Sporadic or acquired MNDs occur in patients with no family history of degenerative motor neuron disease. Inherited or genetic MNDs adhere to one of the following inheritance patterns: autosomal dominant, autosomal recessive, or X-linked. Some disorders, like ALS, can occur sporadically (85%) or can have a genetic cause (15%) with the same clinical symptoms and progression of disease.

UMNs are motor neurons that project from the cortex down to the brainstem or spinal cord. LMNs originate in the anterior horns of the spinal cord and synapse on peripheral muscles. Both motor neurons are necessary for the strong contraction of a muscle, but damage to an UMN can be distinguished from damage to a LMN by physical exam.


There are no known curative treatments for the majority of motor neuron disorders. Please refer to the articles on individual disorders for more details.

The table below lists life expectancy for patients who are diagnosed with MND.
In the United States and Canada, the term "motor neuron disease" usually refers to the group of disorders while amyotrophic lateral sclerosis is frequently called "Lou Gehrig's disease". In the United Kingdom and Australia, the term "motor neuron(e) disease" is used for amyotrophic lateral sclerosis, although is not uncommon to refer to the entire group.

While MND refers to a specific subset of similar diseases, there are numerous other diseases of motor neurons that are referred to collectively as "motor neuron disorders", for instance the diseases belonging to the spinal muscular atrophies group. However, they are not classified as "motor neuron diseases" by the 11th edition of the International Statistical Classification of Diseases and Related Health Problems (ICD-11), which is the definition followed in this article.



Abjad

An abjad (, ; Phoenician: abgad) is a writing system in which only consonants are represented, leaving vowel sounds to be inferred by the reader. This contrasts with alphabets, which provide graphemes for both consonants and vowels. The term was introduced in 1990 by Peter T. Daniels. Other terms for the same concept include: partial phonemic script, segmentally linear defective phonographic script, consonantary, consonant writing, and consonantal alphabet.

Impure abjads represent vowels with either optional diacritics, a limited number of distinct vowel glyphs, or both.

The name "abjad" is based on the Arabic alphabet's first (in its original order) four letters—corresponding to a, b, j, and d—to replace the more common terms "consonantary" and "consonantal alphabet", in describing the family of scripts classified as "West Semitic". Similar to other Semitic languages such as Phoenician, Hebrew and Semitic proto-alphabets: specifically, aleph, bet, gimel, dalet.

According to the formulations of Peter T. Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category defined by Daniels, in that in abjads, the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and ḥarakāt for Arabic, their use is optional and not the dominant (or literate) form. Abugidas mark all vowels (other than the "inherent" vowel) with a diacritic, a minor attachment to the letter, a standalone glyph, or (in Canadian Aboriginal syllabics) by rotation of the letter. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds.

The contrast of abjad versus alphabet has been rejected by other scholars because "abjad" is also used as a term for the Arabic numeral system. Also, it may be taken as suggesting that consonantal alphabets, in contrast to e.g. the Greek alphabet, were not yet true alphabets. Florian Coulmas, a critic of Daniels and of the abjad terminology, argues that this terminology can confuse alphabets with "transcription systems", and that there is no reason to relegate the Hebrew, Aramaic or Phoenician alphabets to second-class status as an "incomplete alphabet".
However, Daniels's terminology has found acceptance in the linguistic community.

The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only a few dozen symbols. This made the script easy to learn, and seafaring Phoenician merchants took the script throughout the then-known world.

The Phoenician abjad was a radical simplification of phonetic writing, since hieroglyphics required the writer to pick a hieroglyph starting with the same sound that the writer wanted to write in order to write phonetically, much as "man'yōgana" (kanji used solely for phonetic use) was used to represent Japanese phonetically before the invention of kana.

Phoenician gave rise to a number of new writing systems, including the widely used Aramaic abjad and the Greek alphabet. The Greek alphabet evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia.

Impure abjads have characters for some vowels, optional vowel diacritics, or both. The term pure abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Hebrew, Aramaic, and Pahlavi, are "impure" abjadsthat is, they also contain symbols for some of the vowel phonemes, although the said non-diacritic vowel letters are also used to write certain consonants, particularly approximants that sound similar to long vowels. A "pure" abjad is exemplified (perhaps) by very early forms of ancient Phoenician, though at some point (at least by the 9th century BC) it and most of the contemporary Semitic abjads had begun to overload a few of the consonant symbols with a secondary function as vowel markers, called "matres lectionis". This practice was at first rare and limited in scope but became increasingly common and more developed in later times.

In the 9th century BC the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by "aleph", "he", "heth" or "ayin", so these symbols were assigned vocalic values. The letters "waw" and "yod" were also adapted into vowel signs; along with "he", these were already used as "matres lectionis" in Phoenician. The major innovation of Greek was to dedicate these symbols exclusively and unambiguously to vowel sounds that could be combined arbitrarily with consonants (as opposed to syllabaries such as Linear B which usually have vowel symbols but cannot combine them with consonants to form arbitrary syllables).

Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian abjad evolved into the Ge'ez abugida of Ethiopia between the 5th century BC and the 5th century AD. Similarly, the Brāhmī abugida of the Indian subcontinent developed around the 3rd century BC (from the Aramaic abjad, it has been hypothesized).

The other major family of abugidas, Canadian Aboriginal syllabics, was initially developed in the 1840s by missionary and linguist James Evans for the Cree and Ojibwe languages. Evans used features of Devanagari script and Pitman shorthand to create his initial abugida. Later in the 19th century, other missionaries adapted Evans's system to other Canadian aboriginal languages. Canadian syllabics differ from other abugidas in that the vowel is indicated by rotation of the consonantal symbol, with each vowel having a consistent orientation.

The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, according to Classical Arabic and Modern Standard Arabic, from the Arabic root "K-T-B" (to write) can be derived the forms ' (he wrote), ' (you (masculine singular) wrote), ' (he writes), and ' (library). In most cases, the absence of full glyphs for vowels makes the common root clearer, allowing readers to guess the meaning of unfamiliar words from familiar roots (especially in conjunction with context clues) and improving word recognition while reading for practiced readers.

By contrast, the Arabic and Hebrew scripts sometimes perform the role of true alphabets rather than abjads when used to write certain Indo-European languages, including Kurdish, Bosnian, Yiddish, and some Romance languages such as Mozarabic, Aragonese, Portuguese, Spanish and Ladino.




Abugida

An abugida (; from Ge'ez: )sometimes also called alphasyllabary, neosyllabary, or pseudo-alphabetis a segmental writing system in which consonant–vowel sequences are written as units; each unit is based on a consonant letter, and vowel notation is secondary, like a diacritical mark. This contrasts with a full alphabet, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent, partial, or optional – in less formal contexts, all three types of the script may be termed "alphabets". The terms also contrast them with a syllabary, in which a single symbol denotes the combination of one consonant and one vowel.

Related concepts were introduced independently in 1948 by James Germain Février (using the term ) and David Diringer (using the term "semisyllabary"), then in 1959 by Fred Householder (introducing the term "pseudo-alphabet"). The Ethiopic term "abugida" was chosen as a designation for the concept in 1990 by Peter T. Daniels. In 1992, Faber suggested "segmentally coded syllabically linear phonographic script", and in 1992 Bright used the term "alphasyllabary", and Gnanadesikan and Rimzhim, Katz, & Fowler have suggested "aksara" or "āksharik".

Abugidas include the extensive Brahmic family of scripts of Tibet, South and Southeast Asia, Semitic Ethiopic scripts, and Canadian Aboriginal syllabics. As is the case for syllabaries, the units of the writing system may consist of the representations both of syllables and of consonants. For scripts of the Brahmic family, the term "akshara" is used for the units.

In several languages of Ethiopia and Eritrea, "abugida" traditionally meant letters of the Ethiopic or Ge‘ez script in which many of these languages are written. Ge'ez is one of several segmental writing systems in the world, others include Indic/Brahmic scripts and Canadian Aboriginal Syllabics. The word abugida is derived from the four letters, "ä, bu, gi," and "da", in much the same way that "abecedary" is derived from Latin letters "a be ce de", "abjad" is derived from the Arabic "a b j d", and "alphabet" is derived from the names of the two first letters in the Greek alphabet, "alpha" and "beta". "Abugida" as a term in linguistics was proposed by Peter T. Daniels in his 1990 typology of writing systems.

As Daniels used the word, an abugida is in contrast with a syllabary, where letters with shared consonant or vowel sounds show no particular resemblance to one another. Furthermore, an abugida is also in contrast with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that, "they share features of both alphabet and syllabary."

The formal definitions given by Daniels and Bright for abugida and alphasyllabary differ; some writing systems are abugidas but not alphasyllabaries, and some are alphasyllabaries but not abugidas. An abugida is defined as "a type of writing system whose basic characters denote consonants followed by a particular vowel, and in which diacritics denote other vowels". (This 'particular vowel' is referred to as the "inherent" or "implicit" vowel, as opposed to the "explicit" vowels marked by the 'diacritics'.)

An alphasyllabary is defined as "a type of writing system in which the vowels are denoted by subsidiary symbols, not all of which occur in a linear order (with relation to the consonant symbols) that is congruent with their temporal order in speech". Bright did not require that an alphabet explicitly represent all vowels. ʼPhags-pa is an example of an abugida because it has an inherent vowel, but it is not an alphasyllabary because its vowels are written in linear order. Modern Lao is an example of an alphasyllabary that is not an abugida, for there is no inherent vowel and its vowels are always written explicitly and not in accordance to their temporal order in speech, meaning that a vowel can be written before, below or above a consonant letter, while the syllable is still pronounced in the order of a consonant-vowel combination (CV).

The fundamental principles of an abugida apply to words made up of consonant-vowel (CV) syllables. The syllables are written as letters in a straight line, where each syllable is either a letter that represents the sound of a consonant and its inherent vowel or a letter modified to indicate the vowel. Letters can be modified either by means of diacritics or by changes in the form of the letter itself. If all modifications are by diacritics and all diacritics follow the direction of the writing of the letters, then the abugida is not an alphasyllabary. However, most languages have words that are more complicated than a sequence of CV syllables, even ignoring tone.

The first complication is syllables that consist of just a vowel (V). For some languages, a zero consonant letter is used as though every syllable began with a consonant. For other languages, each vowel has a separate letter that is used for each syllable consisting of just the vowel. These letters are known as "independent vowels", and are found in most Indic scripts. These letters may be quite different from the corresponding diacritics, which by contrast are known as "dependent vowels". As a result of the spread of writing systems, independent vowels may be used to represent syllables beginning with a glottal stop, even for non-initial syllables.

The next two complications are consonant clusters before a vowel (CCV) and syllables ending in a consonant (CVC). The simplest solution, which is not always available, is to break with the principle of writing words as a sequence of syllables and use a letter representing just a consonant (C). This final consonant may be represented with:

In a true abugida, the lack of distinctive vowel marking of the letter may result from the diachronic loss of the inherent vowel, e.g. by syncope and apocope in Hindi.

When not separating syllables containing consonant clusters (CCV) into C + CV, these syllables are often written by combining the two consonants. In the Indic scripts, the earliest method was simply to arrange them vertically, writing the second consonant of the cluster below the first one. The two consonants may also merge as conjunct consonant letters, where two or more letters are graphically joined in a ligature, or otherwise change their shapes. Rarely, one of the consonants may be replaced by a gemination mark, e.g. the Gurmukhi "addak".

When they are arranged vertically, as in Burmese or Khmer, they are said to be 'stacked'. Often there has been a change to writing the two consonants side by side. In the latter case, this combination may be indicated by a diacritic on one of the consonants or a change in the form of one of the consonants, e.g. the half forms of Devanagari. Generally, the reading order of stacked consonants is top to bottom, or the general reading order of the script, but sometimes the reading order can be reversed.

The division of a word into syllables for the purposes of writing does not always accord with the natural phonetics of the language. For example, Brahmic scripts commonly handle a phonetic sequence CVC-CV as CV-CCV or CV-C-CV. However, sometimes phonetic CVC syllables are handled as single units, and the final consonant may be represented:

More complicated unit structures (e.g. CC or CCVC) are handled by combining the various techniques above.

Examples using the Devanagari script

There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation."

Lao and Tāna have dependent vowels and a zero vowel sign, but no inherent vowel.

Indic scripts originated in India and spread to Southeast Asia, Bangladesh, Sri Lanka, Nepal, Bhutan, Tibet, Mongolia, and Russia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and some other languages of Pakistan and India), mainland Southeast Asia (Myanmar, Thailand, Laos, Cambodia, and Vietnam), Tibet (Tibetan), Indonesian archipelago (Javanese, Balinese, Sundanese, Batak, Lontara, Rejang, Rencong, Makasar, etc.), Philippines (Baybayin, Buhid, Hanunuo, Kulitan, and Aborlan Tagbanwa), Malaysia (Rencong).

The primary division is with North Indic scripts, used in Northern India, Nepal, Tibet, Bhutan, Mongolia, and Russia; and Southern Indic scripts, used in South India, Sri Lanka and Southeast Asia. South Indic letter forms are more rounded than North Indic forms, though Odia, Golmol and Litumol of Nepal script are rounded. Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati and Odia as exceptions; South Indic scripts do not.

Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant.

The most widely used Indic script is Devanagari, shared by Hindi, Bihari, Marathi, Konkani, Nepali, and often Sanskrit. A basic letter such as क in Hindi represents a syllable with the default vowel, in this case "ka" (). In some languages, including Hindi, it becomes a final closing consonant at the end of a word, in this case "k". The inherent vowel may be changed by adding vowel mark (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko."

In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like ि "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट ; the diacritic for appears before the consonant cluster , not before the . A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable.

In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virāma" or "halantam" in Sanskrit. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. Thus in Sanskrit, a default vowel consonant such as क does not take on a final consonant sound. Instead, it keeps its vowel. For writing two consonants without a vowel in between, instead of using diacritics on the first consonant to remove its vowel, another popular method of special conjunct forms is used in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two "aksharas" to write.

The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single "akshara" can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable [sok] would be written as something like s̥̽, here with an underring representing and an overcross representing the diacritic for final . Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as or , if they can indicate any at all.

In Ethiopic or Ge'ez script, "fidels" (individual "letters" of the script) have "diacritics" that are fused with the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant are readily apparent, unlike the case in a true syllabary.

Though now an abugida, the Ge'ez script, until the advent of Christianity ("ca." AD 350), had originally been what would now be termed an "abjad". In the Ge'ez abugida (or "fidel"), the base form of the letter (also known as "fidel") may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that does not alter the letter), ሂ "hi" (with a subdiacritic that compresses the consonant, so it is the same height), ህ "hə" or (where the letter is modified with a kink in the left arm).

In the family known as Canadian Aboriginal syllabics, which was inspired by the Devanagari script of India, vowels are indicated by changing the orientation of the syllabogram. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the "aksharas"; there is no vowel-killer mark.

Consonantal scripts ("abjads") are normally written without indication of many vowels. However, in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks ("harakat", "niqqud") making them effectively alphasyllabaries.
The Arabic scripts used for Kurdish in Iraq and for Uyghur in Xinjiang, China, as well as the Hebrew script of Yiddish, are fully vowelled, but because the vowels are written with full letters rather than diacritics (with the exception of distinguishing between /a/ and /o/ in the latter) and there are no inherent vowels, these are considered alphabets, not abugidas.

The Arabic script used for South Azerbaijani generally writes the vowel /æ/ (written as ə in North Azerbaijani) as a diacritic, but writes all other vowels as full letters (similarly to Kurdish and Uyghur). This means that when no vowel diacritics are present (most of the time), it technically has an inherent vowel. However, like the Phagspa and Meroitic scripts whose status as abugidas is controversial (see below), all other vowels are written in-line. Additionally, the practice of explicitly writing all-but-one vowel does not apply to loanwords from Arabic and Persian, so the script does not have an inherent vowel for Arabic and Persian words. The inconsistency of its vowel notation makes it difficult to categorize.

The imperial Mongol script called Phagspa was derived from the Tibetan abugida, but all vowels are written in-line rather than as diacritics. However, it retains the features of having an inherent vowel /a/ and having distinct initial vowel letters.

Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime (with mid tone) is unwritten, it also has an inherent onset . For the syllable , which requires one or the other of the inherent sounds to be overt, it is that is written. Thus it is the rime (vowel) that is basic to the system.

It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family of abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.

Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone. Pitman shorthand uses straight strokes and quarter-circle marks in different orientations as the principal "alphabet" of consonants; vowels are shown as light and heavy dots, dashes and other marks in one of 3 possible positions to indicate the various vowel-sounds. However, to increase writing speed, Pitman has rules for "vowel indication" using the positioning or choice of consonant signs so that writing vowel-marks can be dispensed with.

As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set.

Most Indian and Indochinese abugidas appear to have first been developed from abjads with the Kharoṣṭhī and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia.

Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction or adoption of Christianity about AD 350. The Ethiopic script is the elaboration of an abjad.

The Cree syllabary was invented with full knowledge of the Devanagari system.

The Meroitic script was developed from Egyptian hieroglyphs, within which various schemes of 'group writing' had been used for showing vowels.



ABBA

ABBA ( , ; formerly named Björn & Benny, Agnetha & Anni-Frid or Björn & Benny, Agnetha & Frida) were a Swedish pop supergroup formed in Stockholm in 1972 by Agnetha Fältskog, Björn Ulvaeus, Benny Andersson, and Anni-Frid Lyngstad. The group's name is an acronym of the first letters of their first names arranged as a palindrome. They are one of the most popular and successful musical groups of all time, and are one of the best-selling music acts in the history of popular music, topping the charts worldwide from 1974 to 1982, and again from 2016 to 2022 following their brief reunion.

In , ABBA were 's first winner of the Eurovision Song Contest with the song "Waterloo", which in 2005 was chosen as the best song in the competition's history as part of the of the contest. During the band's main active years, it consisted of two married couples: Fältskog and Ulvaeus, and Lyngstad and Andersson. With the increase of their popularity, their personal lives suffered, which eventually resulted in the collapse of both marriages. The relationship changes were reflected in the group's music, with later songs featuring darker and more introspective lyrics. After ABBA disbanded in December 1982, Andersson and Ulvaeus continued their success writing music for multiple audiences including stage, musicals and movies, while Fältskog and Lyngstad pursued solo careers.

Ten years after the group broke up, a compilation, "ABBA Gold", was released becoming a worldwide best-seller. In 1999, ABBA's music was adapted into "Mamma Mia!", a stage musical that toured worldwide and, as of April 2022, is still in the top-ten longest running productions on both Broadway (closed in 2015) and the West End (still running). A film of the same title, released in 2008, became the highest-grossing film in the United Kingdom that year. A sequel, "Mamma Mia! Here We Go Again", was released in 2018.

In 2016, the group reunited and started working on a digital avatar concert tour. Newly recorded songs were announced in 2018. "Voyage", their first new album in 40 years, was released on 5 November 2021 to positive critical reviews and strong sales in numerous countries. ABBA Voyage, a concert residency featuring ABBA as virtual avatars, opened in May 2022 in London.

ABBA are among the best-selling music artists in history, with record sales estimated to be between 150 million to 385 million sold worldwide and the group were ranked 3rd best-selling singles artists in the United Kingdom with a total of 11.3 million singles sold by 3 November 2012. In May 2023 ABBA were awarded the BRIT Billion Award which celebrates those who have surpassed the milestone of one billion UK streams in their career. ABBA were the first group from a non-English-speaking country to achieve consistent success in the charts of English-speaking countries, including the United Kingdom, Australia, United States, Republic of Ireland, Canada, New Zealand and South Africa. They are the best-selling Swedish band of all time and the best-selling band originating in continental Europe. ABBA had eight consecutive number-one albums in the UK. The group also enjoyed significant success in Latin America and recorded a collection of their hit songs in Spanish. ABBA were inducted into the Vocal Group Hall of Fame in 2002. The group were inducted into the Rock and Roll Hall of Fame in 2010, the first recording artists to receive this honour from outside an Anglophonic country. In 2015, their song "Dancing Queen" was inducted into the Recording Academy's Grammy Hall of Fame.

Benny Andersson (born 16 December 1946 in Stockholm, Sweden) became (at age 18) a member of a popular Swedish pop-rock group, the Hep Stars, that performed, among other things, covers of international hits. The Hep Stars were known as "the Swedish Beatles". They also set up Hep House, their equivalent of Apple Corps. Andersson played the keyboard and eventually started writing original songs for his band, many of which became major hits, including "No Response", which hit number three in 1965, and "Sunny Girl", "Wedding", and "Consolation", all of which hit number one in 1966. Andersson also had a fruitful songwriting collaboration with Lasse Berghagen, with whom he wrote his first Svensktoppen entry, "Sagan om lilla Sofie" ("The tale of Little Sophie") in 1968.

Björn Ulvaeus (born 25 April 1945 in Gothenburg, Sweden) also began his musical career at the age of 18 (as a singer and guitarist), when he fronted the Hootenanny Singers, a popular Swedish folk–skiffle group. Ulvaeus started writing English-language songs for his group and even had a brief solo career alongside. The Hootenanny Singers and the Hep Stars sometimes crossed paths while touring. In June 1966, Ulvaeus and Andersson decided to write a song together. Their first attempt was "Isn't It Easy to Say", a song that was later recorded by the Hep Stars. Stig Anderson was the manager of the Hootenanny Singers and founder of the Polar Music label. He saw potential in the collaboration, and encouraged them to write more. The two also began playing occasionally with the other's bands on stage and on record, although it was not until 1969 that the pair wrote and produced some of their first real hits together: "Ljuva sextital" ("Sweet Sixties"), recorded by Brita Borg, and the Hep Stars' 1969 hit "Speleman" ("Fiddler").

Andersson wrote and submitted the song "Hej, Clown" for Melodifestivalen 1969, the national festival to select the Swedish entry to the Eurovision Song Contest. The song tied for first place, but re-voting relegated Andersson's song to second place. On that occasion Andersson briefly met his future spouse, singer Anni-Frid Lyngstad, who also participated in the contest. A month later, the two had become a couple. As their respective bands began to break up during 1969, Andersson and Ulvaeus teamed up and recorded their first album together in 1970, called "Lycka" ("Happiness"), which included original songs sung by both men. Their partners were often present in the recording studio, and sometimes added backing vocals; Fältskog even co-wrote a song with the two. Ulvaeus still occasionally recorded and performed with the Hootenanny Singers until the middle of 1974, and Andersson took part in producing their records.

Anni-Frid "Frida" Lyngstad (born 15 November 1945 in Bjørkåsen in Ballangen, Norway) sang from the age of 13 with various dance bands, and worked mainly in a jazz-oriented cabaret style. She also formed her own band, the Anni-Frid Four. In the middle of 1967, she won a national talent competition with "En ledig dag" ("A Day Off"), a Swedish version of the bossa nova song "A Day in Portofino", which is included in the EMI compilation "Frida 1967–1972". The first prize was a recording contract with EMI Sweden and to perform live on the most popular TV shows in the country. This TV performance, among many others, is included in the -hour documentary "Frida – The DVD". Lyngstad released several schlager style singles on EMI with mixed success. When Benny Andersson started to produce her recordings in 1971, she had her first number-one single, "Min egen stad" ("My Own Town"), written by Benny and featuring all the future ABBA members on backing vocals. Lyngstad toured and performed regularly in the folkpark circuit and made appearances on radio and TV. She had a second number-one single with "Man Vill Ju Leva Lite Dessemellan" in late 1972. She had met Ulvaeus briefly in 1963 during a talent contest, and Fältskog during a TV show in early 1968.

Lyngstad linked up with her future bandmates in 1969. On 1 March 1969, she participated in the Melodifestival, where she met Andersson for the first time. A few weeks later they met again during a concert tour in southern Sweden and they soon became a couple. Andersson produced her single "Peter Pan" in September 1969—her first collaboration with Benny & Björn, as they had written the song. Andersson would then produce Lyngstad's debut studio album, "Frida", which was released in March 1971. Lyngstad also played in several revues and cabaret shows in Stockholm between 1969 and 1973. After ABBA formed, she recorded another successful album in 1975, "Frida ensam", which included the original Swedish rendition of "Fernando", a hit on the Swedish radio charts before the English version was released by ABBA.

Agnetha Fältskog (born 5 April 1950 in Jönköping, Sweden) sang with a local dance band (headed by Bernt Enghardt) who sent a demo recording of their music to Karl-Gerhard Lundkvist. The demo tape featured a song written and sung by Agnetha: "Jag var så kär" ("I Was So in Love"). Lundkvist was so impressed with her voice that he was convinced she would be a star. After going through considerable effort to locate the singer, he arranged for Agnetha to come to Stockholm and to record two of her own songs. This led to Agnetha at the age of 18 having a number-one record in Sweden with a self-composed song, which later went on to sell over 80,000 copies. She was soon noticed by the critics and songwriters as a talented singer/songwriter of schlager style songs. Fältskog's main inspiration in her early years was singers such as Connie Francis. Along with her own compositions, she recorded covers of foreign hits and performed them on tours in Swedish folkparks. Most of her biggest hits were self-composed, which was quite unusual for a female singer in the 1960s. Agnetha released four solo LPs between 1968 and 1971. She had many successful singles in the Swedish charts.

During filming of a Swedish TV special in May 1969, Fältskog met Ulvaeus and they married on 6 July 1971. Fältskog and Ulvaeus eventually were involved in each other's recording sessions, and soon even Andersson and Lyngstad added backing vocals to Fältskog's third studio album, "Som jag är" ("As I Am") (1970). In 1972, Fältskog starred as Mary Magdalene in the original Swedish production of "Jesus Christ Superstar" and attracted favourable reviews. Between 1967 and 1975, Fältskog released five studio albums.

An attempt at combining their talents occurred in April 1970 when the two couples went on holiday together to the island of Cyprus. What started as singing for fun on the beach ended up as an improvised live performance in front of the United Nations soldiers stationed on the island. Andersson and Ulvaeus were at this time recording their first album together, "Lycka", which was to be released in September 1970. Fältskog and Lyngstad added backing vocals on several tracks during June, and the idea of their working together saw them launch a stage act, "Festfolket" (which translates from Swedish to "Party People" and in pronunciation also "engaged couples"), on 1 November 1970 in Gothenburg.

The cabaret show attracted generally negative reviews, except for the performance of the Andersson and Ulvaeus hit "Hej, gamle man" ("Hello, Old Man")–the first Björn and Benny recording to feature all four. They also performed solo numbers from respective albums, but the lukewarm reception convinced the foursome to shelve plans for working together for the time being, and each soon concentrated on individual projects again.

"Hej, gamle man", a song about an old Salvation Army soldier, became the quartet's first hit. The record was credited to Björn & Benny and reached number five on the sales charts and number one on Svensktoppen, staying on the latter chart (which was not a chart linked to sales or airplay) for 15 weeks.

It was during 1971 that the four artists began working together more, adding vocals to the others' recordings. Fältskog, Andersson and Ulvaeus toured together in May, while Lyngstad toured on her own. Frequent recording sessions brought the foursome closer together during the summer.

After the 1970 release of "Lycka", two more singles credited to "Björn & Benny" were released in Sweden, "Det kan ingen doktor hjälpa" ("No Doctor Can Help with That") and "Tänk om jorden vore ung" ("Imagine If Earth Was Young"), with more prominent vocals by Fältskog and Lyngstad–and moderate chart success.

Fältskog and Ulvaeus, now married, started performing together with Andersson on a regular basis at the Swedish folkparks in the middle of 1971.

Stig Anderson, founder and owner of Polar Music, was determined to break into the mainstream international market with music by Andersson and Ulvaeus. "One day the pair of you will write a song that becomes a worldwide hit," he predicted. Stig Anderson encouraged Ulvaeus and Andersson to write a song for Melodifestivalen, and after two rejected entries in 1971, Andersson and Ulvaeus submitted their new song "Säg det med en sång" ("Say It with a Song") for the 1972 contest, choosing newcomer Lena Anderson to perform. The song came in third place, encouraging Stig Anderson, and became a hit in Sweden.

The first signs of foreign success came as a surprise, as the Andersson and Ulvaeus single "She's My Kind of Girl" was released through Epic Records in Japan in March 1972, giving the duo a Top 10 hit. Two more singles were released in Japan, "En Carousel" ("En Karusell" in Scandinavia, an earlier version of "Merry-Go-Round") and "Love Has Its Ways" (a song they wrote with Kōichi Morita).

Ulvaeus and Andersson persevered with their songwriting and experimented with new sounds and vocal arrangements. "People Need Love" was released in June 1972, featuring guest vocals by the women, who were now given much greater prominence. Stig Anderson released it as a single, credited to "Björn & Benny, Agnetha & Anni-Frid". The song peaked at number 17 in the Swedish combined single and album charts, enough to convince them they were on to something.

"People Need Love" also became the first record to chart for the quartet in the United States, where it peaked at number 114 on the "Cashbox" singles chart and number 117 on the "Record World" singles chart. Labelled as "Björn & Benny (with Svenska Flicka)" meaning Swedish Girl, it was released there through Playboy Records. According to Stig Anderson, "People Need Love" could have been a much bigger American hit, but a small label like Playboy Records did not have the distribution resources to meet the demand for the single from retailers and radio programmers.

In 1973, the band and their manager Stig Anderson decided to have another try at Melodifestivalen, this time with the song "Ring Ring". The studio sessions were handled by Michael B. Tretow, who experimented with a "wall of sound" production technique that became a distinctive new sound thereafter associated with ABBA. Stig Anderson arranged an English translation of the lyrics by Neil Sedaka and Phil Cody and they thought this would be a success. However, on 10 February 1973, the song came third in Melodifestivalen; thus it never reached the Eurovision Song Contest itself. Nevertheless, the group released their debut studio album, also called "Ring Ring". The album did well and the "Ring Ring" single was a hit in many parts of Europe and also in South Africa. However, Stig Anderson felt that the true breakthrough could only come with a UK or US hit.

When Agnetha Fältskog gave birth to her daughter Linda in 1973, she was replaced for a short period by Inger Brundin on a trip to West Germany.

In 1973, Stig Anderson, tired of unwieldy names, started to refer to the group privately and publicly as ABBA (a palindrome). At first, this was a play on words, as Abba is also the name of a well-known fish-canning company in Sweden, and itself an abbreviation. However, since the fish-canners were unknown outside Sweden, Anderson came to believe the name would work in international markets. A competition to find a suitable name for the group was held in a Gothenburg newspaper and it was officially announced in the summer that the group were to be known as "ABBA". The group negotiated with the canners for the rights to the name.

Fred Bronson reported for "Billboard" that Fältskog told him in a 1988 interview that "[ABBA] had to ask permission and the factory said, 'O.K., as long as you don't make us feel ashamed for what you're doing. "ABBA" is an acronym formed from the first letters of each group member's first name: Agnetha, Björn, Benny, Anni-Frid, although there has never been any official confirmation of who each letter in the sequence refers to. The earliest known example of "ABBA" written on paper is on a recording session sheet from the Metronome Studio in Stockholm dated 16 October 1973. This was first written as "Björn, Benny, Agnetha & Frida", but was subsequently crossed out with "ABBA" written in large letters on top.

Their official logo, with its distinctive backward "B", was designed by Rune Söderqvist, who designed most of ABBA's record sleeves. The ambigram first appeared on the French compilation album, "Golden Double Album", released in May 1976 by Disques Vogue, and would henceforth be used for all official releases.

The idea for the official logo was made by the German photographer on a velvet jumpsuit photo shoot for the teenage magazine "Bravo". In the photo, the ABBA members held giant initial letters of their names. After the pictures were made, Heilemann found out that Benny Andersson reversed his letter "B;" this prompted discussions about the mirrored "B", and the members of ABBA agreed on the mirrored letter. From 1976 onward, the first "B" in the logo version of the name was "mirror-image" reversed on the band's promotional material, thus becoming the group's registered trademark.

Following their acquisition of the group's catalogue, PolyGram began using variations of the ABBA logo, employing a different font. In 1992, Polygram added a crown emblem to it for the first release of the "ABBA Gold: Greatest Hits" compilation. After Universal Music purchased PolyGram (and, thus, ABBA's label Polar Music International), control of the group's catalogue returned to Stockholm. Since then, the original logo has been reinstated on all official products.

As the group entered the Melodifestivalen with "Ring Ring" but failed to qualify as the 1973 Swedish entry, Stig Anderson immediately started planning for the 1974 contest. Ulvaeus, Andersson and Stig Anderson believed in the possibilities of using the Eurovision Song Contest as a way to make the music business aware of them as songwriters, as well as the band itself. In late 1973, they were invited by Swedish television to contribute a song for the Melodifestivalen 1974 and from a number of new songs, the upbeat song "Waterloo" was chosen; the group were now inspired by the growing glam rock scene in England.

ABBA won their nation's hearts on Swedish television on 9 February 1974, and with this third attempt were far more experienced and better prepared for the Eurovision Song Contest. Winning the 1974 Eurovision Song Contest on 6 April 1974 (and singing "Waterloo" in English instead of their native tongue) gave ABBA the chance to tour Europe and perform on major television shows; thus the band saw the "Waterloo" single chart in many European countries. Following their success at the Eurovision Song Contest, ABBA spent an evening of glory partying in the appropriately named first-floor Napoleon suite of The Grand Brighton Hotel.

"Waterloo" was ABBA's first major hit in numerous countries, becoming their first number-one single in nine western and northern European countries, including the big markets of the UK and West Germany, and in South Africa. It also made the top ten in several other countries, including rising to number three in Spain, number four in Australia and France, and number seven in Canada. In the United States, the song peaked at number six on the "Billboard" Hot 100 chart, paving the way for their first album and their first trip as a group there. Albeit a short promotional visit, it included their first performance on American television, "The Mike Douglas Show". The album "Waterloo" only peaked at number 145 on the "Billboard" 200 chart, but received unanimous high praise from the US critics: "Los Angeles Times" called it "a compelling and fascinating debut album that captures the spirit of mainstream pop quite effectively ... an immensely enjoyable and pleasant project", while "Creem" characterised it as "a perfect blend of exceptional, lovable compositions".

ABBA's follow-up single, "Honey, Honey", peaked at number 27 on the US "Billboard" Hot 100, reached the top twenty in several other countries, and was a number-two hit in West Germany although it only reached the top 30 in Australia and the US. In the United Kingdom, ABBA's British record label, Epic, decided to re-release a remixed version of "Ring Ring" instead of "Honey, Honey", and a cover version of the latter by Sweet Dreams peaked at number 10. Both records debuted on the UK chart within one week of each other. "Ring Ring" failed to reach the Top 30 in the UK, increasing growing speculation that the group were simply a Eurovision one-hit wonder.

In November 1974, ABBA embarked on their first European tour, playing dates in Denmark, West Germany and Austria. It was not as successful as the band had hoped, since most of the venues did not sell out. Due to a lack of demand, they were even forced to cancel a few shows, including a sole concert scheduled in Switzerland. The second leg of the tour, which took them through Scandinavia in January 1975, was very different. They played to full houses everywhere and finally got the reception they had aimed for. Live performances continued in the middle of 1975 when ABBA embarked on a fourteen open-air date tour of Sweden and Finland. Their Stockholm show at the Gröna Lund amusement park had an estimated audience of 19,200. Björn Ulvaeus later said, "If you look at the singles we released straight after Waterloo, we were trying to be more like The Sweet, a semi-glam rock group, which was stupid because we were always a pop group."

In late 1974, "So Long" was released as a single in the United Kingdom but it received no airplay from Radio 1 and failed to chart in the UK; the only countries in which it was successful were Austria, Sweden and Germany, reaching the top ten in the first two and number 21 in the latter. In the middle of 1975, ABBA released "I Do, I Do, I Do, I Do, I Do", which again received little airplay on Radio 1, but did manage to climb to number 38 on the UK chart, while making top five in several northern and western European countries, and number one in South Africa. Later that year, the release of their self-titled third studio album "ABBA" and single "SOS" brought back their chart presence in the UK, where the single hit number six and the album peaked at number 13. "SOS" also became ABBA's second number-one single in Germany, their third in Australia and their first in France, plus reached number two in several other European countries, including Italy.

Success was further solidified with "Mamma Mia" reaching number-one in the United Kingdom, Germany and Australia and the top two in a few other western and northern European countries. In the United States, both "I Do, I Do, I Do, I Do, I Do" and "SOS" peaked at number 15 on the "Billboard" Hot 100 chart, with the latter picking up the BMI Award along the way as one of the most played songs on American radio in 1975. "Mamma Mia", however, stalled at number 32. In Canada, the three songs rose to number 12, nine and 18, respectively.

The success of the group in the United States had until that time been limited to single releases. By early 1976, the group already had four Top 30 singles on the US charts, but the album market proved to be tough to crack. The eponymous "ABBA " album generated three American hits, but it only peaked at number 165 on the "Cashbox" album chart and number 174 on the "Billboard" 200 chart. Opinions were voiced, by "Creem" in particular, that in the US ABBA had endured "a very sloppy promotional campaign". Nevertheless, the group enjoyed warm reviews from the American press. "Cashbox" went as far as saying that "there is a recurrent thread of taste and artistry inherent in Abba's marketing, creativity and presentation that makes it almost embarrassing to critique their efforts", while "Creem" wrote: "SOS is surrounded on this LP by so many good tunes that the mind boggles."

In Australia, the airing of the music videos for "I Do, I Do, I Do, I Do, I Do" and "Mamma Mia" on the nationally broadcast TV pop show "Countdown" (which premiered in November 1974) saw the band rapidly gain enormous popularity, and "Countdown" become a key promoter of the group via their distinctive music videos. This started an immense interest for ABBA in Australia, resulting in "I Do, I Do, I Do, I Do, I Do" staying at number one for three weeks, then "SOS" spending a week there, followed by "Mamma Mia" staying there for ten weeks, and the album holding down the number one position for months. The three songs were also successful in nearby New Zealand with the first two topping that chart and the third reaching number two.

In March 1976, the band released the compilation album "Greatest Hits". It became their first UK number-one album, and also took ABBA into the Top 50 on the US album charts for the first time, eventually selling more than a million copies there. Also included on "Greatest Hits" was a new single, "Fernando", which went to number-one in at least thirteen countries all over the world, including the UK, Germany, France, Australia, South Africa and Mexico, and the top five in most other significant markets, including, at number four, becoming their biggest hit to date in Canada; the single went on to sell over 10 million copies worldwide.

In Australia, "Fernando" occupied the top position for a then record breaking 14 weeks (and stayed in the chart for 40 weeks), and was the longest-running chart-topper there for over 40 years until it was overtaken by Ed Sheeran's "Shape of You" in May 2017. It still remains as one of the best-selling singles of all time in Australia. Also in 1976, the group received its first international prize, with "Fernando" being chosen as the "Best Studio Recording of 1975". In the United States, "Fernando" reached the Top 10 of the Cashbox Top 100 singles chart and number 13 on the "Billboard" Hot 100. It topped the "Billboard" Adult Contemporary chart, ABBA's first American number-one single on any chart. At the same time, a compilation named "The Very Best of ABBA" was released in Germany, becoming a number-one album there whereas the "Greatest Hits" compilation which followed a few months later ascended to number two in Germany, despite all similarities with "The Very Best" album.

The group's fourth studio album, "Arrival", a number-one best-seller in parts of Europe, the UK and Australia, and a number-three hit in Canada and Japan, represented a new level of accomplishment in both songwriting and studio work, prompting rave reviews from more rock-oriented UK music weeklies such as "Melody Maker" and "New Musical Express", and mostly appreciative notices from US critics.

Hit after hit flowed from "Arrival": "Money, Money, Money", another number-one in Germany, France, Australia and other countries of western and northern Europe, plus number three in the UK; and, "Knowing Me, Knowing You", ABBA's sixth consecutive German number-one, as well as another UK number-one, plus a top five hit in many other countries, although it was only a number nine hit in Australia and France. The real sensation was the first single, "Dancing Queen", not only topping the charts in loyal markets like the UK, Germany, Sweden, several other western and northern European countries, and Australia, but also reaching number-one in the United States, Canada, the Soviet Union and Japan, and the top ten in France, Spain and Italy. All three songs were number-one hits in Mexico. In South Africa, ABBA had astounding success with each of "Fernando", "Dancing Queen" and "Knowing Me, Knowing You" being among the top 20 best-selling singles for 1976–77. In 1977, "Arrival" was nominated for the inaugural BRIT Award in the category "Best International Album of the Year". By this time ABBA were popular in the UK, most of Europe, Australia, New Zealand and Canada. In "Frida – The DVD", Lyngstad explains how she and Fältskog developed as singers, as ABBA's recordings grew more complex over the years.

The band's mainstream popularity in the United States would remain on a comparatively smaller scale, and "Dancing Queen" became the only "Billboard" Hot 100 number-one single for ABBA (though it immediately became, and remains to this day, a major gay anthem) with "Knowing Me, Knowing You" later peaking at number seven; "Money, Money, Money", however, had barely charted there or in Canada (where "Knowing Me, Knowing You" had reached number five). They did, however, get three more singles to the number-one position on other "Billboard" US charts, including "Billboard" Adult Contemporary and Hot Dance Club Play). Nevertheless, "Arrival" finally became a true breakthrough release for ABBA on the US album market where it peaked at number 20 on the "Billboard" 200 chart and was certified gold by RIAA.

In January 1977, ABBA embarked on their first major tour. The group's status had changed dramatically and they were clearly regarded as superstars. They opened their much anticipated tour in Oslo, Norway, on 28 January, and mounted a lavishly produced spectacle that included a few scenes from their self-written mini-operetta "The Girl with the Golden Hair". The concert attracted huge media attention from across Europe and Australia. They continued the tour through Western Europe, visiting Gothenburg, Copenhagen, Berlin, Cologne, Amsterdam, Antwerp, Essen, Hanover, and Hamburg and ending with shows in the United Kingdom in Manchester, Birmingham, Glasgow and two sold-out concerts at London's Royal Albert Hall. Tickets for these two shows were available only by mail application and it was later revealed that the box-office received 3.5 million requests for tickets, enough to fill the venue 580 times.

Along with praise ("ABBA turn out to be amazingly successful at reproducing their records", wrote "Creem"), there were complaints that "ABBA performed slickly...but with a zero personality coming across from a total of 16 people on stage" ("Melody Maker"). One of the Royal Albert Hall concerts was filmed as a reference for the filming of the Australian tour for what became "", though it is not exactly known how much of the concert was filmed.
After the European leg of the tour, in March 1977, ABBA played 11 dates in Australia before a total of 160,000 people. The opening concert in Sydney at the Sydney Showground on 3 March to an audience of 20,000 was marred by torrential rain with Lyngstad slipping on the wet stage during the concert. However, all four members would later recall this concert as the most memorable of their career.

Upon their arrival in Melbourne, a civic reception was held at the Melbourne Town Hall and ABBA appeared on the balcony to greet an enthusiastic crowd of 6,000. In Melbourne, the group gave three concerts at the Sidney Myer Music Bowl with 14,500 at each including the Australian Prime Minister Malcolm Fraser and his family. At the first Melbourne concert, an additional 16,000 people gathered outside the fenced-off area to listen to the concert. In Adelaide, the group performed one concert at Football Park in front of 20,000 people, with another 10,000 listening outside. During the first of five concerts in Perth, there was a bomb scare with everyone having to evacuate the Entertainment Centre. The trip was accompanied by mass hysteria and unprecedented media attention ("Swedish ABBA stirs box-office in Down Under tour...and the media coverage of the quartet rivals that set to cover the upcoming Royal tour of Australia", wrote "Variety"), and is captured on film in "", directed by Lasse Hallström.

The Australian tour and its subsequent "ABBA: The Movie" produced some ABBA lore, as well. Fältskog's blonde good looks had long made her the band's "pin-up girl", a role she disdained. During the Australian tour, she performed in a skin-tight white jumpsuit, causing one Australian newspaper to use the headline "Agnetha's bottom tops dull show". When asked about this at a news conference, she replied: "Don't they have bottoms in Australia?"

In December 1977, ABBA followed up "Arrival" with the more ambitious fifth album, "", released to coincide with the debut of "ABBA: The Movie". Although the album was less well received by UK reviewers, it did spawn more worldwide hits: "The Name of the Game" and "Take a Chance on Me", which both topped the UK charts and racked up impressive sales in most countries, although "The Name of the Game" was generally the more successful in the Nordic countries and Down Under, while "Take a Chance on Me" was more successful in North America and the German-speaking countries.

"The Name of the Game" was a number two hit in the Netherlands, Belgium and Sweden while also making the Top 5 in Finland, Norway, New Zealand and Australia, while only peaking at numbers 10, 12 and 15 in Mexico, the US and Canada. "Take a Chance on Me" was a number one hit in Austria, Belgium and Mexico, made the Top 3 in the US, Canada, the Netherlands, Germany and Switzerland, while only reaching numbers 12 and 14 in Australia and New Zealand, respectively. Both songs were Top 10 hits in countries as far afield as Rhodesia and South Africa, as well as in France. Although "Take a Chance on Me" did not top the American charts, it proved to be ABBA's biggest hit single there, selling more copies than "Dancing Queen". The drop in sales in Australia was felt to be inevitable by industry observers as an "Abba-Fever" that had existed there for almost three years could only last so long as adolescents would naturally begin to move away from a group so deified by both their parents and grandparents.

A third single, "Eagle", was released in continental Europe and Down Under becoming a number one hit in Belgium and a Top 10 hit in the Netherlands, Germany, Switzerland and South Africa, but barely charting Down Under. The B-side of "Eagle" was "Thank You for the Music", and it was belatedly released as an A-side single in both the United Kingdom and Ireland in 1983. "Thank You for the Music" has become one of the best loved and best known ABBA songs without being released as a single during the group's lifetime. "ABBA: The Album" topped the album charts in the UK, the Netherlands, New Zealand, Sweden, Norway, Switzerland, while ascending to the Top 5 in Australia, Germany, Austria, Finland and Rhodesia, and making the Top 10 in Canada and Japan. Sources also indicate that sales in Poland exceeded 1 million copies and that sales demand in Russia could not be met by the supply available. The album peaked at number 14 in the US.

By 1978, ABBA were one of the biggest bands in the world. They converted a vacant cinema into the Polar Music Studio, a state-of-the-art studio in Stockholm. The studio was used by several other bands; notably Genesis' "Duke", Led Zeppelin's "In Through the Out Door" and Scorpions's "Lovedrive" were recorded there. During May 1978, the group went to the United States for a promotional campaign, performing alongside Andy Gibb on Olivia Newton-John's TV show. Recording sessions for the single "Summer Night City" were an uphill struggle, but upon release the song became another hit for the group. The track would set the stage for ABBA's foray into disco with their next album.

On 9 January 1979, the group performed "Chiquitita" at the Music for UNICEF Concert held at the United Nations General Assembly to celebrate UNICEF's Year of the Child. ABBA donated the copyright of this worldwide hit to the UNICEF; see Music for UNICEF Concert. The single was released the following week, and reached number-one in ten countries.

In mid-January 1979, Ulvaeus and Fältskog announced they were getting divorced. The news caused interest from the media and led to speculation about the band's future. ABBA assured the press and their fan base they were continuing their work as a group and that the divorce would not affect them. Nonetheless, the media continued to confront them with this in interviews. To escape the media swirl and concentrate on their writing, Andersson and Ulvaeus secretly travelled to Compass Point Studios in Nassau, Bahamas, where for two weeks they prepared their next album's songs.

The group's sixth studio album, "Voulez-Vous", was released in April 1979, with its title track recorded at the famous Criteria Studios in Miami, Florida, with the assistance of recording engineer Tom Dowd among others. The album topped the charts across Europe and in Japan and Mexico, hit the Top 10 in Canada and Australia and the Top 20 in the US. While none of the singles from the album reached number one on the UK chart, the lead single, "Chiquitita", and the fourth single, "I Have a Dream", both ascended to number two, and the other two, "Does Your Mother Know" and "Angeleyes" (with "Voulez-Vous", released as a double A-side) both made the top 5. All four singles reached number one in Belgium, although the last three did not chart in Sweden or Norway. "Chiquitita", which was featured in the "Music for UNICEF Concert" after which ABBA decided to donate half of the royalties from the song to UNICEF, topped the singles charts in the Netherlands, Switzerland, Finland, Spain, Mexico, South Africa, Rhodesia and New Zealand, rose to number two in Sweden, and made the Top 5 in Germany, Austria, Norway and Australia, although it only reached number 29 in the US.

"I Have a Dream" was a sizeable hit reaching number one in the Netherlands, Switzerland, and Austria, number three in South Africa, and number four in Germany, although it only reached number 64 in Australia. In Canada, "I Have a Dream" became ABBA's second number one on the RPM Adult Contemporary chart (after "Fernando" hit the top previously) although it did not chart in the US. "Does Your Mother Know", a rare song in which Ulvaeus sings lead vocals, was a Top 5 hit in the Netherlands and Finland, and a Top 10 hit in Germany, Switzerland, Australia, although it only reached number 27 in New Zealand. It did better in North America than "Chiquitita", reaching number 12 in Canada and number 19 in the US, and made the Top 20 in Japan. "Voulez-Vous" was a Top 10 hit in the Netherlands and Switzerland, a Top 20 hit in Germany and Finland, but only peaked in the 80s in Australia, Canada and the US.

Also in 1979, the group released their second compilation album, "Greatest Hits Vol. 2", which featured a brand-new track: "Gimme! Gimme! Gimme! (A Man After Midnight)", which was a Top 3 hit in the UK, Belgium, the Netherlands, Germany, Austria, Switzerland, Finland and Norway, and returned ABBA to the Top 10 in Australia. "Greatest Hits Vol. 2" went to number one in the UK, Belgium, Canada and Japan while making the Top 5 in several other countries, but only reaching number 20 in Australia and number 46 in the US. In the Soviet Union during the late 1970s, the group were paid in oil commodities because of an embargo on the rouble.

On 13 September 1979, ABBA began at Northlands Coliseum in Edmonton, Canada, with a full house of 14,000. "The voices of the band, Agnetha's high sauciness combined with round, rich lower tones of Anni-Frid, were excellent...Technically perfect, melodically correct and always in perfect pitch...The soft lower voice of Anni-Frid and the high, edgy vocals of Agnetha were stunning", raved "Edmonton Journal".

During the next four weeks they played a total of 17 sold-out dates, 13 in the United States and four in Canada. The last scheduled ABBA concert in the United States in Washington, D.C. was cancelled due to emotional distress Fältskog experienced during the flight from New York to Boston. The group's private plane was subjected to extreme weather conditions and was unable to land for an extended period. They appeared at the Boston Music Hall for the performance 90 minutes late. The tour ended with a show in Toronto, Canada at Maple Leaf Gardens before a capacity crowd of 18,000. "ABBA plays with surprising power and volume; but although they are loud, they're also clear, which does justice to the signature vocal sound... Anyone who's been waiting five years to see Abba will be well satisfied", wrote "Record World". On 19 October 1979, the tour resumed in Western Europe where the band played 23 sold-out gigs, including six sold-out nights at London's Wembley Arena.

In March 1980, ABBA travelled to Japan where upon their arrival at Narita International Airport, they were besieged by thousands of fans. The group performed eleven concerts to full houses, including six shows at Tokyo's Budokan. This tour was the last "on the road" adventure of their career.
In July 1980, ABBA released the single "The Winner Takes It All", the group's eighth UK chart topper (and their first since 1978). The song is widely misunderstood as being written about Ulvaeus and Fältskog's marital tribulations; Ulvaeus wrote the lyrics, but has stated they were not about his own divorce; Fältskog has repeatedly stated she was not the loser in their divorce. In the United States, the single peaked at number-eight on the "Billboard" Hot 100 chart and became ABBA's second "Billboard" Adult Contemporary number-one. It was also re-recorded by Andersson and Ulvaeus with a slightly different backing track, by French chanteuse Mireille Mathieu at the end of 1980 – as "Bravo tu as gagné", with French lyrics by Alain Boublil.

In November 1980, ABBA's seventh album "Super Trouper" was released, which reflected a certain change in ABBA's style with more prominent use of synthesizers and increasingly personal lyrics. It set a record for the most pre-orders ever received for a UK album after one million copies were ordered before release. The second single from the album, "Super Trouper", also hit number-one in the UK, becoming the group's ninth and final UK chart-topper. Another track from the album, "Lay All Your Love on Me", released in 1981 as a Twelve-inch single only in selected territories, managed to top the "Billboard" Hot Dance Club Play chart and peaked at number-seven on the UK singles chart becoming, at the time, the highest ever charting 12-inch release in UK chart history.

Also in 1980, ABBA recorded a compilation of Spanish-language versions of their hits called "Gracias Por La Música". This was released in Spanish-speaking countries as well as in Japan and Australia. The album became a major success, and along with the Spanish version of "Chiquitita", this signalled the group's breakthrough in Latin America. "ABBA Oro: Grandes Éxitos", the Spanish equivalent of "ABBA Gold: Greatest Hits", was released in 1999.

In January 1981, Ulvaeus married Lena Källersjö, and manager Stig Anderson celebrated his 50th birthday with a party. For this occasion, ABBA recorded the track "Hovas Vittne" (a pun on the Swedish name for Jehovah's Witness and Anderson's birthplace, Hova) as a tribute to him, and released it only on 200 red vinyl copies, to be distributed to the guests attending the party. This single has become a sought-after collectable. In mid-February 1981, Andersson and Lyngstad announced they were filing for divorce. Information surfaced that their marriage had been an uphill struggle for years, and Benny had already met another woman, Mona Nörklit, whom he married in November 1981.

Andersson and Ulvaeus had songwriting sessions in early 1981, and recording sessions began in mid-March. At the end of April, the group recorded a TV special, "Dick Cavett Meets ABBA" with the US talk show host Dick Cavett. "The Visitors", ABBA's eighth studio album, showed a songwriting maturity and depth of feeling distinctly lacking from their earlier recordings but still placing the band squarely in the pop genre, with catchy tunes and harmonies. Although not revealed at the time of its release, the album's title track, according to Ulvaeus, refers to the secret meetings held against the approval of totalitarian governments in Soviet-dominated states, while other tracks address topics like failed relationships, the threat of war, ageing, and loss of innocence. The album's only major single release, "One of Us", proved to be the last of ABBA's nine number-one singles in Germany, this being in December 1981; and the swansong of their sixteen Top 5 singles on the South African chart. "One of Us" was also ABBA's final Top 3 hit in the UK, reaching number-three on the UK Singles Chart.

Although it topped the album charts across most of Europe, including Ireland, the UK and Germany, "The Visitors" was not as commercially successful as its predecessors, showing a commercial decline in previously loyal markets such as France, Australia and Japan. A track from the album, "When All Is Said and Done", was released as a single in North America, Australia and New Zealand, and fittingly became ABBA's final Top 40 hit in the US (debuting on the US charts on 31 December 1981), while also reaching the US Adult Contemporary Top 10, and number-four on the RPM Adult Contemporary chart in Canada. The song's lyrics, as with "The Winner Takes It All" and "One of Us", dealt with the painful experience of separating from a long-term partner, though it looked at the trauma more optimistically. With the now publicised story of Andersson and Lyngstad's divorce, speculation increased of tension within the band. Also released in the United States was the title track of "The Visitors", which hit the Top Ten on the "Billboard" Hot Dance Club Play chart.

In the spring of 1982, songwriting sessions had started and the group came together for more recordings. Plans were not completely clear, but a new album was discussed and the prospect of a small tour suggested. The recording sessions in May and June 1982 were a struggle, and only three songs were eventually recorded: "You Owe Me One", "I Am the City" and "Just Like That". Andersson and Ulvaeus were not satisfied with the outcome, so the tapes were shelved and the group took a break for the summer.

Back in the studio again in early August, the group had changed plans for the rest of the year: they settled for a Christmas release of a double album compilation of all their past single releases to be named "". New songwriting and recording sessions took place, and during October and December, they released the singles "The Day Before You Came"/"Cassandra" and "Under Attack"/"You Owe Me One", the A-sides of which were included on the compilation album. Neither single made the Top 20 in the United Kingdom, though "The Day Before You Came" became a Top 5 hit in many European countries such as Germany, the Netherlands and Belgium. The album went to number one in the UK and Belgium, Top 5 in the Netherlands and Germany and Top 20 in many other countries. "Under Attack", the group's final release before disbanding, was a Top 5 hit in the Netherlands and Belgium.

"I Am the City" and "Just Like That" were left unreleased on "The Singles: The First Ten Years" for possible inclusion on the next projected studio album, though this never came to fruition. "I Am the City" was eventually released on the compilation album "" in 1993, while "Just Like That" has been recycled in new songs with other artists produced by Andersson and Ulvaeus. A reworked version of the verses ended up in the musical "Chess". The chorus section of "Just Like That" was eventually released on a retrospective box set in 1994, as well as in the "ABBA Undeleted" medley featured on disc 9 of "The Complete Studio Recordings". Despite a number of requests from fans, Ulvaeus and Andersson are still refusing to release ABBA's version of "Just Like That" in its entirety, even though the complete version has surfaced on bootlegs.

The group travelled to London to promote "The Singles: The First Ten Years" in the first week of November 1982, appearing on "Saturday Superstore" and "The Late, Late Breakfast Show", and also to West Germany in the second week, to perform on Show Express. On 19 November 1982, ABBA appeared for the last time in Sweden on the TV programme Nöjesmaskinen, and on 11 December 1982, they made their last performance ever, transmitted to the UK on Noel Edmonds' "The Late, Late Breakfast Show", through a live link from a TV studio in Stockholm.

Andersson and Ulvaeus began collaborating with Tim Rice in early 1983 on writing songs for the musical project "Chess", while Fältskog and Lyngstad both concentrated on international solo careers. While Andersson and Ulvaeus were working on the musical, a further co-operation among the three of them came with the musical "Abbacadabra" that was produced in France for television. It was a children's musical using 14 ABBA songs. Alain and Daniel Boublil, who wrote "Les Misérables", had been in touch with Stig Anderson about the project, and the TV musical was aired over Christmas on French TV and later a Dutch version was also broadcast. Boublil previously also wrote the French lyric for Mireille Mathieu's version of "The Winner Takes It All".

Lyngstad, who had recently moved to Paris, participated in the French version, and recorded a single, "Belle", a duet with French singer Daniel Balavoine. The song was a cover of ABBA's 1976 instrumental track "Arrival". As the single "Belle" sold well in France, Cameron Mackintosh wanted to stage an English-language version of the show in London, with the French lyrics translated by David Wood and Don Black; Andersson and Ulvaeus got involved in the project, and contributed with one new song, "I Am the Seeker". "Abbacadabra" premiered on 8 December 1983 at the Lyric Hammersmith Theatre in London, to mixed reviews and full houses for eight weeks, closing on 21 January 1984. Lyngstad was also involved in this production, recording "Belle" in English as "Time", a duet with actor and singer B. A. Robertson: the single sold well and was produced and recorded by Mike Batt. In May 1984, Lyngstad performed "I Have a Dream" with a children's choir at the United Nations Organisation Gala, in Geneva, Switzerland.

All four members made their (at the time, final) public appearance as four friends more than as ABBA in January 1986, when they recorded a video of themselves performing an acoustic version of "Tivedshambo" (which was the first song written by their manager Stig Anderson), for a Swedish TV show honouring Anderson on his 55th birthday. The four had not seen each other for more than two years. That same year they also performed privately at another friend's 40th birthday: their old tour manager, Claes af Geijerstam. They sang a self-written song titled "Der Kleine Franz" that was later to resurface in "Chess". Also in 1986, "ABBA Live" was released, featuring selections of live performances from the group's 1977 and 1979 tours. The four members were guests at the 50th birthday of Görel Hanser in 1999. Hanser was a long-time friend of all four, and also former secretary of Stig Anderson. Honouring Görel, ABBA performed a Swedish birthday song "Med en enkel tulipan" a cappella.

Andersson has on several occasions performed ABBA songs. In June 1992, he and Ulvaeus appeared with U2 at a Stockholm concert, singing the chorus of "Dancing Queen", and a few years later during the final performance of the B & B in Concert in Stockholm, Andersson joined the cast for an encore at the piano. Andersson frequently adds an ABBA song to the playlist when he performs with his BAO band. He also played the piano during new recordings of the ABBA songs "Like an Angel Passing Through My Room" with opera singer Anne Sofie von Otter, and "When All Is Said and Done" with Swede Viktoria Tolstoy. In 2002, Andersson and Ulvaeus both performed an a cappella rendition of the first verse of "Fernando" as they accepted their Ivor Novello award in London. Lyngstad performed and recorded an a cappella version of "Dancing Queen" with the Swedish group the Real Group in 1993, and also re-recorded "I Have a Dream" with Swiss singer Dan Daniell in 2003.

ABBA never officially announced the end of the group or an indefinite break, but it was long considered dissolved after their final public performance together in 1982. Their final public performance together as ABBA before their 2016 reunion was on the British TV programme "The Late, Late Breakfast Show" (live from Stockholm) on 11 December 1982. While reminiscing on "The Day Before You Came", Ulvaeus said: "we might have continued for a while longer if that had been a number one".

In January 1983, Fältskog started recording sessions for a solo album, as Lyngstad had successfully released her album "Something's Going On" some months earlier. Ulvaeus and Andersson, meanwhile, started songwriting sessions for the musical "Chess". In interviews at the time, Björn and Benny denied the split of ABBA ("Who are we without our ladies? Initials of Brigitte Bardot?"), and Lyngstad and Fältskog kept claiming in interviews that ABBA would come together for a new album repeatedly during 1983 and 1984. Internal strife between the group and their manager escalated and the band members sold their shares in Polar Music during 1983. Except for a TV appearance in 1986, the foursome did not come together publicly again until they were reunited at the Swedish premiere of the "Mamma Mia!" movie on 4 July 2008. The individual members' endeavours shortly before and after their final public performance coupled with the collapse of both marriages and the lack of significant activity in the following few years after that widely suggested that the group had broken up.

In an interview with the "Sunday Telegraph" following the premiere, Ulvaeus and Andersson said that there was nothing that could entice them back on stage again. Ulvaeus said: "We will never appear on stage again. [...] There is simply no motivation to re-group. Money is not a factor and we would like people to remember us as we were. Young, exuberant, full of energy and ambition. I remember Robert Plant saying Led Zeppelin were a cover band now because they cover all their own stuff. I think that hit the nail on the head."
However, on 3 January 2011, Fältskog, long considered to be the most reclusive member of the group and a major obstacle to any reunion, raised the possibility of reuniting for a one-off engagement. She admitted that she has not yet brought the idea up to the other three members. In April 2013, she reiterated her hopes for reunion during an interview with "Die Zeit", stating: "If they ask me, I'll say yes."

In a May 2013 interview, Fältskog, aged 63 at the time, stated that an ABBA reunion would never occur: "I think we have to accept that it will not happen, because we are too old and each one of us has their own life. Too many years have gone by since we stopped, and there's really no meaning in putting us together again". Fältskog further explained that the band members remained on amicable terms: "It's always nice to see each other now and then and to talk a little and to be a little nostalgic." In an April 2014 interview, Fältskog, when asked about whether the band might reunite for a new recording said: "It's difficult to talk about this because then all the news stories will be: 'ABBA is going to record another song!' But as long as we can sing and play, then why not? I would love to, but it's up to Björn and Benny."

The same year the members of ABBA went their separate ways, the French production of a "tribute" show (a children's TV musical named "Abbacadabra" using 14 ABBA songs) spawned new interest in the group's music.

After receiving little attention during the mid-to-late-1980s, ABBA's music experienced a resurgence in the early 1990s due to the UK synth-pop duo Erasure, who released "Abba-esque", a four track extended play release featuring cover versions of ABBA songs which topped several European charts in 1992. As U2 arrived in Stockholm for a concert in June of that year, the band paid homage to ABBA by inviting Björn Ulvaeus and Benny Andersson to join them on stage for a rendition of "Dancing Queen", playing guitar and keyboards. September 1992 saw the release of "", a new compilation album. The single "Dancing Queen" received radio airplay in the UK in the middle of 1992 to promote the album. The song returned to the Top 20 of the UK singles chart in August that year, this time peaking at number 16. With sales of 30 million, "Gold" is the best-selling ABBA album, as well as one of the best-selling albums worldwide. With sales of 5.5 million copies it is the second-highest selling album of all time in the UK, after Queen's "Greatest Hits". "," a follow-up to "Gold", was released in 1993.

In 1994, two Australian cult films caught the attention of the world's media, both focusing on admiration for ABBA: "The Adventures of Priscilla, Queen of the Desert" and "Muriel's Wedding". The same year, "Thank You for the Music", a four-disc box set comprising all the group's hits and stand-out album tracks, was released with the involvement of all four members. "By the end of the twentieth century," American critic Chuck Klosterman wrote a decade later, "it was far more contrarian to hate ABBA than to love them."

ABBA were soon recognised and embraced by other acts: Evan Dando of the Lemonheads recorded a cover version of "Knowing Me, Knowing You"; Sinéad O'Connor and Boyzone's Stephen Gately have recorded "Chiquitita"; Tanita Tikaram, Blancmange and Steven Wilson paid tribute to "The Day Before You Came". Cliff Richard covered "Lay All Your Love on Me", while Dionne Warwick, Peter Cetera, Frank Sidebottom and Celebrity Skin recorded their versions of "SOS". US alternative-rock musician Marshall Crenshaw has also been known to play a version of "Knowing Me, Knowing You" in concert appearances, while legendary English Latin pop songwriter Richard Daniel Roman has recognised ABBA as a major influence. Swedish metal guitarist Yngwie Malmsteen covered "Gimme! Gimme! Gimme! (A Man After Midnight)" with slightly altered lyrics.

Two different compilation albums of ABBA songs have been released. "ABBA: A Tribute" coincided with the 25th anniversary celebration and featured 17 songs, some of which were recorded especially for this release. Notable tracks include Go West's "One of Us", Army of Lovers "Hasta Mañana", Information Society's "Lay All Your Love on Me", Erasure's "Take a Chance on Me" (with MC Kinky), and Lyngstad's a cappella duet with the Real Group of "Dancing Queen". A second 12-track album was released in 1999, titled "ABBAmania", with proceeds going to the Youth Music charity in England. It featured all new cover versions: notable tracks were by Madness ("Money, Money, Money"), Culture Club ("Voulez-Vous"), the Corrs ("The Winner Takes It All"), Steps ("Lay All Your Love on Me", "I Know Him So Well"), and a medley titled "Thank ABBA for the Music" performed by several artists and as featured on the Brits Awards that same year.

In 1998, an ABBA tribute group was formed, the ABBA Teens, which was subsequently renamed the A-Teens to allow the group some independence. The group's first album, "The ABBA Generation", consisting solely of ABBA covers reimagined as 1990s pop songs, was a worldwide success and so were subsequent albums. The group disbanded in 2004 due to a gruelling schedule and intentions to go solo. In Sweden, the growing recognition of the legacy of Andersson and Ulvaeus resulted in the 1998 "B & B Concerts", a tribute concert (with Swedish singers who had worked with the songwriters through the years) showcasing not only their ABBA years, but hits both before and after ABBA. The concert was a success and was ultimately released on CD. It later toured Scandinavia and even went to Beijing in the People's Republic of China for two concerts. In 2000 ABBA were reported to have turned down an offer of approximately one billion US dollars to do a reunion tour consisting of 100 concerts.

For the semi-final of the Eurovision Song Contest 2004, staged in Istanbul 30 years after ABBA had won the contest in Brighton, all four members made cameo appearances in a special comedy video made for the interval act, titled "Our Last Video Ever". Other well-known stars such as Rik Mayall, Cher and Iron Maiden's Eddie also made appearances in the video. It was not included in the official DVD release of the 2004 Eurovision contest, but was issued as a separate DVD release, retitled "The Last Video" at the request of the former ABBA members. The video was made using puppet models of the members of the band. The video has surpassed 13 million views on YouTube as of November 2020.

In 2005, all four members of ABBA appeared at the Stockholm premiere of the musical "Mamma Mia!". On 22 October 2005, at the , "Waterloo" was chosen as the best song in the competition's history. In the same month, American singer Madonna released the single "Hung Up", which contains a sample of the keyboard melody from ABBA's 1979 song "Gimme! Gimme! Gimme! (A Man After Midnight)"; the song was a smash hit, peaking at number one in at least 50 countries. On 4 July 2008, all four ABBA members were reunited at the Swedish premiere of the film "Mamma Mia!". It was only the second time all of them had appeared together in public since 1986. During the appearance, they re-emphasised that they intended never to officially reunite, citing the opinion of Robert Plant that the re-formed Led Zeppelin was more like a cover band of itself than the original band. Ulvaeus stated that he wanted the band to be remembered as they were during the peak years of their success.

"Gold" returned to number-one in the UK album charts for the fifth time on 3 August 2008. On 14 August 2008, the "Mamma Mia! The Movie" film soundtrack went to number-one on the US "Billboard" charts, ABBA's first US chart-topping album. During the band's heyday, the highest album chart position they had ever achieved in America was number 14. In November 2008, all eight studio albums, together with a ninth of rare tracks, were released as "The Albums". It hit several charts, peaking at number-four in Sweden and reaching the Top 10 in several other European territories.

In 2008, Sony Computer Entertainment Europe, in collaboration with Universal Music Group Sweden AB, released "SingStar ABBA" on both the PlayStation 2 and PlayStation 3 games consoles, as part of the SingStar music video games. The PS2 version features 20 ABBA songs, while 25 songs feature on the PS3 version.

On 22 January 2009, Fältskog and Lyngstad appeared together on stage to receive the Swedish music award ""Rockbjörnen"" (for "lifetime achievement"). In an interview, the two women expressed their gratitude for the honorary award and thanked their fans. On 25 November 2009, PRS for Music announced that the British public voted ABBA as the band they would most like to see re-form. On 27 January 2010, ABBAWORLD, a 25-room touring exhibition featuring interactive and audiovisual activities, debuted at Earls Court Exhibition Centre in London. According to the exhibition's website, ABBAWORLD is "approved and fully supported" by the band members.

"Mamma Mia" was released as one of the first few non-premium song selections for the online RPG game "Bandmaster". On 17 May 2011, "Gimme! Gimme! Gimme!" was added as a non-premium song selection for the Bandmaster Philippines server. On 15 November 2011, Ubisoft released a dancing game called "" for the Wii. In January 2012, Universal Music announced the re-release of ABBA's final album "The Visitors", featuring a previously unheard track "From a Twinkling Star to a Passing Angel".

A book titled "ABBA: The Official Photo Book" was published in early 2014 to mark the 40th anniversary of the band's Eurovision victory. The book reveals that part of the reason for the band's outrageous costumes was that Swedish tax laws at the time allowed the cost of garish outfits that were not suitable for daily wear to be tax deductible.

On 20 January 2016, all four members of ABBA made a public appearance at "Mamma Mia! The Party" in Stockholm. On 6 June 2016, the quartet appeared together at a private party at Berns Salonger in Stockholm, which was held to celebrate the 50th anniversary of Andersson and Ulvaeus's first meeting. Fältskog and Lyngstad performed live, singing "The Way Old Friends Do" before they were joined on stage by Andersson and Ulvaeus.

British manager Simon Fuller announced in a statement in October 2016 that the group would be reuniting to work on a new "digital entertainment experience". The project would feature the members in their "life-like" avatar form, called "ABBAtars", based on and would be set to launch by the spring of 2019.
In May 2017, a sequel to the 2008 movie "Mamma Mia!", titled "Mamma Mia! Here We Go Again", was announced; the film was released on 20 July 2018. Cher, who appeared in the movie, also released "Dancing Queen", an ABBA cover album, in September 2018. In June 2017, a blue plaque outside Brighton Dome was set to commemorate their 1974 Eurovision win.

On 27 April 2018, all four original members of ABBA made a joint announcement that they had recorded two new songs, titled "I Still Have Faith in You" and "Don't Shut Me Down", to feature in a TV special set to air later that year. In September 2018, Ulvaeus stated that the two new songs, as well as the TV special, now called "", would not be released until 2019. The TV special was later revealed to be scrapped by 2018, as Andersson and Ulvaeus rejected Fuller's project, and instead partnered with visual effects company Industrial Light and Magic to prepare the ABBAtars for a music video and a concert. In January 2019, it was revealed that neither song would be released before the summer. Andersson hinted at the possibility of a third song.

In June 2019, Ulvaeus announced that the first new song and video containing the ABBAtars would be released in November 2019. In September, he stated in an interview that there were now five new ABBA songs to be released in 2020. In early 2020, Andersson confirmed that he was aiming for the songs to be released in September 2020.

In April 2020, Ulvaeus gave an interview saying that in the wake of the COVID-19 pandemic, the avatar project had been delayed. Five out of the eight original songs written by Benny for the new album had been recorded by the two female members, and the release of a new £15 million music video with new unseen technology was under consideration. In May 2020, it was announced that ABBA's entire studio discography would be released on coloured vinyl for the first time, in a box set titled "ABBA: The Studio Albums." In July 2020, Ulvaeus revealed that the release of the new ABBA recordings had been delayed until 2021.

On 22 September 2020, all four ABBA members reunited at Ealing Studios in London to continue working on the avatar project and filming for the tour. Ulvaeus confirmed that the avatar tour would be scheduled for 2022. When questioned if the new recordings were definitely coming out in 2021, Björn said "There will be new music this year, that is definite, it's not a case anymore of it might happen, it will happen."
On 26 August 2021, a new website was launched, with the title "ABBA Voyage". On the page, visitors were prompted to subscribe "to be the first in line to hear more about "ABBA Voyage"". Simultaneously with the launch of the webpage, new "ABBA Voyage" social media accounts were launched, and billboards around London started to appear, all showing the date "02.09.21", leading to expectation of what was to be revealed on that date. On 29 August, the band officially joined TikTok with a video of Benny Andersson playing "Dancing Queen" on the piano, and media reported on a new album to be announced on 2 September. On that date, "Voyage", their first new album in 40 years, was announced to be released on 5 November 2021, along with ABBA Voyage, a concert residency in a custom-built venue at Queen Elizabeth Olympic Park in London featuring the motion capture digital avatars of the four band members alongside a 10-piece live band, starting 27 May 2022. Fältskog stated that the "Voyage" album and concert residency are likely to be their last activity as a group.

The announcement of the new album was accompanied by the release of the singles "I Still Have Faith in You" and "Don't Shut Me Down". The music video for "I Still Have Faith in You", featuring footage of the band during their performing years and a first look at the ABBAtars, earned over a million views in its first three hours. "Don't Shut Me Down" became the first ABBA release since October 1978 to top the singles chart in Sweden. In October 2021, the third single "Just a Notion" was released, and it was announced that ABBA would split for good after the release of "Voyage". However, in an interview with BBC Radio 2 on 11 November, Lyngstad stated "don't be too sure" that "Voyage" is the final ABBA album. Also, in an interview with BBC News on 5 November, Andersson stated "if they [the ladies] twist my arm I might change my mind." The fourth single from the album, "Little Things", was released on 3 December.

In May 2022, after the premiere of ABBA Voyage, Andersson stated in an interview with "Variety" that "nothing is going to happen after this", confirming the residency as ABBA's final group collaboration. In April 2023, longtime ABBA guitarist Lasse Wellander died at the age of 70; Wellander played on seven of the group's nine studio albums, including "Voyage".

ABBA were perfectionists in the studio, working on tracks until they got them right rather than leaving them to come back to later on. They spent the bulk of their time within the studio; in separate 2021 interviews Ulvaeus stated they may have toured for only 6 months while Andersson said they played fewer than 100 shows during the band's career. Although, counting shorter 30 to 60 minute concerts during their Folkpark tours, the group in fact played over 200 shows. 

The band created a basic rhythm track with a drummer, guitarist and bass player, and overlaid other arrangements and instruments. Vocals were then added, and orchestra overdubs were usually left until last.

Fältskog and Lyngstad contributed ideas at the studio stage. Andersson and Ulvaeus played them the backing tracks and they made comments and suggestions. According to Fältskog, she and Lyngstad had the final say in how the lyrics were shaped.

ABBA was widely noted for the colourful and trend-setting costumes its members wore. The reason for the wild costumes was Swedish tax law: the cost of the clothes was deductible only if they could not be worn other than for performances. In their early years, group member Anni-Frid Lyngstad designed and even hand sewed the outfits. Later, as their success grew, they used professional theatrical clothes designer Owe Sandström together with tailor Lars Wigenius with Lyngstad continuing to suggest ideas while co-ordinating the outfits with concert set designs. Choreography by Graham Tainton also contributed to their performance style.

The videos that accompanied some of the band's biggest hits are often cited as being among the earliest examples of the genre. Most of ABBA's videos (and "ABBA: The Movie") were directed by Lasse Hallström, who would later direct the films "My Life as a Dog", "The Cider House Rules" and "Chocolat".

ABBA made videos because their songs were hits in many different countries and personal appearances were not always possible. This was also done in an effort to minimise travelling, particularly to countries that would have required extremely long flights. Fältskog and Ulvaeus had two young children and Fältskog, who was also afraid of flying, was very reluctant to leave her children for such a long time. ABBA's manager, Stig Anderson, realised the potential of showing a simple video clip on television to publicise a single or album, thereby allowing easier and quicker exposure than a concert tour. Some of these videos have become classics because of the 1970s-era costumes and early video effects, such as the grouping of the band members in different combinations of pairs, overlapping one singer's profile with the other's full face, and the contrasting of one member against another.

In 1976, ABBA participated in an advertising campaign to promote the Matsushita Electric Industrial Co.'s brand, National, in Australia. The campaign was also broadcast in Japan. Five commercial spots, each of approximately one minute, were produced, each presenting the "National Song" performed by ABBA using the melody and instrumental arrangements of "Fernando" and revised lyrics.

In September 2010, band members Andersson and Ulvaeus criticised the right-wing Danish People's Party (DF) for using the ABBA song "Mamma Mia" (with modified lyrics referencing Pia Kjærsgaard) at rallies. The band threatened to file a lawsuit against the DF, saying they never allowed their music to be used politically and that they had absolutely no interest in supporting the party. Their record label Universal Music later stated that no legal action would be taken because an agreement had been reached.

During their active career, from 1972 to 1982, 20 of ABBA's singles entered the "Billboard" Hot 100; 14 of these made the Top 40 (13 on the Cashbox Top 100), with 10 making the Top 20 on both charts. A total of four of those singles reached the Top 10, including "Dancing Queen", which reached number one in April 1977. While "Fernando" and "SOS" did not break the Top 10 on the "Billboard" Hot 100 (reaching number 13 and 15 respectively), they did reach the Top 10 on Cashbox ("Fernando") and Record World ("SOS") charts. Both "Dancing Queen" and "Take a Chance on Me" were certified gold by the Recording Industry Association of America for sales of over one million copies each.

The group also had 12 Top 20 singles on the "Billboard" Adult Contemporary chart with two of them, "Fernando" and "The Winner Takes It All", reaching number one. "Lay All Your Love on Me" was ABBA's fourth number-one single on a "Billboard" chart, topping the Hot Dance Club Play chart.

Ten ABBA albums have made their way into the top half of the "Billboard" 200 album chart, with eight reaching the Top 50, five reaching the Top 20 and one reaching the Top 10. In November 2021, Voyage became ABBA's highest-charting album on the Billboard 200 peaking at No. 2. Five albums received RIAA gold certification (more than 500,000 copies sold), while three acquired platinum status (selling more than one million copies).

The compilation album "" topped the "Billboard" Top Pop Catalog Albums chart in August 2008 (15 years after it was first released in the US in 1993), becoming the group's first number-one album ever on any of the "Billboard" album charts. It has sold 6 million copies there.

On 15 March 2010, ABBA was inducted into the Rock and Roll Hall of Fame by Bee Gees members Barry Gibb and Robin Gibb. The ceremony was held at the Waldorf Astoria Hotel in New York City. The group were represented by Anni-Frid Lyngstad and Benny Andersson.

in November 2021, the group received a Grammy nomination for Record of the Year. The single, "I Still Have Faith in You", from the album, "Voyage", was their first ever nomination.


The members of ABBA were married as follows: Agnetha Fältskog and Björn Ulvaeus from 1971 to 1979; Benny Andersson and Anni-Frid Lyngstad from 1978 to 1981. For their subsequent marriages, see their articles.

In addition to the four members of ABBA, other musicians regularly played on their studio recordings, live appearances and concert performances. These include:

Studio albums






Allegiance

An allegiance is a duty of fidelity said to be owed, or freely committed, by the people, subjects or citizens to their state or sovereign.

The word "allegiance" comes from Middle English ' (see Medieval Latin ', "a liegance"). The "al-" prefix was probably added through confusion with another legal term, "allegiance", an "allegation" (the French ' comes from the English). "Allegiance" is formed from "liege," from Old French ', "liege, free", of Germanic origin. The connection with Latin "", "to bind," is erroneous.

Traditionally, English legal commentators used the term "allegiance" in two ways. In one sense, it referred to the deference which anyone, even a foreigner, was expected to pay to the institutions of the country where one lived. In the other sense, it meant national character and the subjection due to that character.


The English doctrine, which was at one time adopted in the United States, asserted that allegiance was indelible: "Nemo potest exuere patriam". As the law stood prior to 1870, every person who by birth or naturalisation satisfied the conditions set forth, even if removed in infancy to another country where their family resided, owed an allegiance to the British crown which they could never resign or lose, except by act of parliament or by the recognition of the independence or the cession of the portion of British territory in which they resided.

This refusal to accept any renunciation of allegiance to the Crown led to conflict with the United States over impressment, which led to further conflicts during the War of 1812, when thirteen Irish American prisoners of war were executed as traitors after the Battle of Queenston Heights; Winfield Scott urged American reprisal, but none was carried out.

Allegiance was the tie which bound the subject to the sovereign, in return for that protection which the sovereign afforded the subject. It was the mutual bond and obligation between monarch and subjects, whereby subjects were called their liege subjects, because they are bound to obey and serve them; and the monarch was called their liege lord, because they should maintain and defend them ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525; "China Navigation Co v Attorney-General" (1932) 48 TLR 375; "Attorney-General v Nissan" [1969] 1 All ER 629; "Oppenheimer v Cattermole" [1972] 3 All ER 1106). The duty of the crown towards its subjects was to govern and protect them. The reciprocal duty of the subject towards the crown was that of allegiance.

At common law, allegiance was a true and faithful obedience of the subject due to their sovereign. As the subject owed to their sovereign their true and faithful allegiance and obedience, so the sovereign


Natural allegiance and obedience is an incident inseparable to every subject, for parte Anderson (1861) 3 El & El 487; 121 ER 525). Natural-born subjects owe allegiance wherever they may be. Where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).

Allegiance is owed both to the sovereign as a natural person and to the sovereign in the political capacity ("Re Stepney Election Petition, Isaacson v Durant" (1886) 17 QBD 54 (per Lord Coleridge CJ)). Attachment to the person of the reigning sovereign is not sufficient. Loyalty requires affection also to the office of the sovereign, attachment to royalty, attachment to the law and to the constitution of the realm, and he who would, by force or by fraud, endeavour to prostrate that law and constitution, though he may retain his affection for its head, can boast but an imperfect and spurious species of loyalty ("R v O'Connell" (1844) 7 ILR 261).

There were four kinds of allegiances ("Rittson v Stordy" (1855) 3 Sm & G 230; "De Geer v Stone" (1882) 22 Ch D 243; "Isaacson v Durant" (1886) 54 LT 684; "Gibson, Gavin v Gibson" [1913] 3 KB 379; "Joyce v DPP" [1946] AC 347; "Collingwood v Pace" (1661) O Bridg 410; "Lane v Bennett" (1836) 1 M & W 70; "Lyons Corp v East India Co" (1836) 1 Moo PCC 175; "Birtwhistle v Vardill" (1840) 7 Cl & Fin 895; "R v Lopez, R v Sattler" (1858) Dears & B 525; Ex p Brown (1864) 5 B & S 280);

(a) "Ligeantia naturalis, absoluta, pura et indefinita", and this originally is due by nature and birthright, and is called "alta ligeantia", and those that owe this are called "subditus natus";

(b) "Ligeantia acquisita", not by nature but by acquisition or denization, being called a denizen, or rather denizon, because they are "subditus datus";

(c) "Ligeantia localis", by operation of law, when a friendly alien enters the country, because so long as they are in the country they are within the sovereign's protection, therefore they owe the sovereign a local obedience or allegiance ("R v Cowle" (1759) 2 Burr 834; "Low v Routledge" (1865) 1 Ch App 42; "Re Johnson, Roberts v Attorney-General" [1903] 1 Ch 821; "Tingley v Muller" [1917] 2 Ch 144; "Rodriguez v Speyer" [1919] AC 59; "Johnstone v Pedlar" [1921] 2 AC 262; "R v Tucker" (1694) Show Parl Cas 186; "R v Keyn" (1876) 2 Ex D 63; "Re Stepney Election Petn, Isaacson v Durant" (1886) 17 QBD 54);

(d) A legal obedience, where a particular law requires the taking of an oath of allegiance by subject or alien alike.

Natural allegiance was acquired by birth within the sovereign's dominions (except for the issue of diplomats or of invading forces or of an alien in an enemy occupied territory). The natural allegiance and obedience are an incident inseparable from every subject, for as soon as they are born they owe by birthright allegiance and obedience to the Sovereign ("Ex p. Anderson" (1861) 3 E & E 487). A natural-born subject owes allegiance wherever they may be, so that where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).

Acquired allegiance was acquired by naturalisation or denization. Denization, or "ligeantia acquisita", appears to be threefold ("Thomas v Sorrel" (1673) 3 Keb 143);


Local allegiance was due by an alien while in the protection of the crown. All friendly resident aliens incurred all the obligations of subjects ("The Angelique" (1801) 3 Ch Rob App 7). An alien, coming into a colony, also became, temporarily, a subject of the crown, and acquired rights both within and beyond the colony, and these latter rights could not be affected by the laws of that colony ("Routledge v Low" (1868) LR 3 HL 100; 37 LJ Ch 454; 18 LT 874; 16 WR 1081, HL; "Reid v Maxwell" (1886) 2 TLR 790; "Falcon v Famous Players Film Co" [1926] 2 KB 474).

A resident alien owed allegiance even when the protection of the crown was withdrawn owing to the occupation of an enemy, because the absence of the crown's protection was temporary and involuntary ("de Jager v Attorney-General of Natal" [1907] AC 326).

Legal allegiance was due when an alien took an oath of allegiance required for a particular office under the crown.

By the Naturalisation Act 1870, it was made possible for British subjects to renounce their nationality and allegiance, and the ways in which that nationality is lost were defined. So British subjects voluntarily naturalized in a foreign state are deemed aliens from the time of such naturalization, unless, in the case of persons naturalized before the passing of the act, they had declared their desire to remain British subjects within two years from the passing of the act. Persons who, from having been born within British territory, are British subjects, but who, at birth, came under the law of any foreign state or of subjects of such state, and, also, persons who, though born abroad, are British subjects by reason of parentage, may, by declarations of alienage, get rid of British nationality. Emigration to an uncivilized country left British nationality unaffected: indeed the right claimed by all states to follow with their authority their subjects so emigrating was one of the usual and recognized means of colonial expansion.

The doctrine that no man can cast off his native allegiance without the consent of his sovereign was early abandoned in the United States, and Chief Justice John Rutledge also declared in Talbot v. Janson, "a man may, at the same time, enjoy the rights of citizenship under two governments." On July 27, 1868, the day before the Fourteenth Amendment was adopted, U.S. Congress declared in the preamble of the Expatriation Act that "the right of expatriation is a natural and inherent right of all people, indispensable to the enjoyment of the rights of life, liberty and the pursuit of happiness," and (Section I) one of "the fundamental principles of this government" (United States Revised Statutes, sec. 1999). Every natural-born citizen of a foreign state who is also an American citizen, and every natural-born American citizen who is also a citizen of a foreign land, owes a double allegiance, one to the United States, and one to their homeland (in the event of an immigrant becoming a citizen of the US) or to their adopted land (in the event of an emigrant natural-born citizen of the US becoming a citizen of another nation). If these allegiances come into conflict, the person may be guilty of treason against one or both. If the demands of these two sovereigns upon their duty of allegiance come into conflict, those of the United States have the paramount authority in American law; likewise, those of the foreign land have paramount authority in their legal system. In such a situation, it may be incumbent on the individual to renounce one of their citizenships, to avoid possibly being forced into situations where countervailing duties are required of them, such as might occur in the event of war.

The oath of allegiance is an oath of fidelity to the sovereign taken by all persons holding important public office and as a condition of naturalization. By ancient common law, it was required of all persons above the age of 12, and it was repeatedly used as a test for the disaffected. In England, it was first imposed by statute in the reign of Elizabeth I (1558), and its form has, more than once, been altered since. Up to the time of the revolution, the promise was "to be true and faithful to the king and his heirs, and truth and faith to bear of life and limb and terrene honour, and not to know or hear of any ill or damage intended him without defending him therefrom." This was thought to favour the doctrine of absolute non-resistance, and, accordingly, the Convention Parliament enacted the form that has been in use since that time – "I do sincerely promise and swear that I will be faithful and bear true allegiance to His Majesty ..."

In the United States and some other republics, the oath is known as the Pledge of Allegiance. Instead of declaring fidelity to a monarch, the pledge is made to the flag, the republic, and to the core values of the country, specifically liberty and justice. The reciting of the pledge in the United States is voluntary because of the rights guaranteed to the people under the First Amendment to the United States Constitution - specifically, the guarantee of freedom of speech, which inherently includes the freedom "not" to speak.

The word used in the Arabic language for allegiance is "bay'at" (Arabic: بيعة), which means "taking hand". The practice is sanctioned in the Quran by Surah 48:10: "Verily, those who give thee their allegiance, they give it but to Allah Himself". The word is used for the oath of allegiance to an emir. It is also used for the initiation ceremony specific to many Sufi orders.



Altenberg

Altenberg (German for "old mountain" or "mountain of the old") may refer to:







MessagePad

The MessagePad is a discontinued series of personal digital assistant devices developed by Apple Computer for the Newton platform in 1993. Some electronic engineering and the manufacture of Apple's MessagePad devices was undertaken in Japan by Sharp. The devices are based on the ARM 610 RISC processor and all featured handwriting recognition software and were developed and marketed by Apple. The devices run Newton OS.

The development of the Newton MessagePad first began with Apple's former senior vice president of research and development, Jean-Louis Gassée; his team included Steve Capps, co-writer of macOS Finder, and an employed engineer named Steve Sakoman. The development of the Newton MessagePad operated in secret until it was eventually revealed to the Apple Board of Directors in late 1990.

When Gassée resigned from his position due to a significant disagreement with the board, seeing how his employer was treated, Sakoman also stopped developing the MessagePad on March 2, 1990.

Bill Atkinson, an Apple Executive responsible for the company's Lisa graphical interface, invited Steve Capps, John Sculley, Andy Hertzfeld, Susan Kare, and Marc Porat to a meeting on March 11, 1990. There, they brainstormed a way of saving the MessagePad. Sculley suggested adding new features, including libraries, museums, databases, or institutional archives features, allowing customers to navigate through various window tabs or opened galleries/stacks. The Board later approved his suggestion; he then gave Newton it is official and full backing.

The first MessagePad was unveiled by Sculley on the 29th of May 1992 at the summer Consumer Electronics Show (CES) in Chicago. Sculley caved in to pressure to unveil the product early because the Newton did not officially ship for another 14 months on August 2, 1993, starting at a price of . Over 50,000 units were sold by late November 1993.

With the MessagePad 120 with Newton OS 2.0, the Newton Keyboard by Apple became available, which can also be used via the dongle on Newton devices with a Newton InterConnect port, most notably the Apple MessagePad 2000/2100 series, as well as the Apple eMate 300.

Newton devices featuring Newton OS 2.1 or higher can be used with the screen turned horizontally ("landscape") as well as vertically ("portrait"). A change of a setting rotates the contents of the display by 90, 180 or 270 degrees. Handwriting recognition still works properly with the display rotated, although display calibration is needed when rotation in any direction is used for the first time or when the Newton device is reset.

In initial versions (Newton OS 1.x) the handwriting recognition gave extremely mixed results for users and was sometimes inaccurate. The original handwriting recognition engine was called Calligrapher, and was licensed from a Russian company called . Calligrapher's design was quite sophisticated; it attempted to learn the user's natural handwriting, using a database of known words to make guesses as to what the user was writing, and could interpret writing anywhere on the screen, whether hand-printed, in cursive, or a mix of the two. By contrast, Palm Pilot's Graffiti had a less sophisticated design than Calligrapher, but was sometimes found to be more accurate and precise due to its reliance on a fixed, predefined stroke alphabet. The stroke alphabet used letter shapes which resembled standard handwriting, but which were modified to be both simple and very easy to differentiate. Palm Computing also released two versions of Graffiti for Newton devices. The Newton version sometimes performed better and could also show strokes as they were being written as input was done on the display itself, rather than on a silkscreen area.

For editing text, Newton had a very intuitive system for handwritten editing, such as scratching out words to be deleted, circling text to be selected, or using written carets to mark inserts.

Later releases of the Newton operating system retained the original recognizer for compatibility, but added a hand-printed-text-only (not cursive) recognizer, called "Rosetta", which was developed by Apple, included in version 2.0 of the Newton operating system, and refined in Newton 2.1. Rosetta is generally considered a significant improvement and many reviewers, testers, and most users consider the Newton 2.1 handwriting recognition software better than any of the alternatives even 10 years after it was introduced. Recognition and computation of handwritten horizontal and vertical formulas such as "1 + 2 =" was also under development but never released. However, users wrote similar programs which could evaluate mathematical formulas using the Newton OS Intelligent Assistant, a unique part of every Newton device.

The handwriting recognition and parts of the user interface for the Newton are best understood in the context of the broad history of pen computing, which is quite extensive.

A vital feature of the Newton handwriting recognition system is the modeless error correction. That is, correction done "in situ" without using a separate window or widget, using a minimum of gestures. If a word is recognized improperly, the user could double-tap the word and a list of alternatives would pop up in a menu under the stylus. Most of the time, the correct word will be in the list. If not, a button at the bottom of the list allows the user to edit individual characters in that word. Other pen gestures could do such things as transpose letters (also "in situ"). The correction popup also allowed the user to revert to the original, un-recognized letter shapes - this would be useful in note-taking scenarios if there was insufficient time to make corrections immediately. To conserve memory and storage space, alternative recognition hypotheses would not be saved indefinitely. If the user returned to a note a week later, for example, they would only see the best match. Error correction in many current handwriting systems provides such functionality but adds more steps to the process, greatly increasing the interruption to a user's workflow that a given correction requires.

Text could also be entered by tapping with the stylus on a small on-screen pop-up QWERTY virtual keyboard, although more layouts were developed by users. Newton devices could also accept free-hand "Sketches", "Shapes", and "Ink Text", much like a desktop computer graphics tablet. With "Shapes", Newton could recognize that the user was attempting to draw a circle, a line, a polygon, etc., and it would clean them up into perfect vector representations (with modifiable control points and defined vertices) of what the user was attempting to draw. "Shapes" and "Sketches" could be scaled or deformed once drawn. "Ink text" captured the user's free-hand writing but allowed it to be treated somewhat like recognized text when manipulating for later editing purposes ("ink text" supported word wrap, could be formatted to be bold, italic, etc.). At any time a user could also direct their Newton device to recognize selected "ink text" and turn it into recognized text (deferred recognition). A Newton note (or the notes attached to each contact in Names and each Dates calendar or to-do event) could contain any mix of interleaved text, Ink Text, Shapes, and Sketches.

While the Newton offered handwriting recognition training and would clean up sketches into vector shapes, both were unreliable and required much rewriting and redrawing. The most reliable application of the Newton was collecting and organizing address and phone numbers. While handwritten messages could be stored, they could not be easily filed, sorted or searched. While the technology was a probable cause for the failure of the device (which otherwise met or exceeded expectations), the technology has been instrumental in producing the future generation of handwriting software that realizes the potential and promise that began in the development of Newton-Apple's Ink Handwriting Recognition.

The MessagePad 100 series of devices used Macintosh's proprietary serial ports—round Mini-DIN 8 connectors. The MessagePad 2000/2100 models (as well as the eMate 300) have a small, proprietary "Newton InterConnect" port. However, the development of the Newton hardware/software platform was canceled by Steve Jobs on February 27, 1998, so the InterConnect port, while itself very advanced, can only be used to connect a serial dongle. A prototype multi-purpose InterConnect device containing serial, audio in, audio out, and other ports was also discovered. In addition, all Newton devices have infrared connectivity, initially only the Sharp ASK protocol, but later also IrDA, though the Sharp ASK protocol was kept in for compatibility reasons. Unlike the Palm Pilot, all Newton devices are equipped with a standard PC Card expansion slot (two on the 2000/2100). This allows native modem and even Ethernet connectivity; Newton users have also written drivers for 802.11b wireless networking cards and ATA-type flash memory cards (including the popular CompactFlash format), as well as for Bluetooth cards. Newton can also dial a phone number through the built-in speaker of the Newton device by simply holding a telephone handset up to the speaker and transmitting the appropriate tones. Fax and printing support is also built in at the operating system level, although it requires peripherals such as parallel adapters, PCMCIA cards, or serial modems, the most notable of which is the lightweight Newton Fax Modem released by Apple in 1993. It is powered by 2 AA batteries, and can also be used with a power adapter. It provides data transfer at 2,400 bit/s, and can also send and receive fax messages at 9,600 and 4,800 bit/s respectively.

The original Apple MessagePad and MessagePad 100 used four AAA batteries. They were eventually replaced by AA batteries with the release of the Apple MessagePad 110.

The use of 4 AA NiCd (MessagePad 110, 120 and 130) and 4x AA NiMH cells (MP2x00 series, eMate 300) give a runtime of up to 30 hours (MP2100 with two 20 MB Linear Flash memory PC Cards, no backlight usage) and up to 24 hours with backlight on. While adding more weight to the handheld Newton devices than AAA batteries or custom battery packs, the choice of an easily replaceable/rechargeable cell format gives the user a still unsurpassed runtime and flexibility of power supply. This, together with the flash memory used as internal storage starting with the Apple MessagePad 120 (if all cells lost their power, no data was lost due to the non-volatility of this storage), gave birth to the slogan "Newton never dies, it only gets new batteries".

The Apple MessagePad 2000/2100, with a vastly improved handwriting recognition system, 162 MHz StrongARM SA-110 RISC processor, Newton OS 2.1, and a better, clearer, backlit screen, attracted critical plaudits.

The eMate 300 was a Newton device in a laptop form factor offered to schools in 1997 as an inexpensive ($799 US, originally sold to education markets only) and durable computer for classroom use. However, in order to achieve its low price, the eMate 300 did not have all the speed and features of the contemporary MessagePad equivalent, the MessagePad 2000. The eMate was cancelled along with the rest of the Newton products in 1998. It is the only Newton device to use the ARM710 microprocessor (running at 25 MHz), have an integrated keyboard, use Newton OS 2.2 (officially numbered 2.1), and its batteries are officially irreplaceable, although several users replaced them with longer-lasting ones without any damage to the eMate hardware whatsoever.

Many prototypes of additional Newton devices were spotted. Most notable was a Newton tablet or "slate", a large, flat screen that could be written on. Others included a "Kids Newton" with side handgrips and buttons, "VideoPads" which would have incorporated a video camera and screen on their flip-top covers for two-way communications, the "Mini 2000" which would have been very similar to a Palm Pilot, and the NewtonPhone developed by Siemens, which incorporated a handset and a keyboard.

Fourteen months after Sculley demoed it at the May 1992, Chicago CES, the MessagePad was first offered for sale on August 2, 1993, at the Boston Macworld Expo. The hottest item at the show, it cost $900. 50,000 MessagePads were sold in the device's first three months on the market.

The original Apple MessagePad and MessagePad 100 were limited by the very short lifetime of their inadequate AAA batteries.

Later versions of Newton OS offered improved handwriting recognition, quite possibly a leading reason for the continued popularity of the devices among Newton users. Even given the age of the hardware and software, Newtons still demand a sale price on the used market far greater than that of comparatively aged PDAs produced by other companies. In 2006, CNET compared an Apple MessagePad 2000 to a Samsung Q1, and the Newton was declared better. In 2009, CNET compared an Apple MessagePad 2000 to an iPhone 3GS, and the Newton was declared more innovative at its time of release.

A chain of dedicated Newton-only stores called Newton Source, independently run by Stephen Elms, existed from 1994 until 1998. Locations included New York, Los Angeles, San Francisco, Chicago and Boston. The Westwood Village, California, near UCLA featured the trademark red and yellow light bulb Newton logo in neon. The stores provided an informative educational venue to learn about the Newton platform in a hands on relaxed fashion. The stores had no traditional computer retail counters and featured oval desktops where interested users could become intimately involved with the Newton product range. The stores were a model for the later Apple Stores.

<nowiki>*</nowiki> Varies with installed OS

Notes: The eMate 300 actually has ROM chips silk screened with 2.2 on them. Stephanie Mak on her website discusses this:
If one removes all patches to the eMate 300 (by replacing the ROM chip, and then putting in the original one again, as the eMate and the MessagePad 2000/2100 devices erase their memory completely after replacing the chip), the result will be the Newton OS saying that this is version 2.2.00. Also, the Original MessagePad and the MessagePad 100 share the same model number, as they only differ in the ROM chip version. (The OMP has OS versions 1.0 to 1.05, or 1.10 to 1.11, while the MP100 has 1.3 that can be upgraded with various patches.)

The Newton OS was also licensed to a number of third party developers including Sharp and Motorola who developed additional PDA devices based on the Newton platform. Motorola added wireless connectivity, as well as made a unique two-part design, and shipped additional software with its Newton device, called the Marco. Sharp developed a line of Newton devices called the ExpertPad PI-7000/7100; those were the same as Apple's MessagePad and MessagePad 100, the only difference is the physical design (the ExpertPads feature a screen lid, which Apple added in 1994 with the release of the MessagePad 110) and the naming.

There were a number of projects that used the Newton as a portable information device in cultural settings such as museums. For example, Visible Interactive created a walking tour in San Francisco's Chinatown but the most significant effort took place in Malaysia at the Petronas Discovery Center, known as Petrosains.

In 1995, an exhibit design firm, DMCD Inc., was awarded the contract to design a new science museum in the Petronas Towers in Kuala Lumpur. A major factor in the award was the concept that visitors would use a Newton device to access additional information, find out where they were in the museum, listen to audio, see animations, control robots and other media, and to bookmark information for printout at the end of the exhibit.

The device became known as the ARIF, a Malay word for "wise man" or "seer" and it was also an acronym for A Resourceful Informative Friend. Some 400 ARIFS were installed and over 300 are still in use today. The development of the ARIF system was extremely complex and required a team of hardware and software engineers, designers, and writers. ARIF is an ancestor of the PDA systems used in museums today and it boasted features that have not been attempted since.

Anyway & Company firm was involved with the Petronas Discovery Center project back in 1998 and NDAs were signed which prevents getting to know more information about this project. It was confirmed that they purchased of MP2000u or MP2100's by this firm on the behalf of the project under the name of "Petrosains Project Account". By 1998 they had invested heavily into the R&D of this project with the Newton at the center. After Apple officially cancelled the Newton in 1998 they had to acquire as many Newtons as possible for this project. It was estimated initially 1000 Newtons, but later readjusted the figure to possibly 750 Newtons. They placed an “Internet Call” for Newtons. They purchased them in large and small quantities.

The Newton was also used in healthcare applications, for example in collecting data directly from patients. Newtons were used as electronic diaries, with patients entering their symptoms and other information concerning their health status on a daily basis. The compact size of the device and its ease of use made it possible for the electronic diaries to be carried around and used in the patients' everyday life setting. This was an early example of electronic patient-reported outcomes (ePRO).





A. E. van Vogt

Alfred Elton van Vogt ( ; April 26, 1912 – January 26, 2000) was a Canadian-born American science fiction author. His fragmented, bizarre narrative style influenced later science fiction writers, notably Philip K. Dick. He was one of the most popular and influential practitioners of science fiction in the mid-twentieth century, the genre's so-called Golden Age, and one of the most complex. The Science Fiction Writers of America named him their 14th Grand Master in 1995 (presented 1996).

Alfred Vogt (both "Elton" and "van" were added much later) was born on April 26, 1912, on his grandparents' farm in Edenburg, Manitoba, a tiny (and now defunct) Russian Mennonite community east of Gretna, Manitoba, Canada, in the Mennonite West Reserve. He was the third of six children born to Heinrich "Henry" Vogt and Aganetha "Agnes" Vogt (née Buhr), both of whom were born in Manitoba and grew up in heavily immigrant communities. Until he was four, van Vogt spoke only Plautdietsch at home.

For the first dozen or so years of his life, van Vogt's father, Henry Vogt, a lawyer, moved his family several times within western Canada, moving to Neville, Saskatchewan; Morden, Manitoba; and finally Winnipeg, Manitoba. Alfred Vogt found these moves difficult, later remarking:

By the 1920s, living in Winnipeg, father Henry worked as an agent for a steamship company, but the stock market crash of 1929 proved financially disastrous, and the family could not afford to send Alfred to college. During his teen years, Alfred worked as a farmhand and a truck driver, and by the age of 19, he was working in Ottawa for the Canadian Census Bureau.

In "the dark days of '31 and '32," van Vogt took a correspondence course in writing from the Palmer Institute of Authorship. He sold his first story in fall 1932. His early published works were stories in the true confession style of magazines such as "True Story". Most of these stories were published anonymously, with the first-person narratives allegedly being written by people (often women) in extraordinary, emotional, and life-changing circumstances.

After a year in Ottawa, he moved back to Winnipeg, where he sold newspaper advertising space and continued to write. While continuing to pen melodramatic "true confessions" stories through 1937, he also began writing short radio dramas for local radio station CKY, as well as conducting interviews published in trade magazines. He added the middle name "Elton" at some point in the mid-1930s, and at least one confessional story (1937's "To Be His Keeper") was sold to the "Toronto Star", who misspelled his name "Alfred Alton Bogt" in the byline. Shortly thereafter, he added the "van" to his surname, and from that point forward he used the name "A. E. van Vogt" both personally and professionally.

By 1938, van Vogt decided to switch to writing science fiction, a genre he enjoyed reading. He was inspired by the August 1938 issue of "Astounding Science Fiction," which he picked up at a newsstand. John W. Campbell's novelette "Who Goes There?" (later adapted into "The Thing from Another World" and "The Thing") inspired van Vogt to write "Vault of the Beast", which he submitted to that same magazine. Campbell, who edited "Astounding" (and had written the story under a pseudonym), sent van Vogt a rejection letter in which Campbell encouraged van Vogt to try again. Van Vogt sent another story, entitled "Black Destroyer", which was accepted. It featured a fierce, carnivorous alien stalking the crew of a spaceship, and served as the inspiration for multiple science fiction movies, including "Alien" (1979). A revised version of "Vault of the Beast" was published in 1940.

While still living in Winnipeg, in 1939 van Vogt married Edna Mayne Hull, a fellow Manitoban. Hull, who had previously worked as a private secretary, went on to act as van Vogt's typist, and was credited with writing several SF stories of her own throughout the early 1940s.

The outbreak of World War II in September 1939 caused a change in van Vogt's circumstances. Ineligible for military service due to his poor eyesight, he accepted a clerking job with the Canadian Department of National Defence. This necessitated a move back to Ottawa, where he and his wife stayed for the next year and a half.

Meanwhile, his writing career continued. "Discord in Scarlet" was van Vogt's second story to be published, also appearing as the cover story. It was accompanied by interior illustrations created by Frank Kramer and Paul Orban. (Van Vogt and Kramer thus debuted in the issue of "Astounding" that is sometimes identified as the start of the Golden Age of Science Fiction.) Among his most famous works of this era, "Far Centaurus" appeared in the January 1944 edition of "Astounding".

Van Vogt's first completed novel, and one of his most famous, is "Slan" (Arkham House, 1946), which Campbell serialized in "Astounding" (September to December 1940). Using what became one of van Vogt's recurring themes, it told the story of a nine-year-old superman living in a world in which his kind are slain by "Homo sapiens".

Others saw van Vogt's talent from his first story, and in May 1941 van Vogt decided to become a full-time writer, quitting his job at the Canadian Department of National Defence. Freed from the necessity of living in Ottawa, he and his wife lived for a time in the Gatineau region of Quebec before moving to Toronto in the fall of 1941.

Prolific throughout this period, van Vogt wrote many of his more famous short stories and novels in the years from 1941 through 1944. The novels "The Book of Ptath" and "The Weapon Makers" both appeared in magazines in serial form during this period; they were later published in book form after World War II. As well, several (though not all) of the stories that were compiled to make up the novels "The Weapon Shops of Isher", "The Mixed Men" and "The War Against the Rull" were published during this time.

In November 1944, van Vogt and Hull moved to Hollywood; van Vogt would spend the rest of his life in California. He had been using the name "A. E. van Vogt" in his public life for several years, and as part of the process of obtaining American citizenship in 1945 he finally and formally changed his legal name from Alfred Vogt to Alfred Elton van Vogt. To his friends in the California science fiction community, he was known as "Van".

Van Vogt systematized his writing method, using scenes of 800 words or so where a new complication was added or something resolved. Several of his stories hinge on temporal conundra, a favorite theme. He stated that he acquired many of his writing techniques from three books: "Narrative Technique" by Thomas Uzzell, "The Only Two Ways to Write a Story" by John Gallishaw, and "Twenty Problems of the Fiction Writer" by Gallishaw. He also claimed many of his ideas came from dreams; throughout his writing life he arranged to be awakened every 90 minutes during his sleep period so he could write down his dreams.

Van Vogt was also always interested in the idea of all-encompassing systems of knowledge (akin to modern meta-systems). The characters in his very first story used a system called "Nexialism" to analyze the alien's behavior. Around this time, he became particularly interested in the general semantics of Alfred Korzybski.

He subsequently wrote a novel merging these overarching themes, "The World of Ā", originally serialized in "Astounding" in 1945. Ā (often rendered as "Null-A"), or non-Aristotelian logic, refers to the capacity for, and practice of, using intuitive, inductive reasoning (compare fuzzy logic), rather than reflexive, or conditioned, deductive reasoning. The novel recounts the adventures of an individual living in an apparent Utopia, where those with superior brainpower make up the ruling class... though all is not as it seems. A sequel, "The Players of Ā" (later re-titled "The Pawns of Null-A") was serialized in 1948–49.

At the same time, in his fiction, van Vogt was consistently sympathetic to absolute monarchy as a form of government. This was the case, for instance, in the "Weapon Shop" series, the "Mixed Men" series, and in single stories such as "Heir Apparent" (1945), whose protagonist was described as a "benevolent dictator". These sympathies were the subject of much critical discussion during van Vogt's career, and afterwards.

Van Vogt published "Enchanted Village" in the July 1950 issue of "Other Worlds Science Stories". It was reprinted in over 20 collections or anthologies, and appeared many times in translation.

In 1950, van Vogt was briefly appointed as head of L. Ron Hubbard's Dianetics operation in California. Van Vogt had first met Hubbard in 1945, and became interested in his theories, which were published shortly thereafter. Dianetics was the secular precursor to Hubbard's Church of Scientology; van Vogt would have no association with Scientology, as he did not approve of its mysticism.

The California Dianetics operation went broke nine months later, but never went bankrupt, due to van Vogt's arrangements with creditors. Shortly afterward, van Vogt and his wife opened their own Dianetics center, partly financed by his writings, until he "signed off" around 1961. From 1951 until 1961, van Vogt's focus was on Dianetics, and no new story ideas flowed from his typewriter.

However, during the 1950s, van Vogt retrospectively patched together many of his previously published stories into novels, sometimes creating new interstitial material to help bridge gaps in the narrative. Van Vogt referred to the resulting books as "fix-ups", a term that entered the vocabulary of science-fiction criticism. When the original stories were closely related this was often successful, although some van Vogt fix-ups featured disparate stories thrown together that bore little relation to each other, generally making for a less coherent plot. One of his best-known (and well-regarded) novels, "The Voyage of the Space Beagle" (1950) was a fix-up of four short stories including "Discord in Scarlet"; it was published in at least five European languages by 1955.

Although Van Vogt averaged a new book title every ten months from 1951 to 1961, none of them were entirely new content; they were all fix-ups, collections of previously published stories, expansions of previously published short stories to novel length, or republications of previous books under new titles and all based on story material written and originally published between 1939 and 1950. Examples include "The Weapon Shops of Isher" (1951), "The Mixed Men" (1952), "The War Against the Rull" (1959), and the two "Clane" novels, "Empire of the Atom" (1957) and "The Wizard of Linn" (1962), which were inspired (like Asimov's Foundation series) by Roman imperial history; specifically, as Damon Knight wrote, the plot of "Empire of the Atom" was "lifted almost bodily" from that of Robert Graves' "I, Claudius". (Also, one non-fiction work, "The Hypnotism Handbook", appeared in 1956, though it had apparently been written much earlier.)

After more than a decade of running their Dianetics center, Hull and van Vogt closed it in 1961. Nevertheless, van Vogt maintained his association with the organization and was still president of the Californian Association of Dianetic Auditors into the 1980s.

Though the constant re-packaging of his older work meant that he had never really been away from the book publishing world, van Vogt had not published any wholly new fiction for almost 12 years when he decided to return to writing in 1962. He did not return immediately to science fiction, but instead wrote the only mainstream, non-sf novel of his career.
Van Vogt was profoundly affected by revelations of totalitarian police states that emerged after World War II. Accordingly, he wrote a mainstream novel that he set in Communist China, "The Violent Man" (1962). Van Vogt explained that to research this book he had read 100 books about China. Into this book he incorporated his view of "the violent male type", which he described as a "man who had to be right", a man who "instantly attracts women" and who he said were the men who "run the world". Contemporary reviews were lukewarm at best, and van Vogt thereafter returned to science fiction.

From 1963 through the mid-1980s, van Vogt once again published new material on a regular basis, though fix-ups and reworked material also appeared relatively often. His later novels included fix-ups such as "The Beast" (also known as "Moonbeast") (1963), "Rogue Ship" (1965), "Quest for the Future" (1970) and "Supermind" (1977). He also wrote novels by expanding previously published short stories; works of this type include "The Darkness on Diamondia" (1972) and "Future Glitter" (also known as "Tyranopolis"; 1973).

Novels that were written simply as novels, and not serialized magazine pieces or fix-ups, had been very rare in van Vogt's oeuvre, but began to appear regularly beginning in the 1970s. Van Vogt's original novels included "Children of Tomorrow" (1970), "The Battle of Forever" (1971) and "The Anarchistic Colossus" (1977). Over the years, many sequels to his classic works were promised, but only one appeared: "Null-A Three" (1984; originally published in French). Several later books were initially published in Europe, and at least one novel only ever appeared in foreign language editions and was never published in its original English.

When the 1979 film "Alien" appeared, it was noted that the plot closely matched the plots of both "Black Destroyer" and "Discord in Scarlet", both published in "Astounding magazine" in 1939, and then later published in the 1950 book "Voyage of the Space Beagle". Van Vogt sued the production company for plagiarism, and eventually collected an out-of-court settlement of $50,000 from 20th Century Fox. 

In increasingly frail health, van Vogt published his final short story in 1986.

Van Vogt's first wife, Edna Mayne Hull, died in 1975. Van Vogt married Lydia Bereginsky in 1979; they remained together until his death.

On January 26, 2000, A. E. van Vogt died in Los Angeles from Alzheimer's disease. He was survived by his second wife.

Critical opinion about the quality of van Vogt's work is sharply divided. An early and articulate critic was Damon Knight. In a 1945 chapter-long essay reprinted in "In Search of Wonder," entitled "Cosmic Jerrybuilder: A. E. van Vogt", Knight described van Vogt as "no giant; he is a pygmy who has learned to operate an overgrown typewriter". Knight described "The World of Null-A" as "one of the worst allegedly adult science fiction stories ever published". Concerning van Vogt's writing, Knight said:

About "Empire of the Atom" Knight wrote:

Knight also expressed misgivings about van Vogt's politics. He noted that van Vogt's stories almost invariably present absolute monarchy in a favorable light. In 1974, Knight retracted some of his criticism after finding out about Vogt's writing down his dreams as a part of his working methods:

Knight's criticism greatly damaged van Vogt's reputation. On the other hand, when science fiction author Philip K. Dick was asked which science fiction writers had influenced his work the most, he replied:

Dick also defended van Vogt against Damon Knight's criticisms:

In a review of "Transfinite: The Essential A. E. van Vogt", science fiction writer Paul Di Filippo said:

In "The John W. Campbell Letters", Campbell says, "The son-of-a-gun gets hold of you in the first paragraph, ties a knot around you, and keeps it tied in every paragraph thereafter—including the ultimate last one".

Harlan Ellison (who had begun reading van Vogt as a teenager) wrote, "Van was the first writer to shine light on the restricted ways in which I had been taught to view the universe and the human condition".

Writing in 1984, David Hartwell said:

The literary critic Leslie A. Fiedler said something similar:

American literary critic Fredric Jameson says of van Vogt:

Van Vogt still has his critics. For example, Darrell Schweitzer, writing to "The New York Review of Science Fiction" in 1999, quoted a passage from the original van Vogt novelette "The Mixed Men", which he was then reading, and remarked:

In 1946, van Vogt and his first wife, Edna Mayne Hull, were Guests of Honor at the fourth World Science Fiction Convention.

In 1980, van Vogt received a "Casper Award" (precursor to the Canadian Prix Aurora Awards) for Lifetime Achievement.

The Science Fiction Writers of America (SFWA) named him its 14th Grand Master in 1995 (presented 1996). Great controversy within SFWA accompanied its long wait in bestowing its highest honor (limited to living writers, no more than one annually). Writing an obituary of van Vogt, Robert J. Sawyer, a fellow Canadian writer of science fiction, remarked:

It is generally held that a key factor in the delay was "damnable SFWA politics" reflecting the concerns of Damon Knight, the founder of the SFWA, who abhorred van Vogt's style and politics and thoroughly demolished his literary reputation in the 1950s.

Harlan Ellison was more explicit in 1999 introduction to "Futures Past: The Best Short Fiction of A. E. van Vogt":

In 1996, van Vogt received a Special Award from the World Science Fiction Convention "for six decades of golden age science fiction". That same year, the Science Fiction and Fantasy Hall of Fame inducted him in its inaugural class of two deceased and two living persons, along with writer Jack Williamson (also living) and editors Hugo Gernsback and John W. Campbell.

The works of van Vogt were translated into French by the surrealist Boris Vian ("The World of Null-A" as "Le Monde des Å" in 1958), and van Vogt's works were "viewed as great literature of the surrealist school". In addition, "Slan" was published in French, translated by Jean Rosenthal, under the title "À la poursuite des Slans", as part of the paperback series 'Editions J'ai Lu: Romans-Texte Integral' in 1973. This edition also listing the following works by van Vogt as having been published in French as part of this series: "Le Monde des Å", "La faune de l'espace", "Les joueurs du Å", "L'empire de l'atome", "Le sorcier de Linn", "Les armureries d'Isher", "Les fabricants d'armes", and "Le livre de Ptath". Van Vogt's last novel, 1985's "To Conquer Kiber", has only been released in French (as "À la conquête de Kiber".)






Anna Kournikova

Anna Sergeyevna Kournikova (; born 7 June 1981) is a Russian former professional tennis player and American television personality. Her appearance and celebrity status made her one of the best known tennis stars worldwide. At the peak of her fame, fans looking for images of Kournikova made her name one of the most common search strings on Google Search.

Despite never winning a singles title, she reached No. 8 in the world in 2000. She achieved greater success playing doubles, where she was at times the world No. 1 player. With Martina Hingis as her partner, she won Grand Slam titles in Australia in 1999 and 2002, and the WTA Championships in 1999 and 2000. They referred to themselves as the "Spice Girls of Tennis".

Kournikova retired from professional tennis in 2003 due to serious back and spinal problems, including a herniated disk. She lives in Miami Beach, Florida, and played in occasional exhibitions and in doubles for the St. Louis Aces of World Team Tennis before the team folded in 2011. She was a new trainer for season 12 of the television show "The Biggest Loser", replacing Jillian Michaels, but did not return for season 13. In addition to her tennis and television work, Kournikova serves as a Global Ambassador for Population Services International's "Five & Alive" program, which addresses health crises facing children under the age of five and their families.

Kournikova was born in Moscow, Russia, on 7 June 1981. Her father, Sergei Kournikov (born 1961), a former Greco-Roman wrestling champion, eventually earned a PhD and was a professor at the University of Physical Culture and Sport in Moscow. As of 2001, he was still a part-time martial arts instructor there. Her mother Alla (born 1963) had been a 400-metre runner. Her younger half-brother, Allan, is a youth golf world champion who was featured in the 2013 documentary film "The Short Game".

Sergei Kournikov has said, "We were young and we liked the clean, physical life, so Anna was in a good environment for sport from the beginning".

Kournikova received her first tennis racquet as a New Year gift in 1986 at the age of five. Describing her early regimen, she said, "I played two times a week from age six. It was a children's program. And it was just for fun; my parents didn't know I was going to play professionally, they just wanted me to do something because I had lots of energy. It was only when I started playing well at seven that I went to a professional academy. I would go to school, and then my parents would take me to the club, and I'd spend the rest of the day there just having fun with the kids." In 1986, Kournikova became a member of the Spartak Tennis Club, coached by Larissa Preobrazhenskaya. In 1989, at the age of eight, Kournikova began appearing in junior tournaments, and by the following year, was attracting attention from tennis scouts across the world. She signed a management deal at age ten and went to Bradenton, Florida, to train at Nick Bollettieri's celebrated tennis academy.

Following her arrival in the United States, she became prominent on the tennis scene. At the age of 14, she won the European Championships and the Italian Open Junior tournament. In December 1995, she became the youngest player to win the 18-and-under division of the Junior Orange Bowl tennis tournament. By the end of the year, Kournikova was crowned the ITF Junior World Champion U-18 and Junior European Champion U-18.

Earlier, in September 1995, Kournikova, still only 14 years of age, debuted in the WTA Tour, when she received a wildcard into the qualifications at the WTA tournament in Moscow, the Moscow Ladies Open, and qualified before losing in the second round of the main draw to third-seeded Sabine Appelmans. She also reached her first WTA Tour doubles final in that debut appearance — partnering with 1995 Wimbledon girls' champion in both singles and doubles Aleksandra Olsza, she lost the title match to Meredith McGrath and Larisa Savchenko-Neiland.

In February–March 1996, Kournikova won two ITF titles, in Midland, Michigan and Rockford, Illinois. Still only 14 years of age, in April 1996 she debuted at the Fed Cup for Russia, the youngest player ever to participate and win a match.

In 1996, she started playing under a new coach, Ed Nagel. Her six-year association with Nagel was successful. At 15, she made her Grand Slam debut, reaching the fourth round of the 1996 US Open, losing to Steffi Graf, the eventual champion. After this tournament, Kournikova's ranking jumped from No. 144 to debut in the Top 100 at No. 69. Kournikova was a member of the Russian delegation to the 1996 Olympic Games in Atlanta, Georgia. In 1996, she was named WTA Newcomer of the Year, and she was ranked No. 57 in the end of the season.

Kournikova entered the 1997 Australian Open as world No. 67, where she lost in the first round to world No. 12, Amanda Coetzer. At the Italian Open, Kournikova lost to Amanda Coetzer in the second round. She reached the semi-finals in the doubles partnering with Elena Likhovtseva, before losing to the sixth seeds Mary Joe Fernández and Patricia Tarabini.

At the French Open, Kournikova made it to the third round before losing to world No. 1, Martina Hingis. She also reached the third round in doubles with Likhovtseva. At the Wimbledon Championships, Kournikova became only the second woman in the open era to reach the semi-finals in her Wimbledon debut, the first being Chris Evert in 1972. There she lost to eventual champion Martina Hingis.

At the US Open, she lost in the second round to the eleventh seed Irina Spîrlea. Partnering with Likhovtseva, she reached the third round of the women's doubles event. Kournikova played her last WTA Tour event of 1997 at Porsche Tennis Grand Prix in Filderstadt, losing to Amanda Coetzer in the second round of singles, and in the first round of doubles to Lindsay Davenport and Jana Novotná partnering with Likhovtseva. She broke into the top 50 on 19 May, and was ranked No. 32 in singles and No. 41 in doubles at the end of the season.

In 1998, Kournikova broke into the WTA's top 20 rankings for the first time, when she was ranked No. 16. At the Australian Open, Kournikova lost in the third round to world No. 1 player, Martina Hingis. She also partnered with Larisa Savchenko-Neiland in women's doubles, and they lost to eventual champions Hingis and Mirjana Lučić in the second round. Although she lost in the second round of the Paris Open to Anke Huber in singles, Kournikova reached her second doubles WTA Tour final, partnering with Larisa Savchenko-Neiland. They lost to Sabine Appelmans and Miriam Oremans. Kournikova and Savchenko-Neiland reached their second consecutive final at the Linz Open, losing to Alexandra Fusai and Nathalie Tauziat. At the Miami Open, Kournikova reached her first WTA Tour singles final, before losing to Venus Williams in the final.

Kournikova then reached two consecutive quarterfinals, at Amelia Island and the Italian Open, losing respectively to Lindsay Davenport and Martina Hingis. At the German Open, she reached the semi-finals in both singles and doubles, partnering with Larisa Savchenko-Neiland. At the French Open Kournikova had her best result at this tournament, making it to the fourth round before losing to Jana Novotná. She also reached her first Grand Slam doubles semi-finals, losing with Savchenko-Neiland to Lindsay Davenport and Natasha Zvereva. During her quarterfinals match at the grass-court Eastbourne Open versus Steffi Graf, Kournikova injured her thumb, which would eventually force her to withdraw from the 1998 Wimbledon Championships. However, she won that match, but then withdrew from her semi-finals match against Arantxa Sánchez Vicario. Kournikova returned for the Du Maurier Open and made it to the third round, before losing to Conchita Martínez. At the US Open Kournikova reached the fourth round before losing to Arantxa Sánchez Vicario. Her strong year qualified her for the year-end 1998 WTA Tour Championships, but she lost to Monica Seles in the first round. However, with Seles, she won her first WTA doubles title, in Tokyo, beating Mary Joe Fernández and Arantxa Sánchez Vicario in the final. At the end of the season, she was ranked No. 10 in doubles.

At the start of the 1999 season, Kournikova advanced to the fourth round in singles at the Australian Open before losing to Mary Pierce. In the doubles Kournikova won her first Grand Slam title, partnering with Martina Hingis to defeat Lindsay Davenport and Natasha Zvereva in the final. At the Tier I Family Circle Cup, Kournikova reached her second WTA Tour final, but lost to Martina Hingis. She then defeated Jennifer Capriati, Lindsay Davenport and Patty Schnyder on her route to the Bausch & Lomb Championships semi-finals, losing to Ruxandra Dragomir. At The French Open, Kournikova reached the fourth round before losing to eventual champion Steffi Graf. Once the grass-court season commenced in England, Kournikova lost to Nathalie Tauziat in the semi-finals in Eastbourne. At Wimbledon, Kournikova lost to Venus Williams in the fourth round. She also reached the final in mixed doubles, partnering with Jonas Björkman, but they lost to Leander Paes and Lisa Raymond. Kournikova again qualified for year-end WTA Tour Championships, but lost to Mary Pierce in the first round, and ended the season as World No. 12.
While Kournikova had a successful singles season, she was even more successful in doubles. After their victory at the Australian Open, she and Martina Hingis won tournaments in Indian Wells, Rome, Eastbourne and the WTA Tour Championships, and reached the final of The French Open where they lost to Serena and Venus Williams. Partnering with Elena Likhovtseva, Kournikova also reached the final in Stanford. On 22 November 1999 she reached the world No. 1 ranking in doubles, and ended the season at this ranking. Kournikova and Hingis were presented with the WTA Award for Doubles Team of the Year.

Kournikova opened her 2000 season winning the Gold Coast Open doubles tournament partnering with Julie Halard. She then reached the singles semi-finals at the Medibank International Sydney, losing to Lindsay Davenport. At the Australian Open, she reached the fourth round in singles and the semi-finals in doubles. That season, Kournikova reached eight semi-finals (Sydney, Scottsdale, Stanford, San Diego, Luxembourg, Leipzig and Tour Championships), seven quarterfinals (Gold Coast, Tokyo, Amelia Island, Hamburg, Eastbourne, Zürich and Philadelphia) and one final. On 20 November 2000 she broke into top 10 for the first time, reaching No. 8. She was also ranked No. 4 in doubles at the end of the season. Kournikova was once again, more successful in doubles. She reached the final of the US Open in mixed doubles, partnering with Max Mirnyi, but they lost to Jared Palmer and Arantxa Sánchez Vicario. She also won six doubles titles – Gold Coast (with Julie Halard), Hamburg (with Natasha Zvereva), Filderstadt, Zürich, Philadelphia and the Tour Championships (with Martina Hingis).

Her 2001 season was plagued by injuries, including a left foot stress fracture which made her withdraw from 12 tournaments, including the French Open and Wimbledon. She underwent surgery in April. She reached her second career grand slam quarterfinals, at the Australian Open. Kournikova then withdrew from several events due to continuing problems with her left foot and did not return until Leipzig. With Barbara Schett, she won the doubles title in Sydney. She then lost in the finals in Tokyo, partnering with Iroda Tulyaganova, and at San Diego, partnering with Martina Hingis. Hingis and Kournikova also won the Kremlin Cup. At the end of the 2001 season, she was ranked No. 74 in singles and No. 26 in doubles.
Kournikova regained some success in 2002. She reached the semi-finals of Auckland, Tokyo, Acapulco and San Diego, and the final of the China Open, losing to Anna Smashnova. This was Kournikova's last singles final. With Martina Hingis, she lost in the final at Sydney, but they won their second Grand Slam title together, the Australian Open. They also lost in the quarterfinals of the US Open. With Chanda Rubin, Kournikova played the semi-finals of Wimbledon, but they lost to Serena and Venus Williams. Partnering with Janet Lee, she won the Shanghai title. At the end of 2002 season, she was ranked No. 35 in singles and No. 11 in doubles.

In 2003, Anna Kournikova achieved her first Grand Slam match victory in two years at the Australian Open. She defeated Henrieta Nagyová in the first round, and then lost to Justine Henin-Hardenne in the 2nd round. She withdrew from Tokyo due to a sprained back suffered at the Australian Open and did not return to Tour until Miami. On 9 April, in what would be the final WTA match of her career, Kournikova dropped out in the first round of the Family Circle Cup in Charleston, due to a left adductor strain. Her singles world ranking was 67. She reached the semi-finals at the ITF tournament in Sea Island, before withdrawing from a match versus Maria Sharapova due to the adductor injury. She lost in the first round of the ITF tournament in Charlottesville. She did not compete for the rest of the season due to a continuing back injury. At the end of the 2003 season and her professional career, she was ranked No. 305 in singles and No. 176 in doubles.

Kournikova's two Grand Slam doubles titles came in 1999 and 2002, both at the Australian Open in the Women's Doubles event with partner Martina Hingis. Kournikova proved a successful doubles player on the professional circuit, winning 16 tournament doubles titles, including two Australian Opens and being a finalist in mixed doubles at the US Open and at Wimbledon, and reaching the No. 1 ranking in doubles in the WTA Tour rankings. Her pro career doubles record was 200–71. However, her singles career plateaued after 1999. For the most part, she managed to retain her ranking between 10 and 15 (her career high singles ranking was No.8), but her expected finals breakthrough failed to occur; she only reached four finals out of 130 singles tournaments, never in a Grand Slam event, and never won one.

Her singles record is 209–129. Her final playing years were marred by a string of injuries, especially back injuries, which caused her ranking to erode gradually. As a personality Kournikova was among the most common search strings for both articles and images in her prime.

Kournikova has not played on the WTA Tour since 2003, but still plays exhibition matches for charitable causes. In late 2004, she participated in three events organized by Elton John and by fellow tennis players Serena Williams and Andy Roddick. In January 2005, she played in a doubles charity event for the Indian Ocean tsunami with John McEnroe, Andy Roddick, and Chris Evert. In November 2005, she teamed up with Martina Hingis, playing against Lisa Raymond and Samantha Stosur in the WTT finals for charity. Kournikova is also a member of the St. Louis Aces in the World Team Tennis (WTT), playing doubles only.

In September 2008, Kournikova showed up for the 2008 Nautica Malibu Triathlon held at Zuma Beach in Malibu, California. The Race raised funds for children's Hospital Los Angeles. She won that race for women's K-Swiss team. On 27 September 2008, Kournikova played exhibition mixed doubles matches in Charlotte, North Carolina, partnering with Tim Wilkison and Karel Nováček. Kournikova and Wilkison defeated Jimmy Arias and Chanda Rubin, and then Kournikova and Novacek defeated Rubin and Wilkison.

On 12 October 2008, Anna Kournikova played one exhibition match for the annual charity event, hosted by Billie Jean King and Elton John, and raised more than $400,000 for the Elton John AIDS Foundation and Atlanta AIDS Partnership Fund. She played doubles with Andy Roddick (they were coached by David Chang) versus Martina Navratilova and Jesse Levine (coached by Billie Jean King); Kournikova and Roddick won.

Kournikova was one of "four former world No. 1 players" who participated in "Legendary Night", held on 2 May 2009, at the Turning Stone Event Center in Verona, New York, the others being John McEnroe (who had been No. 1 in both singles and doubles), Tracy Austin and Jim Courier (both of whom who had been No. 1 in singles but not doubles). The exhibition included a mixed doubles match in which McEnroe and Kournikova defeated Courier and Austin.

In 2008, she was named a spokesperson for K-Swiss. In 2005, Kournikova stated that if she were 100% fit, she would like to come back and compete again.

In June 2010, Kournikova reunited with her doubles partner Martina Hingis to participate in competitive tennis for the first time in seven years in the Invitational Ladies Doubles event at Wimbledon. On 29 June 2010 they defeated the British pair Samantha Smith and Anne Hobbs.

Kournikova plays right-handed with a two-handed backhand. She is a great player at the net. She can hit forceful groundstrokes and also drop shots.

Her playing style fits the profile for a doubles player, and is complemented by her height. She has been compared to such doubles specialists as Pam Shriver and Peter Fleming.

Kournikova was in a relationship with fellow Russian, Pavel Bure, an NHL ice hockey player. The two met in 1999, when Kournikova was still linked to Bure's former Russian teammate Sergei Fedorov. Bure and Kournikova were reported to have been engaged in 2000 after a reporter took a photo of them together in a Florida restaurant where Bure supposedly asked Kournikova to marry him. As the story made headlines in Russia, where they were both heavily followed in the media as celebrities, Bure and Kournikova both denied any engagement. Kournikova, 10 years younger than Bure, was 18 years old at the time.

Fedorov claimed that he and Kournikova were married in 2001, and divorced in 2003. Kournikova's representatives deny any marriage to Fedorov; however, Fedorov's agent Pat Brisson claims that although he does not know when they got married, he knew "Fedorov was married".

Kournikova started dating singer Enrique Iglesias in late 2001 after she had appeared in his music video for "Escape". The couple have three children together, fraternal twins, a son and daughter, born on 16 December 2017, and another daughter born on 30 January 2020.

It was reported in 2010 that Kournikova had become an American citizen.

In 2000, Kournikova became the new face for Berlei's shock absorber sports bras, and appeared in the "only the ball should bounce" billboard campaign. Following that, she was cast by the Farrelly brothers for a minor role in the 2000 film "Me, Myself & Irene" starring Jim Carrey and Renée Zellweger. Photographs of her have appeared on covers of various publications, including men's magazines, such as one in the much-publicized 2004 "Sports Illustrated Swimsuit Issue", where she posed in bikinis and swimsuits, as well as in "FHM" and "Maxim".

Kournikova was named one of "People"s 50 Most Beautiful People in 1998 and was voted "hottest female athlete" on ESPN.com. In 2002, she also placed first in "FHM's 100 Sexiest Women in the World" in US and UK editions. By contrast, ESPN – citing the degree of hype as compared to actual accomplishments as a singles player – ranked Kournikova 18th in its "25 Biggest Sports Flops of the Past 25 Years". Kournikova was also ranked No. 1 in the ESPN Classic series "Who's number 1?" when the series featured sport's most overrated athletes.

She continued to be the most searched athlete on the Internet through 2008 even though she had retired from the professional tennis circuit years earlier. After slipping from first to sixth among athletes in 2009, she moved back up to third place among athletes in terms of search popularity in 2010.

In October 2010, Kournikova headed to NBC's "The Biggest Loser" where she led the contestants in a tennis-workout challenge. In May 2011, it was announced that Kournikova would join "The Biggest Loser" as a regular celebrity trainer in season 12. She did not return for season 13.





 

Alfons Maria Jakob

Alfons Maria Jakob (2 July 1884 – 17 October 1931) was a German neurologist who worked in the field of neuropathology.

He was born in Aschaffenburg, Bavaria and educated in medicine at the universities of Munich, Berlin, and Strasbourg, where he received his doctorate in 1908. During the following year, he began clinical work under the psychiatrist Emil Kraepelin and did laboratory work with Franz Nissl and Alois Alzheimer in Munich.

In 1911, by way of an invitation from Wilhelm Weygandt, he relocated to Hamburg, where he worked with Theodor Kaes and eventually became head of the laboratory of anatomical pathology at the psychiatric State Hospital Hamburg-Friedrichsberg. Following the death of Kaes in 1913, Jakob succeeded him as prosector. During World War I he served as an army physician in Belgium, and afterwards returned to Hamburg. In 1919, he obtained his habilitation for neurology and in 1924 became a professor of neurology. Under Jakob's guidance the department grew rapidly. He made significant contributions to knowledge on concussion and secondary nerve degeneration and became a doyen of neuropathology.

Jakob was the author of five monographs and nearly 80 scientific papers. His neuropathological research contributed greatly to the delineation of several diseases, including multiple sclerosis and Friedreich's ataxia. He first recognised and described Alper's disease and Creutzfeldt–Jakob disease (named along with Munich neuropathologist Hans Gerhard Creutzfeldt). He gained experience in neurosyphilis, having a 200-bed ward devoted entirely to that disorder. Jakob made a lecture tour of the United States (1924) and South America (1928), of which, he wrote a paper on the neuropathology of yellow fever.

He suffered from chronic osteomyelitis for the last seven years of his life. This eventually caused a retroperitoneal abscess and paralytic ileus from which he died following operation.



Agnosticism

Agnosticism is the view or belief that the existence of God, of the divine or the supernatural is unknown or unknowable. Another definition provided is the view that "human reason is incapable of providing sufficient rational grounds to justify either the belief that God exists or the belief that God does not exist."

The English biologist Thomas Henry Huxley coined the word "agnostic" in 1869, and said "It simply means that a man shall not say he knows or believes that which he has no scientific grounds for professing to know or believe." Earlier thinkers, however, had written works that promoted agnostic points of view, such as Sanjaya Belatthiputta, a 5th-century BCE Indian philosopher who expressed agnosticism about any afterlife; and Protagoras, a 5th-century BCE Greek philosopher who expressed agnosticism about the existence of "the gods".

Being a scientist, above all else, Huxley presented agnosticism as a form of demarcation. A hypothesis with no supporting, objective, testable evidence is not an objective, scientific claim. As such, there would be no way to test said hypotheses, leaving the results inconclusive. His agnosticism was not compatible with forming a belief as to the truth, or falsehood, of the claim at hand. Karl Popper would also describe himself as an agnostic. According to philosopher William L. Rowe, in this strict sense, agnosticism is the view that human reason is incapable of providing sufficient rational grounds to justify either the belief that God exists or the belief that God does not exist.

George H. Smith, while admitting that the narrow definition of atheist was the common usage definition of that word, and admitting that the broad definition of agnostic was the common usage definition of that word, promoted broadening the definition of atheist and narrowing the definition of agnostic. When properly consider, Smith rejects agnosticism as a third alternative, as the term agnostic does not, in itself, indicate whether or not one believes in a god. Instead of theism and atheism, Alexander J. Harrison promotes terms such as agnostic atheism (the view of those who do not hold a belief in the existence of any deity, but claim that the existence of a deity is unknown or inherently unknowable) and agnostic theism (the view of those who believe in the existence of a deity(s), but claim that the existence of a deity is unknown or inherently unknowable).

"Agnostic" () was used by Thomas Henry Huxley in a speech at a meeting of the Metaphysical Society in 1869 to describe his philosophy, which rejects all claims of spiritual or mystical knowledge.

Early Christian church leaders used the Greek word "gnosis" (knowledge) to describe "spiritual knowledge". Agnosticism is not to be confused with religious views opposing the ancient religious movement of Gnosticism in particular; Huxley used the term in a broader, more abstract sense. Huxley identified agnosticism not as a creed but rather as a method of skeptical, evidence-based inquiry.

The term "Agnostic" is also cognate with the Sanskrit word "Ajñasi" which translates literally to "not knowable", and relates to the ancient Indian philosophical school of Ajñana, which proposes that it is impossible to obtain knowledge of metaphysical nature or ascertain the truth value of philosophical propositions; and even if knowledge was possible, it is useless and disadvantageous for final salvation.

In recent years, scientific literature dealing with neuroscience and psychology has used the word to mean "not knowable". In technical and marketing literature, "agnostic" can also mean independence from some parameters—for example, "platform agnostic" (referring to cross-platform software)
or "hardware-agnostic".

Scottish Enlightenment philosopher David Hume contended that meaningful statements about the universe are always qualified by some degree of doubt. He asserted that the fallibility of human beings means that they cannot obtain absolute certainty except in trivial cases where a statement is true by definition (e.g. tautologies such as "all bachelors are unmarried" or "all triangles have three corners").


Throughout the history of Hinduism there has been a strong tradition of philosophic speculation and skepticism.

The Rig Veda takes an agnostic view on the fundamental question of how the universe and the gods were created. Nasadiya Sukta ("Creation Hymn") in the tenth chapter of the Rig Veda says:
Aristotle,
Anselm,
Aquinas,
Descartes,
and Gödel
presented arguments attempting to rationally prove the existence of God. The skeptical empiricism of David Hume, the antinomies of Immanuel Kant, and the existential philosophy of Søren Kierkegaard convinced many later philosophers to abandon these attempts, regarding it impossible to construct any unassailable proof for the existence or non-existence of God.

In his 1844 book, "Philosophical Fragments", Kierkegaard writes:
Hume was Huxley's favourite philosopher, calling him "the Prince of Agnostics". Diderot wrote to his mistress, telling of a visit by Hume to the Baron D'Holbach, and describing how a word for the position that Huxley would later describe as agnosticism did not seem to exist, or at least was not common knowledge, at the time.

Raised in a religious environment, Charles Darwin (1809–1882) studied to be an Anglican clergyman. While eventually doubting parts of his faith, Darwin continued to help in church affairs, even while avoiding church attendance. Darwin stated that it would be "absurd to doubt that a man might be an ardent theist and an evolutionist". Although reticent about his religious views, in 1879 he wrote that "I have never been an atheist in the sense of denying the existence of a God. – I think that generally ... an agnostic would be the most correct description of my state of mind."

Agnostic views are as old as philosophical skepticism, but the terms agnostic and agnosticism were created by Huxley (1825–1895) to sum up his thoughts on contemporary developments of metaphysics about the "unconditioned" (William Hamilton) and the "unknowable" (Herbert Spencer). Though Huxley began to use the term "agnostic" in 1869, his opinions had taken shape some time before that date. In a letter of September 23, 1860, to Charles Kingsley, Huxley discussed his views extensively:
And again, to the same correspondent, May 6, 1863:
Of the origin of the name agnostic to describe this attitude, Huxley gave the following account:
In 1889, Huxley wrote:

William Stewart Ross (1844–1906) wrote under the name of Saladin. He was associated with Victorian Freethinkers and the organization the British Secular Union. He edited the "Secular Review" from 1882; it was renamed "Agnostic Journal and Eclectic Review" and closed in 1907. Ross championed agnosticism in opposition to the atheism of Charles Bradlaugh as an open-ended spiritual exploration.

In "Why I am an Agnostic" () he claims that agnosticism is "the very reverse of atheism".

Bertrand Russell (1872–1970) declared "Why I Am Not a Christian" in 1927, a classic statement of agnosticism.
He calls upon his readers to "stand on their own two feet and look fair and square at the world with a fearless attitude and a free intelligence".

In 1939, Russell gave a lecture on "The existence and nature of God", in which he characterized himself as an atheist. He said:
However, later in the same lecture, discussing modern non-anthropomorphic concepts of God, Russell states:
In Russell's 1947 pamphlet, "Am I An Atheist or an Agnostic?" (subtitled "A Plea For Tolerance in the Face of New Dogmas"), he ruminates on the problem of what to call himself:
In his 1953 essay, "What Is An Agnostic?" Russell states:
Later in the essay, Russell adds:
In 1965, Christian theologian Leslie Weatherhead (1893–1976) published "The Christian Agnostic", in which he argues:
Although radical and unpalatable to conventional theologians, Weatherhead's "agnosticism" falls far short of Huxley's, and short even of "weak agnosticism":

Robert G. Ingersoll (1833–1899), an Illinois lawyer and politician who evolved into a well-known and sought-after orator in 19th-century America, has been referred to as the "Great Agnostic".

In an 1896 lecture titled "Why I Am An Agnostic", Ingersoll related why he was an agnostic:
In the conclusion of the speech he simply sums up the agnostic position as:
In 1885, Ingersoll explained his comparative view of agnosticism and atheism as follows:

Canon Bernard Iddings Bell (1886–1958), a popular cultural commentator, Episcopal priest, and author, lauded the necessity of agnosticism in "Beyond Agnosticism: A Book for Tired Mechanists", calling it the foundation of "all intelligent Christianity". Agnosticism was a temporary mindset in which one rigorously questioned the truths of the age, including the way in which one believed God. His view of Robert Ingersoll and Thomas Paine was that they were not denouncing true Christianity but rather "a gross perversion of it". Part of the misunderstanding stemmed from ignorance of the concepts of God and religion. Historically, a god was any real, perceivable force that ruled the lives of humans and inspired admiration, love, fear, and homage; religion was the practice of it. Ancient peoples worshiped gods with real counterparts, such as Mammon (money and material things), Nabu (rationality), or Ba'al (violent weather); Bell argued that modern peoples were still paying homage—with their lives and their children's lives—to these old gods of wealth, physical appetites, and self-deification. Thus, if one attempted to be agnostic passively, he or she would incidentally join the worship of the world's gods.

In "Unfashionable Convictions" (1931), he criticized the Enlightenment's complete faith in human sensory perception, augmented by scientific instruments, as a means of accurately grasping Reality. Firstly, it was fairly new, an innovation of the Western World, which Aristotle invented and Thomas Aquinas revived among the scientific community. Secondly, the divorce of "pure" science from human experience, as manifested in American Industrialization, had completely altered the environment, often disfiguring it, so as to suggest its insufficiency to human needs. Thirdly, because scientists were constantly producing more data—to the point where no single human could grasp it all at once—it followed that human intelligence was incapable of attaining a complete understanding of universe; therefore, to admit the mysteries of the unobserved universe was to be "actually" scientific.

Bell believed that there were two other ways that humans could perceive and interact with the world. "Artistic experience" was how one expressed meaning through speaking, writing, painting, gesturing—any sort of communication which shared insight into a human's inner reality. "Mystical experience" was how one could "read" people and harmonize with them, being what we commonly call love. In summary, man was a scientist, artist, and lover. Without exercising all three, a person became "lopsided".

Bell considered a humanist to be a person who cannot rightly ignore the other ways of knowing. However, humanism, like agnosticism, was also temporal, and would eventually lead to either scientific materialism or theism. He lays out the following thesis:


Demographic research services normally do not differentiate between various types of non-religious respondents, so agnostics are often classified in the same category as atheists or other non-religious people.

A 2010 survey published in "Encyclopædia Britannica" found that the non-religious people or the agnostics made up about 9.6% of the world's population.
A November–December 2006 poll published in the "Financial Times" gives rates for the United States and five European countries. The rates of agnosticism in the United States were at 14%, while the rates of agnosticism in the European countries surveyed were considerably higher: Italy (20%), Spain (30%), Great Britain (35%), Germany (25%), and France (32%).

A study conducted by the Pew Research Center found that about 16% of the world's people, the third largest group after Christianity and Islam, have no religious affiliation.
According to a 2012 report by the Pew Research Center, agnostics made up 3.3% of the US adult population.
In the "U.S. Religious Landscape Survey", conducted by the Pew Research Center, 55% of agnostic respondents expressed "a belief in God or a universal spirit",
whereas 41% stated that they thought that they felt a tension "being non-religious in a society where most people are religious".

According to the 2021 Australian Bureau of Statistics, 38.9% of Australians have "no religion", a category that includes agnostics.
Between 64% and 65%
of Japanese and up to 81%
of Vietnamese are atheists, agnostics, or do not believe in a god. An official European Union survey reported that 3% of the EU population is unsure about their belief in a god or spirit.

Agnosticism is criticized from a variety of standpoints. Some atheists criticize the use of the term agnosticism as functionally indistinguishable from atheism; this results in frequent criticisms of those who adopt the term as avoiding the atheist label.

Theistic critics claim that agnosticism is impossible in practice, since a person can live only either as if God did not exist ("etsi deus non-daretur"), or as if God did exist ("etsi deus daretur").

According to Pope Benedict XVI, strong agnosticism in particular contradicts itself in affirming the power of reason to know scientific truth. He blames the exclusion of reasoning from religion and ethics for dangerous pathologies such as crimes against humanity and ecological disasters.
"Agnosticism", said Benedict, "is always the fruit of a refusal of that knowledge which is in fact offered to man ... The knowledge of God has always existed". He asserted that agnosticism is a choice of comfort, pride, dominion, and utility over truth, and is opposed by the following attitudes: the keenest self-criticism, humble listening to the whole of existence, the persistent patience and self-correction of the scientific method, a readiness to be purified by the truth.

The Catholic Church sees merit in examining what it calls "partial agnosticism", specifically those systems that "do not aim at constructing a complete philosophy of the unknowable, but at excluding special kinds of truth, notably religious, from the domain of knowledge". However, the Church is historically opposed to a full denial of the capacity of human reason to know God. The Council of the Vatican declares, "God, the beginning and end of all, can, by the natural light of human reason, be known with certainty from the works of creation".

Blaise Pascal argued that even if there were truly no evidence for God, agnostics should consider what is now known as Pascal's Wager: the infinite expected value of acknowledging God is always greater than the finite expected value of not acknowledging his existence, and thus it is a safer "bet" to choose God.

According to Richard Dawkins, a distinction between agnosticism and atheism is unwieldy and depends on how close to zero a person is willing to rate the probability of existence for any given god-like entity. About himself, Dawkins continues, "I am agnostic only to the extent that I am agnostic about fairies at the bottom of the garden." Dawkins also identifies two categories of agnostics; "Temporary Agnostics in Practice" (TAPs), and "Permanent Agnostics in Principle" (PAPs). He states that "agnosticism about the existence of God belongs firmly in the temporary or TAP category. Either he exists or he doesn't. It is a scientific question; one day we may know the answer, and meanwhile we can say something pretty strong about the probability" and considers PAP a "deeply inescapable kind of fence-sitting".

A related concept is ignosticism, the view that a coherent definition of a deity must be put forward before the question of the existence of a deity can be meaningfully discussed. If the chosen definition is not coherent, the ignostic holds the noncognitivist view that the existence of a deity is meaningless or empirically untestable. A. J. Ayer, Theodore Drange, and other philosophers see both atheism and agnosticism as incompatible with ignosticism on the grounds that atheism and agnosticism accept the statement "a deity exists" as a meaningful proposition that can be argued for or against.



Argon

Argon is a chemical element; it has symbol Ar and atomic number 18. It is in group 18 of the periodic table and is a noble gas. Argon is the third most abundant gas in Earth's atmosphere, at 0.934% (9340 ppmv). It is more than twice as abundant as water vapor (which averages about 4000 ppmv, but varies greatly), 23 times as abundant as carbon dioxide (400 ppmv), and more than 500 times as abundant as neon (18 ppmv). Argon is the most abundant noble gas in Earth's crust, comprising 0.00015% of the crust.

Nearly all argon in Earth's atmosphere is radiogenic argon-40, derived from the decay of potassium-40 in Earth's crust. In the universe, argon-36 is by far the most common argon isotope, as it is the most easily produced by stellar nucleosynthesis in supernovas.

The name "argon" is derived from the Greek word , neuter singular form of meaning 'lazy' or 'inactive', as a reference to the fact that the element undergoes almost no chemical reactions. The complete octet (eight electrons) in the outer atomic shell makes argon stable and resistant to bonding with other elements. Its triple point temperature of 83.8058 K is a defining fixed point in the International Temperature Scale of 1990.

Argon is extracted industrially by the fractional distillation of liquid air. It is mostly used as an inert shielding gas in welding and other high-temperature industrial processes where ordinarily unreactive substances become reactive; for example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning. It is also used in incandescent, fluorescent lighting, and other gas-discharge tubes. It makes a distinctive blue-green gas laser. It is also used in fluorescent glow starters.

Argon has approximately the same solubility in water as oxygen and is 2.5 times more soluble in water than nitrogen. Argon is colorless, odorless, nonflammable and nontoxic as a solid, liquid or gas. Argon is chemically inert under most conditions and forms no confirmed stable compounds at room temperature.

Although argon is a noble gas, it can form some compounds under various extreme conditions. Argon fluorohydride (HArF), a compound of argon with fluorine and hydrogen that is stable below , has been demonstrated. Although the neutral ground-state chemical compounds of argon are presently limited to HArF, argon can form clathrates with water when atoms of argon are trapped in a lattice of water molecules. Ions, such as , and excited-state complexes, such as ArF, have been demonstrated. Theoretical calculation predicts several more argon compounds that should be stable but have not yet been synthesized.

"Argon" (Greek , neuter singular form of meaning "lazy" or "inactive") is named in reference to its chemical inactivity. This chemical property of this first noble gas to be discovered impressed the namers. An unreactive gas was suspected to be a component of air by Henry Cavendish in 1785.

Argon was first isolated from air in 1894 by Lord Rayleigh and Sir William Ramsay at University College London by removing oxygen, carbon dioxide, water, and nitrogen from a sample of clean air. They first accomplished this by replicating an experiment of Henry Cavendish's. They trapped a mixture of atmospheric air with additional oxygen in a test-tube (A) upside-down over a large quantity of dilute alkali solution (B), which in Cavendish's original experiment was potassium hydroxide, and conveyed a current through wires insulated by U-shaped glass tubes (CC) which sealed around the platinum wire electrodes, leaving the ends of the wires (DD) exposed to the gas and insulated from the alkali solution. The arc was powered by a battery of five Grove cells and a Ruhmkorff coil of medium size. The alkali absorbed the oxides of nitrogen produced by the arc and also carbon dioxide. They operated the arc until no more reduction of volume of the gas could be seen for at least an hour or two and the spectral lines of nitrogen disappeared when the gas was examined. The remaining oxygen was reacted with alkaline pyrogallate to leave behind an apparently non-reactive gas which they called argon.
Before isolating the gas, they had determined that nitrogen produced from chemical compounds was 0.5% lighter than nitrogen from the atmosphere. The difference was slight, but it was important enough to attract their attention for many months. They concluded that there was another gas in the air mixed in with the nitrogen. Argon was also encountered in 1882 through independent research of H. F. Newall and W. N. Hartley. Each observed new lines in the emission spectrum of air that did not match known elements.

Until 1957, the symbol for argon was "A", but now it is "Ar".

Argon constitutes 0.934% by volume and 1.288% by mass of Earth's atmosphere. Air is the primary industrial source of purified argon products. Argon is isolated from air by fractionation, most commonly by cryogenic fractional distillation, a process that also produces purified nitrogen, oxygen, neon, krypton and xenon. Earth's crust and seawater contain 1.2 ppm and 0.45 ppm of argon, respectively.

The main isotopes of argon found on Earth are (99.6%), (0.34%), and (0.06%). Naturally occurring , with a half-life of 1.25 years, decays to stable (11.2%) by electron capture or positron emission, and also to stable (88.8%) by beta decay. These properties and ratios are used to determine the age of rocks by K–Ar dating.

In Earth's atmosphere, is made by cosmic ray activity, primarily by neutron capture of followed by two-neutron emission. In the subsurface environment, it is also produced through neutron capture by , followed by proton emission. is created from the neutron capture by followed by an alpha particle emission as a result of subsurface nuclear explosions. It has a half-life of 35 days.

Between locations in the Solar System, the isotopic composition of argon varies greatly. Where the major source of argon is the decay of in rocks, will be the dominant isotope, as it is on Earth. Argon produced directly by stellar nucleosynthesis is dominated by the alpha-process nuclide . Correspondingly, solar argon contains 84.6% (according to solar wind measurements), and the ratio of the three isotopes Ar : Ar : Ar in the atmospheres of the outer planets is 8400 : 1600 : 1. This contrasts with the low abundance of primordial in Earth's atmosphere, which is only 31.5 ppmv (= 9340 ppmv × 0.337%), comparable with that of neon (18.18 ppmv) on Earth and with interplanetary gasses, measured by probes.

The atmospheres of Mars, Mercury and Titan (the largest moon of Saturn) contain argon, predominantly as , and its content may be as high as 1.93% (Mars).

The predominance of radiogenic is the reason the standard atomic weight of terrestrial argon is greater than that of the next element, potassium, a fact that was puzzling when argon was discovered. Mendeleev positioned the elements on his periodic table in order of atomic weight, but the inertness of argon suggested a placement "before" the reactive alkali metal. Henry Moseley later solved this problem by showing that the periodic table is actually arranged in order of atomic number (see History of the periodic table).

Argon's complete octet of electrons indicates full s and p subshells. This full valence shell makes argon very stable and extremely resistant to bonding with other elements. Before 1962, argon and the other noble gases were considered to be chemically inert and unable to form compounds; however, compounds of the heavier noble gases have since been synthesized. The first argon compound with tungsten pentacarbonyl, W(CO)Ar, was isolated in 1975. However, it was not widely recognised at that time. In August 2000, another argon compound, argon fluorohydride (HArF), was formed by researchers at the University of Helsinki, by shining ultraviolet light onto frozen argon containing a small amount of hydrogen fluoride with caesium iodide. This discovery caused the recognition that argon could form weakly bound compounds, even though it was not the first. It is stable up to 17 kelvins (−256 °C). The metastable dication, which is valence-isoelectronic with carbonyl fluoride and phosgene, was observed in 2010. Argon-36, in the form of argon hydride (argonium) ions, has been detected in interstellar medium associated with the Crab Nebula supernova; this was the first noble-gas molecule detected in outer space.

Solid argon hydride (Ar(H)) has the same crystal structure as the MgZn Laves phase. It forms at pressures between 4.3 and 220 GPa, though Raman measurements suggest that the H molecules in Ar(H) dissociate above 175 GPa.

Argon is extracted industrially by the fractional distillation of liquid air in a cryogenic air separation unit; a process that separates liquid nitrogen, which boils at 77.3 K, from argon, which boils at 87.3 K, and liquid oxygen, which boils at 90.2 K. About 700,000 tonnes of argon are produced worldwide every year.

Argon has several desirable properties:

Other noble gases would be equally suitable for most of these applications, but argon is by far the cheapest. It is inexpensive, since it occurs naturally in air and is readily obtained as a byproduct of cryogenic air separation in the production of liquid oxygen and liquid nitrogen: the primary constituents of air are used on a large industrial scale. The other noble gases (except helium) are produced this way as well, but argon is the most plentiful by far. The bulk of its applications arise simply because it is inert and relatively cheap.

Argon is used in some high-temperature industrial processes where ordinarily non-reactive substances become reactive. For example, an argon atmosphere is used in graphite electric furnaces to prevent the graphite from burning.

For some of these processes, the presence of nitrogen or oxygen gases might cause defects within the material. Argon is used in some types of arc welding such as gas metal arc welding and gas tungsten arc welding, as well as in the processing of titanium and other reactive elements. An argon atmosphere is also used for growing crystals of silicon and germanium.
Argon is used in the poultry industry to asphyxiate birds, either for mass culling following disease outbreaks, or as a means of slaughter more humane than electric stunning. Argon is denser than air and displaces oxygen close to the ground during inert gas asphyxiation. Its non-reactive nature makes it suitable in a food product, and since it replaces oxygen within the dead bird, argon also enhances shelf life.

Argon is sometimes used for extinguishing fires where valuable equipment may be damaged by water or foam.

Liquid argon is used as the target for neutrino experiments and direct dark matter searches. The interaction between the hypothetical WIMPs and an argon nucleus produces scintillation light that is detected by photomultiplier tubes. Two-phase detectors containing argon gas are used to detect the ionized electrons produced during the WIMP–nucleus scattering. As with most other liquefied noble gases, argon has a high scintillation light yield (about 51 photons/keV), is transparent to its own scintillation light, and is relatively easy to purify. Compared to xenon, argon is cheaper and has a distinct scintillation time profile, which allows the separation of electronic recoils from nuclear recoils. On the other hand, its intrinsic beta-ray background is larger due to contamination, unless one uses argon from underground sources, which has much less contamination. Most of the argon in Earth's atmosphere was produced by electron capture of long-lived ( + e → + ν) present in natural potassium within Earth. The activity in the atmosphere is maintained by cosmogenic production through the knockout reaction (n,2n) and similar reactions. The half-life of is only 269 years. As a result, the underground Ar, shielded by rock and water, has much less contamination. Dark-matter detectors currently operating with liquid argon include DarkSide, WArP, ArDM, microCLEAN and DEAP. Neutrino experiments include ICARUS and MicroBooNE, both of which use high-purity liquid argon in a time projection chamber for fine grained three-dimensional imaging of neutrino interactions.

At Linköping University, Sweden, the inert gas is being utilized in a vacuum chamber in which plasma is introduced to ionize metallic films. This process results in a film usable for manufacturing computer processors. The new process would eliminate the need for chemical baths and use of expensive, dangerous and rare materials.

Argon is used to displace oxygen- and moisture-containing air in packaging material to extend the shelf-lives of the contents (argon has the European food additive code E938). Aerial oxidation, hydrolysis, and other chemical reactions that degrade the products are retarded or prevented entirely. High-purity chemicals and pharmaceuticals are sometimes packed and sealed in argon.

In winemaking, argon is used in a variety of activities to provide a barrier against oxygen at the liquid surface, which can spoil wine by fueling both microbial metabolism (as with acetic acid bacteria) and standard redox chemistry.

Argon is sometimes used as the propellant in aerosol cans.

Argon is also used as a preservative for such products as varnish, polyurethane, and paint, by displacing air to prepare a container for storage.

Since 2002, the American National Archives stores important national documents such as the Declaration of Independence and the Constitution within argon-filled cases to inhibit their degradation. Argon is preferable to the helium that had been used in the preceding five decades, because helium gas escapes through the intermolecular pores in most containers and must be regularly replaced.

Argon may be used as the inert gas within Schlenk lines and gloveboxes. Argon is preferred to less expensive nitrogen in cases where nitrogen may react with the reagents or apparatus.

Argon may be used as the carrier gas in gas chromatography and in electrospray ionization mass spectrometry; it is the gas of choice for the plasma used in ICP spectroscopy. Argon is preferred for the sputter coating of specimens for scanning electron microscopy. Argon gas is also commonly used for sputter deposition of thin films as in microelectronics and for wafer cleaning in microfabrication.

Cryosurgery procedures such as cryoablation use liquid argon to destroy tissue such as cancer cells. It is used in a procedure called "argon-enhanced coagulation", a form of argon plasma beam electrosurgery. The procedure carries a risk of producing gas embolism and has resulted in the death of at least one patient.

Blue argon lasers are used in surgery to weld arteries, destroy tumors, and correct eye defects.

Argon has also been used experimentally to replace nitrogen in the breathing or decompression mix known as Argox, to speed the elimination of dissolved nitrogen from the blood.

Incandescent lights are filled with argon, to preserve the filaments at high temperature from oxidation. It is used for the specific way it ionizes and emits light, such as in plasma globes and calorimetry in experimental particle physics. Gas-discharge lamps filled with pure argon provide lilac/violet light; with argon and some mercury, blue light. Argon is also used for blue and green argon-ion lasers.

Argon is used for thermal insulation in energy-efficient windows. Argon is also used in technical scuba diving to inflate a dry suit because it is inert and has low thermal conductivity.

Argon is used as a propellant in the development of the Variable Specific Impulse Magnetoplasma Rocket (VASIMR). Compressed argon gas is allowed to expand, to cool the seeker heads of some versions of the AIM-9 Sidewinder missile and other missiles that use cooled thermal seeker heads. The gas is stored at high pressure.

Argon-39, with a half-life of 269 years, has been used for a number of applications, primarily ice core and ground water dating. Also, potassium–argon dating and related argon-argon dating are used to date sedimentary, metamorphic, and igneous rocks.

Argon has been used by athletes as a doping agent to simulate hypoxic conditions. In 2014, the World Anti-Doping Agency (WADA) added argon and xenon to the list of prohibited substances and methods, although at this time there is no reliable test for abuse.

Although argon is non-toxic, it is 38% more dense than air and therefore considered a dangerous asphyxiant in closed areas. It is difficult to detect because it is colorless, odorless, and tasteless. A 1994 incident, in which a man was asphyxiated after entering an argon-filled section of oil pipe under construction in Alaska, highlights the dangers of argon tank leakage in confined spaces and emphasizes the need for proper use, storage and handling.




Arsenic

Arsenic is a chemical element; it has symbol As and atomic number 33. Arsenic occurs in many minerals, usually in combination with sulfur and metals, but also as a pure elemental crystal. Arsenic is a notoriously toxic metalloid. It has various allotropes, but only the grey form, which has a metallic appearance, is important to industry.

The primary use of arsenic is in alloys of lead (for example, in car batteries and ammunition). Arsenic is a common n-type dopant in semiconductor electronic devices. It is also a component of the III–V compound semiconductor gallium arsenide. Arsenic and its compounds, especially the trioxide, are used in the production of pesticides, treated wood products, herbicides, and insecticides. These applications are declining with the increasing recognition of the toxicity of arsenic and its compounds.

A few species of bacteria are able to use arsenic compounds as respiratory metabolites. Trace quantities of arsenic are an essential dietary element in rats, hamsters, goats, chickens, and presumably other species. A role in human metabolism is not known. However, arsenic poisoning occurs in multicellular life if quantities are larger than needed. Arsenic contamination of groundwater is a problem that affects millions of people across the world.

The United States' Environmental Protection Agency states that all forms of arsenic are a serious risk to human health. The United States' Agency for Toxic Substances and Disease Registry ranked arsenic as number 1 in its 2001 Priority List of Hazardous Substances at Superfund sites. Arsenic is classified as a Group-A carcinogen.

The three most common arsenic allotropes are grey, yellow, and black arsenic, with grey being the most common. Grey arsenic (α-As, space group Rm No. 166) adopts a double-layered structure consisting of many interlocked, ruffled, six-membered rings. Because of weak bonding between the layers, grey arsenic is brittle and has a relatively low Mohs hardness of 3.5. Nearest and next-nearest neighbors form a distorted octahedral complex, with the three atoms in the same double-layer being slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 5.73 g/cm. Grey arsenic is a semimetal, but becomes a semiconductor with a bandgap of 1.2–1.4 eV if amorphized. Grey arsenic is also the most stable form. 
Yellow arsenic is soft and waxy, and somewhat similar to tetraphosphorus (). Both have four atoms arranged in a tetrahedral structure in which each atom is bound to each of the other three atoms by a single bond. This unstable allotrope, being molecular, is the most volatile, least dense, and most toxic. Solid yellow arsenic is produced by rapid cooling of arsenic vapor, . It is rapidly transformed into grey arsenic by light. The yellow form has a density of 1.97 g/cm. Black arsenic is similar in structure to black phosphorus.
Black arsenic can also be formed by cooling vapor at around 100–220 °C and by crystallization of amorphous arsenic in the presence of mercury vapors. It is glassy and brittle. Black arsenic is also a poor electrical conductor. As arsenic's triple point is at 3.628 MPa (35.81 atm), it does not have a melting point at standard pressure but instead sublimes from solid to vapor at 887 K (615 °C or 1137 °F).

Arsenic occurs in nature as one stable isotope, As, a monoisotopic element. As of 2003, at least 33 radioisotopes have also been synthesized, ranging in atomic mass from 60 to 92. The most stable of these is As with a half-life of 80.30 days. All other isotopes have half-lives of under one day, with the exception of As ("t"=65.30 hours), As ("t"=26.0 hours), As ("t"=17.77 days), As ("t"=26.26 hours), and As ("t"=38.83 hours). Isotopes that are lighter than the stable As tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions.

At least 10 nuclear isomers have been described, ranging in atomic mass from 66 to 84. The most stable of arsenic's isomers is As with a half-life of 111 seconds.

Arsenic has a similar electronegativity and ionization energies to its lighter congener phosphorus and accordingly readily forms covalent molecules with most of the nonmetals. Though stable in dry air, arsenic forms a golden-bronze tarnish upon exposure to humidity which eventually becomes a black surface layer. When heated in air, arsenic oxidizes to arsenic trioxide; the fumes from this reaction have an odor resembling garlic. This odor can be detected on striking arsenide minerals such as arsenopyrite with a hammer. It burns in oxygen to form arsenic trioxide and arsenic pentoxide, which have the same structure as the more well-known phosphorus compounds, and in fluorine to give arsenic pentafluoride. Arsenic (and some arsenic compounds) sublimes upon heating at atmospheric pressure, converting directly to a gaseous form without an intervening liquid state at . The triple point is 3.63 MPa and . Arsenic makes arsenic acid with concentrated nitric acid, arsenous acid with dilute nitric acid, and arsenic trioxide with concentrated sulfuric acid; however, it does not react with water, alkalis, or non-oxidising acids. Arsenic reacts with metals to form arsenides, though these are not ionic compounds containing the As ion as the formation of such an anion would be highly endothermic and even the group 1 arsenides have properties of intermetallic compounds. Like germanium, selenium, and bromine, which like arsenic succeed the 3d transition series, arsenic is much less stable in the group oxidation state of +5 than its vertical neighbors phosphorus and antimony, and hence arsenic pentoxide and arsenic acid are potent oxidizers.

Compounds of arsenic resemble in some respects those of phosphorus which occupies the same group (column) of the periodic table. The most common oxidation states for arsenic are: −3 in the arsenides, which are alloy-like intermetallic compounds, +3 in the arsenites, and +5 in the arsenates and most organoarsenic compounds. Arsenic also bonds readily to itself as seen in the square ions in the mineral skutterudite. In the +3 oxidation state, arsenic is typically pyramidal owing to the influence of the lone pair of electrons.

One of the simplest arsenic compounds is the trihydride, the highly toxic, flammable, pyrophoric arsine (AsH). This compound is generally regarded as stable, since at room temperature it decomposes only slowly. At temperatures of 250–300 °C decomposition to arsenic and hydrogen is rapid. Several factors, such as humidity, presence of light and certain catalysts (namely aluminium) facilitate the rate of decomposition. It oxidises readily in air to form arsenic trioxide and water, and analogous reactions take place with sulfur and selenium instead of oxygen.

Arsenic forms colorless, odorless, crystalline oxides AsO ("white arsenic") and AsO which are hygroscopic and readily soluble in water to form acidic solutions. Arsenic(V) acid is a weak acid and the salts are called arsenates, the most common arsenic contamination of groundwater, and a problem that affects many people. Synthetic arsenates include Scheele's Green (cupric hydrogen arsenate, acidic copper arsenate), calcium arsenate, and lead hydrogen arsenate. These three have been used as agricultural insecticides and poisons.

The protonation steps between the arsenate and arsenic acid are similar to those between phosphate and phosphoric acid. Unlike phosphorous acid, arsenous acid is genuinely tribasic, with the formula As(OH).

A broad variety of sulfur compounds of arsenic are known. Orpiment (AsS) and realgar (AsS) are somewhat abundant and were formerly used as painting pigments. In AsS, arsenic has a formal oxidation state of +2 in AsS which features As-As bonds so that the total covalency of As is still 3. Both orpiment and realgar, as well as AsS, have selenium analogs; the analogous AsTe is known as the mineral kalgoorlieite, and the anion AsTe is known as a ligand in cobalt complexes.

All trihalides of arsenic(III) are well known except the astatide, which is unknown. Arsenic pentafluoride (AsF) is the only important pentahalide, reflecting the lower stability of the +5 oxidation state; even so, it is a very strong fluorinating and oxidizing agent. (The pentachloride is stable only below −50 °C, at which temperature it decomposes to the trichloride, releasing chlorine gas.)

Arsenic is used as the group 5 element in the III-V semiconductors gallium arsenide, indium arsenide, and aluminium arsenide. The valence electron count of GaAs is the same as a pair of Si atoms, but the band structure is completely different which results in distinct bulk properties. Other arsenic alloys include the II-V semiconductor cadmium arsenide.

A large variety of organoarsenic compounds are known. Several were developed as chemical warfare agents during World War I, including vesicants such as lewisite and vomiting agents such as adamsite. Cacodylic acid, which is of historic and practical interest, arises from the methylation of arsenic trioxide, a reaction that has no analogy in phosphorus chemistry. Cacodyl was the first organometallic compound known (even though arsenic is not a true metal) and was named from the Greek "κακωδία" "stink" for its offensive odor; it is very poisonous.

Arsenic comprises about 1.5 ppm (0.00015%) of the Earth's crust, and is the 53rd most abundant element. Typical background concentrations of arsenic do not exceed 3 ng/m in the atmosphere; 100 mg/kg in soil; 400 μg/kg in vegetation; 10 μg/L in freshwater and 1.5 μg/L in seawater.

Minerals with the formula MAsS and MAs (M = Fe, Ni, Co) are the dominant commercial sources of arsenic, together with realgar (an arsenic sulfide mineral) and native (elemental) arsenic. An illustrative mineral is arsenopyrite (FeAsS), which is structurally related to iron pyrite. Many minor As-containing minerals are known. Arsenic also occurs in various organic forms in the environment.

In 2014, China was the top producer of white arsenic with almost 70% world share, followed by Morocco, Russia, and Belgium, according to the British Geological Survey and the United States Geological Survey. Most arsenic refinement operations in the US and Europe have closed over environmental concerns. Arsenic is found in the smelter dust from copper, gold, and lead smelters, and is recovered primarily from copper refinement dust.

On roasting arsenopyrite in air, arsenic sublimes as arsenic(III) oxide leaving iron oxides, while roasting without air results in the production of gray arsenic. Further purification from sulfur and other chalcogens is achieved by sublimation in vacuum, in a hydrogen atmosphere, or by distillation from molten lead-arsenic mixture.

The word "arsenic" has its origin in the Syriac word "zarnika",
from Arabic al-zarnīḵ 'the orpiment', based on Persian zar ("gold") from the word "zarnikh", meaning "yellow" (literally "gold-colored") and hence "(yellow) orpiment". It was adopted into Greek (using folk etymology) as "arsenikon" () – a neuter form of the Greek adjective "arsenikos" (), meaning "male", "virile".

Latin-speakers adopted the Greek term as , which in French ultimately became , whence the English word "arsenic".
Arsenic sulfides (orpiment, realgar) and oxides have been known and used since ancient times. Zosimos () describes roasting "sandarach" (realgar) to obtain "cloud of arsenic" (arsenic trioxide), which he then reduces to gray arsenic. As the symptoms of arsenic poisoning are not very specific, the substance was frequently used for murder until the advent in the 1830s of the Marsh test, a sensitive chemical test for its presence. (Another less sensitive but more general test is the Reinsch test.) Owing to its use by the ruling class to murder one another and its potency and discreetness, arsenic has been called the "poison of kings" and the "king of poisons". Arsenic became known as "the inheritance powder" due to its use in killing family members in the Renaissance era.

During the Bronze Age, arsenic was often included in the manufacture of bronze, making the alloy harder (so-called "arsenical bronze").
Jabir ibn Hayyan described the isolation of arsenic before 815 AD. Albertus Magnus (Albert the Great, 1193–1280) later isolated the element from a compound in 1250, by heating soap together with arsenic trisulfide. In 1649, Johann Schröder published two ways of preparing arsenic. Crystals of elemental (native) arsenic are found in nature, although rarely.

Cadet's fuming liquid (impure cacodyl), often claimed as the first synthetic organometallic compound, was synthesized in 1760 by Louis Claude Cadet de Gassicourt through the reaction of potassium acetate with arsenic trioxide.

In the Victorian era, women would eat "arsenic" ("white arsenic" or arsenic trioxide) mixed with vinegar and chalk to improve the complexion of their faces, making their skin paler (to show they did not work in the fields). The accidental use of arsenic in the adulteration of foodstuffs led to the Bradford sweet poisoning in 1858, which resulted in 21 deaths. From the late-18th century wallpaper production began to use dyes made from arsenic,
which was thought to increase the pigment's brightness. One account of the illness and 1821 death of Napoleon I implicates arsenic poisoning involving wallpaper.

Two arsenic pigments have been widely used since their discovery – Paris Green in 1814 and Scheele's Green in 1775. After the toxicity of arsenic became widely known, these chemicals were used less often as pigments and more often as insecticides. In the 1860s, an arsenic byproduct of dye production, London Purple, was widely used. This was a solid mixture of arsenic trioxide, aniline, lime, and ferrous oxide, insoluble in water and very toxic by inhalation or ingestion But it was later replaced with Paris Green, another arsenic-based dye. With better understanding of the toxicology mechanism, two other compounds were used starting in the 1890s. Arsenite of lime and arsenate of lead were used widely as insecticides until the discovery of DDT in 1942.

The toxicity of arsenic to insects, bacteria, and fungi led to its use as a wood preservative. In the 1930s, a process of treating wood with chromated copper arsenate (also known as CCA or Tanalith) was invented, and for decades, this treatment was the most extensive industrial use of arsenic. An increased appreciation of the toxicity of arsenic led to a ban of CCA in consumer products in 2004, initiated by the European Union and United States. However, CCA remains in heavy use in other countries (such as on Malaysian rubber plantations).

Arsenic was also used in various agricultural insecticides and poisons. For example, lead hydrogen arsenate was a common insecticide on fruit trees, but contact with the compound sometimes resulted in brain damage among those working the sprayers. In the second half of the 20th century, monosodium methyl arsenate (MSMA) and disodium methyl arsenate (DSMA) – less toxic organic forms of arsenic – replaced lead arsenate in agriculture. These organic arsenicals were in turn phased out in the United States by 2013 in all agricultural activities except cotton farming.

The biogeochemistry of arsenic is complex and includes various adsorption and desorption processes. The toxicity of arsenic is connected to its solubility and is affected by pH. Arsenite () is more soluble than arsenate () and is more toxic; however, at a lower pH, arsenate becomes more mobile and toxic. It was found that addition of sulfur, phosphorus, and iron oxides to high-arsenite soils greatly reduces arsenic phytotoxicity.

Arsenic is used as a feed additive in poultry and swine production, in particular it was used in the U.S. until 2015 to increase weight gain, improve feed efficiency, and prevent disease. An example is roxarsone, which had been used as a broiler starter by about 70% of U.S. broiler growers. In 2011, Alpharma, a subsidiary of Pfizer Inc., which produces roxarsone, voluntarily suspended sales of the drug in response to studies showing elevated levels of inorganic arsenic, a carcinogen, in treated chickens. A successor to Alpharma, Zoetis, continued to sell nitarsone until 2015, primarily for use in turkeys.

A 2006 study of the remains of the Australian racehorse, Phar Lap, determined that the 1932 death of the famous champion was caused by a massive overdose of arsenic. Sydney veterinarian Percy Sykes stated, "In those days, arsenic was quite a common tonic, usually given in the form of a solution (Fowler's Solution) ... It was so common that I'd reckon 90 per cent of the horses had arsenic in their system."

During the 17th, 18th, and 19th centuries, a number of arsenic compounds were used as medicines, including arsphenamine (by Paul Ehrlich) and arsenic trioxide (by Thomas Fowler). Arsphenamine, as well as neosalvarsan, was indicated for syphilis, but has been superseded by modern antibiotics. However, arsenicals such as melarsoprol are still used for the treatment of trypanosomiasis, since although these drugs have the disadvantage of severe toxicity, the disease is almost uniformly fatal if untreated.

Arsenic trioxide has been used in a variety of ways since the 15th century, most commonly in the treatment of cancer, but also in medications as diverse as Fowler's solution in psoriasis. The US Food and Drug Administration in the year 2000 approved this compound for the treatment of patients with acute promyelocytic leukemia that is resistant to all-trans retinoic acid.

A 2008 paper reports success in locating tumors using arsenic-74 (a positron emitter). This isotope produces clearer PET scan images than the previous radioactive agent, iodine-124, because the body tends to transport iodine to the thyroid gland producing signal noise. Nanoparticles of arsenic have shown ability to kill cancer cells with lesser cytotoxicity than other arsenic formulations.

In subtoxic doses, soluble arsenic compounds act as stimulants, and were once popular in small doses as medicine by people in the mid-18th to 19th centuries; its use as a stimulant was especially prevalent as sport animals such as race horses or with work dogs.

The main use of arsenic is in alloying with lead. Lead components in car batteries are strengthened by the presence of a very small percentage of arsenic. Dezincification of brass (a copper-zinc alloy) is greatly reduced by the addition of arsenic. "Phosphorus Deoxidized Arsenical Copper" with an arsenic content of 0.3% has an increased corrosion stability in certain environments. Gallium arsenide is an important semiconductor material, used in integrated circuits. Circuits made from GaAs are much faster (but also much more expensive) than those made from silicon. Unlike silicon, GaAs has a direct bandgap, and can be used in laser diodes and LEDs to convert electrical energy directly into light.

After World War I, the United States built a stockpile of 20,000 tons of weaponized lewisite (ClCH=CHAsCl), an organoarsenic vesicant (blister agent) and lung irritant. The stockpile was neutralized with bleach and dumped into the Gulf of Mexico in the 1950s. During the Vietnam War, the United States used Agent Blue, a mixture of sodium cacodylate and its acid form, as one of the rainbow herbicides to deprive North Vietnamese soldiers of foliage cover and rice.


Some species of bacteria obtain their energy in the absence of oxygen by oxidizing various fuels while reducing arsenate to arsenite. Under oxidative environmental conditions some bacteria use arsenite as fuel, which they oxidize to arsenate. The enzymes involved are known as arsenate reductases (Arr).

In 2008, bacteria were discovered that employ a version of photosynthesis in the absence of oxygen with arsenites as electron donors, producing arsenates (just as ordinary photosynthesis uses water as electron donor, producing molecular oxygen). Researchers conjecture that, over the course of history, these photosynthesizing organisms produced the arsenates that allowed the arsenate-reducing bacteria to thrive. One strain, PHS-1, has been isolated and is related to the gammaproteobacterium "Ectothiorhodospira shaposhnikovii". The mechanism is unknown, but an encoded Arr enzyme may function in reverse to its known homologues.

In 2011, it was postulated that a strain of "Halomonadaceae" could be grown in the absence of phosphorus if that element were substituted with arsenic, exploiting the fact that the arsenate and phosphate anions are similar structurally. The study was widely criticised and subsequently refuted by independent researcher groups.

Arsenic is understood to be an essential trace mineral in birds as it is involved in the synthesis of methionine metabolites, with feeding recommendations being between 0.012 and 0.050 mg/kg.

Some evidence indicates that arsenic is an essential trace mineral in mammals. However, the biological function is not known.

Arsenic has been linked to epigenetic changes, heritable changes in gene expression that occur without changes in DNA sequence. These include DNA methylation, histone modification, and RNA interference. Toxic levels of arsenic cause significant DNA hypermethylation of tumor suppressor genes p16 and p53, thus increasing risk of carcinogenesis. These epigenetic events have been studied "in vitro" using human kidney cells and "in vivo" using rat liver cells and peripheral blood leukocytes in humans. Inductively coupled plasma mass spectrometry (ICP-MS) is used to detect precise levels of intracellular arsenic and other arsenic bases involved in epigenetic modification of DNA. Studies investigating arsenic as an epigenetic factor can be used to develop precise biomarkers of exposure and susceptibility.

The Chinese brake fern ("Pteris vittata") hyperaccumulates arsenic from the soil into its leaves and has a proposed use in phytoremediation.

Inorganic arsenic and its compounds, upon entering the food chain, are progressively metabolized through a process of methylation. For example, the mold "Scopulariopsis brevicaulis" produces trimethylarsine if inorganic arsenic is present. The organic compound arsenobetaine is found in some marine foods such as fish and algae, and also in mushrooms in larger concentrations. The average person's intake is about 10–50 µg/day. Values about 1000 µg are not unusual following consumption of fish or mushrooms, but there is little danger in eating fish because this arsenic compound is nearly non-toxic.

Naturally occurring sources of human exposure include volcanic ash, weathering of minerals and ores, and mineralized groundwater. Arsenic is also found in food, water, soil, and air. Arsenic is absorbed by all plants, but is more concentrated in leafy vegetables, rice, apple and grape juice, and seafood. An additional route of exposure is inhalation of atmospheric gases and dusts.
During the Victorian era, arsenic was widely used in home decor, especially wallpapers.

Extensive arsenic contamination of groundwater has led to widespread arsenic poisoning in Bangladesh and neighboring countries. It is estimated that approximately 57 million people in the Bengal basin are drinking groundwater with arsenic concentrations elevated above the World Health Organization's standard of 10 parts per billion (ppb). However, a study of cancer rates in Taiwan suggested that significant increases in cancer mortality appear only at levels above 150 ppb. The arsenic in the groundwater is of natural origin, and is released from the sediment into the groundwater, caused by the anoxic conditions of the subsurface. This groundwater was used after local and western NGOs and the Bangladeshi government undertook a massive shallow tube well drinking-water program in the late twentieth century. This program was designed to prevent drinking of bacteria-contaminated surface waters, but failed to test for arsenic in the groundwater. Many other countries and districts in Southeast Asia, such as Vietnam and Cambodia, have geological environments that produce groundwater with a high arsenic content. was reported in Nakhon Si Thammarat, Thailand, in 1987, and the Chao Phraya River probably contains high levels of naturally occurring dissolved arsenic without being a public health problem because much of the public uses bottled water. In Pakistan, more than 60 million people are exposed to arsenic polluted drinking water indicated by a 2017 report in "Science". Podgorski's team investigated more than 1200 samples and more than 66% exceeded the WHO minimum contamination level.

Since the 1980s, residents of the Ba Men region of Inner Mongolia, China have been chronically exposed to arsenic through drinking water from contaminated wells. A 2009 research study observed an elevated presence of skin lesions among residents with well water arsenic concentrations between 5 and 10 µg/L, suggesting that arsenic induced toxicity may occur at relatively low concentrations with chronic exposure. Overall, 20 of China's 34 provinces have high arsenic concentrations in the groundwater supply, potentially exposing 19 million people to hazardous drinking water.

A study by IIT Kharagpur found high levels of Arsenic in groundwater of 20% of India's land, exposing more than 250 million people. States such as Punjab, Bihar, West Bengal, Assam, Haryana, Uttar Pradesh, and Gujarat have highest land area exposed to arsenic.

In the United States, arsenic is most commonly found in the ground waters of the southwest. Parts of New England, Michigan, Wisconsin, Minnesota and the Dakotas are also known to have significant concentrations of arsenic in ground water. Increased levels of skin cancer have been associated with arsenic exposure in Wisconsin, even at levels below the 10 ppb drinking water standard. According to a recent film funded by the US Superfund, millions of private wells have unknown arsenic levels, and in some areas of the US, more than 20% of the wells may contain levels that exceed established limits.

Low-level exposure to arsenic at concentrations of 100 ppb (i.e., above the 10 ppb drinking water standard) compromises the initial immune response to H1N1 or swine flu infection according to NIEHS-supported scientists. The study, conducted in laboratory mice, suggests that people exposed to arsenic in their drinking water may be at increased risk for more serious illness or death from the virus.

Some Canadians are drinking water that contains inorganic arsenic. Private-dug–well waters are most at risk for containing inorganic arsenic. Preliminary well water analysis typically does not test for arsenic. Researchers at the Geological Survey of Canada have modeled relative variation in natural arsenic hazard potential for the province of New Brunswick. This study has important implications for potable water and health concerns relating to inorganic arsenic.

Epidemiological evidence from Chile shows a dose-dependent connection between chronic arsenic exposure and various forms of cancer, in particular when other risk factors, such as cigarette smoking, are present. These effects have been demonstrated at contaminations less than 50 ppb. Arsenic is itself a constituent of tobacco smoke.

Analyzing multiple epidemiological studies on inorganic arsenic exposure suggests a small but measurable increase in risk for bladder cancer at 10 ppb. According to Peter Ravenscroft of the Department of Geography at the University of Cambridge, roughly 80 million people worldwide consume between 10 and 50 ppb arsenic in their drinking water. If they all consumed exactly 10 ppb arsenic in their drinking water, the previously cited multiple epidemiological study analysis would predict an additional 2,000 cases of bladder cancer alone. This represents a clear underestimate of the overall impact, since it does not include lung or skin cancer, and explicitly underestimates the exposure. Those exposed to levels of arsenic above the current WHO standard should weigh the costs and benefits of arsenic remediation.
Early (1973) evaluations of the processes for removing dissolved arsenic from drinking water demonstrated the efficacy of co-precipitation with either iron or aluminium oxides. In particular, iron as a coagulant was found to remove arsenic with an efficacy exceeding 90%. Several adsorptive media systems have been approved for use at point-of-service in a study funded by the United States Environmental Protection Agency (US EPA) and the National Science Foundation (NSF). A team of European and Indian scientists and engineers have set up six arsenic treatment plants in West Bengal based on in-situ remediation method (SAR Technology). This technology does not use any chemicals and arsenic is left in an insoluble form (+5 state) in the subterranean zone by recharging aerated water into the aquifer and developing an oxidation zone that supports arsenic oxidizing micro-organisms. This process does not produce any waste stream or sludge and is relatively cheap.

Another effective and inexpensive method to avoid arsenic contamination is to sink wells 500 feet or deeper to reach purer waters. A recent 2011 study funded by the US National Institute of Environmental Health Sciences' Superfund Research Program shows that deep sediments can remove arsenic and take it out of circulation. In this process, called "adsorption", arsenic sticks to the surfaces of deep sediment particles and is naturally removed from the ground water.

Magnetic separations of arsenic at very low magnetic field gradients with high-surface-area and monodisperse magnetite (FeO) nanocrystals have been demonstrated in point-of-use water purification. Using the high specific surface area of FeO nanocrystals, the mass of waste associated with arsenic removal from water has been dramatically reduced.

Epidemiological studies have suggested a correlation between chronic consumption of drinking water contaminated with arsenic and the incidence of all leading causes of mortality. The literature indicates that arsenic exposure is causative in the pathogenesis of diabetes.

Chaff-based filters have recently been shown to reduce the arsenic content of water to 3 µg/L. This may find applications in areas where the potable water is extracted from underground aquifers.

For several centuries, the people of San Pedro de Atacama in Chile have been drinking water that is contaminated with arsenic, and some evidence suggests they have developed some immunity.

Around one-third of the world's population drinks water from groundwater resources. Of this, about 10 percent, approximately 300 million people, obtains water from groundwater resources that are contaminated with unhealthy levels of arsenic or fluoride. These trace elements derive mainly from minerals and ions in the ground.

Arsenic is unique among the trace metalloids and oxyanion-forming trace metals (e.g. As, Se, Sb, Mo, V, Cr, U, Re). It is sensitive to mobilization at pH values typical of natural waters (pH 6.5–8.5) under both oxidizing and reducing conditions. Arsenic can occur in the environment in several oxidation states (−3, 0, +3 and +5), but in natural waters it is mostly found in inorganic forms as oxyanions of trivalent arsenite [As(III)] or pentavalent arsenate [As(V)]. Organic forms of arsenic are produced by biological activity, mostly in surface waters, but are rarely quantitatively important. Organic arsenic compounds may, however, occur where waters are significantly impacted by industrial pollution.

Arsenic may be solubilized by various processes. When pH is high, arsenic may be released from surface binding sites that lose their positive charge. When water level drops and sulfide minerals are exposed to air, arsenic trapped in sulfide minerals can be released into water. When organic carbon is present in water, bacteria are fed by directly reducing As(V) to As(III) or by reducing the element at the binding site, releasing inorganic arsenic.

The aquatic transformations of arsenic are affected by pH, reduction-oxidation potential, organic matter concentration and the concentrations and forms of other elements, especially iron and manganese. The main factors are pH and the redox potential. Generally, the main forms of arsenic under oxic conditions are HAsO, HAsO, HAsO, and AsO at pH 2, 2–7, 7–11 and 11, respectively. Under reducing conditions, HAsO is predominant at pH 2–9.

Oxidation and reduction affects the migration of arsenic in subsurface environments. Arsenite is the most stable soluble form of arsenic in reducing environments and arsenate, which is less mobile than arsenite, is dominant in oxidizing environments at neutral pH. Therefore, arsenic may be more mobile under reducing conditions. The reducing environment is also rich in organic matter which may enhance the solubility of arsenic compounds. As a result, the adsorption of arsenic is reduced and dissolved arsenic accumulates in groundwater. That is why the arsenic content is higher in reducing environments than in oxidizing environments.

The presence of sulfur is another factor that affects the transformation of arsenic in natural water. Arsenic can precipitate when metal sulfides form. In this way, arsenic is removed from the water and its mobility decreases. When oxygen is present, bacteria oxidize reduced sulfur to generate energy, potentially releasing bound arsenic.

Redox reactions involving Fe also appear to be essential factors in the fate of arsenic in aquatic systems. The reduction of iron oxyhydroxides plays a key role in the release of arsenic to water. So arsenic can be enriched in water with elevated Fe concentrations. Under oxidizing conditions, arsenic can be mobilized from pyrite or iron oxides especially at elevated pH. Under reducing conditions, arsenic can be mobilized by reductive desorption or dissolution when associated with iron oxides. The reductive desorption occurs under two circumstances. One is when arsenate is reduced to arsenite which adsorbs to iron oxides less strongly. The other results from a change in the charge on the mineral surface which leads to the desorption of bound arsenic.

Some species of bacteria catalyze redox transformations of arsenic. Dissimilatory arsenate-respiring prokaryotes (DARP) speed up the reduction of As(V) to As(III). DARP use As(V) as the electron acceptor of anaerobic respiration and obtain energy to survive. Other organic and inorganic substances can be oxidized in this process. Chemoautotrophic arsenite oxidizers (CAO) and heterotrophic arsenite oxidizers (HAO) convert As(III) into As(V). CAO combine the oxidation of As(III) with the reduction of oxygen or nitrate. They use obtained energy to fix produce organic carbon from CO. HAO cannot obtain energy from As(III) oxidation. This process may be an arsenic detoxification mechanism for the bacteria.

Equilibrium thermodynamic calculations predict that As(V) concentrations should be greater than As(III) concentrations in all but strongly reducing conditions, i.e. where SO reduction is occurring. However, abiotic redox reactions of arsenic are slow. Oxidation of As(III) by dissolved O is a particularly slow reaction. For example, Johnson and Pilson (1975) gave half-lives for the oxygenation of As(III) in seawater ranging from several months to a year. In other studies, As(V)/As(III) ratios were stable over periods of days or weeks during water sampling when no particular care was taken to prevent oxidation, again suggesting relatively slow oxidation rates. Cherry found from experimental studies that the As(V)/As(III) ratios were stable in anoxic solutions for up to 3 weeks but that gradual changes occurred over longer timescales. Sterile water samples have been observed to be less susceptible to speciation changes than non-sterile samples. Oremland found that the reduction of As(V) to As(III) in Mono Lake was rapidly catalyzed by bacteria with rate constants ranging from 0.02 to 0.3-day.

As of 2002, US-based industries consumed 19,600 metric tons of arsenic. Ninety percent of this was used for treatment of wood with chromated copper arsenate (CCA). In 2007, 50% of the 5,280 metric tons of consumption was still used for this purpose. In the United States, the voluntary phasing-out of arsenic in production of consumer products and residential and general consumer construction products began on 31 December 2003, and alternative chemicals are now used, such as Alkaline Copper Quaternary, borates, copper azole, cyproconazole, and propiconazole.

Although discontinued, this application is also one of the most concerning to the general public. The vast majority of older pressure-treated wood was treated with CCA. CCA lumber is still in widespread use in many countries, and was heavily used during the latter half of the 20th century as a structural and outdoor building material. Although the use of CCA lumber was banned in many areas after studies showed that arsenic could leach out of the wood into the surrounding soil (from playground equipment, for instance), a risk is also presented by the burning of older CCA timber. The direct or indirect ingestion of wood ash from burnt CCA lumber has caused fatalities in animals and serious poisonings in humans; the lethal human dose is approximately 20 grams of ash. Scrap CCA lumber from construction and demolition sites may be inadvertently used in commercial and domestic fires. Protocols for safe disposal of CCA lumber are not consistent throughout the world. Widespread landfill disposal of such timber raises some concern, but other studies have shown no arsenic contamination in the groundwater.

One tool that maps the location (and other information) of arsenic releases in the United States is TOXMAP. TOXMAP is a Geographic Information System (GIS) from the Division of Specialized Information Services of the United States National Library of Medicine (NLM) funded by the US Federal Government. With marked-up maps of the United States, TOXMAP enables users to visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs. TOXMAP's chemical and environmental health information is taken from NLM's Toxicology Data Network (TOXNET), PubMed, and from other authoritative sources.

Physical, chemical, and biological methods have been used to remediate arsenic contaminated water. Bioremediation is said to be cost-effective and environmentally friendly. Bioremediation of ground water contaminated with arsenic aims to convert arsenite, the toxic form of arsenic to humans, to arsenate. Arsenate (+5 oxidation state) is the dominant form of arsenic in surface water, while arsenite (+3 oxidation state) is the dominant form in hypoxic to anoxic environments. Arsenite is more soluble and mobile than arsenate. Many species of bacteria can transform arsenite to arsenate in anoxic conditions by using arsenite as an electron donor. This is a useful method in ground water remediation. Another bioremediation strategy is to use plants that accumulate arsenic in their tissues via phytoremediation but the disposal of contaminated plant material needs to be considered.

Bioremediation requires careful evaluation and design in accordance with existing conditions. Some sites may require the addition of an electron acceptor while others require microbe supplementation (bioaugmentation). Regardless of the method used, only constant monitoring can prevent future contamination.

Coagulation and flocculation are closely related processes common in arsenate removal from water. Due to the net negative charge carried by arsenate ions, they settle slowly or do not settle at all due to charge repulsion. In coagulation, a positively charged coagulent such as Fe and Alum (common used salts: FeCl, Fe(SO), Al(SO)) neutralise the negatively charged arsenate, enable it to settle. Flocculation follows where an flocculant bridge smaller particles and allows the aggregate to precipitate out from water. However, such methods may not be efficient on arsenite as As(III) exist in uncharged arsenious acid, HAsO, at near neutral pH.

The major drawbacks of coagulation and flocculation is the costly disposal of arsenate-concentrated sludge, and possible secondary contamination of environment. Moreover, coagulents such as Fe may produce ion contamination that exceeds safety level.

Arsenic and many of its compounds are especially potent poisons. Small amount of arsenic can be detected by pharmacopoial methods which includes reduction of arsenic to arsenious with help of zinc and can be confirmed with mercuric chloride paper.

Elemental arsenic and arsenic sulfate and trioxide compounds are classified as "toxic" and "dangerous for the environment" in the European Union under directive 67/548/EEC.

The International Agency for Research on Cancer (IARC) recognizes arsenic and inorganic arsenic compounds as group 1 carcinogens, and the EU lists arsenic trioxide, arsenic pentoxide, and arsenate salts as category 1 carcinogens.

Arsenic is known to cause arsenicosis when present in drinking water, "the most common species being arsenate [; As(V)] and arsenite [; As(III)]".

In the United States since 2006, the maximum concentration in drinking water allowed by the Environmental Protection Agency (EPA) is 10 ppb and the FDA set the same standard in 2005 for bottled water. The Department of Environmental Protection for New Jersey set a drinking water limit of 5 ppb in 2006. The IDLH (immediately dangerous to life and health) value for arsenic metal and inorganic arsenic compounds is 5 mg/m (5 ppb). The Occupational Safety and Health Administration has set the permissible exposure limit (PEL) to a time-weighted average (TWA) of 0.01 mg/m (0.01 ppb), and the National Institute for Occupational Safety and Health (NIOSH) has set the recommended exposure limit (REL) to a 15-minute constant exposure of 0.002 mg/m (0.002 ppb). The PEL for organic arsenic compounds is a TWA of 0.5 mg/m. (0.5 ppb).

In 2008, based on its ongoing testing of a wide variety of American foods for toxic chemicals, the U.S. Food and Drug Administration set the "level of concern" for inorganic arsenic in apple and pear juices at 23 ppb, based on non-carcinogenic effects, and began blocking importation of products in excess of this level; it also required recalls for non-conforming domestic products. In 2011, the national "Dr. Oz" television show broadcast a program highlighting tests performed by an independent lab hired by the producers. Though the methodology was disputed (it did not distinguish between organic and inorganic arsenic) the tests showed levels of arsenic up to 36 ppb. In response, the FDA tested the worst brand from the "Dr." "Oz" show and found much lower levels. Ongoing testing found 95% of the apple juice samples were below the level of concern. Later testing by Consumer Reports showed inorganic arsenic at levels slightly above 10 ppb, and the organization urged parents to reduce consumption. In July 2013, on consideration of consumption by children, chronic exposure, and carcinogenic effect, the FDA established an "action level" of 10 ppb for apple juice, the same as the drinking water standard.

Concern about arsenic in rice in Bangladesh was raised in 2002, but at the time only Australia had a legal limit for food (one milligram per kilogram). Concern was raised about people who were eating U.S. rice exceeding WHO standards for personal arsenic intake in 2005. In 2011, the People's Republic of China set a food standard of 150 ppb for arsenic.

In the United States in 2012, testing by separate groups of researchers at the Children's Environmental Health and Disease Prevention Research Center at Dartmouth College (early in the year, focusing on urinary levels in children) and Consumer Reports (in November) found levels of arsenic in rice that resulted in calls for the FDA to set limits. The FDA released some testing results in September 2012, and as of July 2013, is still collecting data in support of a new potential regulation. It has not recommended any changes in consumer behavior.

Consumer Reports recommended: 
A 2014 World Health Organization advisory conference was scheduled to consider limits of 200–300 ppb for rice.

In 2020, scientists assessed multiple preparation procedures of rice for their capacity to reduce arsenic content and preserve nutrients, recommending a procedure involving parboiling and water-absorption.

Arsenic is bioaccumulative in many organisms, marine species in particular, but it does not appear to biomagnify significantly in food webs. In polluted areas, plant growth may be affected by root uptake of arsenate, which is a phosphate analog and therefore readily transported in plant tissues and cells. In polluted areas, uptake of the more toxic arsenite ion (found more particularly in reducing conditions) is likely in poorly-drained soils.

Arsenic's toxicity comes from the affinity of arsenic(III) oxides for thiols. Thiols, in the form of cysteine residues and cofactors such as lipoic acid and coenzyme A, are situated at the active sites of many important enzymes.

Arsenic disrupts ATP production through several mechanisms. At the level of the citric acid cycle, arsenic inhibits lipoic acid, which is a cofactor for pyruvate dehydrogenase. By competing with phosphate, arsenate uncouples oxidative phosphorylation, thus inhibiting energy-linked reduction of NAD+, mitochondrial respiration and ATP synthesis. Hydrogen peroxide production is also increased, which, it is speculated, has potential to form reactive oxygen species and oxidative stress. These metabolic interferences lead to death from multi-system organ failure. The organ failure is presumed to be from necrotic cell death, not apoptosis, since energy reserves have been too depleted for apoptosis to occur.

Occupational exposure and arsenic poisoning may occur in persons working in industries involving the use of inorganic arsenic and its compounds, such as wood preservation, glass production, nonferrous metal alloys, and electronic semiconductor manufacturing. Inorganic arsenic is also found in coke oven emissions associated with the smelter industry.

The conversion between As(III) and As(V) is a large factor in arsenic environmental contamination. According to Croal, Gralnick, Malasarn and Newman, "[the] understanding [of] what stimulates As(III) oxidation and/or limits As(V) reduction is relevant for bioremediation of contaminated sites (Croal). The study of chemolithoautotrophic As(III) oxidizers and the heterotrophic As(V) reducers can help the understanding of the oxidation and/or reduction of arsenic.

Treatment of chronic arsenic poisoning is possible. British anti-lewisite (dimercaprol) is prescribed in doses of 5 mg/kg up to 300 mg every 4 hours for the first day, then every 6 hours for the second day, and finally every 8 hours for 8 additional days. However the USA's Agency for Toxic Substances and Disease Registry (ATSDR) states that the long-term effects of arsenic exposure cannot be predicted. Blood, urine, hair, and nails may be tested for arsenic; however, these tests cannot foresee possible health outcomes from the exposure. Long-term exposure and consequent excretion through urine has been linked to bladder and kidney cancer in addition to cancer of the liver, prostate, skin, lungs, and nasal cavity.



Antimony

Antimony is a chemical element; it has symbol Sb () and atomic number 51. A lustrous gray metalloid, it is found in nature mainly as the sulfide mineral stibnite (SbS). Antimony compounds have been known since ancient times and were powdered for use as medicine and cosmetics, often known by the Arabic name kohl. The earliest known description of the metalloid in the West was written in 1540 by Vannoccio Biringuccio.

China is the largest producer of antimony and its compounds, with most production coming from the Xikuangshan Mine in Hunan. The industrial methods for refining antimony from stibnite are roasting followed by reduction with carbon, or direct reduction of stibnite with iron.

The largest applications for metallic antimony are in alloys with lead and tin, which have improved properties for solders, bullets, and plain bearings. It improves the rigidity of lead-alloy plates in lead–acid batteries. Antimony trioxide is a prominent additive for halogen-containing flame retardants. Antimony is used as a dopant in semiconductor devices.

Antimony is a member of group 15 of the periodic table, one of the elements called pnictogens, and has an electronegativity of 2.05. In accordance with periodic trends, it is more electronegative than tin or bismuth, and less electronegative than tellurium or arsenic. Antimony is stable in air at room temperature, but reacts with oxygen if heated to produce antimony trioxide, SbO.

Antimony is a silvery, lustrous gray metalloid with a Mohs scale hardness of 3, which is too soft to mark hard objects. Coins of antimony were issued in China's Guizhou province in 1931; durability was poor, and minting was soon discontinued. Antimony is resistant to attack by acids.

The only stable allotrope of antimony under standard conditions is metallic, brittle, silver-white, and shiny. It crystallises in a trigonal cell, isomorphic with bismuth and the gray allotrope of arsenic, and is formed when molten antimony is cooled slowly. Amorphous black antimony is formed upon rapid cooling of antimony vapor, and is only stable as a thin film (thickness in nanometres); thicker samples spontaneously transform into the metallic form. It oxidizes in air and may ignite spontaneously. At 100 °C, it gradually transforms into the stable form. The supposed yellow allotrope of antimony, generated only by oxidation of stibine (SbH) at −90 °C, is also impure and not a true allotrope; above this temperature and in ambient light, it transforms into the more stable black allotrope. A rare explosive form of antimony can be formed from the electrolysis of antimony trichloride, but it always contains appreciable chlorine and is not really an antimony allotrope. When scratched with a sharp implement, an exothermic reaction occurs and white fumes are given off as metallic antimony forms; when rubbed with a pestle in a mortar, a strong detonation occurs.

Elemental antimony adopts a layered structure (space group Rm No. 166) whose layers consist of fused, ruffled, six-membered rings. The nearest and next-nearest neighbors form an irregular octahedral complex, with the three atoms in each double layer slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 6.697 g/cm, but the weak bonding between the layers leads to the low hardness and brittleness of antimony.

Antimony has two stable isotopes: Sb with a natural abundance of 57.36% and Sb with a natural abundance of 42.64%. It also has 35 radioisotopes, of which the longest-lived is Sb with a half-life of 2.75 years. In addition, 29 metastable states have been characterized. The most stable of these is Sb with a half-life of 5.76 days. Isotopes that are lighter than the stable Sb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions. Antimony is the lightest element to have an isotope with an alpha decay branch, excluding Be and other light nuclides with beta-delayed alpha emission.

The abundance of antimony in the Earth's crust is estimated at 0.2 parts per million, comparable to thallium at 0.5 ppm and silver at 0.07 ppm. Even though this element is not abundant, it is found in more than 100 mineral species. Antimony is sometimes found natively (e.g. on Antimony Peak), but more frequently it is found in the sulfide stibnite (SbS) which is the predominant ore mineral.

Antimony compounds are often classified according to their oxidation state: Sb(III) and Sb(V). The +5 oxidation state is more common.

Antimony trioxide is formed when antimony is burnt in air. In the gas phase, the molecule of the compound is , but it polymerizes upon condensing. Antimony pentoxide () can be formed only by oxidation with concentrated nitric acid. Antimony also forms a mixed-valence oxide, antimony tetroxide (), which features both Sb(III) and Sb(V). Unlike oxides of phosphorus and arsenic, these oxides are amphoteric, do not form well-defined oxoacids, and react with acids to form antimony salts.

Antimonous acid is unknown, but the conjugate base sodium antimonite () forms upon fusing sodium oxide and . Transition metal antimonites are also known. Antimonic acid exists only as the hydrate , forming salts as the antimonate anion . When a solution containing this anion is dehydrated, the precipitate contains mixed oxides.

The most important antimony ore is stibnite (). Other sulfide minerals include pyrargyrite (), zinkenite, jamesonite, and boulangerite. Antimony pentasulfide is non-stoichiometric, which features antimony in the +3 oxidation state and S–S bonds. Several thioantimonides are known, such as and .

Antimony forms two series of halides: and . The trihalides , , , and are all molecular compounds having trigonal pyramidal molecular geometry.

The trifluoride is prepared by the reaction of with HF:

It is Lewis acidic and readily accepts fluoride ions to form the complex anions and . Molten is a weak electrical conductor. The trichloride is prepared by dissolving in hydrochloric acid:
Arsenic sulfides are not readily attacked by the hydrochloric acid, so this method offers a route to As-free Sb.
The pentahalides and have trigonal bipyramidal molecular geometry in the gas phase, but in the liquid phase, is polymeric, whereas is monomeric. is a powerful Lewis acid used to make the superacid fluoroantimonic acid ("HSbF").

Oxyhalides are more common for antimony than for arsenic and phosphorus. Antimony trioxide dissolves in concentrated acid to form oxoantimonyl compounds such as SbOCl and .

Compounds in this class generally are described as derivatives of Sb. Antimony forms antimonides with metals, such as indium antimonide (InSb) and silver antimonide (). The alkali metal and zinc antimonides, such as NaSb and ZnSb, are more reactive. Treating these antimonides with acid produces the highly unstable gas stibine, :
Stibine can also be produced by treating salts with hydride reagents such as sodium borohydride. Stibine decomposes spontaneously at room temperature. Because stibine has a positive heat of formation, it is thermodynamically unstable and thus antimony does not react with hydrogen directly.

Organoantimony compounds are typically prepared by alkylation of antimony halides with Grignard reagents. A large variety of compounds are known with both Sb(III) and Sb(V) centers, including mixed chloro-organic derivatives, anions, and cations. Examples include triphenylstibine (Sb(CH)) and pentaphenylantimony (Sb(CH)). 

Antimony(III) sulfide, SbS, was recognized in predynastic Egypt as an eye cosmetic (kohl) as early as about 3100 BC, when the cosmetic palette was invented.

An artifact, said to be part of a vase, made of antimony dating to about 3000 BC was found at Telloh, Chaldea (part of present-day Iraq), and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt. Austen, at a lecture by Herbert Gladstone in 1892, commented that "we only know of antimony at the present day as a highly brittle and crystalline metal, which could hardly be fashioned into a useful vase, and therefore this remarkable 'find' (artifact mentioned above) must represent the lost art of rendering antimony malleable."

The British archaeologist Roger Moorey was unconvinced the artifact was indeed a vase, mentioning that Selimkhanov, after his analysis of the Tello object (published in 1975), "attempted to relate the metal to Transcaucasian natural antimony" (i.e. native metal) and that "the antimony objects from Transcaucasia are all small personal ornaments." This weakens the evidence for a lost art "of rendering antimony malleable."

The Roman scholar Pliny the Elder described several ways of preparing antimony sulfide for medical purposes in his treatise "Natural History", around 77 AD. Pliny the Elder also made a distinction between "male" and "female" forms of antimony; the male form is probably the sulfide, while the female form, which is superior, heavier, and less friable, has been suspected to be native metallic antimony.

The Greek naturalist Pedanius Dioscorides mentioned that antimony sulfide could be roasted by heating by a current of air. It is thought that this produced metallic antimony.
Antimony was frequently described in alchemical manuscripts, including the "Summa Perfectionis" of Pseudo-Geber, written around the 14th century. A description of a procedure for isolating antimony is later given in the 1540 book "De la pirotechnia" by Vannoccio Biringuccio, predating the more famous 1556 book by Agricola, "De re metallica". In this context Agricola has been often incorrectly credited with the discovery of metallic antimony. The book "Currus Triumphalis Antimonii" (The Triumphal Chariot of Antimony), describing the preparation of metallic antimony, was published in Germany in 1604. It was purported to be written by a Benedictine monk, writing under the name Basilius Valentinus in the 15th century; if it were authentic, which it is not, it would predate Biringuccio.

The metal antimony was known to German chemist Andreas Libavius in 1615 who obtained it by adding iron to a molten mixture of antimony sulfide, salt and potassium tartrate. This procedure produced antimony with a crystalline or starred surface.

With the advent of challenges to phlogiston theory, it was recognized that antimony is an element forming sulfides, oxides, and other compounds, as do other metals.

The first discovery of naturally occurring pure antimony in the Earth's crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783; the type-sample was collected from the Sala Silver Mine in the Bergslagen mining district of Sala, Västmanland, Sweden.

The medieval Latin form, from which the modern languages and late Byzantine Greek take their names for antimony, is "". The origin of this is uncertain; all suggestions have some difficulty either of form or interpretation. The popular etymology, from ἀντίμοναχός "anti-monachos" or French , still has adherents; this would mean "monk-killer", and is explained by many early alchemists being monks, and antimony being poisonous. However, the low toxicity of antimony (see below) makes this unlikely.

Another popular etymology is the hypothetical Greek word ἀντίμόνος "antimonos", "against aloneness", explained as "not found as metal", or "not found unalloyed". Edmund Oscar von Lippmann conjectured a hypothetical Greek word ανθήμόνιον "anthemonion", which would mean "floret", and cites several examples of related Greek words (but not that one) which describe chemical or biological efflorescence.

The early uses of "antimonium" include the translations, in 1050–1100, by Constantine the African of Arabic medical treatises. Several authorities believe "antimonium" is a scribal corruption of some Arabic form; Meyerhof derives it from "ithmid"; other possibilities include "athimar", the Arabic name of the metalloid, and a hypothetical "as-stimmi", derived from or parallel to the Greek.

The standard chemical symbol for antimony (Sb) is credited to Jöns Jakob Berzelius, who derived the abbreviation from "stibium".

The ancient words for antimony mostly have, as their chief meaning, kohl, the sulfide of antimony.

The Egyptians called antimony "mśdmt" or "stm".

The Arabic word for the substance, as opposed to the cosmetic, can appear as "ithmid, athmoud, othmod", or "uthmod". Littré suggests the first form, which is the earliest, derives from "stimmida", an accusative for "stimmi". The Greek word στίμμι (stimmi) is used by Attic tragic poets of the 5th century BC, and is possibly a loan word from Arabic or from Egyptian "stm".

The extraction of antimony from ores depends on the quality and composition of the ore. Most antimony is mined as the sulfide; lower-grade ores are concentrated by froth flotation, while higher-grade ores are heated to 500–600 °C, the temperature at which stibnite melts and separates from the gangue minerals. Antimony can be isolated from the crude antimony sulfide by reduction with scrap iron:

The sulfide is converted to an oxide by roasting. The product is further purified by vaporizing the volatile antimony(III) oxide, which is recovered. This sublimate is often used directly for the main applications, impurities being arsenic and sulfide. Antimony is isolated from the oxide by a carbothermal reduction:

The lower-grade ores are reduced in blast furnaces while the higher-grade ores are reduced in reverberatory furnaces.

In 2022, according to the US Geological Survey, China accounted for 54.5% of total antimony production, followed in second place by Russia with 18.2% and Tajikistan with 15.5%.

Chinese production of antimony is expected to decline in the future as mines and smelters are closed down by the government as part of pollution control. Especially due to an environmental protection law having gone into effect in January 2015 and revised "Emission Standards of Pollutants for Stanum, Antimony, and Mercury" having gone into effect, hurdles for economic production are higher.

Reported production of antimony in China has fallen and is unlikely to increase in the coming years, according to the Roskill report. No significant antimony deposits in China have been developed for about ten years, and the remaining economic reserves are being rapidly depleted.

For antimony-importing regions such as Europe and the U.S., antimony is considered to be a critical mineral for industrial manufacturing that is at risk of supply chain disruption. With global production coming mainly from China (74%), Tajikistan (8%), and Russia (4%), these sources are critical to supply.

Approximately 48% of antimony is consumed in flame retardants, 33% in lead–acid batteries, and 8% in plastics.

Antimony is mainly used as the trioxide for flame-proofing compounds, always in combination with halogenated flame retardants except in halogen-containing polymers. The flame retarding effect of antimony trioxide is produced by the formation of halogenated antimony compounds, which react with hydrogen atoms, and probably also with oxygen atoms and OH radicals, thus inhibiting fire. Markets for these flame-retardants include children's clothing, toys, aircraft, and automobile seat covers. They are also added to polyester resins in fiberglass composites for such items as light aircraft engine covers. The resin will burn in the presence of an externally generated flame, but will extinguish when the external flame is removed.

Antimony forms a highly useful alloy with lead, increasing its hardness and mechanical strength. For most applications involving lead, varying amounts of antimony are used as alloying metal. In lead–acid batteries, this addition improves plate strength and charging characteristics. For sailboats, lead keels are used to provide righting moment, ranging from 600 lbs to over 200 tons for the largest sailing superyachts; to improve hardness and tensile strength of the lead keel, antimony is mixed with lead between 2% and 5% by volume. Antimony is used in antifriction alloys (such as Babbitt metal), in bullets and lead shot, electrical cable sheathing, type metal (for example, for linotype printing machines), solder (some "lead-free" solders contain 5% Sb), in pewter, and in hardening alloys with low tin content in the manufacturing of organ pipes.

Three other applications consume nearly all the rest of the world's supply. One application is as a stabilizer and catalyst for the production of polyethylene terephthalate. Another is as a fining agent to remove microscopic bubbles in glass, mostly for TV screens antimony ions interact with oxygen, suppressing the tendency of the latter to form bubbles. The third application is pigments.

In the 1990s antimony was increasingly being used in semiconductors as a dopant in n-type silicon wafers for diodes, infrared detectors, and Hall-effect devices. In the 1950s, the emitters and collectors of n-p-n alloy junction transistors were doped with tiny beads of a lead-antimony alloy. Indium antimonide (InSb) is used as a material for mid-infrared detectors.

Biology and medicine have few uses for antimony. Treatments containing antimony, known as antimonials, are used as emetics. Antimony compounds are used as antiprotozoan drugs. Potassium antimonyl tartrate, or tartar emetic, was once used as an anti-schistosomal drug from 1919 on. It was subsequently replaced by praziquantel. Antimony and its compounds are used in several veterinary preparations, such as anthiomaline and lithium antimony thiomalate, as a skin conditioner in ruminants. Antimony has a nourishing or conditioning effect on keratinized tissues in animals.

Antimony-based drugs, such as meglumine antimoniate, are also considered the drugs of choice for treatment of leishmaniasis in domestic animals. Besides having low therapeutic indices, the drugs have minimal penetration of the bone marrow, where some of the "Leishmania" amastigotes reside, and curing the disease – especially the visceral form – is very difficult. Elemental antimony as an antimony pill was once used as a medicine. It could be reused by others after ingestion and elimination.

Antimony(III) sulfide is used in the heads of some safety matches. Antimony sulfides help to stabilize the friction coefficient in automotive brake pad materials. Antimony is used in bullets, bullet tracers, paint, glass art, and as an opacifier in enamel. Antimony-124 is used together with beryllium in neutron sources; the gamma rays emitted by antimony-124 initiate the photodisintegration of beryllium. The emitted neutrons have an average energy of 24 keV. Natural antimony is used in startup neutron sources.

Historically, the powder derived from crushed antimony ("kohl") has been applied to the eyes with a metal rod and with one's spittle, thought by the ancients to aid in curing eye infections. The practice is still seen in Yemen and in other Muslim countries.

Antimony and many of its compounds are toxic, and the effects of antimony poisoning are similar to arsenic poisoning. The toxicity of antimony is far lower than that of arsenic; this might be caused by the significant differences of uptake, metabolism and excretion between arsenic and antimony. The uptake of antimony(III) or antimony(V) in the gastrointestinal tract is at most 20%. Antimony(V) is not quantitatively reduced to antimony(III) in the cell (in fact antimony(III) is oxidised to antimony(V) instead).

Since methylation of antimony does not occur, the excretion of antimony(V) in urine is the main way of elimination. Like arsenic, the most serious effect of acute antimony poisoning is cardiotoxicity and the resulted myocarditis, however it can also manifest as Adams–Stokes syndrome which arsenic does not. Reported cases of intoxication by antimony equivalent to 90 mg antimony potassium tartrate dissolved from enamel has been reported to show only short term effects. An intoxication with 6 g of antimony potassium tartrate was reported to result in death after three days.

Inhalation of antimony dust is harmful and in certain cases may be fatal; in small doses, antimony causes headaches, dizziness, and depression. Larger doses such as prolonged skin contact may cause dermatitis, or damage the kidneys and the liver, causing violent and frequent vomiting, leading to death in a few days.

Antimony is incompatible with strong oxidizing agents, strong acids, halogen acids, chlorine, or fluorine. It should be kept away from heat.

Antimony leaches from polyethylene terephthalate (PET) bottles into liquids. While levels observed for bottled water are below drinking water guidelines, fruit juice concentrates (for which no guidelines are established) produced in the UK were found to contain up to 44.7 µg/L of antimony, well above the EU limits for tap water of 5 µg/L. The guidelines are:

The tolerable daily intake (TDI) proposed by WHO is 6 µg antimony per kilogram of body weight. The immediately dangerous to life or health (IDLH) value for antimony is 50 mg/m.

Certain compounds of antimony appear to be toxic, particularly antimony trioxide and antimony potassium tartrate. Effects may be similar to arsenic poisoning. Occupational exposure may cause respiratory irritation, pneumoconiosis, antimony spots on the skin, gastrointestinal symptoms, and cardiac arrhythmias. In addition, antimony trioxide is potentially carcinogenic to humans.

Adverse health effects have been observed in humans and animals following inhalation, oral, or dermal exposure to antimony and antimony compounds. Antimony toxicity typically occurs either due to occupational exposure, during therapy or from accidental ingestion. It is unclear if antimony can enter the body through the skin. The presence of low levels of antimony in saliva may also be associated with dental decay.




Actinium

Actinium is a chemical element; it has symbol Ac and atomic number 89. It was first isolated by Friedrich Oskar Giesel in 1902, who gave it the name "emanium"; the element got its name by being wrongly identified with a substance André-Louis Debierne found in 1899 and called actinium. Actinium gave the name to the actinide series, a set of 15 elements between actinium and lawrencium in the periodic table. Together with polonium, radium, and radon, actinium was one of the first non-primordial radioactive elements to be isolated.

A soft, silvery-white radioactive metal, actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that prevents further oxidation. As with most lanthanides and many actinides, actinium assumes oxidation state +3 in nearly all its chemical compounds. Actinium is found only in traces in uranium and thorium ores as the isotope Ac, which decays with a half-life of 21.772 years, predominantly emitting beta and sometimes alpha particles, and Ac, which is beta active with a half-life of 6.15 hours. One tonne of natural uranium in ore contains about 0.2 milligrams of actinium-227, and one tonne of thorium contains about 5 nanograms of actinium-228. The close similarity of physical and chemical properties of actinium and lanthanum makes separation of actinium from the ore impractical. Instead, the element is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor. Owing to its scarcity, high price and radioactivity, actinium has no significant industrial use. Its current applications include a neutron source and an agent for radiation therapy.

André-Louis Debierne, a French chemist, announced the discovery of a new element in 1899. He separated it from pitchblende residues left by Marie and Pierre Curie after they had extracted radium. In 1899, Debierne described the substance as similar to titanium and (in 1900) as similar to thorium. Friedrich Oskar Giesel found in 1902 a substance similar to lanthanum and called it "emanium" in 1904. After a comparison of the substances' half-lives determined by Debierne, Harriet Brooks in 1904, and Otto Hahn and Otto Sackur in 1905, Debierne's chosen name for the new element was retained because it had seniority, despite the contradicting chemical properties he claimed for the element at different times.

Articles published in the 1970s and later suggest that Debierne's results published in 1904 conflict with those reported in 1899 and 1900. Furthermore, the now-known chemistry of actinium precludes its presence as anything other than a minor constituent of Debierne's 1899 and 1900 results; in fact, the chemical properties he reported make it likely that he had, instead, accidentally identified protactinium, which would not be discovered for another fourteen years, only to have it disappear due to its hydrolysis and adsorption onto his laboratory equipment. This has led some authors to advocate that Giesel alone should be credited with the discovery. A less confrontational vision of scientific discovery is proposed by Adloff. He suggests that hindsight criticism of the early publications should be mitigated by the then nascent state of radiochemistry: highlighting the prudence of Debierne's claims in the original papers, he notes that nobody can contend that Debierne's substance did not contain actinium. Debierne, who is now considered by the vast majority of historians as the discoverer, lost interest in the element and left the topic. Giesel, on the other hand, can rightfully be credited with the first preparation of radiochemically pure actinium and with the identification of its atomic number 89.

The name actinium originates from the Ancient Greek "aktis, aktinos" (ακτίς, ακτίνος), meaning beam or ray. Its symbol Ac is also used in abbreviations of other compounds that have nothing to do with actinium, such as acetyl, acetate and sometimes acetaldehyde.

Actinium is a soft, silvery-white, radioactive, metallic element. Its estimated shear modulus is similar to that of lead. Owing to its strong radioactivity, actinium glows in the dark with a pale blue light, which originates from the surrounding air ionized by the emitted energetic particles. Actinium has similar chemical properties to lanthanum and other lanthanides, and therefore these elements are difficult to separate when extracting from uranium ores. Solvent extraction and ion chromatography are commonly used for the separation.

The first element of the actinides, actinium gave the set its name, much as lanthanum had done for the lanthanides. The actinides are much more diverse than the lanthanides and therefore it was not until 1945 that the most significant change to Dmitri Mendeleev's periodic table since the recognition of the lanthanides, the introduction of the actinides, was generally accepted after Glenn T. Seaborg's research on the transuranium elements (although it had been proposed as early as 1892 by British chemist Henry Bassett).

Actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that impedes further oxidation. As with most lanthanides and actinides, actinium exists in the oxidation state +3, and the Ac ions are colorless in solutions. The oxidation state +3 originates from the [Rn] 6d7s electronic configuration of actinium, with three valence electrons that are easily donated to give the stable closed-shell structure of the noble gas radon. Although the 5f orbitals are unoccupied in an actinium atom, it can be used as a valence orbital in actinium complexes and hence it is generally considered the first 5f element by authors working on it. Ac is the largest of all known tripositive ions and its first coordination sphere contains approximately 10.9 ± 0.5 water molecules.

Due to actinium's intense radioactivity, only a limited number of actinium compounds are known. These include: AcF, AcCl, AcBr, AcOF, AcOCl, AcOBr, AcS, AcO, AcPO and Ac(NO). They all contain actinium in the oxidation state +3. In particular, the lattice constants of the analogous lanthanum and actinium compounds differ by only a few percent.

Here "a", "b" and "c" are lattice constants, No is space group number and "Z" is the number of formula units per unit cell. Density was not measured directly but calculated from the lattice parameters.

Actinium oxide (AcO) can be obtained by heating the hydroxide at 500 °C or the oxalate at 1100 °C, in vacuum. Its crystal lattice is isotypic with the oxides of most trivalent rare-earth metals.

Actinium trifluoride can be produced either in solution or in solid reaction. The former reaction is carried out at room temperature, by adding hydrofluoric acid to a solution containing actinium ions. In the latter method, actinium metal is treated with hydrogen fluoride vapors at 700 °C in an all-platinum setup. Treating actinium trifluoride with ammonium hydroxide at 900–1000 °C yields oxyfluoride AcOF. Whereas lanthanum oxyfluoride can be easily obtained by burning lanthanum trifluoride in air at 800 °C for an hour, similar treatment of actinium trifluoride yields no AcOF and only results in melting of the initial product.

Actinium trichloride is obtained by reacting actinium hydroxide or oxalate with carbon tetrachloride vapors at temperatures above 960 °C. Similar to oxyfluoride, actinium oxychloride can be prepared by hydrolyzing actinium trichloride with ammonium hydroxide at 1000 °C. However, in contrast to the oxyfluoride, the oxychloride could well be synthesized by igniting a solution of actinium trichloride in hydrochloric acid with ammonia.

Reaction of aluminium bromide and actinium oxide yields actinium tribromide:

and treating it with ammonium hydroxide at 500 °C results in the oxybromide AcOBr.

Actinium hydride was obtained by reduction of actinium trichloride with potassium at 300 °C, and its structure was deduced by analogy with the corresponding LaH hydride. The source of hydrogen in the reaction was uncertain.

Mixing monosodium phosphate (NaHPO) with a solution of actinium in hydrochloric acid yields white-colored actinium phosphate hemihydrate (AcPO·0.5HO), and heating actinium oxalate with hydrogen sulfide vapors at 1400 °C for a few minutes results in a black actinium sulfide AcS. It may possibly be produced by acting with a mixture of hydrogen sulfide and carbon disulfide on actinium oxide at 1000 °C.

Naturally occurring actinium is composed of two radioactive isotopes; (from the radioactive family of ) and (a granddaughter of ). decays mainly as a beta emitter with a very small energy, but in 1.38% of cases it emits an alpha particle, so it can readily be identified through alpha spectrometry. Thirty-three radioisotopes have been identified, the most stable being with a half-life of 21.772 years, with a half-life of 10.0 days and with a half-life of 29.37 hours. All remaining radioactive isotopes have half-lives that are less than 10 hours and the majority of them have half-lives shorter than one minute. The shortest-lived known isotope of actinium is (half-life of 69 nanoseconds) which decays through alpha decay. Actinium also has two known meta states. The most significant isotopes for chemistry are Ac, Ac, and Ac.

Purified comes into equilibrium with its decay products after about a half of year. It decays according to its 21.772-year half-life emitting mostly beta (98.62%) and some alpha particles (1.38%); the successive decay products are part of the actinium series. Owing to the low available amounts, low energy of its beta particles (maximum 44.8 keV) and low intensity of alpha radiation, is difficult to detect directly by its emission and it is therefore traced via its decay products. The isotopes of actinium range in atomic weight from 203 u () to 236 u ().

Actinium is found only in traces in uranium ores – one tonne of uranium in ore contains about 0.2 milligrams of Ac – and in thorium ores, which contain about 5 nanograms of Ac per one tonne of thorium. The actinium isotope Ac is a transient member of the uranium-actinium series decay chain, which begins with the parent isotope U (or Pu) and ends with the stable lead isotope Pb. The isotope Ac is a transient member of the thorium series decay chain, which begins with the parent isotope Th and ends with the stable lead isotope Pb. Another actinium isotope (Ac) is transiently present in the neptunium series decay chain, beginning with Np (or U) and ending with thallium (Tl) and near-stable bismuth (Bi); even though all primordial Np has decayed away, it is continuously produced by neutron knock-out reactions on natural U.

The low natural concentration, and the close similarity of physical and chemical properties to those of lanthanum and other lanthanides, which are always abundant in actinium-bearing ores, render separation of actinium from the ore impractical, and complete separation was never achieved. Instead, actinium is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor.
The reaction yield is about 2% of the radium weight. Ac can further capture neutrons resulting in small amounts of Ac. After the synthesis, actinium is separated from radium and from the products of decay and nuclear fusion, such as thorium, polonium, lead and bismuth. The extraction can be performed with thenoyltrifluoroacetone-benzene solution from an aqueous solution of the radiation products, and the selectivity to a certain element is achieved by adjusting the pH (to about 6.0 for actinium). An alternative procedure is anion exchange with an appropriate resin in nitric acid, which can result in a separation factor of 1,000,000 for radium and actinium vs. thorium in a two-stage process. Actinium can then be separated from radium, with a ratio of about 100, using a low cross-linking cation exchange resin and nitric acid as eluant.

Ac was first produced artificially at the Institute for Transuranium Elements (ITU) in Germany using a cyclotron and at St George Hospital in Sydney using a linac in 2000. This rare isotope has potential applications in radiation therapy and is most efficiently produced by bombarding a radium-226 target with 20–30 MeV deuterium ions. This reaction also yields Ac which however decays with a half-life of 29 hours and thus does not contaminate Ac.

Actinium metal has been prepared by the reduction of actinium fluoride with lithium vapor in vacuum at a temperature between 1100 and 1300 °C. Higher temperatures resulted in evaporation of the product and lower ones lead to an incomplete transformation. Lithium was chosen among other alkali metals because its fluoride is most volatile.

Owing to its scarcity, high price and radioactivity, Ac currently has no significant industrial use, but Ac is currently being studied for use in cancer treatments such as targeted alpha therapies.
Ac is highly radioactive and was therefore studied for use as an active element of radioisotope thermoelectric generators, for example in spacecraft. The oxide of Ac pressed with beryllium is also an efficient neutron source with the activity exceeding that of the standard americium-beryllium and radium-beryllium pairs. In all those applications, Ac (a beta source) is merely a progenitor which generates alpha-emitting isotopes upon its decay. Beryllium captures alpha particles and emits neutrons owing to its large cross-section for the (α,n) nuclear reaction:

The AcBe neutron sources can be applied in a neutron probe – a standard device for measuring the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Such probes are also used in well logging applications, in neutron radiography, tomography and other radiochemical investigations.

Ac is applied in medicine to produce in a reusable generator or can be used alone as an agent for radiation therapy, in particular targeted alpha therapy (TAT). This isotope has a half-life of 10 days, making it much more suitable for radiation therapy than Bi (half-life 46 minutes). Additionally, Ac decays to nontoxic Bi rather than toxic lead, which is the final product in the decay chains of several other candidate isotopes, namely Th, Th, and U. Not only Ac itself, but also its daughters, emit alpha particles which kill cancer cells in the body. The major difficulty with application of Ac was that intravenous injection of simple actinium complexes resulted in their accumulation in the bones and liver for a period of tens of years. As a result, after the cancer cells were quickly killed by alpha particles from Ac, the radiation from the actinium and its daughters might induce new mutations. To solve this problem, Ac was bound to a chelating agent, such as citrate, ethylenediaminetetraacetic acid (EDTA) or diethylene triamine pentaacetic acid (DTPA). This reduced actinium accumulation in the bones, but the excretion from the body remained slow. Much better results were obtained with such chelating agents as HEHA () or DOTA () coupled to trastuzumab, a monoclonal antibody that interferes with the HER2/neu receptor. The latter delivery combination was tested on mice and proved to be effective against leukemia, lymphoma, breast, ovarian, neuroblastoma and prostate cancers.

The medium half-life of Ac (21.77 years) makes it a very convenient radioactive isotope in modeling the slow vertical mixing of oceanic waters. The associated processes cannot be studied with the required accuracy by direct measurements of current velocities (of the order 50 meters per year). However, evaluation of the concentration depth-profiles for different isotopes allows estimating the mixing rates. The physics behind this method is as follows: oceanic waters contain homogeneously dispersed U. Its decay product, Pa, gradually precipitates to the bottom, so that its concentration first increases with depth and then stays nearly constant. Pa decays to Ac; however, the concentration of the latter isotope does not follow the Pa depth profile, but instead increases toward the sea bottom. This occurs because of the mixing processes which raise some additional Ac from the sea bottom. Thus analysis of both Pa and Ac depth profiles allows researchers to model the mixing behavior.

There are theoretical predictions that AcH hydrides (in this case with very high pressure) are a candidate for a near room-temperature superconductor as they have T significantly higher than HS, possibly near 250 K.

Ac is highly radioactive and experiments with it are carried out in a specially designed laboratory equipped with a tight glove box. When actinium trichloride is administered intravenously to rats, about 33% of actinium is deposited into the bones and 50% into the liver. Its toxicity is comparable to, but slightly lower than that of americium and plutonium. For trace quantities, fume hoods with good aeration suffice; for gram amounts, hot cells with shielding from the intense gamma radiation emitted by Ac are necessary.




Americium

Americium is a synthetic chemical element; it has symbol Am and atomic number 95. It is radioactive and a transuranic member of the actinide series in the periodic table, located under the lanthanide element europium and was thus named after the Americas by analogy.

Americium was first produced in 1944 by the group of Glenn T. Seaborg from Berkeley, California, at the Metallurgical Laboratory of the University of Chicago, as part of the Manhattan Project. Although it is the third element in the transuranic series, it was discovered fourth, after the heavier curium. The discovery was kept secret and only released to the public in November 1945. Most americium is produced by uranium or plutonium being bombarded with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 100 grams of americium. It is widely used in commercial ionization chamber smoke detectors, as well as in neutron sources and industrial gauges. Several unusual applications, such as nuclear batteries or fuel for space ships with nuclear propulsion, have been proposed for the isotope Am, but they are as yet hindered by the scarcity and high price of this nuclear isomer.

Americium is a relatively soft radioactive metal with silvery appearance. Its most common isotopes are Am and Am. In chemical compounds, americium usually assumes the oxidation state +3, especially in solutions. Several other oxidation states are known, ranging from +2 to +7, and can be identified by their characteristic optical absorption spectra. The crystal lattices of solid americium and its compounds contain small intrinsic radiogenic defects, due to metamictization induced by self-irradiation with alpha particles, which accumulates with time; this can cause a drift of some material properties over time, more noticeable in older samples.

Although americium was likely produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in late autumn 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Leon O. Morgan, Ralph A. James, and Albert Ghiorso. They used a 60-inch cyclotron at the University of California, Berkeley. The element was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) of the University of Chicago. Following the lighter neptunium, plutonium, and heavier curium, americium was the fourth transuranium element to be discovered. At the time, the periodic table had been restructured by Seaborg to its present layout, containing the actinide row below the lanthanide one. This led to americium being located right below its twin lanthanide element europium; it was thus by analogy named after the Americas: "The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series."

The new element was isolated from its oxides in a complex, multi-step process. First plutonium-239 nitrate (PuNO) solution was coated on a platinum foil of about 0.5 cm area, the solution was evaporated and the residue was converted into plutonium dioxide (PuO) by calcining. After cyclotron irradiation, the coating was dissolved with nitric acid, and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid. Further separation was carried out by ion exchange, yielding a certain isotope of curium. The separation of curium and americium was so painstaking that those elements were initially called by the Berkeley group as "pandemonium" (from Greek for "all demons" or "hell") and "delirium" (from Latin for "madness").

Initial experiments yielded four americium isotopes: Am, Am, Am and Am. Americium-241 was directly obtained from plutonium upon absorption of two neutrons. It decays by emission of a α-particle to Np; the half-life of this decay was first determined as years but then corrected to 432.2 years.

The second isotope Am was produced upon neutron bombardment of the already-created Am. Upon rapid β-decay, Am converts into the isotope of curium Cm (which had been discovered previously). The half-life of this decay was initially determined at 17 hours, which was close to the presently accepted value of 16.02 h.

The discovery of americium and curium in 1944 was closely related to the Manhattan Project; the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children "Quiz Kids" five days before the official presentation at an American Chemical Society meeting on 11 November 1945, when one of the listeners asked whether any new transuranium element besides plutonium and neptunium had been discovered during the war. After the discovery of americium isotopes Am and Am, their production and compounds were patented listing only Seaborg as the inventor. The initial americium samples weighed a few micrograms; they were barely visible and were identified by their radioactivity. The first substantial amounts of metallic americium weighing 40–200 micrograms were not prepared until 1951 by reduction of americium(III) fluoride with barium metal in high vacuum at 1100 °C.

The longest-lived and most common isotopes of americium, Am and Am, have half-lives of 432.2 and 7,370 years, respectively. Therefore, any primordial americium (americium that was present on Earth during its formation) should have decayed by now. Trace amounts of americium probably occur naturally in uranium minerals as a result of neutron capture and beta decay (U → Pu → Pu → Am), though the quantities would be tiny and this has not been confirmed. Extraterrestrial long-lived Cm is probably also deposited on Earth and has Am as one of its intermediate decay products, but again this has not been confirmed.

Existing americium is concentrated in the areas used for the atmospheric nuclear weapons tests conducted between 1945 and 1980, as well as at the sites of nuclear incidents, such as the Chernobyl disaster. For example, the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), revealed high concentrations of various actinides including americium; but due to military secrecy, this result was not published until later, in 1956. Trinitite, the glassy residue left on the desert floor near Alamogordo, New Mexico, after the plutonium-based Trinity nuclear bomb test on 16 July 1945, contains traces of americium-241. Elevated levels of americium were also detected at the crash site of a US Boeing B-52 bomber aircraft, which carried four hydrogen bombs, in 1968 in Greenland.

In other regions, the average radioactivity of surface soil due to residual americium is only about 0.01 picocuries per gram (0.37 mBq/g). Atmospheric americium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 1,900 times higher concentration of americium inside sandy soil particles than in the water present in the soil pores; an even higher ratio was measured in loam soils.

Americium is produced mostly artificially in small quantities, for research purposes. A tonne of spent nuclear fuel contains about 100 grams of various americium isotopes, mostly Am and Am. Their prolonged radioactivity is undesirable for the disposal, and therefore americium, together with other long-lived actinides, must be neutralized. The associated procedure may involve several steps, where americium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure is well known as nuclear transmutation, but it is still being developed for americium. The transuranic elements from americium to fermium occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.

Americium is also one of the elements that have theoretically been detected in Przybylski's Star.

Americium has been produced in small quantities in nuclear reactors for decades, and kilograms of its Am and Am isotopes have been accumulated by now. Nevertheless, since it was first offered for sale in 1962, its price, about of Am, remains almost unchanged owing to the very complex separation procedure. The heavier isotope Am is produced in much smaller amounts; it is thus more difficult to separate, resulting in a higher cost of the order .

Americium is not synthesized directly from uranium – the most common reactor material – but from the plutonium isotope Pu. The latter needs to be produced first, according to the following nuclear process:

The capture of two neutrons by Pu (a so-called (n,γ) reaction), followed by a β-decay, results in Am:

The plutonium present in spent nuclear fuel contains about 12% of Pu. Because it beta-decays to Am, Pu can be extracted and may be used to generate further Am. However, this process is rather slow: half of the original amount of Pu decays to Am after about 15 years, and the Am amount reaches a maximum after 70 years.

The obtained Am can be used for generating heavier americium isotopes by further neutron capture inside a nuclear reactor. In a light water reactor (LWR), 79% of Am converts to Am and 10% to its nuclear isomer Am:
Americium-242 has a half-life of only 16 hours, which makes its further conversion to Am extremely inefficient. The latter isotope is produced instead in a process where Pu captures four neutrons under high neutron flux:

Most synthesis routines yield a mixture of different actinide isotopes in oxide forms, from which isotopes of americium can be separated. In a typical procedure, the spent reactor fuel (e.g. MOX fuel) is dissolved in nitric acid, and the bulk of uranium and plutonium is removed using a PUREX-type extraction (Plutonium–URanium EXtraction) with tributyl phosphate in a hydrocarbon. The lanthanides and remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction, to give, after stripping, a mixture of trivalent actinides and lanthanides. Americium compounds are then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. A large amount of work has been done on the solvent extraction of americium. For example, a 2003 EU-funded project codenamed "EUROPART" studied triazines and other compounds as potential extraction agents. A "bis"-triazinyl bipyridine complex was proposed in 2009 as such a reagent is highly selective to americium (and curium). Separation of americium from the highly similar curium can be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone, at elevated temperatures. Both Am and Cm are mostly present in solutions in the +3 valence state; whereas curium remains unchanged, americium oxidizes to soluble Am(IV) complexes which can be washed away.

Metallic americium is obtained by reduction from its compounds. Americium(III) fluoride was first used for this purpose. The reaction was conducted using elemental barium as reducing agent in a water- and oxygen-free environment inside an apparatus made of tantalum and tungsten.

An alternative is the reduction of americium dioxide by metallic lanthanum or thorium:

In the periodic table, americium is located to the right of plutonium, to the left of curium, and below the lanthanide europium, with which it shares many physical and chemical properties. Americium is a highly radioactive element. When freshly prepared, it has a silvery-white metallic lustre, but then slowly tarnishes in air. With a density of 12 g/cm, americium is less dense than both curium (13.52 g/cm) and plutonium (19.8 g/cm); but has a higher density than europium (5.264 g/cm)—mostly because of its higher atomic mass. Americium is relatively soft and easily deformable and has a significantly lower bulk modulus than the actinides before it: Th, Pa, U, Np and Pu. Its melting point of 1173 °C is significantly higher than that of plutonium (639 °C) and europium (826 °C), but lower than for curium (1340 °C).

At ambient conditions, americium is present in its most stable α form which has a hexagonal crystal symmetry, and a space group P6/mmc with cell parameters "a" = 346.8 pm and "c" = 1124 pm, and four atoms per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum and several actinides such as α-curium. The crystal structure of americium changes with pressure and temperature. When compressed at room temperature to 5 GPa, α-Am transforms to the β modification, which has a face-centered cubic ("fcc") symmetry, space group Fmm and lattice constant "a" = 489 pm. This "fcc" structure is equivalent to the closest packing with the sequence ABC. Upon further compression to 23 GPa, americium transforms to an orthorhombic γ-Am structure similar to that of α-uranium. There are no further transitions observed up to 52 GPa, except for an appearance of a monoclinic phase at pressures between 10 and 15 GPa. There is no consistency on the status of this phase in the literature, which also sometimes lists the α, β and γ phases as I, II and III. The β-γ transition is accompanied by a 6% decrease in the crystal volume; although theory also predicts a significant volume change for the α-β transition, it is not observed experimentally. The pressure of the α-β transition decreases with increasing temperature, and when α-americium is heated at ambient pressure, at 770 °C it changes into an "fcc" phase which is different from β-Am, and at 1075 °C it converts to a body-centered cubic structure. The pressure-temperature phase diagram of americium is thus rather similar to those of lanthanum, praseodymium and neodymium.

As with many other actinides, self-damage of the crystal structure due to alpha-particle irradiation is intrinsic to americium. It is especially noticeable at low temperatures, where the mobility of the produced structure defects is relatively low, by broadening of X-ray diffraction peaks. This effect makes somewhat uncertain the temperature of americium and some of its properties, such as electrical resistivity. So for americium-241, the resistivity at 4.2 K increases with time from about 2 µOhm·cm to 10 µOhm·cm after 40 hours, and saturates at about 16 µOhm·cm after 140 hours. This effect is less pronounced at room temperature, due to annihilation of radiation defects; also heating to room temperature the sample which was kept for hours at low temperatures restores its resistivity. In fresh samples, the resistivity gradually increases with temperature from about 2 µOhm·cm at liquid helium to 69 µOhm·cm at room temperature; this behavior is similar to that of neptunium, uranium, thorium and protactinium, but is different from plutonium and curium which show a rapid rise up to 60 K followed by saturation. The room temperature value for americium is lower than that of neptunium, plutonium and curium, but higher than for uranium, thorium and protactinium.

Americium is paramagnetic in a wide temperature range, from that of liquid helium, to room temperature and above. This behavior is markedly different from that of its neighbor curium which exhibits antiferromagnetic transition at 52 K. The thermal expansion coefficient of americium is slightly anisotropic and amounts to along the shorter "a" axis and for the longer "c" hexagonal axis. The enthalpy of dissolution of americium metal in hydrochloric acid at standard conditions is , from which the standard enthalpy change of formation (Δ"H"°) of aqueous Am ion is . The standard potential Am/Am is .

Americium metal readily reacts with oxygen and dissolves in aqueous acids. The most stable oxidation state for americium is +3. The chemistry of americium(III) has many similarities to the chemistry of lanthanide(III) compounds. For example, trivalent americium forms insoluble fluoride, oxalate, iodate, hydroxide, phosphate and other salts. Compounds of americium in oxidation states 2, 4, 5, 6 and 7 have also been studied. This is the widest range that has been observed with actinide elements. The color of americium compounds in aqueous solution is as follows: Am (yellow-reddish), Am (yellow-reddish), ; (yellow), (brown) and (dark green). The absorption spectra have sharp peaks, due to "f"-"f" transitions' in the visible and near-infrared regions. Typically, Am(III) has absorption maxima at ca. 504 and 811 nm, Am(V) at ca. 514 and 715 nm, and Am(VI) at ca. 666 and 992 nm.

Americium compounds with oxidation state +4 and higher are strong oxidizing agents, comparable in strength to the permanganate ion () in acidic solutions. Whereas the Am ions are unstable in solutions and readily convert to Am, compounds such as americium dioxide (AmO) and americium(IV) fluoride (AmF) are stable in the solid state.

The pentavalent oxidation state of americium was first observed in 1951. In acidic aqueous solution the ion is unstable with respect to disproportionation. The reaction

is typical. The chemistry of Am(V) and Am(VI) is comparable to the chemistry of uranium in those oxidation states. In particular, compounds like and are comparable to uranates and the ion is comparable to the uranyl ion, . Such compounds can be prepared by oxidation of Am(III) in dilute nitric acid with ammonium persulfate. Other oxidising agents that have been used include silver(I) oxide, ozone and sodium persulfate.

Three americium oxides are known, with the oxidation states +2 (AmO), +3 (AmO) and +4 (AmO). Americium(II) oxide was prepared in minute amounts and has not been characterized in detail. Americium(III) oxide is a red-brown solid with a melting point of 2205 °C. Americium(IV) oxide is the main form of solid americium which is used in nearly all its applications. As most other actinide dioxides, it is a black solid with a cubic (fluorite) crystal structure.

The oxalate of americium(III), vacuum dried at room temperature, has the chemical formula Am(CO)·7HO. Upon heating in vacuum, it loses water at 240 °C and starts decomposing into AmO at 300 °C, the decomposition completes at about 470 °C. The initial oxalate dissolves in nitric acid with the maximum solubility of 0.25 g/L.

Halides of americium are known for the oxidation states +2, +3 and +4, where the +3 is most stable, especially in solutions.

Reduction of Am(III) compounds with sodium amalgam yields Am(II) salts – the black halides AmCl, AmBr and AmI. They are very sensitive to oxygen and oxidize in water, releasing hydrogen and converting back to the Am(III) state. Specific lattice constants are:

Americium(III) fluoride (AmF) is poorly soluble and precipitates upon reaction of Am and fluoride ions in weak acidic solutions:

The tetravalent americium(IV) fluoride (AmF) is obtained by reacting solid americium(III) fluoride with molecular fluorine:

Another known form of solid tetravalent americium fluoride is KAmF. Tetravalent americium has also been observed in the aqueous phase. For this purpose, black Am(OH) was dissolved in 15-M NHF with the americium concentration of 0.01 M. The resulting reddish solution had a characteristic optical absorption spectrum which is similar to that of AmF but differed from other oxidation states of americium. Heating the Am(IV) solution to 90 °C did not result in its disproportionation or reduction, however a slow reduction was observed to Am(III) and assigned to self-irradiation of americium by alpha particles.

Most americium(III) halides form hexagonal crystals with slight variation of the color and exact structure between the halogens. So, chloride (AmCl) is reddish and has a structure isotypic to uranium(III) chloride (space group P6/m) and the melting point of 715 °C. The fluoride is isotypic to LaF (space group P6/mmc) and the iodide to BiI (space group R). The bromide is an exception with the orthorhombic PuBr-type structure and space group Cmcm. Crystals of americium hexahydrate (AmCl·6HO) can be prepared by dissolving americium dioxide in hydrochloric acid and evaporating the liquid. Those crystals are hygroscopic and have yellow-reddish color and a monoclinic crystal structure.

Oxyhalides of americium in the form AmOX, AmOX, AmOX and AmOX can be obtained by reacting the corresponding americium halide with oxygen or SbO, and AmOCl can also be produced by vapor phase hydrolysis:

The known chalcogenides of americium include the sulfide AmS, selenides AmSe and AmSe, and tellurides AmTe and AmTe. The pnictides of americium (Am) of the AmX type are known for the elements phosphorus, arsenic, antimony and bismuth. They crystallize in the rock-salt lattice.

Americium monosilicide (AmSi) and "disilicide" (nominally AmSi with: 1.87 < x < 2.0) were obtained by reduction of americium(III) fluoride with elementary silicon in vacuum at 1050 °C (AmSi) and 1150−1200 °C (AmSi). AmSi is a black solid isomorphic with LaSi, it has an orthorhombic crystal symmetry. AmSi has a bright silvery lustre and a tetragonal crystal lattice (space group "I"4/amd), it is isomorphic with PuSi and ThSi. Borides of americium include AmB and AmB. The tetraboride can be obtained by heating an oxide or halide of americium with magnesium diboride in vacuum or inert atmosphere.

Analogous to uranocene, americium forms the organometallic compound amerocene with two cyclooctatetraene ligands, with the chemical formula (η-CH)Am. A cyclopentadienyl complex is also known that is likely to be stoichiometrically AmCp.

Formation of the complexes of the type Am(n-CH-BTP), where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-CH-BTP and Am ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with americium and therefore are useful in its selective separation from lanthanides and another actinides.

Americium is an artificial element of recent origin, and thus does not have a biological requirement. It is harmful to life. It has been proposed to use bacteria for removal of americium and other heavy metals from rivers and streams. Thus, Enterobacteriaceae of the genus "Citrobacter" precipitate americium ions from aqueous solutions, binding them into a metal-phosphate complex at their cell walls. Several studies have been reported on the biosorption and bioaccumulation of americium by bacteria and fungi.

The isotope Am (half-life 141 years) has the largest cross sections for absorption of thermal neutrons (5,700 barns), that results in a small critical mass for a sustained nuclear chain reaction. The critical mass for a bare Am sphere is about 9–14 kg (the uncertainty results from insufficient knowledge of its material properties). It can be lowered to 3–5 kg with a metal reflector and should become even smaller with a water reflector. Such small critical mass is favorable for portable nuclear weapons, but those based on Am are not known yet, probably because of its scarcity and high price. The critical masses of the two readily available isotopes, Am and Am, are relatively high – 57.6 to 75.6 kg for Am and 209 kg for Am. Scarcity and high price yet hinder application of americium as a nuclear fuel in nuclear reactors.

There are proposals of very compact 10-kW high-flux reactors using as little as 20 grams of Am. Such low-power reactors would be relatively safe to use as neutron sources for radiation therapy in hospitals.

About 19 isotopes and 11 nuclear isomers are known for americium, including mass numbers 223, 229, 230, and 232 through 247. There are two long-lived alpha-emitters; Am has a half-life of 7,370 years and is the most stable isotope, and Am has a half-life of 432.2 years. The most stable nuclear isomer is Am; it has a long half-life of 141 years. The half-lives of other isotopes and isomers range from 0.64 microseconds for Am to 50.8 hours for Am. As with most other actinides, the isotopes of americium with odd number of neutrons have relatively high rate of nuclear fission and low critical mass.

Americium-241 decays to Np emitting alpha particles of 5 different energies, mostly at 5.486 MeV (85.2%) and 5.443 MeV (12.8%). Because many of the resulting states are metastable, they also emit gamma rays with the discrete energies between 26.3 and 158.5 keV.

Americium-242 is a short-lived isotope with a half-life of 16.02 h. It mostly (82.7%) converts by β-decay to Cm, but also by electron capture to Pu (17.3%). Both Cm and Pu transform via nearly the same decay chain through Pu down to U.

Nearly all (99.541%) of Am decays by internal conversion to Am and the remaining 0.459% by α-decay to Np. The latter subsequently decays to Pu and then to U.

Americium-243 transforms by α-emission into Np, which converts by β-decay to Pu, and the Pu changes into U by emitting an α-particle.

Americium is used in the most common type of household smoke detector, which uses Am in the form of americium dioxide as its source of ionizing radiation. This isotope is preferred over Ra because it emits 5 times more alpha particles and relatively little harmful gamma radiation.

The amount of americium in a typical new smoke detector is 1 microcurie (37 kBq) or 0.29 microgram. This amount declines slowly as the americium decays into neptunium-237, a different transuranic element with a much longer half-life (about 2.14 million years). With its half-life of 432.2 years, the americium in a smoke detector includes about 3% neptunium after 19 years, and about 5% after 32 years. The radiation passes through an ionization chamber, an air-filled space between two electrodes, and permits a small, constant current between the electrodes. Any smoke that enters the chamber absorbs the alpha particles, which reduces the ionization and affects this current, triggering the alarm. Compared to the alternative optical smoke detector, the ionization smoke detector is cheaper and can detect particles which are too small to produce significant light scattering; however, it is more prone to false alarms.

As Am has a roughly similar half-life to Pu (432.2 years vs. 87 years), it has been proposed as an active element of radioisotope thermoelectric generators, for example in spacecraft. Although americium produces less heat and electricity – the power yield is 114.7 mW/g for Am and 6.31 mW/g for Am (cf. 390 mW/g for Pu) – and its radiation poses more threat to humans owing to neutron emission, the European Space Agency is considering using americium for its space probes.

Another proposed space-related application of americium is a fuel for space ships with nuclear propulsion. It relies on the very high rate of nuclear fission of Am, which can be maintained even in a micrometer-thick foil. Small thickness avoids the problem of self-absorption of emitted radiation. This problem is pertinent to uranium or plutonium rods, in which only surface layers provide alpha-particles. The fission products of Am can either directly propel the spaceship or they can heat a thrusting gas. They can also transfer their energy to a fluid and generate electricity through a magnetohydrodynamic generator.

One more proposal which utilizes the high nuclear fission rate of Am is a nuclear battery. Its design relies not on the energy of the emitted by americium alpha particles, but on their charge, that is the americium acts as the self-sustaining "cathode". A single 3.2 kg Am charge of such battery could provide about 140 kW of power over a period of 80 days. Even with all the potential benefits, the current applications of Am are as yet hindered by the scarcity and high price of this particular nuclear isomer.

In 2019, researchers at the UK National Nuclear Laboratory and the University of Leicester demonstrated the use of heat generated by americium to illuminate a small light bulb. This technology could lead to systems to power missions with durations up to 400 years into interstellar space, where solar panels do not function.

The oxide of Am pressed with beryllium is an efficient neutron source. Here americium acts as the alpha source, and beryllium produces neutrons owing to its large cross-section for the (α,n) nuclear reaction:

The most widespread use of AmBe neutron sources is a neutron probe – a device used to measure the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Am neutron sources are also used in well logging applications, as well as in neutron radiography, tomography and other radiochemical investigations.

Americium is a starting material for the production of other transuranic elements and transactinides – for example, 82.7% of Am decays to Cm and 17.3% to Pu. In the nuclear reactor, Am is also up-converted by neutron capture to Am and Am, which transforms by β-decay to Cm:

Irradiation of Am by C or Ne ions yields the isotopes Es (einsteinium) or Db (dubnium), respectively. Furthermore, the element berkelium (Bk isotope) had been first intentionally produced and identified by bombarding Am with alpha particles, in 1949, by the same Berkeley group, using the same 60-inch cyclotron. Similarly, nobelium was produced at the Joint Institute for Nuclear Research, Dubna, Russia, in 1965 in several reactions, one of which included irradiation of Am with N ions. Besides, one of the synthesis reactions for lawrencium, discovered by scientists at Berkeley and Dubna, included bombardment of Am with O.

Americium-241 has been used as a portable source of both gamma rays and alpha particles for a number of medical and industrial uses. The 59.5409 keV gamma ray emissions from Am in such sources can be used for indirect analysis of materials in radiography and X-ray fluorescence spectroscopy, as well as for quality control in fixed nuclear density gauges and nuclear densometers. For example, the element has been employed to gauge glass thickness to help create flat glass. Americium-241 is also suitable for calibration of gamma-ray spectrometers in the low-energy range, since its spectrum consists of nearly a single peak and negligible Compton continuum (at least three orders of magnitude lower intensity). Americium-241 gamma rays were also used to provide passive diagnosis of thyroid function. This medical application is however obsolete.

As a highly radioactive element, americium and its compounds must be handled only in an appropriate laboratory under special arrangements. Although most americium isotopes predominantly emit alpha particles which can be blocked by thin layers of common materials, many of the daughter products emit gamma-rays and neutrons which have a long penetration depth.

If consumed, most of the americium is excreted within a few days, with only 0.05% absorbed in the blood, of which roughly 45% goes to the liver and 45% to the bones, and the remaining 10% is excreted. The uptake to the liver depends on the individual and increases with age. In the bones, americium is first deposited over cortical and trabecular surfaces and slowly redistributes over the bone with time. The biological half-life of Am is 50 years in the bones and 20 years in the liver, whereas in the gonads (testicles and ovaries) it remains permanently; in all these organs, americium promotes formation of cancer cells as a result of its radioactivity.

Americium often enters landfills from discarded smoke detectors. The rules associated with the disposal of smoke detectors are relaxed in most jurisdictions. In 1994, 17-year-old David Hahn extracted the americium from about 100 smoke detectors in an attempt to build a breeder nuclear reactor. There have been a few cases of exposure to americium, the worst case being that of chemical operations technician Harold McCluskey, who at the age of 64 was exposed to 500 times the occupational standard for americium-241 as a result of an explosion in his lab. McCluskey died at the age of 75 of unrelated pre-existing disease.





Astatine

Astatine is a chemical element; it has symbol At and atomic number 85. It is the rarest naturally occurring element in the Earth's crust, occurring only as the decay product of various heavier elements. All of astatine's isotopes are short-lived; the most stable is astatine-210, with a half-life of 8.1 hours. Consequently, a solid sample of the element has never been seen, because any macroscopic specimen would be immediately vaporized by the heat of its radioactivity.

The bulk properties of astatine are not known with certainty. Many of them have been estimated from its position on the periodic table as a heavier analog of fluorine, chlorine, bromine, and iodine, the four stable halogens. However, astatine also falls roughly along the dividing line between metals and nonmetals, and some metallic behavior has also been observed and predicted for it. Astatine is likely to have a dark or lustrous appearance and may be a semiconductor or possibly a metal. Chemically, several anionic species of astatine are known and most of its compounds resemble those of iodine, but it also sometimes displays metallic characteristics and shows some similarities to silver.

The first synthesis of astatine was in 1940 by Dale R. Corson, Kenneth Ross MacKenzie, and Emilio G. Segrè at the University of California, Berkeley. They named it from the Ancient Greek () 'unstable'. Four isotopes of astatine were subsequently found to be naturally occurring, although much less than one gram is present at any given time in the Earth's crust. Neither the most stable isotope, astatine-210, nor the medically useful astatine-211 occur naturally; they are usually produced by bombarding bismuth-209 with alpha particles.

Astatine is an extremely radioactive element; all its isotopes have half-lives of 8.1 hours or less, decaying into other astatine isotopes, bismuth, polonium, or radon. Most of its isotopes are very unstable, with half-lives of seconds or less. Of the first 101 elements in the periodic table, only francium is less stable, and all the astatine isotopes more stable than the longest-lived francium isotopes (At) are in any case synthetic and do not occur in nature.

The bulk properties of astatine are not known with any certainty. Research is limited by its short half-life, which prevents the creation of weighable quantities. A visible piece of astatine would immediately vaporize itself because of the heat generated by its intense radioactivity. It remains to be seen if, with sufficient cooling, a macroscopic quantity of astatine could be deposited as a thin film. Astatine is usually classified as either a nonmetal or a metalloid; metal formation has also been predicted.

Most of the physical properties of astatine have been estimated (by interpolation or extrapolation), using theoretically or empirically derived methods. For example, halogens get darker with increasing atomic weight – fluorine is nearly colorless, chlorine is yellow-green, bromine is red-brown, and iodine is dark gray/violet. Astatine is sometimes described as probably being a black solid (assuming it follows this trend), or as having a metallic appearance (if it is a metalloid or a metal).

Astatine sublimes less readily than iodine, having a lower vapor pressure. Even so, half of a given quantity of astatine will vaporize in approximately an hour if put on a clean glass surface at room temperature. The absorption spectrum of astatine in the middle ultraviolet region has lines at 224.401 and 216.225 nm, suggestive of 6p to 7s transitions.

The structure of solid astatine is unknown. As an analog of iodine it may have an orthorhombic crystalline structure composed of diatomic astatine molecules, and be a semiconductor (with a band gap of 0.7 eV). Alternatively, if condensed astatine forms a metallic phase, as has been predicted, it may have a monatomic face-centered cubic structure; in this structure, it may well be a superconductor, like the similar high-pressure phase of iodine. Metallic astatine is expected to have a density of 8.91–8.95 g/cm.

Evidence for (or against) the existence of diatomic astatine (At) is sparse and inconclusive. Some sources state that it does not exist, or at least has never been observed, while other sources assert or imply its existence. Despite this controversy, many properties of diatomic astatine have been predicted; for example, its bond length would be , dissociation energy , and heat of vaporization (∆H) 54.39 kJ/mol. Many values have been predicted for the melting and boiling points of astatine, but only for At.

The chemistry of astatine is "clouded by the extremely low concentrations at which astatine experiments have been conducted, and the possibility of reactions with impurities, walls and filters, or radioactivity by-products, and other unwanted nano-scale interactions". Many of its apparent chemical properties have been observed using tracer studies on extremely dilute astatine solutions, typically less than 10 mol·L. Some properties, such as anion formation, align with other halogens. Astatine has some metallic characteristics as well, such as plating onto a cathode, and coprecipitating with metal sulfides in hydrochloric acid. It forms complexes with EDTA, a metal chelating agent, and is capable of acting as a metal in antibody radiolabeling; in some respects, astatine in the +1 state is akin to silver in the same state. Most of the organic chemistry of astatine is, however, analogous to that of iodine. It has been suggested that astatine can form a stable monatomic cation in aqueous solution.

Astatine has an electronegativity of 2.2 on the revised Pauling scale – lower than that of iodine (2.66) and the same as hydrogen. In hydrogen astatide (HAt), the negative charge is predicted to be on the hydrogen atom, implying that this compound could be referred to as astatine hydride according to certain nomenclatures. That would be consistent with the electronegativity of astatine on the Allred–Rochow scale (1.9) being less than that of hydrogen (2.2). However, official IUPAC stoichiometric nomenclature is based on an idealized convention of determining the relative electronegativities of the elements by the mere virtue of their position within the periodic table. According to this convention, astatine is handled as though it is more electronegative than hydrogen, irrespective of its true electronegativity. The electron affinity of astatine, at 233 kJ mol, is 21% less than that of iodine. In comparison, the value of Cl (349) is 6.4% higher than F (328); Br (325) is 6.9% less than Cl; and I (295) is 9.2% less than Br. The marked reduction for At was predicted as being due to spin–orbit interactions. The first ionization energy of astatine is about 899 kJ mol, which continues the trend of decreasing first ionization energies down the halogen group (fluorine, 1681; chlorine, 1251; bromine, 1140; iodine, 1008).

Less reactive than iodine, astatine is the least reactive of the halogens; the chemical properties of tennessine, the next-heavier group 17 element, have not yet been investigated, however. Astatine compounds have been synthesized in nano-scale amounts and studied as intensively as possible before their radioactive disintegration. The reactions involved have been typically tested with dilute solutions of astatine mixed with larger amounts of iodine. Acting as a carrier, the iodine ensures there is sufficient material for laboratory techniques (such as filtration and precipitation) to work. Like iodine, astatine has been shown to adopt odd-numbered oxidation states ranging from −1 to +7.

Only a few compounds with metals have been reported, in the form of astatides of sodium, palladium, silver, thallium, and lead. Some characteristic properties of silver and sodium astatide, and the other hypothetical alkali and alkaline earth astatides, have been estimated by extrapolation from other metal halides.

The formation of an astatine compound with hydrogen – usually referred to as hydrogen astatide – was noted by the pioneers of astatine chemistry. As mentioned, there are grounds for instead referring to this compound as astatine hydride. It is easily oxidized; acidification by dilute nitric acid gives the At or At forms, and the subsequent addition of silver(I) may only partially, at best, precipitate astatine as silver(I) astatide (AgAt). Iodine, in contrast, is not oxidized, and precipitates readily as silver(I) iodide.

Astatine is known to bind to boron, carbon, and nitrogen. Various boron cage compounds have been prepared with At–B bonds, these being more stable than At–C bonds. Astatine can replace a hydrogen atom in benzene to form astatobenzene CHAt; this may be oxidized to CHAtCl by chlorine. By treating this compound with an alkaline solution of hypochlorite, CHAtO can be produced. The dipyridine-astatine(I) cation, [At(CHN)], forms ionic compounds with perchlorate (a non-coordinating anion) and with nitrate, [At(CHN)]NO. This cation exists as a coordination complex in which two dative covalent bonds separately link the astatine(I) centre with each of the pyridine rings via their nitrogen atoms.

With oxygen, there is evidence of the species AtO and AtO in aqueous solution, formed by the reaction of astatine with an oxidant such as elemental bromine or (in the last case) by sodium persulfate in a solution of perchloric acid. The species previously thought to be has since been determined to be , a hydrolysis product of AtO (another such hydrolysis product being AtOOH). The well characterized anion can be obtained by, for example, the oxidation of astatine with potassium hypochlorite in a solution of potassium hydroxide. Preparation of lanthanum triastatate La(AtO), following the oxidation of astatine by a hot NaSO solution, has been reported. Further oxidation of , such as by xenon difluoride (in a hot alkaline solution) or periodate (in a neutral or alkaline solution), yields the perastatate ion ; this is only stable in neutral or alkaline solutions. Astatine is also thought to be capable of forming cations in salts with oxyanions such as iodate or dichromate; this is based on the observation that, in acidic solutions, monovalent or intermediate positive states of astatine coprecipitate with the insoluble salts of metal cations such as silver(I) iodate or thallium(I) dichromate.

Astatine may form bonds to the other chalcogens; these include SAt and with sulfur, a coordination selenourea compound with selenium, and an astatine–tellurium colloid with tellurium.

Astatine is known to react with its lighter homologs iodine, bromine, and chlorine in the vapor state; these reactions produce diatomic interhalogen compounds with formulas AtI, AtBr, and AtCl. The first two compounds may also be produced in water – astatine reacts with iodine/iodide solution to form AtI, whereas AtBr requires (aside from astatine) an iodine/iodine monobromide/bromide solution. The excess of iodides or bromides may lead to and ions, or in a chloride solution, they may produce species like or via equilibrium reactions with the chlorides. Oxidation of the element with dichromate (in nitric acid solution) showed that adding chloride turned the astatine into a molecule likely to be either AtCl or AtOCl. Similarly, or may be produced. The polyhalides PdAtI, CsAtI, TlAtI, and PbAtI are known or presumed to have been precipitated. In a plasma ion source mass spectrometer, the ions [AtI], [AtBr], and [AtCl] have been formed by introducing lighter halogen vapors into a helium-filled cell containing astatine, supporting the existence of stable neutral molecules in the plasma ion state. No astatine fluorides have been discovered yet. Their absence has been speculatively attributed to the extreme reactivity of such compounds, including the reaction of an initially formed fluoride with the walls of the glass container to form a non-volatile product. Thus, although the synthesis of an astatine fluoride is thought to be possible, it may require a liquid halogen fluoride solvent, as has already been used for the characterization of radon fluoride.

In 1869, when Dmitri Mendeleev published his periodic table, the space under iodine was empty; after Niels Bohr established the physical basis of the classification of chemical elements, it was suggested that the fifth halogen belonged there. Before its officially recognized discovery, it was called "eka-iodine" (from Sanskrit "eka" – "one") to imply it was one space under iodine (in the same manner as eka-silicon, eka-boron, and others). Scientists tried to find it in nature; given its extreme rarity, these attempts resulted in several false discoveries.

The first claimed discovery of eka-iodine was made by Fred Allison and his associates at the Alabama Polytechnic Institute (now Auburn University) in 1931. The discoverers named element 85 "alabamine", and assigned it the symbol Ab, designations that were used for a few years. In 1934, H. G. MacPherson of University of California, Berkeley disproved Allison's method and the validity of his discovery. There was another claim in 1937, by the chemist Rajendralal De. Working in Dacca in British India (now Dhaka in Bangladesh), he chose the name "dakin" for element 85, which he claimed to have isolated as the thorium series equivalent of radium F (polonium-210) in the radium series. The properties he reported for dakin do not correspond to those of astatine, and astatine's radioactivity would have prevented him from handling it in the quantities he claimed. Moreover, astatine is not found in the thorium series, and the true identity of dakin is not known.

In 1936, the team of Romanian physicist Horia Hulubei and French physicist Yvette Cauchois claimed to have discovered element 85 by observing its X-ray emission lines. In 1939, they published another paper which supported and extended previous data. In 1944, Hulubei published a summary of data he had obtained up to that time, claiming it was supported by the work of other researchers. He chose the name "dor", presumably from the Romanian for "longing" [for peace], as World War II had started five years earlier. As Hulubei was writing in French, a language which does not accommodate the "ine" suffix, dor would likely have been rendered in English as "dorine", had it been adopted. In 1947, Hulubei's claim was effectively rejected by the Austrian chemist Friedrich Paneth, who would later chair the IUPAC committee responsible for recognition of new elements. Even though Hulubei's samples did contain astatine-218, his means to detect it were too weak, by current standards, to enable correct identification; moreover, he could not perform chemical tests on the element. He had also been involved in an earlier false claim as to the discovery of element 87 (francium) and this is thought to have caused other researchers to downplay his work.
In 1940, the Swiss chemist Walter Minder announced the discovery of element 85 as the beta decay product of radium A (polonium-218), choosing the name "helvetium" (from , the Latin name of Switzerland). Berta Karlik and Traude Bernert were unsuccessful in reproducing his experiments, and subsequently attributed Minder's results to contamination of his radon stream (radon-222 is the parent isotope of polonium-218). In 1942, Minder, in collaboration with the English scientist Alice Leigh-Smith, announced the discovery of another isotope of element 85, presumed to be the product of thorium A (polonium-216) beta decay. They named this substance "anglo-helvetium", but Karlik and Bernert were again unable to reproduce these results.

Later in 1940, Dale R. Corson, Kenneth Ross MacKenzie, and Emilio Segrè isolated the element at the University of California, Berkeley. Instead of searching for the element in nature, the scientists created it by bombarding bismuth-209 with alpha particles in a cyclotron (particle accelerator) to produce, after emission of two neutrons, astatine-211. The discoverers, however, did not immediately suggest a name for the element. The reason for this was that at the time, an element created synthetically in "invisible quantities" that had not yet been discovered in nature was not seen as a completely valid one; in addition, chemists were reluctant to recognize radioactive isotopes as legitimately as stable ones. In 1943, astatine was found as a product of two naturally occurring decay chains by Berta Karlik and Traude Bernert, first in the so-called uranium series, and then in the actinium series. (Since then, astatine was also found in a third decay chain, the neptunium series.) Friedrich Paneth in 1946 called to finally recognize synthetic elements, quoting, among other reasons, recent confirmation of their natural occurrence, and proposed that the discoverers of the newly discovered unnamed elements name these elements. In early 1947, "Nature" published the discoverers' suggestions; a letter from Corson, MacKenzie, and Segrè suggested the name "astatine" coming from the Ancient Greek () meaning , because of its propensity for radioactive decay, with the ending "-ine", found in the names of the four previously discovered halogens. The name was also chosen to continue the tradition of the four stable halogens, where the name referred to a property of the element.

Corson and his colleagues classified astatine as a metal on the basis of its analytical chemistry. Subsequent investigators reported iodine-like, cationic, or amphoteric behavior. In a 2003 retrospective, Corson wrote that "some of the properties [of astatine] are similar to iodine ... it also exhibits metallic properties, more like its metallic neighbors Po and Bi."

There are 41 known isotopes of astatine, with mass numbers of 188 and 190–229. Theoretical modeling suggests that about 37 more isotopes could exist. No stable or long-lived astatine isotope has been observed, nor is one expected to exist.

Astatine's alpha decay energies follow the same trend as for other heavy elements. Lighter astatine isotopes have quite high energies of alpha decay, which become lower as the nuclei become heavier. Astatine-211 has a significantly higher energy than the previous isotope, because it has a nucleus with 126 neutrons, and 126 is a magic number corresponding to a filled neutron shell. Despite having a similar half-life to the previous isotope (8.1 hours for astatine-210 and 7.2 hours for astatine-211), the alpha decay probability is much higher for the latter: 41.81% against only 0.18%. The two following isotopes release even more energy, with astatine-213 releasing the most energy. For this reason, it is the shortest-lived astatine isotope. Even though heavier astatine isotopes release less energy, no long-lived astatine isotope exists, because of the increasing role of beta decay (electron emission). This decay mode is especially important for astatine; as early as 1950 it was postulated that all isotopes of the element undergo beta decay, though nuclear mass measurements indicate that At is in fact beta-stable, as it has the lowest mass of all isobars with "A" = 215. Astatine-210 and most of the lighter isotopes exhibit beta plus decay (positron emission), astatine-217 and heavier isotopes except astatine-218 exhibit beta minus decay, while astatine-211 undergoes electron capture.

The most stable isotope is astatine-210, which has a half-life of 8.1 hours. The primary decay mode is beta plus, to the relatively long-lived (in comparison to astatine isotopes) alpha emitter polonium-210. In total, only five isotopes have half-lives exceeding one hour (astatine-207 to -211). The least stable ground state isotope is astatine-213, with a half-life of 125 nanoseconds. It undergoes alpha decay to the extremely long-lived bismuth-209.

Astatine has 24 known nuclear isomers, which are nuclei with one or more nucleons (protons or neutrons) in an excited state. A nuclear isomer may also be called a "meta-state", meaning the system has more internal energy than the "ground state" (the state with the lowest possible internal energy), making the former likely to decay into the latter. There may be more than one isomer for each isotope. The most stable of these nuclear isomers is astatine-202m1, which has a half-life of about 3 minutes, longer than those of all the ground states bar those of isotopes 203–211 and 220. The least stable is astatine-213m1; its half-life of 110 nanoseconds is shorter than 125 nanoseconds for astatine-213, the shortest-lived ground state.

Astatine is the rarest naturally occurring element. The total amount of astatine in the Earth's crust (quoted mass 2.36 × 10 grams) is estimated by some to be less than one gram at any given time. Other sources estimate the amount of ephemeral astatine, present on earth at any given moment, to be up to one ounce (about 28 grams).

Any astatine present at the formation of the Earth has long since disappeared; the four naturally occurring isotopes (astatine-215, -217, -218 and -219) are instead continuously produced as a result of the decay of radioactive thorium and uranium ores, and trace quantities of neptunium-237. The landmass of North and South America combined, to a depth of 16 kilometers (10 miles), contains only about one trillion astatine-215 atoms at any given time (around 3.5 × 10 grams). Astatine-217 is produced via the radioactive decay of neptunium-237. Primordial remnants of the latter isotope—due to its relatively short half-life of 2.14 million years—are no longer present on Earth. However, trace amounts occur naturally as a product of transmutation reactions in uranium ores. Astatine-218 was the first astatine isotope discovered in nature. Astatine-219, with a half-life of 56 seconds, is the longest lived of the naturally occurring isotopes.

Isotopes of astatine are sometimes not listed as naturally occurring because of misconceptions that there are no such isotopes, or discrepancies in the literature. Astatine-216 has been counted as a naturally occurring isotope but reports of its observation (which were described as doubtful) have not been confirmed.

Astatine was first produced by bombarding bismuth-209 with energetic alpha particles, and this is still the major route used to create the relatively long-lived isotopes astatine-209 through astatine-211. Astatine is only produced in minuscule quantities, with modern techniques allowing production runs of up to 6.6 gigabecquerels (about 86 nanograms or 2.47 atoms). Synthesis of greater quantities of astatine using this method is constrained by the limited availability of suitable cyclotrons and the prospect of melting the target. Solvent radiolysis due to the cumulative effect of astatine decay is a related problem. With cryogenic technology, microgram quantities of astatine might be able to be generated via proton irradiation of thorium or uranium to yield radon-211, in turn decaying to astatine-211. Contamination with astatine-210 is expected to be a drawback of this method.

The most important isotope is astatine-211, the only one in commercial use. To produce the bismuth target, the metal is sputtered onto a gold, copper, or aluminium surface at 50 to 100 milligrams per square centimeter. Bismuth oxide can be used instead; this is forcibly fused with a copper plate. The target is kept under a chemically neutral nitrogen atmosphere, and is cooled with water to prevent premature astatine vaporization. In a particle accelerator, such as a cyclotron, alpha particles are collided with the bismuth. Even though only one bismuth isotope is used (bismuth-209), the reaction may occur in three possible ways, producing astatine-209, astatine-210, or astatine-211. In order to eliminate undesired nuclides, the maximum energy of the particle accelerator is set to a value (optimally 29.17 MeV) above that for the reaction producing astatine-211 (to produce the desired isotope) and below the one producing astatine-210 (to avoid producing other astatine isotopes).

Since astatine is the main product of the synthesis, after its formation it must only be separated from the target and any significant contaminants. Several methods are available, "but they generally follow one of two approaches—dry distillation or [wet] acid treatment of the target followed by solvent extraction." The methods summarized below are modern adaptations of older procedures, as reviewed by Kugler and Keller. Pre-1985 techniques more often addressed the elimination of co-produced toxic polonium; this requirement is now mitigated by capping the energy of the cyclotron irradiation beam.

The astatine-containing cyclotron target is heated to a temperature of around 650 °C. The astatine volatilizes and is condensed in (typically) a cold trap. Higher temperatures of up to around 850 °C may increase the yield, at the risk of bismuth contamination from concurrent volatilization. Redistilling the condensate may be required to minimize the presence of bismuth (as bismuth can interfere with astatine labeling reactions). The astatine is recovered from the trap using one or more low concentration solvents such as sodium hydroxide, methanol or chloroform. Astatine yields of up to around 80% may be achieved. Dry separation is the method most commonly used to produce a chemically useful form of astatine.

The irradiated bismuth (or sometimes bismuth trioxide) target is first dissolved in, for example, concentrated nitric or perchloric acid. Following this first step, the acid can be distilled away to leave behind a white residue that contains both bismuth and the desired astatine product. This residue is then dissolved in a concentrated acid, such as hydrochloric acid. Astatine is extracted from this acid using an organic solvent such as dibutyl ether, diisopropyl ether (DIPE), or thiosemicarbazide. Using liquid-liquid extraction, the astatine product can be repeatedly washed with an acid, such as HCl, and extracted into the organic solvent layer. A separation yield of 93% using nitric acid has been reported, falling to 72% by the time purification procedures were completed (distillation of nitric acid, purging residual nitrogen oxides, and redissolving bismuth nitrate to enable liquid–liquid extraction). Wet methods involve "multiple radioactivity handling steps" and have not been considered well suited for isolating larger quantities of astatine. However, wet extraction methods are being examined for use in production of larger quantities of astatine-211, as it is thought that wet extraction methods can provide more consistency. They can enable the production of astatine in a specific oxidation state and may have greater applicability in experimental radiochemistry.

Newly formed astatine-211 is the subject of ongoing research in nuclear medicine. It must be used quickly as it decays with a half-life of 7.2 hours; this is long enough to permit multistep labeling strategies. Astatine-211 has potential for targeted alpha-particle therapy, since it decays either via emission of an alpha particle (to bismuth-207), or via electron capture (to an extremely short-lived nuclide, polonium-211, which undergoes further alpha decay), very quickly reaching its stable granddaughter lead-207. Polonium X-rays emitted as a result of the electron capture branch, in the range of 77–92 keV, enable the tracking of astatine in animals and patients. Although astatine-210 has a slightly longer half-life, it is wholly unsuitable because it usually undergoes beta plus decay to the extremely toxic polonium-210.

The principal medicinal difference between astatine-211 and iodine-131 (a radioactive iodine isotope also used in medicine) is that iodine-131 emits high-energy beta particles, and astatine does not. Beta particles have much greater penetrating power through tissues than do the much heavier alpha particles. An average alpha particle released by astatine-211 can travel up to 70 µm through surrounding tissues; an average-energy beta particle emitted by iodine-131 can travel nearly 30 times as far, to about 2 mm. The short half-life and limited penetrating power of alpha radiation through tissues offers advantages in situations where the "tumor burden is low and/or malignant cell populations are located in close proximity to essential normal tissues." Significant morbidity in cell culture models of human cancers has been achieved with from one to ten astatine-211 atoms bound per cell.
Several obstacles have been encountered in the development of astatine-based radiopharmaceuticals for cancer treatment. World War II delayed research for close to a decade. Results of early experiments indicated that a cancer-selective carrier would need to be developed and it was not until the 1970s that monoclonal antibodies became available for this purpose. Unlike iodine, astatine shows a tendency to dehalogenate from molecular carriers such as these, particularly at sp carbon sites (less so from sp sites). Given the toxicity of astatine accumulated and retained in the body, this emphasized the need to ensure it remained attached to its host molecule. While astatine carriers that are slowly metabolized can be assessed for their efficacy, more rapidly metabolized carriers remain a significant obstacle to the evaluation of astatine in nuclear medicine. Mitigating the effects of astatine-induced radiolysis of labeling chemistry and carrier molecules is another area requiring further development. A practical application for astatine as a cancer treatment would potentially be suitable for a "staggering" number of patients; production of astatine in the quantities that would be required remains an issue.

Animal studies show that astatine, similarly to iodine—although to a lesser extent, perhaps because of its slightly more metallic nature—is preferentially (and dangerously) concentrated in the thyroid gland. Unlike iodine, astatine also shows a tendency to be taken up by the lungs and spleen, possibly because of in-body oxidation of At to At. If administered in the form of a radiocolloid it tends to concentrate in the liver. Experiments in rats and monkeys suggest that astatine-211 causes much greater damage to the thyroid gland than does iodine-131, with repetitive injection of the nuclide resulting in necrosis and cell dysplasia within the gland. Early research suggested that injection of astatine into female rodents caused morphological changes in breast tissue; this conclusion remained controversial for many years. General agreement was later reached that this was likely caused by the effect of breast tissue irradiation combined with hormonal changes due to irradiation of the ovaries. Trace amounts of astatine can be handled safely in fume hoods if they are well-aerated; biological uptake of the element must be avoided.




Atom

The atom is the basic particle of the chemical elements. An atom consists of a nucleus of protons and generally neutrons, surrounded by an electromagnetically bound swarm of electrons. The chemical elements are distinguished from each other by the number of protons that are in their atoms. For example, any atom that contains 11 protons is sodium, and any atom that contains 29 protons is copper. Atoms with the same number of protons but a different number of neutrons are called isotopes of the same element.

Atoms are extremely small, typically around 100 picometers across. A human hair is about a million carbon atoms wide. This is smaller than the shortest wavelength of visible light, which means humans cannot see atoms with conventional microscopes. Atoms are so small that accurately predicting their behavior using classical physics is not possible due to quantum effects.

More than 99.94% of an atom's mass is in the nucleus. Protons have a positive electric charge and neutrons have no charge, so the nucleus is positively charged. The electrons are negatively charged, and this opposing charge is what binds them to the nucleus. If the numbers of protons and electrons are equal, as they normally are, then the atom is electrically neutral as a whole. If an atom has more electrons than protons, then it has an overall negative charge, and is called a negative ion (or anion). Conversely, if it has more protons than electrons, it has a positive charge, and is called a positive ion (or cation).

The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.

Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to attach and detach from each other is responsible for most of the physical changes observed in nature. Chemistry is the science that studies these changes.

The basic idea that matter is made up of tiny indivisible particles is an old idea that appeared in many ancient cultures. The word "atom" is derived from the ancient Greek word "atomos", which means "uncuttable". This ancient idea was based in philosophical reasoning rather than scientific reasoning. Modern atomic theory is not based on these old concepts. In the early 19th century, the scientist John Dalton noticed that chemical substances seemed to combine with each other by discrete and consistent units of weight, and he decided to use the word "atom" to refer to these units.

In the early 1800s, the English chemist John Dalton compiled experimental data gathered by him and other scientists and discovered a pattern now known as the "law of multiple proportions". He noticed that in chemical compounds which contain two particular chemical elements, the amount of Element A per measure of Element B will differ across these compounds by ratios of small whole numbers. This pattern suggested that the elements combine with each other by basic units of weight, and Dalton decided to call these units "atoms".

For example, there are two types of tin oxide: one is a grey powder that is 88.1% tin and 11.9% oxygen, and the other is a white powder that is 78.7% tin and 21.3% oxygen. Adjusting these figures, in the grey powder there is about 13.5 g of oxygen for every 100 g of tin, and in the white powder there is about 27 g of oxygen for every 100 g of tin. 13.5 and 27 form a ratio of 1:2. Dalton concluded that in these oxides, for every tin atom there are one or two oxygen atoms respectively (SnO and SnO).

Dalton also analyzed iron oxides. There is one type of iron oxide that is a black powder which is 78.1% iron and 21.9% oxygen; and there is another iron oxide that is a red powder which is 70.4% iron and 29.6% oxygen. Adjusting these figures, in the black powder there is about 28 g of oxygen for every 100 g of iron, and in the red powder there is about 42 g of oxygen for every 100 g of iron. 28 and 42 form a ratio of 2:3. Dalton concluded that in these oxides, for every two atoms of iron, there are two or three atoms of oxygen respectively (FeO and FeO).

As a final example: nitrous oxide is 63.3% nitrogen and 36.7% oxygen, nitric oxide is 44.05% nitrogen and 55.95% oxygen, and nitrogen dioxide is 29.5% nitrogen and 70.5% oxygen. Adjusting these figures, in nitrous oxide there is 80 g of oxygen for every 140 g of nitrogen, in nitric oxide there is about 160 g of oxygen for every 140 g of nitrogen, and in nitrogen dioxide there is 320 g of oxygen for every 140 g of nitrogen. 80, 160, and 320 form a ratio of 1:2:4. The respective formulas for these oxides are NO, NO, and NO.

In 1897, J. J. Thomson discovered that cathode rays are not electromagnetic waves but made of particles because they can be deflected by electric and magnetic fields. He measured these particles to be 1,800 times lighter than hydrogen (the lightest atom). Thomson concluded that these particles came from the atoms within the cathode—they were "subatomic" particles. He called these new particles "corpuscles" but they were later renamed "electrons". Thomson also showed that electrons were identical to particles given off by photoelectric and radioactive materials. It was quickly recognized that electrons are the particles that carry electric currents in metal wires. Thomson concluded that these electrons emerged from the very atoms of the cathode in his instruments, which meant that atoms are not indivisible as their name suggests.

J. J. Thomson thought that the negatively-charged electrons were distributed throughout the atom in a sea of positive charge that was distributed across the whole volume of the atom. This model is sometimes known as the plum pudding model.

Between 1908 and 1913, Ernest Rutherford and his colleagues Hans Geiger and Ernest Marsden performed a series of experiments in which they bombarded thin foils of metal with alpha particles. They did this to measure the scattering patterns of the alpha particles. They spotted alpha particles being deflected by angles greater than 90°. This shouldn't have been possible according to the Thomson model of the atom, whose charges were too diffuse to produce a sufficiently strong electric field. Rutherford proposed that the positive charge of the atom is not distributed throughout the atom's volume as Thomson believed, but is concentrated in a tiny nucleus at the center. Only such an intense concentration of charge could produce an electric field strong enough to deflect the alpha particles as observed.

In 1913, the physicist Niels Bohr proposed a model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits, and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of a photon. This quantization was used to explain why the electrons' orbits are stable (given that in classical physics, charges in acceleration, including circular motion, lose kinetic energy which is emitted as electromagnetic radiation) and why elements absorb and emit electromagnetic radiation in discrete spectra.

Later in the same year Henry Moseley provided additional experimental evidence in favor of Niels Bohr's theory. These results refined Ernest Rutherford's and Antonius van den Broek's model, which proposed that the atom contains in its nucleus a number of positive nuclear charges that is equal to its (atomic) number in the periodic table. Until these experiments, atomic number was not known to be a physical and experimental quantity. That it is equal to the atomic nuclear charge remains the accepted atomic model today.

Chemical bonds between atoms were explained by Gilbert Newton Lewis in 1916, as the interactions between their constituent electrons. As the chemical properties of the elements were known to largely repeat themselves according to the periodic law, in 1919 the American chemist Irving Langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner. Groups of electrons were thought to occupy a set of electron shells about the nucleus.

The Bohr model of the atom was the first complete physical model of the atom. It described the overall structure of the atom, how atoms bond to each other, and predicted the spectral lines of hydrogen. Bohr's model was not perfect and was soon superseded by the more accurate Schrödinger model, but it was sufficient to evaporate any remaining doubts that matter is composed of atoms. For chemists, the idea of the atom had been a useful heuristic tool, but physicists had doubts as to whether matter really is made up of atoms as nobody had yet developed a complete physical model of the atom.

In 1917 Rutherford bombarded nitrogen gas with alpha particles and observed hydrogen nuclei being emitted from the gas (Rutherford recognized these, because he had previously obtained them bombarding hydrogen with alpha particles, and observing hydrogen nuclei in the products). Rutherford concluded that the hydrogen nuclei emerged from the nuclei of the nitrogen atoms themselves (in effect, he had split a nitrogen).

From his own work and the work of his students Bohr and Henry Moseley, Rutherford knew that the positive charge of any atom could always be equated to that of an integer number of hydrogen nuclei. This, coupled with the atomic mass of many elements being roughly equivalent to an integer number of hydrogen atoms - then assumed to be the lightest particles - led him to conclude that hydrogen nuclei were singular particles and a basic constituent of all atomic nuclei. He named such particles protons. Further experimentation by Rutherford found that the nuclear mass of most atoms exceeded that of the protons it possessed; he speculated that this surplus mass was composed of previously unknown neutrally charged particles, which were tentatively dubbed "neutrons".

In 1928, Walter Bothe observed that beryllium emitted a highly penetrating, electrically neutral radiation when bombarded with alpha particles. It was later discovered that this radiation could knock hydrogen atoms out of paraffin wax. Initially it was thought to be high-energy gamma radiation, since gamma radiation had a similar effect on electrons in metals, but James Chadwick found that the ionization effect was too strong for it to be due to electromagnetic radiation, so long as energy and momentum were conserved in the interaction. In 1932, Chadwick exposed various elements, such as hydrogen and nitrogen, to the mysterious "beryllium radiation", and by measuring the energies of the recoiling charged particles, he deduced that the radiation was actually composed of electrically neutral particles which could not be massless like the gamma ray, but instead were required to have a mass similar to that of a proton. Chadwick now claimed these particles as Rutherford's neutrons. For his discovery of the neutron, Chadwick received the Nobel Prize in 1935.

The discovery of the neutron explained the existence of isotopes, which are atoms of the same element which have slightly different masses, due to them having different numbers of neutrons but the same number of protons.

In 1925, Werner Heisenberg published the first consistent mathematical formulation of quantum mechanics (matrix mechanics). One year earlier, Louis de Broglie had proposed that all particles behave like waves to some extent, and in 1926 Erwin Schroedinger used this idea to develop the Schroedinger equation, a mathematical model of the atom that described the electrons as three-dimensional waveforms rather than points in space.

A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at a given point in time. This became known as the uncertainty principle, formulated by Werner Heisenberg in 1927. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa.
This model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed.

Though the word "atom" originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton and the neutron.

The electron is the least massive of these particles by four orders of magnitude at , with a negative electrical charge and a size that is too small to be measured using available techniques. It was the lightest particle with a positive rest mass measured, until the discovery of neutrino mass. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. If an atom has more or fewer electrons than its atomic number, then it becomes respectively negatively or positively charged as a whole; a charged atom is called an ion. Electrons have been known since the late 19th century, mostly thanks to J.J. Thomson; see history of subatomic physics for details.

Protons have a positive charge and a mass of . The number of protons in an atom is called its atomic number. Ernest Rutherford (1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920 he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton.

Neutrons have no electrical charge and have a mass of . Neutrons are the heaviest of the three constituent particles, but their mass can be reduced by the nuclear binding energy. Neutrons and protons (collectively known as nucleons) have comparable dimensions—on the order of —although the 'surface' of these particles is not sharply defined. The neutron was discovered in 1932 by the English physicist James Chadwick.

In the Standard Model of physics, electrons are truly elementary particles with no internal structure, whereas protons and neutrons are composite particles composed of elementary particles called quarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of two up quarks (each with charge +) and one down quark (with a charge of −). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.

The quarks are held together by the strong interaction (or strong force), which is mediated by gluons. The protons and neutrons, in turn, are held to each other in the nucleus by the nuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.

All the bound protons and neutrons in an atom make up a tiny atomic nucleus, and are collectively called nucleons. The radius of a nucleus is approximately equal to formula_1 femtometres, where formula_2 is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 10 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other.

Atoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay.

The proton, the electron, and the neutron are classified as fermions. Fermions obey the Pauli exclusion principle which prohibits "identical" fermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud.

A nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with matching numbers of protons and neutrons are more stable against decay, but with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus.

The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3 to 10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.

If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass–energy equivalence formula, "e=mc", where "m" is the mass loss and "c" is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.

The fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction. For heavier nuclei, the binding energy per nucleon begins to decrease. That means that a fusion process producing a nucleus that has an atomic number higher than about 26, and a mass number higher than about 60, is an endothermic process. Thus, more massive nuclei cannot undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.

The electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.

Electrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and differ from each other in size, shape and orientation.

Each atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines.

The amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 "million" eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals.

By definition, any two atoms with an identical number of "protons" in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of "neutrons" are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form, also called protium), one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The known elements form a set of atomic numbers, from the single-proton element hydrogen up to the 118-proton element oganesson. All known isotopes of elements with atomic numbers greater than 82 are radioactive, although the radioactivity of element 83 (bismuth) is so slight as to be practically negligible.

About 339 nuclides occur naturally on Earth, of which 251 (about 74%) have not been observed to decay, and are referred to as "stable isotopes". Only 90 nuclides are stable theoretically, while another 161 (bringing the total to 251) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as "stable". An additional 35 radioactive nuclides have half-lives longer than 100 million years, and are long-lived enough to have been present since the birth of the Solar System. This collection of 286 nuclides are known as primordial nuclides. Finally, an additional 53 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such as radium from uranium), or as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).

For 80 of the chemical elements, at least one stable isotope exists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.1 stable isotopes per element. Twenty-six "monoisotopic elements" have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes.

Stability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain "magic numbers" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 251 known stable nuclides, only four have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10, and nitrogen-14. (Tantalum-180m is odd-odd and observationally stable, but is predicted to decay with a very long half-life.) Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138, and lutetium-176. Most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.

The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called "nucleons") in a given atom is called the mass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is "carbon-12," which has 12 nucleons (six protons and six neutrons).

The actual mass of an atom at rest is often expressed in daltons (Da), also called the unified atomic mass unit (u). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately . Hydrogen-1 (the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 Da. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the atomic mass unit (for example the mass of a nitrogen-14 is roughly 14 Da), but this number will not be exactly an integer except (by definition) in the case of carbon-12. The heaviest stable atom is lead-208, with a mass of .

As even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about ). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the unified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 Da, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.

Atoms lack a well-defined outer boundary, so their dimensions are usually described in terms of an atomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus. This assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm.

When subjected to external forces, like electrical fields, the shape of an atom may deviate from spherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown by group-theoretical considerations. Aspherical deviations might be elicited for instance in crystals, where large crystal-electrical fields may occur at low-symmetry lattice sites. Significant ellipsoidal deformations have been shown to occur for sulfur ions and chalcogen ions in pyrite-type compounds.

Atomic dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they cannot be viewed using an optical microscope, although individual atoms can be observed using a scanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion () atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of contains about 10 sextillion (10) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.

Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.

The most common forms of radioactive decay are:

Other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission.

Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.

Elementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin  ħ, or "spin-". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.

The magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field, but the most dominant contribution comes from electron spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.

In ferromagnetic elements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a spontaneous process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.

The nucleus of an atom will have no spin when it has even numbers of both neutrons and protons, but for other cases of odd numbers, the nucleus may have a spin. Normally nuclei with spin are aligned in random directions because of thermal equilibrium, but for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.

The potential energy of an electron in an atom is negative relative to when the distance from the nucleus goes to infinity; its dependence on the electron's position reaches the minimum inside the nucleus, roughly in inverse proportion to the distance. In the quantum-mechanical model, a bound electron can occupy only a set of states centered on the nucleus, and each state corresponds to a specific energy level; see time-independent Schrödinger equation for a theoretical explanation. An energy level can be measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). The lowest energy state of a bound electron is called the ground state, i.e. stationary state, while an electron transition to a higher level results in an excited state. The electron's energy increases along with "n" because the (average) distance to the nucleus increases. Dependence of the energy on is caused not by the electrostatic potential of the nucleus, but by interaction between electrons.

For an electron to transition between two different states, e.g. ground state to first excited state, it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels, according to the Niels Bohr model, what can be precisely calculated by the Schrödinger equation.
Electrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; see Electron properties.

The energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.

When a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. (An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series of emission lines from the photons emitted by the atoms.) Spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined.

Close examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin–orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect.

If a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.

Valency is the combining power of an element. It is determined by the number of bonds it can form to other atoms or groups. The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in
that shell are called valence electrons. The number of valence electrons determines the bonding
behavior with other atoms. Atoms tend to chemically react with each other in a manner that fills (or empties) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. Many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds.

The chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.

Quantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases and plasmas. Within a state, a material can also exist in different allotropes. An example of this is solid carbon, which can exist as graphite or diamond. Gaseous allotropes exist as well, such as dioxygen and ozone.

At temperatures close to absolute zero, atoms can form a Bose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms
then behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.

While atoms are too small to be seen, devices such as the scanning tunneling microscope (STM) enable their visualization at the surfaces of solids. The microscope uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would be insurmountable in the classical perspective. Electrons tunnel through the vacuum between two biased electrodes, providing a tunneling current that is exponentially dependent on their separation. One electrode is a sharp tip ideally ending with a single atom. At each point of the scan of the surface the tip's height is adjusted so as to keep the tunneling current at a set value. How much the tip moves to and away from the surface is interpreted as the height profile. For low bias, the microscope images the averaged electron orbitals across closely packed energy levels—the local density of the electronic states near the Fermi level. Because of the distances involved, both electrodes need to be extremely stable; only then periodicities can be observed that correspond to individual atoms. The method alone is not chemically specific, and cannot identify the atomic species present at the surface.

Atoms can be easily identified by their mass. If an atom is ionized by removing one of its electrons, its trajectory when it passes through a magnetic field will bend. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.

The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry.

Electron emission techniques such as X-ray photoelectron spectroscopy (XPS) and Auger electron spectroscopy (AES), which measure the binding energies of the core electrons, are used to identify the atomic species present in a sample in a non-destructive way. With proper focusing both can be made area-specific. Another such method is electron energy loss spectroscopy (EELS), which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample.

Spectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.

Baryonic matter forms about 4% of the total energy density of the observable universe, with an average density of about 0.25 particles/m (mostly protons and electrons). Within a galaxy such as the Milky Way, particles have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 10 to 10 atoms/m. The Sun is believed to be inside the Local Bubble, so the density in the solar neighborhood is only about 10 atoms/m. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium.

Up to 95% of the Milky Way's baryonic matter are concentrated inside stars, where conditions are unfavorable for atomic matter. The total baryonic mass is about 10% of the mass of the galaxy; the remainder of the mass is an unknown dark matter. High temperature inside stars makes most "atoms" fully ionized, that is, separates "all" electrons from the nuclei. In stellar remnants—with exception of their surface layers—an immense pressure make electron shells impossible.

Electrons are thought to exist in the Universe since early stages of the Big Bang. Atomic nuclei forms in nucleosynthesis reactions. In about three minutes Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the Universe, and perhaps some of the beryllium and boron.

Ubiquitousness and stability of atoms relies on their binding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where the temperature is much higher than ionization potential, the matter exists in the form of plasma—a gas of positively charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms become statistically favorable. Atoms (complete with bound electrons) became to dominate over charged particles 380,000 years after the Big Bang—an epoch called recombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.

Since the Big Bang, which produced no carbon or heavier elements, atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium, and (via the triple alpha process) the sequence of elements from carbon up to iron; see stellar nucleosynthesis for details.

Isotopes such as lithium-6, as well as some beryllium and boron are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.

Elements heavier than iron were produced in supernovae and colliding neutron stars through the r-process, and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.

Most of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay.

There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore.

The Earth contains approximately atoms. Although small numbers of independent atoms of noble gases exist, such as argon, neon, and helium, 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, including water, salt, silicates and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.

All nuclides with atomic numbers higher than 82 (lead) are known to be radioactive. No nuclide with an atomic number exceeding 92 (uranium) exists on Earth as a primordial nuclide, and heavier elements generally have shorter half-lives. Nevertheless, an "island of stability" encompassing relatively long-lived isotopes of superheavy elements with atomic numbers 110 to 114 might exist. Predictions for the half-life of the most stable nuclide on the island range from a few minutes to millions of years. In any case, superheavy elements (with "Z" > 104) would not exist due to increasing Coulomb repulsion (which results in spontaneous fission with increasingly short half-lives) in the absence of any stabilizing effects.

Each particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories of baryogenesis may offer an explanation. As a result, no antimatter atoms have been discovered in nature. In 1996, the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at the CERN laboratory in Geneva.

Other exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test fundamental predictions of physics.




Arable land

Arable land (from the , "able to be ploughed") is any land capable of being ploughed and used to grow crops. Alternatively, for the purposes of agricultural statistics, the term often has a more precise definition: 

A more concise definition appearing in the Eurostat glossary similarly refers to actual rather than potential uses: "land worked (ploughed or tilled) regularly, generally under a system of crop rotation". In Britain, arable land has traditionally been contrasted with pasturable land such as heaths, which could be used for sheep-rearing but not as farmland.

Arable land is vulnerable to land degradation and some types of un-arable land can be enriched to create useful land. Climate change and biodiversity loss, are driving pressure on arable land.

According to the Food and Agriculture Organization of the United Nations, in 2013, the world's arable land amounted to 1.407 billion hectares, out of a total of 4.924 billion hectares of land used for agriculture.

Agricultural land that is not arable according to the FAO definition above includes:

Other non-arable land includes land that is not suitable for any agricultural use. Land that is not arable, in the sense of lacking capability or suitability for cultivation for crop production, has one or more limitationsa lack of sufficient freshwater for irrigation, stoniness, steepness, adverse climate, excessive wetness with the impracticality of drainage, excessive salts, or a combination of these, among others. Although such limitations may preclude cultivation, and some will in some cases preclude any agricultural use, large areas unsuitable for cultivation may still be agriculturally productive. For example, United States NRCS statistics indicate that about 59 percent of US non-federal pasture and unforested rangeland is unsuitable for cultivation, yet such land has value for grazing of livestock. In British Columbia, Canada, 41 percent of the provincial Agricultural Land Reserve area is unsuitable for the production of cultivated crops, but is suitable for uncultivated production of forage usable by grazing livestock. Similar examples can be found in many rangeland areas elsewhere.

Land incapable of being cultivated for the production of crops can sometimes be converted to arable land. New arable land makes more food and can reduce starvation. This outcome also makes a country more self-sufficient and politically independent, because food importation is reduced. Making non-arable land arable often involves digging new irrigation canals and new wells, aqueducts, desalination plants, planting trees for shade in the desert, hydroponics, fertilizer, nitrogen fertilizer, pesticides, reverse osmosis water processors, PET film insulation or other insulation against heat and cold, digging ditches and hills for protection against the wind, and installing greenhouses with internal light and heat for protection against the cold outside and to provide light in cloudy areas. Such modifications are often prohibitively expensive. An alternative is the seawater greenhouse, which desalinates water through evaporation and condensation using solar energy as the only energy input. This technology is optimized to grow crops on desert land close to the sea.

The use of artifices does not make the land arable. Rock still remains rock, and shallowless than turnable soil is still not considered toilable. The use of artifice is an open-air none recycled water hydroponics relationship. The below described circumstances are not in perspective, have limited duration, and have a tendency to accumulate trace materials in soil that either there or elsewhere cause deoxygenation. The use of vast amounts of fertilizer may have unintended consequences for the environment by devastating rivers, waterways, and river endings through the accumulation of non-degradable toxins and nitrogen-bearing molecules that remove oxygen and cause non-aerobic processes to form.

Examples of infertile non-arable land being turned into fertile arable land include:

Examples of fertile arable land being turned into infertile land include:



Aluminium

Aluminium (aluminum in North American English) is a chemical element; it has symbol Al and atomic number 13. Aluminium has a density lower than that of other common metals, about one-third that of steel. It has a great affinity towards oxygen, forming a protective layer of oxide on the surface when exposed to air. Aluminium visually resembles silver, both in its color and in its great ability to reflect light. It is soft, nonmagnetic, and ductile. It has one stable isotope, Al, which is highly abundant, making aluminium the twelfth-most common element in the universe. The radioactivity of Al is used in radiometric dating.

Chemically, aluminium is a post-transition metal in the boron group; as is common for the group, aluminium forms compounds primarily in the +3 oxidation state. The aluminium cation Al is small and highly charged; as such, it has more polarizing power, and bonds formed by aluminium have a more covalent character. The strong affinity of aluminium for oxygen leads to the common occurrence of its oxides in nature. Aluminium is found on Earth primarily in rocks in the crust, where it is the third-most abundant element, after oxygen and silicon, rather than in the mantle, and virtually never as the free metal. It is obtained industrially by mining bauxite, a sedimentary rock rich in aluminium minerals.

The discovery of aluminium was announced in 1825 by Danish physicist Hans Christian Ørsted. The first industrial production of aluminium was initiated by French chemist Henri Étienne Sainte-Claire Deville in 1856. Aluminium became much more available to the public with the Hall–Héroult process developed independently by French engineer Paul Héroult and American engineer Charles Martin Hall in 1886, and the mass production of aluminium led to its extensive use in industry and everyday life. In World Wars I and II, aluminium was a crucial strategic resource for aviation. In 1954, aluminium became the most produced non-ferrous metal, surpassing copper. In the 21st century, most aluminium was consumed in transportation, engineering, construction, and packaging in the United States, Western Europe, and Japan.

Despite its prevalence in the environment, no living organism is known to use aluminium salts for metabolism, but aluminium is well tolerated by plants and animals. Because of the abundance of these salts, the potential for a biological role for them is of interest, and studies continue.

Of aluminium isotopes, only is stable. This situation is common for elements with an odd atomic number. It is the only primordial aluminium isotope, i.e. the only one that has existed on Earth in its current form since the formation of the planet. It is therefore a mononuclidic element and its standard atomic weight is virtually the same as that of the isotope. This makes aluminium very useful in nuclear magnetic resonance (NMR), as its single stable isotope has a high NMR sensitivity. The standard atomic weight of aluminium is low in comparison with many other metals.

All other isotopes of aluminium are radioactive. The most stable of these is Al: while it was present along with stable Al in the interstellar medium from which the Solar System formed, having been produced by stellar nucleosynthesis as well, its half-life is only 717,000 years and therefore a detectable amount has not survived since the formation of the planet. However, minute traces of Al are produced from argon in the atmosphere by spallation caused by cosmic ray protons. The ratio of Al to Be has been used for radiodating of geological processes over 10 to 10 year time scales, in particular transport, deposition, sediment storage, burial times, and erosion. Most meteorite scientists believe that the energy released by the decay of Al was responsible for the melting and differentiation of some asteroids after their formation 4.55 billion years ago.

The remaining isotopes of aluminium, with mass numbers ranging from 22 to 43, all have half-lives well under an hour. Three metastable states are known, all with half-lives under a minute.

An aluminium atom has 13 electrons, arranged in an electron configuration of , with three electrons beyond a stable noble gas configuration. Accordingly, the combined first three ionization energies of aluminium are far lower than the fourth ionization energy alone. Such an electron configuration is shared with the other well-characterized members of its group, boron, gallium, indium, and thallium; it is also expected for nihonium. Aluminium can surrender its three outermost electrons in many chemical reactions (see below). The electronegativity of aluminium is 1.61 (Pauling scale).

A free aluminium atom has a radius of 143 pm. With the three outermost electrons removed, the radius shrinks to 39 pm for a 4-coordinated atom or 53.5 pm for a 6-coordinated atom. At standard temperature and pressure, aluminium atoms (when not affected by atoms of other elements) form a face-centered cubic crystal system bound by metallic bonding provided by atoms' outermost electrons; hence aluminium (at these conditions) is a metal. This crystal system is shared by many other metals, such as lead and copper; the size of a unit cell of aluminium is comparable to that of those other metals. The system, however, is not shared by the other members of its group; boron has ionization energies too high to allow metallization, thallium has a hexagonal close-packed structure, and gallium and indium have unusual structures that are not close-packed like those of aluminium and thallium. The few electrons that are available for metallic bonding in aluminium metal are a probable cause for it being soft with a low melting point and low electrical resistivity.

Aluminium metal has an appearance ranging from silvery white to dull gray, depending on the surface roughness. Aluminium mirrors are the most reflective of all metal mirrors for the near ultraviolet and far infrared light, and one of the most reflective in the visible spectrum, nearly on par with silver, and the two therefore look similar. Aluminium is also good at reflecting solar radiation, although prolonged exposure to sunlight in air adds wear to the surface of the metal; this may be prevented if aluminium is anodized, which adds a protective layer of oxide on the surface.

The density of aluminium is 2.70 g/cm, about 1/3 that of steel, much lower than other commonly encountered metals, making aluminium parts easily identifiable through their lightness. Aluminium's low density compared to most other metals arises from the fact that its nuclei are much lighter, while difference in the unit cell size does not compensate for this difference. The only lighter metals are the metals of groups 1 and 2, which apart from beryllium and magnesium are too reactive for structural use (and beryllium is very toxic). Aluminium is not as strong or stiff as steel, but the low density makes up for this in the aerospace industry and for many other applications where light weight and relatively high strength are crucial.

Pure aluminium is quite soft and lacking in strength. In most applications various aluminium alloys are used instead because of their higher strength and hardness. The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa. Aluminium is ductile, with a percent elongation of 50-70%, and malleable allowing it to be easily drawn and extruded. It is also easily machined and cast.

Aluminium is an excellent thermal and electrical conductor, having around 60% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density. Aluminium is capable of superconductivity, with a superconducting critical temperature of 1.2 kelvin and a critical magnetic field of about 100 gauss (10 milliteslas). It is paramagnetic and thus essentially unaffected by static magnetic fields. The high electrical conductivity, however, means that it is strongly affected by alternating magnetic fields through the induction of eddy currents.

Aluminium combines characteristics of pre- and post-transition metals. Since it has few available electrons for metallic bonding, like its heavier group 13 congeners, it has the characteristic physical properties of a post-transition metal, with longer-than-expected interatomic distances. Furthermore, as Al is a small and highly charged cation, it is strongly polarizing and bonding in aluminium compounds tends towards covalency; this behavior is similar to that of beryllium (Be), and the two display an example of a diagonal relationship.

The underlying core under aluminium's valence shell is that of the preceding noble gas, whereas those of its heavier congeners gallium, indium, thallium, and nihonium also include a filled d-subshell and in some cases a filled f-subshell. Hence, the inner electrons of aluminium shield the valence electrons almost completely, unlike those of aluminium's heavier congeners. As such, aluminium is the most electropositive metal in its group, and its hydroxide is in fact more basic than that of gallium. Aluminium also bears minor similarities to the metalloid boron in the same group: AlX compounds are valence isoelectronic to BX compounds (they have the same valence electronic structure), and both behave as Lewis acids and readily form adducts. Additionally, one of the main motifs of boron chemistry is regular icosahedral structures, and aluminium forms an important part of many icosahedral quasicrystal alloys, including the Al–Zn–Mg class.

Aluminium has a high chemical affinity to oxygen, which renders it suitable for use as a reducing agent in the thermite reaction. A fine powder of aluminium metal reacts explosively on contact with liquid oxygen; under normal conditions, however, aluminium forms a thin oxide layer (~5 nm at room temperature) that protects the metal from further corrosion by oxygen, water, or dilute acid, a process termed passivation. Because of its general resistance to corrosion, aluminium is one of the few metals that retains silvery reflectance in finely powdered form, making it an important component of silver-colored paints. Aluminium is not attacked by oxidizing acids because of its passivation. This allows aluminium to be used to store reagents such as nitric acid, concentrated sulfuric acid, and some organic acids.

In hot concentrated hydrochloric acid, aluminium reacts with water with evolution of hydrogen, and in aqueous sodium hydroxide or potassium hydroxide at room temperature to form aluminates—protective passivation under these conditions is negligible. Aqua regia also dissolves aluminium. Aluminium is corroded by dissolved chlorides, such as common sodium chloride, which is why household plumbing is never made from aluminium. The oxide layer on aluminium is also destroyed by contact with mercury due to amalgamation or with salts of some electropositive metals. As such, the strongest aluminium alloys are less corrosion-resistant due to galvanic reactions with alloyed copper, and aluminium's corrosion resistance is greatly reduced by aqueous salts, particularly in the presence of dissimilar metals.

Aluminium reacts with most nonmetals upon heating, forming compounds such as aluminium nitride (AlN), aluminium sulfide (AlS), and the aluminium halides (AlX). It also forms a wide range of intermetallic compounds involving metals from every group on the periodic table.

The vast majority of compounds, including all aluminium-containing minerals and all commercially significant aluminium compounds, feature aluminium in the oxidation state 3+. The coordination number of such compounds varies, but generally Al is either six- or four-coordinate. Almost all compounds of aluminium(III) are colorless.
In aqueous solution, Al exists as the hexaaqua cation [Al(HO)], which has an approximate K of 10. Such solutions are acidic as this cation can act as a proton donor and progressively hydrolyze until a precipitate of aluminium hydroxide, Al(OH), forms. This is useful for clarification of water, as the precipitate nucleates on suspended particles in the water, hence removing them. Increasing the pH even further leads to the hydroxide dissolving again as aluminate, [Al(HO)(OH)], is formed.

Aluminium hydroxide forms both salts and aluminates and dissolves in acid and alkali, as well as on fusion with acidic and basic oxides. This behavior of Al(OH) is termed amphoterism and is characteristic of weakly basic cations that form insoluble hydroxides and whose hydrated species can also donate their protons. One effect of this is that aluminium salts with weak acids are hydrolyzed in water to the aquated hydroxide and the corresponding nonmetal hydride: for example, aluminium sulfide yields hydrogen sulfide. However, some salts like aluminium carbonate exist in aqueous solution but are unstable as such; and only incomplete hydrolysis takes place for salts with strong acids, such as the halides, nitrate, and sulfate. For similar reasons, anhydrous aluminium salts cannot be made by heating their "hydrates": hydrated aluminium chloride is in fact not AlCl·6HO but [Al(HO)]Cl, and the Al–O bonds are so strong that heating is not sufficient to break them and form Al–Cl bonds instead:

All four trihalides are well known. Unlike the structures of the three heavier trihalides, aluminium fluoride (AlF) features six-coordinate aluminium, which explains its involatility and insolubility as well as high heat of formation. Each aluminium atom is surrounded by six fluorine atoms in a distorted octahedral arrangement, with each fluorine atom being shared between the corners of two octahedra. Such {AlF} units also exist in complex fluorides such as cryolite, NaAlF. AlF melts at and is made by reaction of aluminium oxide with hydrogen fluoride gas at .

With heavier halides, the coordination numbers are lower. The other trihalides are dimeric or polymeric with tetrahedral four-coordinate aluminium centers. Aluminium trichloride (AlCl) has a layered polymeric structure below its melting point of but transforms on melting to AlCl dimers. At higher temperatures those increasingly dissociate into trigonal planar AlCl monomers similar to the structure of BCl. Aluminium tribromide and aluminium triiodide form AlX dimers in all three phases and hence do not show such significant changes of properties upon phase change. These materials are prepared by treating aluminium metal with the halogen. The aluminium trihalides form many addition compounds or complexes; their Lewis acidic nature makes them useful as catalysts for the Friedel–Crafts reactions. Aluminium trichloride has major industrial uses involving this reaction, such as in the manufacture of anthraquinones and styrene; it is also often used as the precursor for many other aluminium compounds and as a reagent for converting nonmetal fluorides into the corresponding chlorides (a transhalogenation reaction).

Aluminium forms one stable oxide with the chemical formula AlO, commonly called alumina. It can be found in nature in the mineral corundum, α-alumina; there is also a γ-alumina phase. Its crystalline form, corundum, is very hard (Mohs hardness 9), has a high melting point of , has very low volatility, is chemically inert, and a good electrical insulator, it is often used in abrasives (such as toothpaste), as a refractory material, and in ceramics, as well as being the starting material for the electrolytic production of aluminium metal. Sapphire and ruby are impure corundum contaminated with trace amounts of other metals. The two main oxide-hydroxides, AlO(OH), are boehmite and diaspore. There are three main trihydroxides: bayerite, gibbsite, and nordstrandite, which differ in their crystalline structure (polymorphs). Many other intermediate and related structures are also known. Most are produced from ores by a variety of wet processes using acid and base. Heating the hydroxides leads to formation of corundum. These materials are of central importance to the production of aluminium and are themselves extremely useful. Some mixed oxide phases are also very useful, such as spinel (MgAlO), Na-β-alumina (NaAlO), and tricalcium aluminate (CaAlO, an important mineral phase in Portland cement).

The only stable chalcogenides under normal conditions are aluminium sulfide (AlS), selenide (AlSe), and telluride (AlTe). All three are prepared by direct reaction of their elements at about and quickly hydrolyze completely in water to yield aluminium hydroxide and the respective hydrogen chalcogenide. As aluminium is a small atom relative to these chalcogens, these have four-coordinate tetrahedral aluminium with various polymorphs having structures related to wurtzite, with two-thirds of the possible metal sites occupied either in an orderly (α) or random (β) fashion; the sulfide also has a γ form related to γ-alumina, and an unusual high-temperature hexagonal form where half the aluminium atoms have tetrahedral four-coordination and the other half have trigonal bipyramidal five-coordination.

Four pnictides – aluminium nitride (AlN), aluminium phosphide (AlP), aluminium arsenide (AlAs), and aluminium antimonide (AlSb) – are known. They are all III-V semiconductors isoelectronic to silicon and germanium, all of which but AlN have the zinc blende structure. All four can be made by high-temperature (and possibly high-pressure) direct reaction of their component elements.
Aluminium alloys well with most other metals (with the exception of most alkali metals and group 13 metals) and over 150 intermetallics with other metals are known. Preparation involves heating fixed metals together in certain proportion, followed by gradual cooling and annealing. Bonding in them is predominantly metallic and the crystal structure primarily depends on efficiency of packing.

There are few compounds with lower oxidation states. A few aluminium(I) compounds exist: AlF, AlCl, AlBr, and AlI exist in the gaseous phase when the respective trihalide is heated with aluminium, and at cryogenic temperatures. A stable derivative of aluminium monoiodide is the cyclic adduct formed with triethylamine, AlI(NEt). AlO and AlS also exist but are very unstable. Very simple aluminium(II) compounds are invoked or observed in the reactions of Al metal with oxidants. For example, aluminium monoxide, AlO, has been detected in the gas phase after explosion and in stellar absorption spectra. More thoroughly investigated are compounds of the formula RAl which contain an Al–Al bond and where R is a large organic ligand.

A variety of compounds of empirical formula AlR and AlRCl exist. The aluminium trialkyls and triaryls are reactive, volatile, and colorless liquids or low-melting solids. They catch fire spontaneously in air and react with water, thus necessitating precautions when handling them. They often form dimers, unlike their boron analogues, but this tendency diminishes for branched-chain alkyls (e.g. Pr, Bu, MeCCH); for example, triisobutylaluminium exists as an equilibrium mixture of the monomer and dimer. These dimers, such as trimethylaluminium (AlMe), usually feature tetrahedral Al centers formed by dimerization with some alkyl group bridging between both aluminium atoms. They are hard acids and react readily with ligands, forming adducts. In industry, they are mostly used in alkene insertion reactions, as discovered by Karl Ziegler, most importantly in "growth reactions" that form long-chain unbranched primary alkenes and alcohols, and in the low-pressure polymerization of ethene and propene. There are also some heterocyclic and cluster organoaluminium compounds involving Al–N bonds.

The industrially most important aluminium hydride is lithium aluminium hydride (LiAlH), which is used in as a reducing agent in organic chemistry. It can be produced from lithium hydride and aluminium trichloride. The simplest hydride, aluminium hydride or alane, is not as important. It is a polymer with the formula (AlH), in contrast to the corresponding boron hydride that is a dimer with the formula (BH).

Aluminium's per-particle abundance in the Solar System is 3.15 ppm (parts per million). It is the twelfth most abundant of all elements and third most abundant among the elements that have odd atomic numbers, after hydrogen and nitrogen. The only stable isotope of aluminium, Al, is the eighteenth most abundant nucleus in the Universe. It is created almost entirely after fusion of carbon in massive stars that will later become Type II supernovas: this fusion creates Mg, which, upon capturing free protons and neutrons becomes aluminium. Some smaller quantities of Al are created in hydrogen burning shells of evolved stars, where Mg can capture free protons. Essentially all aluminium now in existence is Al. Al was present in the early Solar System with abundance of 0.005% relative to Al but its half-life of 728,000 years is too short for any original nuclei to survive; Al is therefore extinct. Unlike for Al, hydrogen burning is the primary source of Al, with the nuclide emerging after a nucleus of Mg catches a free proton. However, the trace quantities of Al that do exist are the most common gamma ray emitter in the interstellar gas; if the original Al were still present, gamma ray maps of the Milky Way would be brighter.

Overall, the Earth is about 1.59% aluminium by mass (seventh in abundance by mass). Aluminium occurs in greater proportion in the Earth's crust than in the Universe at large, because aluminium easily forms the oxide and becomes bound into rocks and stays in the Earth's crust, while less reactive metals sink to the core. In the Earth's crust, aluminium is the most abundant metallic element (8.23% by mass) and the third most abundant of all elements (after oxygen and silicon). A large number of silicates in the Earth's crust contain aluminium. In contrast, the Earth's mantle is only 2.38% aluminium by mass. Aluminium also occurs in seawater at a concentration of 2 μg/kg.

Because of its strong affinity for oxygen, aluminium is almost never found in the elemental state; instead it is found in oxides or silicates. Feldspars, the most common group of minerals in the Earth's crust, are aluminosilicates. Aluminium also occurs in the minerals beryl, cryolite, garnet, spinel, and turquoise. Impurities in AlO, such as chromium and iron, yield the gemstones ruby and sapphire, respectively. Native aluminium metal is extremely rare and can only be found as a minor phase in low oxygen fugacity environments, such as the interiors of certain volcanoes. Native aluminium has been reported in cold seeps in the northeastern continental slope of the South China Sea. It is possible that these deposits resulted from bacterial reduction of tetrahydroxoaluminate Al(OH).

Although aluminium is a common and widespread element, not all aluminium minerals are economically viable sources of the metal. Almost all metallic aluminium is produced from the ore bauxite (AlO(OH)). Bauxite occurs as a weathering product of low iron and silica bedrock in tropical climatic conditions. In 2017, most bauxite was mined in Australia, China, Guinea, and India.

The history of aluminium has been shaped by usage of alum. The first written record of alum, made by Greek historian Herodotus, dates back to the 5th century BCE. The ancients are known to have used alum as a dyeing mordant and for city defense. After the Crusades, alum, an indispensable good in the European fabric industry, was a subject of international commerce; it was imported to Europe from the eastern Mediterranean until the mid-15th century.

The nature of alum remained unknown. Around 1530, Swiss physician Paracelsus suggested alum was a salt of an earth of alum. In 1595, German doctor and chemist Andreas Libavius experimentally confirmed this. In 1722, German chemist Friedrich Hoffmann announced his belief that the base of alum was a distinct earth. In 1754, German chemist Andreas Sigismund Marggraf synthesized alumina by boiling clay in sulfuric acid and subsequently adding potash.

Attempts to produce aluminium metal date back to 1760. The first successful attempt, however, was completed in 1824 by Danish physicist and chemist Hans Christian Ørsted. He reacted anhydrous aluminium chloride with potassium amalgam, yielding a lump of metal looking similar to tin. He presented his results and demonstrated a sample of the new metal in 1825. In 1827, German chemist Friedrich Wöhler repeated Ørsted's experiments but did not identify any aluminium. (The reason for this inconsistency was only discovered in 1921.) He conducted a similar experiment in the same year by mixing anhydrous aluminium chloride with potassium and produced a powder of aluminium. In 1845, he was able to produce small pieces of the metal and described some physical properties of this metal. For many years thereafter, Wöhler was credited as the discoverer of aluminium.

As Wöhler's method could not yield great quantities of aluminium, the metal remained rare; its cost exceeded that of gold. The first industrial production of aluminium was established in 1856 by French chemist Henri Etienne Sainte-Claire Deville and companions. Deville had discovered that aluminium trichloride could be reduced by sodium, which was more convenient and less expensive than potassium, which Wöhler had used. Even then, aluminium was still not of great purity and produced aluminium differed in properties by sample. Because of its electricity-conducting capacity, aluminium was used as the cap of the Washington Monument, completed in 1885. The tallest building in the world at the time, the non-corroding metal cap was intended to serve as a lightning rod peak.

The first industrial large-scale production method was independently developed in 1886 by French engineer Paul Héroult and American engineer Charles Martin Hall; it is now known as the Hall–Héroult process. The Hall–Héroult process converts alumina into metal. Austrian chemist Carl Joseph Bayer discovered a way of purifying bauxite to yield alumina, now known as the Bayer process, in 1889. Modern production of the aluminium metal is based on the Bayer and Hall–Héroult processes.

Prices of aluminium dropped and aluminium became widely used in jewelry, everyday items, eyeglass frames, optical instruments, tableware, and foil in the 1890s and early 20th century. Aluminium's ability to form hard yet light alloys with other metals provided the metal with many uses at the time. During World War I, major governments demanded large shipments of aluminium for light strong airframes; during World War II, demand by major governments for aviation was even higher.

By the mid-20th century, aluminium had become a part of everyday life and an essential component of housewares. In 1954, production of aluminium surpassed that of copper, historically second in production only to iron, making it the most produced non-ferrous metal. During the mid-20th century, aluminium emerged as a civil engineering material, with building applications in both basic construction and interior finish work, and increasingly being used in military engineering, for both airplanes and land armor vehicle engines. Earth's first artificial satellite, launched in 1957, consisted of two separate aluminium semi-spheres joined and all subsequent space vehicles have used aluminium to some extent. The aluminium can was invented in 1956 and employed as a storage for drinks in 1958.

Throughout the 20th century, the production of aluminium rose rapidly: while the world production of aluminium in 1900 was 6,800 metric tons, the annual production first exceeded 100,000 metric tons in 1916; 1,000,000 tons in 1941; 10,000,000 tons in 1971. In the 1970s, the increased demand for aluminium made it an exchange commodity; it entered the London Metal Exchange, the oldest industrial metal exchange in the world, in 1978. The output continued to grow: the annual production of aluminium exceeded 50,000,000 metric tons in 2013.

The real price for aluminium declined from $14,000 per metric ton in 1900 to $2,340 in 1948 (in 1998 United States dollars). Extraction and processing costs were lowered over technological progress and the scale of the economies. However, the need to exploit lower-grade poorer quality deposits and the use of fast increasing input costs (above all, energy) increased the net cost of aluminium; the real price began to grow in the 1970s with the rise of energy cost. Production moved from the industrialized countries to countries where production was cheaper. Production costs in the late 20th century changed because of advances in technology, lower energy prices, exchange rates of the United States dollar, and alumina prices. The BRIC countries' combined share in primary production and primary consumption grew substantially in the first decade of the 21st century. China is accumulating an especially large share of the world's production thanks to an abundance of resources, cheap energy, and governmental stimuli; it also increased its consumption share from 2% in 1972 to 40% in 2010. In the United States, Western Europe, and Japan, most aluminium was consumed in transportation, engineering, construction, and packaging. In 2021, prices for industrial metals such as aluminium have soared to near-record levels as energy shortages in China drive up costs for electricity.

The names "aluminium" and "aluminum" are derived from the word "alumine", an obsolete term for "alumina", a naturally occurring oxide of aluminium. "Alumine" was borrowed from French, which in turn derived it from "alumen", the classical Latin name for alum, the mineral from which it was collected. The Latin word "alumen" stems from the Proto-Indo-European root "*alu-" meaning "bitter" or "beer".

British chemist Humphry Davy, who performed a number of experiments aimed to isolate the metal, is credited as the person who named the element. The first name proposed for the metal to be isolated from alum was "alumium", which Davy suggested in an 1808 article on his electrochemical research, published in Philosophical Transactions of the Royal Society. It appeared that the name was created from the English word "alum" and the Latin suffix "-ium"; but it was customary then to give elements names originating in Latin, so this name was not adopted universally. This name was criticized by contemporary chemists from France, Germany, and Sweden, who insisted the metal should be named for the oxide, alumina, from which it would be isolated. The English name "alum" does not come directly from Latin, whereas "alumine"/"alumina" obviously comes from the Latin word "alumen" (upon declension, "alumen" changes to "alumin-").

One example was "Essai sur la Nomenclature chimique" (July 1811), written in French by a Swedish chemist, Jöns Jacob Berzelius, in which the name "aluminium" is given to the element that would be synthesized from alum. (Another article in the same journal issue also refers to the metal whose oxide is the basis of sapphire, i.e. the same metal, as to "aluminium".) A January 1811 summary of one of Davy's lectures at the Royal Society mentioned the name "aluminium" as a possibility. The next year, Davy published a chemistry textbook in which he used the spelling "aluminum". Both spellings have coexisted since. Their usage is currently regional: "aluminum" dominates in the United States and Canada; "aluminium" is prevalent in the rest of the English-speaking world.

In 1812, British scientist Thomas Young wrote an anonymous review of Davy's book, in which he proposed the name "aluminium" instead of "aluminum", which he thought had a "less classical sound". This name caught on: although the ' spelling was occasionally used in Britain, the American scientific language used ' from the start. Most scientists throughout the world used ' in the 19th century; and it was entrenched in several other European languages, such as French, German, and Dutch. In 1828, an American lexicographer, Noah Webster, entered only the "aluminum" spelling in his "American Dictionary of the English Language". In the 1830s, the ' spelling gained usage in the United States; by the 1860s, it had become the more common spelling there outside science. In 1892, Hall used the ' spelling in his advertising handbill for his new electrolytic method of producing the metal, despite his constant use of the ' spelling in all the patents he filed between 1886 and 1903: it is unknown whether this spelling was introduced by mistake or intentionally; but Hall preferred "aluminum" since its introduction because it resembled "platinum", the name of a prestigious metal. By 1890, both spellings had been common in the United States, the ' spelling being slightly more common; by 1895, the situation had reversed; by 1900, "aluminum" had become twice as common as "aluminium"; in the next decade, the ' spelling dominated American usage. In 1925, the American Chemical Society adopted this spelling.

The International Union of Pure and Applied Chemistry (IUPAC) adopted "aluminium" as the standard international name for the element in 1990. In 1993, they recognized "aluminum" as an acceptable variant; the most recent 2005 edition of the IUPAC nomenclature of inorganic chemistry also acknowledges this spelling. IUPAC official publications use the "" spelling as primary, and they list both where it is appropriate.

The production of aluminium starts with the extraction of bauxite rock from the ground. The bauxite is processed and transformed using the Bayer process into alumina, which is then processed using the Hall–Héroult process, resulting in the final aluminium metal.

Aluminium production is highly energy-consuming, and so the producers tend to locate smelters in places where electric power is both plentiful and inexpensive. Production of one kilogram of aluminium requires 7 kilograms of oil energy equivalent, as compared to 1.5 kilograms for steel and 2 kilograms for plastic. As of 2019, the world's largest smelters of aluminium are located in China, India, Russia, Canada, and the United Arab Emirates, while China is by far the top producer of aluminium with a world share of fifty-five percent.

According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics, etc.) is . Much of this is in more-developed countries ( per capita) rather than less-developed countries ( per capita).

Bauxite is converted to alumina by the Bayer process. Bauxite is blended for uniform composition and then is ground. The resulting slurry is mixed with a hot solution of sodium hydroxide; the mixture is then treated in a digester vessel at a pressure well above atmospheric, dissolving the aluminium hydroxide in bauxite while converting impurities into relatively insoluble compounds:

After this reaction, the slurry is at a temperature above its atmospheric boiling point. It is cooled by removing steam as pressure is reduced. The bauxite residue is separated from the solution and discarded. The solution, free of solids, is seeded with small crystals of aluminium hydroxide; this causes decomposition of the [Al(OH)] ions to aluminium hydroxide. After about half of aluminium has precipitated, the mixture is sent to classifiers. Small crystals of aluminium hydroxide are collected to serve as seeding agents; coarse particles are converted to alumina by heating; the excess solution is removed by evaporation, (if needed) purified, and recycled.

The conversion of alumina to aluminium metal is achieved by the Hall–Héroult process. In this energy-intensive process, a solution of alumina in a molten () mixture of cryolite (NaAlF) with calcium fluoride is electrolyzed to produce metallic aluminium. The liquid aluminium metal sinks to the bottom of the solution and is tapped off, and usually cast into large blocks called aluminium billets for further processing.

Anodes of the electrolysis cell are made of carbon—the most resistant material against fluoride corrosion—and either bake at the process or are prebaked. The former, also called Söderberg anodes, are less power-efficient and fumes released during baking are costly to collect, which is why they are being replaced by prebaked anodes even though they save the power, energy, and labor to prebake the cathodes. Carbon for anodes should be preferably pure so that neither aluminium nor the electrolyte is contaminated with ash. Despite carbon's resistivity against corrosion, it is still consumed at a rate of 0.4–0.5 kg per each kilogram of produced aluminium. Cathodes are made of anthracite; high purity for them is not required because impurities leach only very slowly. The cathode is consumed at a rate of 0.02–0.04 kg per each kilogram of produced aluminium. A cell is usually terminated after 2–6 years following a failure of the cathode.

The Hall–Heroult process produces aluminium with a purity of above 99%. Further purification can be done by the Hoopes process. This process involves the electrolysis of molten aluminium with a sodium, barium, and aluminium fluoride electrolyte. The resulting aluminium has a purity of 99.99%.

Electric power represents about 20 to 40% of the cost of producing aluminium, depending on the location of the smelter. Aluminium production consumes roughly 5% of electricity generated in the United States. Because of this, alternatives to the Hall–Héroult process have been researched, but none has turned out to be economically feasible.

Recovery of the metal through recycling has become an important task of the aluminium industry. Recycling was a low-profile activity until the late 1960s, when the growing use of aluminium beverage cans brought it to public awareness. Recycling involves melting the scrap, a process that requires only 5% of the energy used to produce aluminium from ore, though a significant part (up to 15% of the input material) is lost as dross (ash-like oxide). An aluminium stack melter produces significantly less dross, with values reported below 1%.

White dross from primary aluminium production and from secondary recycling operations still contains useful quantities of aluminium that can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia), which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, the waste is used as a filler in asphalt and concrete.

The global production of aluminium in 2016 was 58.8 million metric tons. It exceeded that of any other metal except iron (1,231 million metric tons).

Aluminium is almost always alloyed, which markedly improves its mechanical properties, especially when tempered. For example, the common aluminium foils and beverage cans are alloys of 92% to 99% aluminium. The main alloying agents are copper, zinc, magnesium, manganese, and silicon (e.g., duralumin) with the levels of other metals in a few percent by weight. Aluminium, both wrought and cast, has been alloyed with: manganese, silicon, magnesium, copper and zinc among others. For example, the Kynal family of alloys was developed by the British chemical manufacturer Imperial Chemical Industries.

The major uses for aluminium metal are in:

The great majority (about 90%) of aluminium oxide is converted to metallic aluminium. Being a very hard material (Mohs hardness 9), alumina is widely used as an abrasive; being extraordinarily chemically inert, it is useful in highly reactive environments such as high pressure sodium lamps. Aluminium oxide is commonly used as a catalyst for industrial processes; e.g. the Claus process to convert hydrogen sulfide to sulfur in refineries and to alkylate amines. Many industrial catalysts are supported by alumina, meaning that the expensive catalyst material is dispersed over a surface of the inert alumina. Another principal use is as a drying agent or absorbent.
Several sulfates of aluminium have industrial and commercial application. Aluminium sulfate (in its hydrate form) is produced on the annual scale of several millions of metric tons. About two-thirds is consumed in water treatment. The next major application is in the manufacture of paper. It is also used as a mordant in dyeing, in pickling seeds, deodorizing of mineral oils, in leather tanning, and in production of other aluminium compounds. Two kinds of alum, ammonium alum and potassium alum, were formerly used as mordants and in leather tanning, but their use has significantly declined following availability of high-purity aluminium sulfate. Anhydrous aluminium chloride is used as a catalyst in chemical and petrochemical industries, the dyeing industry, and in synthesis of various inorganic and organic compounds. Aluminium hydroxychlorides are used in purifying water, in the paper industry, and as antiperspirants. Sodium aluminate is used in treating water and as an accelerator of solidification of cement.

Many aluminium compounds have niche applications, for example:

Despite its widespread occurrence in the Earth's crust, aluminium has no known function in biology. At pH 6–9 (relevant for most natural waters), aluminium precipitates out of water as the hydroxide and is hence not available; most elements behaving this way have no biological role or are toxic. Aluminium sulfate has an LD of 6207 mg/kg (oral, mouse), which corresponds to 435 grams (about one pound) for a person.

Aluminium is classified as a non-carcinogen by the United States Department of Health and Human Services. A review published in 1988 said that there was little evidence that normal exposure to aluminium presents a risk to healthy adult, and a 2014 multi-element toxicology review was unable to find deleterious effects of aluminium consumed in amounts not greater than 40 mg/day per kg of body mass. Most aluminium consumed will leave the body in feces; most of the small part of it that enters the bloodstream, will be excreted via urine; nevertheless some aluminium does pass the blood-brain barrier and is lodged preferentially in the brains of Alzheimer's patients. Evidence published in 1989 indicates that, for Alzheimer's patients, aluminium may act by electrostatically crosslinking proteins, thus down-regulating genes in the superior temporal gyrus.

Aluminium, although rarely, can cause vitamin D-resistant osteomalacia, erythropoietin-resistant microcytic anemia, and central nervous system alterations. People with kidney insufficiency are especially at a risk. Chronic ingestion of hydrated aluminium silicates (for excess gastric acidity control) may result in aluminium binding to intestinal contents and increased elimination of other metals, such as iron or zinc; sufficiently high doses (>50 g/day) can cause anemia.

During the 1988 Camelford water pollution incident people in Camelford had their drinking water contaminated with aluminium sulfate for several weeks. A final report into the incident in 2013 concluded it was unlikely that this had caused long-term health problems.

Aluminium has been suspected of being a possible cause of Alzheimer's disease, but research into this for over 40 years has found, , no good evidence of causal effect.

Aluminium increases estrogen-related gene expression in human breast cancer cells cultured in the laboratory. In very high doses, aluminium is associated with altered function of the blood–brain barrier. A small percentage of people have contact allergies to aluminium and experience itchy red rashes, headache, muscle pain, joint pain, poor memory, insomnia, depression, asthma, irritable bowel syndrome, or other symptoms upon contact with products containing aluminium.

Exposure to powdered aluminium or aluminium welding fumes can cause pulmonary fibrosis. Fine aluminium powder can ignite or explode, posing another workplace hazard.

Food is the main source of aluminium. Drinking water contains more aluminium than solid food; however, aluminium in food may be absorbed more than aluminium from water. Major sources of human oral exposure to aluminium include food (due to its use in food additives, food and beverage packaging, and cooking utensils), drinking water (due to its use in municipal water treatment), and aluminium-containing medications (particularly antacid/antiulcer and buffered aspirin formulations). Dietary exposure in Europeans averages to 0.2–1.5 mg/kg/week but can be as high as 2.3 mg/kg/week. Higher exposure levels of aluminium are mostly limited to miners, aluminium production workers, and dialysis patients.

Consumption of antacids, antiperspirants, vaccines, and cosmetics provide possible routes of exposure. Consumption of acidic foods or liquids with aluminium enhances aluminium absorption, and maltol has been shown to increase the accumulation of aluminium in nerve and bone tissues.

In case of suspected sudden intake of a large amount of aluminium, the only treatment is deferoxamine mesylate which may be given to help eliminate aluminium from the body by chelation. However, this should be applied with caution as this reduces not only aluminium body levels, but also those of other metals such as copper or iron.

High levels of aluminium occur near mining sites; small amounts of aluminium are released to the environment at the coal-fired power plants or incinerators. Aluminium in the air is washed out by the rain or normally settles down but small particles of aluminium remain in the air for a long time.

Acidic precipitation is the main natural factor to mobilize aluminium from natural sources and the main reason for the environmental effects of aluminium; however, the main factor of presence of aluminium in salt and freshwater are the industrial processes that also release aluminium into air.

In water, aluminium acts as a toxiс agent on gill-breathing animals such as fish when the water is acidic, in which aluminium may precipitate on gills, which causes loss of plasma- and hemolymph ions leading to osmoregulatory failure. Organic complexes of aluminium may be easily absorbed and interfere with metabolism in mammals and birds, even though this rarely happens in practice.

Aluminium is primary among the factors that reduce plant growth on acidic soils. Although it is generally harmless to plant growth in pH-neutral soils, in acid soils the concentration of toxic Al cations increases and disturbs root growth and function. Wheat has developed a tolerance to aluminium, releasing organic compounds that bind to harmful aluminium cations. Sorghum is believed to have the same tolerance mechanism.

Aluminium production possesses its own challenges to the environment on each step of the production process. The major challenge is the greenhouse gas emissions. These gases result from electrical consumption of the smelters and the byproducts of processing. The most potent of these gases are perfluorocarbons from the smelting process. Released sulfur dioxide is one of the primary precursors of acid rain.

Biodegradation of metallic aluminium is extremely rare; most aluminium-corroding organisms do not directly attack or consume the aluminium, but instead produce corrosive wastes. The fungus "Geotrichum candidum" can consume the aluminium in compact discs. The bacterium "Pseudomonas aeruginosa" and the fungus "Cladosporium resinae" are commonly detected in aircraft fuel tanks that use kerosene-based fuels (not avgas), and laboratory cultures can degrade aluminium.





Advanced Chemistry

Advanced Chemistry is a German hip hop group from Heidelberg, a scenic city in Baden-Württemberg, South Germany. Advanced Chemistry was founded in 1987 by Toni L, Linguist, Gee-One, DJ Mike MD (Mike Dippon) and MC Torch. Each member of the group holds German citizenship, and Toni L, Linguist, and Torch are of Italian, Ghanaian, and Haitian backgrounds, respectively.

Influenced by North American socially conscious rap and the Native tongues movement, Advanced Chemistry is regarded as one of the main pioneers in German hip hop. They were one of the first groups to rap in German (although their name is in English). Furthermore, their songs tackled controversial social and political issues, distinguishing them from early German hip hop group "Die Fantastischen Vier" (The Fantastic Four), which had a more light-hearted, playful, party image.

Advanced Chemistry frequently rapped about their lives and experiences as children of immigrants, exposing the marginalization experienced by most ethnic minorities in Germany, and the feelings of frustration and resentment that being denied a German identity can cause. The song "Fremd im eigenen Land" (Foreign in your own nation) was released by Advanced Chemistry in November 1992. The single became a staple in the German hip hop scene. It made a strong statement about the status of immigrants throughout Germany, as the group was composed of multi-national and multi-racial members. The video shows several members brandishing their German passports as a demonstration of their German citizenship to skeptical and unaccepting 'ethnic' Germans.

This idea of national identity is important, as many rap artists in Germany have been of foreign origin. These so-called "Gastarbeiter" (guest workers) children saw breakdance, graffiti, rap music, and hip hop culture as a means of expressing themselves. Since the release of "Fremd im eigenen Land", many other German-language rappers have also tried to confront anti-immigrant ideas and develop themes of citizenship. However, though many ethnic minority youth in Germany find these German identity themes appealing, others view the desire of immigrants to be seen as German negatively, and they have actively sought to revive and recreate concepts of identity in connection to traditional ethnic origins.

Advanced Chemistry helped to found the German chapter of the Zulu nation.

The rivalry between Advanced Chemistry and Die Fantastischen Vier has served to highlight a dichotomy in the routes that hip hop has taken in becoming a part of the German soundscape. While Die Fantastischen Vier may be said to view hip hop primarily as an aesthetic art form, Advanced Chemistry understand hip hop as being inextricably linked to the social and political circumstances under which it is created. For Advanced Chemistry, hip hop is a “vehicle of general human emancipation,”. In their undertaking of social and political issues, the band introduced the term "Afro-German" into the context of German hip hop, and the theme of race is highlighted in much of their music.

With the release of the single “Fremd im eigenen Land”, Advanced Chemistry separated itself from the rest of the rap being produced in Germany. This single was the first of its kind to go beyond simply imitating US rap and addressed the current issues of the time. Fremd im eigenen Land which translates to “foreign in my own country” dealt with the widespread racism that non-white German citizens faced. This change from simple imitation to political commentary was the start of German identification with rap. The sound of “Fremd im eigenen Land” was influenced by the 'wall of noise' created by Public Enemy's producers, The Bomb Squad.

After the reunification of Germany, an abundance of anti-immigrant sentiment emerged, as well as attacks on the homes of refugees in the early 1990s. Advanced Chemistry came to prominence in the wake of these actions because of their pro-multicultural society stance in their music. Advanced Chemistry's attitudes revolve around their attempts to create a distinct "Germanness" in hip hop, as opposed to imitating American hip hop as other groups had done. Torch has said, "What the Americans do is exotic for us because we don't live like they do. What they do seems to be more interesting and newer. But not for me. For me it's more exciting to experience my fellow Germans in new contexts...For me, it's interesting to see what the kids try to do that's different from what I know." Advanced Chemistry were the first to use the term "Afro-German" in a hip hop context. This was part of the pro-immigrant political message they sent via their music.

While Advanced Chemistry's use of the German language in their rap allows them to make claims to authenticity and true German heritage, bolstering pro-immigration sentiment, their style can also be problematic for immigrant notions of any real ethnic roots. Indeed, part of the Turkish ethnic minority of Frankfurt views Advanced Chemistry's appeal to the German image as a "symbolic betrayal of the right of ethnic minorities to 'roots' or to any expression of cultural heritage." In this sense, their rap represents a complex social discourse internal to the German soundscape in which they attempt to negotiate immigrant assimilation into a xenophobic German culture with the maintenance of their own separate cultural traditions. It is quite possibly the feelings of alienation from the pure-blooded German demographic that drive Advanced Chemistry to attack nationalistic ideologies by asserting their "Germanness" as a group composed primarily of ethnic others. The response to this pseudo-German authenticity can be seen in what Andy Bennett refers to as "alternative forms of local hip hop culture which actively seek to rediscover and, in many cases, reconstruct notions of identity tied to cultural roots." These alternative local hip hop cultures include oriental hip hop, the members of which cling to their Turkish heritage and are confused by Advanced Chemistry's elicitation of a German identity politics to which they technically do not belong. This cultural binary illustrates that rap has taken different routes in Germany and that, even among an already isolated immigrant population, there is still disunity and, especially, disagreement on the relative importance of assimilation versus cultural defiance. According to German hip hop enthusiast 9@home, Advanced Chemistry is part of a "hip-hop movement [which] took a clear stance for the minorities and against the [marginalization] of immigrants who...might be German on paper, but not in real life," which speaks to the group's hope of actually being recognized as German citizens and not foreigners, despite their various other ethnic and cultural ties.

Advanced Chemistry's work was rooted in German history and the country's specific political realities. However, they also drew inspiration from African-American hip-hop acts like A Tribe Called Quest and Public Enemy, who had helped bring a soulful sound and political consciousness to American hip-hop. One member, Torch, later explicitly listed his references on his solo song "Als (When I Was in School):" "My favorite subject, which was quickly discovered poetry in load Poets, awakens the intellect or policy at Chuck D I'll never forget the lyrics by Public Enemy." Torch goes on to list other American rappers like Biz Markie, Big Daddy Kane and Dr. Dre as influences.



El-Tayeb, Fatima “‘If You Cannot Pronounce My Name, You Can Just Call Me 
Pride.’ Afro-German Activism, Gender, and Hip Hop,” "Gender & History"15/3(2003):459-485.

Felbert, Oliver von. “Die Unbestechlichen.” "Spex" (March 1993): 50–53.

Weheliye, Alexander G. "Phonographies:Grooves in Sonic Afro-Modernity", Duke University Press, 2005.

Anglican Communion

The Anglican Communion is the third largest Christian communion after the Roman Catholic and Eastern Orthodox churches. Founded in 1867 in London, the communion has more than 85 million members within the Church of England and other autocephalous national and regional churches in full communion. The traditional origins of Anglican doctrine are summarised in the Thirty-nine Articles (1571). The archbishop of Canterbury (, Justin Welby) in England acts as a focus of unity, recognised as "" ("first among equals"), but does not exercise authority in Anglican provinces outside of the Church of England. Most, but not all, member churches of the communion are the historic national or regional Anglican churches.

The Anglican Communion was officially and formally organised and recognised as such at the Lambeth Conference in 1867 in London under the leadership of Charles Longley, Archbishop of Canterbury. The churches of the Anglican Communion consider themselves to be part of the one, holy, catholic and apostolic church, and to be both catholic and Reformed. As in the Church of England itself, the Anglican Communion includes the broad spectrum of beliefs and liturgical practises found in the Evangelical, Central and Anglo-Catholic traditions of Anglicanism. Each national or regional church is fully independent, retaining its own legislative process and episcopal polity under the leadership of local primates. For some adherents, Anglicanism represents a non-papal Catholicism, for others a form of Protestantism though without a guiding figure such as Martin Luther, John Knox, John Calvin, Huldrych Zwingli, John Wesley or Jan Hus, or, for yet others, a combination of the two.

Most of its members live in the Anglosphere of former British territories. Full participation in the sacramental life of each church is available to all communicant members. Because of their historical link to England ("ecclesia anglicana" means "English church"), some of the member churches are known as "Anglican", such as the Anglican Church of Canada. Others, for example the Church of Ireland and the Scottish and American Episcopal churches, have official names that do not include "Anglican". Conversely, some churches that do use the name "Anglican" are not part of the communion. These have generally disaffiliated over disagreement with the direction of the communion.

On 20 February 2023, ten communion provinces and Anglican realignment churches within Global South Fellowship of Anglican Churches released a statement stating that they had declared "impaired communion" with the Church of England and no longer recognised Justin Welby as "first among equals" among the bishops of the communion, "de facto" marking a schism within the Anglican Communion.

The Anglican Communion traces much of its growth to the older mission organisations of the Church of England such as the Society for Promoting Christian Knowledge (founded 1698), the Society for the Propagation of the Gospel in Foreign Parts (founded 1701) and the Church Missionary Society (founded 1799). The Church of England (which until the 20th century included the Church in Wales) initially separated from the Roman Catholic Church in 1534 in the reign of Henry VIII, reunited in 1555 under Mary I and then separated again in 1570 under Elizabeth I (the Roman Catholic Church excommunicated Elizabeth I in 1570 in response to the Act of Supremacy 1559).

The Church of England has always thought of itself not as a new foundation but rather as a reformed continuation of the ancient "English Church" ("Ecclesia Anglicana") and a reassertion of that church's rights. As such it was a distinctly national phenomenon. The Church of Scotland was formed as a separate church from the Roman Catholic Church as a result of the Scottish Reformation in 1560 and the later formation of the Scottish Episcopal Church began in 1582 in the reign of James VI over disagreements about the role of bishops.

The oldest-surviving Anglican church building outside the British Isles (Britain and Ireland) is St Peter's Church in St. George's, Bermuda, established in 1612 (though the actual building had to be rebuilt several times over the following century). This is also the oldest surviving non-Roman Catholic church in the New World. It remained part of the Church of England until 1978 when the Anglican Church of Bermuda separated. The Church of England was the established church not only in England, but in its trans-Oceanic colonies.

Thus the only member churches of the present Anglican Communion existing by the mid-18th century were the Church of England, its closely linked sister church the Church of Ireland (which also separated from Roman Catholicism under Henry VIII) and the Scottish Episcopal Church which for parts of the 17th and 18th centuries was partially underground (it was suspected of Jacobite sympathies).

The enormous expansion in the 18th and 19th centuries of the British Empire brought Anglicanism along with it. At first all these colonial churches were under the jurisdiction of the bishop of London. After the American Revolution, the parishes in the newly independent country found it necessary to break formally from a church whose supreme governor was (and remains) the British monarch. Thus they formed their own dioceses and national church, the Episcopal Church in the United States of America, in a mostly amicable separation.

At about the same time, in the colonies which remained linked to the crown, the Church of England began to appoint colonial bishops. In 1787, Charles Inglis (Bishop of Nova Scotia) was appointed with a jurisdiction over all of British North America; in time several more colleagues were appointed to other cities in present-day Canada. In 1814, a bishop of Calcutta was made; in 1824 the first bishop was sent to the West Indies and in 1836 to Australia. By 1840 there were still only ten colonial bishops for the Church of England; but even this small beginning greatly facilitated the growth of Anglicanism around the world. In 1841, a "Colonial Bishoprics Council" was set up and soon many more dioceses were created.

In time, it became natural to group these into provinces and a metropolitan bishop was appointed for each province. Although it had at first been somewhat established in many colonies, in 1861 it was ruled that, except where specifically established, the Church of England had just the same legal position as any other church. Thus a colonial bishop and colonial diocese was by nature quite a different thing from their counterparts back home. In time bishops came to be appointed locally rather than from England and eventually national synods began to pass ecclesiastical legislation independent of England.

A crucial step in the development of the modern communion was the idea of the Lambeth Conferences (discussed above). These conferences demonstrated that the bishops of disparate churches could manifest the unity of the church in their episcopal collegiality despite the absence of universal legal ties. Some bishops were initially reluctant to attend, fearing that the meeting would declare itself a council with power to legislate for the church; but it agreed to pass only advisory resolutions. These Lambeth Conferences have been held roughly every ten years since 1878 (the second such conference) and remain the most visible coming-together of the whole communion.

The Lambeth Conference of 1998 included what has been seen by Philip Jenkins and others as a "watershed in global Christianity". The 1998 Lambeth Conference considered the issue of the theology of same-sex attraction in relation to human sexuality. At this 1998 conference for the first time in centuries the Christians of developing regions, especially, Africa, Asia, and Latin America, prevailed over the bishops of more prosperous countries (many from the US, Canada, and the UK) who supported a redefinition of Anglican doctrine. Seen in this light 1998 is a date that marked the shift from a West-dominated Christianity to one wherein the growing churches of the two-thirds world are predominant.

One effect of the Anglican Communion's dispersed authority has been the conflicts arising over divergent practices and doctrines in parts of the communion. Disputes that had been confined to the Church of England could be dealt with legislatively in that realm, but as the Communion spread out into new nations and disparate cultures, such controversies multiplied and intensified. These controversies have generally been of two types: liturgical and social.

Rapid social change and the dissipation of British cultural hegemony over its former colonies contributed to disputes over the role of women, and the parameters of marriage and divorce. In the late 1970s, the Continuing Anglican movement produced a number of new church bodies in opposition to women's ordination, prayer book changes, and the new understandings concerning marriage.

The first such controversy of note concerned that of the growing influence of the Catholic Revival manifested in the Tractarian and so-called Ritualist controversies of the late nineteenth and early twentieth centuries. This controversy produced the Free Church of England and, in the United States and Canada, the Reformed Episcopal Church.

While individual Anglicans and member churches within the Communion differ in good faith over the circumstances in which abortion should or should not be permitted, Lambeth Conference resolutions have consistently held to a conservative view on the issue. The 1930 Conference, the first to be held since the initial legalisation of abortion in Europe (in Russia in 1920), stated:

The 1958 Conference's "Family in Contemporary Society" report affirmed the following position on abortion and was commended by the 1968 Conference:

The subsequent Lambeth Conference, in 1978, made no change to this position and commended the need for "programmes at diocesan level, involving both men and women ... to emphasise the sacredness of all human life, the moral issues inherent in clinical abortion, and the possible implications of genetic engineering." In the context of debates around and proposals for the legalisation of euthanasia, the 1998 Conference affirmed that "life is God-given and has intrinsic sanctity, significance and worth".

More recently, disagreements over homosexuality have strained the unity of the communion as well as its relationships with other Christian denominations, leading to another round of withdrawals from the Anglican Communion. Some churches were founded outside the Anglican Communion in the late 20th and early 21st centuries, largely in opposition to the ordination of openly homosexual bishops and other clergy and are usually referred to as belonging to the Anglican realignment movement, or else as "orthodox" Anglicans. These disagreements were especially noted when the Episcopal Church (US) consecrated an openly gay bishop in a same-sex relationship, Gene Robinson, in 2003, which led some Episcopalians to defect and found the Anglican Church in North America (ACNA); then, the debate reignited when the Church of England agreed to allow clergy to enter into same-sex civil partnerships, as long as they remained celibate, in 2005. The Church of Nigeria opposed the Episcopal Church's decision as well as the Church of England's approval for celibate civil partnerships.

"The more liberal provinces that are open to changing Church doctrine on marriage in order to allow for same-sex unions include Brazil, Canada, New Zealand, Scotland, South India, South Africa, the US and Wales". In 2023, the Church of England announced that it will authorise "prayers of thanksgiving, dedication and for God's blessing for same-sex couples". The Church of England also permits clergy to enter into same-sex civil partnerships. The Church of Ireland has no official position on civil unions, and one senior cleric has entered into a same-sex civil partnership. The Church of Ireland recognised that it will "treat civil partners the same as spouses". The Anglican Church of Australia does not have an official position on homosexuality.

The conservative Anglican churches encouraging the realignment movement are more concentrated in the Global South. For example, the Anglican Church of Kenya, the Church of Nigeria and the Church of Uganda have opposed homosexuality. GAFCON, a fellowship of conservative Anglican churches, has appointed "missionary bishops" in response to the disagreements with the perceived liberalisation in the Anglican churches in North America and Europe. In 2023, ten archbishops within the Anglican Communion and two breakaway churches in North America and Brazil from the Global South Fellowship of Anglican Churches (GSFA) declared a state of impaired communion with the Church of England and announced that they would no longer recognise the archbishop of Canterbury as the "first among equals" among the bishops in the Anglican Communion. However, in the same statement, the ten archbishops said that they would not leave the Anglican Communion.

Debates about social theology and ethics have occurred at the same time as debates on prayer book revision and the acceptable grounds for achieving full communion with non-Anglican churches.

The Anglican Communion has no official legal existence nor any governing structure that might exercise authority over the member churches. There is an Anglican Communion Office in London, under the aegis of the archbishop of Canterbury, but it serves only in a supporting and organisational role. The communion is held together by a shared history, expressed in its ecclesiology, polity and ethos, and also by participation in international consultative bodies.

Three elements have been important in holding the communion together: first, the shared ecclesial structure of the component churches, manifested in an episcopal polity maintained through the apostolic succession of bishops and synodical government; second, the principle of belief expressed in worship, investing importance in approved prayer books and their rubrics; and third, the historical documents and the writings of early Anglican divines that have influenced the ethos of the communion.

Originally, the Church of England was self-contained and relied for its unity and identity on its own history, its traditional legal and episcopal structure, and its status as an established church of the state. As such, Anglicanism was from the outset a movement with an explicitly episcopal polity, a characteristic that has been vital in maintaining the unity of the communion by conveying the episcopate's role in manifesting visible catholicity and ecumenism.

Early in its development following the English Reformation, Anglicanism developed a vernacular prayer book, called the "Book of Common Prayer". Unlike other traditions, Anglicanism has never been governed by a magisterium nor by appeal to one founding theologian, nor by an extra-credal summary of doctrine (such as the Westminster Confession of the Presbyterian churches). Instead, Anglicans have typically appealed to the "Book of Common Prayer" (1662) and its offshoots as a guide to Anglican theology and practise. This has had the effect of inculcating in Anglican identity and confession the principle of ("the law of praying [is] the law of believing").

Protracted conflict through the 17th century, with radical Protestants on the one hand and Roman Catholics who recognised the primacy of the Pope on the other, resulted in an association of churches that was both deliberately vague about doctrinal principles, yet bold in developing parameters of acceptable deviation. These parameters were most clearly articulated in the various rubrics of the successive prayer books, as well as the Thirty-nine Articles of Religion (1563). These articles have historically shaped and continue to direct the ethos of the communion, an ethos reinforced by its interpretation and expansion by such influential early theologians such as Richard Hooker, Lancelot Andrewes and John Cosin.

With the expansion of the British Empire and the growth of Anglicanism outside Great Britain and Ireland, the communion sought to establish new vehicles of unity. The first major expressions of this were the Lambeth Conferences of the communion's bishops, first convened in 1867 by Charles Longley, the archbishop of Canterbury. From the beginning, these were not intended to displace the autonomy of the emerging provinces of the communion, but to "discuss matters of practical interest, and pronounce what we deem expedient in resolutions which may serve as safe guides to future action".

One of the enduringly influential early resolutions of the conference was the so-called Chicago-Lambeth Quadrilateral of 1888. Its intent was to provide the basis for discussions of reunion with the Roman Catholic and Orthodox churches, but it had the ancillary effect of establishing parameters of Anglican identity. It establishes four principles with these words:
As mentioned above, the Anglican Communion has no international juridical organisation. The archbishop of Canterbury's role is strictly symbolic and unifying and the communion's three international bodies are consultative and collaborative, their resolutions having no legal effect on the autonomous provinces of the communion. Taken together, however, the four do function as "instruments of communion", since all churches of the communion participate in them. In order of antiquity, they are:


Since there is no binding authority in the Anglican Communion, these international bodies are a vehicle for consultation and persuasion. In recent times, persuasion has tipped over into debates over conformity in certain areas of doctrine, discipline, worship and ethics. The most notable example has been the objection of many provinces of the communion (particularly in Africa and Asia) to the changing acceptance of LGBTQ+ individuals in the North American churches (e.g., by blessing same-sex unions and ordaining and consecrating same-sex relationships) and to the process by which changes were undertaken. (See Anglican realignment)

Those who objected condemned these actions as unscriptural, unilateral, and without the agreement of the communion prior to these steps being taken. In response, the American Episcopal Church and the Anglican Church of Canada answered that the actions had been undertaken after lengthy scriptural and theological reflection, legally in accordance with their own canons and constitutions and after extensive consultation with the provinces of the communion.

The Primates' Meeting voted to request the two churches to withdraw their delegates from the 2005 meeting of the Anglican Consultative Council. Canada and the United States decided to attend the meeting but without exercising their right to vote. They have not been expelled or suspended, since there is no mechanism in this voluntary association to suspend or expel an independent province of the communion. Since membership is based on a province's communion with Canterbury, expulsion would require the archbishop of Canterbury's refusal to be in communion with the affected jurisdictions. In line with the suggestion of the Windsor Report, Rowan Williams (the then archbishop of Canterbury) established a working group to examine the feasibility of an Anglican covenant which would articulate the conditions for communion in some fashion.

The Anglican Communion consists of forty-two autonomous provinces each with its own primate and governing structure. These provinces may take the form of national churches (such as in Canada, Uganda, or Japan) or a collection of nations (such as the West Indies, Central Africa, or Southeast Asia).

In addition to the forty-two provinces, there are five extraprovincial churches under the metropolitical authority of the archbishop of Canterbury.

In September 2020, the Archbishop of Canterbury announced that he had asked the bishops of the Church of Ceylon to begin planning for the formation of an autonomous province of Ceylon, so as to end his current position as metropolitan of the two dioceses in that country.

In addition to other member churches, the churches of the Anglican Communion are in full communion with the Old Catholic churches of the Union of Utrecht and the Scandinavian Lutheran churches of the Porvoo Communion in Europe, the India-based Malankara Mar Thoma Syrian and Malabar Independent Syrian churches and the Philippine Independent Church, also known as the Aglipayan Church.

The churches of the Anglican Communion have traditionally held that ordination in the historic episcopate is a core element in the validity of clerical ordinations. The Roman Catholic Church, however, does not recognise Anglican orders (see "Apostolicae curae"). Some Eastern Orthodox churches have issued statements to the effect that Anglican orders could be accepted, yet have still reordained former Anglican clergy; other Eastern Orthodox churches have rejected Anglican orders altogether. Orthodox bishop Kallistos Ware explains this apparent discrepancy as follows:


Arne Kaijser

Arne Kaijser (born 1950) is a professor emeritus of history of technology at the KTH Royal Institute of Technology in Stockholm, and a former president of the Society for the History of Technology.

Kaijser has published two books in Swedish: "Stadens ljus. Etableringen av de första svenska gasverken" and "I fädrens spår. Den svenska infrastrukturens historiska utveckling och framtida utmaningar", and has co-edited several anthologies. Kaijser is a member of the Royal Swedish Academy of Engineering Sciences since 2007 and also a member of the editorial board of two scientific journals: "Journal of Urban Technology" and "Centaurus". Lately, he has been occupied with the history of Large Technical Systems.


Archipelago

An archipelago ( ), sometimes called an island group or island chain, is a chain, cluster, or collection of islands, or sometimes a sea containing a small number of scattered islands.
Examples of archipelagos include: the Indonesian Archipelago, the Andaman and Nicobar Islands, the Lakshadweep Islands, the Galápagos Islands, the Japanese archipelago, the Philippine Archipelago, the Maldives, the Balearic Islands, the Åland Islands, The Bahamas, the Aegean Islands, the Hawaiian Islands, the Canary Islands, Malta, the Azores, the Canadian Arctic Archipelago, the British Isles, the islands of the Archipelago Sea, and Shetland. Archipelagos are sometimes defined by political boundaries. For example, while they are geopolitically divided, the San Juan Islands and Gulf Islands geologically form part of a larger Gulf Archipelago.

The word "archipelago" is derived from the Ancient Greek ἄρχι-("arkhi-", "chief") and πέλαγος ("pélagos", "sea") through the Italian "arcipelago". In antiquity, "Archipelago" (from Medieval Greek *ἀρχιπέλαγος and Latin ) was the proper name for the Aegean Sea. Later, usage shifted to refer to the Aegean Islands (since the sea has a large number of islands).

Archipelagos may be found isolated in large amounts of water or neighbouring a large land mass. For example, Scotland has more than 700 islands surrounding its mainland, which form an archipelago.

Archipelagos are often volcanic, forming along island arcs generated by subduction zones or hotspots, but may also be the result of erosion, deposition, and land elevation. Depending on their geological origin, islands forming archipelagos can be referred to as "oceanic islands", "continental fragments", or "continental islands".

Oceanic islands are mainly of volcanic origin, and widely separated from any adjacent continent. The Hawaiian Islands and Galapagos Islands in the Pacific, and Mascarene Islands in the south Indian Ocean are examples.

Continental fragments correspond to land masses that have separated from a continental mass due to tectonic displacement. The Farallon Islands off the coast of California are an example.

Sets of islands formed close to the coast of a continent are considered continental archipelagos when they form part of the same continental shelf, when those islands are above-water extensions of the shelf. The islands of the Inside Passage off the coast of British Columbia and the Canadian Arctic Archipelago are examples.

Artificial archipelagos have been created in various countries for different purposes. Palm Islands and The World Islands off Dubai were or are being created for leisure and tourism purposes. Marker Wadden in the Netherlands is being built as a conservation area for birds and other wildlife.

The largest archipelago in the world by number of islands is the Archipelago Sea, which is part of Finland. There are approximately 40,000, mostly uninhabited, islands 

The largest archipelagic state in the world by area, and by population, is Indonesia.



Author

In legal discourse, an author is the creator of an original work, whether that work is in written, graphic, or recorded medium. The creation of such a work is an act of authorship. Thus, a sculptor, painter, or composer, is an author of their respective sculptures, paintings, or compositions, even though in common parlance, an author is often thought of as the writer of a book, article, play, or other written work. In the case of a work for hire, the employer or commissioning party is considered the author of the work, even if they did not write or otherwise create the work, but merely instructed another individual to do so.

Typically, the first owner of a copyright is the person who created the work, i.e. the author. If more than one person created the work, then a case of joint authorship takes place. Copyright laws differ around the world. The United States Copyright Office, for example, defines copyright as "a form of protection provided by the laws of the United States (title 17, U.S. Code) to authors of 'original works of authorship.'"

Some works are considered to be authorless. For example, the monkey selfie copyright dispute in the 2010s involved photographs taken by Celebes crested macaques using equipment belonging to a nature photographer. The photographer asserted authorship of the photographs, which the United States Copyright Office denied, stating: "To qualify as a work of 'authorship' a work must be created by a human being". More recently, questions have arisen as to whether images or text created by a generative artificial intelligence have an author.

Holding the title of "author" over any "literary, dramatic, musical, artistic, [or] certain other intellectual works" gives rights to this person, the owner of the copyright, especially the exclusive right to engage in or authorize any production or distribution of their work. Any person or entity wishing to use intellectual property held under copyright must receive permission from the copyright holder to use this work, and often will be asked to pay for the use of copyrighted material.

The copyrights on intellectual work expire after a certain time. It enters the public domain, where it can be used without limit. Copyright laws in many jurisdictions – mostly following the lead of the United States, in which the entertainment and publishing industries have very strong lobbying power – have been amended repeatedly since their inception, to extend the length of this fixed period where the work is exclusively controlled by the copyright holder. Technically, someone owns their work from the time it's created. A notable aspect of authorship emerges with copyright in that, in many jurisdictions, it can be passed down to another, upon one's death. The person who inherits the copyright is not the author, but has access to the same legal benefits.

Intellectual property laws are complex. Works of fiction involve trademark law, likeness rights, fair use rights held by the public (including the right to parody or satirize), and many other interacting complications.

Authors may portion out the different rights that they hold to different parties at different times, and for different purposes or uses, such as the right to adapt a plot into a film, television series, or video game. If another party chooses to adapt the work, they may have to alter plot elements or character names in order to avoid infringing previous adaptations. An author may also not have rights when working under contract that they would otherwise have, such as when creating a work for hire (e.g., hired to write a city tour guide by a municipal government that totally owns the copyright to the finished work), or when writing material using intellectual property owned by others (such as when writing a novel or screenplay that is a new installment in an already established media franchise).

In the United States, the Copyright Clause of the Constitution of the United States () provides the Congress with the power of "securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries". The language regarding authors was derived from proposals by Charles Pinckney, "to secure to authors exclusive rights for a limited time", and by James Madison, "to secure to literary authors their copyrights for a limited time", or, in the alternative, "to encourage, by proper premiums & Provisions, the advancement of useful knowledge and discoveries". Both proposals were referred to the Committee of Detail, which reported back a proposal containing the final language, which was incorporated into the Constitution by unanimous agreement of the convention.

In literary theory, critics find complications in the term "author" beyond what constitutes authorship in a legal setting. In the wake of postmodern literature, critics such as Roland Barthes and Michel Foucault have examined the role and relevance of authorship to the meaning or interpretation of a literary text.

Barthes challenges the idea that a text can be attributed to any single author. He writes, in his essay "Death of the Author" (1968), that "it is language which speaks, not the author." The words and language of a text itself determine and expose meaning for Barthes, and not someone possessing legal responsibility for the process of its production. Every line of written text is a mere reflection of references from any of a multitude of traditions, or, as Barthes puts it, "the text is a tissue of quotations drawn from the innumerable centers of culture"; it is never original. With this, the perspective of the author is removed from the text, and the limits formerly imposed by the idea of one authorial voice, one ultimate and universal meaning, are destroyed. The explanation and meaning of a work does not have to be sought in the one who produced it, "as if it were always in the end, through the more or less transparent allegory of the fiction, the voice of a single person, the author 'confiding' in us." The psyche, culture, fanaticism of an author can be disregarded when interpreting a text, because the words are rich enough themselves with all of the traditions of language. To expose meanings in a written work without appealing to the celebrity of an author, their tastes, passions, vices, is, to Barthes, to allow language to speak, rather than author.

Michel Foucault argues in his essay "What is an author?" (1969) that all authors are writers, but not all writers are authors. He states that "a private letter may have a signatory—it does not have an author." For a reader to assign the title of author upon any written work is to attribute certain standards upon the text which, for Foucault, are working in conjunction with the idea of "the author function." Foucault's author function is the idea that an author exists only as a function of a written work, a part of its structure, but not necessarily part of the interpretive process. The author's name "indicates the status of the discourse within a society and culture," and at one time was used as an anchor for interpreting a text, a practice which Barthes would argue is not a particularly relevant or valid endeavour.

Expanding upon Foucault's position, Alexander Nehamas writes that Foucault suggests "an author [...] is whoever can be understood to have produced a particular text as we interpret it," not necessarily who penned the text. It is this distinction between producing a written work and producing the interpretation or meaning in a written work that both Barthes and Foucault are interested in. Foucault warns of the risks of keeping the author's name in mind during interpretation, because it could affect the value and meaning with which one handles an interpretation.

Literary critics Barthes and Foucault suggest that readers should not rely on or look for the notion of one overarching voice when interpreting a written work, because of the complications inherent with a writer's title of "author." They warn of the dangers interpretations could suffer from when associating the subject of inherently meaningful words and language with the personality of one authorial voice. Instead, readers should allow a text to be interpreted in terms of the language as "author."

Self-publishing is a model where the author takes full responsibility and control of arranging financing, editing, printing, and distribution of their own work. In other words, the author also acts as the publisher of their work.

With commissioned publishing, the publisher makes all the publication arrangements and the author covers all expenses.

The author of a work may receive a percentage calculated on a wholesale or a specific price or a fixed amount on each book sold. Publishers, at times, reduced the risk of this type of arrangement, by agreeing only to pay this after a certain number of copies had sold. In Canada, this practice occurred during the 1890s, but was not commonplace until the 1920s. Established and successful authors may receive advance payments, set against future royalties, but this is no longer common practice. Most independent publishers pay royalties as a percentage of net receipts – how net receipts are calculated varies from publisher to publisher. Under this arrangement, the author does not pay anything towards the expense of publication. The costs and financial risk are all carried by the publisher, who will then take the greatest percentage of the receipts. See Compensation for more.

Vanity publishers normally charge a flat fee for arranging publication, offer a platform for selling, and then take a percentage of the sale of every copy of a book. The author receives the rest of the money made. Most materials published this way are for niche groups and not for large audiences.

Vanity publishing, or subsidy publishing, is stigmatized in the professional world. In 1983, Bill Henderson defined vanity publishers as people who would "publish anything for which an author will pay, usually at a loss for the author and a nice profit for the publisher." In subsidy publishing, the book sales are not the publishers' main source of income, but instead the fees that the authors are charged to initially produce the book are. Because of this, the vanity publishers need not invest in making books marketable as much as other publishers need to. This leads to low quality books being introduced to the market.

The relationship between the author and the editor, often the author's only liaison to the publishing company, is typically characterized as the site of tension. For the author to reach their audience, often through publication, the work usually must attract the attention of the editor. The idea of the author as the sole meaning-maker of necessity changes to include the influences of the editor and the publisher to engage the audience in writing as a social act. 

There are three principal kinds of editing: 

Pierre Bourdieu's essay "The Field of Cultural Production" depicts the publishing industry as a "space of literary or artistic position-takings," also called the "field of struggles," which is defined by the tension and movement inherent among the various positions in the field. Bourdieu claims that the "field of position-takings [...] is not the product of coherence-seeking intention or objective consensus," meaning that an industry characterized by position-takings is not one of harmony and neutrality. In particular for the writer, their authorship in their work makes their work part of their identity, and there is much at stake personally over the negotiation of authority over that identity. However, it is the editor who has "the power to impose the dominant definition of the writer and therefore to delimit the population of those entitled to take part in the struggle to define the writer". As "cultural investors," publishers rely on the editor position to identify a good investment in "cultural capital" which may grow to yield economic capital across all positions.

According to the studies of James Curran, the system of shared values among editors in Britain has generated a pressure among authors to write to fit the editors' expectations, removing the focus from the reader-audience and putting a strain on the relationship between authors and editors and on writing as a social act. Even the book review by the editors has more significance than the readership's reception.

Authors rely on advance fees, royalty payments, adaptation of work to a screenplay, and fees collected from giving speeches.

A standard contract for an author will usually include provision for payment in the form of an advance and royalties.



Usually, an author's book must earn the advance before any further royalties are paid. For example, if an author is paid a modest advance of $2000, and their royalty rate is 10% of a book priced at $20 – that is, $2 per book – the book will need to sell 1000 copies before any further payment will be made. Publishers typically withhold payment of a percentage of royalties earned against returns.

In some countries, authors also earn income from a government scheme such as the ELR (educational lending right) and PLR (public lending right) schemes in Australia. Under these schemes, authors are paid a fee for the number of copies of their books in educational and/or public libraries.

These days, many authors supplement their income from book sales with public speaking engagements, school visits, residencies, grants, and teaching positions.

Ghostwriters, technical writers, and textbooks writers are typically paid in a different way: usually a set fee or a per word rate rather than on a percentage of sales.

In the year 2016, according to the U.S. Bureau of Labor Statistics, nearly 130,000 people worked in the country as authors, making an average of $61,240 per year.


Andrey Markov

Andrey Andreyevich Markov (14 June 1856 – 20 July 1922) was a Russian mathematician best known for his work on stochastic processes. A primary subject of his research later became known as the Markov chain. He was also a strong, close to master-level chess player.

Markov and his younger brother Vladimir Andreevich Markov (1871–1897) proved the Markov brothers' inequality.
His son, another Andrey Andreyevich Markov (1903–1979), was also a notable mathematician, making contributions to constructive mathematics and recursive function theory.

Andrey Markov was born on 14 June 1856 in Russia. He attended the St. Petersburg Grammar School, where some teachers saw him as a rebellious student. In his academics he performed poorly in most subjects other than mathematics. Later in life he attended Saint Petersburg Imperial University (now Saint Petersburg State University). Among his teachers were Yulian Sokhotski (differential calculus, higher algebra), Konstantin Posse (analytic geometry), Yegor Zolotarev (integral calculus), Pafnuty Chebyshev (number theory and probability theory), Aleksandr Korkin (ordinary and partial differential equations), Mikhail Okatov (mechanism theory), Osip Somov (mechanics), and Nikolai Budajev (descriptive and higher geometry). He completed his studies at the university and was later asked if he would like to stay and have a career as a Mathematician. He later taught at high schools and continued his own mathematical studies. In this time he found a practical use for his mathematical skills. He figured out that he could use chains to model the alliteration of vowels and consonants in Russian literature. He also contributed to many other mathematical aspects in his time. He died at age 66 on 20 July 1922.

In 1877, Markov was awarded a gold medal for his outstanding solution of the problem

"About Integration of Differential Equations by Continued Fractions with an Application to the Equation" formula_1.

During the following year, he passed the candidate's examinations, and he remained at the university to prepare for a lecturer's position.

In April 1880, Markov defended his master's thesis "On the Binary Square Forms with Positive Determinant", which was directed by Aleksandr Korkin and Yegor Zolotarev. Four years later in 1884, he defended his doctoral thesis titled "On Certain Applications of the Algebraic Continuous Fractions".

His pedagogical work began after the defense of his master's thesis in autumn 1880. As a privatdozent he lectured on differential and integral calculus. Later he lectured alternately on "introduction to analysis", probability theory (succeeding Chebyshev, who had left the university in 1882) and the calculus of differences. From 1895 through 1905 he also lectured in differential calculus.
One year after the defense of his doctoral thesis, Markov was appointed extraordinary professor (1886) and in the same year he was elected adjunct to the Academy of Sciences. In 1890, after the death of Viktor Bunyakovsky, Markov became an extraordinary member of the academy. His promotion to an ordinary professor of St. Petersburg University followed in the fall of 1894.

In 1896, Markov was elected an ordinary member of the academy as the successor of Chebyshev. In 1905, he was appointed merited professor and was granted the right to retire, which he did immediately. Until 1910, however, he continued to lecture in the calculus of differences.

In connection with student riots in 1908, professors and lecturers of St. Petersburg University were ordered to monitor their students. Markov refused to accept this decree, and he wrote an explanation in which he declined to be an "agent of the governance". Markov was removed from further teaching duties at St. Petersburg University, and hence he decided to retire from the university.

Markov was an atheist. In 1912, he responded to Leo Tolstoy's excommunication from the Russian Orthodox Church by requesting his own excommunication. The Church complied with his request.
In 1913, the council of St. Petersburg elected nine scientists honorary members of the university. Markov was among them, but his election was not affirmed by the minister of education. The affirmation only occurred four years later, after the February Revolution in 1917. Markov then resumed his teaching activities and lectured on probability theory and the calculus of differences until his death in 1922.


Angst

Angst is fear or anxiety ("anguish" is its Latinate equivalent, and the words "anxious" and "anxiety" are of similar origin). The dictionary definition for angst is a feeling of anxiety, apprehension, or insecurity.

The word "angst" was introduced into English from the Danish, Norwegian, and Dutch word and the German word . It is attested since the 19th century in English translations of the works of Søren Kierkegaard and Sigmund Freud. It is used in English to describe an intense feeling of apprehension, anxiety, or inner turmoil.

In other languages (with words from the Latin for "fear" or "panic"), the derived words differ in meaning; for example, as in the French and . The word "angst" has existed in German since the 8th century, from the Proto-Indo-European root "", "restraint" from which Old High German developed. It is pre-cognate with the Latin , "tensity, tightness" and , "choking, clogging"; compare to the Ancient Greek () "strangle". It entered English in the 19th century as a technical term used in Psychiatry, though earlier cognates existed, such as "ange".

In existentialist philosophy, the term "angst" carries a specific conceptual meaning. The use of the term was first attributed to Danish philosopher Søren Kierkegaard (1813–1855). In "The Concept of Anxiety" (also known as "The Concept of Dread"), Kierkegaard used the word "Angest" (in common Danish, "angst", meaning "dread" or "anxiety") to describe a profound and deep-seated condition. Where non-human animals are guided solely by instinct, said Kierkegaard, human beings enjoy a freedom of choice that we find both appealing and terrifying. It is the anxiety of understanding of being free when considering undefined possibilities of one's life and the immense responsibility of having the power of choice over them. Kierkegaard's concept of angst reappeared in the works of existentialist philosophers who followed, such as Friedrich Nietzsche, Jean-Paul Sartre, and Martin Heidegger, each of whom developed the idea further in individual ways. While Kierkegaard's angst referred mainly to ambiguous feelings about moral freedom within a religious personal belief system, later existentialists discussed conflicts of personal principles, cultural norms, and existential despair.

Existential angst makes its appearance in classical musical composition in the early twentieth century as a result of both philosophical developments and as a reflection of the war-torn times. Notable composers whose works are often linked with the concept include Gustav Mahler, Richard Strauss (operas ' and '), Claude Debussy (opera "", ballet "Jeux"), Jean Sibelius (especially the Fourth Symphony), Arnold Schoenberg ("A Survivor from Warsaw"), Alban Berg, Francis Poulenc (opera "Dialogues of the Carmelites"), Dmitri Shostakovich (opera "Lady Macbeth of Mtsensk", symphonies and chamber music), Béla Bartók (opera "Bluebeard's Castle"), and Krzysztof Penderecki (especially "Threnody to the Victims of Hiroshima").
Angst began to be discussed in reference to popular music in the mid- to late 1950s, amid widespread concerns over international tensions and nuclear proliferation. Jeff Nuttall's book "Bomb Culture" (1968) traced angst in popular culture to Hiroshima. Dread was expressed in works of folk rock such as Bob Dylan's "Masters of War" (1963) and "A Hard Rain's a-Gonna Fall". The term often makes an appearance in reference to punk rock, grunge, nu metal, and works of emo where expressions of melancholy, existential despair, or nihilism predominate.

Anxiety

Anxiety is an emotion which is characterised by an unpleasant state of inner turmoil and includes feelings of dread over anticipated events. Anxiety is different from fear in that fear is defined as the emotional response to a real threat, whereas anxiety is the anticipation of a future threat. It is often accompanied by nervous behavior such as pacing back and forth, somatic complaints, and rumination.

Anxiety is a feeling of uneasiness and worry, usually generalized and unfocused as an overreaction to a situation that is only subjectively seen as menacing. It is often accompanied by muscular tension, restlessness, fatigue, inability to catch one's breath, tightness in the abdominal region, nausea, and problems in concentration. Anxiety is closely related to fear, which is a response to a real or perceived immediate threat (fight or flight response); anxiety involves the expectation of a future threat including dread. People facing anxiety may withdraw from situations which have provoked anxiety in the past.

The emotion of anxiety can persist beyond the developmentally appropriate time-periods in response to specific events, and thus turning into one of the multiple anxiety disorders (e.g. generalized anxiety disorder, panic disorder). The difference between "anxiety disorder" (as mental disorder) and "anxiety" (as normal emotion), is that people with an anxiety disorder experience anxiety most of the days during approximately 6 months, or even during shorter time-periods in children. Anxiety disorders are among the most persistent mental problems and often last decades. Besides, strong perceptions of anxiety exist within other mental disorders, e.g., obsessive-compulsive disorder, post-traumatic stress disorder.

Anxiety is distinguished from fear, which is an appropriate cognitive and emotional response to a perceived threat. Anxiety is related to the specific behaviors of fight-or-flight responses, defensive behavior or escape. There is a false presumption that often circulates that anxiety only occurs in situations perceived as uncontrollable or unavoidable, but this is not always so. David Barlow defines anxiety as "a future-oriented mood state in which one is not ready or prepared to attempt to cope with upcoming negative events," and that it is a distinction between future and present dangers which divides anxiety and fear. Another description of anxiety is agony, dread, terror, or even apprehension. In positive psychology, anxiety is described as the mental state that results from a difficult challenge for which the subject has insufficient coping skills.

Fear and anxiety can be differentiated into four domains: (1) duration of emotional experience, (2) temporal focus, (3) specificity of the threat, and (4) motivated direction. Fear is short-lived, present-focused, geared towards a specific threat, and facilitating escape from threat. On the other hand, anxiety is long-acting, future-focused, broadly focused towards a diffuse threat, and promoting excessive caution while approaching a potential threat and interferes with constructive coping.

Joseph E. LeDoux and Lisa Feldman Barrett have both sought to separate automatic threat responses from additional associated cognitive activity within anxiety.

Anxiety can be experienced with long, drawn-out daily symptoms that reduce quality of life, known as chronic (or generalized) anxiety, or it can be experienced in short spurts with sporadic, stressful panic attacks, known as acute anxiety. Symptoms of anxiety can range in number, intensity, and frequency, depending on the person. However, most people do not suffer from chronic anxiety.

Anxiety can induce several psychological pains (e.g. depression) or mental disorders, and may lead to self-harm or suicide for which dedicated hotlines exist.

The behavioral effects of anxiety may include withdrawal from situations which have provoked anxiety or negative feelings in the past. Other effects may include changes in sleeping patterns, changes in habits, increase or decrease in food intake, and increased motor tension (such as foot tapping).

The emotional effects of anxiety may include "feelings of apprehension or dread, trouble concentrating, feeling tense or jumpy, anticipating the worst, irritability, restlessness, watching (and waiting) for signs (and occurrences) of danger, and, feeling like your mind's gone blank" as well as "nightmares/bad dreams, obsessions about sensations, déjà vu, a trapped-in-your-mind feeling, and feeling like everything is scary." It may include a vague experience and feeling of helplessness.

The cognitive effects of anxiety may include thoughts about suspected dangers, such as fear of dying: "You may ... fear that the chest pains are a deadly heart attack or that the shooting pains in your head are the result of a tumor or an aneurysm. You feel an intense fear when you think of dying, or you may think of it more often than normal, or can't get it out of your mind."

The physiological symptoms of anxiety may include:

There are various types of anxiety. Existential anxiety can occur when a person faces angst, an existential crisis, or nihilistic feelings. People can also face mathematical anxiety, somatic anxiety, stage fright, or test anxiety. Social anxiety refers to a fear of rejection and negative evaluation (being judged) by other people.

The philosopher Søren Kierkegaard, in "The Concept of Anxiety" (1844), described anxiety or dread associated with the "dizziness of freedom" and suggested the possibility for positive resolution of anxiety through the self-conscious exercise of responsibility and choosing. In "Art and Artist" (1932), the psychologist Otto Rank wrote that the psychological trauma of birth was the pre-eminent human symbol of existential anxiety and encompasses the creative person's simultaneous fear of – and desire for – separation, individuation, and differentiation.

The theologian Paul Tillich characterized existential anxiety as "the state in which a being is aware of its possible nonbeing" and he listed three categories for the nonbeing and resulting anxiety: ontic (fate and death), moral (guilt and condemnation), and spiritual (emptiness and meaninglessness). According to Tillich, the last of these three types of existential anxiety, i.e. spiritual anxiety, is predominant in modern times while the others were predominant in earlier periods. Tillich argues that this anxiety can be accepted as part of the human condition or it can be resisted but with negative consequences. In its pathological form, spiritual anxiety may tend to "drive the person toward the creation of certitude in systems of meaning which are supported by tradition and authority" even though such "undoubted certitude is not built on the rock of reality".

According to Viktor Frankl, the author of "Man's Search for Meaning", when a person is faced with extreme mortal dangers, the most basic of all human wishes is to find a meaning of life to combat the "trauma of nonbeing" as death is near.

Depending on the source of the threat, psychoanalytic theory distinguishes the following types of anxiety, namely realistic, neurotic and moral.

According to Yerkes-Dodson law, an optimal level of arousal is necessary to best complete a task such as an exam, performance, or competitive event. However, when the anxiety or level of arousal exceeds that optimum, the result is a decline in performance.

Test anxiety is the uneasiness, apprehension, or nervousness felt by students who have a fear of failing an exam. Students who have test anxiety may experience any of the following: the association of grades with personal worth; fear of embarrassment by a teacher; fear of alienation from parents or friends; time pressures; or feeling a loss of control. Sweating, dizziness, headaches, racing heartbeats, nausea, fidgeting, uncontrollable crying or laughing and drumming on a desk are all common. Because test anxiety hinges on fear of negative evaluation, debate exists as to whether test anxiety is itself a unique anxiety disorder or whether it is a specific type of social phobia. The DSM-IV classifies test anxiety as a type of social phobia.

Research indicates that test anxiety among U.S. high-school and college students has been rising since the late 1950s. Test anxiety remains a challenge for students, regardless of age, and has considerable physiological and psychological impacts. Management of test anxiety focuses on achieving relaxation and developing mechanisms to manage anxiety. The routine practice of slow, Device-Guided Breathing (DGB) is a
major component of behavioral treatments for anxiety conditions.

Performance anxiety and competitive anxiety () happen when an individual's performance is measured against others. An important distinction between competitive and non-competitive anxiety is that competitive anxiety makes people view their performance as a threat. As a result, they experience a drop in their ordinary ability, whether physical or mental, due to that perceived stress.

Competitive anxiety is caused by a range of internal factors including high expectations, outside pressure, lack of experience, and external factors like the location of a competition. It commonly occurs in those participating in high pressure activities like sports and debates. Some common symptoms of competitive anxiety include muscle tension, fatigue, weakness, sense of panic, apprehensiveness, and panic attacks.

There are 4 major theories of how anxiety affects performance: Drive theory, Inverted U theory, Reversal theory, and The Zone of Optimal Functioning theory.

"Drive theory" believes that anxiety is positive and performance improves proportionally to the level of anxiety. This theory is not well accepted.

The "Inverted U theory" is based on the idea that performance peaks at a moderate stress level. It is called Inverted U theory because the graph that plots performance against anxiety looks like an inverted "U".

"Reversal theory" suggests that performance increases in relation to the individual's interpretation of their arousal levels. If they believed their physical arousal level would help them, their performance would increase, if they didn't, their performance would decrease. For example: Athletes were shown to worry more when focusing on results and perfection rather than the effort and growth involved.

The "Zone of Optimal Functioning theory" proposes that there is a zone where positive and negative emotions are in a balance which lead to feelings of dissociation and intense concentration, optimizing the individual's performance levels.

Humans generally require social acceptance and thus sometimes dread the disapproval of others. Apprehension of being judged by others may cause anxiety in social environments.

Anxiety during social interactions, particularly between strangers, is common among young people. It may persist into adulthood and become social anxiety or social phobia. "Stranger anxiety" in small children is not considered a phobia. In adults, an excessive fear of other people is not a developmentally common stage; it is called social anxiety. According to Cutting, social phobics do not fear the crowd but the fact that they may be judged negatively.

Social anxiety varies in degree and severity. For some people, it is characterized by experiencing discomfort or awkwardness during physical social contact (e.g. embracing, shaking hands, etc.), while in other cases it can lead to a fear of interacting with unfamiliar people altogether. Those with this condition may restrict their lifestyles to accommodate the anxiety, minimizing social interaction whenever possible. Social anxiety also forms a core aspect of certain personality disorders, including avoidant personality disorder.

To the extent that a person is fearful of social encounters with unfamiliar others, some people may experience anxiety particularly during interactions with outgroup members, or people who share different group memberships (i.e., by race, ethnicity, class, gender, etc.). Depending on the nature of the antecedent relations, cognitions, and situational factors, intergroup contact may be stressful and lead to feelings of anxiety. This apprehension or fear of contact with outgroup members is often called interracial or intergroup anxiety.

As is the case with the more generalized forms of social anxiety, intergroup anxiety has behavioral, cognitive, and affective effects. For instance, increases in schematic processing and simplified information processing can occur when anxiety is high. Indeed, such is consistent with related work on attentional bias in implicit memory. Additionally recent research has found that implicit racial evaluations (i.e. automatic prejudiced attitudes) can be amplified during intergroup interaction. Negative experiences have been illustrated in producing not only negative expectations, but also avoidant, or antagonistic, behavior such as hostility. Furthermore, when compared to anxiety levels and cognitive effort (e.g., impression management and self-presentation) in intragroup contexts, levels and depletion of resources may be exacerbated in the intergroup situation.

Anxiety can be either a short-term "state" or a long-term "personality trait." Trait anxiety reflects a stable tendency across the lifespan of responding with acute, state anxiety in the anticipation of threatening situations (whether they are actually deemed threatening or not). A meta-analysis showed that a high level of neuroticism is a risk factor for development of anxiety symptoms and disorders. Such anxiety may be conscious or unconscious.

Personality can also be a trait leading to anxiety and depression and their persistence. Through experience, many find it difficult to collect themselves due to their own personal nature.

Anxiety induced by the need to choose between similar options is increasingly being recognized as a problem for individuals and for organizations. In 2004, Capgemini wrote: "Today we're all faced with greater choice, more competition and less time to consider our options or seek out the right advice."

In a decision context, unpredictability or uncertainty may trigger emotional responses in anxious individuals that systematically alter decision-making. There are primarily two forms of this anxiety type. The first form refers to a choice in which there are multiple potential outcomes with known or calculable probabilities. The second form refers to the uncertainty and ambiguity related to a decision context in which there are multiple possible outcomes with unknown probabilities.

Panic disorder may share symptoms of stress and anxiety, but it is actually very different. Panic disorder is an anxiety disorder that occurs without any triggers. According to the U.S. Department of Health and Human Services, this disorder can be distinguished by unexpected and repeated episodes of intense fear. Someone with panic disorder will eventually develop constant fear of another attack and as this progresses it will begin to affect daily functioning and an individual's general quality of life. It is reported by the Cleveland Clinic that panic disorder affects 2 to 3 percent of adult Americans and can begin around the time of the teenage and early adult years. Some symptoms include: difficulty breathing, chest pain, dizziness, trembling or shaking, feeling faint, nausea, fear that you are losing control or are about to die. Even though they have these symptoms during an attack, the main symptom is the persistent fear of having future panic attacks.

Anxiety disorders are a group of mental disorders characterized by exaggerated feelings of anxiety and fear responses. Anxiety is a worry about future events and fear is a reaction to current events. These feelings may cause physical symptoms, such as a fast heart rate and shakiness. There are a number of anxiety disorders: including generalized anxiety disorder, specific phobia, social anxiety disorder, separation anxiety disorder, agoraphobia, panic disorder, and selective mutism. The disorder differs by what results in the symptoms. People often have more than one anxiety disorder.
Anxiety disorders are caused by a complex combination of genetic and environmental factors. To be diagnosed, symptoms typically need to be present for at least six months, be more than would be expected for the situation, and decrease a person's ability to function in their daily lives. Other problems that may result in similar symptoms include hyperthyroidism, heart disease, caffeine, alcohol, or cannabis use, and withdrawal from certain drugs, among others.
Without treatment, anxiety disorders tend to remain. Treatment may include lifestyle changes, counselling, and medications. Counselling is typically with a type of cognitive behavioral therapy. Medications, such as antidepressants or beta blockers, may improve symptoms. A 2023 review found that regular physical activity is effective for reducing anxiety.
About 12% of people are affected by an anxiety disorder in a given year and between 12% and 30% are affected at some point in their life. They occur about twice as often in women than they do in men, and generally begin before the age of 25. The most common anxiety disorders are specific phobias, which affect nearly 12% of people, and social anxiety disorder, which affects 10% of people at some point in their life. They affect those between the ages of 15 and 35 the most and become less common after the age of 55. Rates appear to be higher in the United States and Europe.

Anxiety can be either a short-term "state" or a long-term "trait." Whereas trait anxiety represents worrying about future events, anxiety disorders are a group of mental disorders characterized by feelings of anxiety and fears.

In his book "Anxious: The Modern Mind in the Age of Anxiety" Joseph LeDoux examines four experiences of anxiety through a brain-based lens:


Anxiety disorders often occur with other mental health disorders, particularly major depressive disorder, bipolar disorder, eating disorders, or certain personality disorders. It also commonly occurs with personality traits such as neuroticism. This observed co-occurrence is partly due to genetic and environmental influences shared between these traits and anxiety.

It is common for those with obsessive–compulsive disorder to experience anxiety. Anxiety is also commonly found in those who experience panic disorders, phobic anxiety disorders, severe stress, dissociative disorders, somatoform disorders, and some neurotic disorders.
Anxiety has also been linked to the experience of intrusive thoughts. Studies have revealed that individuals who experience high levels of anxiety (also known as clinical anxiety) are highly vulnerable to the experience of intense intrusive thoughts or psychological disorders that are characterised by intrusive thoughts.

Anxiety disorders are partly genetic, with twin studies suggesting 30-40% genetic influence on individual differences in anxiety. Environmental factors are also important. Twin studies show that individual-specific environments have a large influence on anxiety, whereas shared environmental influences (environments that affect twins in the same way) operate during childhood but decline through adolescence. Specific measured 'environments' that have been associated with anxiety include child abuse, family history of mental health disorders, and poverty. Anxiety is also associated with drug use, including alcohol, caffeine, and benzodiazepines, which are often prescribed to treat anxiety.
Neural circuitry involving the amygdala, which regulates emotions like anxiety and fear, stimulating the HPA axis and sympathetic nervous system, and hippocampus, which is implicated in emotional memory along with the amygdala, is thought to underlie anxiety. People who have anxiety tend to show high activity in response to emotional stimuli in the amygdala. Some writers believe that excessive anxiety can lead to an overpotentiation of the limbic system (which includes the amygdala and nucleus accumbens), giving increased future anxiety, but this does not appear to have been proven.

Research upon adolescents who as infants had been highly apprehensive, vigilant, and fearful finds that their nucleus accumbens is more sensitive than that in other people when deciding to make an action that determined whether they received a reward. This suggests a link between circuits responsible for fear and also reward in anxious people. As researchers note, "a sense of 'responsibility', or self-agency, in a context of uncertainty (probabilistic outcomes) drives the neural system underlying appetitive motivation (i.e., nucleus accumbens) more strongly in temperamentally inhibited than noninhibited adolescents".

The microbes of the gut can connect with the brain to affect anxiety. There are various pathways along which this communication can take place. One is through the major neurotransmitters. The gut microbes such as "Bifidobacterium" and "Bacillus" produce the neurotransmitters GABA and dopamine, respectively. The neurotransmitters signal to the nervous system of the gastrointestinal tract, and those signals will be carried to the brain through the vagus nerve or the spinal system. This is demonstrated by the fact that altering the microbiome has shown anxiety- and depression-reducing effects in mice, but not in subjects without vagus nerves.

Another key pathway is the HPA axis, as mentioned above. The microbes can control the levels of cytokines in the body, and altering cytokine levels creates direct effects on areas of the brain such as the hypothalamus, the area that triggers HPA axis activity. The HPA axis regulates production of cortisol, a hormone that takes part in the body's stress response. When HPA activity spikes, cortisol levels increase, processing and reducing anxiety in stressful situations. These pathways, as well as the specific effects of individual taxa of microbes, are not yet completely clear, but the communication between the gut microbiome and the brain is undeniable, as is the ability of these pathways to alter anxiety levels.

With this communication comes the potential to treat. Prebiotics and probiotics have been shown to reduce anxiety. For example, experiments in which mice were given fructo- and galacto-oligosaccharide prebiotics and "Lactobacillus" probiotics have both demonstrated a capability to reduce anxiety. In humans, results are not as concrete, but promising.

Genetics and family history (e.g. parental anxiety) may put an individual at increased risk of an anxiety disorder, but generally external stimuli will trigger its onset or exacerbation. Estimates of genetic influence on anxiety, based on studies of twins, range from 25 to 40% depending on the specific type and age-group under study. For example, genetic differences account for about 43% of variance in panic disorder and 28% in generalized anxiety disorder. Longitudinal twin studies have shown the moderate stability of anxiety from childhood through to adulthood is mainly influenced by stability in genetic influence. When investigating how anxiety is passed on from parents to children, it is important to account for sharing of genes as well as environments, for example using the intergenerational children-of-twins design.

Many studies in the past used a candidate gene approach to test whether single genes were associated with anxiety. These investigations were based on hypotheses about how certain known genes influence neurotransmitters (such as serotonin and norepinephrine) and hormones (such as cortisol) that are implicated in anxiety. None of these findings are well replicated, with the possible exception of TMEM132D, COMT and MAO-A. The epigenetic signature of "BDNF", a gene that codes for a protein called "brain derived neurotrophic factor" that is found in the brain, has also been associated with anxiety and specific patterns of neural activity. and a receptor gene for "BDNF" called "NTRK2" was associated with anxiety in a large genome-wide investigation. The reason that most candidate gene findings have not replicated is that anxiety is a complex trait that is influenced by many genomic variants, each of which has a small effect on its own. Increasingly, studies of anxiety are using a hypothesis-free approach to look for parts of the genome that are implicated in anxiety using big enough samples to find associations with variants that have small effects. The largest explorations of the common genetic architecture of anxiety have been facilitated by the UK Biobank, the ANGST consortium and the CRC Fear, Anxiety and Anxiety Disorders.

Many medical conditions can cause anxiety. This includes conditions that affect the ability to breathe, like COPD and asthma, and the difficulty in breathing that often occurs near death. Conditions that cause abdominal pain or chest pain can cause anxiety and may in some cases be a somatization of anxiety; the same is true for some sexual dysfunctions. Conditions that affect the face or the skin can cause social anxiety especially among adolescents, and developmental disabilities often lead to social anxiety for children as well. Life-threatening conditions like cancer also cause anxiety.

Furthermore, certain organic diseases may present with anxiety or symptoms that mimic anxiety. These disorders include certain endocrine diseases (hypo- and hyperthyroidism, hyperprolactinemia), metabolic disorders (diabetes), deficiency states (low levels of vitamin D, B2, B12, folic acid), gastrointestinal diseases (celiac disease, non-celiac gluten sensitivity, inflammatory bowel disease), heart diseases, blood diseases (anemia), cerebral vascular accidents (transient ischemic attack, stroke), and brain degenerative diseases (Parkinson's disease, dementia, multiple sclerosis, Huntington's disease), among others.

Several drugs can cause or worsen anxiety, whether in intoxication, withdrawal or as side effect. These include alcohol, tobacco, sedatives (including prescription benzodiazepines), opioids (including prescription pain killers and illicit drugs like heroin), stimulants (such as caffeine, cocaine and amphetamines), hallucinogens, and inhalants.

While many often report self-medicating anxiety with these substances, improvements in anxiety from drugs are usually short-lived (with worsening of anxiety in the long term, sometimes with acute anxiety as soon as the drug effects wear off) and tend to be exaggerated. Acute exposure to toxic levels of benzene may cause euphoria, anxiety, and irritability lasting up to 2 weeks after the exposure.

Poor coping skills (e.g., rigidity/inflexible problem solving, denial, avoidance, impulsivity, extreme self-expectation, negative thoughts, affective instability, and inability to focus on problems) are associated with anxiety. Anxiety is also linked and perpetuated by the person's own pessimistic outcome expectancy and how they cope with feedback negativity. Temperament (e.g., neuroticism) and attitudes (e.g. pessimism) have been found to be risk factors for anxiety.

Cognitive distortions such as overgeneralizing, catastrophizing, mind reading, emotional reasoning, binocular trick, and mental filter can result in anxiety. For example, an overgeneralized belief that something bad "always" happens may lead someone to have excessive fears of even minimally risky situations and to avoid benign social situations due to anticipatory anxiety of embarrassment. In addition, those who have high anxiety can also create future stressful life events. Together, these findings suggest that anxious thoughts can lead to anticipatory anxiety as well as stressful events, which in turn cause more anxiety. Such unhealthy thoughts can be targets for successful treatment with cognitive therapy.

Psychodynamic theory posits that anxiety is often the result of opposing unconscious wishes or fears that manifest via maladaptive defense mechanisms (such as suppression, repression, anticipation, regression, somatization, passive aggression, dissociation) that develop to adapt to problems with early objects (e.g., caregivers) and empathic failures in childhood. For example, persistent parental discouragement of anger may result in repression/suppression of angry feelings which manifests as gastrointestinal distress (somatization) when provoked by another while the anger remains unconscious and outside the individual's awareness. Such conflicts can be targets for successful treatment with psychodynamic therapy. While psychodynamic therapy tends to explore the underlying roots of anxiety, cognitive behavioral therapy has also been shown to be a successful treatment for anxiety by altering irrational thoughts and unwanted behaviors.

An evolutionary psychology explanation is that increased anxiety serves the purpose of increased vigilance regarding potential threats in the environment as well as increased tendency to take proactive actions regarding such possible threats. This may cause false positive reactions but an individual with anxiety may also avoid real threats. This may explain why anxious people are less likely to die due to accidents. There is ample empirical evidence that anxiety can have adaptive value. Within a school, timid fish are more likely than bold fish to survive a predator.

When people are confronted with unpleasant and potentially harmful stimuli such as foul odors or tastes, PET-scans show increased blood flow in the amygdala. In these studies, the participants also reported moderate anxiety. This might indicate that anxiety is a protective mechanism designed to prevent the organism from engaging in potentially harmful behaviors.

Social risk factors for anxiety include a history of trauma (e.g., physical, sexual or emotional abuse or assault), bullying, early life experiences and parenting factors (e.g., rejection, lack of warmth, high hostility, harsh discipline, high parental negative affect, anxious childrearing, modelling of dysfunctional and drug-abusing behaviour, discouragement of emotions, poor socialization, poor attachment, and child abuse and neglect), cultural factors (e.g., stoic families/cultures, persecuted minorities including those with disabilities), and socioeconomics (e.g., uneducated, unemployed, impoverished although developed countries have higher rates of anxiety disorders than developing countries). 
A 2019 comprehensive systematic review of over 50 studies showed that food insecurity in the United States is strongly associated with depression, anxiety, and sleep disorders. Food-insecure individuals had an almost 3 fold risk increase of testing positive for anxiety when compared to food-secure individuals.

Contextual factors that are thought to contribute to anxiety include gender socialization and learning experiences. In particular, learning mastery (the degree to which people perceive their lives to be under their own control) and instrumentality, which includes such traits as self-confidence, self-efficacy, independence, and competitiveness fully mediate the relation between gender and anxiety. That is, though gender differences in anxiety exist, with higher levels of anxiety in women compared to men, gender socialization and learning mastery explain these gender differences.

The first step in the management of a person with anxiety symptoms involves evaluating the possible presence of an underlying medical cause, the recognition of which is essential in order to decide the correct treatment. Anxiety symptoms may mask an organic disease, or appear associated with or as a result of a medical disorder.

Cognitive behavioral therapy (CBT) is effective for anxiety disorders and is a first line treatment. CBT appears to be equally effective when carried out via the internet. While evidence for mental health apps is promising, it is preliminary.

Anxiety often affects relationships, and interpersonal psychotherapy addresses these issues by improving communication and relationship skills.

Psychopharmacological treatment can be used in parallel to CBT or can be used alone. As a general rule, most anxiety disorders respond well to first-line agents. Such drugs, also used as anti-depressants, are the selective serotonin reuptake inhibitors and serotonin-norepinephrine reuptake inhibitors, that work by blocking the reuptake of specific neurotransmitters and resulting in the increase in availability of these neurotransmitters. Additionally, benzodiazepines are often prescribed to individuals with anxiety disorder. Benzodiazepines produce an anxiolytic response by modulating GABA and increasing its receptor binding. A third common treatment involves a category of drug known as serotonin agonists. This category of drug works by initiating a physiological response at 5-HT1A receptor by increasing the action of serotonin at this receptor. Other treatment options include pregabalin, tricyclic antidepressants, and moclobemide, among others.

Anxiety is considered to be a serious psychiatric illness that has an unknown true pervasiveness due to affected individuals not asking for proper treatment or aid, and due to professionals missing the diagnosis.

The above risk factors give natural avenues for prevention. A 2017 review found that psychological or educational interventions have a small yet statistically significant benefit for the prevention of anxiety in varied population types.

Anxiety disorder appears to be a genetically inherited neurochemical dysfunction that may involve autonomic imbalance; decreased GABA-ergic tone; allelic polymorphism of the catechol-O-methyltransferase (COMT) gene; increased adenosine receptor function; increased cortisol.

In the central nervous system (CNS), the major mediators of the symptoms of anxiety disorders appear to be norepinephrine, serotonin, dopamine, and gamma-aminobutyric acid (GABA). Other neurotransmitters and peptides, such as corticotropin-releasing factor, may be involved. Peripherally, the autonomic nervous system, especially the sympathetic nervous system, mediates many of the symptoms. Increased flow in the right parahippocampal region and reduced serotonin type 1A receptor binding in the anterior and posterior cingulate and raphe of patients are the diagnostic factors for prevalence of anxiety disorder.

The amygdala is central to the processing of fear and anxiety, and its function may be disrupted in anxiety disorders. Anxiety processing in the basolateral amygdala has been implicated with expansion of dendritic arborization of the amygdaloid neurons. SK2 potassium channels mediate inhibitory influence on action potentials and reduce arborization.


A. A. Milne

Alan Alexander Milne (; 18 January 1882 – 31 January 1956) was an English writer best known for his books about the teddy bear Winnie-the-Pooh, as well as for children's poetry. Milne was primarily a playwright before the huge success of Winnie-the-Pooh overshadowed all his previous work. Milne served in both world wars, as a lieutenant in the Royal Warwickshire Regiment in the First World War and as a captain in the Home Guard in the Second World War.

Milne was the father of bookseller Christopher Robin Milne, upon whom the character Christopher Robin is based. It was during a visit to London Zoo, where Christopher became enamoured with the tame and amiable bear Winnipeg, that Milne was inspired to write the story of Winnie-the-Pooh for his son. Milne bequeathed the original manuscripts of the Winnie-the-Pooh stories to the Wren Library at Trinity College, Cambridge, his alma mater.

Alan Alexander Milne was born in Kilburn, London, to John Vine Milne, who was born in Jamaica, and Sarah Marie Milne (née Heginbotham), on 18 January 1882. He grew up at Henley House School, 6/7 Mortimer Road (now Crescent), Kilburn, a small independent school run by his father. One of his teachers was H. G. Wells, who taught there in 1889–90. Milne attended Westminster School and Trinity College, Cambridge, where he studied on a mathematics scholarship, graduating with a B.A. in Mathematics in 1903. He edited and wrote for "Granta", a student magazine. He collaborated with his brother Kenneth and their articles appeared over the initials AKM. Milne's work came to the attention of the leading British humour magazine "Punch", where Milne was to become a contributor and later an assistant editor. Considered a talented cricket fielder, Milne played for two amateur teams that were largely composed of British writers: the Allahakbarries and the Authors XI. His teammates included fellow writers J. M. Barrie, Arthur Conan Doyle and P. G. Wodehouse.

Milne joined the British Army during World War I and served as an officer in the Royal Warwickshire Regiment. He was commissioned into the 4th Battalion, Royal Warwickshire Regiment, on 1 February 1915 as a second lieutenant (on probation). His commission was confirmed on 20 December 1915. He served on the Somme as a signals officer from July-November 1916, but caught trench fever and was invalided back to England. Having recuperated, he worked as a signals instructor, before being recruited into military intelligence to write propaganda articles for MI7 (b) between 1917 and 1918. He was discharged on 14 February 1919, and settled in Mallord Street, Chelsea. He relinquished his commission on 19 February 1920, retaining the rank of lieutenant.
After the war, he wrote a denunciation of war titled "Peace with Honour" (1934), which he retracted somewhat with 1940's "War with Honour". During World War II, Milne was one of the most prominent critics of fellow English writer (and Authors XI cricket teammate) P. G. Wodehouse, who was captured at his country home in France by the Nazis and imprisoned for a year. Wodehouse made radio broadcasts about his internment, which were broadcast from Berlin. Although the light-hearted broadcasts made fun of the Germans, Milne accused Wodehouse of committing an act of near treason by cooperating with his country's enemy. Wodehouse got some revenge on his former friend (e.g. in "The Mating Season") by creating fatuous parodies of the Christopher Robin poems in some of his later stories, and claiming that Milne "was probably jealous of all other writers... But I loved his stuff."

Milne married Dorothy "Daphne" de Sélincourt (1890–1971) in 1913 and their son Christopher Robin Milne was born in 1920. In 1925, Milne bought a country home, Cotchford Farm, in Hartfield, East Sussex.

During World War II, Milne was a captain in the British Home Guard in Hartfield & Forest Row, insisting on being plain "Mr. Milne" to the members of his platoon. He retired to the farm after a stroke and brain surgery in 1952 left him an invalid; and by August 1953, "he seemed very old and disenchanted." Milne died in January 1956, aged 74.

After graduating from Cambridge University in 1903, A. A. Milne contributed humorous verse and whimsical essays to "Punch", joining the staff in 1906 and becoming an assistant editor.

During this period he published 18 plays and three novels, including the murder mystery "The Red House Mystery" (1922). His son was born in August 1920 and in 1924 Milne produced a collection of children's poems, "When We Were Very Young", which were illustrated by "Punch" staff cartoonist E. H. Shepard. A collection of short stories for children "A Gallery of Children", and other stories that became part of the Winnie-the-Pooh books, were first published in 1925.

Milne was an early screenwriter for the nascent British film industry, writing four stories filmed in 1920 for the company Minerva Films (founded in 1920 by the actor Leslie Howard and his friend and story editor Adrian Brunel). These were "The Bump", starring Aubrey Smith; "Twice Two"; "Five Pound Reward"; and "Bookworms". Some of these films survive in the archives of the British Film Institute. Milne had met Howard when the actor starred in Milne's play "Mr Pim Passes By" in London.

Looking back on this period (in 1926), Milne observed that when he told his agent that he was going to write a detective story, he was told that what the country wanted from a ""Punch" humorist" was a humorous story; when two years later he said he was writing nursery rhymes, his agent and publisher were convinced he should write another detective story; and after another two years, he was being told that writing a detective story would be in the worst of taste given the demand for children's books. He concluded that "the only excuse which I have yet discovered for writing anything is that I want to write it; and I should be as proud to be delivered of a Telephone Directory "con amore" as I should be ashamed to create a Blank Verse Tragedy at the bidding of others."

Milne is most famous for his two "Pooh" books about a boy named Christopher Robin after his son, Christopher Robin Milne (1920–1996), and various characters inspired by his son's stuffed animals, most notably the bear named Winnie-the-Pooh. Christopher Robin Milne's stuffed bear, originally named Edward, was renamed Winnie after a Canadian black bear named Winnie (after Winnipeg), which was used as a military mascot in World War I, and left to London Zoo during the war. "The Pooh" comes from a swan the young Milne named "Pooh". E. H. Shepard illustrated the original Pooh books, using his own son's teddy Growler ("a magnificent bear") as the model. The rest of Christopher Robin Milne's toys, Piglet, Eeyore, Kanga, Roo and Tigger, were incorporated into A. A. Milne's stories, and two more characters – Rabbit and Owl – were created by Milne's imagination. Christopher Robin Milne's own toys are now on display in New York where 750,000 people visit them every year.
The fictional Hundred Acre Wood of the Pooh stories derives from Five Hundred Acre Wood in Ashdown Forest in East Sussex, South East England, where the Pooh stories were set. Milne lived on the northern edge of the forest at Cotchford Farm, , and took his son walking there. E. H. Shepard drew on the landscapes of Ashdown Forest as inspiration for many of the illustrations he provided for the Pooh books. The adult Christopher Robin commented: "Pooh's Forest and Ashdown Forest are identical." Popular tourist locations at Ashdown Forest include: "Galleon's Lap", "The Enchanted Place", the "Heffalump Trap" and "Lone Pine", "Eeyore's Sad and Gloomy Place", and the wooden "Pooh Bridge" where Pooh and Piglet invented Poohsticks.

Not yet known as Pooh, he made his first appearance in a poem, "Teddy Bear", published in "Punch" magazine in February 1924 and republished that year in "When We Were Very Young". Pooh first appeared in the "London Evening News" on Christmas Eve, 1925, in a story called "The Wrong Sort of Bees". "Winnie-the-Pooh" was published in 1926, followed by "The House at Pooh Corner" in 1928. A second collection of nursery rhymes, "Now We Are Six", was published in 1927. All four books were illustrated by E. H. Shepard. Milne also published four plays in this period. He also "gallantly stepped forward" to contribute a quarter of the costs of dramatising P. G. Wodehouse's "A Damsel in Distress". "The World of Pooh" won the Lewis Carroll Shelf Award in 1958.

The success of his children's books was to become a source of considerable annoyance to Milne, whose self-avowed aim was to write whatever he pleased and who had, until then, found a ready audience for each change of direction: he had freed pre-war "Punch" from its ponderous facetiousness; he had made a considerable reputation as a playwright (like his idol J. M. Barrie) on both sides of the Atlantic; he had produced a witty piece of detective writing in "The Red House Mystery" (although this was severely criticised by Raymond Chandler for the implausibility of its plot in his essay "The Simple Art of Murder" in the eponymous collection that appeared in 1950). But once Milne had, in his own words, "said goodbye to all that in 70,000 words" (the approximate length of his four principal children's books), he had no intention of producing any reworkings lacking in originality, given that one of the sources of inspiration, his son, was growing older.

Another reason Milne stopped writing children's books, and especially about Winnie-the-Pooh, was that he felt "amazement and disgust" over the immense fame his son was exposed to, and said that "I feel that the legal Christopher Robin has already had more publicity than I want for him. I do not want CR Milne to ever wish that his name were Charles Robert."

In his literary home, "Punch", where the "When We Were Very Young" verses had first appeared, Methuen continued to publish whatever Milne wrote, including the long poem "The Norman Church" and an assembly of articles entitled "Year In, Year Out" (which Milne likened to a benefit night for the author).

In 1929, Milne adapted Kenneth Grahame's novel "The Wind in the Willows" for the stage as "Toad of Toad Hall". The title was an implicit admission that such chapters as Chapter 7, "The Piper at the Gates of Dawn," could not survive translation to the theatre. A special introduction written by Milne is included in some editions of Grahame's novel. It was first performed at the Playhouse Theatre, Liverpool, on 21 December 1929 before it made its West End debut the following year at the Lyric Theatre on 17 December 1930. The play was revived in the West End from 1931 to 1935, and since the 1960s there have been West End revivals during the Christmas season; actors who have performed in the play include Judi Dench and Ian McKellen.

Milne and his wife became estranged from their son, who came to resent what he saw as his father's exploitation of his childhood and came to hate the books that had thrust him into the public eye. Christopher's marriage to his first cousin, Lesley de Sélincourt, distanced him still further from his parents – Lesley's father and Christopher's mother had not spoken to each other for 30 years.

A. A. Milne died at his home in Hartfield, Sussex, on 31 January 1956, nearly two weeks after his 74th birthday. A memorial service took place on 10 February at All Hallows-by-the-Tower church in London.

The rights to A. A. Milne's Pooh books were left to four beneficiaries: his family, the Royal Literary Fund, Westminster School and the Garrick Club. After Milne's death in 1956, thirteen days after his 74th birthday, his widow sold her rights to the Pooh characters to Stephen Slesinger, whose widow sold the rights after Slesinger's death to the Walt Disney Company, which has made many Pooh cartoon movies, a Disney Channel television show, as well as Pooh-related merchandise. In 2001, the other beneficiaries sold their interest in the estate to the Disney Corporation for $350m. Previously Disney had been paying twice-yearly royalties to these beneficiaries. The estate of E. H. Shepard also received a sum in the deal. The UK copyright on the text of the original Winnie the Pooh books expires on 1 January 2027; at the beginning of the year after the 70th anniversary of the author's death (PMA-70), and has already expired in those countries with a PMA-50 rule. This applies to all of Milne's works except those first published posthumously. The illustrations in the Pooh books will remain under copyright until the same amount of time has passed, after the illustrator's death; in the UK, this will be on 1 January 2047. In the US, copyright will not expire until 95 years after publication for each of Milne's books first published before 1978, but this includes the illustrations.

In 2008, a collection of original illustrations featuring Winnie-the-Pooh and his animal friends sold for more than £1.2 million at auction in Sotheby's, London. "Forbes" magazine ranked Winnie the Pooh the most valuable fictional character in 2002; Winnie the Pooh merchandising products alone had annual sales of more than $5.9 billion. In 2005, Winnie the Pooh generated $6 billion, a figure surpassed only by Mickey Mouse.

A memorial plaque in Ashdown Forest, unveiled by Christopher Robin in 1979, commemorates the work of A. A. Milne and Shepard in creating the world of Pooh. The inscription states they "captured the magic of Ashdown Forest, and gave it to the world". Milne once wrote of Ashdown Forest: "In that enchanted place on the top of the forest a little boy and his bear will always be playing."

In 2003, "Winnie-the-Pooh" was ranked number 7 on the BBC's The Big Read poll which determined the UK's "best-loved novels". In 2006, Winnie-the-Pooh received a star on the Hollywood Walk of Fame, marking the 80th birthday of Milne's creation.

Marking the 90th anniversary of Milne's creation of the character, and the 90th birthday of Queen Elizabeth II, "Winnie-the-Pooh Meets the Queen" (2016) sees Pooh meet the Queen at Buckingham Palace. The illustrated and audio adventure is narrated by the actor Jim Broadbent. Also in 2016, a new character, a Penguin, was unveiled in "The Best Bear in All the World", which was inspired by a long-lost photograph of Milne and his son Christopher with a toy penguin.

An exhibition entitled "" appeared at the Victoria and Albert Museum in London from 9 December 2017 to 8 April 2018.

The composer Harold Fraser-Simson, a near neighbour, produced six books of Milne songs between 1924 and 1932. The poems have been parodied many times, including with the books "When We Were Rather Older" and "Now We Are Sixty". The 1963 film "The King's Breakfast" was based on Milne's poem of the same name.

Milne has been portrayed in television and film. Domhnall Gleeson plays him in "Goodbye Christopher Robin", a 2017 biographical drama film. In the 2018 fantasy film "Christopher Robin", an extension of the Disney Winnie the Pooh franchise, Tristan Sturrock plays Milne, and filming took place at Ashdown Forest.

An elementary school in Houston, Texas, operated by the Houston Independent School District (HISD), is named after Milne. The school, A. A. Milne Elementary School in Brays Oaks, opened in 1991.

The original manuscripts for "Winnie-the-Pooh" and "The House at Pooh Corner" are archived at Trinity College Library, Cambridge.

The bulk of A. A. Milne's papers are housed at the Harry Ransom Center at the University of Texas at Austin. The collection, established at the centre in 1964, consists of manuscript drafts and fragments for over 150 of Milne's works, as well as correspondence, legal documents, genealogical records, and some personal effects. The library division holds several books formerly belonging to Milne and his wife Dorothy. The center also has small collections of correspondence from Christopher Robin Milne and Milne's frequent illustrator E. H. Shepard.

Milne did not speak out much on the subject of religion, although he used religious terms to explain his decision, while remaining a pacifist, to join the British Home Guard. He wrote: "In fighting Hitler we are truly fighting the Devil, the Anti-Christ ... Hitler was a crusader against God."

His best known comment on the subject was recalled on his death:
He wrote in the poem "Explained":

He also wrote in the poem "Vespers":












Asociación Alumni

Asociación Alumni, usually just Alumni, is an Argentine rugby union club located in Tortuguitas, Greater Buenos Aires. The senior squad currently competes at Top 12, the first division of the Unión de Rugby de Buenos Aires league system.

The club has ties with former football club Alumni because both were established by Buenos Aires English High School students.

The first club with the name "Alumni" played association football, having been found in 1898 by students of Buenos Aires English High School (BAEHS) along with director Alexander Watson Hutton. Originally under the name "English High School A.C.", the team would be later obliged by the Association to change its name, therefore "Alumni" was chosen, following a proposal by Carlos Bowers, a former student of the school.

Alumni was the most successful team during the first years of Argentine football, winning 10 of 14 league championships contested. Alumni is still considered the first great football team in the country. Alumni was reorganised in 1908, "in order to encourage people to practise all kind of sports, specially football". This was the last try to develop itself as a sports club rather than just a football team, such as Lomas, Belgrano and Quilmes had successfully done in the past, but the efforts were not enough. Alumni played its last game in 1911 and was definitely dissolved on April 24, 1913.

In 1951, two guards of the BAEHS, Daniel Ginhson (also a former player of Buenos Aires F.C.) and Guillermo Cubelli, supported by the school's alumni and fathers of the students, they decided to establish a club focused on rugby union exclusively. Former players still alive of Alumni football club and descendants of other players already dead gave their permission to use the name "Alumni".
On December 13, in a meeting presided by Carlos Bowers himself (who had proposed the name "Alumni" to the original football team 50 years before), the club was officially established under the name "Asociación Juvenil Alumni", also adopting the same colors as its predecessor.

The team achieved good results and in 1960 the club presented a team that won the third division of the Buenos Aires league, reaching the second division. Since then, Alumni has played at the highest level of Argentine rugby and its rivalry with Belgrano Athletic Club is one of the fiercest local derbies in Buenos Aires. Alumni would later climb up to first division winning 5 titles: 4 consecutive between 1989 and 1992, and the other in 2001.

In 2002, Alumni won its first Nacional de Clubes title, defeating Jockey Club de Rosario 23–21 in the final.

As of January 2018:


Axiom

An axiom, postulate, or assumption is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Ancient Greek word (), meaning 'that which is thought worthy or fit' or 'that which commends itself as evident'.

The precise definition varies across fields of study. In classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question. In modern logic, an axiom is a premise or starting point for reasoning.

In mathematics, an "axiom" may be a "logical axiom" or a "non-logical axiom". Logical axioms are taken to be true within the system of logic they define and are often shown in symbolic form (e.g., ("A" and "B") implies "A"), while non-logical axioms (e.g., ) are substantive assertions about the elements of the domain of a specific mathematical theory, such as arithmetic.

Non-logical axioms may also be called "postulates" or "assumptions". In most cases, a non-logical axiom is simply a formal logical expression used in deduction to build a mathematical theory, and might or might not be self-evident in nature (e.g., the parallel postulate in Euclidean geometry). To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there are typically many ways to axiomatize a given mathematical domain.

Any axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be "true" is a subject of debate in the philosophy of mathematics.

The word "axiom" comes from the Greek word ("axíōma"), a verbal noun from the verb ("axioein"), meaning "to deem worthy", but also "to require", which in turn comes from ("áxios"), meaning "being in balance", and hence "having (the same) value (as)", "worthy", "proper". Among the ancient Greek philosophers and mathematicians, axioms were taken to be immediately evident propositions, foundational and common to many fields of investigation, and self-evidently true without any further argument or proof.

The root meaning of the word "postulate" is to "demand"; for instance, Euclid demands that one agree that some things can be done (e.g., any two points can be joined by a straight line).

Ancient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that "Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property." Boethius translated 'postulate' as "petitio" and called the axioms "notiones communes" but in later manuscripts this usage was not always strictly kept.

The logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference) was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are thus the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, in the case of mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms "axiom" and "postulate" hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.

The ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.

An "axiom", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that:
When an equal amount is taken from equals, an equal amount results.

At the foundation of the various sciences lay certain additional hypotheses that were accepted without proof. Such a hypothesis was termed a "postulate". While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Aristotle warns that the content of a science cannot be successfully communicated if the learner is in doubt about the truth of the postulates.

The classical approach is well-illustrated by Euclid's "Elements", where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of "common notions" (very basic, self-evident assertions).

A lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.

Structuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without "any" particular application in mind. The distinction between an "axiom" and a "postulate" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate, one can get theories that have meaning in wider contexts (e.g., hyperbolic geometry). As such, one must simply be prepared to use labels such as "line" and "parallel" with greater flexibility. The development of hyperbolic geometry taught mathematicians that it is useful to regard postulates as purely formal statements, and not as facts based on experience.

When mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.

It is not correct to say that the axioms of field theory are "propositions that are regarded as true without proof." Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.

Modern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.

Another lesson learned in modern mathematics is to examine purported proofs carefully for hidden assumptions.

In the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow – by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axioms. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.

It was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.

In a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here, the emergence of Russell's paradox and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.

The formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.

It is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.

Experimental sciences - as opposed to mathematics and logic - also have general founding assertions from which a deductive reasoning can be built so as to express propositions that predict properties - either still general or much more specialized to a specific experimental context. For instance, Newton's laws in classical mechanics, Maxwell's equations in classical electromagnetism, Einstein's equation in general relativity, Mendel's laws of genetics, Darwin's Natural selection law, etc. These founding assertions are usually called "principles" or "postulates" so as to distinguish from mathematical "axioms".

As a matter of facts, the role of axioms in mathematics and postulates in experimental sciences is different. In mathematics one neither "proves" nor "disproves" an axiom. A set of mathematical axioms gives a set of rules that fix a conceptual realm, in which the theorems logically follow. In contrast, in experimental sciences, a set of postulates shall allow deducing results that match or do not match experimental results. If postulates do not allow deducing experimental predictions, they do not set a scientific conceptual framework and have to be completed or made more accurate. If the postulates allow deducing predictions of experimental results, the comparison with experiments allows falsifying (falsified) the theory that the postulates install. A theory is considered valid as long as it has not been falsified.

Now, the transition between the mathematical axioms and scientific postulates is always slightly blurred, especially in physics. This is due to the heavy use of mathematical tools to support the physical theories. For instance, the introduction of Newton's laws rarely establishes as a prerequisite neither Euclidean geometry or differential calculus that they imply. It became more apparent when Albert Einstein first introduced special relativity where the invariant quantity is no more the Euclidean length formula_1 (defined as formula_2) > but the Minkowski spacetime interval formula_3 (defined as formula_4), and then general relativity where flat Minkowskian geometry is replaced with pseudo-Riemannian geometry on curved manifolds.

In quantum physics, two sets of postulates have coexisted for some time, which provide a very nice example of falsification. The 'Copenhagen school' (Niels Bohr, Werner Heisenberg, Max Born) developed an operational approach with a complete mathematical formalism that involves the description of quantum system by vectors ('states') in a separable Hilbert space, and physical quantities as linear operators that act in this Hilbert space. This approach is fully falsifiable and has so far produced the most accurate predictions in physics. But it has the unsatisfactory aspect of not allowing answers to questions one would naturally ask. For this reason, another 'hidden variables' approach was developed for some time by Albert Einstein, Erwin Schrödinger, David Bohm. It was created so as to try to give deterministic explanation to phenomena such as entanglement. This approach assumed that the Copenhagen school description was not complete, and postulated that some yet unknown variable was to be added to the theory so as to allow answering some of the questions it does not answer (the founding elements of which were discussed as the EPR paradox in 1935). Taking this ideas seriously, John Bell derived in 1964 a prediction that would lead to different experimental results (Bell's inequalities) in the Copenhagen and the Hidden variable case. The experiment was conducted first by Alain Aspect in the early 1980's, and the result excluded the simple hidden variable approach (sophisticated hidden variables could still exist but their properties would still be more disturbing than the problems they try to solve). This does not mean that the conceptual framework of quantum physics can be considered as complete now, since some open questions still exist (the limit between the quantum and classical realms, what happens during a quantum measurement, what happens in a completely closed quantum system such as the universe itself, etc.).

In the field of mathematical logic, a clear distinction is made between two notions of axioms: "logical" and "non-logical" (somewhat similar to the ancient distinction between "axioms" and "postulates" respectively).

These are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms "at least" some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.

In propositional logic it is common to take as logical axioms all formulae of the following forms, where formula_5, formula_6, and formula_7 can be any formulae of the language and where the included primitive connectives are only "formula_8" for negation of the immediately following proposition and "formula_9" for implication from antecedent to consequent propositions:


Each of these patterns is an "axiom schema", a rule for generating an infinite number of axioms. For example, if formula_13, formula_14, and formula_15 are propositional variables, then formula_16 and formula_17 are both instances of axiom schema 1, and hence are axioms. It can be shown that with only these three axiom schemata and "modus ponens", one can prove all tautologies of the propositional calculus. It can also be shown that no pair of these schemata is sufficient for proving all tautologies with "modus ponens".

Other axiom schemata involving the same or different sets of primitive connectives can be alternatively constructed.

These axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.

This means that, for any variable symbol formula_18, the formula formula_19 can be regarded as an axiom. Also, in this example, for this not to fall into vagueness and a never-ending series of "primitive notions", either a precise notion of what we mean by formula_19 (or, for that matter, "to be equal") has to be well established first, or a purely formal and syntactical usage of the symbol formula_21 has to be enforced, only regarding it as a string and only a string of symbols, and mathematical logic does indeed do that.

Another, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:

Where the symbol formula_22 stands for the formula formula_5 with the term formula_24 substituted for formula_18. (See Substitution of variables.) In informal terms, this example allows us to state that, if we know that a certain property formula_26 holds for every formula_18 and that formula_24 stands for a particular object in our structure, then we should be able to claim formula_29. Again, "we are claiming that the formula" formula_30 "is valid", that is, we must be able to give a "proof" of this fact, or more properly speaking, a "metaproof". These examples are "metatheorems" of our theory of mathematical logic since we are dealing with the very concept of "proof" itself. Aside from this, we can also have Existential Generalization:

Non-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example, the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not "tautologies". Another name for a non-logical axiom is "postulate".

Almost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that, in principle, every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.

Non-logical axioms are often simply referred to as "axioms" in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom, we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.

Thus, an "axiom" is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.

This section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.

Basic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe is used, but in fact, most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.

The study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of "abstract algebra" brought with itself group theory, rings, fields, and Galois theory.

This list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.

The Peano axioms are the most widely used "axiomatization" of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.

We have a language formula_31 where formula_32 is a constant symbol and formula_33 is a unary function and the following axioms:


The standard structure is formula_39 where formula_40 is the set of natural numbers, formula_33 is the successor function and formula_32 is naturally interpreted as the number 0.

Probably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as "4 + 1" because for nearly two millennia the fifth (parallel) postulate ("through a point outside a line there is exactly one parallel") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. One can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate ("a line can be extended indefinitely") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.

The objectives of the study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a "Dedekind complete ordered field", meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires the use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.

A deductive system consists of a set formula_43 of logical axioms, a set formula_44 of non-logical axioms, and a set formula_45 of "rules of inference". A desirable property of a deductive system is that it be complete. A system is said to be complete if, for all formulas formula_5,
that is, for any statement that is a "logical consequence" of formula_44 there actually exists a "deduction" of the statement from formula_44. This is sometimes expressed as "everything that is true is provable", but it must be understood that "true" here means "made true by the set of axioms", and not, for example, "true in the intended interpretation". Gödel's completeness theorem establishes the completeness of a certain commonly used type of deductive system.

Note that "completeness" has a different meaning here than it does in the context of Gödel's first incompleteness theorem, which states that no "recursive", "consistent" set of non-logical axioms formula_44 of the Theory of Arithmetic is "complete", in the sense that there will always exist an arithmetic statement formula_5 such that neither formula_5 nor formula_52 can be proved from the given set of axioms.

There is thus, on the one hand, the notion of "completeness of a deductive system" and on the other hand that of "completeness of a set of non-logical axioms". The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.

Early mathematicians regarded axiomatic geometry as a model of physical space, and obviously, there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details, and modern algebra was born. In the modern view, axioms may be any set of formulas, as long as they are not known to be inconsistent.




Alpha

Alpha (uppercase , lowercase ; , ", or ) is the first letter of the Greek alphabet. In the system of Greek numerals, it has a value of one. Alpha is derived from the Phoenician letter aleph , which is the West Semitic word for "ox". Letters that arose from alpha include the Latin letter A and the Cyrillic letter А.

In Ancient Greek, alpha was pronounced and could be either phonemically long ([aː]) or short ([a]). Where there is ambiguity, long and short alpha are sometimes written with a macron and breve today: .

In Modern Greek, vowel length has been lost, and all instances of alpha simply represent the open front unrounded vowel .

In the polytonic orthography of Greek, alpha, like other vowel letters, can occur with several diacritic marks: any of three accent symbols (), and either of two breathing marks (), as well as combinations of these. It can also combine with the iota subscript ().

In the Attic–Ionic dialect of Ancient Greek, long alpha fronted to (eta). In Ionic, the shift took place in all positions. In Attic, the shift did not take place after epsilon, iota, and rho (; ). In Doric and Aeolic, long alpha is preserved in all positions.

Privative a is the Ancient Greek prefix or , added to words to negate them. It originates from the Proto-Indo-European *"" (syllabic nasal) and is cognate with English "un-".

Copulative a is the Greek prefix or . It comes from Proto-Indo-European *"".

The letter alpha represents various concepts in physics and chemistry, including alpha radiation, angular acceleration, alpha particles, alpha carbon and strength of electromagnetic interaction (as fine-structure constant). Alpha also stands for thermal expansion coefficient of a compound in physical chemistry. It is also commonly used in mathematics in algebraic solutions representing quantities such as angles. Furthermore, in mathematics, the letter alpha is used to denote the area underneath a normal curve in statistics to denote significance level when proving null and alternative hypotheses. In ethology, it is used to name the dominant individual in a group of animals. In aerodynamics, the letter is used as a symbol for the angle of attack of an aircraft and the word "alpha" is used as a synonym for this property. In mathematical logic, α is sometimes used as a placeholder for ordinal numbers.

The proportionality operator "∝" (in Unicode: U+221D) is sometimes mistaken for alpha.

The uppercase letter alpha is not generally used as a symbol because it tends to be rendered identically to the uppercase Latin A.

In the International Phonetic Alphabet, the letter ɑ, which looks similar to the lower-case alpha, represents the open back unrounded vowel.

The Phoenician alphabet was adopted for Greek in the early 8th century BC, perhaps in Euboea. 
The majority of the letters of the Phoenician alphabet were adopted into Greek with much the same sounds as they had had in Phoenician, but "ʼāleph", the Phoenician letter representing the glottal stop ,
was adopted as representing the vowel ; similarly, "hē" and "ʽayin" are Phoenician consonants that became Greek vowels, epsilon and omicron , respectively.

Plutarch, in "Moralia", presents a discussion on why the letter alpha stands first in the alphabet. Ammonius asks Plutarch what he, being a Boeotian, has to say for Cadmus, the Phoenician who reputedly settled in Thebes and introduced the alphabet to Greece, placing "alpha" first because it is the Phoenician name for ox—which, unlike Hesiod, the Phoenicians considered not the second or third, but the first of all necessities. "Nothing at all," Plutarch replied. He then added that he would rather be assisted by Lamprias, his own grandfather, than by Dionysus' grandfather, i.e. Cadmus. For Lamprias had said that the first articulate sound made is "alpha", because it is very plain and simple—the air coming off the mouth does not require any motion of the tongue—and therefore this is the first sound that children make.

According to Plutarch's natural order of attribution of the vowels to the planets, alpha was connected with the Moon.

As the first letter of the alphabet, Alpha as a Greek numeral came to represent the number 1.
Therefore, Alpha, both as a symbol and term, is used to refer to the "first", or "primary", or "principal" (most significant) occurrence or status of a thing.
The New Testament has God declaring himself to be the "Alpha and Omega, the beginning and the end, the first and the last." (, KJV, and see also ).

Consequently, the term "alpha" has also come to be used to denote "primary" position in social hierarchy, examples being the concept of dominant "alpha" members in groups of animals.


For accented Greek characters, see Greek diacritics: Computer encoding.



Alvin Toffler

Alvin Eugene Toffler (October 4, 1928 – June 27, 2016) was an American writer, futurist, and businessman known for his works discussing modern technologies, including the digital revolution and the communication revolution, with emphasis on their effects on cultures worldwide. He is regarded as one of the world's outstanding futurists.

Toffler was an associate editor of "Fortune" magazine. In his early works he focused on technology and its impact, which he termed "information overload". In 1970, his first major book about the future, "Future Shock", became a worldwide best-seller and has sold over 6 million copies.

He and his wife Heidi Toffler (1929-2019), who collaborated with him for most of his writings, moved on to examining the reaction to changes in society with another best-selling book, "The Third Wave", in 1980. In it, he foresaw such technological advances as cloning, personal computers, the Internet, cable television and mobile communication. His later focus, via their other best-seller, "Powershift", (1990), was on the increasing power of 21st-century military hardware and the proliferation of new technologies.

He founded Toffler Associates, a management consulting company, and was a visiting scholar at the Russell Sage Foundation, visiting professor at Cornell University, faculty member of the New School for Social Research, a White House correspondent, and a business consultant. Toffler's ideas and writings were a significant influence on the thinking of business and government leaders worldwide, including China's Zhao Ziyang, and AOL founder Steve Case.

Alvin Toffler was born on October 4, 1928, in New York City, and raised in Brooklyn. He was the son of Rose (Albaum) and Sam Toffler, a furrier, both Polish Jews who had migrated to America. He had one younger sister. He was inspired to become a writer at the age of 7 by his aunt and uncle, who lived with the Tofflers. "They were Depression-era literary intellectuals," Toffler said, "and they always talked about exciting ideas."

Toffler graduated from New York University in 1950 as an English major, though by his own account he was more focused on political activism than grades. He met his future wife, Adelaide Elizabeth Farrell (nicknamed "Heidi"), when she was starting a graduate course in linguistics. Being radical students, they decided against further graduate work and moved to Cleveland, Ohio, where they married on April 29, 1950.

Seeking experiences to write about, Alvin and Heidi Toffler spent the next five years as blue collar workers on assembly lines while studying industrial mass production in their daily work. He compared his own desire for experience to other writers, such as Jack London, who in his quest for subjects to write about sailed the seas, and John Steinbeck, who went to pick grapes with migrant workers. In their first factory jobs, Heidi became a union shop steward in the aluminum foundry where she worked. Alvin became a millwright and welder. In the evenings Alvin would write poetry and fiction, but discovered he was proficient at neither.

His hands-on practical labor experience helped Alvin Toffler land a position at a union-backed newspaper, a transfer to its Washington bureau in 1957, then three years as a White House correspondent, covering Congress and the White House for a Pennsylvania daily newspaper.

They returned to New York City in 1959 when "Fortune" magazine invited Alvin to become its labor columnist, later having him write about business and management. After leaving "Fortune" magazine in 1962, Toffler began a freelance career, writing long form articles for scholarly journals and magazines. His 1964 "Playboy interviews" with Russian novelist Vladimir Nabokov and Ayn Rand were considered among the magazine's best. His interview with Rand was the first time the magazine had given such a platform to a female intellectual, which as one commentator said, "the real bird of paradise Toffler captured for Playboy in 1964 was Ayn Rand."

Toffler was hired by IBM to conduct research and write a paper on the social and organizational impact of computers, leading to his contact with the earliest computer "gurus" and artificial intelligence researchers and proponents. Xerox invited him to write about its research laboratory and AT&T consulted him for strategic advice. This AT&T work led to a study of telecommunications, which advised the company's top management to break up the company more than a decade before the government forced AT&T to break up.

In the mid-1960s, the Tofflers began five years of research on what would become "Future Shock", published in 1970. It has sold over 6 million copies worldwide, according to the "New York Times," or over 15 million copies according to the Tofflers' Web site. Toffler coined the term "future shock" to refer to what happens to a society when change happens too fast, which results in social confusion and normal decision-making processes breaking down. The book has never been out of print and has been translated into dozens of languages.

He continued the theme in "The Third Wave" in 1980. While he describes the first and second waves as the agricultural and industrial revolutions, the "third wave," a phrase he coined, represents the current information, computer-based revolution. He forecast the spread of the Internet and email, interactive media, cable television, cloning, and other digital advancements. He claimed that one of the side effects of the digital age has been "information overload," another term he coined. In 1990, he wrote "Powershift", also with the help of his wife, Heidi.

In 1996, with American business consultant Tom Johnson, they co-founded Toffler Associates, an advisory firm designed to implement many of the ideas the Tofflers had written on. The firm worked with businesses, NGOs, and governments in the United States, South Korea, Mexico, Brazil, Singapore, Australia, and other countries. During this period in his career, Toffler lectured worldwide, taught at several schools and met world leaders, such as Mikhail Gorbachev, along with key executives and military officials.

Toffler stated many of his ideas during an interview with the Australian Broadcasting Corporation in 1998. "Society needs people who take care of the elderly and who know how to be compassionate and honest," he said. "Society needs people who work in hospitals. Society needs all kinds of skills that are not just cognitive; they're emotional, they're affectional. You can't run the society on data and computers alone."

His opinions about the future of education, many of which were in "Future Shock", have often been quoted. An often misattributed quote, however, is that of psychologist Herbert Gerjuoy: "Tomorrow's illiterate will not be the man who can't read; he will be the man who has not learned how to learn."

Early in his career, after traveling to other countries, he became aware of the new and myriad inputs that visitors received from these other cultures. He explained during an interview that some visitors would become "truly disoriented and upset" by the strange environment, which he described as a reaction to culture shock. From that issue, he foresaw another problem for the future, when a culturally "new environment comes to you ... and comes to you rapidly." That kind of sudden cultural change within one's own country, which he felt many would not understand, would lead to a similar reaction, one of "future shock", which he wrote about in his book by that title. Toffler writes:
In "The Third Wave", Toffler describes three types of societies, based on the concept of "waves"—each wave pushes the older societies and cultures aside. He describes the "First Wave" as the society after agrarian revolution and replaced the first hunter-gatherer cultures. The "Second Wave," he labels society during the Industrial Revolution (ca. late 17th century through the mid-20th century). That period saw the increase of urban industrial populations which had undermined the traditional nuclear family, and initiated a factory-like education system, and the growth of the corporation. Toffler said:

The "Third Wave" was a term he coined to describe the post-industrial society, which began in the late 1950s. His description of this period dovetails with other futurist writers, who also wrote about the Information Age, Space Age, Electronic Era, Global Village, terms which highlighted a scientific-technological revolution. The Tofflers claimed to have predicted a number of geopolitical events, such as the collapse of the Soviet Union, the fall of the Berlin Wall and the future economic growth in the Asia-Pacific region.

Toffler often visited with dignitaries in Asia, including China's Zhao Ziyang, Singapore's Lee Kuan Yew and South Korea's Kim Dae Jung, all of whom were influenced by his views as Asia's emerging markets increased in global significance during the 1980s and 1990s. Although they had originally censored some of his books and ideas, China's government cited him along with Franklin Roosevelt and Bill Gates as being among the Westerners who had most influenced their country. "The Third Wave" along with a video documentary based on it became best-sellers in China and were widely distributed to schools. The video's success inspired the marketing of videos on related themes in the late 1990s by Infowars, whose name is derived from the term coined by Toffler in the book. Toffler's influence on Asian thinkers was summed up in an article in "Daedalus", published by the American Academy of Arts & Sciences:
U.S. House Speaker Newt Gingrich publicly lauded his ideas about the future, and urged members of Congress to read Toffler's book, "Creating a New Civilization" (1995). Others, such as AOL founder Steve Case, cited Toffler's "The Third Wave" as a formative influence on his thinking, which inspired him to write "The Third Wave: An Entrepreneur's Vision of the Future" in 2016. Case said that Toffler was a "real pioneer in helping people, companies and even countries lean into the future."

In 1980, Ted Turner founded CNN, which he said was inspired by Toffler's forecasting the end of the dominance of the three main television networks. Turner's company, Turner Broadcasting, published Toffler's "Creating a New Civilization" in 1995. Shortly after the book was released, the former Soviet president Mikhail Gorbachev hosted the Global Governance Conference in San Francisco with the theme, "Toward a New Civilization", which was attended by dozens of world figures, including the Tofflers, George H. W. Bush, Margaret Thatcher, Carl Sagan, Abba Eban and Turner with his then-wife, actress Jane Fonda.

Mexican billionaire Carlos Slim was influenced by his works, and became a friend of the writer. Global marketer J.D. Power also said he was inspired by Toffler's works.

Since the 1960s, people had tried to make sense out of the effect of new technologies and social change, a problem which made Toffler's writings widely influential beyond the confines of scientific, economic, and public policy. His works and ideas have been subject to various criticisms, usually with the same argumentation used against futurology: that foreseeing the future is nigh impossible.

Techno music pioneer Juan Atkins cites Toffler's phrase "techno rebels" in "The Third Wave" as inspiring him to use the word "techno" to describe the musical style he helped to create

Musician Curtis Mayfield released a disco song called "Future Shock," later covered in an electro version by Herbie Hancock. Science fiction author John Brunner wrote "The Shockwave Rider," from the concept of "future shock."

The nightclub Toffler, in Rotterdam, is named after him.

In the song "Victoria" by The Exponents, the protagonist's daily routine and cultural interests are described: "She's up in time to watch the soap operas, reads Cosmopolitan and Alvin Toffler".

Accenture, the management consultancy firm, identified Toffler in 2002 as being among the most influential voices in business leaders, along with Bill Gates and Peter Drucker. Toffler has also been described in a "Financial Times" interview as the "world's most famous futurologist". In 2006, the "People's Daily" classed him among the 50 foreigners who shaped modern China, which one U.S. newspaper notes made him a "guru of sorts to world statesmen." Chinese Premier and General Secretary Zhao Ziyang was greatly influenced by Toffler. He convened conferences to discuss "The Third Wave" in the early 1980s, and in 1985 the book was the No. 2 best seller in China.

Author Mark Satin characterizes Toffler as an important early influence on radical centrist political thought.

Newt Gingrich became close to the Tofflers in the 1970s and said "The Third Wave" had immensely influenced his own thinking and was "one of the great seminal works of our time."

Toffler has received several prestigious prizes and awards, including the McKinsey Foundation Book Award for Contributions to Management Literature, Officier de L'Ordre des Arts et Lettres, and appointments, including Fellow of the American Association for the Advancement of Science and the International Institute for Strategic Studies.

In 2006, Alvin and Heidi Toffler were recipients of Brown University's Independent Award.

Toffler was married to Heidi Toffler (born Adelaide Elizabeth Farrell), also a writer and futurist. They lived in the Bel Air section of Los Angeles, California, and previously lived in Redding, Connecticut.

The couple's only child, Karen Toffler (1954–2000), died at age 46 after more than a decade suffering from Guillain–Barré syndrome.

Alvin Toffler died in his sleep on June 27, 2016, at his home in Los Angeles. No cause of death was given. He is buried at Westwood Memorial Park.

Alvin Toffler co-wrote his books with his wife Heidi.



The Amazing Spider-Man

The Amazing Spider-Man is an ongoing American superhero comic book series featuring the Marvel Comics superhero Spider-Man as its title character and main protagonist. Being in the mainstream continuity of the franchise, it was the character's first title, launching seven months after his introduction in the final issue of "Amazing Fantasy". The series began publication with a March 1963 cover date and has been published nearly continuously to date over six volumes with only one significant interruption. Issues of the title currently feature an issue number within its sixth volume, as well as a "legacy" number reflecting the issue's overall number across all "Amazing Spider-Man" volumes. The title reached 900 issues in 2022.

The series began as a bimonthly periodical before being increased to monthly after four issues. It was the character's sole monthly headlining title until "Peter Parker, the Spectacular Spider-Man" launched in 1978. After 441 issues, "The Amazing Spider-Man" was restarted in 1999 as issue No. 1 of Volume 2. It ran for 58 issues before reverting to the title's overall issue number with #500 in 2003. The series ran essentially continuously over the first two volumes from 1963 until its landmark 700th issue at the end of 2012 when it was replaced by "The Superior Spider-Man" as part of the Marvel NOW! relaunch of Marvel's comic lines. The title was occasionally published biweekly during the first two volumes, and was published three times a month from 2008 to 2010. After the relaunch of "Action Comics" and "Detective Comics", "The Amazing Spider-Man" briefly became the highest-numbered active American comic book.

"The Amazing Spider-Man" returned with volume 3 in April 2014 following the conclusion of "The Superior Spider-Man" story arc after 32 issues. In late 2015, the series was relaunched with a fourth volume following the 2015 "Secret Wars" event. After 45 years , the volume was once again relaunched as part of "Marvel Legacy", returning to the overall "legacy" numbering with issue No. 789 in late 2017. Less than a year later, the series was relaunched again with a fifth volume as part of Marvel's "Fresh Start". For the first time, although the issue numbers were again restarted from #1, the issues also bore the overall "legacy" issue number. A sixth volume commenced in April 2022 to celebrate Spider-Man's 60th anniversary. Since the second volume, the title has had various release schedules, including monthly and bi-weekly, among others.

Writer-editor Stan Lee and artist and co-plotter Steve Ditko created the character of Spider-Man, and the pair produced 38 issues from March 1963 to July 1966. Ditko left after the 38th issue, while Lee remained as writer until issue 100. Since then, many writers and artists have taken over the monthly comic through the years, chronicling the adventures of Marvel's most identifiable hero.

"The Amazing Spider-Man" has been the character's flagship series for his first fifty years in publication, and was the only monthly series to star Spider-Man until "Peter Parker, The Spectacular Spider-Man", in 1976, although 1972 saw the debut of "Marvel Team-Up", with the vast majority of issues featuring Spider-Man along with a rotating cast of other Marvel characters. Most of the major characters and villains of the Spider-Man saga have been introduced in "Amazing", and with few exceptions, it is where most key events in the character's history have occurred. The title was published continuously until No. 441 (Nov. 1998) when Marvel Comics relaunched it as vol. 2 No. 1 (Jan. 1999), but on Spider-Man's 40th anniversary, this new title reverted to using the numbering of the original series, beginning again with issue No. 500 (Dec. 2003) and lasting until the final issue, No. 700 (Feb. 2013).
Due to strong sales on the character's first appearance in "Amazing Fantasy" No. 15, Spider-Man was given his own ongoing series in March 1963. The initial years of the series, under Lee and Ditko, chronicled Spider-Man's nascent career as a masked super-human vigilante with his civilian life as hard-luck yet perpetually good-humored and well-meaning teenager Peter Parker. Peter balanced his career as Spider-Man with his job as a freelance photographer for "The Daily Bugle" under the bombastic editor-publisher J. Jonah Jameson to support himself and his frail Aunt May. At the same time, Peter dealt with public hostility towards Spider-Man and the antagonism of his classmates Flash Thompson and Liz Allan at Midtown High School, while embarking on a tentative, ill-fated romance with Jameson's secretary, Betty Brant.

By focusing on Parker's everyday problems, Lee and Ditko created a groundbreakingly flawed, self-doubting superhero, and the first major teenaged superhero to be a protagonist and not a sidekick. Ditko's quirky art provided a stark contrast to the more cleanly dynamic stylings of Marvel's most prominent artist, Jack Kirby, and combined with the humor and pathos of Lee's writing to lay the foundation for what became an enduring mythos.

Most of Spider-Man's key villains and supporting characters were introduced during this time. Issue No. 1 (Mar. 1963) featured the first appearances of J. Jonah Jameson and his astronaut son John Jameson, and the supervillain the Chameleon. It included the hero's first encounter with the superhero team the Fantastic Four. Issue No. 2 (May 1963) featured the first appearance of the Vulture and the Tinkerer as well as the beginning of Parker's freelance photography career at the newspaper "The Daily Bugle".

The Lee-Ditko era continued to usher in a significant number of villains and supporting characters, including Doctor Octopus in No. 3 (July 1963); the Sandman and Betty Brant in No. 4 (Sept. 1963); the Lizard in No. 6 (Nov. 1963); Living Brain in No. 8 (Jan. 1964); Electro in No. 9 (Mar. 1964); Mysterio in No. 13 (June 1964); the Green Goblin in No. 14 (July 1964); Kraven The Hunter in No. 15 (Aug. 1964); reporter Ned Leeds in No. 18 (Nov. 1964); and the Scorpion in No. 20 (Jan. 1965). The Molten Man was introduced in No. 28 (Sept. 1965) which also featured Parker's graduation from high school. Peter began attending Empire State University in No. 31 (Dec. 1965), which featured the first appearances of friends and classmates Gwen Stacy and Harry Osborn. Harry's father, Norman Osborn first appeared in No. 23 (April 1965) as a member of Jameson's country club but was not named nor revealed as Harry's father until No. 37 (June 1966).

One of the most celebrated issues of the Lee-Ditko run is No. 33 (Feb. 1966), the third part of the story arc "If This Be My Destiny...!", which features the dramatic scene of Spider-Man, through force of will and thoughts of family, escaping from being pinned by heavy machinery. Comics historian Les Daniels noted that "Steve Ditko squeezes every ounce of anguish out of Spider-Man's predicament, complete with visions of the uncle he failed and the aunt he has sworn to save." Peter David observed that "After his origin, this two-page sequence from "Amazing Spider-Man" No. 33 is perhaps the best-loved sequence from the Stan Lee/Steve Ditko era." Steve Saffel stated the "full page Ditko image from "The Amazing Spider-Man" No. 33 is one of the most powerful ever to appear in the series and influenced writers and artists for many years to come." and Matthew K. Manning wrote that "Ditko's illustrations for the first few pages of this Lee story included what would become one of the most iconic scenes in Spider-Man's history." The story was chosen as No. 15 in the 100 Greatest Marvels of All Time poll of Marvel's readers in 2001. Editor Robert Greenberger wrote in his introduction to the story that "These first five pages are a modern-day equivalent to Shakespeare as Parker's soliloquy sets the stage for his next action. And with dramatic pacing and storytelling, Ditko delivers one of the great sequences in all comics."

Although credited only as artist for most of his run, Ditko would eventually plot the stories as well as draw them, leaving Lee to script the dialogue. A rift between Ditko and Lee developed, and the two men were not on speaking terms long before Ditko completed his last issue, "The Amazing Spider-Man" No. 38 (July 1966). The exact reasons for the Ditko-Lee split have never been fully explained. Spider-Man successor artist John Romita Sr., in a 2010 deposition, recalled that Lee and Ditko "ended up not being able to work together because they disagreed on almost everything, cultural, social, historically, everything, they disagreed on characters..."

In successor penciler Romita Sr.'s first issue, No. 39 (Aug. 1966), nemesis the Green Goblin discovers Spider-Man's secret identity and reveals his own to the captive hero. Romita's Spider-Man – more polished and heroic-looking than Ditko's – became the model for two decades. The Lee-Romita era saw the introduction of such characters as "Daily Bugle" managing editor Robbie Robertson in No. 52 (Sept. 1967) and NYPD Captain George Stacy, father of Parker's girlfriend Gwen Stacy, in No. 56 (Jan. 1968). The most important supporting character to be introduced during the Romita era was Mary Jane Watson, who made her first full appearance in No. 42 (Nov. 1966), although she first appeared in No. 25 (June 1965) with her face obscured and had been mentioned since No. 15 (Aug. 1964). Peter David wrote in 2010 that Romita "made the definitive statement of his arrival by pulling Mary Jane out from behind the oversized potted plant [that blocked the reader's view of her face in issue no. 25] and placing her on panel in what would instantly become an iconic moment." Romita has stated that in designing Mary Jane, he "used Ann-Margret from the movie "Bye Bye Birdie" as a guide, using her coloring, the shape of her face, her red hair and her form-fitting short skirts."

Lee and Romita toned down the prevalent sense of antagonism in Parker's world by improving Parker's relationship with the supporting characters and having stories focused as much on the social and college lives of the characters as they did on Spider-Man's adventures. The stories became more topical, addressing issues such as civil rights, racism, prisoners' rights, the Vietnam War, and political elections.

Issue No. 50 (June 1967) introduced the highly enduring criminal mastermind the Kingpin, who would become a major force as well in the superhero series "Daredevil". Other notable first appearances in the Lee-Romita era include the Rhino in No. 41 (Oct. 1966), the Shocker in No. 46 (Mar. 1967), the Prowler in No. 78 (Nov. 1969), and the Kingpin's son, Richard Fisk, in No. 83 (Apr. 1970).

Several spin-off series debuted in the 1970s: "Marvel Team-Up" in 1972, and "The Spectacular Spider-Man" in 1976. A short-lived series titled "Giant-Size Spider-Man" began in July 1974 and ran six issues through 1975. "Spidey Super Stories", a series aimed at children ages 6–10, ran for 57 issues from October 1974 through 1982.
The flagship title's second decade took a grim turn with a story in #89-90 (Oct.-Nov. 1970) featuring the death of Captain George Stacy. This was the first Spider-Man story to be penciled by Gil Kane, who would alternate drawing duties with Romita for the next year-and-a-half and would draw several landmark issues.

One such story took place in the controversial issues #96–98 (May–July 1971). Writer-editor Lee defied the Comics Code Authority with this story, in which Parker's friend Harry Osborn, was hospitalized after over-dosing on pills. Lee wrote this story upon a request from the U. S. Department of Health, Education, and Welfare for a story about the dangers of drugs. Citing its dictum against depicting drug use, even in an anti-drug context, the CCA refused to put its seal on these issues. With the approval of Marvel publisher Martin Goodman, Lee had the comics published without the seal. The comics sold well and Marvel won praise for its socially conscious efforts. The CCA subsequently loosened the Code to permit negative depictions of drugs, among other new freedoms.

"The Six Arms Saga" of #100–102 (Sept.–Nov. 1971) introduced Morbius, the Living Vampire. The second installment was the first "Amazing Spider-Man" story not written by co-creator Lee, with Roy Thomas taking over writing the book for several months before Lee returned to write #105–110 (Feb.-July 1972). Lee, who was going on to become Marvel Comics' publisher, with Thomas becoming editor-in-chief, then turned writing duties over to 19-year-old Gerry Conway, who scripted the series through 1975. Romita penciled Conway's first half-dozen issues, which introduced the gangster Hammerhead in No. 113 (Oct. 1972). Kane then succeeded Romita as penciler, although Romita would continue inking Kane for a time.

Issue 121 (June 1973 by Conway-Kane-Romita) featured the death of Gwen Stacy at the hands of the Green Goblin in "The Night Gwen Stacy Died." Her demise and the Goblin's apparent death one issue later formed a story arc widely considered as the most defining in the history of Spider-Man. The aftermath of the story deepened both the characterization of Mary Jane Watson and her relationship with Parker.

In 1973 Gil Kane was succeeded by Ross Andru, whose run lasted from issue #125 (Oct. 1973) to #185 (Oct. 1978). Issue#129 (Feb. 1974) introduced the Punisher, who would become one of Marvel Comics' most popular characters. The Conway-Andru era featured the first appearances of the Man-Wolf in #124–125 (Sept.-Oct. 1973); the near-marriage of Doctor Octopus and Aunt May in #131 (Apr. 1974); Harry Osborn stepping into his father's role as the Green Goblin in #135–137 (Aug.-Oct.1974); and the original "Clone Saga", containing the introduction of Spider-Man's clone, in #147–149 (Aug.-Oct. 1975).

Archie Goodwin and Gil Kane produced the title's 150th issue (Nov. 1975) before Len Wein became writer with issue No. 151. During Wein's tenure, Harry Osborn and Liz Allen dated and became engaged; J. Jonah Jameson was introduced to his eventual second wife, Marla Madison; and Aunt May suffered a heart attack. Wein's last story on "Amazing" was a five-issue arc in #176–180 (Jan.-May 1978) featuring a third Green Goblin (Harry Osborn's psychiatrist, Bart Hamilton).

Marv Wolfman, Marvel's editor-in-chief from 1975 to 1976, succeeded Wein as writer and, in his first issue, #182 (July 1978), had Parker propose marriage to Watson, who refused in the following issue. Keith Pollard succeeded Andru as artist shortly afterward and, with Wolfman, introduced the likable rogue the Black Cat (Felicia Hardy) in #194 (July 1979). As a love interest for Spider-Man, the Black Cat would go on to be an important supporting character for the better part of the next decade and remain a friend and occasional lover into the 2010s.

"The Amazing Spider-Man" #200 (Jan. 1980) featured the return and death of the burglar who killed Spider-Man's Uncle Ben. Writer Marv Wolfman and penciler Keith Pollard both left the title by mid-year, succeeded by Dennis O'Neil, a writer known for groundbreaking 1970s work at rival DC Comics, and penciler John Romita Jr. O'Neil wrote two issues of "The Amazing Spider-Man Annual" which were both drawn by Frank Miller. The 1980 "Annual" featured a team-up with Doctor Strange while the 1981 "Annual" showcased a meeting with the Punisher. Roger Stern, who had written nearly 20 issues of sister title "The Spectacular Spider-Man", took over "Amazing" with #224 (Jan. 1982). During his two years on the title, Stern augmented the backgrounds of long-established Spider-Man villains and, with Romita Jr., created the mysterious supervillain the Hobgoblin in #238–239 (Mar.–Apr. 1983). Fans engaged with the mystery of the Hobgoblin's secret identity, which continued throughout #244–245 and 249–251 (Sept.-Oct. 1983 and Feb.-April 1984). One lasting change was the reintroduction of Mary Jane Watson as a more serious, mature woman who becomes Peter's confidante after she reveals that she knows his secret identity. Stern also wrote "The Kid Who Collects Spider-Man" in "The Amazing Spider-Man" #248 (Jan. 1984), a story which ranks among his most popular.

By mid-1984, Tom DeFalco and Ron Frenz took over scripting and penciling. DeFalco helped establish Parker and Watson's mature relationship, laying the foundation for the characters' wedding in 1987. Notably, in #257 (Oct. 1984), Watson tells Parker that she knows he is Spider-Man, and in #259 (Dec. 1984), she reveals to Parker the extent of her troubled childhood. Other notable issues of the DeFalco-Frenz era include #252 (May 1984), the first appearance of Spider-Man's black costume, which the hero would wear almost exclusively for the next four years' worth of comics; the debut of criminal mastermind the Rose in #253 (June 1984); the revelation in #258 (Nov. 1984) that the black costume is a living being, a symbiote; and the introduction of the female mercenary Silver Sable in #265 (June 1985).

DeFalco and Frenz were both removed from "The Amazing Spider-Man" in 1986 by editor Jim Owsley under acrimonious circumstances. A succession of artists including Alan Kupperberg, John Romita Jr., and Alex Saviuk penciled the series from 1987 to 1988, and Owsley wrote the book for the first half of 1987, scripting the five-part "Gang War" story (#284–288) that DeFalco plotted. Former "Spectacular Spider-Man" writer Peter David scripted #289 (June 1987), which revealed Ned Leeds as being the Hobgoblin although this was retconned in 1996 by Roger Stern into Leeds not being the original Hobgoblin after all.

David Michelinie took over as writer in the next issue, for a story arc in #290–292 (July–Sept. 1987) that led to the marriage of Peter Parker and Mary Jane Watson in "Amazing Spider-Man Annual" No. 21. The "Kraven's Last Hunt" storyline by writer J.M. DeMatteis and artists Mike Zeck and Bob McLeod crossed over into "The Amazing Spider-Man" #293 and 294. Issue No.298 (Mar. 1988) was the first Spider-Man comic to be drawn by future industry star Todd McFarlane, the first regular artist on "The Amazing Spider-Man" since Frenz's departure. McFarlane revolutionized Spider-Man's look. His depiction – "Ditko-esque" poses, large eyes; wiry, contorted limbs; and messy, knotted, convoluted webbing – influenced the way virtually all subsequent artists would draw the character. McFarlane's other significant contribution to the Spider-Man canon was the design for what would become one of Spider-Man's most wildly popular antagonists, the supervillain Venom. Issue No. 299 (Apr. 1988) featured Venom's first appearance (a last-page cameo) before his first full appearance in #300 (May 1988). The latter issue featured Spider-Man reverting to his original red-and-blue costume.

Other notable issues of the Michelinie-McFarlane era include #312 (Feb. 1989), featuring the Green Goblin vs. the Hobgoblin; and #315–317 (May–July 1989), with the return of Venom. In July 2012, Todd McFarlane's original cover art for "The Amazing Spider-Man" No. 328 sold for a bid of $657,250, making it the most expensive American comic book art ever sold at auction.

With a civilian life as a married man, the Spider-Man of the 1990s was different from the superhero of the previous three decades. McFarlane left the title in 1990 to write and draw a new series titled simply "". His successor, Erik Larsen, penciled the book from early 1990 to mid-1991. After issue No. 350, Larsen was succeeded by Mark Bagley, who had won the 1986 Marvel Tryout Contest and was assigned a number of low-profile penciling jobs followed by a run on "New Warriors" in 1990. Bagley penciled the flagship Spider-Man title from 1991 to 1996. During that time, Bagley's rendition of Spider-Man was used extensively for licensed material and merchandise.

Issues #361–363 (April–June 1992) introduced Carnage, a second symbiote nemesis for Spider-Man. The series' 30th-anniversary issue, No. 365 (Aug. 1992), was a double-sized, hologram-cover issue with the cliffhanger ending of Peter Parker's parents, long thought dead, reappearing alive. It would be close to two years before they were revealed to be impostors, who are killed in No. 388 (April 1994), scripter Michelinie's last issue. His 1987–1994 stint gave him the second-longest run as writer on the title, behind Stan Lee.

Issue No. 375 was released with a gold foil cover. There was an error affecting some issues and which are missing the majority of the foil.

With No. 389, writer J. M. DeMatteis, whose Spider-Man credits included the 1987 "Kraven's Last Hunt" story arc and a 1991–1993 run on "The Spectacular Spider-Man", took over the title. From October 1994 to June 1996, "Amazing" stopped running stories exclusive to it, and ran installments of multi-part stories that crossed over into all the Spider-Man books. One of the few self-contained stories during this period was in No. 400 (April 1995), which featured the death of Aunt May – later revealed to have been faked (although the death still stands in the MC2 continuity). The "Clone Saga" culminated with the revelation that the Spider-Man who had appeared in the previous 20 years of comics was a clone of the real Spider-Man. This plot twist was massively unpopular with many readers, and was later reversed in the "Revelations" story arc that crossed over the Spider-Man books in late 1996.

The Clone Saga tied into a publishing gap after No. 406 (Oct. 1995), when the title was temporarily replaced by "The Amazing Scarlet Spider" #1–2 (Nov.-Dec. 1995), featuring Ben Reilly. The series picked up again with No. 407 (Jan. 1996), with Tom DeFalco returning as writer. Bagley completed his 5½-year run by September 1996. A succession of artists, including Ron Garney, Steve Skroce, Joe Bennett, Rafael Kayanan and John Byrne penciled the book until the final issue, No. 441 (Nov. 1998), after which Marvel rebooted the title with vol. 2, No. 1 (Jan. 1999).

Marvel began "The Amazing Spider-Man" relaunching the 'Amazing' comic book series with (vol. 2) #1 (Jan. 1999). Howard Mackie wrote the first 29 issues. The relaunch included the Sandman being regressed to his criminal ways and the "death" of Mary Jane, which was ultimately reversed. Other elements included the introduction of a new Spider-Woman (who was spun off into her own short-lived series) and references to John Byrne's miniseries "", which was launched at the same time as the reboot. Byrne also penciled issues #1–18 (from 1999 to 2000) and wrote #13–14, John Romita Jr. took his place soon after in October 2000. Mackie's run ended with "The Amazing Spider-Man Annual 2001", which saw the return of Mary Jane, who then left Parker upon reuniting with him.

With issue No. 30 (June 2001), J. Michael Straczynski took over as writer and oversaw additional storylines – most notably his lengthy "Spider-Totem" arc, which raised the issue of whether Spider-Man's powers were magic-based, rather than as the result of a radioactive spider's bite. Additionally, Straczynski resurrected the plot point of Aunt May discovering her nephew was Spider-Man, and returned Mary Jane, with the couple reuniting in "The Amazing Spider-Man" (vol. 2) #50. Straczynski gave Spider-Man a new profession, having Parker teach at his former high school.

Issue No. 30 began a dual numbering system, with the original series numbering (#471) returned and placed alongside the volume two number on the cover. Other longtime, rebooted Marvel Comics titles, including "Fantastic Four", likewise were given the dual numbering around this time. After (vol. 2) #58 (Nov. 2003), the title reverted completely to its original numbering for issue No. 500 (Dec. 2003). Mike Deodato, Jr. penciled the series from mid-2004 until 2006.

That year Peter Parker revealed his Spider-Man identity on live television in the company-crossover storyline "Civil War", in which the superhero community is split over whether to conform to the federal government's new Superhuman Registration Act. This knowledge was erased from the world with the event of the four-part, crossover story arc, "", written partially by J. Michael Straczynski and illustrated by Joe Quesada, running through "The Amazing Spider-Man" #544–545 (Nov.-Dec. 2007), "Friendly Neighborhood Spider-Man" No. 24 (Nov. 2007) and "The Sensational Spider-Man" No. 41 (Dec. 2007), the final issues of those two titles. Here, the demon Mephisto makes a Faustian bargain with Parker and Mary Jane, offering to save Parker's dying Aunt May if the couple will allow their marriage to have never existed, rewriting that portion of their pasts. This story arc marked the end of Straczynski's work on the title.

Following this, Marvel made "The Amazing Spider-Man" the company's sole Spider-Man title, increasing its frequency of publication to three issues monthly, and inaugurating the series with a sequence of "back to basics" story arcs under the banner of "". Parker now exists in a changed world where he and Mary Jane had never married, and Parker has no memory of being married to her, with domino effect differences in their immediate world. The most notable of these revisions to Spider-Man continuity are the return of Harry Osborn, whose death in "The Spectacular Spider-Man" No. 200 (May 1993) is erased; and the reestablishment of Spider-Man's secret identity, with no one except Mary Jane able to recall that Parker is Spider-Man (although he soon reveals his secret identity to the New Avengers and the Fantastic Four). Under the banner of "Brand New Day", Marvel tried to only use newly created villains instead of relying on older ones. Characters like Mister Negative and Overdrive both in Free Comic Book Day 2007 Spider-Man (July 2007), Menace in No. 549 (March 2008), Ana and Sasha Kravinoff in No. 565 (September 2008) and No. 567 (October 2008) respectively, and several more were introduced. The alternating regular writers were initially Dan Slott, Bob Gale, Marc Guggenheim, and Zeb Wells, joined by a rotation of artists that included Steve McNiven, Salvador Larroca, Phil Jimenez, Barry Kitson, Chris Bachalo, Mike McKone, Marcos Martín, and John Romita Jr. Joe Kelly, Mark Waid, Fred Van Lente and Roger Stern later joined the writing team and Paolo Rivera, Lee Weeks and Marco Checchetto the artist roster. Waid's work on the series included a meeting between Spider-Man and Stephen Colbert in "The Amazing Spider-Man" No. 573 (Dec. 2008).
Issue No. 583 (March 2009) included a back-up story in which Spider-Man meets President Barack Obama.

Mark Waid scripted the opening of "The Gauntlet" storyline in issue No. 612 (Jan. 2010). The "Gauntlet" story was concluded by "Grim Hunt" (No. 634-637) which saw the resurrection of long-dead Spider-Man villain, Kraven the Hunter. The series became a twice-monthly title with Dan Slott as sole writer at issue No. 648 (Jan. 2011), launching the "" storyline. Eight additional pages were added per issue. "Big Time" saw major changes in Spider-Man/Peter Parker's life, Peter would start working at Horizon Labs and begin a relationship with Carlie Cooper (his first serious relationship since his marriage to Mary Jane), Mac Gargan returned as Scorpion after spending the past few years as Venom, Phil Urich would take up the mantle of Hobgoblin, and the death of J. Jonah Jameson's wife, Marla Jameson. Issues 654 and 654.1 saw the birth of Agent Venom, Flash Thompson bonded with the Venom symbiote, which would lead to Venom getting his own series "Venom (volume 2)". Starting in No. 659 and going to No. 665, the series built-up to the "Spider-Island" event which officially started in No. 666 and ended in No. 673. "Ends of the Earth" was the next event that ran from No. 682 through No. 687. This publishing format lasted until issue No. 700, which concluded the "Dying Wish" storyline, in which Parker and Doctor Octopus swapped bodies, and the latter taking on the mantle of Spider-Man when Parker apparently died in Doctor Octopus' body. "The Amazing Spider-Man" ended with this issue, with the story continuing in the new series "The Superior Spider-Man". Despite "The Superior Spider-Man" being considered a different series to "The Amazing Spider-Man", the first 33 issue run goes towards the legacy numbering of "The Amazing Spider-Man" acting as issues 701–733. In December 2013, the series returned for five issues, numbered 700.1 through 700.5, with the first two written by David Morrell and drawn by Klaus Janson.

In January 2014, Marvel confirmed that "The Amazing Spider-Man" would be relaunched on April 30, 2014, starting from issue No. 1, with Peter Parker as Spider-Man once again.
The first issue of this new version of "The Amazing Spider-Man" was, according to Diamond Comics Distributors, the "best-selling comic book... in over a decade."
Issues #1–6 were a story arc called "Lucky to be Alive", taking place immediately after "Goblin Nation", with issues No. 4 and No. 5 being a crossover with the "Original Sin" storyline. Issue No. 4 introduced Silk, a new heroine who was bitten by the same spider as Peter Parker. Issues #7–8 featured a team-up between Ms. Marvel and Spider-Man, and had backup stories that tied into "Edge of Spider-Verse". The next major plot arc, titled "Spider-Verse", began in Issue No. 9 and ended in No. 15, features every Spider-Man from across the dimensions being hunted by Morlun, and a team-up to stop him, with Peter Parker of Earth-616 in command of the Spider-Men's Alliance. "The Amazing Spider-Man Annual" No. 1 of the relaunched series was released in December 2014, featuring stories unrelated to "Spider-Verse".

In 2015, Marvel started the universe wide Secret Wars event where the core and several other Marvel universes were combined into one big planet called Battleworld. Battleworld was divided into sections with most of them being self-contained universes. Marvel announced that several of these self-contained universes would get their own tie in series and one of them was "", an alternate universe where Peter Parker and Mary Jane are still married and give birth to their child Annie May Parker, written by Dan Slott. Despite the series being considered separate from the main "Amazing Spider-Man" series, the original 5 issue run is counted towards its legacy numbering acting as No. 752-756.

Following the 2015 "Secret Wars" event, a number of Spider-Man-related titles were either relaunched or created as part of the "All-New, All-Different Marvel" event. Among them, "The Amazing Spider-Man" was relaunched as well and primarily focused on Peter Parker continuing to run Parker Industries and becoming a successful businessman operating worldwide. It also tied with "Civil War II" (involving an Inhuman named Ulysses Cain who can predict possible futures), "Dead No More" (where Ben Reilly [the original Scarlet Spider] revealed to be revived and as one of the antagonists instead), and "Secret Empire" (during Hydra's reign led by a Hydra influenced Captain America/Steve Rogers, and the dismissal of Parker Industries by Peter Parker to stop Otto Octavius). Starting in September 2017, Marvel started the Marvel Legacy event which renumbered several Marvel series to their original numbering. The "Amazing Spider-Man" was put back to its original numbering for #789. Issues #789 through 791 focused on the aftermath of Peter destroying Parker Industries and his fall from grace. Issues #792 and 793 were part of the "Venom Inc." story. "Threat Level: Red" was the story for the next three issues which saw Norman Osborn obtain and bond with the Carnage symbiote. "Go Down Swinging" saw the results of the combination of Osborn's goblin serum and Carnage symbiote creating the Red Goblin. Issue No. 801 was Dan Slott's goodbye issue.

In March 2018, it was announced that writer Nick Spencer would be writing the main semi-monthly "The Amazing Spider-Man" series beginning with a new No. 1, replacing long-time writer Dan Slott, as part of the Fresh Start relaunch that July.
The first five-issue story arc was titled 'Back to Basics.' During the "Back to Basics" story, Kindred, a mysterious villain with some relation to Peter's past, was introduced, and Peter resumed his romantic relationship with Mary Jane once more. The first major story under Spencer was "Hunted" which ran through issues 16 through 23, the story also included four ".HU" issues for issues 16, 18, 19, and 20. The end of the story saw the death of long-running Spider-Man villain Kraven the Hunter, being replaced by his clone son, The Last Son of Kraven.

Issue 45 kicked off the "Sins Rising" story which saw the resurrected Sin-Eater carry out the plans of Kindred to cleanse the world of sin, particularly that of Norman Osborn. The story concluded with issue 49, issue 850 in legacy numbering, seeing Spider-Man and Green Goblin team up to defeat Sin-Eater. "Last Remains" started in issue 50 and concluded in issue 55, the story saw Kindred's plans come to fruition as he tormented Spider-Man. The story has also seen five ".LR" for issues 50, 51, 52, 53, and 54 which focused on The Order of the Web, a new faction of Spider-People consisting of Julia Carpenter (Madame Web), Miles Morales (Spider-Man), Gwen Stacy (Ghost-Spider), Cindy Moon (Silk), Jessica Drew (Spider-Woman), and Anya Corazon (Spider-Girl) . The story also revealed that Kindred is Harry Osborn. "Last Remains" also received two fallout issues called "Last Remains Post-Mortem".

Nick Spencer concluded his run with the "Sinister War" story which wrapped up in No. 74 (legacy numbering 875). The story saw several retcons to the Spider-Man mythos including that Kindred was Gabriel and Sarah Stacy all along, the fact that the Stacy twins were actually genetically engineered beings using Norman Osborn and Gwen Stacy's DNA, that the Harry Osborn that returned in "Brand New Day" was actually a clone, and that Norman had made a deal with Mephisto where he sold Harry's soul to the demon. The story ended with the deaths of the Harry clone, Gabriel, and Sarah and the real Harry's soul being freed from Mephisto's grasp.

After Spencer left the book, Marvel announced the "Beyond" era of Spider-Man would start in #75. The book would be moving back to the format it had during "Brand New Day" where it would have a rotating cast of writers including Kelly Thompson, Saladin Ahmed, Cody Ziglar, Patrick Gleason, and Zeb Wells. The book would also be released three times a month. "Beyond" would focus on Ben Reilly taking up the mantle of Spider-Man once again but backed by the Beyond corporation. Peter also falls ill and cannot be Spider-Man so he gives Ben his blessing to carry on as the main Spider-Man. However, following the conclusion of the storyline in #93, Peter has resumed active duties as Spider-Man, while Ben suffers a mental breakdown and becomes the villain Chasm.

In January 2022, it was announced that writer Zeb Wells and John Romita Jr. would be working on a relaunched "The Amazing Spider-Man", bringing the number of volumes for the title to its sixth, with the series beginning in April 2022 as a semi-monthly publication. The relaunch encompasses both a legacy numbering of #900 as well as the 60th anniversary for the character. The relaunch took place months after a mysterious event that left Peter on bad terms with the superhero community and ended his relationship with Mary Jane. He ends up taking a job at Oscorp and begins working closely with Norman Osborn and starts dating Black Cat. The volume's first crossover event was entitled "Dark Web", with Chasm having teamed up with Madelyne Pryor to bring limbo to Earth. It's later revealed that Benjamin Rabin, the emissary of the Mayan god of mischief Wayeb', sent Peter and Mary Jane to an alternate dimension to conduct a ceremony that would allow Wayeb to control the Earth. Peter was sent back to his Earth, while due to the alternative passage of time, Mary Jane and Paul, Rabin's son in that dimension, spent four years in the realm together and adopted two children. When Peter eventually rescued them, Mary Jane refused to part with her new family. Rabin then planned to sacrifice Mary Jane to resurrect Wayeb, but is ultimately stopped by Ms. Marvel sacrificing herself, but not before Rabin reveals that Paul and Mary Jane's kids were illusions created by him and ceased their existence.

Black-and-white

Major story arcs

Collections


Antigua and Barbuda

Antigua and Barbuda (, ) is a sovereign island country in the Caribbean. It lies at the conjuncture of the Caribbean Sea and the Atlantic Ocean in the Leeward Islands part of the Lesser Antilles.

The country consists of two major islands, Antigua and Barbuda, which are approximately apart, and several smaller islands, including Great Bird, Green, Guiana, Long, Maiden, Prickly Pear, York, and Redonda. The permanent population is approximately 97,120 ( estimates), with 97% residing in Antigua. St. John's, Antigua, is the country's capital, major city, and largest port. Codrington is Barbuda's largest town.

In 1493, Christopher Columbus surveyed the island of Antigua, which he named for the Church of Santa María La Antigua. Great Britain colonized Antigua in 1632 and Barbuda in 1678. A part of the Federal Colony of the Leeward Islands from 1871, Antigua and Barbuda joined the West Indies Federation in 1958. With the breakup of the federation in 1962, it became one of the West Indies Associated States in 1967. Following a period of internal self-governance, it gained full independence from the United Kingdom on 1 November 1981. Antigua and Barbuda is a member of the Commonwealth and a Commonwealth realm; it is a constitutional monarchy with Charles III as its head of state.

The economy of Antigua and Barbuda is largely dependent on tourism, which accounts for 80% of its GDP. Like other island nations, Antigua and Barbuda is vulnerable to the effects of climate change, such as sea level rise, and increased intensity of extreme weather like hurricanes. These cause coastal erosion, water scarcity, and other challenges.

Antigua and Barbuda offers a citizenship by investment program. The country levies no personal income tax.

 is Spanish for 'ancient' and is Spanish for 'bearded'. The island of Antigua was originally called by the Arawaks and is locally known by that name today; the Caribs possibly called Barbuda . Christopher Columbus, while sailing by in 1493, may have named it , after an icon in the Spanish Seville Cathedral. The "bearded" of Barbuda is thought to refer either to the male inhabitants of the island, or the bearded fig trees present there.

Antigua was first settled by archaic age hunter-gatherer Native Americans called the Ciboney. Carbon dating has established the earliest settlements started around 3100 BC. They were succeeded by the ceramic age pre-Columbian Arawak-speaking Saladoid people who migrated from the lower Orinoco River. They introduced agriculture, raising, among other crops, the famous Antigua Black Pineapple ("Ananas comosus"), corn, sweet potatoes, chiles, guava, tobacco, and cotton. Later on the more bellicose Caribs also settled the island, possibly by force.

Christopher Columbus was the first European to sight the islands in 1493. The Spanish did not colonise Antigua until after a combination of European and African diseases, malnutrition, and slavery eventually extirpated most of the native population; smallpox was probably the greatest killer.

The English settled on Antigua in 1632; Christopher Codrington settled on Barbuda in 1685. Tobacco and then sugar was grown, worked by a large population of slaves transported from West Africa, who soon came to vastly outnumber the European settlers.

The English maintained control of the islands, repulsing an attempted French attack in 1666. The brutal conditions endured by the slaves led to revolts in 1701 and 1729 and a planned revolt in 1736, the last led by Prince Klaas, though it was discovered before it began and the ringleaders were executed. Slavery was abolished in the British Empire in 1833, affecting the economy. This was exacerbated by natural disasters such as the 1843 earthquake and the 1847 hurricane. Mining occurred on the isle of Redonda, however, this ceased in 1929 and the island has since remained uninhabited.

Part of the Leeward Islands colony, Antigua and Barbuda became part of the short-lived West Indies Federation from 1958 to 1962. Antigua and Barbuda subsequently became an associated state of the United Kingdom with full internal autonomy on 27 February 1967. The 1970s were dominated by discussions as to the islands' future and the rivalry between Vere Bird of the Antigua and Barbuda Labour Party (ABLP) (Premier from 1967 to 1971 and 1976 to 1981) and the Progressive Labour Movement (PLM) of George Walter (Premier 1971–1976). Eventually, Antigua and Barbuda gained full independence on 1 November 1981; Vere Bird became prime minister of the new country. The country opted to remain within the Commonwealth, retaining Queen Elizabeth as head of state, with the first governor, Sir Wilfred Jacobs, as governor-general. Succeeding Sir Wilfred Jacobs were Sir James Carlisle (June 10, 1993 – June 30, 2007), Dame Louise Lake-Tack (July 17, 2007 – August 14, 2014.), and the present governor, Sir Rodney Williams: (August 14, 2014 – present).

The first two decades of Antigua's independence were dominated politically by the Bird family and the ABLP, with Vere Bird ruling from 1981 to 1994, followed by his son Lester Bird from 1994 to 2004. Though providing a degree of political stability, and boosting tourism to the country, the Bird governments were frequently accused of corruption, cronyism and financial malfeasance. Vere Bird Jr., the elder son, was forced to leave the cabinet in 1990 following a scandal in which he was accused of smuggling Israeli weapons to Colombian drug-traffickers. Another son, Ivor Bird, was convicted of selling cocaine in 1995.

In 1995, Hurricane Luis caused severe damage on Barbuda.

The ABLP's dominance of Antiguan politics ended with the 2004 Antiguan general election, which was won by Winston Baldwin Spencer's United Progressive Party (UPP). Winston Baldwin Spencer was Prime Minister of Antigua and Barbuda from 2004 to 2014. However the UPP lost the 2014 Antiguan general election, with the ABLP returning to power under Gaston Browne. ABLP won 15 of the 17 seats in the 2018 snap election under the leadership of incumbent Prime Minister Gaston Browne.

In 2016, Nelson's Dockyard was designated as a UNESCO World Heritage Site.

Most of Barbuda was devastated in early September 2017 by Hurricane Irma, which brought winds with speeds reaching 295 km/h (185 mph). The storm damaged or destroyed 95% of the island's buildings and infrastructure, leaving Barbuda "barely habitable" according to Prime Minister Gaston Browne. Nearly everyone on the island was evacuated to Antigua.
Amidst the following rebuilding efforts on Barbuda that were estimated to cost at least $100 million, the government announced plans to revoke a century-old law of communal land ownership by allowing residents to buy land; a move that has been criticised as promoting "disaster capitalism".

Limestone formations, rather than volcanic activity, have had the most impact on the topography of both Antigua and Barbuda, which are both relatively low-lying islands. Boggy Peak, also known as Mt. Obama from 2008 to 2016, is the highest point on both Antigua and Barbuda. It is the remnant of a volcanic crater and rises a total of 402 meters. Boggy Peak is located in the southwest of Antigua (1,319 feet).

Both of these islands have very irregularly shaped coastlines that are dotted with beaches, lagoons, and natural harbors. There are reefs and shoals that surround the islands on all sides. Because of the low amount of rainfall, there are not many streams. On neither of these islands can sufficient quantities of fresh groundwater be found.

Redonda is a small, uninhabited island located about 40 kilometers (25 miles) to the south-west of Antigua. Redonda is a rocky island.

The most populous cities in Antigua and Barbuda are mostly on Antigua, being Saint John's, All Saints, Piggotts, and Liberta. The most populous city on Barbuda is Codrington. It is estimated that 25% of the population lives in an urban area, which is much lower than the international average of 55%.

Antigua and Barbuda consists mostly of its two namesake islands, Antigua, and Barbuda. Other than that, Antigua and Barbuda's biggest islands are Guiana Island and Long Island off the coast of Antigua, and Redonda island, which is far from both of the main islands.

Rainfall averages per year, with the amount varying widely from season to season. In general the wettest period is between September and November. The islands generally experience low humidity and recurrent droughts. Temperatures average , with a range from to in the winter to from to in the summer and autumn. The coolest period is between December and February.

Hurricanes strike on an average of once a year, including the powerful Category 5 Hurricane Irma, on 6 September 2017, which damaged 95% of the structures on Barbuda. Some 1,800 people were evacuated to Antigua.

Officials quoted by "Time" indicated that over $100 million would be required to rebuild homes and infrastructure. Philmore Mullin, Director of Barbuda's National Office of Disaster Services, said that "all critical infrastructure and utilities are non-existent – food supply, medicine, shelter, electricity, water, communications, waste management". He summarised the situation as follows: "Public utilities need to be rebuilt in their entirety... It is optimistic to think anything can be rebuilt in six months ... In my 25 years in disaster management, I have never seen something like this."

Antigua has a population of , mostly made up of people of West African, British, and Portuguese descent. The ethnic distribution consists of 91% Black, 4.4% mixed race, 1.7% White, and 2.9% other (primarily East Indian). Most Whites are of British descent. Christian Levantine Arabs and a small number of East Asians and Sephardic Jews make up the remainder of the population.

An increasingly large percentage of the population lives abroad, most notably in the United Kingdom (Antiguan Britons), the United States and Canada. A minority of Antiguan residents are immigrants from other countries, particularly from Dominica, Guyana and Jamaica, and, increasingly, from the Dominican Republic, St. Vincent and the Grenadines and Nigeria. An estimated 4,500 American citizens also make their home in Antigua and Barbuda, making their numbers one of the largest American populations in the English-speaking Eastern Caribbean. 68.47% of the population was born in Antigua and Barbuda.

The language most commonly used in business is English. There is a noticeable distinction between the Antiguan accent and the Barbudan one.

When compared to Antiguan Creole, Standard English was the language of choice in the years leading up to Antigua and Barbuda's attainment of their independence. The Antiguan Creole language is looked down upon by the upper and middle classes in general. The Antiguan Creole language is discouraged from use in the educational system, and instruction is carried out in Standard (British) English instead.

A significant number of the words that are utilized in the Antiguan dialect are derived from both the British and African languages. This is readily apparent in phrases such as "Innit?" which literally translates to "Isn't it?" Many common island proverbs can be traced back to Africa, such as the pidgin language.

Approximately 10,000 people are able to speak in Spanish.

A majority (77%) of Antiguans are Christians, with the Anglicans (17.6%) being the largest single denomination. Other Christian denominations present are Seventh-day Adventist Church (12.4%), Pentecostalism (12.2%), Moravian Church (8.3%), Roman Catholics
(8.2%), Methodist Church (5.6%), Wesleyan Holiness Church (4.5%), Church of God (4.1%), Baptists (3.6%), Mormonism (<1.0%), as well as Jehovah's Witnesses.

The politics of Antigua and Barbuda take place within a framework of a unitary, parliamentary, representative democratic monarchy, in which the head of state is the monarch who appoints the governor-general as vice-regal representative. Charles III is the present King of Antigua and Barbuda, having served in that position since the death of his mother, Elizabeth II. She had been the queen since the islands' independence from the United Kingdom in 1981. The King is currently represented by Governor-General Sir Rodney Williams. A council of ministers is appointed by the governor-general on the advice of the prime minister, currently Gaston Browne (2014–). The prime minister is the head of government.

Executive power is exercised by the government while legislative power is vested in both the government and the two chambers of Parliament. The bicameral Parliament consists of the Senate (17 members appointed by members of the government and the opposition party, and approved by the governor-general), and the House of Representatives (17 members elected by first past the post) to serve five-year terms.

The current Leader of His Majesty's Loyal Opposition is Jamale Pringle.

There has been recent development in the republicanism movement in Antigua and Barbuda, following Barbados becoming a republic in 2021 and following the death of Elizabeth II in 2022, with an opinion poll showing majority support for the change.

The last election was held on 21 March 2018. The Antigua Barbuda Labour Party (ABLP) led by Prime Minister Gaston Browne won 15 of the 17 seats in the House of Representatives. The previous election was on 12 June 2014, during which the Antigua Labour Party won 14 seats, and the United Progressive Party 3 seats.

Since 1951, elections have been won by the populist Antigua Labour Party. However, in the Antigua and Barbuda legislative election of 2004 saw the defeat of the longest-serving elected government in the Caribbean.

Vere Bird was prime minister from 1981 to 1994 and chief minister of Antigua from 1960 to 1981, except for the 1971–1976 period when the Progressive Labour Movement (PLM) defeated his party. Bird, the nation's first prime minister, is credited with having brought Antigua and Barbuda and the Caribbean into a new era of independence. Prime Minister Lester Bryant Bird succeeded the elder Bird in 1994.

Gaston Browne defeated his predecessor Lester Bryant Bird at the Antigua Labour Party's biennial convention in November 2012 held to elect a political leader and other officers. The party then altered its name from the Antigua Labour Party (ALP) to the Antigua and Barbuda Labour Party (ABLP). This was done to officially include the party's presence on the sister island of Barbuda in its organisation, the only political party on the mainland to have a physical branch in Barbuda.

The Eastern Caribbean Supreme Court is the highest court in the region's judicial system (based in Saint Lucia; one judge of the Supreme Court is a resident of the islands and presides over the High Court of Justice). The Caribbean Court of Justice counts Antigua as one of its member states. Its highest court of appeal is the Judicial Committee of the Privy Council, which acts in that capacity.

Antigua and Barbuda is a member of the United Nations, the Bolivarian Alliance for the Americas, the Commonwealth of Nations, the Caribbean Community, the Organization of Eastern Caribbean States, the Organization of American States, the World Trade Organization and the Eastern Caribbean's Regional Security System.

Antigua and Barbuda is also a member of the International Criminal Court (with a Bilateral Immunity Agreement of Protection for the US military as covered under Article 98 of the Rome Statute).

In 2013, Antigua and Barbuda called for reparations for slavery at the United Nations. Prime Minister Baldwin Spencer said "We have recently seen a number of leaders apologising", and that they should now "match their words with concrete and material benefits."

About 260 people are currently serving in the Antigua and Barbuda Defense Force in a variety of capacities. These personnel are distributed across the line infantry regiment, the service and support unit, the air force, and the coast guard. In addition there is the Antigua and Barbuda Cadet Corps, which is made up of two hundred young people between the ages of 12 and 18. The Defence Board is in charge of directing the activities of the armed forces of the nation. The National Security Council and the Financial Intelligence Unit are the two intelligence agencies that Antigua and Barbuda have at their disposal. Camp Blizzard serves as the administrative center for the Defense Force.

In 2018, Antigua and Barbuda signed the UN treaty on the Prohibition of Nuclear Weapons.

Antigua and Barbuda is divided into six parishes and two dependencies:

Although they are referred to as dependencies, both Barbuda and Redonda are actually integral parts of the state and can be thought of as administrative divisions. Simply put, "dependency" is just a title. The Redonda is a second-level administrative division that is part of the Saint John Parish's District "A." Barbuda is a local administrative division on the same level as Antigua and Barbuda, and its council is the name of its local governing body. In the year 2023, there have been discussions regarding the possibility of extending governmental authority to Antigua's parishes.

There are currently sixty of what are known as major divisions on the islands of Antigua and Barbuda. This administrative tier is known as the second level.

Local government in the country of Antigua and Barbuda is only present on the island of Barbuda at the present time; however, there is legislation in place for a system of village councils on the island of Antigua; however, village councils have not been active since the 1940s and 1950s.

As of July 2022, Same-sex sexual activity is legal in Antigua and Barbuda.

Tourism dominates the economy, accounting for more than half of the gross domestic product (GDP). As a destination for the most affluent travelers, Antigua is well known for its extensive collection of five-star resorts. However, weaker tourist activity in lower and middle market segments since the beginning of the year 2000 has slowed the economy and put the government into a tight fiscal corner. Antigua and Barbuda has enacted policies to attract high-net-worth citizens and residents, such as enacting a 0% personal income tax rate in 2019.

The provision of investment banking and financial services also constitutes a significant portion of the economy. Major international financial institutions such as the Royal Bank of Canada (RBC) and Scotiabank both maintain offices in Antigua. PriceWaterhouseCoopers, Pannell Kerr Forster, and KPMG are some of the other companies in the financial services industry that have offices in Antigua. The United States Securities and Exchange Commission has leveled allegations against the Antigua-based Stanford International Bank, which is owned by the Texas billionaire Allen Stanford, of orchestrating a massive fraud that may have resulted in the theft of approximately $8 billion from investors.

The nation, which consists of two islands, directs the majority of its agricultural production toward the markets that are found within the nation. This is done despite the fact that the nation has a limited water supply and a shortage of laborers as a result of the higher wages offered in the tourism and construction industries.

Manufacturing comprises 2% of GDP and is made up of enclave-type assembly for export, the major products being bedding, handicrafts, and electronic components. Prospects for economic growth in the medium term will continue to depend on income growth in the industrialised world, especially in the United States, from which about one-third to one-half of all tourists come.

Access to biocapacity is lower than world average. In 2016, Antigua and Barbuda had 0.8 global hectares of biocapacity per person within its territory, much less than the world average of 1.6 global hectares per person. In 2016, Antigua and Barbuda used 4.3 global hectares of biocapacity per person – their ecological footprint of consumption. This means they use more biocapacity than Antigua and Barbuda contains. As a result, Antigua and Barbuda are running a biocapacity deficit.

The Citizenship by Investment Unit (CIU) is the government authority responsible for processing all applications for Agent's Licenses as well as all applications for Citizenship by Investment made by applicants and their family members. This unit was established by the Prime Minister and is known as the Citizenship by Investment Unit.

The transportation networks of Antigua and Barbuda are made up of both publicly operated and privately managed services. The roads in the countryside are paved, and their paths are winding and gradual in their ascents and descents; they connect parishes to villages and communities. Cars are driven on the left side of the road. Antigua and Barbuda has a speed limit of 40 miles per hour, and there are traffic signs posted along the main roads that make it easier to commute. Additionally, GPS coordinates have been posted throughout the country, which has made the process of navigation more manageable.

On the yellow license plates of public transportation vehicles, the letters "BUS" indicate that the vehicle is a bus, and the letters "TX" indicate that the vehicle is a taxi. Taxi services are subject to government regulation, which results in the establishment of flat rates rather than the use of meters. It is required that taxi drivers keep a copy of the rates posted inside the cab at all times. Taxis are not hard to come by on Antigua, particularly at the airport and major hotels. The role of tour guide is one that is frequently taken on by taxi drivers.

On the island of Antigua, buses run continuously throughout the day from 5:30 a.m. until 6:00 p.m., connecting the capital city of St. John's with a number of the surrounding villages. On the other hand, buses do not make stops at the airport or in the tourist area to the north. Although the timing of the bus' departure is frequently up to the discretion of the driver, most buses operate according to a predetermined schedule. The routes that most buses take are typically displayed in the front windows of the vehicles, which are typically private minivans with seating for approximately 15 passengers each. Both the East Bus Station on Independence Avenue close to the Botanical Gardens and the Market Street Bus Station close to the Central Market are the two bus stations that serve the city of St. John's. There are also a number of bus companies operating on the island of Barbuda.

The culture is primarily influenced by the traditions of West Africa as well as those of the United Kingdom.

The most popular sport in the country is cricket. Football, boat racing, and surfing are three additional popular forms of athletic competition. (Antigua Sailing Week attracts locals and visitors from all over the world).

Since the majority of Antiguans and Barbudans are descended from West Africans who were brought to the islands as slaves by Europeans, the musical traditions of Antigua and Barbuda are predominately of African origin and have only been marginally influenced by European musical traditions.

The island nation of Antigua and Barbuda can be found in the Caribbean's Lesser Antilles chain of islands. It is a second home for many of the popular music genres that are popular throughout the Caribbean, including calypso, soca, steeldrum, zouk, and reggae, and it has produced stars in these genres. Steeldrum and calypso are two musical styles that were brought to Antigua from Trinidad and Tobago, and they are the two that have had the most significant impact on the development of modern Antiguan popular music.

Other than this, very little to no research has been done on the musical history of Antigua and Barbuda. Consequently, a significant amount of the knowledge on the subject comes from novels, essays, and other types of secondary sources.

Although on some islands Carnival may be used to celebrate the beginning of Lent, the national Carnival held every August is held to commemorate the abolition of slavery in the British West Indies. The national Carnival is held in August. The festive pageants, shows, and competitions, along with the other events that take place, are a major draw for tourists.

Antigua and Barbuda cuisine is a term used to describe the culinary traditions of the islands of Antigua and Barbuda in the Caribbean. Fungie, pronounced "foon-jee", and pepperpot are the country's official dish and dish of pride. Cornmeal is the main ingredient in fungie, which is a dish that is very similar to the Italian dish polenta. Other popular dishes from this region include ducana, saltfish, seasoned rice, and lobster (from Barbuda). In addition, there are sweets that are made locally, such as peanut brittle, sugar cake, fudge, raspberry and tamarind stew, and other similar dishes.

Despite the fact that these foods are native to Antigua and Barbuda as well as to a number of other Caribbean nations, the diet of the locals has become increasingly diverse and now also includes traditional dishes from Jamaica and Trinidad, such as jerk meats and roti, as well as specialties from a number of other Caribbean nations.

Saltfish, eggs, eggplant (also referred to as troba), lettuce, and other vegetables are typically served for breakfast. Lunches typically consist of a starch, such as rice, macaroni, or pasta, with vegetables or salad, an entree (such as fish, chicken, pork, or beef), and a side dish, such as macaroni pie, scalloped potatoes, or plantains. Dinners typically consist of a protein, such as fish, chicken, pork, or beef. On Sundays, the majority of people in the country attend religious services, and then they return home to prepare a wide variety of meals for their families. Due to the fact that most people are off work on Sundays, dinner is typically served earlier in the day (around 2:00 pm). Dinners might consist of pork, chicken baked in the oven, stewed lamb, or turkey, served with rice (prepared in a variety of ways), macaroni pie, salads, and a local beverage. Dessert options include ice cream and cake, apple pie (or mango or pineapple pie when those fruits are in season), gelatin, and cake. The soft, buttery loaf of bread known as Antiguan butter bread does not require any additional butter to be added once it has been baked. This dish is another mainstay of Antiguan cuisine. Breakfast and other meals throughout the day often consist of fresh-baked butter bread and cheese for the community's residents. Throughout the city of Antigua, there are a great number of homes that have small bakeries built onto them. These bakeries sell freshly baked loaves, and locals can go to these bakeries to buy them. They are served alongside cheese, sardines or a bright red sausage that residents of the area occasionally refer to as salami, in addition to a great deal of other foods. In addition, the majority of meals feature something known as "provisions", which is typically a root vegetable or starch such as potatoes, yams, sweet potatoes, or eddo. During Carnival, a popular snack is souse, which is a type of soup that is very spicy and is made with pig feet, knuckles, and tails in addition to many onions. This soup is sold by vendors on the side of the road. Black pudding, also referred to as blood sausage, is a well-seasoned sausage that is made with rice, meat, and blood that is also enjoyed by locals in Antigua. On improvised grills, locals in the countryside sell freshly picked corn that has been roasted, typically while still in the husk. The Antiguan pineapple is typically quite succulent and sugary, despite its diminutive size. The entire island is covered with numerous small pineapple plantations.

The following are some examples of local beverages: mauby, seamoss, tamarind juice, raspberry juice, mango juice, lemonade, coconut milk, hibiscus juice, ginger beer, passion fruit juice, guava juice, soursop juice, and ginger beer, which is a soft drink. Beer, malts, and rums are some of the alcoholic beverages that can be found here. Many of these drinks are produced locally, such as the award-winning English Harbour Rum and the Wadadli beer, which takes its name from the island's former name. A significant number of residents in the area consume bottled sodas, which they refer to as sweet drink. Punch is a flavor that is enjoyed by many. In addition to Red Stripe beer, Guinness stout, Heineken beer, and Malta, the locals like to drink Red Malta. Ponche Kuba Cream Liqueur is a special celebratory alcoholic drink that is very popular in Antigua during the Christmas holiday season. This beverage has a brown color, has a thick and creamy consistency, is extremely sweet, and contains a high percentage of alcohol.

The "Antigua Daily Observer, the Antigua News Room, and The Antiguan Times" are the names of the country's three newspapers. The Antigua Observer is the only newspaper that is published every day in printed form.

It is possible to watch the local television channel, ABS TV 10. (it is the only station that shows exclusively local programs). There are also a number of radio stations that broadcast regionally and locally. Some of these stations include V2C-AM 620, ZDK-AM 1100, VYBZ-FM 92.9, ZDK-FM 97.1, Observer Radio 91.1 FM, DNECA Radio 90.1 FM, Second Advent Radio 101.5 FM, Abundant Life Radio 103.9 FM, Crusader Radio 107.3 FM, Nice FM 104.3, Pointe FM 99.1, and WTP 93.5FM.



In 2016, Antigua Naval Dockyard and Related Archaeological Sites have been inscribed in the UNESCO World Heritage List.

Cricket is the most popular sport within the islands. With Sir Isaac Vivian Alexander Richards who represented the West Indies cricket team between 1974 and 1991, Antigua had one of the world's most famous batsmen ever. The Antigua and Barbuda national cricket team represented the country at the 1998 Commonwealth Games, but Antiguan cricketers otherwise play for the Leeward Islands cricket team in domestic matches and the West Indies cricket team internationally. The 2007 Cricket World Cup was hosted in the West Indies from 11 March to 28 April 2007. Antigua hosted eight matches at the Sir Vivian Richards Stadium, which was completed on 11 February 2007 and can hold up to 20,000 people.<br>
Antigua is a Host of Stanford Twenty20 – Twenty20 Cricket, a version started by Allen Stanford in 2006 as a regional cricket game with almost all Caribbean islands taking part. From 15 January to 5 February 2022, the Sir Vivian Richards Stadium was one of the venues for the 2022 ICC Under-19 Cricket World Cup.<br>Rugby and Netball are popular as well.

Association football, or soccer, is also a very popular sport. Antigua has a national football team which entered World Cup qualification for the 1974 tournament for 1986 and beyond. A professional team was formed in 2011, Antigua Barracuda FC, which played in the USL Pro, a lower professional league in the US. The nation's team had a major achievement in 2012, getting out of its preliminary group for the 2014 World Cup, notably due to a victory over powerful Haiti. In its first game in the next CONCACAF group play on 8 June 2012 in Tampa, FL, Antigua and Barbuda, comprising 17 Barracuda players and 7 from the lower English professional leagues, scored a goal against the United States. However, the team lost 3:1 to the US.

Daniel Bailey had become the first Antiguan to reach a world indoor final, where he won a bronze medal at the 2010 IAAF World Indoor Championships. He was also the first Antiguan to make a 100m final at the 2009 World Championships in Athletics, and the first Antiguan to run under 10 seconds over 100m.

Brendan Christian won a gold medal in the 200m and a bronze medal in the 100m at the 2007 Pan American Games. James Grayman won a bronze medal at the same games in the men's High Jump.

Miguel Francis is the first Antiguan to run sub 20 seconds in the 200m.

Heather Samuel won a bronze medal at the 1995 Pan American Games over 100m.

400m Hurdles Olympian Gold Medalist Rai Benjamin previously represented Antigua and Barbuda before representing the United States. His Silver medal run at the 2020 Olympic Games made him the second-fastest person in history over 400m Hurdles with a time of 46.17.

The frigatebird is the country's official national bird, and the bucida buceras (Whitewood tree) is the official national tree.

Clare Waight Keller designed Meghan Markle's wedding veil, which featured the distinctive flora of each Commonwealth nation. To represent Antigua and Barbuda, "Agave karatto" was included in the veil.

The European fallow deer, or Dama dama, is the country's official mammal, despite the fact that it is a non-native species.

In 1992, the government held a contest to design a new national dress for the country, and the winner of the competition was the artist Heather Doram.



Azincourt

Azincourt ( ; ) is a commune in the Pas-de-Calais department in northern France. It is situated north-west of Saint-Pol-sur-Ternoise on the D71 road between Hesdin and Fruges. It is often erroneously known as Agincourt ( ) in English, as the village had never been called as such throughout its history, however, there is a village that is named "Agincourt", located in the Meurthe-et-Moselle department in Eastern France. 

The Late Medieval Battle of Azincourt between the English and the French took place in the commune in 1415.

The name is attested as "Aisincurt" in 1175, derived from a Germanic masculine name Aizo, Aizino and the early Northern French word "curt" (which meant a farm with a courtyard; derived from the Late Latin "cortem"). The name has no etymological link with Agincourt, Meurthe-et-Moselle (attested as "Egincourt" 875), which is derived separately from another Germanic male name "*Ingin-".

Azincourt is known for being near the site of the battle fought on 25 October 1415 in which the army led by King Henry V of England defeated the forces led by Charles d'Albret on behalf of Charles VI of France, which has gone down in history as the Battle of Agincourt. According to M. Forrest, the French knights were so encumbered by their armour that they were exhausted even before the start of the battle.

After he became king in 1509, Henry VIII is purported to have commissioned an English translation of a Life of Henry V so that he could emulate him, on the grounds that he thought that launching a campaign against France would help him to impose himself on the European stage. In 1513, Henry VIII crossed the English Channel, stopping by at Azincourt.

The battle, as was the tradition, was named after a nearby castle called Azincourt. The castle has since disappeared and the settlement now known as Azincourt adopted the name in the seventeenth century.

John Cassell wrote in 1857 that "the village of Azincourt itself is now a group of dirty farmhouses and wretched cottages, but where the hottest of the battle raged, between that village and the commune of Tramecourt, there still remains a wood precisely corresponding with the one in which Henry placed his ambush; and there are yet existing the foundations of the castle of Azincourt, from which the king named the field."

The original battlefield museum in the village featured model knights made out of Action Man figures. This has now been replaced by the Centre historique médiéval d'Azincourt (CHM)a more professional museum, conference centre and exhibition space incorporating laser, video, slide shows, audio commentaries, and some interactive elements. The museum building is shaped like a longbow similar to those used at the battle by archers under King Henry.

Since 2004 a large medieval festival organised by the local community, the CHM, The Azincourt Alliance, and various other UK societies commemorating the battle, local history and medieval life, arts and crafts has been held in the village. Prior to this date the festival was held in October, but due to the inclement weather and local heavy clay soil (like the battle) making the festival difficult, it was moved to the last Sunday in July. 
Azincourt is twinned with Middleham, United Kingdom.


