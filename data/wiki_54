Camouflage

Camouflage is the use of any combination of materials, coloration, or illumination for concealment, either by making animals or objects hard to see, or by disguising them as something else. Examples include the leopard's spotted coat, the battledress of a modern soldier, and the leaf-mimic katydid's wings. A third approach, motion dazzle, confuses the observer with a conspicuous pattern, making the object visible but momentarily harder to locate, as well as making general aiming easier. The majority of camouflage methods aim for crypsis, often through a general resemblance to the background, high contrast disruptive coloration, eliminating shadow, and countershading. In the open ocean, where there is no background, the principal methods of camouflage are transparency, silvering, and countershading, while the ability to produce light is among other things used for counter-illumination on the undersides of cephalopods such as squid. Some animals, such as chameleons and octopuses, are capable of actively changing their skin pattern and colours, whether for camouflage or for signalling. It is possible that some plants use camouflage to evade being eaten by herbivores.

Military camouflage was spurred by the increasing range and accuracy of firearms in the 19th century. In particular the replacement of the inaccurate musket with the rifle made personal concealment in battle a survival skill. In the 20th century, military camouflage developed rapidly, especially during the First World War. On land, artists such as André Mare designed camouflage schemes and observation posts disguised as trees. At sea, merchant ships and troop carriers were painted in dazzle patterns that were highly visible, but designed to confuse enemy submarines as to the target's speed, range, and heading. During and after the Second World War, a variety of camouflage schemes were used for aircraft and for ground vehicles in different theatres of war. The use of radar since the mid-20th century has largely made camouflage for fixed-wing military aircraft obsolete.

Non-military use of camouflage includes making cell telephone towers less obtrusive and helping hunters to approach wary game animals. Patterns derived from military camouflage are frequently used in fashion clothing, exploiting their strong designs and sometimes their symbolism. Camouflage themes recur in modern art, and both figuratively and literally in science fiction and works of literature.

In ancient Greece, Aristotle (384–322 BC) commented on the colour-changing abilities, both for camouflage and for signalling, of cephalopods including the octopus, in his "Historia animalium":

Camouflage has been a topic of interest and research in zoology for well over a century. According to Charles Darwin's 1859 theory of natural selection, features such as camouflage evolved by providing individual animals with a reproductive advantage, enabling them to leave more offspring, on average, than other members of the same species. In his "Origin of Species", Darwin wrote:

The English zoologist Edward Bagnall Poulton studied animal coloration, especially camouflage. In his 1890 book "The Colours of Animals", he classified different types such as "special protective resemblance" (where an animal looks like another object), or "general aggressive resemblance" (where a predator blends in with the background, enabling it to approach prey). His experiments showed that swallow-tailed moth pupae were camouflaged to match the backgrounds on which they were reared as larvae. Poulton's "general protective resemblance" was at that time considered to be the main method of camouflage, as when Frank Evers Beddard wrote in 1892 that "tree-frequenting animals are often green in colour. Among vertebrates numerous species of parrots, iguanas, tree-frogs, and the green tree-snake are examples". Beddard did however briefly mention other methods, including the "alluring coloration" of the flower mantis and the possibility of a different mechanism in the orange tip butterfly. He wrote that "the scattered green spots upon the under surface of the wings might have been intended for a rough sketch of the small flowerets of the plant [an umbellifer], so close is their mutual resemblance." He also explained the coloration of sea fish such as the mackerel: "Among pelagic fish it is common to find the upper surface dark-coloured and the lower surface white, so that the animal is inconspicuous when seen either from above or below."

The artist Abbott Handerson Thayer formulated what is sometimes called Thayer's Law, the principle of countershading. However, he overstated the case in the 1909 book "Concealing-Coloration in the Animal Kingdom", arguing that "All patterns and colors whatsoever of all animals that ever preyed or are preyed on are under certain normal circumstances obliterative" (that is, cryptic camouflage), and that "Not one 'mimicry' mark, not one 'warning color'... nor any 'sexually selected' color, exists anywhere in the world where there is not every reason to believe it the very best conceivable device for the concealment of its wearer", and using paintings such as "Peacock in the Woods" (1907) to reinforce his argument. Thayer was roundly mocked for these views by critics including Teddy Roosevelt.

The English zoologist Hugh Cott's 1940 book "Adaptive Coloration in Animals" corrected Thayer's errors, sometimes sharply: "Thus we find Thayer straining the theory to a fantastic extreme in an endeavour to make it cover almost every type of coloration in the animal kingdom." Cott built on Thayer's discoveries, developing a comprehensive view of camouflage based on "maximum disruptive contrast", countershading and hundreds of examples. The book explained how disruptive camouflage worked, using streaks of boldly contrasting colour, paradoxically making objects less visible by breaking up their outlines. While Cott was more systematic and balanced in his view than Thayer, and did include some experimental evidence on the effectiveness of camouflage, his 500-page textbook was, like Thayer's, mainly a natural history narrative which illustrated theories with examples.

Experimental evidence that camouflage helps prey avoid being detected by predators was first provided in 2016, when ground-nesting birds (plovers and coursers) were shown to survive according to how well their egg contrast matched the local environment.

As there is a lack of evidence for camouflage in the fossil record, studying the evolution of camouflage strategies is very difficult. Furthermore, camouflage traits must be both adaptable (provide a fitness gain in a given environment) and heritable (in other words, the trait must undergo positive selection). Thus, studying the evolution of camouflage strategies requires an understanding of the genetic components and various ecological pressures that drive crypsis.

Camouflage is a soft-tissue feature that is rarely preserved in the fossil record, but rare fossilised skin samples from the Cretaceous period show that some marine reptiles were countershaded. The skins, pigmented with dark-coloured eumelanin, reveal that both leatherback turtles and mosasaurs had dark backs and light bellies. There is fossil evidence of camouflaged insects going back over 100 million years, for example lacewings larvae that stick debris all over their bodies much as their modern descendants do, hiding them from their prey. Dinosaurs appear to have been camouflaged, as a 120 million year old fossil of a "Psittacosaurus" has been preserved with countershading.

Camouflage does not have a single genetic origin. However, studying the genetic components of camouflage in specific organisms illuminates the various ways that crypsis can evolve among lineages.

Many cephalopods have the ability to actively camouflage themselves, controlling crypsis through neural activity. For example, the genome of the common cuttlefish includes 16 copies of the reflectin gene, which grants the organism remarkable control over coloration and iridescence. The reflectin gene is thought to have originated through transposition from symbiotic "Aliivibrio fischeri" bacteria, which provide bioluminescence to its hosts. While not all cephalopods use active camouflage, ancient cephalopods may have inherited the gene horizontally from symbiotic "A. fischeri", with divergence occurred through subsequent gene duplication (such as in the case of "Sepia officinalis") or gene loss (as with cephalopods with no active camouflage capabilities). This is unique as an instance of camouflage arising as an instance of horizontal gene transfer from an endosymbiont. However, other methods of horizontal gene transfer are common in the evolution of camouflage strategies in other lineages. Peppered moths and walking stick insects both have camouflage-related genes that stem from transposition events.

The Agouti genes are orthologous genes involved in camouflage across many lineages. They produce yellow and red coloration (phaeomelanin), and work in competition with other genes that produce black (melanin) and brown (eumelanin) colours. In eastern deer mice, over a period of about 8000 years the single agouti gene developed 9 mutations that each made expression of yellow fur stronger under natural selection, and largely eliminated melanin-coding black fur coloration. On the other hand, all black domesticated cats have deletions of the agouti gene that prevent its expression, meaning no yellow or red color is produced. The evolution, history and widespread scope of the agouti gene shows that different organisms often rely on orthologous or even identical genes to develop a variety of camouflage strategies.

While camouflage can increase an organism's fitness, it has genetic and energetic costs. There is a trade-off between detectability and mobility. Species camouflaged to fit a specific microhabitat are less likely to be detected when in that microhabitat, but must spend energy to reach, and sometimes to remain in, such areas. Outside the microhabitat, the organism has a higher chance of detection. Generalized camouflage allows species to avoid predation over a wide range of habitat backgrounds, but is less effective. The development of generalized or specialized camouflage strategies is highly dependent on the biotic and abiotic composition of the surrounding environment.

There are many examples of the tradeoffs between specific and general cryptic patterning. "Phestilla melanocrachia", a species of nudibranch that feeds on stony coral, utilizes specific cryptic patterning in reef ecosystems. The nudibranch syphons pigments from the consumed coral into the epidermis, adopting the same shade as the consumed coral. This allows the nudibranch to change colour (mostly between black and orange) depending on the coral system that it inhabits. However, "P. melanocrachia" can only feed and lay eggs on the branches of host-coral, "Platygyra carnosa", which limits the geographical range and efficacy in nudibranch nutritional crypsis. Furthermore, the nudibranch colour change is not immediate, and switching between coral hosts when in search for new food or shelter can be costly.

The costs associated with distractive or disruptive crypsis are more complex than the costs associated with background matching. Disruptive patterns distort the body outline, making it harder to precisely identify and locate. However, disruptive patterns result in higher predation. Disruptive patterns that specifically involve visible symmetry (such as in some butterflies) reduce survivability and increase predation. Some researchers argue that because wing-shape and color pattern are genetically linked, it is genetically costly to develop asymmetric wing colorations that would enhance the efficacy of disruptive cryptic patterning. Symmetry does not carry a high survival cost for butterflies and moths that their predators views from above on a homogeneous background, such as the bark of a tree. On the other hand, natural selection drives species with variable backgrounds and habitats to move symmetrical patterns away from the centre of the wing and body, disrupting their predators' symmetry recognition.

Camouflage can be achieved by different methods, described below. Most of the methods help to hide against a background; but mimesis and motion dazzle protect without hiding. Methods may be applied on their own or in combination. Many mechanisms are visual, but some research has explored the use of techniques against olfactory (scent) and acoustic (sound) detection. Methods may also apply to military equipment.

Some animals' colours and patterns resemble a particular natural background. This is an important component of camouflage in all environments. For instance, tree-dwelling parakeets are mainly green; woodcocks of the forest floor are brown and speckled; reedbed bitterns are streaked brown and buff; in each case the animal's coloration matches the hues of its habitat. Similarly, desert animals are almost all desert coloured in tones of sand, buff, ochre, and brownish grey, whether they are mammals like the gerbil or fennec fox, birds such as the desert lark or sandgrouse, or reptiles like the skink or horned viper. Military uniforms, too, generally resemble their backgrounds; for example khaki uniforms are a muddy or dusty colour, originally chosen for service in South Asia. Many moths show industrial melanism, including the peppered moth which has coloration that blends in with tree bark. The coloration of these insects evolved between 1860 and 1940 to match the changing colour of the tree trunks on which they rest, from pale and mottled to almost black in polluted areas. This is taken by zoologists as evidence that camouflage is influenced by natural selection, as well as demonstrating that it changes where necessary to resemble the local background.

Disruptive patterns use strongly contrasting, non-repeating markings such as spots or stripes to break up the outlines of an animal or military vehicle, or to conceal telltale features, especially by masking the eyes, as in the common frog. Disruptive patterns may use more than one method to defeat visual systems such as edge detection. Predators like the leopard use disruptive camouflage to help them approach prey, while potential prey use it to avoid detection by predators. Disruptive patterning is common in military usage, both for uniforms and for military vehicles. Disruptive patterning, however, does not always achieve crypsis on its own, as an animal or a military target may be given away by factors like shape, shine, and shadow.

The presence of bold skin markings does not in itself prove that an animal relies on camouflage, as that depends on its behaviour. For example, although giraffes have a high contrast pattern that could be disruptive coloration, the adults are very conspicuous when in the open. Some authors have argued that adult giraffes are cryptic, since when standing among trees and bushes they are hard to see at even a few metres' distance. However, adult giraffes move about to gain the best view of an approaching predator, relying on their size and ability to defend themselves, even from lions, rather than on camouflage. A different explanation is implied by young giraffes being far more vulnerable to predation than adults. More than half of all giraffe calves die within a year, and giraffe mothers hide their newly born calves, which spend much of the time lying down in cover while their mothers are away feeding. The mothers return once a day to feed their calves with milk. Since the presence of a mother nearby does not affect survival, it is argued that these juvenile giraffes must be very well camouflaged; this is supported by coat markings being strongly inherited.

The possibility of camouflage in plants has been little studied until the late 20th century. Leaf variegation with white spots may serve as camouflage in forest understory plants, where there is a dappled background; leaf mottling is correlated with closed habitats. Disruptive camouflage would have a clear evolutionary advantage in plants: they would tend to escape from being eaten by herbivores. Another possibility is that some plants have leaves differently coloured on upper and lower surfaces or on parts such as veins and stalks to make green-camouflaged insects conspicuous, and thus benefit the plants by favouring the removal of herbivores by carnivores. These hypotheses are testable.

Some animals, such as the horned lizards of North America, have evolved elaborate measures to eliminate shadow. Their bodies are flattened, with the sides thinning to an edge; the animals habitually press their bodies to the ground; and their sides are fringed with white scales which effectively hide and disrupt any remaining areas of shadow there may be under the edge of the body. The theory that the body shape of the horned lizards which live in open desert is adapted to minimise shadow is supported by the one species which lacks fringe scales, the roundtail horned lizard, which lives in rocky areas and resembles a rock. When this species is threatened, it makes itself look as much like a rock as possible by curving its back, emphasizing its three-dimensional shape. Some species of butterflies, such as the speckled wood, "Pararge aegeria", minimise their shadows when perched by closing the wings over their backs, aligning their bodies with the sun, and tilting to one side towards the sun, so that the shadow becomes a thin inconspicuous line rather than a broad patch. Similarly, some ground-nesting birds, including the European nightjar, select a resting position facing the sun. Eliminating shadow was identified as a principle of military camouflage during the Second World War.

Many prey animals have conspicuous high-contrast markings which paradoxically attract the predator's gaze. These distractive markings may serve as camouflage by distracting the predator's attention from recognising the prey as a whole, for example by keeping the predator from identifying the prey's outline. Experimentally, search times for blue tits increased when artificial prey had distractive markings.

Some animals actively seek to hide by decorating themselves with materials such as twigs, sand, or pieces of shell from their environment, to break up their outlines, to conceal the features of their bodies, and to match their backgrounds. For example, a caddisfly larva builds a decorated case and lives almost entirely inside it; a decorator crab covers its back with seaweed, sponges, and stones. The nymph of the predatory masked bug uses its hind legs and a 'tarsal fan' to decorate its body with sand or dust. There are two layers of bristles (trichomes) over the body. On these, the nymph spreads an inner layer of fine particles and an outer layer of coarser particles. The camouflage may conceal the bug from both predators and prey.

Similar principles can be applied for military purposes, for instance when a sniper wears a ghillie suit designed to be further camouflaged by decoration with materials such as tufts of grass from the sniper's immediate environment. Such suits were used as early as 1916, the British army having adopted "coats of motley hue and stripes of paint" for snipers. Cott takes the example of the larva of the blotched emerald moth, which fixes a screen of fragments of leaves to its specially hooked bristles, to argue that military camouflage uses the same method, pointing out that the "device is ... essentially the same as one widely practised during the Great War for the concealment, not of caterpillars, but of caterpillar-tractors, [gun] battery positions, observation posts and so forth."

Movement catches the eye of prey animals on the lookout for predators, and of predators hunting for prey. Most methods of crypsis therefore also require suitable cryptic behaviour, such as lying down and keeping still to avoid being detected, or in the case of stalking predators such as the tiger, moving with extreme stealth, both slowly and quietly, watching its prey for any sign they are aware of its presence. As an example of the combination of behaviours and other methods of crypsis involved, young giraffes seek cover, lie down, and keep still, often for hours until their mothers return; their skin pattern blends with the pattern of the vegetation, while the chosen cover and lying position together hide the animals' shadows. The flat-tail horned lizard similarly relies on a combination of methods: it is adapted to lie flat in the open desert, relying on stillness, its cryptic coloration, and concealment of its shadow to avoid being noticed by predators. In the ocean, the leafy sea dragon sways mimetically, like the seaweeds amongst which it rests, as if rippled by wind or water currents. Swaying is seen also in some insects, like Macleay's spectre stick insect, "Extatosoma tiaratum". The behaviour may be motion crypsis, preventing detection, or motion masquerade, promoting misclassification (as something other than prey), or a combination of the two.

Most forms of camouflage are ineffective when the camouflaged animal or object moves, because the motion is easily seen by the observing predator, prey or enemy. However, insects such as hoverflies and dragonflies use motion camouflage: the hoverflies to approach possible mates, and the dragonflies to approach rivals when defending territories. Motion camouflage is achieved by moving so as to stay on a straight line between the target and a fixed point in the landscape; the pursuer thus appears not to move, but only to loom larger in the target's field of vision.
Some insects sway while moving to appear to be blown back and forth by the breeze.

The same method can be used for military purposes, for example by missiles to minimise their risk of detection by an enemy. However, missile engineers, and animals such as bats, use the method mainly for its efficiency rather than camouflage.

Animals such as chameleon, frog, flatfish such as the peacock flounder, squid, octopus and even the isopod idotea balthica actively change their skin patterns and colours using special chromatophore cells to resemble their current background, or, as in most chameleons, for signalling. However, Smith's dwarf chameleon does use active colour change for camouflage.

Each chromatophore contains pigment of only one colour. In fish and frogs, colour change is mediated by a type of chromatophore known as melanophores that contain dark pigment. A melanophore is star-shaped; it contains many small pigmented organelles which can be dispersed throughout the cell, or aggregated near its centre. When the pigmented organelles are dispersed, the cell makes a patch of the animal's skin appear dark; when they are aggregated, most of the cell, and the animal's skin, appears light. In frogs, the change is controlled relatively slowly, mainly by hormones. In fish, the change is controlled by the brain, which sends signals directly to the chromatophores, as well as producing hormones.

The skins of cephalopods such as the octopus contain complex units, each consisting of a chromatophore with surrounding muscle and nerve cells. The cephalopod chromatophore has all its pigment grains in a small elastic sac, which can be stretched or allowed to relax under the control of the brain to vary its opacity. By controlling chromatophores of different colours, cephalopods can rapidly change their skin patterns and colours.

On a longer timescale, animals like the Arctic hare, Arctic fox, stoat, and rock ptarmigan have snow camouflage, changing their coat colour (by moulting and growing new fur or feathers) from brown or grey in the summer to white in the winter; the Arctic fox is the only species in the dog family to do so. However, Arctic hares which live in the far north of Canada, where summer is very short, remain white year-round.

The principle of varying coloration either rapidly or with the changing seasons has military applications. "Active camouflage" could in theory make use of both dynamic colour change and counterillumination. Simple methods such as changing uniforms and repainting vehicles for winter have been in use since World War II. In 2011, BAE Systems announced their Adaptiv infrared camouflage technology. It uses about 1,000 hexagonal panels to cover the sides of a tank. The Peltier plate panels are heated and cooled to match either the vehicle's surroundings (crypsis), or an object such as a car (mimesis), when viewed in infrared.

Countershading uses graded colour to counteract the effect of self-shadowing, creating an illusion of flatness. Self-shadowing makes an animal appear darker below than on top, grading from light to dark; countershading 'paints in' tones which are darkest on top, lightest below, making the countershaded animal nearly invisible against a suitable background. Thayer observed that "Animals are painted by Nature, darkest on those parts which tend to be most lighted by the sky's light, and "vice versa"". Accordingly, the principle of countershading is sometimes called "Thayer's Law". Countershading is widely used by terrestrial animals, such as gazelles and grasshoppers; marine animals, such as sharks and dolphins; and birds, such as snipe and dunlin.

Countershading is less often used for military camouflage, despite Second World War experiments that showed its effectiveness. English zoologist Hugh Cott encouraged the use of methods including countershading, but despite his authority on the subject, failed to persuade the British authorities. Soldiers often wrongly viewed camouflage netting as a kind of invisibility cloak, and they had to be taught to look at camouflage practically, from an enemy observer's viewpoint. At the same time in Australia, zoologist William John Dakin advised soldiers to copy animals' methods, using their instincts for wartime camouflage.

The term countershading has a second meaning unrelated to "Thayer's Law". It is that the upper and undersides of animals such as sharks, and of some military aircraft, are different colours to match the different backgrounds when seen from above or from below. Here the camouflage consists of two surfaces, each with the simple function of providing concealment against a specific background, such as a bright water surface or the sky. The body of a shark or the fuselage of an aircraft is not gradated from light to dark to appear flat when seen from the side. The camouflage methods used are the matching of background colour and pattern, and disruption of outlines.

Counter-illumination means producing light to match a background that is brighter than an animal's body or military vehicle; it is a form of active camouflage. It is notably used by some species of squid, such as the firefly squid and the midwater squid. The latter has light-producing organs (photophores) scattered all over its underside; these create a sparkling glow that prevents the animal from appearing as a dark shape when seen from below. Counterillumination camouflage is the likely function of the bioluminescence of many marine organisms, though light is also produced to attract or to detect prey and for signalling.

Counterillumination has rarely been used for military purposes. "Diffused lighting camouflage" was trialled by Canada's National Research Council during the Second World War. It involved projecting light on to the sides of ships to match the faint glow of the night sky, requiring awkward external platforms to support the lamps. The Canadian concept was refined in the American Yehudi lights project, and trialled in aircraft including B-24 Liberators and naval Avengers. The planes were fitted with forward-pointing lamps automatically adjusted to match the brightness of the night sky. This enabled them to approach much closer to a target – within – before being seen. Counterillumination was made obsolete by radar, and neither diffused lighting camouflage nor Yehudi lights entered active service.

Many marine animals that float near the surface are highly transparent, giving them almost perfect camouflage. However, transparency is difficult for bodies made of materials that have different refractive indices from seawater. Some marine animals such as jellyfish have gelatinous bodies, composed mainly of water; their thick mesogloea is acellular and highly transparent. This conveniently makes them buoyant, but it also makes them large for their muscle mass, so they cannot swim fast, making this form of camouflage a costly trade-off with mobility. Gelatinous planktonic animals are between 50 and 90 percent transparent. A transparency of 50 percent is enough to make an animal invisible to a predator such as cod at a depth of ; better transparency is required for invisibility in shallower water, where the light is brighter and predators can see better. For example, a cod can see prey that are 98 percent transparent in optimal lighting in shallow water. Therefore, sufficient transparency for camouflage is more easily achieved in deeper waters.
Some tissues such as muscles can be made transparent, provided either they are very thin or organised as regular layers or fibrils that are small compared to the wavelength of visible light. A familiar example is the transparency of the lens of the vertebrate eye, which is made of the protein crystallin, and the vertebrate cornea which is made of the protein collagen. Other structures cannot be made transparent, notably the retinas or equivalent light-absorbing structures of eyes – they must absorb light to be able to function. The camera-type eye of vertebrates and cephalopods must be completely opaque. Finally, some structures are visible for a reason, such as to lure prey. For example, the nematocysts (stinging cells) of the transparent siphonophore "Agalma okenii" resemble small copepods. Examples of transparent marine animals include a wide variety of larvae, including radiata (coelenterates), siphonophores, salps (floating tunicates), gastropod molluscs, polychaete worms, many shrimplike crustaceans, and fish; whereas the adults of most of these are opaque and pigmented, resembling the seabed or shores where they live. Adult comb jellies and jellyfish obey the rule, often being mainly transparent. Cott suggests this follows the more general rule that animals resemble their background: in a transparent medium like seawater, that means being transparent. The small Amazon river fish "Microphilypnus amazonicus" and the shrimps it associates with, "Pseudopalaemon gouldingi", are so transparent as to be "almost invisible"; further, these species appear to select whether to be transparent or more conventionally mottled (disruptively patterned) according to the local background in the environment.

Where transparency cannot be achieved, it can be imitated effectively by silvering to make an animal's body highly reflective. At medium depths at sea, light comes from above, so a mirror oriented vertically makes animals such as fish invisible from the side. Most fish in the upper ocean such as sardine and herring are camouflaged by silvering.

The marine hatchetfish is extremely flattened laterally, leaving the body just millimetres thick, and the body is so silvery as to resemble aluminium foil. The mirrors consist of microscopic structures similar to those used to provide structural coloration: stacks of between 5 and 10 crystals of guanine spaced about of a wavelength apart to interfere constructively and achieve nearly 100 per cent reflection. In the deep waters that the hatchetfish lives in, only blue light with a wavelength of 500 nanometres percolates down and needs to be reflected, so mirrors 125 nanometres apart provide good camouflage.

In fish such as the herring which live in shallower water, the mirrors must reflect a mixture of wavelengths, and the fish accordingly has crystal stacks with a range of different spacings. A further complication for fish with bodies that are rounded in cross-section is that the mirrors would be ineffective if laid flat on the skin, as they would fail to reflect horizontally. The overall mirror effect is achieved with many small reflectors, all oriented vertically. Silvering is found in other marine animals as well as fish. The cephalopods, including squid, octopus and cuttlefish, have multilayer mirrors made of protein rather than guanine.

Some deep sea fishes have very black skin, reflecting under 0.5% of ambient light. This can prevent detection by predators or prey fish which use bioluminescence for illumination. "Oneirodes" had a particularly black skin which reflected only 0.044% of 480 nm wavelength light. The ultra-blackness is achieved with a thin but continuous layer of particles in the dermis, melanosomes. These particles both absorb most of the light, and are sized and shaped so as to scatter rather than reflect most of the rest. Modelling suggests that this camouflage should reduce the distance at which such a fish can be seen by a factor of 6 compared to a fish with a nominal 2% reflectance. Species with this adaptation are widely dispersed in various orders of the phylogenetic tree of bony fishes (Actinopterygii), implying that natural selection has driven the convergent evolution of ultra-blackness camouflage independently many times.

In mimesis (also called "masquerade"), the camouflaged object looks like something else which is of no special interest to the observer. Mimesis is common in prey animals, for example when a peppered moth caterpillar mimics a twig, or a grasshopper mimics a dry leaf. It is also found in nest structures; some eusocial wasps, such as "Leipomeles dorsata", build a nest envelope in patterns that mimic the leaves surrounding the nest.

Mimesis is also employed by some predators and parasites to lure their prey. For example, a flower mantis mimics a particular kind of flower, such as an orchid. This tactic has occasionally been used in warfare, for example with heavily armed Q-ships disguised as merchant ships.

The common cuckoo, a brood parasite, provides examples of mimesis both in the adult and in the egg. The female lays her eggs in nests of other, smaller species of bird, one per nest. The female mimics a sparrowhawk. The resemblance is sufficient to make small birds take action to avoid the apparent predator. The female cuckoo then has time to lay her egg in their nest without being seen to do so. The cuckoo's egg itself mimics the eggs of the host species, reducing its chance of being rejected.

Most forms of camouflage are made ineffective by movement: a deer or grasshopper may be highly cryptic when motionless, but instantly seen when it moves. But one method, motion dazzle, requires rapidly moving bold patterns of contrasting stripes. Motion dazzle may degrade predators' ability to estimate the prey's speed and direction accurately, giving the prey an improved chance of escape. Motion dazzle distorts speed perception and is most effective at high speeds; stripes can also distort perception of size (and so, perceived range to the target). As of 2011, motion dazzle had been proposed for military vehicles, but never applied. Since motion dazzle patterns would make animals more difficult to locate accurately when moving, but easier to see when stationary, there would be an evolutionary trade-off between motion dazzle and crypsis.

An animal that is commonly thought to be dazzle-patterned is the zebra. The bold stripes of the zebra have been claimed to be disruptive camouflage, background-blending and countershading. After many years in which the purpose of the coloration was disputed, an experimental study by Tim Caro suggested in 2012 that the pattern reduces the attractiveness of stationary models to biting flies such as horseflies and tsetse flies. However, a simulation study by Martin How and Johannes Zanker in 2014 suggests that when moving, the stripes may confuse observers, such as mammalian predators and biting insects, by two visual illusions: the wagon-wheel effect, where the perceived motion is inverted, and the barberpole illusion, where the perceived motion is in a wrong direction.

Ship camouflage was occasionally used in ancient times. Philostratus () wrote in his "Imagines" that Mediterranean pirate ships could be painted blue-gray for concealment. Vegetius () says that "Venetian blue" (sea green) was used in the Gallic Wars, when Julius Caesar sent his "speculatoria navigia" (reconnaissance boats) to gather intelligence along the coast of Britain; the ships were painted entirely in bluish-green wax, with sails, ropes and crew the same colour. There is little evidence of military use of camouflage on land before 1800, but two unusual ceramics show men in Peru's Mochica culture from before 500 AD, hunting birds with blowpipes which are fitted with a kind of shield near the mouth, perhaps to conceal the hunters' hands and faces. Another early source is a 15th-century French manuscript, "The Hunting Book of Gaston Phebus", showing a horse pulling a cart which contains a hunter armed with a crossbow under a cover of branches, perhaps serving as a hide for shooting game. Jamaican Maroons are said to have used plant materials as camouflage in the First Maroon War ().

The development of military camouflage was driven by the increasing range and accuracy of infantry firearms in the 19th century. In particular the replacement of the inaccurate musket with weapons such as the Baker rifle made personal concealment in battle essential. Two Napoleonic War skirmishing units of the British Army, the 95th Rifle Regiment and the 60th Rifle Regiment, were the first to adopt camouflage in the form of a rifle green jacket, while the Line regiments continued to wear scarlet tunics. A contemporary study in 1800 by the English artist and soldier Charles Hamilton Smith provided evidence that grey uniforms were less visible than green ones at a range of 150 yards.

In the American Civil War, rifle units such as the 1st United States Sharp Shooters (in the Federal army) similarly wore green jackets while other units wore more conspicuous colours. The first British Army unit to adopt khaki uniforms was the Corps of Guides at Peshawar, when Sir Harry Lumsden and his second in command, William Hodson introduced a "drab" uniform in 1848. Hodson wrote that it would be more appropriate for the hot climate, and help make his troops "invisible in a land of dust". Later they improvised by dyeing cloth locally. Other regiments in India soon adopted the khaki uniform, and by 1896 khaki drill uniform was used everywhere outside Europe; by the Second Boer War six years later it was used throughout the British Army.

During the late 19th century camouflage was applied to British coastal fortifications. The fortifications around Plymouth, England were painted in the late 1880s in "irregular patches of red, brown, yellow and green." From 1891 onwards British coastal artillery was permitted to be painted in suitable colours "to harmonise with the surroundings" and by 1904 it was standard practice that artillery and mountings should be painted with "large irregular patches of different colours selected to suit local conditions."

In the First World War, the French army formed a camouflage corps, led by Lucien-Victor Guirand de Scévola, employing artists known as "camoufleurs" to create schemes such as tree observation posts and covers for guns. Other armies soon followed them. The term "camouflage" probably comes from "camoufler", a Parisian slang term meaning "to disguise", and may have been influenced by "camouflet", a French term meaning "smoke blown in someone's face". The English zoologist John Graham Kerr, artist Solomon J. Solomon and the American artist Abbott Thayer led attempts to introduce scientific principles of countershading and disruptive patterning into military camouflage, with limited success. In early 1916 the Royal Naval Air Service began to create dummy air fields to draw the attention of enemy planes to empty land. They created decoy homes and lined fake runways with flares, which were meant to help protect real towns from night raids. This strategy was not common practice and did not succeed at first, but in 1918 it caught the Germans off guard multiple times.

Ship camouflage was introduced in the early 20th century as the range of naval guns increased, with ships painted grey all over. In April 1917, when German U-boats were sinking many British ships with torpedoes, the marine artist Norman Wilkinson devised dazzle camouflage, which paradoxically made ships more visible but harder to target. In Wilkinson's own words, dazzle was designed "not for low visibility, but in such a way as to break up her form and thus confuse a submarine officer as to the course on which she was heading".

In the Second World War, the zoologist Hugh Cott, a protégé of Kerr, worked to persuade the British army to use more effective camouflage methods, including countershading, but, like Kerr and Thayer in the First World War, with limited success. For example, he painted two rail-mounted coastal guns, one in conventional style, one countershaded. In aerial photographs, the countershaded gun was essentially invisible. The power of aerial observation and attack led every warring nation to camouflage targets of all types. The Soviet Union's Red Army created the comprehensive doctrine of "Maskirovka" for military deception, including the use of camouflage. For example, during the Battle of Kursk, General Katukov, the commander of the Soviet 1st Tank Army, remarked that the enemy "did not suspect that our well-camouflaged tanks were waiting for him. As we later learned from prisoners, we had managed to move our tanks forward unnoticed". The tanks were concealed in previously prepared defensive emplacements, with only their turrets above ground level. In the air, Second World War fighters were often painted in ground colours above and sky colours below, attempting two different camouflage schemes for observers above and below. Bombers and night fighters were often black, while maritime reconnaissance planes were usually white, to avoid appearing as dark shapes against the sky. For ships, dazzle camouflage was mainly replaced with plain grey in the Second World War, though experimentation with colour schemes continued.

As in the First World War, artists were pressed into service; for example, the surrealist painter Roland Penrose became a lecturer at the newly founded Camouflage Development and Training Centre at Farnham Castle, writing the practical "Home Guard Manual of Camouflage". The film-maker Geoffrey Barkas ran the Middle East Command Camouflage Directorate during the 1941–1942 war in the Western Desert, including the successful deception of Operation Bertram. Hugh Cott was chief instructor; the artist camouflage officers, who called themselves "camoufleurs", included Steven Sykes and Tony Ayrton. In Australia, artists were also prominent in the Sydney Camouflage Group, formed under the chairmanship of Professor William John Dakin, a zoologist from Sydney University. Max Dupain, Sydney Ure Smith, and William Dobell were among the members of the group, which worked at Bankstown Airport, RAAF Base Richmond and Garden Island Dockyard. In the United States, artists like John Vassos took a certificate course in military and industrial camouflage at the American School of Design with Baron Nicholas Cerkasoff, and went on to create camouflage for the Air Force.

Camouflage has been used to protect military equipment such as vehicles, guns, ships, aircraft and buildings as well as individual soldiers and their positions.
Vehicle camouflage methods begin with paint, which offers at best only limited effectiveness. Other methods for stationary land vehicles include covering with improvised materials such as blankets and vegetation, and erecting nets, screens and soft covers which may suitably reflect, scatter or absorb near infrared and radar waves. Some military textiles and vehicle camouflage paints also reflect infrared to help provide concealment from night vision devices.
After the Second World War, radar made camouflage generally less effective, though coastal boats are sometimes painted like land vehicles. Aircraft camouflage too came to be seen as less important because of radar, and aircraft of different air forces, such as the Royal Air Force's Lightning, were often uncamouflaged.

Many camouflaged textile patterns have been developed to suit the need to match combat clothing to different kinds of terrain (such as woodland, snow, and desert). The design of a pattern effective in all terrains has proved elusive. The American Universal Camouflage Pattern of 2004 attempted to suit all environments, but was withdrawn after a few years of service. Terrain-specific patterns have sometimes been developed but are ineffective in other terrains. The problem of making a pattern that works at different ranges has been solved with multiscale designs, often with a pixellated appearance and designed digitally, that provide a fractal-like range of patch sizes so they appear disruptively coloured both at close range and at a distance. The first genuinely digital camouflage pattern was the Canadian Disruptive Pattern (CADPAT), issued to the army in 2002, soon followed by the American Marine pattern (MARPAT). A pixellated appearance is not essential for this effect, though it is simpler to design and to print.

Hunters of game have long made use of camouflage in the form of materials such as animal skins, mud, foliage, and green or brown clothing to enable them to approach wary game animals. Field sports such as driven grouse shooting conceal hunters in hides (also called blinds or shooting butts). Modern hunting clothing makes use of fabrics that provide a disruptive camouflage pattern; for example, in 1986 the hunter Bill Jordan created cryptic clothing for hunters, printed with images of specific kinds of vegetation such as grass and branches.

Camouflage is occasionally used to make built structures less conspicuous: for example, in South Africa, towers carrying cell telephone antennae are sometimes camouflaged as tall trees with plastic branches, in response to "resistance from the community". Since this method is costly (a figure of three times the normal cost is mentioned), alternative forms of camouflage can include using neutral colours or familiar shapes such as cylinders and flagpoles. Conspicuousness can also be reduced by siting masts near, or on, other structures.

Automotive manufacturers often use patterns to disguise upcoming products. This camouflage is designed to obfuscate the vehicle's visual lines, and is used along with padding, covers, and decals. The patterns' purpose is to prevent visual observation (and to a lesser degree photography), that would subsequently enable reproduction of the vehicle's form factors.

Military camouflage patterns influenced fashion and art from the time of the First World War onwards. Gertrude Stein recalled the cubist artist Pablo Picasso's reaction in around 1915:

In 1919, the attendants of a "dazzle ball", hosted by the Chelsea Arts Club, wore dazzle-patterned black and white clothing. The ball influenced fashion and art via postcards and magazine articles. The "Illustrated London News" announced:

More recently, fashion designers have often used camouflage fabric for its striking designs, its "patterned disorder" and its symbolism. Camouflage clothing can be worn largely for its symbolic significance rather than for fashion, as when, during the late 1960s and early 1970s in the United States, anti-war protestors often ironically wore military clothing during demonstrations against the American involvement in the Vietnam War.

Modern artists such as Ian Hamilton Finlay have used camouflage to reflect on war. His 1973 screenprint of a tank camouflaged in a leaf pattern, "Arcadia", is described by the Tate as drawing "an ironic parallel between this idea of a natural paradise and the camouflage patterns on a tank". The title refers to the Utopian Arcadia of poetry and art, and the "memento mori" Latin phrase "Et in Arcadia ego" which recurs in Hamilton Finlay's work. In science fiction, "Camouflage" is a novel about shapeshifting alien beings by Joe Haldeman. The word is used more figuratively in works of literature such as Thaisa Frank's collection of stories of love and loss, "A Brief History of Camouflage".







Clock

A clock or chronometer is a device that measures and displays time. The clock is one of the oldest human inventions, meeting the need to measure intervals of time shorter than the natural units such as the day, the lunar month, and the year. Devices operating on several physical processes have been used over the millennia.

Some predecessors to the modern clock may be considered "clocks" that are based on movement in nature: A sundial shows the time by displaying the position of a shadow on a flat surface. There is a range of duration timers, a well-known example being the hourglass. Water clocks, along with sundials, are possibly the oldest time-measuring instruments. A major advance occurred with the invention of the verge escapement, which made possible the first mechanical clocks around 1300 in Europe, which kept time with oscillating timekeepers like balance wheels.

Traditionally, in horology (the study of timekeeping), the term "clock" was used for a striking clock, while a clock that did not strike the hours audibly was called a timepiece. This distinction is not generally made any longer. Watches and other timepieces that can be carried on one's person are usually not referred to as clocks. Spring-driven clocks appeared during the 15th century. During the 15th and 16th centuries, clockmaking flourished. The next development in accuracy occurred after 1656 with the invention of the pendulum clock by Christiaan Huygens. A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The mechanism of a timepiece with a series of gears driven by a spring or weights is referred to as clockwork; the term is used by extension for a similar mechanism not used in a timepiece. The electric clock was patented in 1840, and electronic clocks were introduced in the 20th century, becoming widespread with the development of small battery-powered semiconductor devices.

The timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates at a particular frequency.
This object can be a pendulum, a balance wheel, a tuning fork, a quartz crystal, or the vibration of electrons in atoms as they emit microwaves, the last of which is so precise that it serves as the definition of the second.

Clocks have different ways of displaying the time. Analog clocks indicate time with a traditional clock face and moving hands. Digital clocks display a numeric representation of time. Two numbering systems are in use: 12-hour time notation and 24-hour notation. Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays. For the blind and for use over telephones, speaking clocks state the time audibly in words. There are also clocks for the blind that have displays that can be read by touch.

The word "clock" derives from the medieval Latin word for 'bell'——and has cognates in many European languages. Clocks spread to England from the Low Countries, so the English word came from the Middle Low German and Middle Dutch .
The word derives from the Middle English , Old North French , or Middle Dutch , all of which mean 'bell'.

The apparent position of the Sun in the sky changes over the course of each day, reflecting the rotation of the Earth. Shadows cast by stationary objects move correspondingly, so their positions can be used to indicate the time of day. A sundial shows the time by displaying the position of a shadow on a (usually) flat surface that has markings that correspond to the hours. Sundials can be horizontal, vertical, or in other orientations. Sundials were widely used in ancient times. With knowledge of latitude, a well-constructed sundial can measure local solar time with reasonable accuracy, within a minute or two. Sundials continued to be used to monitor the performance of clocks until the 1830s, when the use of the telegraph and trains standardized time and time zones between cities.

Many devices can be used to mark the passage of time without respect to reference time (time of day, hours, minutes, etc.) and can be useful for measuring duration or intervals. Examples of such duration timers are candle clocks, incense clocks, and the hourglass. Both the candle clock and the incense clock work on the same principle, wherein the consumption of resources is more or less constant, allowing reasonably precise and repeatable estimates of time passages. In the hourglass, fine sand pouring through a tiny hole at a constant rate indicates an arbitrary, predetermined passage of time. The resource is not consumed, but re-used.

Water clocks, along with sundials, are possibly the oldest time-measuring instruments, with the only exception being the day-counting tally stick. Given their great antiquity, where and when they first existed is not known and is perhaps unknowable. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and Egypt around the 16th century BC. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, write about water clocks appearing as early as 4000 BC in these regions of the world.

The Macedonian astronomer Andronicus of Cyrrhus supervised the construction of the Tower of the Winds in Athens in the 1st century BC, which housed a large clepsydra inside as well as multiple prominent sundials outside, allowing it to function as a kind of early clocktower. The Greek and Roman civilizations advanced water clock design with improved accuracy. These advances were passed on through Byzantine and Islamic times, eventually making their way back to Europe. Independently, the Chinese developed their own advanced water clocks () by 725 AD, passing their ideas on to Korea and Japan.

Some water clock designs were developed independently, and some knowledge was transferred through the spread of trade. Pre-modern societies do not have the same precise timekeeping requirements that exist in modern industrial societies, where every hour of work or rest is monitored and work may start or finish at any time regardless of external conditions. Instead, water clocks in ancient societies were used mainly for astrological reasons. These early water clocks were calibrated with a sundial. While never reaching the level of accuracy of a modern timepiece, the water clock was the most accurate and commonly used timekeeping device for millennia until it was replaced by the more accurate pendulum clock in 17th-century Europe.

Islamic civilization is credited with further advancing the accuracy of clocks through elaborate engineering. In 797 (or possibly 801), the Abbasid caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian elephant named Abul-Abbas together with a "particularly elaborate example" of a water clock. Pope Sylvester II introduced clocks to northern and western Europe around 1000 AD.

The first known geared clock was invented by the great mathematician, physicist, and engineer Archimedes during the 3rd century BC. Archimedes created his astronomical clock, which was also a cuckoo clock with birds singing and moving every hour. It is the first carillon clock as it plays music simultaneously with a person blinking his eyes, surprised by the singing birds. The Archimedes clock works with a system of four weights, counterweights, and strings regulated by a system of floats in a water container with siphons that regulate the automatic continuation of the clock. The principles of this type of clock are described by the mathematician and physicist Hero, who says that some of them work with a chain that turns a gear in the mechanism. Another Greek clock probably constructed at the time of Alexander was in Gaza, as described by Procopius. The Gaza clock was probably a Meteoroskopeion, i.e., a building showing celestial phenomena and the time. It had a pointer for the time and some automations similar to the Archimedes clock. There were 12 doors opening one every hour, with Hercules performing his labors, the Lion at one o'clock, etc., and at night a lamp becomes visible every hour, with 12 windows opening to show the time.

The Tang dynasty Buddhist monk Yi Xing along with government official Liang Lingzan made the escapement in 723 (or 725) to the workings of a water-powered armillary sphere and clock drive, which was the world's first clockwork escapement. The Song dynasty polymath and genius Su Song (1020–1101) incorporated it into his monumental innovation of the astronomical clock tower of Kaifeng in 1088. His astronomical clock and rotating armillary sphere still relied on the use of either flowing water during the spring, summer, and autumn seasons or liquid mercury during the freezing temperatures of winter (i.e., hydraulics).
In Su Song's waterwheel linkwork device, the action of the escapement's arrest and release was achieved by gravity exerted periodically as the continuous flow of liquid-filled containers of a limited size. In a single line of evolution, Su Song's clock therefore united the concepts of the clepsydra and the mechanical clock into one device run by mechanics and hydraulics. In his memorial, Su Song wrote about this concept:

According to your servant's opinion there have been many systems and designs for astronomical instruments during past dynasties all differing from one another in minor respects. But the principle of the use of water-power for the driving mechanism has always been the same. The heavens move without ceasing but so also does water flow (and fall). Thus if the water is made to pour with perfect evenness, then the comparison of the rotary movements (of the heavens and the machine) will show no discrepancy or contradiction; for the unresting follows the unceasing.

Song was also strongly influenced by the earlier armillary sphere created by Zhang Sixun (976 AD), who also employed the escapement mechanism and used liquid mercury instead of water in the waterwheel of his astronomical clock tower. The mechanical clockworks for Su Song's astronomical tower featured a great driving-wheel that was 11 feet in diameter, carrying 36 scoops, into each of which water was poured at a uniform rate from the "constant-level tank". The main driving shaft of iron, with its cylindrical necks supported on iron crescent-shaped bearings, ended in a pinion, which engaged a gear wheel at the lower end of the main vertical transmission shaft. This great astronomical hydromechanical clock tower was about ten metres high (about 30 feet), featured a clock escapement, and was indirectly powered by a rotating wheel either with falling water or liquid mercury. A full-sized working replica of Su Song's clock exists in the Republic of China (Taiwan)'s National Museum of Natural Science, Taichung city. This full-scale, fully functional replica, approximately 12 meters (39 feet) in height, was constructed from Su Song's original descriptions and mechanical drawings. The Chinese escapement spread west and was the source for Western escapement technology.

In the 12th century, Al-Jazari, an engineer from Mesopotamia (lived 1136–1206) who worked for the Artuqid king of Diyar-Bakr, Nasir al-Din, made numerous clocks of all shapes and sizes. The most reputed clocks included the elephant, scribe, and castle clocks, some of which have been successfully reconstructed. As well as telling the time, these grand clocks were symbols of the status, grandeur, and wealth of the Urtuq State. Knowledge of these mercury escapements may have spread through Europe with translations of Arabic and Spanish texts.

The word (from the Greek —'hour', and —'to tell') was used to describe early mechanical clocks, but the use of this word (still used in several Romance languages) for all timekeepers conceals the true nature of the mechanisms. For example, there is a record that in 1176, Sens Cathedral in France installed an 'horologe', but the mechanism used is unknown. According to Jocelyn de Brakelond, in 1198, during a fire at the abbey of St Edmundsbury (now Bury St Edmunds), the monks "ran to the clock" to fetch water, indicating that their water clock had a reservoir large enough to help extinguish the occasional fire. The word "clock" (via Medieval Latin from Old Irish , both meaning 'bell'), which gradually supersedes "horologe", suggests that it was the sound of bells that also characterized the prototype mechanical clocks that appeared during the 13th century in Europe.

In Europe, between 1280 and 1320, there was an increase in the number of references to clocks and horologes in church records, and this probably indicates that a new type of clock mechanism had been devised. Existing clock mechanisms that used water power were being adapted to take their driving power from falling weights. This power was controlled by some form of oscillating mechanism, probably derived from existing bell-ringing or alarm devices. This controlled release of power – the escapement – marks the beginning of the true mechanical clock, which differed from the previously mentioned cogwheel clocks. The verge escapement mechanism appeared during the surge of true mechanical clock development, which did not need any kind of fluid power, like water or mercury, to work.

These mechanical clocks were intended for two main purposes: for signalling and notification (e.g., the timing of services and public events) and for modeling the solar system. The former purpose is administrative; the latter arises naturally given the scholarly interests in astronomy, science, and astrology and how these subjects integrated with the religious philosophy of the time. The astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system.

Simple clocks intended mainly for notification were installed in towers and did not always require faces or hands. They would have announced the canonical hours or intervals between set times of prayer. Canonical hours varied in length as the times of sunrise and sunset shifted. The more sophisticated astronomical clocks would have had moving dials or hands and would have shown the time in various time systems, including Italian hours, canonical hours, and time as measured by astronomers at the time. Both styles of clocks started acquiring extravagant features, such as automata.

In 1283, a large clock was installed at Dunstable Priory in Bedfordshire in southern England; its location above the rood screen suggests that it was not a water clock. In 1292, Canterbury Cathedral installed a 'great horloge'. Over the next 30 years, there were mentions of clocks at a number of ecclesiastical institutions in England, Italy, and France. In 1322, a new clock was installed in Norwich, an expensive replacement for an earlier clock installed in 1273. This had a large (2 metre) astronomical dial with automata and bells. The costs of the installation included the full-time employment of two clockkeepers for two years.

An elaborate water clock, the 'Cosmic Engine', was invented by Su Song, a Chinese polymath, designed and constructed in China in 1092. This great astronomical hydromechanical clock tower was about ten metres high (about 30 feet) and was indirectly powered by a rotating wheel with falling water and liquid mercury, which turned an armillary sphere capable of calculating complex astronomical problems.

In Europe, there were the clocks constructed by Richard of Wallingford in Albans by 1336, and by Giovanni de Dondi in Padua from 1348 to 1364. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. They illustrate how quickly the theory of the mechanical clock had been translated into practical constructions, and also that one of the many impulses to their development had been the desire of astronomers to investigate celestial phenomena.

The Astrarium of Giovanni Dondi dell'Orologio was a complex astronomical clock built between 1348 and 1364 in Padua, Italy, by the doctor and clock-maker Giovanni Dondi dell'Orologio. The Astrarium had seven faces and 107 moving gears; it showed the positions of the sun, the moon and the five planets then known, as well as religious feast days. The astrarium stood about 1 metre high, and consisted of a seven-sided brass or iron framework resting on 7 decorative paw-shaped feet. The lower section provided a 24-hour dial and a large calendar drum, showing the fixed feasts of the church, the movable feasts, and the position in the zodiac of the moon's ascending node. The upper section contained 7 dials, each about 30 cm in diameter, showing the positional data for the Primum Mobile, Venus, Mercury, the moon, Saturn, Jupiter, and Mars. Directly above the 24-hour dial is the dial of the Primum Mobile, so called because it reproduces the diurnal motion of the stars and the annual motion of the sun against the background of stars. Each of the 'planetary' dials used complex clockwork to produce reasonably accurate models of the planets' motion. These agreed reasonably well both with Ptolemaic theory and with observations.

Wallingford's clock had a large astrolabe-type dial, showing the sun, the moon's age, phase, and node, a star map, and possibly the planets. In addition, it had a wheel of fortune and an indicator of the state of the tide at London Bridge. Bells rang every hour, the number of strokes indicating the time. Dondi's clock was a seven-sided construction, 1 metre high, with dials showing the time of day, including minutes, the motions of all the known planets, an automatic calendar of fixed and movable feasts, and an eclipse prediction hand rotating once every 18 years. It is not known how accurate or reliable these clocks would have been. They were probably adjusted manually every day to compensate for errors caused by wear and imprecise manufacture. Water clocks are sometimes still used today, and can be examined in places such as ancient castles and museums. The Salisbury Cathedral clock, built in 1386, is considered to be the world's oldest surviving mechanical clock that strikes the hours.

Clockmakers developed their art in various ways. Building smaller clocks was a technical challenge, as was improving accuracy and reliability. Clocks could be impressive showpieces to demonstrate skilled craftsmanship, or less expensive, mass-produced items for domestic use. The escapement in particular was an important factor affecting the clock's accuracy, so many different mechanisms were tried.

Spring-driven clocks appeared during the 15th century, although they are often erroneously credited to Nuremberg watchmaker Peter Henlein (or Henle, or Hele) around 1511. The earliest existing spring driven clock is the chamber clock given to Phillip the Good, Duke of Burgundy, around 1430, now in the Germanisches Nationalmuseum. Spring power presented clockmakers with a new problem: how to keep the clock movement running at a constant rate as the spring ran down. This resulted in the invention of the "stackfreed" and the fusee in the 15th century, and many other innovations, down to the invention of the modern "going barrel" in 1760.

Early clock dials did not indicate minutes and seconds. A clock with a dial indicating minutes was illustrated in a 1475 manuscript by Paulus Almanus, and some 15th-century clocks in Germany indicated minutes and seconds.
An early record of a seconds hand on a clock dates back to about 1560 on a clock now in the Fremersdorf collection.

During the 15th and 16th centuries, clockmaking flourished, particularly in the metalworking towns of Nuremberg and Augsburg, and in Blois, France. Some of the more basic table clocks have only one time-keeping hand, with the dial between the hour markers being divided into four equal parts making the clocks readable to the nearest 15 minutes. Other clocks were exhibitions of craftsmanship and skill, incorporating astronomical indicators and musical movements. The cross-beat escapement was invented in 1584 by Jost Bürgi, who also developed the remontoire. Bürgi's clocks were a great improvement in accuracy as they were correct to within a minute a day. These clocks helped the 16th-century astronomer Tycho Brahe to observe astronomical events with much greater precision than before.

The next development in accuracy occurred after 1656 with the invention of the pendulum clock. Galileo had the idea to use a swinging bob to regulate the motion of a time-telling device earlier in the 17th century. Christiaan Huygens, however, is usually credited as the inventor. He determined the mathematical formula that related pendulum length to time (about 99.4 cm or 39.1 inches for the one second movement) and had the first pendulum-driven clock made. The first model clock was built in 1657 in the Hague, but it was in England that the idea was taken up. The longcase clock (also known as the "grandfather clock") was created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671. It was also at this time that clock cases began to be made of wood and clock faces to use enamel as well as hand-painted ceramics.

In 1670, William Clement created the anchor escapement, an improvement over Huygens' crown escapement. Clement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clockmaker and others, and the second hand was first introduced.

In 1675, Huygens and Robert Hooke invented the spiral balance spring, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. The great English clockmaker Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration. The rack and snail striking mechanism for striking clocks, was introduced during the 17th century and had distinct advantages over the 'countwheel' (or 'locking plate') mechanism. During the 20th century there was a common misconception that Edward Barlow invented "rack and snail" striking. In fact, his invention was connected with a repeating mechanism employing the rack and snail. The repeating clock, that chimes the number of hours (or even minutes) on demand was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.

A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. This clock could not contain a pendulum, which would be virtually useless on a rocking ship. In 1714, the British government offered large financial rewards to the value of 20,000 pounds for anyone who could determine longitude accurately. John Harrison, who dedicated his life to improving the accuracy of his clocks, later received considerable sums under the Longitude Act.

In 1735, Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat. The chronometer was tested in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.

The British had dominated watch manufacture for much of the 17th and 18th centuries, but maintained a system of production that was geared towards high quality products for the elite. Although there was an attempt to modernise clock manufacture with mass-production techniques and the application of duplicating tools and machinery by the British Watch Company in 1843, it was in the United States that this system took off. In 1816, Eli Terry and some other Connecticut clockmakers developed a way of mass-producing clocks by using interchangeable parts. Aaron Lufkin Dennison started a factory in 1851 in Massachusetts that also used interchangeable parts, and by 1861 was running a successful enterprise incorporated as the Waltham Watch Company.

In 1815, the English scientist Francis Ronalds published the first electric clock powered by dry pile batteries. Alexander Bain, a Scottish clockmaker, patented the electric clock in 1840. The electric clock's mainspring is wound either with an electric motor or with an electromagnet and armature. In 1841, he first patented the electromagnetic pendulum. By the end of the nineteenth century, the advent of the dry cell battery made it feasible to use electric power in clocks. Spring or weight driven clocks that use electricity, either alternating current (AC) or direct current (DC), to rewind the spring or raise the weight of a mechanical clock would be classified as an electromechanical clock. This classification would also apply to clocks that employ an electrical impulse to propel the pendulum. In electromechanical clocks the electricity serves no time keeping function. These types of clocks were made as individual timepieces but more commonly used in synchronized time installations in schools, businesses, factories, railroads and government facilities as a master clock and slave clocks.

Where an AC electrical supply of stable frequency is available, timekeeping can be maintained very reliably by using a synchronous motor, essentially counting the cycles. The supply current alternates with an accurate frequency of 50 hertz in many countries, and 60 hertz in others. While the frequency may vary slightly during the day as the load changes, generators are designed to maintain an accurate number of cycles over a day, so the clock may be a fraction of a second slow or fast at any time, but will be perfectly accurate over a long time. The rotor of the motor rotates at a speed that is related to the alternation frequency. Appropriate gearing converts this rotation speed to the correct ones for the hands of the analog clock. Time in these cases is measured in several ways, such as by counting the cycles of the AC supply, vibration of a tuning fork, the behaviour of quartz crystals, or the quantum vibrations of atoms. Electronic circuits divide these high-frequency oscillations to slower ones that drive the time display.

The piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first crystal oscillator was invented in 1917 by Alexander M. Nicholson, after which the first quartz crystal oscillator was built by Walter G. Cady in 1921. In 1927 the first quartz clock was built by Warren Marrison and J.W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes at the time, limited their practical use elsewhere. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production resulted in the subsequent proliferation of quartz clocks and watches.

Currently, atomic clocks are the most accurate clocks in existence. They are considerably more accurate than quartz clocks as they can be accurate to within a few seconds over trillions of years. Atomic clocks were first theorized by Lord Kelvin in 1879. In the 1930s the development of magnetic resonance created practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale "ephemeris time" (ET). As of 2013, the most stable atomic clocks are ytterbium clocks, which are stable to within less than two parts in 1 quintillion ().

The invention of the mechanical clock in the 13th century initiated a change in timekeeping methods from continuous processes, such as the motion of the gnomon's shadow on a sundial or the flow of liquid in a water clock, to periodic oscillatory processes, such as the swing of a pendulum or the vibration of a quartz crystal, which had the potential for more accuracy. All modern clocks use oscillation.

Although the mechanisms they use vary, all oscillating clocks, mechanical, electric, and atomic, work similarly and can be divided into analogous parts. They consist of an object that repeats the same motion over and over again, an "oscillator", with a precisely constant time interval between each repetition, or 'beat'. Attached to the oscillator is a "controller" device, which sustains the oscillator's motion by replacing the energy it loses to friction, and converts its oscillations into a series of pulses. The pulses are then counted by some type of "counter", and the number of counts is converted into convenient units, usually seconds, minutes, hours, etc. Finally some kind of "indicator" displays the result in human readable form.

The timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates repetitively at a precisely constant frequency.

The advantage of a harmonic oscillator over other forms of oscillator is that it employs resonance to vibrate at a precise natural resonant frequency or "beat" dependent only on its physical characteristics, and resists vibrating at other rates. The possible precision achievable by a harmonic oscillator is measured by a parameter called its Q, or quality factor, which increases (other things being equal) with its resonant frequency. This is why there has been a long-term trend toward higher frequency oscillators in clocks. Balance wheels and pendulums always include a means of adjusting the rate of the timepiece. Quartz timepieces sometimes include a rate screw that adjusts a capacitor for that purpose. Atomic clocks are primary standards, and their rate cannot be adjusted.

Some clocks rely for their accuracy on an external oscillator; that is, they are automatically synchronized to a more accurate clock:



This has the dual function of keeping the oscillator running by giving it 'pushes' to replace the energy lost to friction, and converting its vibrations into a series of pulses that serve to measure the time.
In mechanical clocks, the low Q of the balance wheel or pendulum oscillator made them very sensitive to the disturbing effect of the impulses of the escapement, so the escapement had a great effect on the accuracy of the clock, and many escapement designs were tried. The higher Q of resonators in electronic clocks makes them relatively insensitive to the disturbing effects of the drive power, so the driving oscillator circuit is a much less critical component.

This counts the pulses and adds them up to get traditional time units of seconds, minutes, hours, etc. It usually has a provision for "setting" the clock by manually entering the correct time into the counter.

This displays the count of seconds, minutes, hours, etc. in a human readable form.

Clocks can be classified by the type of time display, as well as by the method of timekeeping.

Analog clocks usually use a clock face which indicates time using rotating pointers called "hands" on a fixed numbered dial or dials. The standard clock face, known universally throughout the world, has a short "hour hand" which indicates the hour on a circular dial of 12 hours, making two revolutions per day, and a longer "minute hand" which indicates the minutes in the current hour on the same dial, which is also divided into 60 minutes. It may also have a "second hand" which indicates the seconds in the current minute. The only other widely used clock face today is the 24 hour analog dial, because of the use of 24 hour time in military organizations and timetables. Before the modern clock face was standardized during the Industrial Revolution, many other face designs were used throughout the years, including dials divided into 6, 8, 10, and 24 hours. During the French Revolution the French government tried to introduce a 10-hour clock, as part of their decimal-based metric system of measurement, but it did not achieve widespread use. An Italian 6 hour clock was developed in the 18th century, presumably to save power (a clock or watch striking 24 times uses more power).

Another type of analog clock is the sundial, which tracks the sun continuously, registering the time by the shadow position of its gnomon. Because the sun does not adjust to daylight saving time, users must add an hour during that time. Corrections must also be made for the equation of time, and for the difference between the longitudes of the sundial and of the central meridian of the time zone that is being used (i.e. 15 degrees east of the prime meridian for each hour that the time zone is ahead of GMT). Sundials use some or part of the 24 hour analog dial. There also exist clocks which use a digital display despite having an analog mechanism—these are commonly referred to as flip clocks. Alternative systems have been proposed. For example, the "Twelv" clock indicates the current hour using one of twelve colors, and indicates the minute by showing a proportion of a circular disk, similar to a moon phase.

Digital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks:


Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays; many other display technologies are used as well (cathode-ray tubes, nixie tubes, etc.). After a reset, battery change or power failure, these clocks without a backup battery or capacitor either start counting from 12:00, or stay at 12:00, often with blinking digits indicating that the time needs to be set. Some newer clocks will reset themselves based on radio or Internet time servers that are tuned to national atomic clocks. Since the introduction of digital clocks in the 1960s, there has been a notable decline in the use of analog clocks.

Some clocks, called 'flip clocks', have digital displays that work mechanically. The digits are painted on sheets of material which are mounted like the pages of a book. Once a minute, a page is turned over to reveal the next digit. These displays are usually easier to read in brightly lit conditions than LCDs or LEDs. Also, they do not go back to 12:00 after a power interruption. Flip clocks generally do not have electronic mechanisms. Usually, they are driven by AC-synchronous motors.

Clocks with analog quadrants, with a digital component, usually minutes and hours displayed analogously and seconds displayed in digital mode.

For convenience, distance, telephony or blindness, auditory clocks present the time as sounds. The sound is either spoken natural language, (e.g. "The time is twelve thirty-five"), or as auditory codes (e.g. number of sequential bell rings on the hour represents the number of the hour like the bell, Big Ben). Most telecommunication companies also provide a speaking clock service as well.

Word clocks are clocks that display the time visually using sentences. E.g.: "It's about three o'clock." These clocks can be implemented in hardware or software.

Some clocks, usually digital ones, include an optical projector that shines a magnified image of the time display onto a screen or onto a surface such as an indoor ceiling or wall. The digits are large enough to be easily read, without using glasses, by persons with moderately imperfect vision, so the clocks are convenient for use in their bedrooms. Usually, the timekeeping circuitry has a battery as a backup source for an uninterrupted power supply to keep the clock on time, while the projection light only works when the unit is connected to an A.C. supply. Completely battery-powered portable versions resembling flashlights are also available.

Auditory and projection clocks can be used by people who are blind or have limited vision. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. Another type is essentially digital, and uses devices that use a code such as Braille to show the digits so that they can be felt with the fingertips.

Some clocks have several displays driven by a single mechanism, and some others have several completely separate mechanisms in a single case. Clocks in public places often have several faces visible from different directions, so that the clock can be read from anywhere in the vicinity; all the faces show the same time. Other clocks show the current time in several time-zones. Watches that are intended to be carried by travellers often have two displays, one for the local time and the other for the time at home, which is useful for making pre-arranged phone calls. Some equation clocks have two displays, one showing mean time and the other solar time, as would be shown by a sundial. Some clocks have both analog and digital displays. Clocks with Braille displays usually also have conventional digits so they can be read by sighted people.

Clocks are in homes, offices and many other places; smaller ones (watches) are carried on the wrist or in a pocket; larger ones are in public places, e.g. a railway station or church. A small clock is often shown in a corner of computer displays, mobile phones and many MP3 players.

The primary purpose of a clock is to "display" the time. Clocks may also have the facility to make a loud alert signal at a specified time, typically to waken a sleeper at a preset time; they are referred to as "alarm clocks". The alarm may start at a low volume and become louder, or have the facility to be switched off for a few minutes then resume. Alarm clocks with visible indicators are sometimes used to indicate to children too young to read the time that the time for sleep has finished; they are sometimes called "training clocks".

A clock mechanism may be used to "control" a device according to time, e.g. a central heating system, a VCR, or a time bomb (see: digital counter). Such mechanisms are usually called timers. Clock mechanisms are also used to drive devices such as solar trackers and astronomical telescopes, which have to turn at accurately controlled speeds to counteract the rotation of the Earth.

Most digital computers depend on an internal signal at constant frequency to synchronize processing; this is referred to as a clock signal. (A few research projects are developing CPUs based on asynchronous circuits.) Some equipment, including computers, also maintains time and date for use as required; this is referred to as time-of-day clock, and is distinct from the system clock signal, although possibly based on counting its cycles.

In Chinese culture, giving a clock () is often taboo, especially to the elderly as the term for this act is a homophone with the term for the act of attending another's funeral (). This homonymic pair works in both Mandarin and Cantonese, although in most parts of China only clocks and large bells, and not watches, are called ""zhong"", and watches are commonly given as gifts in China. However, should such a gift be given, the "unluckiness" of the gift can be countered by exacting a small monetary payment so the recipient is buying the clock and thereby counteracting the ("give") expression of the phrase.

For some scientific work timing of the utmost accuracy is essential. It is also necessary to have a standard of the maximum accuracy against which working clocks can be calibrated. An ideal clock would give the time to unlimited accuracy, but this is not realisable. Many physical processes, in particular including some transitions between atomic energy levels, occur at exceedingly stable frequency; counting cycles of such a process can give a very accurate and consistent time—clocks which work this way are usually called atomic clocks. Such clocks are typically large, very expensive, require a controlled environment, and are far more accurate than required for most purposes; they are typically used in a standards laboratory.

Until advances in the late twentieth century, navigation depended on the ability to measure latitude and longitude. Latitude can be determined through celestial navigation; the measurement of longitude requires accurate knowledge of time. This need was a major motivation for the development of accurate mechanical clocks. John Harrison created the first highly accurate marine chronometer in the mid-18th century. The Noon gun in Cape Town still fires an accurate signal to allow ships to check their chronometers. Many buildings near major ports used to have (some still do) a large ball mounted on a tower or mast arranged to drop at a pre-determined time, for the same purpose. While satellite navigation systems such as GPS require unprecedentedly accurate knowledge of time, this is supplied by equipment on the satellites; vehicles no longer need timekeeping equipment.

Clocks can be used to measure varying periods of time in games and sports. Stopwatches can be used to time the performance of track athletes. Chess clocks are used to limit the board game players' time to make a move. In various sports, "" measure the duration the game or subdivisions of the game, while other clocks may be used for tracking different durations; these include play clocks, shot clocks, and pitch clocks.




Charles Proteus Steinmetz

Charles Proteus Steinmetz (born Karl August Rudolph Steinmetz; April 9, 1865 – October 26, 1923) was an American mathematician and electrical engineer and professor at Union College. He fostered the development of alternating current that made possible the expansion of the electric power industry in the United States, formulating mathematical theories for engineers. He made ground-breaking discoveries in the understanding of hysteresis that enabled engineers to design better electromagnetic apparatus equipment, especially electric motors for use in industry.

At the time of his death, Steinmetz held over 200 patents. A genius in both mathematics and electronics, he did work that earned him the nicknames "Forger of Thunderbolts" and "The Wizard of Schenectady". Steinmetz's equation, Steinmetz solids, Steinmetz curves, and Steinmetz equivalent circuit are all named after him, as are numerous honors and scholarships, including the IEEE Charles Proteus Steinmetz Award, one of the highest technical recognitions given by the Institute of Electrical and Electronics Engineers professional society.

Steinmetz was born Karl August Rudolph Steinmetz on April 9, 1865, in Breslau, Province of Silesia, Prussia (now Wrocław, Poland) the son of Caroline (Neubert) and Karl Heinrich Steinmetz. He was baptized as a Lutheran into the Evangelical Church of Prussia. Steinmetz, who stood only tall as an adult, had dwarfism, hunchback, and hip dysplasia, as did his father and grandfather. Steinmetz attended Johannes Gymnasium and astonished his teachers with his proficiency in mathematics and physics.

Following the Gymnasium, Steinmetz went on to the University of Breslau to begin work on his undergraduate degree in 1883. He was on the verge of finishing his doctorate in 1888 when he came under investigation by the German police for activities on behalf of a socialist university group and articles he had written for a local socialist newspaper.

As socialist meetings and press had been banned in Germany, Steinmetz fled to Zürich in 1889 to escape possible arrest. Cornell University Professor Ronald R. Kline, author of "Steinmetz: Engineer and Socialist", points to other factors which reinforced Steinmetz's decision to leave his homeland such as financial problems and the prospect of a more harmonious life with his socialist friends and supporters than the stressful domestic circumstances of his father's household.

Faced with an expiring visa, he emigrated to the United States in 1889. He changed his first name to "Charles" in order to sound more American, and chose the middle name "Proteus", a wise hunchbacked character from the "Odyssey" who knew many secrets, after a childhood epithet given by classmates Steinmetz felt suited him.

Steinmetz was politically active in the US as a technocratic socialist for over thirty years. Following the Bolshevik introduction of a technocratic plan to electrify Russia, Steinmetz spoke of Lenin alongside Albert Einstein as the "two greatest minds of our time."
He believed in a corporatist industrial government also covering its human wellfare function.

A member of the original Technical Alliance, which also included Thorstein Veblen and Leland Olds, Steinmetz had great faith in the ability of machines to eliminate human toil and create abundance for all. He put it this way: "Some day we [will] make the good things of life for everybody."

Steinmetz is known for his contribution in three major fields of alternating current (AC) systems theory: hysteresis, steady-state analysis, and transients.

Shortly after arriving in the United States, Steinmetz went to work for Rudolf Eickemeyer in Yonkers, New York, and published in the field of magnetic hysteresis, earning worldwide professional recognition. Eickemeyer's firm developed transformers for use in the transmission of electrical power among many other mechanical and electrical devices. In 1893 Eickemeyer's company, along with all of its patents and designs, was bought by the newly formed General Electric Company, where Steinmetz quickly became known as the engineering wizard in GE's engineering community.

Steinmetz's work revolutionized AC circuit theory and analysis, which had been carried out using complicated, time-consuming calculus-based methods. In the groundbreaking paper, "Complex Quantities and Their Use in Electrical Engineering", presented at a July 1893 meeting published in the American Institute of Electrical Engineers (AIEE), Steinmetz simplified these complicated methods to "a simple problem of algebra". He systematized the use of complex number phasor representation in electrical engineering education texts, whereby the lower-case letter "j" is used to designate the 90-degree rotation operator in AC system analysis. His seminal books and many other AIEE papers "taught a whole generation of engineers how to deal with AC phenomena".

Steinmetz also greatly advanced the understanding of lightning. His systematic experiments resulted in the first laboratory created "man-made lightning", earning him the nickname the "Forger of Thunderbolts". These were conducted in a football field-sized laboratory at General Electric, using 120,000 volt generators. He also erected a lightning tower to attract natural lightning to study its patterns and effects, which resulted in several theories.

Steinmetz acted in the following professional capacities:

He was granted an honorary degree from Harvard University in 1901 and a doctorate from Union College in 1903.

Steinmetz wrote 13 books and 60 articles, not exclusively about engineering. He was a member and adviser to the fraternity Phi Gamma Delta at Union College, whose chapter house was one of the first electrified residences.

While serving as president of the Schenectady Board of Education, Steinmetz introduced numerous progressive reforms, including extended school hours, school meals, school nurses, special classes for the children of immigrants, and the distribution of free textbooks.

Steinmetz was affected by kyphosis, as were his father and grandfather. In spite of his love for children and family life, Steinmetz remained unmarried, to prevent his spinal deformity from being passed to any offspring.

When Joseph LeRoy Hayden, a loyal and hardworking lab assistant, announced that he would marry and look for his own living quarters, Steinmetz made the unusual proposal of opening his large home, complete with research lab, greenhouse, and office to the Haydens and their prospective family. Hayden favored the idea, but his future wife was wary of the unorthodox arrangement. She agreed after Steinmetz's assurance that she could run the house as she saw fit.

After an uneasy start, the arrangement worked well for all parties, especially after three Hayden children were born. Steinmetz legally adopted Joseph Hayden as his son, becoming grandfather to the youngsters, entertaining them with fantastic stories and spectacular scientific demonstrations. The unusual, harmonious living arrangement lasted for the rest of Steinmetz's life.

Steinmetz founded America's first glider club, but none of its prototypes "could be dignified with the term 'flight.

Steinmetz was a lifelong agnostic. He died on October 26, 1923, and was buried in Vale Cemetery in Schenectady.

Steinmetz earned wide recognition among the scientific community and numerous awards and honors both during his life and posthumously.

Steinmetz's equation, derived from his experiments, defines the approximate heat energy due to magnetic hysteresis released, per cycle per unit volume of magnetic material. A Steinmetz solid is the solid body generated by the intersection of two or three cylinders of equal radius at right angles. Steinmetz' equivalent circuit is still widely used for the design and testing of induction machines.

One of the highest technical recognitions given by the Institute of Electrical and Electronics Engineers, the "IEEE Charles Proteus Steinmetz Award", is given for major contributions to standardization within the field of electrical and electronics engineering. Other awards include the Certificate of Merit of Franklin Institute, 1908; the Elliott Cresson Medal, 1913; and the Cedergren Medal, 1914. Steinmetz was also an elected member of both the American Academy of Arts and Sciences and the American Philosophical Society.

The Charles P. Steinmetz Memorial Lecture series was begun in his honor in 1925, sponsored by the Schenectady branch of the IEEE. Through 2017 seventy-three gatherings have taken place, held almost exclusively at Union College, featuring notable figures such as Nobel laureate experimental physicist Robert A. Millikan, helicopter inventor Igor Sikorsky, nuclear submarine pioneer Admiral Hyman G. Rickover (1963), Nobel-winning semiconductor inventor William Shockley, and Internet "founding father" Leonard Kleinrock.

Steinmetz's connection to Union is further celebrated with the annual Steinmetz Symposium, a day-long event in which Union undergraduates give presentations on research they have done. Steinmetz Hall, which houses the Union College computer center, is named after him.

The Charles P. Steinmetz Scholarship is awarded annually by the college, underwritten since its inception in 1923 by the General Electric Company. An additional Charles P. Steinmetz Memorial Scholarship was later established at Union by Marjorie Hayden, daughter of Joseph and Corrine Hayden, and is awarded to students majoring in engineering or physics.

A 1914 "Duplex Drive Brougham" Detroit Electric automobile that once belonged to Steinmetz was purchased by Union College in 1971, and restored for use in campus ceremonies. The Steinmetz car is permanent displayed in the first-floor corridor between the Wold Center and F.W. Olin building.

A Chicago public high school, Steinmetz College Prep, is named for him, as well as a Schenectady public school, the Steinmetz Career and Leadership Academy, formerly Steinmetz Middle-School.

A public park in north Schenectady, New York was named for him in 1931.

In 1983, the US Post Office included Steinmetz in a series of postage stamps commemorating American inventors.

In May 2015, a life-size bronze statue of Charles Steinmetz meeting Thomas Edison by sculptor and caster Dexter Benedict was unveiled on a plaza on the corner of Erie Boulevards and South Ferry Street in Schenectady.

Steinmetz is featured in John Dos Passos' "U.S.A." trilogy in one of the biographies. He also serves as a major character in Starling Lawrence's "The Lightning Keeper".

Steinmetz is a major character in the novel "Electric City" by Elizabeth Rosner.

Steinmetz was portrayed in 1959 by the actor Rod Steiger in the CBS television anthology series, "The Joseph Cotten Show". The episode focused on his socialist activities in Germany.

A famous anecdote about Steinmetz concerns a troubleshooting consultation at Henry Ford's River Rouge Plant. A humorous aspect of the story is the "itemized bill" he submitted for the work performed.

At the time of his death, Steinmetz held over 200 patents:




Charles Martel

Charles Martel ( – 22 October 741), "Martel" being a sobriquet in Old French for "The Hammer", was a Frankish political and military leader who, as Duke and Prince of the Franks and Mayor of the Palace to the Merovingian kings of the Franks, was the de facto ruler of the Franks from 718 until his death. He was a son of the Frankish statesman Pepin of Herstal and a noblewoman named Alpaida. Charles successfully asserted his claims to power as successor to his father as the power behind the throne in Frankish politics. Continuing and building on his father's work, he restored centralized government in Francia and began the series of military campaigns that re-established the Franks as the undisputed masters of all Gaul. According to a near-contemporary source, the "Liber Historiae Francorum", Charles was "a warrior who was uncommonly ... effective in battle".

Charles gained a very consequential victory against an Umayyad invasion of Aquitaine at the Battle of Tours, at a time when the Umayyad Caliphate controlled most of the Iberian Peninsula. Alongside his military endeavours, Charles has been traditionally credited with an influential role in the development of the Frankish system of feudalism.

At the end of his reign, Charles divided Francia between his sons, Carloman and Pepin. The latter became the first king of the Carolingian dynasty. Pepin's son Charlemagne, grandson of Charles, extended the Frankish realms and became the first emperor in the West since the Fall of the Western Roman Empire.

Charles, nicknamed "Martel" ("the Hammer") in later chronicles, was a son of Pepin of Herstal and his mistress, possible second wife, Alpaida. He had a brother named Childebrand, who later became the Frankish "dux" (that is, "duke") of Burgundy.

Older historiography commonly describes Charles as "illegitimate", but the dividing line between wives and concubines was not clear-cut in eighth-century Francia. It is likely that the accusation of "illegitimacy" derives from the desire of Pepin's first wife Plectrude to see her progeny as heirs to Pepin's power.

By Charles's lifetime the Merovingians had ceded power to the Mayors of the Palace, who controlled the royal treasury, dispensed patronage, and granted land and privileges in the name of the figurehead king. Charles's father, Pepin of Herstal, had united the Frankish realm by conquering Neustria and Burgundy. Pepin was the first to call himself Duke and Prince of the Franks, a title later taken up by Charles.

In December 714, Pepin of Herstal died. A few months before his death and shortly after the murder of his son Grimoald the Younger, he had, at his wife Plectrude's urging, designated Theudoald, his grandson by their late son Grimoald, his heir in the entire realm. This was immediately opposed by the Austrasian nobles because Theudoald was a child of only eight years of age. To prevent Charles using this unrest to his own advantage, Plectrude had him imprisoned in Cologne, the city which was intended to be her capital. This prevented an uprising on his behalf in Austrasia, but not in Neustria.

Pepin's death occasioned open conflict between his heirs and the Neustrian nobles who sought political independence from Austrasian control. In 715, Dagobert III named Raganfrid mayor of their palace. On 26 September 715, Raganfrid's Neustrians met the young Theudoald's forces at the Battle of Compiègne. Theudoald was defeated and fled back to Cologne. Before the end of the year, Charles had escaped from prison and been acclaimed mayor by the nobles of Austrasia. That same year, Dagobert III died and the Neustrians proclaimed Chilperic II, the cloistered son of Childeric II, as king.

In 716, Chilperic and Raganfrid together led an army into Austrasia intent on seizing the Pippinid wealth at Cologne. The Neustrians allied with another invading force under Redbad, King of the Frisians and met Charles in battle near Cologne, which was still held by Plectrude. Charles had little time to gather men or prepare and the result was inevitable. The Frisians held off Charles, while the king and his mayor besieged Plectrude at Cologne, where she bought them off with a substantial portion of Pepin's treasure. After that they withdrew. The Battle of Cologne is the only defeat of Charles's career.

Charles retreated to the hills of the Eifel to gather and train men. In April 716, he fell upon the triumphant army near Malmedy as it was returning to Neustria. In the ensuing Battle of Amblève, Charles attacked as the enemy rested at midday. According to one source, he split his forces into several groups which fell at them from many sides. Another suggests that while this was his intention, he then decided, given the enemy's unpreparedness, this was not necessary. In any event, the suddenness of the assault led them to believe they were facing a much larger host. Many of the enemy fled and Charles's troops gathered the spoils of the camp. His reputation increased considerably as a result, and he attracted more followers. This battle is often considered by historians as the turning point in Charles's struggle.

Richard Gerberding points out that up to this time, much of Charles's support was probably from his mother's kindred in the lands around Liege. After Amblève, he seems to have won the backing of the influential Willibrord, founder of the Abbey of Echternach. The abbey had been built on land donated by Plectrude's mother, Irmina of Oeren, but most of Willibrord's missionary work had been carried out in Frisia. In joining Chilperic and Ragqnfrid, Radbod of Frisia sacked Utrecht, burning churches and killing many missionaries. Willibrord and his monks were forced to flee to Echternach. Gerberding suggests that Willibrord had decided that the chances of preserving his life's work were better with a successful field commander like Charles than with Plectrude in Cologne. Willibrord subsequently baptized Charles's son Pepin. Gerberding suggests a likely date of Easter 716. Charles also received support from bishop Pepo of Verdun.

Charles took time to rally more men and prepare. By the following spring, he had attracted enough support to invade Neustria. Charles sent an envoy who proposed a cessation of hostilities if Chilperic would recognize his rights as mayor of the palace in Austrasia. The refusal was not unexpected but served to impress upon Charles's forces the unreasonableness of the Neustrians. They met near Cambrai at the Battle of Vincy on 21 March 717. The victorious Charles pursued the fleeing king and mayor to Paris, but as he was not yet prepared to hold the city, he turned back to deal with Plectrude and Cologne. He took the city and dispersed her adherents. Plectrude was allowed to retire to a convent. Theudoald lived to 741 under his uncle's protection.

Upon this success, Charles proclaimed Chlothar IV king in Austrasia in opposition to Chilperic and deposed Rigobert, archbishop of Reims, replacing him with Milo, a lifelong supporter.

In 718, Chilperic responded to Charles's new ascendancy by making an alliance with Odo the Great (or Eudes, as he is sometimes known), the duke of Aquitaine, who had become independent during the civil war in 715, but was again defeated, at the Battle of Soissons, by Charles. Chilperic fled with his ducal ally to the land south of the Loire and Raganfrid fled to Angers. Soon Chlotar IV died and Odo surrendered King Chilperic in exchange for Charles recognizing his dukedom. Charles recognized Chilperic as king of the Franks in return for legitimate royal affirmation of his own mayoralty over all the kingdoms.

Between 718 and 732, Charles secured his power through a series of victories. Having unified the Franks under his banner, Charles was determined to punish the Saxons who had invaded Austrasia. Therefore, late in 718, he laid waste their country to the banks of the Weser, the Lippe, and the Ruhr. He defeated them in the Teutoburg Forest and thus secured the Frankish border.

When the Frisian leader Radbod died in 719, Charles seized West Frisia without any great resistance on the part of the Frisians, who had been subjected to the Franks but had rebelled upon the death of Pippin. When Chilperic II died in 721, Charles appointed as his successor the son of Dagobert III, Theuderic IV, who was still a minor, and who occupied the throne from 721 to 737. Charles was now appointing the kings whom he supposedly served ("rois fainéants"). By the end of his reign, he didn't appoint any at all. At this time, Charles again marched against the Saxons. Then the Neustrians rebelled under Raganfrid, who had left the county of Anjou. They were easily defeated in 724 but Raganfrid gave up his sons as hostages in turn for keeping his county. This ended the civil wars of Charles' reign.

The next six years were devoted in their entirety to assuring Frankish authority over the neighboring political groups. Between 720 and 723, Charles was fighting in Bavaria, where the Agilolfing dukes had gradually evolved into independent rulers, recently in alliance with Liutprand the Lombard. He forced the Alemanni to accompany him, and Duke Hugbert submitted to Frankish suzerainty. In 725 he brought back the Agilolfing Princess Swanachild as a second wife.

In 725 and 728, he again entered Bavaria but, in 730, he marched against Lantfrid, Duke of Alemannia, who had also become independent, and killed him in battle. He forced the Alemanni to capitulate to Frankish suzerainty and did not appoint a successor to Lantfrid. Thus, southern Germany once more became part of the Frankish kingdom, as had northern Germany during the first years of the reign.

In 731, after defeating the Saxons, Charles turned his attention to the rival southern realm of Aquitaine, and crossed the Loire, breaking the treaty with Duke Odo. The Franks ransacked Aquitaine twice, and captured Bourges, although Odo retook it. The "Continuations of Fredegar" allege that Odo called on assistance from the recently established emirate of al-Andalus, but there had been Arab raids into Aquitaine from the 720s onwards. Indeed, the anonymous Chronicle of 754 records a victory for Odo in 721 at the Battle of Toulouse, while the "Liber Pontificalis" records that Odo had killed 375,000 Saracens. It is more likely that this invasion or raid took place in revenge for Odo's support for a rebel Berber leader named Munnuza.

Whatever the precise circumstances were, it is clear that an army under the leadership of Abd al-Rahman al-Ghafiqi headed north, and after some minor engagements marched on the wealthy city of Tours. According to British medieval historian Paul Fouracre, "Their campaign should perhaps be interpreted as a long-distance raid rather than the beginning of a war". They were, however, defeated by the army of Charles at the Battle of Tours (known in France as the Battle of Poitiers), at a location between the French cities of Tours and Poitiers, in a victory described by the "Continuations of Fredegar". According to the historian Bernard Bachrach, the Arab army, mostly mounted, failed to break through the Frankish infantry. News of this battle spread, and may be recorded in Bede's "Ecclesiastical History" (Book V, ch. 23). However, it is not given prominence in Arabic sources from the period.

Despite his victory, Charles did not gain full control of Aquitaine, and Odo remained duke until 735.

Between his victory of 732 and 735, Charles reorganized the kingdom of Burgundy, replacing the counts and dukes with his loyal supporters, thus strengthening his hold on power. He was forced, by the ventures of Bubo, Duke of the Frisians, to invade independent-minded Frisia again in 734. In that year, he slew the duke at the Battle of the Boarn. Charles ordered the Frisian pagan shrines destroyed, and so wholly subjugated the populace that the region was peaceful for twenty years after.

In 735, Duke Odo of Aquitaine died. Though Charles wished to rule the duchy directly and went there to elicit the submission of the Aquitanians, the aristocracy proclaimed Odo's son, Hunald I of Aquitaine, as duke, and Charles and Hunald eventually recognised each other's position.

In 737, at the tail end of his campaigning in Provence and Septimania, the Merovingian king, Theuderic IV, died. Charles, titling himself "maior domus" and "princeps et dux Francorum", did not appoint a new king and nobody acclaimed one. The throne lay vacant until Charles' death. The interregnum, the final four years of Charles' life, was relatively peaceful although in 738 he compelled the Saxons of Westphalia to submit and pay tribute and in 739 he checked an uprising in Provence where some rebels united under the leadership of Maurontus.

Charles used the relative peace to set about integrating the outlying realms of his empire into the Frankish church. He erected four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them Boniface as archbishop and metropolitan over all Germany east of the Rhine, with his seat at Mainz. Boniface had been under his protection from 723 on. Indeed, the saint himself explained to his old friend, Daniel of Winchester, that without it he could neither administer his church, defend his clergy nor prevent idolatry.

In 739, Pope Gregory III begged Charles for his aid against Liutprand, but Charles was loath to fight his onetime ally and ignored the plea. Nonetheless, the pope's request for Frankish protection showed how far Charles had come from the days when he was tottering on excommunication, and set the stage for his son and grandson to assert themselves in the peninsula.

Charles died on 22 October 741, at Quierzy-sur-Oise in what is today the Aisne "département" in the Picardy region of France. He was buried at Saint Denis Basilica in Paris.

His territories had been divided among his adult sons a year earlier: to Carloman he gave Austrasia, Alemannia, and Thuringia, and to Pippin the Younger Neustria, Burgundy, Provence, and Metz and Trier in the "Mosel duchy". Grifo was given several lands throughout the kingdom, but at a later date, just before Charles died.

Earlier in his life Charles had many internal opponents and felt the need to appoint his own kingly claimant, Chlotar IV. Later, however, the dynamics of rulership in Francia had changed, and no hallowed Merovingian ruler was required. Charles divided his realm among his sons without opposition (though he ignored his young son Bernard). For many historians, Charles laid the foundations for his son Pepin's rise to the Frankish throne in 751, and his grandson Charlemagne's imperial acclamation in 800. However, for Paul Fouracre, while Charles was "the most effective military leader in Francia", his career "finished on a note of unfinished business".

Charles married twice, his first wife being Rotrude of Treves, daughter either of Lambert II, Count of Hesbaye, or of Leudwinus, Count of Treves. They had the following children:


Most of the children married and had issue. Hiltrud married Odilo I (Duke of Bavaria). Landrade was once believed to have married a Sigrand (Count of Hesbania) but Sigrand's wife was more likely the sister of Rotrude. Auda married Theoderic, Count of Autun.

Charles also married a second time, to Swanhild and they had a child named Grifo.

Charles also had a known mistress, Ruodhaid, with whom he had:


Charles also had an unknown mistress:


For early medieval authors, Charles was famous for his military victories. Paul the Deacon for instance attributed a victory against the Saracens actually won by Odo of Aquitaine to Charles. However, alongside this there soon developed a darker reputation, for his alleged abuse of church property. A ninth-century text, the "Visio Eucherii", possibly written by Hincmar of Reims, portrayed Charles as suffering in hell for this reason. According to British medieval historian Paul Fouracre, this was "the single most important text in the construction of Charles's reputation as a seculariser or despoiler of church lands".

By the eighteenth century, historians such as Edward Gibbon had begun to portray the Frankish leader as the saviour of Christian Europe from a full-scale Islamic invasion.

In the nineteenth century, the German historian Heinrich Brunner argued that Charles had confiscated church lands in order to fund military reforms that allowed him to defeat the Arab conquests, in this way brilliantly combining two traditions about the ruler. However, Fouracre argued that "...there is not enough evidence to show that there was a decisive change either in the way in which the Franks fought, or in the way in which they organised the resources needed to support their warriors."

Many twentieth-century European historians continued to develop Gibbon's perspectives, such as French medievalist Christian Pfister, who wrote in 1911 that

Similarly, William E. Watson, who wrote of the battle's importance in Frankish and world history in 1993, suggested that

And in 1993, the influential political scientist Samuel Huntington saw the battle of Tours as marking the end of the "Arab and Moorish surge west and north".

Other recent historians, however, argue that the importance of the battle is dramatically overstated, both for European history in general and for Charles's reign in particular. This view is typified by Alessandro Barbero, who in 2004 wrote,

Similarly, in 2002 Tomaž Mastnak wrote:
More recently, the memory of Charles has been appropriated by far right and white nationalist groups, such as the 'Charles Martel Group' in France, and by the perpetrator of the Christchurch mosque shootings at Al Noor Mosque and Linwood Islamic Centre in Christchurch, New Zealand, in 2019. The memory of Charles is a topic of debate in contemporary French politics on both the right and the left.

In the seventeenth century, a legend emerged that Charles had formed the first regular order of knights in France. In 1620, Andre Favyn stated (without providing a source) that among the spoils Charles's forces captured after the Battle of Tours were many genets (raised for their fur) and several of their pelts. Charles gave these furs to leaders amongst his army, forming the first order of knighthood, the Order of the Genet. Favyn's claim was then repeated and elaborated in later works in English, for instance by Elias Ashmole in 1672, and James Coats in 1725.


 
Charles Edward Jones

Colonel Charles Edward ("Chuck") Jones (November 8, 1952 – September 11, 2001) was a United States Air Force officer, an aeronautical engineer, computer programmer, and an astronaut in the USAF Manned Spaceflight Engineer Program. He was killed during the September 11 attacks, aboard American Airlines Flight 11.

Charles Edward Jones was born November 8, 1952, in Clinton, Indiana. He graduated from Wichita East High School in 1970, earned a Bachelor of Science degree in Astronautical Engineering from the United States Air Force Academy in 1974, and received a Master of Science degree in Astronautics from Massachusetts Institute of Technology in 1980. He entered the USAF Manned Spaceflight Engineer program in 1982, and was scheduled to fly on mission STS-71-B in December 1986, but the mission was canceled after the "Challenger" Disaster in January 1986. He left the Manned Spaceflight Engineer program in 1987.

He later worked for Defense Intelligence Agency, Bolling Air Force Base in Washington, D.C., and was Systems Program Director for Intelligence and Information Systems, Hanscom Air Force Base, Massachusetts. Jones later was the manager of space programs for BAE Systems.

Jones was killed at the age of 48 in the attacks of September 11, 2001, aboard American Airlines Flight 11. Jones was flying that day on a routine business trip for BAE Systems, and had been living as a retired U.S. Air Force Colonel in Bedford, Massachusetts, at the time of his death. He was survived by his wife Jeanette.

At the National 9/11 Memorial, Jones is memorialized at the North Pool, on Panel N-74.

His awards include:



Ceramic

A ceramic is any of the various hard, brittle, heat-resistant, and corrosion-resistant materials made by shaping and then firing an inorganic, nonmetallic material, such as clay, at a high temperature. Common examples are earthenware, porcelain, and brick.

The earliest ceramics made by humans were pottery objects (pots, vessels, or vases) or figurines made from clay, either by itself or mixed with other materials like silica, hardened and sintered in fire. Later, ceramics were glazed and fired to create smooth, colored surfaces, decreasing porosity through the use of glassy, amorphous ceramic coatings on top of the crystalline ceramic substrates. Ceramics now include domestic, industrial, and building products, as well as a wide range of materials developed for use in advanced ceramic engineering, such as semiconductors.

The word "ceramic" comes from the Ancient Greek word (), meaning "of or for pottery" (). The earliest known mention of the root "ceram-" is the Mycenaean Greek , workers of ceramic, written in Linear B syllabic script. The word "ceramic" can be used as an adjective to describe a material, product, or process, or it may be used as a noun, either singular or, more commonly, as the plural noun "ceramics".

Ceramic material is an inorganic, metallic oxide, nitride, or carbide material. Some elements, such as carbon or silicon, may be considered ceramics. Ceramic materials are brittle, hard, strong in compression, and weak in shearing and tension. They withstand the chemical erosion that occurs in other materials subjected to acidic or caustic environments. Ceramics generally can withstand very high temperatures, ranging from 1,000 °C to 1,600 °C (1,800 °F to 3,000 °F).
The crystallinity of ceramic materials varies widely. Most often, fired ceramics are either vitrified or semi-vitrified, as is the case with earthenware, stoneware, and porcelain. Varying crystallinity and electron composition in the ionic and covalent bonds cause most ceramic materials to be good thermal and electrical insulators (researched in ceramic engineering). With such a large range of possible options for the composition/structure of a ceramic (nearly all of the elements, nearly all types of bonding, and all levels of crystallinity), the breadth of the subject is vast, and identifiable attributes (hardness, toughness, electrical conductivity) are difficult to specify for the group as a whole. General properties such as high melting temperature, high hardness, poor conductivity, high moduli of elasticity, chemical resistance, and low ductility are the norm, with known exceptions to each of these rules (piezoelectric ceramics, glass transition temperature, superconductive ceramics).

Composites such as fiberglass and carbon fiber, while containing ceramic materials, are not considered to be part of the ceramic family.

Highly oriented crystalline ceramic materials are not amenable to a great range of processing. Methods for dealing with them tend to fall into one of two categories: either making the ceramic in the desired shape by reaction "in situ" or "forming" powders into the desired shape and then sintering to form a solid body. Ceramic forming techniques include shaping by hand (sometimes including a rotation process called "throwing"), slip casting, tape casting (used for making very thin ceramic capacitors), injection molding, dry pressing, and other variations. 

Many ceramics experts do not consider materials with an amorphous (noncrystalline) character (i.e., glass) to be ceramics, even though glassmaking involves several steps of the ceramic process and its mechanical properties are similar to those of ceramic materials. However, heat treatments can convert glass into a semi-crystalline material known as glass-ceramic. 

Traditional ceramic raw materials include clay minerals such as kaolinite, whereas more recent materials include aluminium oxide, more commonly known as alumina. Modern ceramic materials, which are classified as advanced ceramics, include silicon carbide and tungsten carbide. Both are valued for their abrasion resistance and are therefore used in applications such as the wear plates of crushing equipment in mining operations. Advanced ceramics are also used in the medical, electrical, electronics, and armor industries.

Human beings appear to have been making their own ceramics for at least 26,000 years, subjecting clay and silica to intense heat to fuse and form ceramic materials. The earliest found so far were in southern central Europe and were sculpted figures, not dishes. The earliest known pottery was made by mixing animal products with clay and firing it at up to . While pottery fragments have been found up to 19,000 years old, it was not until about 10,000 years later that regular pottery became common. An early people that spread across much of Europe is named after its use of pottery: the Corded Ware culture. These early Indo-European peoples decorated their pottery by wrapping it with rope while it was still wet. When the ceramics were fired, the rope burned off but left a decorative pattern of complex grooves on the surface.
The invention of the wheel eventually led to the production of smoother, more even pottery using the wheel-forming (throwing) technique, like the pottery wheel. Early ceramics were porous, absorbing water easily. It became useful for more items with the discovery of glazing techniques, which involved coating pottery with silicon, bone ash, or other materials that could melt and reform into a glassy surface, making a vessel less pervious to water. 

Ceramic artifacts have an important role in archaeology for understanding the culture, technology, and behavior of peoples of the past. They are among the most common artifacts to be found at an archaeological site, generally in the form of small fragments of broken pottery called sherds. The processing of collected sherds can be consistent with two main types of analysis: technical and traditional.

The traditional analysis involves sorting ceramic artifacts, sherds, and larger fragments into specific types based on style, composition, manufacturing, and morphology. By creating these typologies, it is possible to distinguish between different cultural styles, the purpose of the ceramic, and the technological state of the people, among other conclusions. Besides, by looking at stylistic changes in ceramics over time, it is possible to separate (seriate) the ceramics into distinct diagnostic groups (assemblages). A comparison of ceramic artifacts with known dated assemblages allows for a chronological assignment of these pieces.

The technical approach to ceramic analysis involves a finer examination of the composition of ceramic artifacts and sherds to determine the source of the material and, through this, the possible manufacturing site. Key criteria are the composition of the clay and the temper used in the manufacture of the article under study: the temper is a material added to the clay during the initial production stage and is used to aid the subsequent drying process. Types of temper include shell pieces, granite fragments, and ground sherd pieces called 'grog'. Temper is usually identified by microscopic examination of the tempered material. Clay identification is determined by a process of refiring the ceramic and assigning a color to it using Munsell Soil Color notation. By estimating both the clay and temper compositions and locating a region where both are known to occur, an assignment of the material source can be made. Based on the source assignment of the artifact, further investigations can be made into the site of manufacture.

The physical properties of any ceramic substance are a direct result of its crystalline structure and chemical composition. Solid-state chemistry reveals the fundamental connection between microstructure and properties, such as localized density variations, grain size distribution, type of porosity, and second-phase content, which can all be correlated with ceramic properties such as mechanical strength σ by the Hall-Petch equation, hardness, toughness, dielectric constant, and the optical properties exhibited by transparent materials.

Ceramography is the art and science of preparation, examination, and evaluation of ceramic microstructures. Evaluation and characterization of ceramic microstructures are often implemented on similar spatial scales to that used commonly in the emerging field of nanotechnology: from nanometers to tens of micrometers (µm). This is typically somewhere between the minimum wavelength of visible light and the resolution limit of the naked eye.

The microstructure includes most grains, secondary phases, grain boundaries, pores, micro-cracks, structural defects, and hardness micro indentions. Most bulk mechanical, optical, thermal, electrical, and magnetic properties are significantly affected by the observed microstructure. The fabrication method and process conditions are generally indicated by the microstructure. The root cause of many ceramic failures is evident in the cleaved and polished microstructure. Physical properties which constitute the field of materials science and engineering include the following:

Mechanical properties are important in structural and building materials as well as textile fabrics. In modern materials science, fracture mechanics is an important tool in improving the mechanical performance of materials and components. It applies the physics of stress and strain, in particular the theories of elasticity and plasticity, to the microscopic crystallographic defects found in real materials in order to predict the macroscopic mechanical failure of bodies. Fractography is widely used with fracture mechanics to understand the causes of failures and also verify the theoretical failure predictions with real-life failures.

Ceramic materials are usually ionic or covalent bonded materials. A material held together by either type of bond will tend to fracture before any plastic deformation takes place, which results in poor toughness in these materials. Additionally, because these materials tend to be porous, the pores and other microscopic imperfections act as stress concentrators, decreasing the toughness further, and reducing the tensile strength. These combine to give catastrophic failures, as opposed to the more ductile failure modes of metals.

These materials do show plastic deformation. However, because of the rigid structure of crystalline material, there are very few available slip systems for dislocations to move, and so they deform very slowly.

To overcome the brittle behavior, ceramic material development has introduced the class of ceramic matrix composite materials, in which ceramic fibers are embedded and with specific coatings are forming fiber bridges across any crack. This mechanism substantially increases the fracture toughness of such ceramics. Ceramic disc brakes are an example of using a ceramic matrix composite material manufactured with a specific process.

Scientists are working on developing ceramic materials that can withstand significant deformation without breaking. A first such material that can deform in room temperature was found in 2024.

If a ceramic is subjected to substantial mechanical loading, it can undergo a process called ice-templating, which allows some control of the microstructure of the ceramic product and therefore some control of the mechanical properties. Ceramic engineers use this technique to tune the mechanical properties to their desired application. Specifically, the strength is increased when this technique is employed. Ice templating allows the creation of macroscopic pores in a unidirectional arrangement. The applications of this oxide strengthening technique are important for solid oxide fuel cells and water filtration devices.

To process a sample through ice templating, an aqueous colloidal suspension is prepared to contain the dissolved ceramic powder evenly dispersed throughout the colloid, for example Yttria-stabilized zirconia (YSZ). The solution is then cooled from the bottom to the top on a platform that allows for unidirectional cooling. This forces ice crystals to grow in compliance with the unidirectional cooling, and these ice crystals force the dissolved YSZ particles to the solidification front of the solid-liquid interphase boundary, resulting in pure ice crystals lined up unidirectionally alongside concentrated pockets of colloidal particles. The sample is then heated and at the same the pressure is reduced enough to force the ice crystals to sublime and the YSZ pockets begin to anneal together to form macroscopically aligned ceramic microstructures. The sample is then further sintered to complete the evaporation of the residual water and the final consolidation of the ceramic microstructure.

During ice-templating, a few variables can be controlled to influence the pore size and morphology of the microstructure. These important variables are the initial solids loading of the colloid, the cooling rate, the sintering temperature and duration, and the use of certain additives which can influence the microstructural morphology during the process. A good understanding of these parameters is essential to understanding the relationships between processing, microstructure, and mechanical properties of anisotropically porous materials.

Some ceramics are semiconductors. Most of these are transition metal oxides that are II-VI semiconductors, such as zinc oxide. While there are prospects of mass-producing blue LEDs from zinc oxide, ceramicists are most interested in the electrical properties that show grain boundary effects. One of the most widely used of these is the varistor. These are devices that exhibit the property that resistance drops sharply at a certain threshold voltage. Once the voltage across the device reaches the threshold, there is a breakdown of the electrical structure in the vicinity of the grain boundaries, which results in its electrical resistance dropping from several megohms down to a few hundred ohms. The major advantage of these is that they can dissipate a lot of energy, and they self-reset; after the voltage across the device drops below the threshold, its resistance returns to being high. This makes them ideal for surge-protection applications; as there is control over the threshold voltage and energy tolerance, they find use in all sorts of applications. The best demonstration of their ability can be found in electrical substations, where they are employed to protect the infrastructure from lightning strikes. They have rapid response, are low maintenance, and do not appreciably degrade from use, making them virtually ideal devices for this application. Semiconducting ceramics are also employed as gas sensors. When various gases are passed over a polycrystalline ceramic, its electrical resistance changes. With tuning to the possible gas mixtures, very inexpensive devices can be produced.

Under some conditions, such as extremely low temperatures, some ceramics exhibit high-temperature superconductivity. The reason for this is not understood, but there are two major families of superconducting ceramics.

Piezoelectricity, a link between electrical and mechanical response, is exhibited by a large number of ceramic materials, including the quartz used to measure time in watches and other electronics. Such devices use both properties of piezoelectrics, using electricity to produce a mechanical motion (powering the device) and then using this mechanical motion to produce electricity (generating a signal). The unit of time measured is the natural interval required for electricity to be converted into mechanical energy and back again.

The piezoelectric effect is generally stronger in materials that also exhibit pyroelectricity, and all pyroelectric materials are also piezoelectric. These materials can be used to inter-convert between thermal, mechanical, or electrical energy; for instance, after synthesis in a furnace, a pyroelectric crystal allowed to cool under no applied stress generally builds up a static charge of thousands of volts. Such materials are used in motion sensors, where the tiny rise in temperature from a warm body entering the room is enough to produce a measurable voltage in the crystal.

In turn, pyroelectricity is seen most strongly in materials that also display the ferroelectric effect, in which a stable electric dipole can be oriented or reversed by applying an electrostatic field. Pyroelectricity is also a necessary consequence of ferroelectricity. This can be used to store information in ferroelectric capacitors, elements of ferroelectric RAM.

The most common such materials are lead zirconate titanate and barium titanate. Aside from the uses mentioned above, their strong piezoelectric response is exploited in the design of high-frequency loudspeakers, transducers for sonar, and actuators for atomic force and scanning tunneling microscopes.

Temperature increases can cause grain boundaries to suddenly become insulating in some semiconducting ceramic materials, mostly mixtures of heavy metal titanates. The critical transition temperature can be adjusted over a wide range by variations in chemistry. In such materials, current will pass through the material until joule heating brings it to the transition temperature, at which point the circuit will be broken and current flow will cease. Such ceramics are used as self-controlled heating elements in, for example, the rear-window defrost circuits of automobiles.

At the transition temperature, the material's dielectric response becomes theoretically infinite. While a lack of temperature control would rule out any practical use of the material near its critical temperature, the dielectric effect remains exceptionally strong even at much higher temperatures. Titanates with critical temperatures far below room temperature have become synonymous with "ceramic" in the context of ceramic capacitors for just this reason.

Optically transparent materials focus on the response of a material to incoming light waves of a range of wavelengths. Frequency selective optical filters can be utilized to alter or enhance the brightness and contrast of a digital image. Guided lightwave transmission via frequency selective waveguides involves the emerging field of fiber optics and the ability of certain glassy compositions as a transmission medium for a range of frequencies simultaneously (multi-mode optical fiber) with little or no interference between competing wavelengths or frequencies. This resonant mode of energy and data transmission via electromagnetic (light) wave propagation, though low powered, is virtually lossless. Optical waveguides are used as components in Integrated optical circuits (e.g. light-emitting diodes, LEDs) or as the transmission medium in local and long haul optical communication systems. Also of value to the emerging materials scientist is the sensitivity of materials to radiation in the thermal infrared (IR) portion of the electromagnetic spectrum. This heat-seeking ability is responsible for such diverse optical phenomena as night-vision and IR luminescence.

Thus, there is an increasing need in the military sector for high-strength, robust materials which have the capability to transmit light (electromagnetic waves) in the visible (0.4 – 0.7 micrometers) and mid-infrared (1 – 5 micrometers) regions of the spectrum. These materials are needed for applications requiring transparency and translucency transparent armor, including next-generation high-speed missiles and pods, as well as protection against improvised explosive devices (IED).

In the 1960s, scientists at General Electric (GE) discovered that under the right manufacturing conditions, some ceramics, especially aluminium oxide (alumina), could be made translucent. These translucent materials were transparent enough to be used for containing the electrical plasma generated in high-pressure sodium street lamps. During the past two decades, additional types of transparent ceramics have been developed for applications such as nose cones for heat-seeking missiles, windows for fighter aircraft, and scintillation counters for computed tomography scanners.
Other ceramic materials, generally requiring greater purity in their make-up than those above, include forms of several chemical compounds, including:


For convenience, ceramic products are usually divided into four main types; these are shown below with some examples:

Frequently, the raw materials of modern ceramics do not include clays.
Those that do have been classified as:

Ceramics can also be classified into three distinct material categories: 

Each one of these classes can be developed into unique material properties.




Wuxing (Chinese philosophy)

The agents are Fire, Water, Wood, Metal, and Earth. The "wuxing" system has been in use since it was formulated in the second or first century BCE during the Han dynasty. It appears in many seemingly disparate fields of early Chinese thought, including music, feng shui, alchemy, astrology, martial arts, military strategy, "I Ching" divination, and traditional medicine, serving as a metaphysics based on cosmic analogy.

"Wuxing" originally referred to the five major planets (Jupiter, Saturn, Mercury, Mars, Venus), which were conceived as creating five forces of earthly life. This is why the word is composed of Chinese characters meaning "five" () and "moving" (). "Moving" is shorthand for "planets", since the word for planets in Chinese literally translates as "moving stars" (). Some of the Mawangdui Silk Texts (before 168 BC) also connect the "wuxing" to the "wude" (), the Five Virtues and Five Emotions. Scholars believe that various predecessors to the concept of "wuxing" were merged into one system with many interpretations during the Han dynasty.

"Wuxing" was first translated into English as "the Five Elements", drawing deliberate parallels with the Western idea of the four elements. This translation is still in common use among practitioners of Traditional Chinese medicine, such as in the name of Five Element acupuncture. However, this analogy is misleading. The four elements are concerned with form, substance and quantity, whereas "wuxing" are "primarily concerned with process, change, and quality". For example, the "wuxing" element "Wood" is more accurately thought of as the "vital essence" of trees rather than the physical substance wood. This led sinologist Nathan Sivin to propose the alternative translation "five phases" in 1987. But "phase" also fails to capture the full meaning of "wuxing". In some contexts, the "wuxing" are indeed associated with physical substances. Historian of Chinese medicine Manfred Porkert proposed the (somewhat unwieldy) term "Evolutive Phase". Perhaps the most widely accepted translation among modern scholars is "the five agents", proposed by Marc Kalinowski.

In traditional doctrine, the five phases are connected in two cycles of interactions: a generating or creation ( "shēng") cycle, also known as "mother-son"; and an overcoming or destructive ( "kè") cycle, also known as "grandfather-grandson" (see diagram). Each of the two cycles can be analyzed going forward or reversed. There is also an "overacting" or excessive version of the destructive cycle.

The generating cycle ( "xiāngshēng") is:

The reverse generating cycle (/ "xiāngxiè") is:

The destructive cycle ( "xiāngkè") is:

The excessive destructive cycle ( "xiāngchéng") is:

A reverse or deficient destructive cycle ( "xiāngwǔ" or "xiānghào") is:

In Ziwei divination, "neiyin" () further classifies the Five Elements into 60 "ming" (), or life orders, based on the ganzhi. Similar to the astrology zodiac, the "ming" is used by fortune-tellers to analyse individual personality and destiny.

The "wuxing" schema is applied to explain phenomena in various fields.

The five phases are around 73 days each and are usually used to describe the transformations of nature rather than their formative states.

The art of feng shui (Chinese geomancy) is based on "wuxing", with the structure of the cosmos mirroring the five phases, as well as the eight trigrams. Each phase has a complex network of associations with different aspects of nature (see table): colors, seasons and shapes all interact according to the cycles.

An interaction or energy flow can be expansive, destructive, or exhaustive, depending on the cycle to which it belongs. By understanding these energy flows, a feng shui practitioner attempts to rearrange energy to benefit the client.

According to the Warring States period political philosopher Zou Yan ( BCE), each of the five elements possesses a personified virtue (), which indicates the foreordained destiny () of a dynasty; hence the cyclic succession of the elements also indicates dynastic transitions. Zou Yan claims that the Mandate of Heaven sanctions the legitimacy of a dynasty by sending self-manifesting auspicious signs in the ritual color (yellow, blue, white, red, and black) that matches the element of the new dynasty (Earth, Wood, Metal, Fire, and Water). From the Qin dynasty onward, most Chinese dynasties invoked the theory of the Five Elements to legitimize their reign.

The interdependence of "zangfu" networks in the body was said to be a circle of five things, and so mapped by the Chinese doctors onto the five phases.

In order to explain the integrity and complexity of the human body, Chinese medical scientists and physicians use the Five Elements theory to classify the human body's endogenous influences on organs, physiological activities, pathological reactions, and environmental or exogenous influences. This diagnostic capacity is extensively used in traditional five phase acupuncture today, as opposed to the modern eight principles based Traditional Chinese medicine. Furthermore, in combination the two systems are the study of postnatal and prenatal influencing on genetics, psychology and sociology.

The "Huainanzi" and the "Yueling" chapter () of the "Book of Rites" make the following correlations:


Tai chi uses the five elements to designate different directions, positions or footwork patterns: forward, backward, left, right and centre, or three steps forward (attack) and two steps back (retreat).

The Five Steps ():

The martial art of "xingyiquan" uses the five elements metaphorically to represent five different states of combat.
"Wuxing heqidao", Gogyo Aikido (五行合气道) is a life art with roots in Confucian, Taoists and Buddhist theory. It centers around applied peace and health studies rather than defence or physical action. It emphasizes the unification of mind, body and environment using the physiological theory of yin, yang and five-element Traditional Chinese medicine. Its movements, exercises, and teachings cultivate, direct, and harmonise the "qi".

The Japanese term is "gogyo" (Japanese:五行, gogyō). During the 5th and 6th centuries (Kofun period), Japan adopted various philosophical disciplines such as Taoism, Chinese Buddhism and Confucianism through monks and physicians from China. In particular, "wuxing" was adapted into gogyo. These theories have been extensively practiced in Japanese acupuncture and traditional Kampo medicine.




Church of Christ, Scientist

The Church of Christ, Scientist was founded in 1879 in Boston, Massachusetts, by Mary Baker Eddy, author of "Science and Health with Key to the Scriptures," and founder of Christian Science. The church was founded "to commemorate the word and works of Christ Jesus" and "reinstate primitive Christianity and its lost element of healing".

In the early decades of the 20th century, Christian Science churches were founded in communities around the world, though in the last several decades of that century, there was a marked decline in membership, except in Africa, where there has been growth. Headquartered in Boston, the church does not officially report membership, and estimates as to worldwide membership range from under 100,000 to about 400,000.

The church was incorporated by Mary Baker Eddy in 1879, following a claimed personal healing in 1866, which she said resulted from reading the Bible. The Bible and Eddy's textbook on Christian healing, "Science and Health with Key to the Scriptures", are together the church's key doctrinal sources and have been ordained as the church's "dual impersonal pastor".

The First Church of Christ, Scientist publishes the weekly newspaper "The Christian Science Monitor" in print and online.

Christian Scientists believe that prayer is effective for healing diseases. The Church has collected over 50,000 testimonies of incidents that it considers healing through Christian Science treatment alone. While most of these testimonies represent ailments neither diagnosed nor treated by medical professionals, the Church requires three other people to vouch for any testimony published in any of its official organs, including the "Christian Science Journal", "Christian Science Sentinel", and "Herald of Christian Science"; verifiers say that they witnessed the healing or know the testifier well enough to vouch for them.

Christian Scientists may take an intensive two-week "Primary" class from an authorized Christian Science teacher. Those who wish to become "Journal-listed" (accredited) practitioners, devoting themselves full-time to the practice of healing, must first have Primary class instruction. When they have what the church regards as a record of healing, they may submit their names for publication in the directory of practitioners and teachers in the "Christian Science Journal". A practitioner who has been listed for at least three years may apply for "Normal" class instruction, given once every three years. Those who receive a certificate are authorized to teach. Both Primary and Normal classes are based on the Bible and the writings of Mary Baker Eddy. The Primary class focuses on the chapter "Recapitulation" in "Science and Health with Key to the Scriptures". This chapter uses the Socratic method of teaching and contains the "Scientific Statement of Being". The "Normal" class focuses on the platform of Christian Science, contained on pages 330-340 of "Science and Health."

The First Church of Christ, Scientist is the legal title of The Mother Church and administrative headquarters of the Christian Science Church. The Mary Baker Eddy Library for the Betterment of Humanity is housed in an 11-story structure originally built for The Christian Science Publishing Society.

An international newspaper, "The Christian Science Monitor", founded by Eddy in 1908 and winner of seven Pulitzer prizes, is published by the church through the Christian Science Publishing Society.

The Christian Science Board of Directors is a five-person executive entity created by Mary Baker Eddy to conduct the business of the Christian Science Church under the terms defined in the by-laws of the "Church Manual". Its functions and restrictions are defined by the "Manual".

Beginning in the mid-1980s, church executives undertook a controversial and ambitious foray into electronic broadcast media. The first significant effort was to create a weekly half-hour syndicated television program, "The Christian Science Monitor" Reports. "Monitor Reports" was anchored in its first season by newspaper veteran Rob Nelson. He was replaced in the second by the "Christian Science Monitor"'s former Moscow correspondent, David Willis.

In October 1991, after a series of conflicts over the boundaries between Christian Science teachings and his journalistic independence, John Hart resigned.

The hundreds of millions lost on broadcasting brought the church to the brink of bankruptcy. However, with the 1991 publication of "The Destiny of The Mother Church" by the late Bliss Knapp, the church secured a $90 million bequest from the Knapp trust. The trust dictated that the book be published as "Authorized Literature", with neither modification nor comment. Historically, the church had censured Knapp for deviating at several points from Eddy's teaching, and had refused to publish the work. The church's archivist, fired in anticipation of the book's publication, wrote to branch churches to inform them of the book's history. Many Christian Scientists thought the book violated the church's by-laws, and the editors of the church's religious periodicals and several other church employees resigned in protest. Alternate beneficiaries subsequently sued to contest the church's claim it had complied fully with the will's terms, and the church ultimately received only half of the original sum.

The fallout of the broadcasting debacle also sparked a minor revolt among some prominent church members. In late 1993, a group of Christian Scientists filed suit against the Board of Directors, alleging a willful disregard for the "Manual of The Mother Church" in its financial dealings. The suit was thrown out by the Supreme Judicial Court of Massachusetts in 1997, but a lingering discontent with the church's financial matters persists to this day." The Destiny Of The Mother Church" ceased publication in September 2023.

In spite of its early meteoric rise, church membership has declined over the past eight decades, according to the church's former treasurer, J. Edward Odegaard. Though the Church is prohibited by the Manual from publishing membership figures, the number of branch churches in the United States has fallen steadily since World War II. In 2009, for the first time in church history, more new members came from Africa than the United States.

In 2005, "The Boston Globe" reported that the church was considering consolidating Boston operations into fewer buildings and leasing out space in buildings it owned. Church official Philip G. Davis noted that the administration and Colonnade buildings had not been fully used for many years and that vacancy increased after staff reductions in 2004. The church posted an $8 million financial loss in fiscal 2003, and in 2004 cut 125 jobs, a quarter of the staff, at the "Christian Science Monitor". Conversely, Davis noted that "the financial situation right now is excellent" and stated that the church was not facing financial problems.

In the United States, Christian Scientist parents whose children have died for lack of access to medical treatment have been the subject of considerable controversy. At least 50 Christian Scientists have been charged with manslaughter or even murder of children whose illnesses were otherwise curable using standard medical techniques. The outcomes of these cases have been inconsistent. Some courts have held that parents are free to refuse treatment for themselves on religious grounds, but cannot refuse treatment for their children, while others have found that religious liberty empowers parents to forego seeking medical care for a child in favor of spiritual healing. 

The lack of consensus on whether Christian Scientist parents can be compelled to obtain medical care for their children is reflected in the laws of various U.S. states. As of 2016, 34 states and the District of Columbia recognized religious exemptions to state child neglect laws. Of those 34, 16 permitted courts to order treatment. A total of 16 states did not recognize any religious exemption. 



Connecticut

Connecticut ( ) is the southernmost state in the New England region of the Northeastern United States. It borders Rhode Island to its east, Massachusetts to its north, New York to its west, and Long Island Sound to its south. Its capital is Hartford, and its most populous city is Bridgeport. Historically, the state is part of New England as well as the tri-state area with New York and New Jersey. The state is named for the Connecticut River which approximately bisects the state. The word "Connecticut" is derived from various anglicized spellings of , a Mohegan-Pequot word for "long tidal river". As of the 2020 United States census, Connecticut was home to over 3.6 million residents, its highest decennial count ever, growing every decade since 1790.

Connecticut's first European settlers were Dutchmen who established a small, short-lived settlement called House of Hope in Hartford at the confluence of the Park and Connecticut Rivers. Half of Connecticut was initially claimed by the Dutch colony New Netherland, which included much of the land between the Connecticut and Delaware Rivers, although the first major settlements were established in the 1630s by the English. Thomas Hooker led a band of followers from the Massachusetts Bay Colony and founded the Connecticut Colony; other settlers from Massachusetts founded the Saybrook Colony and the New Haven Colony. The Connecticut and New Haven colonies established documents of Fundamental Orders, considered the first constitutions in America. In 1662, the three colonies were merged under a royal charter, making Connecticut a crown colony. Connecticut was one of the Thirteen Colonies which rejected British rule in the American Revolution. It was influential in the development of the federal government of the United States.

Connecticut is the third-smallest state by area, the 29th most populous, and the fourth most densely populated of the fifty states. It is known as the "Constitution State", the "Nutmeg State", the "Provisions State", and the "Land of Steady Habits". The state identifies as creators, makers, innovators, and entrepreneurs who are a powerful force for good in the country. The state logo is the iconic C+T, with the T recognizable sideways making up the negative space in the C. The Connecticut River, Thames River, and ports along Long Island Sound have given Connecticut a strong maritime tradition which continues today. Connecticut is home to the nation's oldest newspaper, The Hartford Courant, founded in 1764. The state also has a long history of hosting the financial services industry, including insurance companies in Hartford County and hedge funds in Fairfield County. As of the 2010 census, it has the highest per-capita income, second-highest level of human development behind Massachusetts, and highest median household income in the United States.

The name Connecticut is derived from the Mohegan-Pequot word that has been translated as "long tidal river" and "upon the long river", both referring to the Connecticut River. Evidence of human presence in the Connecticut region dates to as far back as 10,000 years ago. Stone tools were used for hunting, fishing, and woodworking. Semi-nomadic in lifestyle, these peoples moved seasonally to take advantage of various resources in the area. They shared languages based on Algonquian. The Connecticut region was inhabited by multiple Native American tribes which can be grouped into the Nipmuc, the Sequin or "River Indians" (which included the Tunxis, Schaghticoke, Podunk, Wangunk, Hammonasset, and Quinnipiac), the Mattabesec or "Wappinger Confederacy" and the Pequot-Mohegan. Some of these groups still reside in Connecticut, including the Mohegans, the Pequots, and the Paugusetts.

The first European explorer in Connecticut was Dutchman Adriaen Block, who explored the region in 1614. Dutch fur traders then sailed up the Connecticut River, which they called Versche Rivier ("Fresh River"), and built a fort at Dutch Point in Hartford that they named "House of Hope" ().
The Connecticut Colony was originally a number of separate, smaller settlements at Windsor, Wethersfield, Saybrook, Hartford, and New Haven. The first English settlers came in 1633 and settled at Windsor, and then at Wethersfield the following year. John Winthrop the Younger of Massachusetts received a commission to create Saybrook Colony at the mouth of the Connecticut River in 1635.

The main body of settlers came in one large group in 1636. They were Puritans from Massachusetts Bay Colony led by Thomas Hooker, who established the Connecticut Colony at Hartford. The Fundamental Orders of Connecticut were adopted in January 1639, and have been described as the first constitutional document in America.

The Quinnipiack Colony was established by John Davenport, Theophilus Eaton, and others at New Haven in March 1638. The New Haven Colony had its own constitution called "The Fundamental Agreement of the New Haven Colony", signed on June 4, 1639.

The settlements were established without official sanction of the English Crown, and each was an independent political entity. In 1662, Winthrop traveled to England and obtained a charter from CharlesII which united the settlements of Connecticut. Historically important colonial settlements included Windsor (1633), Wethersfield (1634), Saybrook (1635), Hartford (1636), New Haven (1638), Fairfield (1639), Guilford (1639), Milford (1639), Stratford (1639), Farmington (1640), Stamford (1641), and New London (1646).

The Pequot War marked the first major clash between colonists and Native Americans in New England. The Pequots reacted with increasing aggression to Colonial settlements in their territory—while simultaneously taking lands from the Narragansett and Mohegan tribes. Settlers responded to a murder in 1636 with a raid on a Pequot village on Block Island; the Pequots laid siege to Saybrook Colony's garrison that autumn, then raided Wethersfield in the spring of 1637. Colonists declared war on the Pequots, organized a band of militia and allies from the Mohegan and Narragansett tribes, and attacked a Pequot village on the Mystic River, with death toll estimates ranging between 300 and 700 Pequots. After suffering another major loss at a battle in Fairfield, the Pequots asked for a truce and peace terms.

The western boundaries of Connecticut have been subject to change over time. The Hartford Treaty with the Dutch was signed on September 19, 1650, but it was never ratified by the British. According to it, the western boundary of Connecticut ran north from Greenwich Bay for a distance of , "provided the said line come not within of Hudson River". This agreement was observed by both sides until war erupted between England and The Netherlands in 1652. Conflict continued concerning colonial limits until the Duke of York captured New Netherland in 1664.

On the other hand, Connecticut's original Charter in 1662 granted it all the land to the "South Sea"—that is, to the Pacific Ocean. Most Colonial royal grants were for long east–west strips. Connecticut took its grant seriously and established a ninth county between the Susquehanna River and Delaware River named Westmoreland County. This resulted in the brief Pennamite Wars with Pennsylvania.

Yale College was established in 1701, providing Connecticut with an important institution to educate clergy and civil leaders. The Congregational church dominated religious life in the colony and, by extension, town affairs in many parts.

With more than of coastline including along its navigable rivers, Connecticut developed during its colonial years the antecedents of a maritime tradition that would later produce booms in shipbuilding, marine transport, naval support, seafood production, and leisure boating.

Historical records list the "Tryall" as the first vessel built in Connecticut Colony, in 1649 at a site on the Connecticut River in present-day Wethersfield. In the two decades leading up to 1776 and the American Revolution, Connecticut boatyards launched about 100 sloops, schooners and brigs according to a database of U.S. customs records maintained online by the Mystic Seaport Museum, the largest being the 180-ton "Patient Mary" launched in New Haven in 1763. Connecticut's first lighthouse was constructed in 1760 at the mouth of the Thames River with the New London Harbor Lighthouse.

Connecticut designated four delegates to the Second Continental Congress who signed the Declaration of Independence: Samuel Huntington, Roger Sherman, William Williams, and Oliver Wolcott. Connecticut's legislature authorized the outfitting of six new regiments in 1775, in the wake of the clashes between British regulars and Massachusetts militia at Lexington and Concord. There were some 1,200 Connecticut troops on hand at the Battle of Bunker Hill in June 1775. In 1775, David Bushnell invented the "Turtle" which the following year launched the first submarine attack in history, unsuccessfully against a British warship at anchor in New York Harbor.

In 1777, the British got word of Continental Army supplies in Danbury, and they landed an expeditionary force of some 2,000 troops in Westport. This force then marched to Danbury and destroyed homes and much of the depot. Continental Army troops and militia led by General David Wooster and General Benedict Arnold engaged them on their return march at Ridgefield in 1777. For the winter of 1778–79, General George Washington decided to split the Continental Army into three divisions encircling New York City, where British General Sir Henry Clinton had taken up winter quarters. Major General Israel Putnam chose Redding as the winter encampment quarters for some 3,000 regulars and militia under his command. The Redding encampment allowed Putnam's soldiers to guard the replenished supply depot in Danbury and to support any operations along Long Island Sound and the Hudson River Valley. Some of the men were veterans of the winter encampment at Valley Forge, Pennsylvania, the previous winter. Soldiers at the Redding camp endured supply shortages, cold temperatures, and significant snow, with some historians dubbing the encampment "Connecticut's Valley Forge".

The state was also the launching site for a number of raids against Long Island orchestrated by Samuel Holden Parsons and Benjamin Tallmadge, and provided soldiers and material for the war effort, especially to Washington's army outside New York City. General William Tryon raided the Connecticut coast in July 1779, focusing on New Haven, Norwalk, and Fairfield. New London and Groton Heights were raided in September 1781 by Benedict Arnold, who had turned traitor to the British.

At the outset of the American Revolution, the Continental Congress assigned Nathaniel Shaw Jr. of New London as its naval agent in charge of recruiting privateers to seize British vessels as opportunities presented, with nearly 50 operating out of the Thames River which eventually drew the reprisal from the British force led by Arnold.

Connecticut ratified the U.S. Constitution on January 9, 1788, becoming the fifth state.

The state prospered during the era following the American Revolution, as mills and textile factories were built and seaports flourished from trade and fisheries. After Congress established in 1790 the predecessor to the U.S. Revenue Cutter Service that would evolve into the U.S. Coast Guard, President Washington assigned Jonathan Maltbie as one of seven masters to enforce customs regulations, with Maltbie monitoring the southern New England coast with a 48-foot cutter sloop named "Argus".

In 1786, Connecticut ceded territory to the U.S. government that became part of the Northwest Territory. The state retained land extending across the northern part of present-day Ohio called the Connecticut Western Reserve. The Western Reserve section was settled largely by people from Connecticut, and they brought Connecticut place names to Ohio.

Connecticut made agreements with Pennsylvania and New York which extinguished the land claims within those states' boundaries and created the Connecticut Panhandle. The state then ceded the Western Reserve in 1800 to the federal government, which brought it to its present boundaries (other than minor adjustments with Massachusetts).

For the first time in 1800, Connecticut shipwrights launched more than 100 vessels in a single year. Over the following decade to the doorstep of renewed hostilities with Britain that sparked the War of 1812, Connecticut boatyards constructed close to 1,000 vessels, the most productive stretch of any decade in the 19th century.

During the war, the British launched raids in Stonington and Essex and blockaded vessels in the Thames River. Derby native Isaac Hull became Connecticut's best-known naval figure to win renown during the conflict, as captain of the .

The British blockade during the War of 1812 hurt exports and bolstered the influence of Federalists who opposed the war. The cessation of imports from Britain stimulated the construction of factories to manufacture textiles and machinery. Connecticut came to be recognized as a major center for manufacturing, due in part to the inventions of Eli Whitney and other early innovators of the Industrial Revolution.

The war led to the development of fast clippers that helped extend the reach of New England merchants to the Pacific and Indian oceans. The first half of the 19th century saw as well a rapid rise in whaling, with New London emerging as one of the New England industry's three biggest home ports after Nantucket and New Bedford.

The state was known for its political conservatism, typified by its Federalist party and the Yale College of Timothy Dwight. The foremost intellectuals were Dwight and Noah Webster, who compiled his great dictionary in New Haven. Religious tensions polarized the state, as the Congregational Church struggled to maintain traditional viewpoints, in alliance with the Federalists. The failure of the Hartford Convention in 1814 hurt the Federalist cause, with the Democratic-Republican Party gaining control in 1817.

Connecticut had been governed under the "Fundamental Orders" since 1639, but the state adopted a new constitution in 1818.

Connecticut manufacturers played a major role in supplying the Union forces with weapons and supplies during the Civil War. The state furnished 55,000 men, formed into thirty full regiments of infantry, including two in the U.S. Colored Troops, with several Connecticut men becoming generals. The Navy attracted 250 officers and 2,100 men, and Glastonbury native Gideon Welles was Secretary of the Navy. James H. Ward of Hartford was the first U.S. Naval Officer killed in the Civil War. Connecticut casualties included 2,088 killed in combat, 2,801 dying from disease, and 689 dying in Confederate prison camps.

A surge of national unity in 1861 brought thousands flocking to the colors from every town and city. However, as the war became a crusade to end slavery, many Democrats (especially Irish Catholics) pulled back. The Democrats took a pro-slavery position and included many Copperheads willing to let the South secede. The intensely fought 1863 election for governor was narrowly won by the Republicans.

Connecticut's extensive industry, dense population, flat terrain, and wealth encouraged the construction of railroads starting in 1839. By 1840, of line were in operation, growing to in 1850 and in 1860.

The New York, New Haven and Hartford Railroad, called the "New Haven" or "The Consolidated", became the dominant Connecticut railroad company after 1872. J. P. Morgan began financing the major New England railroads in the 1890s, dividing territory so that they would not compete. The New Haven purchased 50 smaller companies, including steamship lines, and built a network of light rails (electrified trolleys) that provided inter-urban transportation for all of southern New England. By 1912, the New Haven operated over of track with 120,000 employees.

As steam-powered passenger ships proliferated after the Civil War, Noank would produce the two largest built in Connecticut during the 19th century, with the 332-foot wooden steam paddle wheeler "Rhode Island" launched in 1882, and the 345-foot paddle wheeler "Connecticut" seven years later. Connecticut shipyards would launch more than 165 steam-powered vessels in the 19th century.

In 1875, the first telephone exchange in the world was established in New Haven.

When World War I broke out in 1914, Connecticut became a major supplier of weaponry to the U.S. military; by 1918, 80% of the state's industries were producing goods for the war effort. Remington Arms in Bridgeport produced half the small-arms cartridges used by the U.S. Army, with other major suppliers including Winchester in New Haven and Colt in Hartford.

Connecticut was also an important U.S. Navy supplier, with Electric Boat receiving orders for 85 submarines, Lake Torpedo Boat building more than 20 subs, and the Groton Iron Works building freighters. On June 21, 1916, the Navy made Groton the site for its East Coast submarine base and school.

The state enthusiastically supported the American war effort in 1917 and 1918 with large purchases of war bonds, a further expansion of industry, and an emphasis on increasing food production on the farms. Thousands of state, local, and volunteer groups mobilized for the war effort and were coordinated by the Connecticut State Council of Defense. Manufacturers wrestled with manpower shortages; Waterbury's American Brass and Manufacturing Company was running at half capacity, so the federal government agreed to furlough soldiers to work there.

In 1919, J. Henry Roraback started the Connecticut Light & Power Co. which became the state's dominant electric utility. In 1925, Frederick Rentschler spurred the creation of Pratt & Whitney in Hartford to develop engines for aircraft; the company became an important military supplier in World WarII and one of the three major manufacturers of jet engines in the world.

On September 21, 1938, the most destructive storm in New England history struck eastern Connecticut, killing hundreds of people. The eye of the "Long Island Express" passed just west of New Haven and devastated the Connecticut shoreline between Old Saybrook and Stonington from the full force of wind and waves, even though they had partial protection by Long Island. The hurricane caused extensive damage to infrastructure, homes, and businesses. In New London, a sailing ship was driven into a warehouse complex, causing a major fire. Heavy rainfall caused the Connecticut River to flood downtown Hartford and East Hartford. An estimated 50,000 trees fell onto roadways.

The advent of lend-lease in support of Britain helped lift Connecticut from the Great Depression, with the state a major production center for weaponry and supplies used in World WarII. Connecticut manufactured 4.1% of total U.S. military armaments produced during the war, ranking ninth among the 48 states, with major factories including Colt for firearms, Pratt & Whitney for aircraft engines, Chance Vought for fighter planes, Hamilton Standard for propellers, and Electric Boat for submarines and PT boats. In Bridgeport, General Electric produced a significant new weapon to combat tanks: the bazooka.

On May 13, 1940, Igor Sikorsky made an untethered flight of the first practical helicopter. The helicopter saw limited use in World War II, but future military production made Sikorsky Aircraft's Stratford plant Connecticut's largest single manufacturing site by the start of the 21st century.

Connecticut lost some wartime factories following the end of hostilities, but the state shared in a general post-war expansion that included the construction of highways and resulting in middle-class growth in suburban areas.

Prescott Bush represented Connecticut in the U.S. Senate from 1952 to 1963; his son George H. W. Bush and grandson George W. Bush both became presidents of the United States. In 1965, Connecticut ratified its current constitution, replacing the document that had served since 1818.

In 1968, commercial operation began for the Connecticut Yankee Nuclear Power Plant in Haddam; in 1970, the Millstone Nuclear Power Station began operations in Waterford. In 1974, Connecticut elected Democratic Governor Ella T. Grasso, who became the first woman in any state to be elected governor without being the wife or widow of a previous governor.

Connecticut's dependence on the defense industry posed an economic challenge at the end of the Cold War. The resulting budget crisis helped elect Lowell Weicker as governor on a third-party ticket in 1990. Weicker's remedy was a state income tax which proved effective in balancing the budget, but only for the short-term. He did not run for a second term, in part because of this politically unpopular move.

In 1992, initial construction was completed on Foxwoods Casino at the Mashantucket Pequots reservation in eastern Connecticut, which became the largest casino in the Western Hemisphere. Mohegan Sun followed four years later.

In 2000, presidential candidate Al Gore chose Senator Joe Lieberman as his running mate, marking the first time that a major party presidential ticket included someone of the Jewish faith. Gore and Lieberman fell five votes short of George W. Bush and Dick Cheney in the Electoral College. In the terrorist attacks of September 11, 2001, 65 state residents were killed, mostly Fairfield County residents who were working in the World Trade Center. In 2004, Republican Governor John G. Rowland resigned during a corruption investigation, later pleading guilty to federal charges.

Connecticut was hit by three major storms in just over 14 months in 2011 and 2012, with all three causing extensive property damage and electric outages. Hurricane Irene struck Connecticut August 28, and damage totaled $235 million. Two months later, the "Halloween nor'easter" dropped extensive snow onto trees, resulting in snapped branches and trunks that damaged power lines; some areas were without electricity for 11 days. Hurricane Sandy hit New Jersey and passed over Connecticut with hurricane-force winds and tides up to 12 feet above normal. Many coastal buildings were damaged or destroyed. . Sandy's winds drove storm surges into streets and cut power to 98% of homes and businesses, with more than $360 million in damage.

On December 14, 2012, Adam Lanza shot and killed 26 people at Sandy Hook Elementary School in Newtown, and then killed himself. The massacre spurred renewed efforts by activists for tighter laws on gun ownership nationally.

In the summer and fall of 2016, Connecticut experienced a drought in many parts of the state, causing some water-use bans. As of , 45% of the state was listed at Severe Drought by the U.S. Drought Monitor, including almost all of Hartford and Litchfield counties. All the rest of the state was in Moderate Drought or Severe Drought, including Middlesex, Fairfield, New London, New Haven, Windham, and Tolland counties. This affected the agricultural economy in the state.

Connecticut is bordered on the south by Long Island Sound, on the west by New York, on the north by Massachusetts, and on the east by Rhode Island. The state capital and fourth largest city is Hartford, and other major cities and towns (by population) include Bridgeport, New Haven, Stamford, Waterbury, Norwalk, Danbury, New Britain, Greenwich, and Bristol. There are 169 incorporated towns in Connecticut, with cities and villages included within some towns.
The highest peak in Connecticut is Bear Mountain in Salisbury in the northwest corner of the state. The highest point is just east of where Connecticut, Massachusetts, and New York meet (42°3′ N, 73°29′ W), on the southern slope of Mount Frissell, whose peak lies nearby in Massachusetts. At the opposite extreme, many of the coastal towns have areas that are less than above sea level.

Connecticut has a long maritime history and a reputation based on that history—yet the state has no direct oceanfront (technically speaking). The coast of Connecticut sits on Long Island Sound, which is an estuary. The state's access to the open Atlantic Ocean is both to the west (toward New York City) and to the east (toward the "race" near Rhode Island). Due to this unique geography, Long Island Sound and the Connecticut shoreline are relatively protected from high waves from storms.

The Connecticut River cuts through the center of the state, flowing into Long Island Sound. The most populous metropolitan region centered within the state lies in the Connecticut River Valley. Despite Connecticut's relatively small size, it features wide regional variations in its landscape; for example, in the northwestern Litchfield Hills, it features rolling mountains and horse farms, whereas in areas to the east of New Haven along the coast, the landscape features coastal marshes, beaches, and large scale maritime activities.

Connecticut's rural areas and small towns in the northeast and northwest corners of the state contrast sharply with its industrial cities such as Stamford, Bridgeport, and New Haven, located along the coastal highways from the New York border to New London, then northward up the Connecticut River to Hartford. Many towns in northeastern and northwestern Connecticut center around a green. Near the green typically stand historical visual symbols of New England towns, such as a white church, a colonial meeting house, a colonial tavern or inn, several colonial houses, and so on, establishing a scenic historical appearance maintained for both historic preservation and tourism. Many of the areas in southern and coastal Connecticut have been built up and rebuilt over the years, and look less visually like traditional New England.

The northern boundary of the state with Massachusetts is marked by the Southwick Jog or Granby Notch, an approximately square detour into Connecticut. The origin of this anomaly is clearly established in a long line of disputes and temporary agreements which were finally concluded in 1804, when southern Southwick's residents sought to leave Massachusetts, and the town was split in half.

The southwestern border of Connecticut where it abuts New York State is marked by a panhandle in Fairfield County, containing the towns of Greenwich, Stamford, New Canaan, Darien, and parts of Norwalk and Wilton. This irregularity in the boundary is the result of territorial disputes in the late 17th century, culminating with New York giving up its claim to the area, whose residents considered themselves part of Connecticut, in exchange for an equivalent area extending northwards from Ridgefield to the Massachusetts border, as well as undisputed claim to Rye, New York.

Areas maintained by the National Park Service include Appalachian National Scenic Trail, Quinebaug and Shetucket Rivers Valley National Heritage Corridor, and Weir Farm National Historic Site.

Connecticut lies at the rough transition zone between the southern end of the humid continental climate, and the northern portion of the humid subtropical climate. Northern Connecticut generally experiences a climate with cold winters with moderate snowfall and hot, humid summers. Far southern and coastal Connecticut has a climate with cool winters with a mix of rain and infrequent snow, and the long hot and humid summers typical of the middle and lower East Coast.

Connecticut sees a fairly even precipitation pattern with rainfall/snowfall spread throughout the 12 months. Connecticut averages 56% of possible sunshine (higher than the U.S. national average), averaging 2,400 hours of sunshine annually. On average, about one third of days in the state see some amount of precipitation each year. Occasionally, some months may see extremes in precipitation, either much higher or lower than normal, though long term droughts and floods are rare.

Early spring can range from slightly cool (40s to low 50s F) to warm (65 to 70 F), while mid and late spring (late April/May) is warm. By late May, the building Bermuda High creates a southerly flow of warm and humid tropical air, bringing hot weather conditions throughout the state. Average highs are in New London and in Windsor Locks at the peak of summer in late July. On occasion, heat waves with highs from 90 to occur across Connecticut. Connecticut's record high temperature is which occurred in Danbury on July 15, 1995. Although summers are sunny in Connecticut, quick moving summer thunderstorms can bring brief downpours with thunder and lightning. Occasionally these thunderstorms can be severe, and the state usually averages one tornado per year. During hurricane season, the remains of tropical cyclones occasionally affect the region, though a direct hit is rare. Some notable hurricanes to impact the state include the 1938 New England hurricane, Hurricane Carol in 1954, Hurricane Sandy in 2012, and Hurricane Isaias in 2020.

Weather commonly associated with the fall season typically begins in October and lasts to the first days of December. Daily high temperatures in October and November range from the 50s to 60s (Fahrenheit) with nights in the 40s and upper 30s. Colorful foliage begins across northern parts of the state in early October and moves south and east reaching southeast Connecticut by early November. Far southern and coastal areas, however, have more oak and hickory trees (and fewer maples) and are often less colorful than areas to the north. By December daytime highs are in the 40s °F for much of the state, and average overnight lows are below freezing.

Winters (December through mid-March) are generally cold from south to north in Connecticut. The coldest month (January) has average high temperatures ranging from in the coastal lowlands to in the inland and northern portions on the state. The lowest temperature recorded in Connecticut is which has been observed twice: in Falls Village on February 16, 1943, and in Coventry on January 22, 1961. The average yearly snowfall ranges from about in the higher elevations of the northern portion of the state to only along the southeast coast of Connecticut (Branford to Groton). Generally, any locale north or west of Interstate 84 receives the most snow, during a storm, and throughout the season. Most of Connecticut has less than 60 days of snow cover. Snow usually falls from late November to late March in the northern part of the state, and from early December to mid-March in the southern and coastal parts of the state.

During winter every few years, Connecticut can occasionally get heavy snowstorms, called nor'easters, which may produce as much as two feet of snow on rare occasions. Ice storms also occur on occasion, such as the Southern New England ice storm of 1973 and the December 2008 Northeastern United States ice storm. These storms can cause widespread power outages and damage.

Forests consist of a mix of Northeastern coastal forests of oak in southern areas of the state, to the upland New England-Acadian forests in the northwestern parts of the state. Mountain Laurel ("Kalmia latifolia") is the state flower and is native to low ridges in several parts of Connecticut. Rosebay rhododendron ("Rhododendron maximum") is also native to eastern uplands of Connecticut and Pachaug State Forest is home to the Rhododendron Sanctuary Trail. Atlantic white cedar ("Chamaecyparis thyoides"), is found in wetlands in the southern parts of the state. Connecticut has one native cactus ("Opuntia humifusa"), found in sandy coastal areas and low hillsides. Several types of beach grasses and wildflowers are also native to Connecticut. Connecticut spans USDA Plant Hardiness Zones 5b to 7a. Coastal Connecticut is the broad transition zone where more southern and subtropical plants are cultivated. In some coastal communities, "Magnolia grandiflora" (southern magnolia), crape myrtles, scrub palms ("Sabal minor"), needle palms ("Rhapidophyllum hystrix"), and other broadleaved evergreens are cultivated in small numbers.

As of the 2020 United States census, Connecticut has a population of 3,605,944, an increase of 31,847 people (0.9%) from the 2010 United States census. Among the census records, 20.4% of the population was under 18.

In 1790, 97% of the population in Connecticut was classified as "rural". The first census in which less than half the population was classified as rural was 1890. In the 2000 census, only 12.3% was considered rural. Most of western and southern Connecticut (particularly the Gold Coast) is strongly associated with New York City; this area is the most affluent and populous region of the state and has high property costs and high incomes. The center of population of Connecticut is located in the town of Cheshire.

According to HUD's 2022 Annual Homeless Assessment Report, there were an estimated 2,930 homeless people in Connecticut.

In common with the majority of the United States, non-Hispanic whites have remained the dominant racial and ethnic group in Connecticut. From being 98% of the population in 1940, however, they have declined to 63% of the population as of the 2020 census. These statistics have represented fewer Americans identifying as non-Hispanic white, which has given rise to the Hispanic and Latino American population and Asian American population overall. , 46.1% of Connecticut's population younger than age1 were minorities. As of 2004, 11.4% of the population (400,000) was foreign-born. In 1870, native-born Americans had accounted for 75% of the state's population, but that had dropped to 35% by 1918. Also as of 2000, 81.69% of Connecticut residents age5 and older spoke English at home and 8.42% spoke Spanish, followed by Italian at 1.59%, French at 1.31%, and Polish at 1.20%.

The largest ancestry groups since 2010 were: 19.3% Italian, 17.9% Irish, 10.7% English, 10.4% German, 8.6% Polish, 6.6% French, 3.0% French Canadian, 2.7% American, 2.0% Scottish, and 1.4% Scotch Irish.

The top countries of origin for Connecticut's immigrants in 2018 were India, Jamaica, the Dominican Republic, Poland and Ecuador.

"Note: Births in table do not add up because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


A Pew survey of Connecticut residents' religious self-identification showed the following distribution of affiliations in 2014: Protestant 35%, Mormonism 1%, Jewish 3%, Roman Catholic 33%, Orthodox 1%, Non-religious 28%, Jehovah's Witness 1%, Hinduism 1%, Buddhism 1% and Islam 1%. Jewish congregations had 108,280 (3.2%) members in 2000.

The Jewish population is concentrated in the towns near Long Island Sound between Greenwich and New Haven, in Greater New Haven and in Greater Hartford, especially the suburb of West Hartford. According to the Association of Religion Data Archives, the largest Christian denominations, by number of adherents, in 2010 were: the Catholic Church, with 1,252,936; the United Church of Christ, with 96,506; and non-denominational Evangelical Protestants, with 72,863.

Recent immigration has brought other non-Christian religions to the state, but the numbers of adherents of other religions are still low. Connecticut is also home to New England's largest Protestant church: The First Cathedral in Bloomfield, Connecticut, located in Hartford County. Hartford is seat to the Roman Catholic Archdiocese of Hartford, which is sovereign over the Diocese of Bridgeport and the Diocese of Norwich.

By the Public Religion Research Institute's study in 2020, 71% of the population identified as Christian. In contrast to the 2014 study by the Pew Research Center, the irreligious declined from 28% of the population to 21% at the 2020 Public Religion Research Institute's study.

Connecticut's economic output in 2019 as measured by gross domestic product was $289 billion, up from $277.9 billion in 2018.

Connecticut's per capita personal income in 2019 was estimated at $79,087, the highest of any state. There is, however, a great disparity in incomes throughout the state; after New York, Connecticut had the second largest gap nationwide between the average incomes of the top 1% and the average incomes of the bottom 99%. According to a 2018 study by Phoenix Marketing International, Connecticut had the third-largest number of millionaires per capita in the United States, with a ratio of 7.75%. New Canaan is the wealthiest town in Connecticut, with a per capita income of $85,459. Hartford is the poorest municipality in Connecticut, with a per capita income of $13,428 in 2000.

As of December 2019, Connecticut's seasonally adjusted unemployment rate was 3.8%, with U.S. unemployment at 3.5% that month. Dating back to 1982, Connecticut recorded its lowest unemployment in 2000 between August and October, at 2.2%. The highest unemployment rate during that period occurred in November and December 2010 at 9.3%, but economists expected record new levels of layoffs as a result of business closures in the spring of 2020 due to the coronavirus pandemic.

Tax is collected by the Connecticut Department of Revenue Services and by local municipalities.

As of 2012, Connecticut residents had the second highest rate in the nation of combined state and local taxes after New York, at 12.6% of income compared to the national average of 9.9% as reported by the Tax Foundation.

Before 1991, Connecticut had an investment-only income tax system. Income from employment was untaxed, but income from investments was taxed at 13%, the highest rate in the U.S., with no deductions allowed for costs of producing the investment income, such as interest on borrowing.

In 1991, under Governor Lowell P. Weicker Jr., an independent, the system was changed to one in which the taxes on employment income and investment income were equalized at a maximum rate of 4%. The new tax policy drew investment firms to Connecticut; , Fairfield County was home to the headquarters for 16 of the 200 largest hedge funds in the world.

, the income tax rates on Connecticut individuals were divided into seven tax brackets of 3% (on income up to $10,000); 5% ($10,000–$50,000); 5.5% ($50,000–$100,000); 6% ($100,000–$200,000); 6.5% ($200,000–$250,000); 6.9% ($250,000–$500,000); and 6.99% above $500,000, with additional amounts owed depending on the bracket.

All wages of Connecticut residents are subject to the state's income tax, even if earned outside the state. However, in those cases, Connecticut income tax must be withheld only to the extent the Connecticut tax exceeds the amount withheld by the other jurisdiction. Since New York has higher income tax rates than Connecticut, this effectively means that Connecticut residents who work in New York have no Connecticut income tax withheld. Connecticut permits a credit for taxes paid to other jurisdictions, but since residents who work in other states are still subject to Connecticut income taxation, they may owe taxes if the jurisdictional credit does not fully offset the Connecticut tax amount.

Connecticut levies a 6.35% state sales tax on the retail sale, lease, or rental of most goods. Some items and services in general are not subject to sales and use taxes unless specifically enumerated as taxable by statute. A provision excluding clothing under $50 from sales tax was repealed . There are no additional sales taxes imposed by local jurisdictions. In 2001, Connecticut instituted what became an annual sales tax "holiday" each August lasting one week, when retailers do not have to remit sales tax on certain items and quantities of clothing that has varied from year to year.

State law authorizes municipalities to tax property, including real estate, vehicles and other personal property, with state statute providing varying exemptions, credits and abatements. All assessments are at 70% of fair market value. The maximum property tax credit is $200 per return and any excess may not be refunded or carried forward. According to the Tax Foundation, on a per capita basis in the 2017 fiscal year Connecticut residents paid the 3rd highest average property taxes in the nation after New Hampshire and New Jersey.

, gasoline taxes and fees in Connecticut were 40.13 cents per gallon, 11th highest in the United States which had a nationwide average of 36.13 cents a gallon excluding federal taxes. Diesel taxes and fees as of January 2020 in Connecticut were 46.50 cents per gallon, ninth highest nationally with the U.S. average at 37.91 cents.

In 2019, sales of single-family homes in Connecticut totaled 33,146 units, a 2.1 percent decline from the 2018 transaction total. The median home sold in 2019 recorded a transaction amount of $260,000, up 0.4 percent from 2018.

Connecticut had the seventh highest rate of home foreclosure activity in the country in 2019 at 0.53 percent of the total housing stock.

Finance, insurance and real estate was Connecticut's largest industry in 2018 as ranked by gross domestic product, generating $75.7 billion in GDP that year. Major employers include The Hartford, Travelers, Harman International, Cigna, the Aetna subsidiary of CVS Health, Mass Mutual, People's United Financial, Bank of America, Realogy, Bridgewater Associates, GE Capital, William Raveis Real Estate, and Berkshire Hathaway through reinsurance and residential real estate subsidiaries.

The combined educational, health and social services sector was the largest single industry as ranked by employment, with a combined workforce of 342,600 people at the end of 2019, ranking fourth the year before in GDP at $28.3 billion.

The broad business and professional services sector had the second highest GDP total in Connecticut in 2018 at an estimated $33.7 billion.

Manufacturing was the third biggest industry in 2018 with GDP of $30.8 billion, dominated by Raytheon Technologies formed in the March 2020 merger of Hartford-based United Technologies and Waltham, Mass.-based Raytheon Co. As of the merger, Raytheon Technologies employed about 19,000 people in Connecticut through subsidiaries Pratt & Whitney and Collins Aerospace. Lockheed Martin subsidiary Sikorsky Aircraft operates Connecticut's single largest manufacturing plant in Stratford, where it makes helicopters.

The world's largest audio equipment manufacturing company Harman International is headquartered in Stamford, Connecticut. It owns many brands like JBL, Akg and Harman kardon.

Other major manufacturers include the Electric Boat division of General Dynamics, which makes submarines in Groton, Boehringer Ingelheim, a pharmaceuticals manufacturer with its U.S. headquarters in Ridgefield, and ASML, which in Wilton makes precision lithography machines used to create circuitry on semiconductors and flat-screen displays.

Connecticut historically was a center of gun manufacturing, and four gun-manufacturing firms continued to operate in the state , employing 2,000 people: Colt, Stag, Ruger, and Mossberg. Marlin, owned by Remington, closed in April 2011.

Other large components of the Connecticut economy in 2018 included wholesale trade ($18.1 billion in GDP); information services ($13.8 billion); retail ($13.7 billion); arts, entertainment and food services ($9.1 billion); and construction ($8.3 billion).

Tourists spent $9.3 billion in Connecticut in 2017 according to estimates as part of a series of studies commissioned by the state of Connecticut. Foxwoods Resort Casino and Mohegan Sun are the two biggest tourist draws and number among the state's largest employers; both are located on Native American reservations in the southeastern Connecticut.
Connecticut's agricultural production totaled $580 million in 2017, with just over half of that revenue the result of nursery stock production. Milk production totaled $81 million that year, with other major product categories including eggs, vegetables and fruit, tobacco and shellfish.

Connecticut's economy uses less energy to produce each dollar of GDP than all other states except California, Massachusetts, and New York. It uses less energy on a per-capita basis than all but six other states. It has no fossil-fuel resources, but does have renewable resources. Average retail electricity prices are the highest among the 48 contiguous states. While the vast majority of state's overall energy consumption is fossil fuels, nuclear power delivered over 40% of state's electricity generation in 2019. Refuse-derived fuels and other biomass provided the largest share of renewable electricity at about a 3% share. Solar and wind generation have grown in recent years. More than three-quarters of solar generation came from distributed small-scale installations such as rooftop solar in 2019, and there is planning underway to significantly increase renewable generation with the state's offshore wind resource.

The Interstate highways in the state are Interstate 95 (I-95) traveling southwest to northeast along the coast, I-84 traveling southwest to northeast in the center of the state, I-91 traveling north to south in the center of the state, and I-395 traveling north to south near the eastern border of the state. The other major highways in Connecticut are the Merritt Parkway and Wilbur Cross Parkway, which together form Connecticut Route 15 (Route 15), traveling from the Hutchinson River Parkway in New York parallel to I-95 before turning north of New Haven and traveling parallel to I-91, finally becoming a surface road in Berlin. I-95 and Route 15 were originally toll roads; they relied on a system of toll plazas at which all traffic stopped and paid fixed tolls. A series of major crashes at these plazas eventually contributed to the decision to remove the tolls in 1988. Other major arteries in the state include U.S. Route7 (US7) in the west traveling parallel to the New York state line, Route8 farther east near the industrial city of Waterbury and traveling north–south along the Naugatuck River Valley nearly parallel with US7, and Route9 in the east.

Between New Haven and New York City, I-95 is one of the most congested highways in the United States. Although I-95 has been widened in several spots, some areas are only three lanes and this strains traffic capacity, resulting in frequent and lengthy rush hour delays. Frequently, the congestion spills over to clog the parallel Merritt Parkway and even US1. The state has encouraged traffic reduction schemes, including rail use and ride-sharing.

Connecticut also has a very active bicycling community, with one of the highest rates of bicycle ownership and use in the United States, particularly in New Haven. According to the U.S. Census 2006 American Community Survey, New Haven has the highest percentage of commuters who bicycle to work of any major metropolitan center on the East Coast.

Rail is a popular travel mode between New Haven and New York City's Grand Central Terminal. Southwestern Connecticut is served by the Metro-North Railroad's New Haven Line, operated by the Metropolitan Transportation Authority. Metro-North provides commuter service between New York City and New Haven, with branches to New Canaan, Danbury, and Waterbury. Connecticut lies along Amtrak's Northeast Corridor, which features frequent Northeast Regional and Acela Express service from New Haven south to New York City, Philadelphia, Baltimore, Washington, DC, and Norfolk, VA, as well as north to New London, Providence and Boston. Since 1990, coastal cities and towns between New Haven and New London are also served by the Shore Line East commuter line.

In June 2018, a commuter rail service called the Hartford Line began operating between New Haven and Springfield on Amtrak's New Haven-Springfield Line. Hartford Line service is provided by both Amtrak and the Connecticut Department of Transportation's CT Rail, and in addition to its termini serves New Haven State Street, Wallingford, Meriden, Berlin, Hartford, Windsor, and Windsor Locks. Several infill stations are planned to be added in the near future as of 2021. Amtrak's Vermonter runs from Washington to St. Albans, Vermont via the same line. In July 2019, Amtrak launched the Valley Flyer, which runs between New Haven and Greenfield, Massachusetts.

A proposed commuter rail service, the Central Corridor Rail Line, would connect New London with Norwich, Willimantic, Storrs, and Stafford Springs, with service continuing into Massachusetts and Brattleboro, Vermont.

Statewide bus service is supplied by Connecticut Transit, owned by the Connecticut Department of Transportation, with smaller municipal authorities providing local service. Bus networks are an important part of the transportation system in Connecticut, especially in urban areas like Hartford, Stamford, Norwalk, Bridgeport and New Haven. Connecticut Transit also operates CTfastrak, a bus rapid transit service between New Britain and Hartford, which opened to the public on March 28, 2015.

Connecticut's largest airport is Bradley International Airport in Windsor Locks, north of Hartford. Many residents of central and southern Connecticut also make heavy use of JFK International Airport and Newark International Airports, especially for international travel. Smaller regional air service is provided at Tweed New Haven Regional Airport. Larger civil airports include Danbury Municipal Airport and Waterbury-Oxford Airport in western Connecticut, Hartford–Brainard Airport in central Connecticut, and Groton-New London Airport in eastern Connecticut. Sikorsky Memorial Airport is located in Stratford and mostly services cargo, helicopter and private aviation.

Several ferry services cross Long Island Sound and connect the state to Long Island. The Bridgeport & Port Jefferson Ferry travels between Bridgeport, Connecticut, and Port Jefferson, New York. Ferry service also operates out of New London to Orient, New York; Fishers Island, New York; and Block Island, Rhode Island, which are popular tourist destinations. Two ferries cross the Connecticut River: the Rocky Hill–Glastonbury ferry and the Chester–Hadlyme ferry, the former of which is the oldest continuously operating ferry in the United States, operating since 1655.

Hartford has been the sole capital of Connecticut since 1875. Before then, New Haven and Hartford alternated as dual capitals.

Connecticut is known as the "Constitution State". The origin of this nickname is uncertain, but it likely comes from Connecticut's pivotal role in the federal constitutional convention of 1787, during which Roger Sherman and Oliver Ellsworth helped to orchestrate what became known as the Connecticut Compromise, or the Great Compromise. This plan combined the Virginia Plan and the New Jersey Plan to form a bicameral legislature, a form copied by almost every state constitution since the adoption of the federal constitution. Variations of the bicameral legislature had been proposed by Virginia and New Jersey, but Connecticut's plan was the one that was in effect until the early 20th century, when Senators ceased to be selected by their state legislatures and were instead directly elected. Otherwise, it is still the design of Congress.

The nickname also might refer to the Fundamental Orders of 1638–39. These Fundamental Orders represent the framework for the first formal Connecticut state government written by a representative body in Connecticut. The State of Connecticut government has operated under the direction of four separate documents in the course of the state's constitutional history. After the Fundamental Orders, Connecticut was granted governmental authority by King Charles II of England through the Connecticut Charter of 1662.

Separate branches of government did not exist during this period, and the General Assembly acted as the supreme authority. A constitution similar to the modern U.S. Constitution was not adopted in Connecticut until 1818. Finally, the current state constitution was implemented in 1965. The 1965 constitution absorbed a majority of its 1818 predecessor, but incorporated a handful of important modifications.

The governor heads the executive branch. , Ned Lamont is the Governor and Susan Bysiewicz is the Lieutenant Governor; both are Democrats. From 1639 until the adoption of the 1818 constitution, the governor presided over the General Assembly. In 1974, Ella Grasso was elected as the governor of Connecticut. This was the first time in United States history when a woman was a governor without her husband being governor first.

There are several executive departments: Administrative Services, Agriculture, Banking, Children and Families, Consumer Protection, Correction, Economic and Community Development, Developmental Services, Construction Services, Education, Emergency Management and Public Protection, Energy & Environmental Protection, Higher Education, Insurance, Labor, Mental Health and Addiction Services, Military, Motor Vehicles, Public Health, Public Utility Regulatory Authority, Public Works, Revenue Services, Social Services, Transportation, and Veterans Affairs. In addition to these departments, there are other independent bureaus, offices and commissions.

In addition to the governor and lieutenant governor, there are four other executive officers named in the state constitution that are elected directly by voters: secretary of the state, treasurer, comptroller, and attorney general. All executive officers are elected to four-year terms.

Connecticut's legislative branch is known as the General Assembly. It is a bicameral legislature consisting of an upper body, the State Senate (36 senators); and a lower body, the House of Representatives (151 representatives). Bills must pass each house in order to become law. The governor can veto bills, but this veto can be overridden by a two-thirds majority in both houses. Per Article XV of the state constitution, Senators and Representatives must be at least 18 years of age and are elected to two-year terms in November on even-numbered years. There also must always be between 30 and 50 senators and 125 to 225 representatives. The Lieutenant Governor presides over the Senate, except when absent from the chamber, when the President pro tempore presides. The Speaker of the House presides over the House. , Matthew Ritter is the Speaker of the House of Connecticut.

, Connecticut's United States Senators are Richard Blumenthal (Democrat) and Chris Murphy (Democrat). Connecticut has five representatives in the U.S. House, all of whom are Democrats.

Locally elected representatives also develop local ordinances to govern cities and towns. The town ordinances often include noise control and zoning guidelines. However, the State of Connecticut also provides statewide ordinances for noise control as well.

The highest court of Connecticut's judicial branch is the Connecticut Supreme Court, headed by the Chief Justice of Connecticut. The Supreme Court is responsible for deciding on the constitutionality of laws, or cases as they relate to the law. Its proceedings are similar to those of the United States Supreme Court: no testimony is given by witnesses, and the lawyers of the two sides each present oral arguments no longer than thirty minutes. Following a court proceeding, the court may take several months to arrive at a judgment. , the Chief Justice is Richard A. Robinson.

In 1818, the court became a separate entity, independent of the legislative and executive branches. The Connecticut Appellate Court is a lesser statewide court, and the Superior Courts are lower courts that resemble county courts of other states.

Connecticut does not have county government, unlike all other states except Rhode Island. Connecticut county governments were mostly eliminated in 1960, with the exception of sheriffs elected in each county. In 2000, the county sheriff was abolished and replaced with the state marshal system, which has districts that follow the old county territories. The judicial system is divided into judicial districts at the trial-court level which largely follow the old county lines. The eight counties are still widely used for purely geographical and statistical purposes, such as weather reports and census reporting.

The state is divided into nine regional councils of government defined by the state Office of Planning and Management, which facilitate regional planning and coordination of services between member towns. The Intragovernmental Policy Division of this Office coordinates regional planning with the administrative bodies of these regions. Each region has an administrative body made up chief executive officers of the member towns. The regions are established for the purpose of planning "coordination of regional and state planning activities; redesignation of logical planning regions and promotion of the continuation of regional planning organizations within the state; and provision for technical aid and the administration of financial assistance to regional planning organizations". By 2015, the State of Connecticut recognized COGs as county equivalents, allowing them to apply for funding and grants made available to county governments in other states. In 2019 the state recommended to the United States Census Bureau that the nine Councils of Governments replace its counties for statistical purposes. This proposal was approved by the Census Bureau in 2022, and will be fully implemented by 2024.

Connecticut shares with the rest of New England a governmental institution called the New England town. The state is divided into 169 towns which serve as the fundamental political jurisdictions. There are also 21 cities, most of which simply follow the boundaries of their namesake towns and have a merged city-town government. There are two exceptions: the City of Groton, which is a subsection of the Town of Groton, and the City of Winsted in the Town of Winchester. There are also nine incorporated boroughs which may provide additional services to a section of town. Naugatuck is a consolidated town and borough.

Connecticut is generally considered to be a blue state. The last Republican presidential candidate to win Connecticut's votes in the Electoral College was George H. W. Bush in 1988.

Connecticut residents who register to vote may declare an affiliation to a political party, may become unaffiliated at will, and may change affiliations subject to certain waiting periods. around 58% of registered voters are enrolled in a political party. The Democratic Party of Connecticut is the largest party in the state by voter registration, with 36% of voters, followed by the Connecticut Republican Party with approximately 20%. An additional 1.6% are registered to third parties. As of 2022, 4 third parties have statewide enrollment privileges (meaning any state resident may register as a member), including the Libertarian Party of Connecticut, the Independent Party of Connecticut, the Connecticut Green Party, and the Connecticut Working Families Party. Connecticut allows electoral fusion, where the same candidate can run on the ballot of more than one political party; this is often used by the Connecticut Working Families Party to cross-endorse Democratic candidates.

In July 2009, the Connecticut legislature overrode a veto by Governor M. Jodi Rell to pass SustiNet, the first significant public-option health care reform legislation in the nation.

In April 2012, both houses of the Connecticut state legislature passed a bill (20 to 16 and 86 to 62) that abolished capital punishment for all future crimes, while 11 inmates who were waiting on the death row at the time could still be executed.

Connecticut ranked third in the nation for educational performance, according to Education Week's Quality Counts 2018 report. It earned an overall score of 83.5 out of 100 points. On average, the country received a score of 75.2. Connecticut posted a B-plus in the Chance-for-Success category, ranking fourth on factors that contribute to a person's success both within and outside the K-12 education system. Connecticut received a mark of B-plus and finished fourth for School Finance. It ranked 12th with a grade of C on the K-12 Achievement Index.

Hartford Public High School (1638) is the third-oldest secondary school in the nation after the Collegiate School (1628) in Manhattan and the Boston Latin School (1635). Today, the Connecticut State Board of Education manages the public school system for children in grades K–12. Board of Education members are appointed by the Governor of Connecticut.

Connecticut has a number of private schools. Private schools may file for approval by the state Department of Education, but are not required to. Per state law, private schools must file yearly attendance reports with the state.

Notable private schools include Choate Rosemary Hall, The Hotchkiss School, Loomis Chaffee School, and Taft School.

Connecticut was home to the nation's first law school, Litchfield Law School, which operated from 1773 to 1833 in Litchfield. Well known universities in the state include Yale University, Wesleyan University, Trinity College, Sacred Heart University, Fairfield University, Quinnipiac University, and the University of Connecticut. The Connecticut State University System includes 4 state universities, and the state also has 12 community colleges. The United States Coast Guard Academy is located in New London.

There are two Connecticut teams in the American Hockey League. The Bridgeport Islanders is a farm team for the New York Islanders which competes at the Total Mortgage Arena in Bridgeport. The Hartford Wolf Pack is the affiliate of the New York Rangers; they play in the XL Center in Hartford.

The Hartford Yard Goats of the Double-A Northeast are a AA affiliate of the Colorado Rockies. Also, the Norwich Sea Unicorns play in the Futures Collegiate Baseball League. The New Britain Bees play in the Atlantic League of Professional Baseball. The Connecticut Sun of the WNBA currently play at the Mohegan Sun Arena in Uncasville. In soccer, Hartford Athletic began play in the USL Championship in 2019.

The state hosts several major sporting events. Since 1952, a PGA Tour golf tournament has been played in the Hartford area. It was originally called the "Insurance City Open" and later the "Greater Hartford Open" and is now known as the Travelers Championship. 
Lime Rock Park in Salisbury is a road racing course, home to the International Motor Sports Association, SCCA, United States Auto Club, and K&N Pro Series East races. Thompson International Speedway, Stafford Motor Speedway, and Waterford Speedbowl are oval tracks holding weekly races for NASCAR Modifieds and other classes, including the NASCAR Whelen Modified Tour. The state also hosts several major mixed martial arts events for Bellator MMA and the Ultimate Fighting Championship.

The Hartford Whalers of the National Hockey League played in Hartford from 1975 to 1997 at the Hartford Civic Center. They departed to Raleigh, North Carolina, after disputes with the state over the construction of a new arena, and they are now known as the Carolina Hurricanes. A baseball team known as the Hartfords (or Hartford Dark Blues) played in the National Association from 1874 to 1875, before becoming charter members of the National League in 1876. The team moved to Brooklyn, New York, and then disbanded one season later. In 1926, Hartford also had a franchise in the National Football League known as the Hartford Blues. From 2000 until 2006 the city was home to the Hartford FoxForce of World TeamTennis.

The Connecticut Huskies are the team of the University of Connecticut (UConn); they play NCAA Division I sports. Both the men's basketball and women's basketball teams have won multiple national championships. In 2004, UConn became the first school in NCAA DivisionI history to have its men's and women's basketball programs win the national title in the same year; they repeated the feat in 2014 and are still the only DivisionI school to win both titles in the same year. The UConn women's basketball team holds the record for the longest consecutive winning streak in NCAA college basketball at 111 games, a streak that ended in 2017. The UConn Huskies football team has played in the Football Bowl Subdivision since 2002, and has played in four bowl games.

New Haven biennially hosts "The Game" between the Yale Bulldogs and the Harvard Crimson, the country's second-oldest college football rivalry. Yale alumnus Walter Camp is deemed the "Father of American Football", and he helped develop modern football while living in New Haven. Other Connecticut universities which feature DivisionI sports teams are Quinnipiac University, Fairfield University, Central Connecticut State University, Sacred Heart University, and the University of Hartford.

The name "Connecticut" originated with the Mohegan word "quonehtacut", meaning "place of long tidal river". Connecticut's official nickname is "The Constitution State", adopted in 1959 and based on its colonial constitution of 1638–1639 which was the first in America and, arguably, the world. Connecticut is also unofficially known as "The Nutmeg State", whose origin is unknown. It may have come from its sailors returning from voyages with nutmeg, which was a very valuable spice in the 18th and 19th centuries. It may have originated in the early machined sheet tin nutmeg grinders sold by early Connecticut peddlers. It is also facetiously said to come from Yankee peddlers from Connecticut who would sell small carved knobs of wood shaped to look like nutmeg to unsuspecting customers. George Washington gave Connecticut the title of "The Provisions State" because of the material aid that the state rendered to the American Revolutionary War effort. Connecticut is also known as "The Land of Steady Habits".

According to "Webster's New International Dictionary" (1993), a person who is a native or resident of Connecticut is a "Connecticuter". There are numerous other terms coined in print but not in use, such as "Connecticotian" (Cotton Mather in 1702) and "Connecticutensian" (Samuel Peters in 1781). Linguist Allen Walker Read suggests the more playful term "Connecticutie". "Nutmegger" is sometimes used, as is "Yankee".

The official state song is "Yankee Doodle". The traditional abbreviation of the state's name is "Conn."; the official postal abbreviation is CT.

Commemorative stamps issued by the United States Postal Service with Connecticut themes include Nathan Hale, Eugene O'Neill, Josiah Willard Gibbs, Noah Webster, Eli Whitney, the whaling ship the "Charles W. Morgan", which is docked at Mystic Seaport, and a decoy of a broadbill duck.


Census Bureau
Library of Congress

Country Liberal Party

The Country Liberal Party of the Northern Territory (CLP), commonly known as the Country Liberals, is a centre-right political party in Australia's Northern Territory. In local politics, it operates in a two-party system with the Australian Labor Party (ALP). It also contests federal elections as an affiliate of the Liberal Party of Australia and National Party of Australia, the two partners in the federal coalition.

The CLP originated in 1971 as a division of the Country Party (later renamed the National Party), the first local branches of which were formed in 1966. It adopted its current name in 1974 to attract Liberal Party supporters, but maintained a sole affiliation with the Country Party until 1979 when it adopted its current joint association. The party dominated the Northern Territory Legislative Assembly from the inaugural election in 1974 through to its defeat at the 2001 election, winning eight consecutive elections and providing the territory's first seven chief ministers. Following its defeat in 2001, the party did not return to power until 2012, but was defeated after a single term and has remained in opposition since 2016. The party is currently led by Lia Finocchiaro, who was elected party leader and leader of the opposition in February 2020.

At federal level, the CLP contests elections for the Northern Territory's House of Representatives and Senate seats, which also cover the Australian Indian Ocean Territories. It is registered with the Australian Electoral Commission (AEC). Its candidates do not form a separate parliamentary party but instead join either the Liberal or National party rooms – for instance, CLP senator Nigel Scullion was a long-serving deputy leader of the Nationals. Its sole current federal legislator Jacinta Nampijinpa Price sits with the National Party.

The CLP's constitution describes it as an "independent conservative" party and commits it to Northern Territory statehood. It has typically prioritised economic development of the territory and originally drew most of its support from Outback towns and the pastoral industry. It later developed a voter base among the urban middle-class populations of Darwin, Palmerston and Alice Springs (the latter two of which are strongholds for the party). The party has had a fluctuating relationship with the territory's large Indigenous population, notably providing the territory's first Indigenous MP (Hyacinth Tungutalum) and Australia's first Indigenous head of government (Adam Giles).

A party system did not develop in the Northern Territory until the 1960s, due to its small population and lack of regular elections. The Australian Labor Party (ALP) contested elections as early as 1905, but rarely faced an organised opposition; anti-Labor candidates usually stood as independents. The regionalist North Australia Party (NAP), established by Lionel Rose for the 1965 Legislative Council election, has been cited as a predecessor of the CLP.

A Darwin branch of the Country Party was established on 20 July 1966, following by an Alice Springs branch on 29 July. The creation of the branches was spurred by the upcoming 1966 federal election and the announcement by the Northern Territory's federal MP Jock Nelson that he would be retiring from politics. The Country Party achieved its first electoral success with the election of Sam Calder as Nelson's replacement. It subsequently won four out of eleven seats at the 1968 Legislative Council election. A third branch of the party was established in Katherine in February 1971. The branches affiliated with the Federal Council of the Australian Country Party in July 1971, establishing a formal entity with a central council, executive and annual conference. The party was formally named the "Australian Country Party – Northern Territory".

The Country Party primarily drew its support from Alice Springs, small towns, and the pastoral industry, including "a fair proportion of the non-urban Aboriginal vote". The party did not have a strong presence in Darwin. A branch of the Liberal Party, the Country Party's coalition partner at a federal level, had been established in Darwin in 1966, representing commercial interests and urban professionals. The Liberals fielded candidates at the 1968 Legislative Council elections, but by 1970 the local branch had ceased to function. In 1973, the Country Party began actively working to include Liberal supporters within its organisation, spurred by the Whitlam government's announcement of a fully elective Northern Territory Legislative Assembly. Following informal negotiations led by Goff Letts, a joint committee was established to determine changes to the Country Party's constitution and policy. These were officially approved, along with the adoption of the name Country Liberal Party, at the party's annual conference in Alice Springs on 20 July 1974. Per its 2018 constitution, the party reckons 1974 as its founding date.

The Whitlam government passed legislation in 1974 to establish a fully elected unicameral Northern Territory Legislative Assembly, replacing the previous partly elected Legislative Council, which had been in existence since 1947. The CLP won 17 out of 19 seats at the inaugural elections in October 1974, with independents holding the other two seats. Goff Letts became the inaugural majority leader, a title changed to chief minister after the granting of self-government in 1978. The CLP governed the Northern Territory from 1974 until the 2001 election. During this time, it never faced more than nine opposition members. Indeed, the CLP's dominance was so absolute that its internal politics were seen as a bigger threat than any opposition party. This was especially pronounced in the mid-1980s, when a series of party-room coups resulted in the Territory having three Chief Ministers in four years and also saw the creation of the Northern Territory Nationals as a short-lived splinter group under the leadership of former CLP chief minister Ian Tuxworth.

The Whitlam government also passed legislation to give the Northern Territory and Australian Capital Territory (ACT) representation in the federal Senate, with each territory electing two senators. Bernie Kilgariff was elected as the CLP's first senator at the 1975 federal election, sitting alongside Sam Calder in the parliamentary National Country Party. On 3 February 1979 a special conference of the CLP resolved that "the Federal CLP Parliamentarians be permitted to sit in the Party Rooms of their choice in Canberra". Despite personal misgivings, Kilgariff chose to sit with the parliamentary Liberal Party from 8 March 1979 in order that the CLP have representation in both parties, a practice which has been maintained where possible.

At the 2001 election, the Australian Labor Party won government by one seat, ending 27 years of CLP government. The loss marked a major turning point in Northern Territory politics, a result which was exacerbated when, at the 2005 election, the ALP won the second-largest majority government in the history of the Territory, reducing the once-dominant party to just four members in the Legislative Assembly. This result was only outdone by the 1974 election, in which the CLP faced only two independents as opposition. The CLP even lost two seats in Palmerston, an area where the ALP had never come close to winning any seats before.

In the 2001 federal election, the CLP won the newly formed seat of Solomon, based on Darwin/Palmerston, in the House of Representatives.
In the 2004 federal election, the CLP held one seat in the House of Representatives, and one seat in the Senate. The CLP lost its federal lower house seat in the 2007 federal election, but regained it when Palmerston deputy mayor Natasha Griggs won back Solomon for the CLP. She sat with the Liberals in the House.

The 2008 election saw the CLP recover from the severe loss it suffered three years earlier, increasing its representation from four to 11 members. Following the 2011 decision of ALP-turned-independent member Alison Anderson to join the CLP, this increased CLP's representation to 12 in the Assembly, leaving the incumbent Henderson Government to govern in minority with the support of Independent MP Gerry Wood.

Historically, the CLP has been particularly dominant in the Territory's two major cities, Darwin/Palmerston and Alice Springs. However, in recent years the ALP has pulled even with the CLP in the Darwin area; indeed, its 2001 victory was fueled by an unexpected swing in Darwin.

The CLP under the leadership of Terry Mills returned to power in the 2012 election with 16 of 25 seats, defeating the incumbent Labor government led by Paul Henderson. In the lead up to the Territory election, CLP Senator Nigel Scullion sharply criticised the Federal Labor government for its suspension of the live cattle trade to Indonesia - an economic mainstay of the territory.

The election victory ended 11 years of ALP rule in the Northern Territory. The victory was also notable for the support it achieved from indigenous people in pastoral and remote electorates. Large swings were achieved in remote Territory electorates (where the indigenous population comprised around two-thirds of voters) and a total of five Aboriginal CLP candidates won election to the Assembly. Among the indigenous candidates elected were high-profile Aboriginal activist Bess Price and former ALP member Alison Anderson. Anderson was appointed Minister for Indigenous Advancement. In a nationally reported speech in November 2012, Anderson condemned welfare dependency and a culture of entitlement in her first ministerial statement on the status of Aboriginal communities in the Territory and said the CLP would focus on improving education and on helping create real jobs for indigenous people.

Adam Giles replaced Mills as Chief Minister of the Northern Territory and party leader at the 2013 CLP leadership ballot on 13 March while Mills was on a trade mission in Japan. Giles was sworn in as Chief Minister on 14 March, becoming the first indigenous head of government of an Australian state or territory.

Willem Westra van Holthe challenged Giles at the 2015 CLP leadership ballot on 2 February and was elected leader by the party room in a late night vote conducted by phone. However, Giles refused to resign as Chief Minister following the vote. On 3 February, "ABC News" reported that officials were preparing an instrument for Giles' removal by the Administrator. The swearing-in of Westra van Holthe, which had been scheduled for 11:00 local time (01:30 UTC), was delayed. After a meeting of the parliamentary wing of the CLP, Giles announced that he would remain as party leader and Chief Minister, and that Westra van Holthe would be his deputy.

After four defections during the parliamentary term, the CLP was reduced to minority government by July 2015. Giles raised the possibility of an early election on 20 July stating that he would "love" to call a snap poll, but that it was "pretty much impossible to do". Crossbenchers dismissed the notion of voting against a confidence motion to bring down the government.

Territory government legislation passed in February 2016 changed the voting method of single-member electorates from full-preferential voting to optional preferential voting ahead of the 2016 territory election held on 27 August.

Federally, a MediaReach seat-level opinion poll of 513 voters in the seat of Solomon conducted 22−23 June ahead of the 2016 federal election held on 2 July surprisingly found Labor candidate Luke Gosling heavily leading two-term CLP incumbent Natasha Griggs 61–39 on the two-party vote from a large 12.4 percent swing. The CLP lost Solomon to Labor at the election, with Gosling defeating Griggs 56–44 on the two-party vote from a 7.4 percent swing.

Polling ahead of the 2016 Territory election indicated a large swing against the CLP, including a near-total collapse in Darwin/Palmerston. By the time the writs were dropped, commentators had almost universally written off the CLP. At 27 August Territory election, the CLP was swept from power in a massive Labor landslide, suffering easily the worst defeat of a sitting government in Territory history and one of the worst defeats a governing party has ever suffered at the state or territory level in Australia. The party not only lost all of the bush seats it picked up in 2012, but was all but shut out of Darwin/Palmerston, winning only one seat there. All told, the CLP only won two seats, easily its worst showing in an election. Giles himself lost his own seat, becoming the second Majority Leader/Chief Minister to lose his own seat. Even before Giles' defeat was confirmed, second-term MP Gary Higgins—the only surviving member of the Giles cabinet—was named the party's new leader, with Lia Finocchiaro as his deputy. On 20 January 2020, Higgins announced his resignation as party leader and announced his retirement at the next election. Finocchiaro succeeded him as CLP leader and leader of the opposition on 1 February 2020.

Finocchiaro led the CLP to a modest recovery at the 2020 Territory election. The CLP picked up a six-seat swing, boosting its seat count to eight. However, it failed to make significant inroads in Darwin/Palmerston, winning only two seats there, including that of Finocchiaro.

The CLP lost the seat of Daly to Labor in a 2021 by-election, the first time an incumbent government had won a seat from the opposition in territory history.

The CLP stands for office in the Northern Territory Assembly and Federal Parliament of Australia and primarily concerns itself with representing Territory interests. It is a regionally based party, that has parliamentary representation in both the Federal Parliament and at the Territory level. It brands as a party with strong roots in the Territory.

The CLP competes against the Australian Labor Party (Northern Territory Branch) (the local branch of Australia's social-democratic party). It is closely affiliated with, but is independent from the Liberal Party of Australia (a mainly urban, pro-business party comprising mainly liberal membership) and the National Party of Australia (a conservative and regional interests party).

The foreword to the constitution of the party describes it as an "independent conservative political party". One of the objectives in the party's constitution is to "work toward the achievement of Statehood in the Northern Territory". The party promotes traditional Liberal Party values such as individualism and private enterprise, and what it describes as "progressive" political policy such as full statehood for the Northern Territory.

In February 2023, the party voted to oppose the Voice to Parliament.

Branch delegates and members of the party's Central Council attend the Annual Conference of the Country Liberal Party to decide the party's platform. The Central Council is composed of the party's office bearers, its leaders from the Territory Assembly and the Federal Parliament and representatives of party branches.

The Annual Conference of the Country Liberal Party, attended by branch delegates and members of the party's Central Council, decides matters relating to the party's platform and philosophy. The Central Council administers the party and makes decisions on pre-selections. It is composed of the party's office bearers, its leaders in the Northern Territory Legislative Assembly, members in the Federal Parliament, and representation from each of the party's branches.

The CLP president has full voting rights with the National Party and observer status with the Liberal Party. Both the Liberals and Nationals receive Country Liberal delegations at their conventions. After federal elections, the CLP directs its federal members and senators as to which of the two other parties they should sit with in the parliamentary chamber. In practice, since the 1980s CLP House members usually sit with the Liberals, while CLP Senators usually sit with the Nationals.



Canon law

Canon law (from , , a 'straight measuring rod, ruler') is a set of ordinances and regulations made by ecclesiastical authority (church leadership) for the government of a Christian organization or church and its members. It is the internal ecclesiastical law, or operational policy, governing the Catholic Church (both the Latin Church and the Eastern Catholic Churches), the Eastern Orthodox and Oriental Orthodox churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these four bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law.

Greek / , Arabic / , Hebrew / , 'straight'; a rule, code, standard, or measure; the root meaning in all these languages is 'reed'; see also the Romance-language ancestors of the English word "cane".

In the fourth century, the First Council of Nicaea (325) calls canons the disciplinary measures of the church: the term canon, κανὠν, means in Greek, a rule. There is a very early distinction between the rules enacted by the church and the legislative measures taken by the state called "leges", Latin for laws.

The "Apostolic Canons" or "Ecclesiastical Canons of the Same Holy Apostles" is a collection of ancient ecclesiastical decrees (eighty-five in the Eastern, fifty in the Western Church) concerning the government and discipline of the Early Christian Church, incorporated with the Apostolic Constitutions which are part of the Ante-Nicene Fathers.

In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the church. It was the first modern Western legal system and is the oldest continuously functioning legal system in the West.

In the Latin Church, positive ecclesiastical laws, based directly or indirectly upon immutable divine law or natural law, derive formal authority in the case of universal laws from the supreme legislator (i.e., the Supreme Pontiff), who possesses the totality of legislative, executive, and judicial power in his person, while particular laws derive formal authority from a legislator inferior to the supreme legislator. The actual subject material of the canons is not just doctrinal or moral in nature, but all-encompassing of the human condition, and therefore extending beyond what is taken as revealed truth.

The Catholic Church also includes the main five rites (groups) of churches which are in full union with the Holy See and the Latin Church:
All of these church groups are in full communion with the Supreme Pontiff and are subject to the "Code of Canons of the Eastern Churches".

The Catholic Church has what is claimed to be the oldest continuously functioning internal legal system in Western Europe, much later than Roman law but predating the evolution of modern European civil law traditions. What some might describe as "canons" adopted by the Apostles at the Council of Jerusalem in the first century would later be developed into a highly complex legal system encapsulating not just norms of the New Testament, but some elements of the Hebrew (Old Testament), Roman, Visigothic, Saxon, and Celtic legal traditions.

The history of Latin canon law can be divided into four periods: the "jus antiquum", the "jus novum", the "jus novissimum" and the "Code of Canon Law". In relation to the Code, history can be divided into the "jus vetus" (all law before the Code) and the "jus novum" (the law of the Code, or "jus codicis").

The canon law of the Eastern Catholic Churches, which had developed some different disciplines and practices, underwent its own process of codification, resulting in the Code of Canons of the Eastern Churches promulgated in 1990 by Pope John Paul II.

Roman Catholic canon law is a fully developed legal system, with all the necessary elements: courts, lawyers, judges, a fully articulated legal code, principles of legal interpretation, and coercive penalties, though it lacks civilly-binding force in most secular jurisdictions. One example where conflict between secular and canon law occurred was in the English legal system, as well as systems, such as the U.S., that derived from it. Here criminals could apply for the benefit of clergy. Being in holy orders, or fraudulently claiming to be, meant that criminals could opt to be tried by ecclesiastical rather than secular courts. The ecclesiastical courts were generally more lenient. Under the Tudors, the scope of clerical benefit was steadily reduced by Henry VII, Henry VIII, and Elizabeth I. The papacy disputed secular authority over priests' criminal offenses. The benefit of clergy was systematically removed from English legal systems over the next 200 years, although it still occurred in South Carolina in 1827.
In English Law, the use of this mechanism, which by that point was a legal fiction used for first offenders, was abolished by the Criminal Law Act 1827.

The academic degrees in Catholic canon law are the J.C.B. ("Juris Canonici Baccalaureatus", Bachelor of Canon Law, normally taken as a graduate degree), J.C.L. ("Juris Canonici Licentiatus", Licentiate of Canon Law) and the J.C.D. ("Juris Canonici Doctor", Doctor of Canon Law). Because of its specialized nature, advanced degrees in civil law or theology are normal prerequisites for the study of canon law.

Much of Catholic canon law's legislative style was adapted from the Roman Code of Justinian. As a result, Roman ecclesiastical courts tend to follow the Roman Law style of continental Europe with some variation, featuring collegiate panels of judges and an investigative form of proceeding, called "inquisitorial", from the Latin "inquirere", to enquire. This is in contrast to the adversarial form of proceeding found in the common law system of English and U.S. law, which features such things as juries and single judges.

The institutions and practices of Catholic canon law paralleled the legal development of much of Europe, and consequently, both modern civil law and common law bear the influences of canon law. As Edson Luiz Sampel, a Brazilian expert in Catholic canon law, says, canon law is contained in the genesis of various institutes of civil law, such as the law in continental Europe and Latin American countries. Indirectly, canon law has significant influence in contemporary society.

Catholic Canonical jurisprudential theory generally follows the principles of Aristotelian-Thomistic legal philosophy. While the term "law" is never explicitly defined in the Catholic Code of Canon Law, the "Catechism of the Catholic Church" cites Aquinas in defining law as "an ordinance of reason for the common good, promulgated by the one who is in charge of the community" and reformulates it as "a rule of conduct enacted by competent authority for the sake of the common good".

The law of the Eastern Catholic Churches in full communion with the Roman papacy was in much the same state as that of the Latin Church before 1917; much more diversity in legislation existed in the various Eastern Catholic Churches. Each had its own special law, in which custom still played an important part. One major difference in Eastern Europe however, specifically in the Eastern Orthodox Christian churches, was in regards to divorce. Divorce started to slowly be allowed in specific instances such as adultery being committed, abuse, abandonment, impotence, and barrenness being the primary justifications for divorce. Eventually, the church began to allow remarriage to occur (for both spouses) post-divorce. In 1929 Pius XI informed the Eastern Churches of his intention to work out a Code for the whole of the Eastern Church. The publication of these Codes for the Eastern Churches regarding the law of persons was made between 1949 through 1958 but finalized nearly 30 years later.

The first Code of Canon Law (1917) was exclusively for the Latin Church, with application to the Eastern Churches only "in cases which pertain to their very nature". After the Second Vatican Council (1962 - 1965), the Vatican produced the "Code of Canons of the Eastern Churches" which became the first code of Eastern Catholic Canon Law.

The Eastern Orthodox Church, principally through the work of 18th-century Athonite monastic scholar Nicodemus the Hagiorite, has compiled canons and commentaries upon them in a work known as the (, 'Rudder'), so named because it is meant to "steer" the church in her discipline. The dogmatic determinations of the Councils are to be applied rigorously since they are considered to be essential for the church's unity and the faithful preservation of the Gospel.

In the Church of England, the ecclesiastical courts that formerly decided many matters such as disputes relating to marriage, divorce, wills, and defamation, still have jurisdiction of certain church-related matters (e.g. discipline of clergy, alteration of church property, and issues related to churchyards). Their separate status dates back to the 12th century when the Normans split them off from the mixed secular/religious county and local courts used by the Saxons. In contrast to the other courts of England, the law used in ecclesiastical matters is at least partially a civil law system, not common law, although heavily governed by parliamentary statutes. Since the Reformation, ecclesiastical courts in England have been royal courts. The teaching of canon law at the Universities of Oxford and Cambridge was abrogated by Henry VIII; thereafter practitioners in the ecclesiastical courts were trained in civil law, receiving a Doctor of Civil Law (D.C.L.) degree from Oxford, or a Doctor of Laws (LL.D.) degree from Cambridge. Such lawyers (called "doctors" and "civilians") were centered at "Doctors Commons", a few streets south of St Paul's Cathedral in London, where they monopolized probate, matrimonial, and admiralty cases until their jurisdiction was removed to the common law courts in the mid-19th century.

Other churches in the Anglican Communion around the world (e.g., the Episcopal Church in the United States and the Anglican Church of Canada) still function under their own private systems of canon law.

In 2002 a Legal Advisors Consultation meeting at Canterbury concluded:(1) There are principles of canon law common to the churches within the Anglican Communion; (2) Their existence can be factually established; (3) Each province or church contributes through its own legal system to the principles of canon law common within the Communion; (4) these principles have strong persuasive authority and are fundamental to the self-understanding of each of the member churches; (5) These principles have a living force, and contain within themselves the possibility for further development; and (6) The existence of the principles both demonstrates and promotes unity in the Communion.

In Presbyterian and Reformed churches, canon law is known as "practice and procedure" or "church order", and includes the church's laws respecting its government, discipline, legal practice, and worship.

Roman canon law had been criticized by the Presbyterians as early as 1572 in the Admonition to Parliament. The protest centered on the standard defense that canon law could be retained so long as it did not contradict the civil law. According to Polly Ha, the Reformed church government refuted this, claiming that the bishops had been enforcing canon law for 1500 years.

The Book of Concord is the historic doctrinal statement of the Lutheran Church, consisting of ten credal documents recognized as authoritative in Lutheranism since the 16th century. However, the Book of Concord is a confessional document (stating orthodox belief) rather than a book of ecclesiastical rules or discipline, like canon law. Each Lutheran national church establishes its own system of church order and discipline, though these are referred to as "canons".

The Book of Discipline contains the laws, rules, policies, and guidelines for The United Methodist Church. Its latest edition was published in 2016.



Catholic

Anglican

Columbanus

Columbanus (; 543 – 23 November 615) was an Irish missionary notable for founding a number of monasteries after 590 in the Frankish and Lombard kingdoms, most notably Luxeuil Abbey in present-day France and Bobbio Abbey in present-day Italy.

Columbanus taught an Irish monastic rule and penitential practices for those repenting of sins, which emphasised private confession to a priest, followed by penances levied by the priest in reparation for the sins. Columbanus is one of the earliest identifiable Hiberno-Latin writers.

Most of what we know about Columbanus is based on Columbanus' own works (as far as they have been preserved) and Jonas of Susa's "Vita Columbani" ("Life of Columbanus"), which was written between 639 and 641.

Jonas entered Bobbio after Columbanus' death but relied on reports of monks who still knew Columbanus. A description of miracles of Columbanus written by an anonymous monk of Bobbio is of much later date. In the second volume of his "Acta Sanctorum O.S.B.", Mabillon gives the life in full, together with an appendix on the miracles of Columbanus, written by an anonymous member of the Bobbio community.

Columbanus (the Latinised form of "Colmán", meaning "little dove") was born in Leinster, Ireland in 543. After his conception, his mother was said to have had a vision of her child's "remarkable genius".

He was first educated under Abbot Sinell of Cluaninis, whose monastery was on an island of the River Erne, in modern County Fermanagh. Under Sinell's instruction, Columbanus composed a commentary on the Psalms.

Columbanus then moved to Bangor Abbey where he studied to become a teacher of the Bible. He was well-educated in the areas of grammar, rhetoric, geometry, and the Holy Scriptures. Abbot Comgall taught him Greek and Latin. He stayed at Bangor until c. 590, when Comgall reluctantly gave him permission to travel to the continent.

Columbanus set sail with twelve companions: Attala, Columbanus the Younger, Gallus, Domgal, Cummain, Eogain, Eunan, Gurgano, Libran, Lua, Sigisbert and Waldoleno. They crossed the channel via Cornwall and landed in Saint-Malo, Brittany.

Columbanus then entered Burgundian France. Jonas writes that:At that time, either because of the numerous enemies from without, or on account of the carelessness of the bishops, the Christian faith had almost departed from that country. The creed alone remained. But the saving grace of penance and the longing to root out the lusts of the flesh were to be found only in a few. Everywhere that he went the noble man [Columbanus] preached the Gospel. And it pleased the people because his teaching was adorned by eloquence and enforced by examples of virtue.Columbanus and his companions were welcomed by King Guntram of Burgundy, who granted them land at Anegray, where they converted a ruined Roman fortress into a school. Despite its remote location in the Vosges Mountains, the school rapidly attracted so many students that they moved to a new site at Luxeuil and then established a second school at Fontaines. These schools remained under Columbanus' authority, and their rules of life reflected the Celtic tradition in which he had been educated.

As these communities expanded and drew more pilgrims, Columbanus sought greater solitude. Often he would withdraw to a cave seven miles away, with a single companion who acted as messenger between himself and his companions.

Tensions arose in 603 CE when St. Columbanus and his followers argued with Frankish bishops over the exact date of Easter. (St. Columbanus celebrated Easter according to Celtic rites and the Celtic Christian calendar.)

The Frankish bishops may have feared his growing influence. During the first half of the sixth century, the councils of Gaul had given to bishops absolute authority over religious communities. Celtic Christians, Columbanus and his monks used the Irish Easter calculation, a version of Bishop Augustalis's 84-year for determining the date of Easter (quartodecimanism), whereas the Franks had adopted the Victorian cycle of 532 years. The bishops objected to the newcomers' continued observance of their own dating, which – among other issues – caused the end of Lent to differ. They also complained about the distinct Irish tonsure.

In 602, the bishops assembled to judge Columbanus, but he did not appear before them as requested. Instead, he sent a letter to the prelates – a strange mixture of freedom, reverence, and charity – admonishing them to hold synods more frequently, and advising them to pay more attention to matters of equal importance to that of the date of Easter. In defence of his following his traditional paschal cycle, he wrote:

When the bishops refused to abandon the matter, Columbanus appealed directly to Pope Gregory I. In the third and only surviving letter, he asks "the holy Pope, his Father" to provide "the strong support of his authority" and to render a "verdict of his favour", apologising for "presuming to argue as it were, with him who sits in the chair of Peter, Apostle and Bearer of the Keys". None of the letters were answered, most likely due to the pope's death in 604.

Columbanus then sent a letter to Gregory's successor, Pope Boniface IV, asking him to confirm the tradition of his elders – if it was not contrary to the Faith – so that he and his monks could follow the rites of their ancestors. Before Boniface responded, Columbanus moved outside the jurisdiction of the Frankish bishops. As the Easter issue appears to end around that time, Columbanus may have stopped celebrating Irish date of Easter after moving to Italy.

Columbanus was also involved in a dispute with members of the Burgundian dynasty. Upon the death of King Guntram of Burgundy, the succession passed to his nephew, Childebert II, the son of his brother Sigebert and Sigebert's wife Brunhilda of Austrasia. When Childebert II died, his territories were divided between his two sons: Theuderic II inherited the Kingdom of Burgundy and Theudebert II inherited the Kingdom of Austrasia. Both were minors and Brunhilda, their grandmother, ruled as their regents.

Theuderic II "very often visited" Columbanus, but when Columbanus rebuked him for having a concubine, Brunhilda became his bitterest foe because she feared the loss of her influence if Theuderic II married. Brunhilda incited the court and Catholic bishops against Columbanus and Theuderic II confronted Columbanus at Luxeuil, accusing him of violating the "common customs" and "not allowing all Christians" in the monastery. Columbanus asserted his independence to run the monastery without interference and was imprisoned at Besançon for execution.

Columbanus escaped and returned to Luxeuil. When the king and his grandmother found out, they sent soldiers to drive him back to Ireland by force, separating him from his monks by insisting that only those from Ireland could accompany him into exile.

Columbanus was taken to Nevers, then travelled by boat down the Loire river to the coast. At Tours he visited the tomb of Martin of Tours, and sent a message to Theuderic II indicating that within three years he and his children would perish. When he arrived at Nantes, he wrote a letter before embarkation to his fellow monks at Luxeuil monastery. The letter urged his brethren to obey Attala, who stayed behind as abbot of the monastic community.

The letter concludes:

Soon after the ship set sail from Nantes, a severe storm drove the vessel back ashore. Convinced that his holy passenger caused the tempest, the captain refused further attempts to transport the monk. Columbanus found sanctuary with Chlothar II of Neustria at Soissons, who gave him an escort to the court of King Theudebert II of Austrasia.

Columbanus arrived at Theudebert II's court in Metz in 611, where members of the Luxeuil school met him and Theudebert II granted them land at Bregenz. They travelled up the Rhine via Mainz to the lands of the Suebi and Alemanni in the northern Alps, intending to preach the Gospel to these people. He followed the Rhine river and its tributaries, the Aar and the Limmat, and then on to Lake Zurich. Columbanus chose the village of Tuggen as his initial community, but the work was not successful. He continued north-east by way of Arbon to Bregenz on Lake Constance. Here he found an oratory dedicated to Aurelia of Strasbourg containing three brass images of their tutelary deities. Columbanus commanded Gallus, who knew the local language, to preach to the inhabitants, and many were converted. The three brass images were destroyed, and Columbanus blessed the little church, placing the relics of Aurelia beneath the altar. A monastery was erected, Mehrerau Abbey, and the brethren observed their regular life. Columbanus stayed in Bregenz for about one year.

In the spring of 612, war broke out between Austrasia and Burgundy and Theudebert II was resoundingly beaten by Theuderic II. Austrasia was subsumed under the kingdom of Burgundy and Columbanus was again vulnerable to Theuderic II's opprobrium. When Columbanus' students began to be murdered in the woods, Columbanus decided to cross the Alps into Lombardy.

Gallus remained in this area until his death in 646. About seventy years later at the place of Gallus' cell the Abbey of Saint Gall was founded. The city of St. Gallen originated as an adjoining settlement of the abbey.

Columbanus arrived in Milan in 612 and was welcomed by King Agilulf and Queen Theodelinda of the Lombards. He immediately began refuting the teachings of Arianism, which had enjoyed a degree of acceptance in Italy. He wrote a treatise against Arianism, which has since been lost. In 614, Agilulf granted Columbanus land for a school at the site of a ruined church at Bobbio.

At the king's request, Columbanus wrote a letter to Pope Boniface IV on the controversy over the "Three Chapters" – writings by Syrian bishops suspected of Nestorianism, which had been condemned in the fifth century as heresy. Pope Gregory I had tolerated in Lombardy those persons who defended the "Three Letters", among them King Agilulf. Columbanus agreed to take up the issue on behalf of the king. The letter has a diplomatic tone and begins with an apology that a "foolish Scot" (, Irishman) would be writing for a Lombard king. After acquainting the pope with the imputations brought against him, he entreats the pontiff to prove his orthodoxy and assemble a council. When critiquing Boniface, he writes that his freedom of speech is consistent with the custom of his country. Some of the language used in the letter might now be regarded as disrespectful, but in that time, faith and austerity could be more indulgent. Columbanus was tactful when making critiques, as he begins the letter expresses with the most affectionate and impassioned devotion to the Holy See.

Later, he reveals charges against the Papacy so as to encourage Boniface to make concessions:

Columbanus' deference towards Rome is sufficiently clear, calling the pope "his Lord and Father in Christ", the "Chosen Watchman", and the "First Pastor, set higher than all mortals", also asserting that "we Irish, inhabitants of the world’s edge, are disciples of Saints Peter and Paul and of all the disciples" and that "the unity of faith has produced in the whole world a unity of power and privilege."
King Agilulf gave Columbanus a tract of land called Bobbio between Milan and Genoa near the Trebbia river, situated in a defile of the Apennine Mountains, to be used as a base for the conversion of the Lombard people. The area contained a ruined church and wastelands known as "Ebovium", which had formed part of the lands of the papacy prior to the Lombard invasion. Columbanus wanted this secluded place, for while enthusiastic in the instruction of the Lombards he preferred solitude for his monks and himself. Next to the little church, which was dedicated to Peter the Apostle, Columbanus erected a monastery in 614. Bobbio Abbey at its foundation followed the Rule of Saint Columbanus, based on the monastic practices of Celtic Christianity. For centuries it remained the stronghold of orthodoxy in northern Italy.

During the last year of his life, Columbanus received messenges from King Chlothar II, inviting him to return to Burgundy, now that his enemies were dead. Columbanus did not return, but requested that the king should always protect his monks at Luxeuil Abbey. He prepared for death by retiring to his cave on the mountainside overlooking the Trebbia river, where, according to a tradition, he had dedicated an oratory to Our Lady. Columbanus died at Bobbio on 21 November 615 and is buried there.

The Rule of Saint Columbanus embodied the customs of Bangor Abbey and other Irish monasteries. Much shorter than the Rule of Saint Benedict, the Rule of Saint Columbanus consists of ten chapters, on the subjects of obedience, silence, food, poverty, humility, chastity, choir offices, discretion, mortification, and perfection.

In the first chapter, Columbanus introduces the great principle of his Rule: obedience, absolute and unreserved. The words of seniors should always be obeyed, just as "Christ obeyed the Father up to death for us". One manifestation of this obedience was constant hard labour designed to subdue the flesh, exercise the will in daily self-denial, and set an example of industry in cultivation of the soil. The least deviation from the Rule entailed corporal punishment, or a severe form of fasting. In the second chapter, Columbanus instructs that the rule of silence be "carefully observed", since it is written: "But the nurture of righteousness is silence and peace". He also warns, "Justly will they be damned who would not say just things when they could, but preferred to say with garrulous loquacity what is evil". In the third chapter, Columbanus instructs, "Let the monks' food be poor and taken in the evening, such as to avoid repletion, and their drink such as to avoid intoxication, so that it may both maintain life and not harm". Columbanus continues:

In the fourth chapter, Columbanus presents the virtue of poverty and of overcoming greed, and that monks should be satisfied with "small possessions of utter need, knowing that greed is a leprosy for monks". Columbanus also instructs that "nakedness and disdain of riches are the first perfection of monks, but the second is the purging of vices, the third the most perfect and perpetual love of God and unceasing affection for things divine, which follows on the forgetfulness of earthly things. Since this is so, we have need of few things, according to the word of the Lord, or even of one." In the fifth chapter, Columbanus warns against vanity, reminding the monks of Jesus' warning in Luke 16:15: "You are the ones who justify yourselves in the eyes of others, but God knows your hearts. What people value highly is detestable in God's sight." In the sixth chapter, Columbanus instructs that "a monk's chastity is indeed judged in his thoughts" and warns, "What profit is it if he be virgin in body, if he be not virgin in mind? For God, being Spirit."

In the seventh chapter, Columbanus instituted a service of perpetual prayer, known as , by which choir succeeded choir, both day and night. In the eighth chapter, Columbanus stresses the importance of discretion in the lives of monks to avoid "the downfall of some, who beginning without discretion and passing their time without a sobering knowledge, have been unable to complete a praiseworthy life". Monks are instructed to pray to God for to "illumine this way, surrounded on every side by the world's thickest darkness". Columbanus continues:

In the ninth chapter, Columbanus presents mortification as an essential element in the lives of monks, who are instructed, "Do nothing without counsel." Monks are warned to "beware of a proud independence, and learn true lowliness as they obey without murmuring and hesitation". According to the Rule, there are three components to mortification: "not to disagree in mind, not to speak as one pleases with the tongue, not to go anywhere with complete freedom". This mirrors the words of Jesus, "For I have come down from heaven not to do my will but to do the will of him who sent me." (John 6:38) In the tenth and final chapter, Columbanus regulates forms of penance (often corporal) for offences, and it is here that the Rule of Saint Columbanus differs significantly from that of Saint Benedict.

The Communal Rule of Columbanus required monks to fast every day until "None" or 3 p.m.; this was later relaxed and observed on designated days. Columbanus' Rule regarding diet was very strict. Monks were to eat a limited diet of beans, vegetables, flour mixed with water and small bread of a loaf, taken in the evenings.

The habit of the monks consisted of a tunic of undyed wool, over which was worn the cuculla, or cowl, of the same material. A great deal of time was devoted to various kinds of manual labour, not unlike the life in monasteries of other rules. The Rule of Saint Columbanus was approved of by the Fourth Council of Mâcon in 627, but it was superseded at the close of the century by the Rule of Saint Benedict. For several centuries in some of the greater monasteries the two rules were observed conjointly.

Columbanus did not lead a perfect life. According to Jonas and other sources, he could be impetuous and even headstrong, for by nature he was eager, passionate, and dauntless. These qualities were both the source of his power and the cause of his mistakes. His virtues, however, were quite remarkable. Like many saints, he had a great love for God's creatures. Stories claim that as he walked in the woods, it was not uncommon for birds to land on his shoulders to be caressed, or for squirrels to run down from the trees and nestle in the folds of his cowl. Although a strong defender of Irish traditions, he never wavered in showing deep respect for the Holy See as the supreme authority. His influence in Europe was due to the conversions he effected and to the rule that he composed. It may be that the example and success of Columba in Caledonia inspired him to similar exertions. The life of Columbanus stands as the prototype of missionary activity in Europe, followed by such men as Kilian, Vergilius of Salzburg, Donatus of Fiesole, Wilfrid, Willibrord, Suitbert of Kaiserwerdt, Boniface, and Ursicinus of Saint-Ursanne.

The following are the principal miracles attributed to his intercession:

Jonas relates the occurrence of a miracle during Columbanus' time in Bregenz, when that region was experiencing a period of severe famine.

Historian Alexander O'Hara states that Columbanus had a "very strong sense of Irish identity ... He's the first person to write about Irish identity, he's the first Irish person that we have a body of literary work from, so even on that point of view he’s very important in terms of Irish identity." In 1950 a congress celebrating the 1,400th anniversary of his birth took place in Luxeuil, France. It was attended by Robert Schuman, Seán MacBride, the future Pope John XXIII, and John A. Costello who said "All statesmen of today might well turn their thoughts to St Columban and his teaching. History records that it was by men like him that civilisation was saved in the 6th century."

Columbanus is also remembered as the first Irish person to be the subject of a biography. An Italian monk named Jonas of Bobbio wrote a biography of him some twenty years after Columbanus’ death. His use of the phrase in 600 AD (all of Europe) in a letter to Pope Gregory the Great is the first known use of the expression.

At Saint-Malo in Brittany, there is a granite cross bearing Columbanus's name to which people once came to pray for rain in times of drought. The nearby village of Saint-Coulomb commemorates him in name.

In France, the ruins of Columbanus' first monastery at Annegray are legally protected through the efforts of the Association Internationale des Amis de St Columban, which purchased the site in 1959. The association also owns and protects the site containing the cave, which served as Columbanus' cell, and the holy well that he created nearby. At Luxeuil-les-Bains, the Basilica of Saint Peter stands on the site of Columbanus' first church. A statue near the entrance, unveiled in 1947, shows him denouncing the immoral life of King Theuderic II. Formally an abbey church, the basilica contains old monastic buildings, which have been used as a minor seminary since the nineteenth century. It is dedicated to Columbanus and houses a bronze statue of him in its courtyard.

Luxeuil Abbey, described in the "Catholic Encyclopedia" as "the nursery of saints and apostles", produced sixty-three apostles who carried his rule, together with the Gospel, into France, Germany, Switzerland, and Italy. These disciples of Columbanus are credited with founding more than a hundred different monasteries. The canton and town still bearing the name of St. Gallen testify to how well one of his disciples succeeded.

Bobbio Abbey became a renowned center of learning in the Early Middle Ages, so famous that it rivaled the monastic community at Monte Cassino in wealth and prestige. St. Attala continued St. Columbanus' work at Bobbio, proselytizing and collecting religious texts for the abbey's library. In Lombardy, San Colombano al Lambro in Milan, San Colombano Belmonte in Turin, and San Colombano Certénoli in Genoa all take their names from the saint.

The Missionary Society of Saint Columban, founded in 1916, and the Missionary Sisters of St. Columban, founded in 1924, are both dedicated to Columbanus.

The remains of Columbanus are preserved in the crypt at Bobbio Abbey. Many miracles have been credited to his intercession. In 1482, the relics were placed in a new shrine and laid beneath the altar of the crypt. The sacristy at Bobbio possesses a portion of the skull of Columbanus, his knife, wooden cup, bell, and an ancient water vessel, formerly containing sacred relics and said to have been given to him by Pope Gregory I. According to some authorities, twelve teeth of Columbanus were taken from the tomb in the fifteenth century and kept in the treasury, but these have since disappeared.

Columbanus is named in the "Roman Martyrology" on 23 November, which is his feast day in Ireland. His feast is observed by the Benedictines on 21 November. Columbanus is the patron saint of motorcyclists. In art, Columbanus is represented bearded bearing the monastic cowl, holding in his hand a book with an Irish satchel, and standing in the midst of wolves. Sometimes he is depicted in the attitude of taming a bear, or with sun-beams over his head.

An Anglican bishop suggested Columbanus as a patron of motorcyclists because of his extensive travels through Europe during his lifetime. His patronage was declared by the Vatican in 2002.



Concord, New Hampshire

Concord () is the capital city of the U.S. state of New Hampshire and the seat of Merrimack County. As of the 2020 census the population was 43,976, making it the 3rd most populous city in New Hampshire after Manchester and Nashua. Governor Benning Wentworth gave the city its current name in 1765 following a boundary dispute with the neighboring town of Bow; the name was meant to signify the new concord, or harmony, between the two towns.

The area was first settled in 1659. On January 17, 1725, the Province of Massachusetts Bay, which then claimed territories west of the Merrimack, granted the Concord area as the Plantation of Penacook. It was settled between 1725 and 1727 and, on February 9, 1734, the town was incorporated as "Rumford." In 1808, Concord was named the official seat of state government. The State House was completed in 1819 and remains the oldest U.S. state capitol wherein the legislature meets in its original chambers.

Concord is entirely within the Merrimack River watershed and the city is centered on the river. The Merrimack runs from northwest to southeast through the city. The city's eastern boundary is formed by the Soucook River, which separates Concord from the town of Pembroke. The Turkey River passes through the southwestern quarter of the city. The city consists of its downtown, including the North End and South End neighborhoods, along with the four villages of Penacook, Concord Heights, East Concord, and West Concord. Penacook sits along the Contoocook River, just before it flows into the Merrimack.

As of 2020, the top employer in the city was the State of New Hampshire, and the largest private employer was Concord Hospital. Concord is home to the University of New Hampshire School of Law, New Hampshire's only law school; St. Paul's School, a private preparatory school; NHTI, a two-year community college; the New Hampshire Police Academy; and the New Hampshire Fire Academy. Concord's Old North Cemetery is the final resting place of Franklin Pierce, 14th President of the United States.

Interstate 89 and Interstate 93 are the two main interstate highways serving the city, and general aviation access is via Concord Municipal Airport. The nearest airport with commercial air service is Manchester–Boston Regional Airport, to the south. There has been no passenger rail service to Concord since 1981. Historically, the Boston and Maine Railroad served the city.

The area that would become Concord was originally settled thousands of years ago by Abenaki Native Americans called the Pennacook. The tribe fished for migrating salmon, sturgeon, and alewives with nets strung across the rapids of the Merrimack River. The stream was also the transportation route for their birch bark canoes, which could travel from Lake Winnipesaukee to the Atlantic Ocean. The broad sweep of the Merrimack River valley floodplain provided good soil for farming beans, gourds, pumpkins, melons and maize.

The area was first settled by Europeans in 1659 as Penacook, after the Abenaki word "pannukog" meaning "bend in the river," referencing the steep bends of the Merrimack River through the area. On January 17, 1725, the Province of Massachusetts Bay, which then claimed territories west of the Merrimack, granted the Concord area as the Plantation of Penacook. It was settled between 1725 and 1727 by Captain Ebenezer Eastman and others from Haverhill, Massachusetts. On February 9, 1734, the town was incorporated as "Rumford", from which Sir Benjamin Thompson, Count Rumford, would take his title. It was renamed "Concord" in 1765 by Governor Benning Wentworth following a bitter boundary dispute between Rumford and the town of Bow; the city name was meant to reflect the new concord, or harmony, between the disputant towns. Citizens displaced by the resulting border adjustment were given land elsewhere as compensation. In 1779, New Pennacook Plantation was granted to Timothy Walker Jr. and his associates at what would be incorporated in 1800 as Rumford, Maine, the site of Pennacook Falls.

Concord grew in prominence throughout the 18th century, and some of the earliest houses from this period survive at the northern end of Main Street. In the years following the Revolution, Concord's central geographical location made it a logical choice for the state capital, particularly after Samuel Blodget in 1807 opened a canal and lock system to allow vessels passage around the Amoskeag Falls downriver, connecting Concord with Boston by way of the Middlesex Canal. In 1808, Concord was named the official seat of state government, and in 1816 architect Stuart Park was commissioned to design a new capitol building for the state legislature on land sold to the state by local Quakers. Construction on the State House was completed in 1819, and it remains the oldest capitol in the nation in which the state's legislative branches meet in their original chambers. Concord was also named the seat of Merrimack County in 1823, and the Merrimack County Courthouse was constructed in 1857 in the North End at the site of the Old Town House.

In the early 19th century, much of the city's economy was dominated by furniture-making, printing, and granite quarrying; granite had become a popular building material for many monumental halls in the early United States, and Concord granite was used in the construction of both the New Hampshire State House and the Library of Congress in Washington, D.C. In 1828, Lewis Downing joined J. Stephens Abbot to form Abbot and Downing. Their most famous product was their Concord coach, widely used in the development of the American West, and their enterprise largely boosted and changed the city economy in the mid-19th century. In subsequent years, Concord would also become a hub for the railroad industry, with Penacook a textile manufacturing center using water power from the Contoocook River. The city also around this time started to become a center for the emerging healthcare industry, with New Hampshire State Hospital opening in 1842 as one of the first psychiatric hospitals in the United States. The State Hospital continued to expand throughout the following decades, and in 1891 Concord Hospital opened its doors as Margaret Pillsbury General Hospital, the first general hospital in the state of New Hampshire.

Concord's economy changed once again in the 20th century with the declining railroad and textile industry. The city developed into a center for national politics due to New Hampshire's first-in-the-nation primary, and many presidential candidates still visit the Concord area during campaign season. The city also developed an identity within the emerging space industry, with the McAuliffe-Shepard Discovery Center opening in 1990 to commemorate Alan Shepard, the first American in space from nearby Derry, and Christa McAuliffe, a teacher at Concord High School who died in the 1986 Space Shuttle "Challenger" disaster. Today, Concord remains a center for politics, law, healthcare, and insurance companies.
Concord is located in south-central New Hampshire at (43.2070, −71.5371). It is north of the Massachusetts border, west of the Maine border, east of the Vermont border, and south of the Canadian border at Pittsburg.

According to the United States Census Bureau, the city has a total area of . of it are land and of it are water, comprising 4.81% of the city. Concord is drained by the Merrimack River. Penacook Lake, the largest lake in the city and its main source of water, is in the west. The highest point in Concord is above sea level on Oak Hill, just west of the hill's summit in neighboring Loudon.

Concord lies fully within the Merrimack River watershed and is centered on the river, which runs from northwest to southeast through the city. Downtown is located on a low terrace to the west of the river, with residential neighborhoods climbing hills to the west and extending southwards towards the town of Bow. To the east of the Merrimack, atop a bluff, is a flat, sandy plain known as Concord Heights, which has seen most of the city's commercial development since 1960. The eastern boundary of Concord (with the town of Pembroke) is formed by the Soucook River, a tributary of the Merrimack. The Turkey River winds through the southwestern quarter of the city, passing through the campus of St. Paul's School before entering the Merrimack River in Bow. In the northern part of the city, the Contoocook River enters the Merrimack at the village of Penacook.
Concord is north of Manchester, New Hampshire's largest city, and north of Boston.

The city of Concord is made up of its downtown, including its North End and South End neighborhoods, plus the four distinct villages of Penacook, Concord Heights, East Concord, and West Concord.


Concord, as with much of New England, is within the humid continental climate zone (Köppen "Dfb"), with long, cold, snowy winters, warm (and at times humid) summers, and relatively brief autumns and springs. In winter, successive storms deliver moderate to at times heavy snowfall amounts, contributing to the relatively reliable snow cover. In addition, lows reach below on an average 15 nights per year, and the city straddles the border between USDA Hardiness Zone 5b and 6a. However, thaws are frequent, with one to three days per month with + highs from December to February. Summer can bring stretches of humid conditions as well as thunderstorms, and there is an average of 12 days of + highs annually. The window for freezing temperatures on average begins on September 27 and expires on May 14.

The monthly daily average temperature range from in January to in July. Temperature extremes have ranged from in February 1943 to in July 1966.

As of the census of 2020, there were 43,976 people residing in the city. The population density was . At the 2010 Census there were 42,695 residents and 10,052 families in the city, as well as 18,852 housing units at an average density of . The racial makeup of the city in 2020 was 84.5% White, 4.9% Black or African American, 1.0% Native American, 4.9% Asian, 0.1% Pacific Islander, 0.4% from some other race, and 1.8% from two or more races. 4.9% of the population were Hispanic or Latino of any race.

In 2010 there were 17,592 households, out of which 28.7% had children under the age of 18 living with them, 41.3% were headed by married couples living together, 11.6% had a female householder with no husband present, and 42.9% were non-families. 33.6% of all households were made up of individuals, and 12.0% were someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 2.90.

In the city, the population was spread out, with 20.7% under the age of 18, 9.3% from 18 to 24, 28.0% from 25 to 44, 28.2% from 45 to 64, and 13.8% who were 65 years of age or older. The median age was 39.4 years. For every 100 females, there were 98.5 males. For every 100 females age 18 and over, there were 96.9 males.

For the period 2009–2011, the estimated median annual income for a household in the city was $52,695, and the median income for a family was $73,457. Male full-time workers had a median income of $49,228 versus $38,782 for females. The per capita income for the city was $29,296. About 5.5% of families and 10.1% of the population were below the poverty line, including 8.4% of those under age 18 and 5.5% of those age 65 or over.

In 2020, the top employer in the city remained the State of New Hampshire, with over 6,000 employed workers, while the largest private employer was Concord Hospital, with just under 3,000 employees. According to the City of Concord's Comprehensive Annual Financial Report, the top 10 employers in the city for the Fiscal Year 2020 were:

Interstate 89 and Interstate 93 are the two main interstate highways serving Concord, and join just south of the city limits. Interstate 89 links Concord with Lebanon and the state of Vermont to the northwest, while Interstate 93 connects the city to Plymouth, Littleton, and the White Mountains to the north and Manchester and Boston to the south. Interstate 393 is a spur highway leading east from Concord and merging with U.S. Route 4 as a direct route to New Hampshire's Seacoast region. North-south U.S. Route 3 serves as Concord's Main Street, while U.S. Route 202 and New Hampshire Route 9 cross the city from east to west. State routes 13 and 132 also serve the city: Route 13 leads southwest out of Concord towards Goffstown and Milford, while Route 132 travels north parallel to Interstate 93. New Hampshire Route 106 passes through the easternmost part of Concord, crossing I-393 and NH 9 before crossing the Soucook River south into the town of Pembroke. To the north, NH 106 leads to Loudon, Belmont and Laconia.

Historically, Concord served as an important railroad terminal and station for the Boston and Maine Railroad. The former Concord Station was located at what is now a Burlington department store on Storrs Street. The station itself was built in 1860, but the fourth and most famous iteration of the station was built in 1885, which had a brick head house designed by Bradford L. Gilbert. The head house was demolished in 1959 and replaced by a smaller "McGinnis Era" station. By 1967, all passenger rail services to Concord had been discontinued. For 13 months from 1980 to 1981, MBTA Commuter Rail ran two round trips a day between Boston and Concord. The service was discontinued after federal funding was pulled by the Reagan administration. Since then, there has not been any passenger rail service to Concord.

In 2021, Amtrak announced their plan to implement new service between Boston and Concord by 2035.

Local bus service is provided by Concord Area Transit (CAT), with three routes through the city. Regional bus service provided by Concord Coach Lines and Greyhound Lines is available from the Concord Transportation Center at 30 Stickney Avenue next to Exit 14 on Interstate 93, with service south to Boston and points in between, as well as north to Littleton and northeast to Berlin.

General aviation services are available through Concord Municipal Airport, located east of downtown. There is no commercial air service within the city limits; the nearest such airport is Manchester–Boston Regional Airport, to the south.

Concord's downtown underwent a significant renovation between 2015 and 2016, during the city's "Complete Streets Improvement Project". At a proposed cost of $12 million, the project promised to deliver on categories of maintenance to aging infrastructure, improved accessibility, increased sustainability, a safer experience for walkers, bikers and motorists alike, and to stimulate economic growth in an increasingly idle downtown. The main infrastructural change was reducing the four-lane street (two in each direction) to two lanes plus a turning lane in the center. The freed-up space would contribute to extra width for bikes to ride in either direction, increased curb size and an added median where there is no need for a turning lane. Concord opted to add shared lane markings for bikes, rather than a dedicated protected bike lane.

By adding curb space, this project created new opportunities for pedestrians to enjoy the downtown. Many power lines were buried, and street trees, colorful benches, art installations, and other green spaces were added, all allowing people to reclaim a space long dominated by cars. Main Street underwent serious traffic calming, including a road diet, increased diagonal parking, widening sidewalks, adding shared lane markings, adding trees, texturing medians and coloring crosswalks red. Another aspect of the new construction was adding heated sidewalk capabilities, utilizing excess steam from the local Concord Steam plant, and minimizing sand and snow blowing needed during the winter months.

Funding for Complete Streets came from a combination of $4,710,000 from a USDOT TIGER grant and the rest from the City of Concord. The project was initially proposed as costing $7,850,000, but ran over budget due to overambitious ideas. After scrapping some of the most expensive offenders, the budget ended up at $14.2 million, with the project actually coming in $1.1 million below that. Although adding final aesthetic touches with the extra money were debated, the city council ended up deciding to save for financially straining years ahead. The design was carried out by McFarland Johnson, IBI Group, and City of Concord Engineering.

Concord is governed via the council-manager system. The city council consists of a mayor and 14 councilors, ten of which are elected to two-year terms representing each of the city wards, while the other four are elected at-large to four-year terms. The mayor is elected directly every two years. The current mayor as of 2024 is Byron Champlin, who was elected on November 7, 2023, with more than 75% of the vote.

According to the Concord city charter, the mayor chairs the council, however has very few formal powers over the day-to-day management of the city. The actual operations of the city are overseen by the city manager, currently Thomas J. Aspell, Jr. The current police chief is Bradley S. Osgood.

In the New Hampshire Senate, Concord is in the 15th District, represented by Democrat Becky Whitley since December 2020. On the New Hampshire Executive Council, Concord is in the 2nd District, represented by Cinde Warmington, the sole Democrat on the council. In the United States House of Representatives, Concord is in New Hampshire's 2nd congressional district, represented by Democrat Ann McLane Kuster.

New Hampshire Department of Corrections operates the New Hampshire State Prison for Men and New Hampshire State Prison for Women in Concord.

Concord leans strongly Democratic in presidential elections; the last Republican nominee to carry the city was then Vice President George H. W. Bush in 1988. Voter turnout was 72.7% in the 2020 general election, down from 76.2% in 2016, but still above the 2020 national turnout of 66.7%.

Newspapers and journals

Radio

The city is otherwise served by . New Hampshire Public Radio is headquartered in Concord.

Television

The New Hampshire State House, designed by architect Stuart Park and constructed between 1815 and 1818, is the oldest state house in which the legislature meets in its original chambers. The building was remodeled in 1866, and the third story and west wing were added in 1910.

Across from the State House is the Eagle Hotel on Main Street, which has been a downtown landmark since its opening in 1827. U.S. Presidents Ulysses S. Grant, Rutherford Hayes, and Benjamin Harrison all dined there, and Franklin Pierce spent the night before departing for his inauguration. Other well-known guests included Jefferson Davis, Charles Lindbergh, Eleanor Roosevelt, Richard M. Nixon (who carried New Hampshire in all three of his presidential bids), and Thomas E. Dewey. The hotel closed in 1961.

South from the Eagle Hotel on Main Street is Phenix Hall, which replaced "Old" Phenix Hall, which burned in 1893. Both the old and new buildings featured multi-purpose auditoriums used for political speeches, theater productions, and fairs. Abraham Lincoln spoke at the old hall in 1860; Theodore Roosevelt, at the new hall in 1912.

North on Main Street is the Walker-Woodman House, also known as the Reverend Timothy Walker House, the oldest standing two-story house in Concord. It was built for the Reverend Timothy Walker between 1733 and 1735.

On the north end of Main Street is the Pierce Manse, in which President Franklin Pierce lived in Concord before and following his presidency. The mid-1830s Greek Revival house was moved from Montgomery Street to North Main Street in 1971 to prevent its demolition.

Beaver Meadow Golf Course, located in the northern part of Concord, is one of the oldest golf courses in New England. Besides this golf course, other important sporting venues in Concord include Everett Arena and Memorial Field.

The SNOB (Somewhat North Of Boston) Film Festival, started in the fall of 2002, brings independent films and filmmakers to Concord and has provided an outlet for local filmmakers to display their films. SNOB Film Festival was a catalyst for the building of Red River Theatres, a locally owned, nonprofit, independent cinema in 2007. The SNOB Film Festival is one of the many arts organizations in the city.

Other sites of interest include the Capitol Center for the Arts, the New Hampshire Historical Society, which has two facilities in Concord, and the McAuliffe-Shepard Discovery Center, a science museum named after Christa McAuliffe, the Concord teacher who died during the Space Shuttle Challenger disaster in 1986, and Alan Shepard, the Derry-born astronaut who was the second person and first American in space as well as the fifth and oldest person to walk on the Moon.

Concord's public schools are within the Concord School District, except for schools in the Penacook area of the city, which are within the Merrimack Valley School District, a district which also includes several towns north of Concord. The only public high school in the Concord School District is Concord High School, which has about 2,000 students. The only public middle school in the Concord School District is Rundlett Middle School, which has roughly 1,500 students. Concord School District's elementary schools underwent a major re-configuration in 2012, with three newly constructed schools opening and replacing six previous schools. Kimball School and Walker School were replaced by Christa McAuliffe School on the Kimball School site, Conant School (and Rumford School, which closed a year earlier) were replaced by Abbot-Downing School at the Conant site, and Eastman and Dame schools were replaced by Mill Brook School, serving kindergarten through grade two, located next to Broken Ground Elementary School, serving grades three to five. Beaver Meadow School, the remaining elementary school, was unaffected by the changes.

Concord schools in the Merrimack Valley School District include Merrimack Valley High School and Merrimack Valley Middle School, which are adjacent to each other and to Rolfe Park in Penacook village, and Penacook Elementary School, just south of the village.

Concord has two parochial schools, Bishop Brady High School and Saint John Regional School.

Other area private schools include Concord Christian Academy, Parker Academy, Trinity Christian School, and Shaker Road School. Also in Concord is St. Paul's School, a boarding school located in the city's West End neighborhood.

Concord is home to New Hampshire Technical Institute, the city's primary community college, and Granite State College, which offers online two-year and four-year degrees. The University of New Hampshire School of Law is located near downtown, and the Franklin Pierce University Doctorate of Physical Therapy program also has a location in the city. Concord Hospital recently announced plans to open a joint program with the New England College School of Nursing as part of their Bachelor of Nursing degree. Concord is also a major clinical site of Dartmouth College's Geisel School of Medicine, New Hampshire's only medical school.


Chlorophyceae

The Chlorophyceae are one of the classes of green algae, distinguished mainly on the basis of ultrastructural morphology. They are usually green due to the dominance of pigments chlorophyll a and chlorophyll b. The chloroplast may be discoid, plate-like, reticulate, cup-shaped, spiral- or ribbon-shaped in different species. Most of the members have one or more storage bodies called pyrenoids located in the chloroplast. Pyrenoids contain protein besides starch. Some green algae may store food in the form of oil droplets. They usually have a cell wall made up of an inner layer of cellulose and outer layer of pectose.


Vegetative reproduction usually takes place by fragmentation. Asexual reproduction is by flagellated zoospores. And haplospore, perennation (akinate and palmella stage). Asexual reproduction by mitospore absent in spyrogyra.
Sexual reproduction shows considerable variation in the type and formation of sex cells and it may be isogamous e.g. "Chlamydomonas, Ulothrix", anisogamous e.g. "Chlamydomonas, Eudorina" or Oogamous e.g. "Chlamydomonas, Volvox". "Chlamydomonas" has all three types of sexual reproduction.
They share many similarities with the higher plants, including the presence of asymmetrical flagellated cells, the breakdown of the nuclear envelope at mitosis, and the presence of phytochromes, flavonoids, and the chemical precursors to the cuticle.

The sole method of reproduction in "Chlorella" is asexual and azoosporic. The content of the cell divides into 2,4 (B), 8(C) sometimes daughter protoplasts. Each daughter protoplast rounds off to form a non-motile spore. These autospores (spores having the same distinctive shape as the parent cell) are liberated by the rupture of the parent cell wall (D). On release each autospore grows to become a new individual. The presence of sulphur in the culture medium is considered essential for cell division. It takes place even in the dark with sulphur alone as the source material but under light conditions nitrogen also required in addition. Pearsal and Loose (1937) reported the occurrence of motile cells in "Chlorella". Bendix (1964) also observed that "Chlorella" produces motile cells which might be gametes. These observations have an important bearing on the concept of the life cycle of "Chlorella," which at present is considered to be strictly asexual in character.

Asexual reproduction in "Chlorella ellipsoides" has been studied in detail and the following four phases have been observed during the asexual reproduction.

(i) Growth Phase - During this phase the cells grow in size by utilizing the photosynthetic products.

(ii) Ripening phase - In this phase the cells mature and prepare themselves for division.

(iii) Post ripening phase - During this phase, each mature cell divides twice either in dark or in light. The cells formed in dark are known as dark to light phase, cells again grow in size.

(iv) Division Phase - During this phase the parent cell wall ruptures and unicells are released.

, AlgaeBase accepted the following orders in the class Chlorophyceae:

Along with these genera, AlgaeBase recognizes several taxa that are incertae sedis (i.e. unplaced to an order):

Other orders that have been recognized include:

In older classifications, the term Chlorophyceae is sometimes used to apply to all the green algae except the Charales, and the internal division is considerably different.



Cyril

Cyril (also Cyrillus or Cyryl) is a masculine given name. It is derived from the Greek name ("Kýrillos"), meaning 'lordly, masterful', which in turn derives from Greek ("kýrios") 'lord'. There are various variant forms of the name "Cyril" such as "Cyrill", "Cyrille", "Ciril", "Kirill", "Kiryl", "Kirillos", "Kyrylo", "Kiril", "Kiro", "Kyril", "Kyrill" and "Quirrel". 

It may also refer to:





Computational complexity

In computer science, the computational complexity or simply complexity of an algorithm is the amount of resources required to run it. Particular focus is given to computation time (generally measured by the number of needed elementary operations) and memory storage requirements. The complexity of a problem is the complexity of the best algorithms that allow solving the problem.

The study of the complexity of explicitly given algorithms is called analysis of algorithms, while the study of the complexity of problems is called computational complexity theory. Both areas are highly related, as the complexity of an algorithm is always an upper bound on the complexity of the problem solved by this algorithm. Moreover, for designing efficient algorithms, it is often fundamental to compare the complexity of a specific algorithm to the complexity of the problem to be solved. Also, in most cases, the only thing that is known about the complexity of a problem is that it is lower than the complexity of the most efficient known algorithms. Therefore, there is a large overlap between analysis of algorithms and complexity theory. 

As the amount of resources required to run an algorithm generally varies with the size of the input, the complexity is typically expressed as a function , where is the size of the input and is either the worst-case complexity (the maximum of the amount of resources that are needed over all inputs of size ) or the average-case complexity (the average of the amount of resources over all inputs of size ). Time complexity is generally expressed as the number of required elementary operations on an input of size , where elementary operations are assumed to take a constant amount of time on a given computer and change only by a constant factor when run on a different computer. Space complexity is generally expressed as the amount of memory required by an algorithm on an input of size .

The resource that is most commonly considered is time. When "complexity" is used without qualification, this generally means time complexity.

The usual units of time (seconds, minutes etc.) are not used in complexity theory because they are too dependent on the choice of a specific computer and on the evolution of technology. For instance, a computer today can execute an algorithm significantly faster than a computer from the 1960s; however, this is not an intrinsic feature of the algorithm but rather a consequence of technological advances in computer hardware. Complexity theory seeks to quantify the intrinsic time requirements of algorithms, that is, the basic time constraints an algorithm would place on "any" computer. This is achieved by counting the number of "elementary operations" that are executed during the computation. These operations are assumed to take constant time (that is, not affected by the size of the input) on a given machine, and are often called "steps".

Formally, the "bit complexity" refers to the number of operations on bits that are needed for running an algorithm. With most models of computation, it equals the time complexity up to a constant factor. On computers, the number of operations on machine words that are needed is also proportional to the bit complexity. So, the "time complexity" and the "bit complexity" are equivalent for realistic models of computation.

Another important resource is the size of computer memory that is needed for running algorithms.

For the class of distributed algorithms that are commonly executed by multiple, interacting parties, the resource that is of most interest is the communication complexity. It is the necessary amount of communication between the executing parties.

The number of arithmetic operations is another resource that is commonly used. In this case, one talks of arithmetic complexity. If one knows an upper bound on the size of the binary representation of the numbers that occur during a computation, the time complexity is generally the product of the arithmetic complexity by a constant factor.
For many algorithms the size of the integers that are used during a computation is not bounded, and it is not realistic to consider that arithmetic operations take a constant time. Therefore, the time complexity, generally called bit complexity in this context, may be much larger than the arithmetic complexity. For example, the arithmetic complexity of the computation of the determinant of a integer matrix is formula_1 for the usual algorithms (Gaussian elimination). The bit complexity of the same algorithms is exponential in , because the size of the coefficients may grow exponentially during the computation. On the other hand, if these algorithms are coupled with multi-modular arithmetic, the bit complexity may be reduced to . 

In sorting and searching, the resource that is generally considered is the number of entry comparisons. This is generally a good measure of the time complexity if data are suitably organized.

It is impossible to count the number of steps of an algorithm on all possible inputs. As the complexity generally increases with the size of the input, the complexity is typically expressed as a function of the size (in bits) of the input, and therefore, the complexity is a function of . However, the complexity of an algorithm may vary dramatically for different inputs of the same size. Therefore, several complexity functions are commonly used.

The worst-case complexity is the maximum of the complexity over all inputs of size , and the average-case complexity is the average of the complexity over all inputs of size (this makes sense, as the number of possible inputs of a given size is finite). Generally, when "complexity" is used without being further specified, this is the worst-case time complexity that is considered.

It is generally difficult to compute precisely the worst-case and the average-case complexity. In addition, these exact values provide little practical application, as any change of computer or of model of computation would change the complexity somewhat. Moreover, the resource use is not critical for small values of , and this makes that, for small , the ease of implementation is generally more interesting than a low complexity.

For these reasons, one generally focuses on the behavior of the complexity for large , that is on its asymptotic behavior when tends to the infinity. Therefore, the complexity is generally expressed by using big O notation.

For example, the usual algorithm for integer multiplication has a complexity of formula_2 this means that there is a constant formula_3 such that the multiplication of two integers of at most digits may be done in a time less than formula_4 This bound is "sharp" in the sense that the worst-case complexity and the average-case complexity are formula_5 which means that there is a constant formula_6 such that these complexities are larger than formula_7 The radix does not appear in these complexity, as changing of radix changes only the constants formula_3 and formula_9

The evaluation of the complexity relies on the choice of a model of computation, which consists in defining the basic operations that are done in a unit of time. When the model of computation is not explicitly specified, this is generally meant as being multitape Turing machine.

A deterministic model of computation is a model of computation such that the successive states of the machine and the operations to be performed are completely determined by the preceding state. Historically, the first deterministic models were recursive functions, lambda calculus, and Turing machines. The model of random-access machines (also called RAM-machines) is also widely used, as a closer counterpart to real computers.

When the model of computation is not specified, it is generally assumed to be a multitape Turing machine. For most algorithms, the time complexity is the same on multitape Turing machines as on RAM-machines, although some care may be needed in how data is stored in memory to get this equivalence.

In a non-deterministic model of computation, such as non-deterministic Turing machines, some choices may be done at some steps of the computation. In complexity theory, one considers all possible choices simultaneously, and the non-deterministic time complexity is the time needed, when the best choices are always done. In other words, one considers that the computation is done simultaneously on as many (identical) processors as needed, and the non-deterministic computation time is the time spent by the first processor that finishes the computation. This parallelism is partly amenable to quantum computing via superposed entangled states in running specific quantum algorithms, like e.g. Shor's factorization of yet only small integers (: 21 = 3 × 7).

Even when such a computation model is not realistic yet, it has theoretical importance, mostly related to the P = NP problem, which questions the identity of the complexity classes formed by taking "polynomial time" and "non-deterministic polynomial time" as least upper bounds. Simulating an NP-algorithm on a deterministic computer usually takes "exponential time". A problem is in the complexity class NP, if it may be solved in polynomial time on a non-deterministic machine. A problem is NP-complete if, roughly speaking, it is in NP and is not easier than any other NP problem. Many combinatorial problems, such as the Knapsack problem, the travelling salesman problem, and the Boolean satisfiability problem are NP-complete. For all these problems, the best known algorithm has exponential complexity. If any one of these problems could be solved in polynomial time on a deterministic machine, then all NP problems could also be solved in polynomial time, and one would have P = NP. it is generally conjectured that with the practical implication that the worst cases of NP problems are intrinsically difficult to solve, i.e., take longer than any reasonable time span (decades!) for interesting lengths of input.

Parallel and distributed computing consist of splitting computation on several processors, which work simultaneously. The difference between the different model lies mainly in the way of transmitting information between processors. Typically, in parallel computing the data transmission between processors is very fast, while, in distributed computing, the data transmission is done through a network and is therefore much slower.

The time needed for a computation on processors is at least the quotient by of the time needed by a single processor. In fact this theoretically optimal bound can never be reached, because some subtasks cannot be parallelized, and some processors may have to wait a result from another processor.

The main complexity problem is thus to design algorithms such that the product of the computation time by the number of processors is as close as possible to the time needed for the same computation on a single processor.

A quantum computer is a computer whose model of computation is based on quantum mechanics. The Church–Turing thesis applies to quantum computers; that is, every problem that can be solved by a quantum computer can also be solved by a Turing machine. However, some problems may theoretically be solved with a much lower time complexity using a quantum computer rather than a classical computer. This is, for the moment, purely theoretical, as no one knows how to build an efficient quantum computer.

Quantum complexity theory has been developed to study the complexity classes of problems solved using quantum computers. It is used in post-quantum cryptography, which consists of designing cryptographic protocols that are resistant to attacks by quantum computers.

The complexity of a problem is the infimum of the complexities of the algorithms that may solve the problem, including unknown algorithms. Thus the complexity of a problem is not greater than the complexity of any algorithm that solves the problems.

It follows that every complexity of an algorithm, that is expressed with big O notation, is also an upper bound on the complexity of the corresponding problem.

On the other hand, it is generally hard to obtain nontrivial lower bounds for problem complexity, and there are few methods for obtaining such lower bounds.

For solving most problems, it is required to read all input data, which, normally, needs a time proportional to the size of the data. Thus, such problems have a complexity that is at least linear, that is, using big omega notation, a complexity formula_10

The solution of some problems, typically in computer algebra and computational algebraic geometry, may be very large. In such a case, the complexity is lower bounded by the maximal size of the output, since the output must be written. For example, a system of polynomial equations of degree in indeterminates may have up to formula_11 complex solutions, if the number of solutions is finite (this is Bézout's theorem). As these solutions must be written down, the complexity of this problem is formula_12 For this problem, an algorithm of complexity formula_13 is known, which may thus be considered as asymptotically quasi-optimal.

A nonlinear lower bound of formula_14 is known for the number of comparisons needed for a sorting algorithm. Thus the best sorting algorithms are optimal, as their complexity is formula_15 This lower bound results from the fact that there are ways of ordering objects. As each comparison splits in two parts this set of orders, the number of of comparisons that are needed for distinguishing all orders must verify formula_16 which implies formula_17 by Stirling's formula.

A standard method for getting lower bounds of complexity consists of "reducing" a problem to another problem. More precisely, suppose that one may encode a problem of size into a subproblem of size of a problem , and that the complexity of is formula_18 Without loss of generality, one may suppose that the function increases with and has an inverse function . Then the complexity of the problem is formula_19 This is the method that is used to prove that, if P ≠ NP (an unsolved conjecture), the complexity of every NP-complete problem is formula_20 for every positive integer .

Evaluating the complexity of an algorithm is an important part of algorithm design, as this gives useful information on the performance that may be expected.

It is a common misconception that the evaluation of the complexity of algorithms will become less important as a result of Moore's law, which posits the exponential growth of the power of modern computers. This is wrong because this power increase allows working with large input data (big data). For example, when one wants to sort alphabetically a list of a few hundreds of entries, such as the bibliography of a book, any algorithm should work well in less than a second. On the other hand, for a list of a million of entries (the phone numbers of a large town, for example), the elementary algorithms that require formula_21 comparisons would have to do a trillion of comparisons, which would need around three hours at the speed of 10 million of comparisons per second. On the other hand, the quicksort and merge sort require only formula_22 comparisons (as average-case complexity for the former, as worst-case complexity for the latter). For , this gives approximately 30,000,000 comparisons, which would only take 3 seconds at 10 million comparisons per second.

Thus the evaluation of the complexity may allow eliminating many inefficient algorithms before any implementation. This may also be used for tuning complex algorithms without testing all variants. By determining the most costly steps of a complex algorithm, the study of complexity allows also focusing on these steps the effort for improving the efficiency of an implementation.



Coercion

Coercion involves compelling a party to act in an involuntary manner by the use of threats, including threats to use force against that party. It involves a set of forceful actions which violate the free will of an individual in order to induce a desired response. These actions may include extortion, blackmail, or even torture and sexual assault.

Common-law systems codify the act of violating a law while under coercion as a duress crime. 

Coercion used as leverage may force victims to act in a way contrary to their own interests. Coercion can involve not only the infliction of bodily harm, but also psychological abuse (the latter intended to enhance the perceived credibility of the threat). The threat of further harm may also lead to the acquiescence of the person being coerced.

The concepts of coercion and persuasion are similar, but various factors distinguish the two. These include the intent, the willingness to cause harm, the result of the interaction, and the options available to the coerced party.

John Rawls, Thomas Nagel, Ronald Dworkin, and other political authors argue that the state is coercive. In 1919, Max Weber (1864–1920), building on the view of Ihering (1818–1892),
defined a state as "a human community that (successfully) claims a monopoly on the legitimate use of physical force".
Morris argues that the state can operate through incentives rather than coercion. Healthcare systems may use informal coercion to make a patient adhere to a doctor's treatment plan. Under certain circumstances, medical staff may use physical coercion to treat a patient involuntarily.

The purpose of coercion is to substitute one's aims for those of the victim. For this reason, many social philosophers have considered coercion as the polar opposite to freedom.

Various forms of coercion are distinguished: first on the basis of the "kind of injury" threatened, second according to its "aims" and "scope", and finally according to its "effects", from which its legal, social, and ethical implications mostly depend.

Physical coercion is the most commonly considered form of coercion, where the content of the conditional threat is the use of force against a victim, their relatives or property. An often used example is "putting a gun to someone's head" ("at gunpoint") or putting a "knife under the throat" ("at knifepoint" or cut-throat) to compel action under the threat that non-compliance may result in the attacker harming or even killing the victim. These are so common that they are also used as metaphors for other forms of coercion.

Armed forces in many countries use firing squads to maintain discipline and intimidate the masses, or opposition, into submission or silent compliance. However, there also are nonphysical forms of coercion, where the threatened injury does not immediately imply the use of force. Byman and Waxman (2000) define coercion as "the use of threatened force, including the limited use of actual force to back up the threat, to induce an adversary to behave differently than it otherwise would." Coercion does not in many cases amount to destruction of property or life since compliance is the goal.

In psychological coercion, the threatened injury regards the victim's relationships with other people. The most obvious example is "blackmail", where the threat consists of the dissemination of damaging information. However, many other types are possible e.g. "emotional blackmail", which typically involves threats of rejection from or disapproval by a peer-group, or creating feelings of guilt/obligation via a display of anger or hurt by someone whom the victim loves or respects.



Client–server model

The client–server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers, and service requesters, called clients. Often clients and servers communicate over a computer network on separate hardware, but both client and server may reside in the same system. A server host runs one or more server programs, which share their resources with clients. A client usually does not share any of its resources, but it requests content or service from a server. Clients, therefore, initiate communication sessions with servers, which await incoming requests.
Examples of computer applications that use the client–server model are email, network printing, and the World Wide Web.

The "client-server" characteristic describes the relationship of cooperating programs in an application. The server component provides a function or service to one or many clients, which initiate requests for such services.
Servers are classified by the services they provide. For example, a web server serves web pages and a file server serves computer files. A shared resource may be any of the server computer's software and electronic components, from programs and data to processors and storage devices. The sharing of resources of a server constitutes a "service".

Whether a computer is a client, a server, or both, is determined by the nature of the application that requires the service functions. For example, a single computer can run a web server and file server software at the same time to serve different data to clients making different kinds of requests. The client software can also communicate with server software within the same computer. Communication between servers, such as to synchronize data, is sometimes called "inter-server" or "server-to-server" communication.

Generally, a service is an abstraction of computer resources and a client does not have to be concerned with how the server performs while fulfilling the request and delivering the response. The client only has to understand the response based on the well-known application protocol, i.e. the content and the formatting of the data for the requested service.

Clients and servers exchange messages in a request–response messaging pattern. The client sends a request, and the server returns a response. This exchange of messages is an example of inter-process communication. To communicate, the computers must have a common language, and they must follow rules so that both the client and the server know what to expect. The language and rules of communication are defined in a communications protocol. All protocols operate in the application layer. The application layer protocol defines the basic patterns of the dialogue. To formalize the data exchange even further, the server may implement an application programming interface (API). The API is an abstraction layer for accessing a service. By restricting communication to a specific content format, it facilitates parsing. By abstracting access, it facilitates cross-platform data exchange.

A server may receive requests from many distinct clients in a short period. A computer can only perform a limited number of tasks at any moment, and relies on a scheduling system to prioritize incoming requests from clients to accommodate them. To prevent abuse and maximize availability, the server software may limit the availability to clients. Denial of service attacks are designed to exploit a server's obligation to process requests by overloading it with excessive request rates.
Encryption should be applied if sensitive information is to be communicated between the client and the server.

When a bank customer accesses online banking services with a web browser (the client), the client initiates a request to the bank's web server. The customer's login credentials may be s

An early form of client–server architecture is remote job entry, dating at least to OS/360 (announced 1964), where the request was to run a job, and the response was the output.

While formulating the client–server model in the 1960s and 1970s, computer scientists building ARPANET (at the Stanford Research Institute) used the terms "server-host" (or "serving host") and "user-host" (or "using-host"), and these appear in the early documents RFC 5 and RFC 4. This usage was continued at Xerox PARC in the mid-1970s.

One context in which researchers used these terms was in the design of a computer network programming language called Decode-Encode Language (DEL). The purpose of this language was to accept commands from one computer (the user-host), which would return status reports to the user as it encoded the commands in network packets. Another DEL-capable computer, the server-host, received the packets, decoded them, and returned formatted data to the user-host. A DEL program on the user-host received the results to present to the user. This is a client–server transaction. Development of DEL was just beginning in 1969, the year that the United States Department of Defense established ARPANET (predecessor of Internet).

"Client-host" and "server-host" have subtly different meanings than "client" and "server". A host is any computer connected to a network. Whereas the words "server" and "client" may refer either to a computer or to a computer program, "server-host" and "client-host" always refer to computers. The host is a versatile, multifunction computer; "clients" and "servers" are just programs that run on a host. In the client–server model, a server is more likely to be devoted to the task of serving.

An early use of the word "client" occurs in "Separating Data from Function in a Distributed File System", a 1978 paper by Xerox PARC computer scientists Howard Sturgis, James Mitchell, and Jay Israel. The authors are careful to define the term for readers, and explain that they use it to distinguish between the user and the user's network node (the client). By 1992, the word "server" had entered into general parlance.

The client-server model does not dictate that server-hosts must have more resources than client-hosts. Rather, it enables any general-purpose computer to extend its capabilities by using the shared resources of other hosts. Centralized computing, however, specifically allocates a large number of resources to a small number of computers. The more computation is offloaded from client-hosts to the central computers, the simpler the client-hosts can be. It relies heavily on network resources (servers and infrastructure) for computation and storage. A diskless node loads even its operating system from the network, and a computer terminal has no operating system at all; it is only an input/output interface to the server. In contrast, a rich client, such as a personal computer, has many resources and does not rely on a server for essential functions.

As microcomputers decreased in price and increased in power from the 1980s to the late 1990s, many organizations transitioned computation from centralized servers, such as mainframes and minicomputers, to rich clients. This afforded greater, more individualized dominion over computer resources, but complicated information technology management. During the 2000s, web applications matured enough to rival application software developed for a specific microarchitecture. This maturation, more affordable mass storage, and the advent of service-oriented architecture were among the factors that gave rise to the cloud computing trend of the 2010s.

In addition to the client-server model, distributed computing applications often use the peer-to-peer (P2P) application architecture.

In the client-server model, the server is often designed to operate as a centralized system that serves many clients. The computing power, memory and storage requirements of a server must be scaled appropriately to the expected workload. Load-balancing and failover systems are often employed to scale the server beyond a single physical machine.

Load balancing is defined as the methodical and efficient distribution of network or application traffic across multiple servers in a server farm. Each load balancer sits between client devices and backend servers, receiving and then distributing incoming requests to any available server capable of fulfilling them.

In a peer-to-peer network, two or more computers ("peers") pool their resources and communicate in a decentralized system. Peers are coequal, or equipotent nodes in a non-hierarchical network. Unlike clients in a client-server or client-queue-client network, peers communicate with each other directly. In peer-to-peer networking, an algorithm in the peer-to-peer communications protocol balances load, and even peers with modest resources can help to share the load. If a node becomes unavailable, its shared resources remain available as long as other peers offer it. Ideally, a peer does not need to achieve high availability because other, redundant peers make up for any resource downtime; as the availability and load capacity of peers change, the protocol reroutes requests.

Both client-server and master-slave are regarded as sub-categories of distributed peer-to-peer systems.

County Dublin

County Dublin ( or ) is a county in Ireland, and holds its capital city, Dublin. It is located on the island's east coast, within the province of Leinster. Until 1994, County Dublin (excluding the city) was a single local government area; in that year, the county council was divided into three new administrative counties: Dún Laoghaire–Rathdown, Fingal and South Dublin. The four areas form a NUTS III statistical region of Ireland (coded IE061). County Dublin remains a single administrative unit for the purposes of the courts (including the Dublin County Sheriff, circuit court and county registrar), law enforcement (the Garda Dublin metropolitan division) and fire services (Dublin Fire Brigade).

Dublin is Ireland's most populous county, with a population of 1,458,154 – approximately 28% of the Republic of Ireland's total population. Dublin city is the capital and largest city of the Republic of Ireland, as well as the largest city on the island of Ireland. Roughly 9 out of every 10 people in County Dublin lives within Dublin city and its suburbs. Several sizeable towns that are considered separate from the city, such as Rush, Donabate and Balbriggan, are located in the far north of the county. Swords, while separated from the city by a green belt around Dublin Airport, is considered a suburban commuter town and an emerging small city.

The third smallest county by land area, Dublin is bordered by Meath to the west and north, Kildare to the west, Wicklow to the south and the Irish Sea to the east. The southern part of the county is dominated by the Dublin Mountains, which rise to around and contain numerous valleys, reservoirs and forests. The county's east coast is punctuated by several bays and inlets, including Rogerstown Estuary, Broadmeadow Estuary, Baldoyle Bay and most prominently, Dublin Bay. The northern section of the county, today known as Fingal, varies enormously in character, from densely populated suburban towns of the city's commuter belt to flat, fertile plains, which are some of the country's largest horticultural and agricultural hubs.

Dublin is the oldest county in Ireland, and was the first part of the island to be shired following the Norman invasion in the late 1100s. While it is no longer a local government area, Dublin retains a strong identity, and continues to be referred to as both a region and county interchangeably, including at government body level.

County Dublin is named after the city of Dublin, which is an anglicisation of its Old Norse name "Dyflin". The city was founded in the 9th century AD by Viking settlers who established the Kingdom of Dublin. The Viking settlement was preceded by a Christian ecclesiastical site known as "Duiblinn", from which Dyflin took its name. Duiblinn derives from the early Classical Irish / – from (, , ) meaning "black, dark", and () "pool", referring to a dark tidal pool. This tidal pool was located where the River Poddle entered the Liffey, to the rear of Dublin Castle.

The hinterland of Dublin in the Norse period was named in .

In addition to Dyflin, a Gaelic settlement known as Áth Cliath ("ford of hurdles") was located further up the Liffey, near present-day Father Mathew Bridge. means "town of the hurdled ford", with Áth Cliath referring to a fording point along the river. Like Duiblinn, an early Christian monastery was also located at Áth Cliath, on the site that is currently occupied by the Whitefriar Street Carmelite Church.

Dublin was the first county in Ireland to be shired after the Norman Conquest in the late 12th century. The Normans captured the Kingdom of Dublin from its Norse-Gael rulers and the name was used as the basis for the county's official Anglo-Norman (and later English) name. However, in modern Irish the region was named after the Gaelic settlement of "Baile Átha Cliath" or simply "Áth Cliath". As a result, Dublin is one of four counties in Ireland with a different name origin for both Irish and English – the others being Wexford, Waterford and Wicklow, whose English names are also derived from Old Norse.

The earliest recorded inhabitants of present-day Dublin settled along the mouth of the River Liffey. The remains of five wooden fish traps were discovered near Spencer Dock in 2007. These traps were designed to catch incoming fish at high tide and could be retrieved at low tide. Thin-bladed stone axes were used to craft the traps and radiocarbon dating places them in the Late Mesolithic period (–5,700 BCE).

The Vikings invaded the region in the mid-9th century AD and founded what would become the city of Dublin. Over time they mixed with the natives of the area, becoming Norse–Gaels. The Vikings raided across Ireland, Britain, France and Spain during this period and under their rule Dublin developed into the largest slave market in Western Europe. While the Vikings were formidable at sea, the superiority of Irish land forces soon became apparent, and the kingdom's Norse rulers were first exiled from the region as early as 902. Dublin was captured by the High King of Ireland, Máel Sechnaill II, in 980, who freed the kingdom's Gaelic slaves. Dublin was again defeated by Máel Sechnaill in 988 and forced to accept Brehon law and pay taxes to the High King. Successive defeats at the hands of Brian Boru in 999 and, most famously, at the Battle of Clontarf in 1014, relegated Dublin to the status of lesser kingdom.

In 1170, the ousted King of Leinster, Diarmait Mac Murchada, and his Norman allies agreed to capture Dublin at a war council in Waterford. They evaded the intercepting army of High King Ruaidrí Ua Conchobair by marching through the Wicklow Mountains, arriving outside the walls of Dublin in late September. The King of Dublin, Ascall mac Ragnaill, met with Mac Murchada for negotiations; however, while talks were ongoing, the Normans, led by de Cogan and FitzGerald, stormed Dublin and overwhelmed its defenders, forcing mac Ragnaill to flee to the Northern Isles. Separate attempts to retake Dublin were launched by both Ua Conchobair and mac Ragnaill in 1171, both of which were unsuccessful.

The authority over Ireland established by the Anglo-Norman King Henry II was gradually lost during the Gaelic resurgence from the 13th century onwards. English power diminished so significantly that by the early 16th century English laws and customs were restricted to a small area around Dublin known as "The Pale". The Earl of Kildare's failed rebellion in 1535 reignited Tudor interest in Ireland, and Henry VIII proclaimed the Kingdom of Ireland in 1542, with Dublin as its capital. Over the next 60 years the Tudor conquest spread to every corner of the island, which was fully subdued by 1603.

Despite harsh penal laws and unfavourable trade restrictions imposed upon Ireland, Dublin flourished in the 18th century. The Georgian buildings which still define much of Dublin's architectural landscape to this day were mostly built over a 50-year period spanning from about 1750 to 1800. Bodies such as the Wide Streets Commission completely reshaped the city, demolishing most of medieval Dublin in the process. During the Enlightenment, the penal laws were gradually repealed and members of the Protestant Ascendancy began to regard themselves as citizens of a distinct Irish nation. The Irish Patriot Party, led by Henry Grattan, agitated for greater autonomy from Great Britain, which was achieved under the Constitution of 1782. These freedoms proved short-lived, as the Irish Parliament was abolished under the Acts of Union 1800 and Ireland was incorporated into the United Kingdom. Dublin lost its political status as a capital and went into a marked decline throughout the 19th century, leading to widespread demands to repeal the union.

Although at one time the second city of the British Empire, by the late 1800s Dublin was one of the poorest cities in Europe. The city had the worst housing conditions of anywhere in the United Kingdom, and overcrowding, disease and malnourishment were rife within central Dublin. In 1901, "The Irish Times" reported that the disease and mortality rates in Calcutta during the 1897 bubonic plague outbreak compared "favourably with those of Dublin at the present moment". Most of the upper and middle class residents of Dublin had moved to wealthier suburbs, and the grand Georgian homes of the 1700s were converted en masse into tenement slums. In 1911, over 20,000 families in Dublin were living in one-room tenements which they rented from wealthy landlords. Henrietta Street was particularly infamous for the density of its tenements, with 845 people living on the street in 1911, including 19 families – totalling 109 people – living in just one house.

After decades of political unrest, Ireland appeared to be on the brink of civil war as a result of the Home Rule Crisis. Despite being the centre of Irish unionism outside of Ulster, Dublin was overwhelmingly in favour of Home Rule. Unionist parties had performed poorly in the county since the 1870s, leading contemporary historian W. E. H. Lecky to conclude that "Ulster unionism is the only form of Irish unionism that is likely to count as a serious political force". Unlike their counterparts in the north, "southern unionists" were a clear minority in the rest of Ireland, and as such were much more willing to co-operate with the Irish Parliamentary Party (IPP) to avoid partition. Following the Anglo-Irish Treaty, Belfast unionist Dawson Bates decried the "effusive professions of loyalty and confidence in the Provisional Government" that was displayed by former unionists in the new Irish Free State.

The question of Home Rule was put on hold due to the outbreak of the First World War but was never to be revisited as a series of missteps by the British government, such as executing the leaders of the 1916 Easter Rising and the Conscription Crisis of 1918, fuelled the Irish revolutionary period. The IPP were nearly wiped out by Sinn Féin in the 1918 general election and, following a brief war of independence, 26 of Ireland's 32 counties seceded from the United Kingdom in December 1922, with Dublin becoming the capital of the Irish Free State, and later the Republic of Ireland.

From the 1960s onwards, Dublin city greatly expanded due to urban renewal works and the construction of large suburbs such as Tallaght, Coolock and Ballymun, which resettled both the rural and urban poor of County Dublin in newer state-built accommodation. Dublin was the driving force behind Ireland's Celtic Tiger period, an era of rapid economic growth that started in the early 1990s. In stark contrast to the turn of the 20th century, Dublin entered the 21st century as one of Europe's richest cities, attracting immigrants and investment from all over the world.

Dublin is the third smallest of Ireland's 32 counties by area, and the largest in terms of population. It is the third-smallest of Leinster's 12 counties in size and the largest by population. Dublin shares a border with three counties – Meath to the north and west, Kildare to the west and Wicklow to the south. To the east, Dublin has an Irish Sea coastline which stretches for .

Dublin is a topographically varied region. The city centre is generally very low-lying, and many areas of coastal Dublin are at or near sea-level. In the south of the county, the topography rises steeply from sea-level at the coast to over in just a few kilometres. This natural barrier has resulted in densely populated coastal settlements in Dún Laoghaire–Rathdown and westward urban sprawl in South Dublin. In contrast, Fingal is generally rural in nature and much less densely populated than the rest of the county. Consequently, Fingal is significantly larger than the other three local authorities and covers about 49.5% of County Dublin's land area. Fingal is also perhaps the flattest region in Ireland, with the low-lying Naul Hills rising to a maximum height of just .

Dublin is bounded to the south by the Wicklow Mountains. Where the mountains extend into County Dublin, they are known locally as the Dublin Mountains ("Sléibhte Bhaile Átha Cliath"). Kippure, on the Dublin–Wicklow border, is the county's highest mountain, at above sea level. Crossed by the Dublin Mountains Way, they are a popular amenity area, with Two Rock, Three Rock, Tibradden, Ticknock, Montpelier Hill, and Glenasmole being among the most heavily foot-falled hiking destinations in Ireland. Forest cover extends to over within the county, nearly all of which is located in the Dublin Mountains. With just 6.5% of Dublin under forest, it is the 6th least forested county in Ireland.

Much of the county is drained by its three major rivers – the River Liffey, the River Tolka in north Dublin, and the River Dodder in south Dublin. The Liffey, at in length, is the 8th longest river in Ireland, and rises near Tonduff in County Wicklow, reaching the Irish Sea at the Dublin Docklands. The Liffey cuts through the centre of Dublin city, and the resultant Northside–Southside divide is an often used social, economic and linguistic distinction. Notable inlets include the central Dublin Bay, Rogerstown Estuary, the estuary of the Broadmeadow and Killiney Bay, under Killiney Hill. Headlands include Howth Head, Drumanagh and the Portraine Shore. In terms of biodiversity, these estuarine and coastal regions are home to a wealth ecologically important areas. County Dublin contains 11 EU-designated Special Areas of Conservation (SACs) and 11 Special Protection Areas (SPAs).

The bedrock geology of Dublin consists primarily of Lower Carboniferous limestone, which underlies about two thirds of the entire county, stretching from Skerries to Booterstown. During the Lower Carboniferous (ca. 340 Mya), the area was part of a warm tropical sea inhabited by an abundance of corals, crinoids and brachiopods. The oldest rocks in Dublin are the Cambrian shales located on Howth Head, which were laid down ca. 500 Mya. Disruption following the closure of the Iapetus Ocean approximately 400 Mya resulted in the formation of granite. This is now exposed at the surface from the Dublin Mountains to the coastal areas of Dún Laoghaire. 19th-century Lead extraction and smelting at the Ballycorus Leadmines caused widespread lead poisoning, and the area was once nicknamed "Death Valley".

Dublin is in a maritime temperate oceanic region according to Köppen climate classification. Its climate is characterised by cool winters, mild humid summers, and a lack of temperature extremes. Met Éireann have a number of weather stations in the county, with its two primary stations at Dublin Airport and Casement Aerodrome.

Annual temperatures typically fall within a narrow range. In Merrion Square, the coldest month is February, with an average minimum temperature of , and the warmest month is July, with an average maximum temperature of . Due to the urban heat island effect, Dublin city has the warmest summertime nights in Ireland. The average minimum temperature at Merrion Square in July is , similar to London and Berlin, and the lowest July temperature ever recorded at the station was on 3 July 1974. At Dublin Airport, the driest month is February with of rainfall, and the wettest month is November, with of rain on average.

As the prevailing wind direction in Ireland is from the south and west, the Wicklow Mountains create a rain shadow over much of the county. Dublin's sheltered location makes it the driest place in Ireland, receiving only about half the rainfall of the west coast. Ringsend in the south of Dublin city records the lowest rainfall in the country, with an average annual precipitation of . The wettest area of the county is the Glenasmole Valley, which receives of rainfall per year. As a temperate coastal county, snow is relatively uncommon in lowland areas; however, Dublin is particularly vulnerable to heavy snowfall on rare occasions where cold, dry easterly winds dominate during the winter.

During the late summer and early autumn, Dublin can experience Atlantic storms, which bring strong winds and torrential rain to Ireland. Dublin was the county worst-affected by Hurricane Charley in 1986. It caused severe flooding, especially along the River Dodder, and is reputed to be the worst flood event in Dublin's history. Rainfall records were shattered across the county. Kippure recorded of rain over a 24-hour period, the greatest daily rainfall total ever recorded in Ireland. The government allocated IR£6,449,000 (equivalent to US$20.5 million in 2020) to repair the damage wrought by Charley. The two reservoirs at Bohernabreena in the Dublin Mountains were upgraded in 2006 after a study into the impact of Hurricane Charley concluded that a slightly larger storm would have caused the reservoir dams to burst, which would have resulted in catastrophic damage and significant loss of life.

In contrast with the Atlantic Coast, the east coast of Ireland has relatively few islands. County Dublin has one of the highest concentrations of islands on the Irish east coast. Colt Island, St. Patrick's Island, Shenick Island and numerous smaller islets are clustered off the coast of Skerries, and are collectively known as the "Skerries Islands Natural Heritage Area". Further out lies Rockabill, which is Dublin's most isolated island, at about offshore. Lambay Island, at , is the largest island off Ireland's east coast and the easternmost point of County Dublin. Lambay supports one of the largest seabird colonies in Ireland and, curiously, also supports a population of non-native Red-necked wallabies. To the south of Lambay lies a smaller island known as Ireland's Eye – the result of a mistranslation of the island's Irish name by invading Vikings.

Bull Island is a man-made island lying roughly parallel to the shoreline which began to form following the construction of the Bull Wall in 1825. The island is still growing and is currently long and wide. In 1981, North Bull Island ("Oileán an Tairbh Thuaidh") was designated as a UNESCO biosphere.

For statistical purposes at European level, the county as a whole forms the Dublin Region – a NUTS III entity – which is in turn part of the Eastern and Midland Region, a NUTS II entity. Each of the local authorities have representatives on the Eastern and Midland Regional Assembly.

There are ten historic baronies in the county. While baronies continue to be officially defined units, they ceased to have any administrative function following the Local Government Act 1898, and any changes to county boundaries after the mid-19th century are not reflected in their extent. The last boundary change of a barony in Dublin was in 1842, when the barony of Balrothery was divided into Balrothery East and Balrothery West. The largest recorded barony in Dublin in 1872 was Uppercross, at , and the smallest barony was Dublin, at .
Townlands are the smallest officially defined geographical divisions in Ireland. There are 1,090 townlands in Dublin, of which 88 are historic town boundaries. These town boundaries are registered as their own townlands and are much larger than rural townlands. The smallest rural townlands in Dublin are just 1 acre in size, most of which are offshore islands ("Clare Rock Island, Lamb Island, Maiden Rock, Muglins, Thulla Island"). The largest rural townland in Dublin is 2,797 acres ("Caastlekelly"). The average size of a townland in the county (excluding towns) is 205 acres.

Under the Local Government (Ireland) Act 1898, County Dublin was divided into urban districts of Blackrock, Clontarf, Dalkey, Drumcondra, Clonliffe and Glasnevin, Killiney and Ballybrack, Kingstown, New Kilmainham, Pembroke, and Rathmines and Rathgar, and the rural districts of Balrothery, Celbridge No. 2, North Dublin, Rathdown, and South Dublin.

Howth, formerly within the rural district of Dublin North, became an urban district in 1919. Kingstown was renamed Dún Laoghaire in 1920. The rural districts were abolished in 1930.

Balbriggan, in the rural district of Balrothery, had town commissioners under the Towns Improvement (Ireland) Act 1854. This became a town council in 2002. In common with all town councils, it was abolished in 2014.

The urban districts were gradually absorbed by the city of Dublin, except for four coastal districts of Blackrock, Dalkey, Dún Laoghaire, and Killiney and Ballybrack, which formed the borough of Dún Laoghaire in 1930.

The city of Dublin had been administered separately since the 13th century. Under the Local Government (Ireland) Act 1898, the two areas were defined as the administrative county of Dublin and the county borough of Dublin, with the latter in the city area.

In 1985, County Dublin was divided into three electoral counties: Dublin–Belgard to the southwest (South Dublin from 1991), Dublin–Fingal to the north (Fingal from 1991), and Dún Laoghaire–Rathdown to the southeast.

On 1 January 1994, under the Local Government (Dublin) Act 1993, the County Dublin ceased to exist as a local government area, and was succeeded by the counties of Dún Laoghaire–Rathdown, Fingal and South Dublin, each coterminous (with minor boundary adjustments) with the area of the corresponding electoral county. In discussing the legislation, Avril Doyle TD said, "The Bill before us today effectively abolishes County Dublin, and as one born and bred in these parts of Ireland I find it rather strange that we in this House are abolishing County Dublin. I am not sure whether Dubliners realise that that is what we are about today, but in effect that is the case."

Although the Electoral Commission should, as far as practicable, avoid breaching county boundaries when recommending Dáil constituencies, this does not include the boundaries of a city or the boundary between the three counties in Dublin. There is also still a sheriff appointed for County Dublin.

The term "County Dublin" is still in common usage. Many organisations and sporting teams continue to organise on a County Dublin basis. The Placenames Branch of the Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media maintains a Placenames Database that records all placenames, past and present. County Dublin is listed in the database along with the subdivisions of that county. It is also used as an address for areas within Dublin outside of the Dublin postal district system.

For a period in 2020 during the COVID-19 pandemic, to reduce person-to-person contact, government regulations restricted activity to "within the county in which the relevant residence is situated". Within the regulations, the local government areas of "Dún Laoghaire–Rathdown, Fingal, South Dublin and Dublin City" were deemed to be a single county (as were the city and the county of Cork, and the city and the county of Galway).

The latest Ordnance Survey Ireland "Discovery Series" (Third Edition 2005) 1:50,000 map of the Dublin Region, Sheet 50, shows the boundaries of the city and three surrounding counties of the region. Extremities of the Dublin Region, in the north and south of the region, appear in other sheets of the series, 43 and 56 respectively.

There are four local authorities whose remit collectively encompasses the geographic area of the county and city of Dublin. These are Dublin City Council, South Dublin County Council, Dún Laoghaire–Rathdown County Council and Fingal County Council.

Until 1 January 1994, the administrative county of Dublin was administered by Dublin County Council. From that date, its functions were succeeded by Dún Laoghaire–Rathdown County Council, Fingal County Council and South Dublin County Council, each with its county seat, respectively administering the new counties established on that date.

The city was previously designated a county borough and administered by Dublin Corporation. Under the Local Government Act 2001, the country was divided into local government areas of cities and counties, with the county borough of Dublin being designated a city for all purposes, now administered by Dublin City Council. Each local authority is responsible for certain local services such as sanitation, planning and development, libraries, the collection of motor taxation, local roads and social housing.

Dublin, comprising the four local government areas in the county, is a strategic planning area within the Eastern and Midland Regional Assembly (EMRA). It is a NUTS Level III region of Ireland. The region is one of eight regions of Ireland for Eurostat statistics at NUTS 3 level. Its NUTS code is IE061.

This area formerly came under the remit of the Dublin Regional Authority. This Authority was dissolved in 2014.

As of the 2022 census, the population of Dublin was 1,458,154, an 8.4% increase since the 2016 Census. The county's population first surpassed 1 million in 1981, and is projected to reach 1.8 million by 2036.

Dublin is Ireland's most populous county, a position it has held since the 1926 Census, when it overtook County Antrim. As of 2022, County Dublin has over twice the population of County Antrim and two and a half times the population of County Cork. Approximately 21% of Ireland's population lives within County Dublin (28% if only the Republic of Ireland is counted). Additionally, Dublin has more people than the combined populations of Ireland's 16 smallest counties.

With an area of just , Dublin is by far the most densely populated county in Ireland. The population density of the county is 1,582 people per square kilometre – over 7 times higher than Ireland's second most densely populated county, County Down in Northern Ireland.

During the Celtic Tiger period, a large number of Dublin natives (Dubliners) moved to the rapidly expanding commuter towns in the adjoining counties. As of 2022, approximately 27.2% (345,446) of Dubliners were living outside of County Dublin. People born within Dublin account for 28% of the population of Meath, 32% of Kildare, and 37% of Wicklow. There are 922,744 Dublin natives living within the county, accounting for 63.3% of the population. People born in other Irish counties living within Dublin account for roughly 11% of the population.

Between 2016 and 2022, international migration produced a net increase of 88,300 people. Dublin has the highest proportion of international residents of any county in Ireland, with around 25% of the county's population being born outside of the Republic of Ireland.

As of the 2022 census, 5.6 percent of the county's population was reported as younger than 5 years old, 25.7 percent were between 5 and 25, 55.3 percent were between 25 and 65, and 13.4 percent of the population was older than 65. Of this latter group, 48,865 people (3.4 percent) were over the age of 80, more than doubling since 2016. Across all age groups, there were slightly more females (51.06 percent) than males (48.94 percent).

In 2021, there were 16,596 births within the county, and the average age of a first time mother was 31.9.

Over a quarter (25.2 percent) of County Dublin's population was born outside of the Republic of Ireland. In 2022, Dublin City had the highest percentage of non-nationals in the county (27.3 percent), and South Dublin had the lowest (20.9 percent). Historically, the immigrant population of Dublin was mainly from the United Kingdom and other European Union member states. However, results from the 2022 census revealed that immigrants from non-EU/UK countries were the largest source of foreign-born residents for the first time, accounting for 12.9 percent of the county's population. Those from other European Union member states accounted for 8.3 percent of Dublin's population, and those from the United Kingdom a further 4.1 percent.

Prior to the 2000s, the UK was consistently the largest single source of non-nationals living in Dublin. After declining in the previous two census periods, the number of UK-born residents living in Dublin increased by 5.8 percent between 2016 and 2022. There was a large difference between the number of people living in Dublin who were born in the UK (58,586) and those who held sole-UK citizenship in the 2022 census (22,936). This discrepancy can arise for a variety of factors, such as people born in Northern Ireland claiming Irish citizenship rather than UK citizenship, Irish people born in the UK who now live in Dublin, British people who have become natural citizens, and foreign residents of Dublin who were born in the UK but are not UK citizens. Depending on an individual's responses in the census, all of these examples could result in the country of birth being registered by the CSO as the United Kingdom, but nationality being registered as Irish or a third country.

Following its accession to the EU, the Polish quickly became the fastest growing immigrant community in Dublin. Just 188 Poles applied for Irish work permits in 1999. By 2006 this number had grown to 93,787. After the 2008 Irish economic downturn, as many as 3,000 Poles left Ireland each month. Despite this, Poles remain one of Dublin's largest foreign-born groups. In contrast to more recent arrivals, a large percentage of Dublin's Polish citizens (30.9 percent) also hold Irish citizenship. 

Outside of Europe, Indians and Brazilians are the predominant foreign-national groups. As of 2022, Indians were the fastest growing major immigrant group in Dublin, and they are now the county's second largest foreign-born group after the UK. Dublin's Indian community grew by 155.2 percent between 2016 and 2022. There were 29,582 Indian-born residents within Dublin as of 2022, up from 9,884 in the 2011 census. The influx of Indians is driven in part by multinational tech companies such as Microsoft, Google and Meta who have located their European headquarters within the county, in areas such as the Silicon Docks and Sandyford. In August 2020, the first dedicated Hindu temple in Ireland was built in Walkinstown.

The number of Brazilian citizens living in Dublin more than tripled between 2011 and 2022, from 4,641 to 16,441. This increase is mainly a result of Ireland's participation in the Brazilian government's "Ciência sem Fronteiras" programme, which sees thousands of Brazilian students come to study in Ireland each year, many of whom remain in the country afterwards.

Although not fully captured during the census period, Dublin also houses a significant number of Ukrainian refugees under the Temporary Protection Directive. As of October 2023, the number of Ukrainians living in emergency accommodation within the county is estimated to be around 14,000.

According to the Central Statistics Office, in 2022 the population of County Dublin self-identified as:

By ethnicity, in 2022 the population was 80.4% white. Those who identified as White Irish constituted 68.0% of the county's population, and Irish Travellers accounted for a further 0.4%. Caucasians who did not identify as ethnically Irish accounted for 12.0% of the population.

In terms of total numbers, Dublin has the largest non-white population in Ireland, with an estimated 158,653 residents, accounting for 11.1% of the county's population. Over two-fifths (42.2 percent) of Ireland's black residents live within the county. In terms of percentage of population, Fingal has the highest percentage of both black (3.6 percent) and non-white (12.4 percent) residents of any local authority in Ireland. Conversely, Dún Laoghaire–Rathdown in the south of the county has one of Ireland's lowest percentages of black residents, with only 0.77% of the population identifying as black in 2022. Additionally, 43.3% of Ireland's multiracial population lives within County Dublin. Those who did not state their ethnicity more than doubled between 2016 and 2022, from 4.1% to 8.5%. 

The largest religious denomination by both number of adherents and as a percentage of Dublin's population in 2022 was the Roman Catholic Church, at 57.4 percent. All other Christian denominations including Church of Ireland, Eastern Orthodox, Presbyterian and Methodist accounted for 8.1 percent of Dublin's population. Together, all denominations of Christianity accounted for 65.5 percent of the county's population. According to the 2022 census, Dún Laoghaire–Rathdown is the least religious local authority in Ireland, with 23.9 percent of the population declaring themselves non-religious, followed closely by Dublin city (22.6 percent). In the county as a whole, those unaffiliated with any religion represented 20.1 percent of the population, which is the largest percentage of non-religious people of any county in Ireland. A further 9.1 percent of the population did not state their religion, up from just 4.1 percent in 2016.

Of the non-Christian religions, Islam is the largest in terms of number of adherents, with Muslims accounting for 2.6% of the population. After Islam, the largest non-Christian religions in 2022 were Hinduism (1.4 percent) and Buddhism (0.27 percent). While relatively small in absolute terms, County Dublin contains over half of Ireland's Hindu (58.7 percent) residents, and just under half of its Eastern Orthodox (45.3 percent), Islamic (45.0 percent) and Buddhist (41.7 percent) residents.

Dublin and its hinterland has been a Christian diocese since 1028. For centuries, the Primacy of Ireland was disputed between Dublin, the social and political capital of Ireland, and Armagh, site of Saint Patrick's main church, which was founded in 445 AD. In 1353 the dispute was settled by Pope Innocent VI, who proclaimed that the Archbishop of Dublin was "Primate of Ireland", while the Archbishop of Armagh was titled "Primate of All Ireland". These two distinct titles were replicated in the Church of Ireland following the Reformation. Historically, County Dublin was the epicentre of Protestantism in Ireland outside of Ulster. Records from the 1891 census show that the county was 21.4 percent Protestant towards the end of the 19th century. By the 1911 census this had gradually declined to around 20% due to poor economic conditions, as Dublin Protestants moved to industrial Belfast. Following the War of Independence (1919–1921), Dublin's Protestant community went into a steady decline, falling to 8.5 percent of the population by 1936.

Between 2016 and 2022, the fastest growing religions in Dublin were Hinduism (148.9 percent), Eastern Orthodox (51.6 percent), and Islam (27.9 percent), while the most rapidly declining religions were Evangelicalism (-10.4 percent), Catholicism (−8.7 percent), Jehovah's Witnesses (−5.9 percent) and Buddhism (−5.4 percent).

The boundaries of Dublin City Council form the urban core of the city, often referred to as "Dublin city centre", an area of 117.8 square kilometres. This encompasses the central suburbs of the city, extending as far south as Terenure and Donnybrook; as far north as Ballymun and Donaghmede; and as far west as Ballyfermot. As of 2022, there were 592,713 people living within Dublin city centre. However, as the continuous built-up area extends beyond the city boundaries, the term "Dublin city and suburbs" is commonly employed when referring to the actual extent of Dublin.

Dublin city and suburbs is a CSO-designated urban area which includes the densely populated contiguous built-up area which surrounds Dublin city centre. It encompasses 317.5 km and contains approximately 87% of County Dublin's population (1,263,219 people) as of the 2022 census.

As the city proper does not extend beyond Dublin Airport, the towns of "North County Dublin" such as Swords, Lusk, Rush and Malahide are not considered part of the city, and are recorded by the CSO as separate settlements. Under Ireland's National Planning Framework, these towns are considered part of the Dublin Metropolitan Area (DMA). The DMA also includes towns outside of the county, such as Naas, Leixlip and Maynooth in County Kildare, and Bray and Greystones in County Wicklow, but does not include Balbriggan or Skerries, which are located in the far north of County Dublin.

The Greater Dublin Area (GDA) is a commonly used planning jurisdiction which extends to the wider network of commuter towns that are economically connected to Dublin city. The GDA consists of County Dublin and its three neighboring counties, Kildare, Meath and Wicklow.

With a population of 2.1 million and an area of 6,986 square kilometres, it contains 40% of the population of the State, and covers 9.9% of its land area.

Under CSO classification, an "Urban Area" is a town with a population greater than 1,500. Dublin is the most urbanised county in Ireland, with 98% of its residents residing in urban areas as of 2022. Of Dublin's three non-city local authorities, Fingal has the highest proportion of people living in rural areas (7.9%), while Dún Laoghaire–Rathdown has the lowest (1.19%). The western suburbs of Dublin city such as Tallaght and Blanchardstown have experienced rapid growth in recent decades, and both areas have a population roughly equivalent to Galway city.

County Dublin has the oldest and most extensive transportation infrastructure in Ireland. The Dublin and Kingstown Railway, opened in December 1834, was Ireland's first railway line. The line, which ran from Westland Row to Dún Laoghaire, was originally intended to be used for cargo. However, it proved far more popular with passengers and became the world's first commuter railway line. The line has been upgraded multiple times throughout its history and is still in use to this day, making it the oldest commuter railway route in the world.
Public transport in Dublin was managed by the Dublin Transportation Office until 2009, when it was replaced by the National Transport Authority (NTA). The three pillars currently underpinning the public transport network of the Greater Dublin Area (GDA) are Dublin Suburban Rail, the Luas and the bus system. There are six commuter lines in Dublin, which are managed by Iarnród Éireann. Five of these lines serve as routes between Dublin and towns across the GDA and beyond. The sixth route, known as Dublin Area Rapid Transit (DART), is electrified and serves only Dublin and northern Wicklow. The newest addition to Dublin's public transport network is a tram system called the Luas. The service began with two disconnected lines in 2004, with three extensions opened in 2009, 2010 and 2011 before a cross-city link between the lines and further extension opened in 2017.

Historically, Dublin had an extensive tram system which commenced in 1871 and at its peak had over of active line. It was operated by the Dublin United Transport Company (DUTC) and was very advanced for its day, with near-full electrification from 1901. From the 1920s onwards, the DUTC began to acquire private bus operators and gradually closed some of its lines. Further declines in passenger numbers were driven in part by a belief at the time that trams were outdated and archaic. All tram lines terminated in 1949, except for the tram to Howth, which ran until 1959.
Dublin Bus is the county's largest bus operator, carrying 138 million passengers in 2019. For much of the city, particularly west Dublin, the bus is the only public transport option available, and there are numerous smaller private bus companies in operation across County Dublin. National bus operator Bus Éireann provides long-distance routes to towns and villages located outside of Dublin city and its immediate hinterland.

In November 2005, the government announced a €34 billion initiative called Transport 21 which included a substantial expansion to Dublin's transport network. The project was cancelled in May 2011 in the aftermath of the 2008 recession. Consequently, by 2017 Hugh Creegan, deputy chief of the NTA, stated that there had been a ""chronic underinvestment in public transport for more than a decade"". By 2019, Dublin was reportedly the 17th most congested city in the world, and had the 5th highest average commute time in the European Union. The Luas and rail network regularly experience significant overcrowding and delays during peak hours, and in 2019 Iarnród Éireann was widely ridiculed for asking commuters to ""stagger morning journeys"" to alleviate the problem.

The M50 is a orbital motorway around Dublin city, and is the busiest motorway in the country. It serves as the centre of both Dublin and Ireland's motorway network, and most of the national primary roads to other cities begin at the M50 and radiate outwards. The current route was built in various sections over the course of 27 years, from 1983 to 2010. All major roads in Ireland are managed by Transport Infrastructure Ireland (TII), which is headquartered in Parkgate Street, Dublin 8. As of 2019, there were over 550,000 cars registered in County Dublin, accounting for 25.3% of all cars registered in the State. Due to the county's small area and high degree of urbanisation, there is a preference for "D" registered used cars throughout Ireland, as they are considered to have undergone less wear and tear.

For international travel, around 1.7 million passengers travel by ferry through Dublin Port each year. A Dún Laoghaire to Holyhead ferry was formerly operated by Stena Line, but the route was closed in 2015. Dublin Airport is Ireland's largest airport, and 32.9 million passengers passed through it in 2019, making it Europe's 12th-busiest airport.

The Dublin Region, which is conterminous with County Dublin, has the largest and most highly developed economy in Ireland, accounting for over two-fifths of national Gross Domestic Product (GDP). The Central Statistics Office estimates that the GDP of the Dublin Region in 2020 was €157.2 billion ($187 billion / £141 billion at 2020 exchange rates). In nominal terms, Dublin's economy is larger than roughly 140 sovereign states. The county's GDP per capita is €107,808 ($117,688 / £92,620), one of the highest regional GDPs per capita in the EU. As of 2019, Dublin also had the highest Human Development Index in Ireland at 0.965, placing it among the most developed places in the world in terms of life expectancy, education and per capita income.

In 2020, average disposable income per person in Dublin was €27,686, or 118% of the national average (€23,400), the highest of any county in Ireland. As Ireland's most populous county, Dublin has the highest total household income in the country, at an estimated €46.8 billion in 2017 – higher than the Border, Midlands, West and South-East regions combined. Dublin residents were the highest per capita tax contributors in the State, returning a total of €15.1 billion in taxes in 2017.

Many of Ireland's most prominent political, educational, cultural and media centres are concentrated south of the River Liffey in Dublin city. Further south, areas like Dún Laoghaire, Dalkey and Killiney have long been some of Dublin's most affluent areas, and Dún Laoghaire–Rathdown consistently has the highest average house prices in Ireland. This has resulted in a perceived socio-economic divide in Dublin, between the generally less affluent Northside and the wealthier Southside. In Dublin (both city and county), residents will commonly refer to themselves as a "Northsider" or a "Southsider", and the division is often caricatured in Irish comedy, media and literature, for example Ross O'Carroll-Kelly and Damo and Ivor. References to the divide have also become colloquialisms in their own right, such as "D4" (referring to the Dublin 4 postal district), which is a pejorative term for an upper middle class Irish person.

While the northside-southside divide remains prevalent in popular culture, economic indices such as the Pobal HP deprivation index have shown that the distinction does not reflect economic reality. Many of Dublin's most affluent areas (Clontarf, Raheny, Howth, Portmarnock, Malahide) are located in the north of the county, and many of its most deprived areas (Jobstown, Ballyogan, Ballybrack, Dolphin's Barn, Clondalkin) are located in the south of the county.

Utilising CSO data from the past three censuses, Pobal HP revealed that there was a much higher concentration of below average, disadvantaged and very disadvantaged areas in west Dublin. In 2012, Irish Times columnist Fintan O'Toole posited that the real economic divide in Dublin was not north–south, but east–west – between the older coastal areas of eastern Dublin and the newer sprawling suburbs of western Dublin – and that the perpetuation of the northside–southside "myth" was a convenient way to gloss over class division within the county. O'Toole argued that framing the city's wealth divide as a light-hearted north–south stereotype was easier than having to address the socio-economic impacts of deliberate government policy to remove working-class people from the city centre and settle them on the margins.

Dublin is both a European and Global financial hub, and around 200 of the world's leading financial services firms have operations within the county. In 2017 and 2018 respectively, Dublin was ranked 5th in Europe and 31st globally in the Global Financial Centres Index (GFCI). In the mid-1980s, parts of central Dublin had fallen into a state of dereliction and the Irish government pursued an urban regeneration programme. An 11-hectare special economic zone (SEZ) was set up in 1987, known as the International Financial Services Centre (IFSC). At the time of its establishment, the SEZ had the lowest corporate tax rate in the EU. The IFSC has since expanded into a 37.8-hectare site centred around the Dublin Docklands. As of 2020, over €1.8 trillion of funds are administered from Ireland.

There was renewed interest in Dublin's financial services sector in the wake of the UK's vote to withdraw from the European Union in 2016. Many firms, including Barclays and Bank of America, pre-emptively moved some of their operations from London to Dublin in anticipation of restricted EU market access. A survey conducted by Ernst & Young in 2021 found that Dublin was the most popular destination for firms in the UK considering relocating to the EU, ahead of Luxembourg and Frankfurt. It is estimated that Dublin's financial sector will grow by about 25% as a direct result of Brexit, and as many as 13,000 jobs could move from the UK to County Dublin in the years immediately after its withdrawal.

The economy of Dublin benefits from substantial amounts of both indigenous and foreign investment. In 2018, the Financial Times ranked Dublin the most attractive large city in the world for Foreign Direct Investment, and the city has been consistently ranked by Forbes as one of the world's most business-friendly. The economy is centered on financial services, the pharmaceuticals and biotechnology industries, information technology, logistics and storage, professional services, agriculture and tourism. IDA Ireland, the state agency responsible for attracting foreign direct investment, was founded in Dublin in 1949.

Dublin has four power plants, all of which are concentrated in the docklands area of Dublin city. Three are natural-gas plants operated by the ESB, and the Poolbeg Incinerator is operated by Covanta Energy. The four plants have a combined capacity of 1.039 GW, roughly 12.5% of the island of Ireland's generation capacity as of 2019. The disused Poolbeg chimneys are the tallest structures in the county, and were granted protection by Dublin city council in 2014.

As a result of Dublin city's location within a sheltered bay at the mouth of a navigable river, shipping has been a key industry in the county since medieval times. By the 18th-century, Dublin was a bustling maritime city and large-scale engineering projects were undertaken to enhance the port's capacity, such as the Great South Wall, which was the largest sea wall in the world at the time of its construction in 1715. Dublin Port was originally located along the Liffey, but gradually moved towards the coast over the centuries as vessel size increased. It is today the largest and busiest port in Ireland. It handles 50% of the Republic of Ireland's trade, and receives 60% of all vessel arrivals.

Dublin Port occupies an area of in one of the most expensive places in the country, with an estimated price per acre of around €10 million. Since the 2000s, there have been calls to relocate Dublin Port out of the city and free up its land for residential and commercial development. This was first proposed by the Progressive Democrats at the height of the Celtic Tiger in 2006, who valued the land at between €25 and €30 billion, although nothing became of this proposal. During the housing crisis of the late 2010s the idea again began to attract supporters, among them economist David McWilliams. Currently, there are no official plans to move the port elsewhere, and the Dublin Port Company strongly opposes relocation.

Dublin hosts the headquarters of some of Ireland's largest multinational corporations, including 14 of the 20 companies which make up the ISEQ 20 index – those with the highest trading volume and market capitalisation of all Irish Stock Exchange listed companies. These are: AIB, Applegreen, Bank of Ireland, Cairn Homes, Continental Group, CRH, Dalata Hotel Group, Flutter Entertainment, Greencoat Renewables, Hibernia REIT, IRES, Origin Enterprises, Ryanair and Smurfit Kappa.

County Dublin receives by far the most overseas tourists of any county in Ireland. This is primarily due to Dublin city's status as Ireland's largest city and its transportation hub. Dublin is also Ireland's most popular destination for domestic tourists. According to Fáilte Ireland, in 2017 Dublin received nearly 6 million overseas tourists, and just under 1.5 million domestic tourists. Most of Ireland's international flights transit through Dublin Airport, and the vast majority of passenger ferry arrivals dock at Dublin Port. In 2019, the port also facilitated 158 cruise ship arrivals. The tourism industry in the county is worth approximately €2.3 billion per year.

As of 2019, 4 of the top 10 fee-paying tourist attractions in Ireland are located within County Dublin, as well as 5 of the top 10 free attractions. The Guinness Storehouse at St. James's Gate is Ireland's most visited tourist attraction, receiving 1.7 million visitors in 2019, and over 20 million total visits since 2000. Additionally, Dublin also contains Ireland's 3rd (Dublin Zoo), 4th (Book of Kells) and 6th (St Patrick's Cathedral) most visited fee-paying attractions. The top free attractions in Dublin are the National Gallery of Ireland, the National Botanic Gardens, the National Museum of Ireland and the Irish Museum of Modern Art, all of which receive over half a million visitors per year.

Despite having the smallest farmed area of any county, Dublin is one of Ireland's major agricultural producers. Dublin is the largest producer of fruit and vegetables in Ireland, the third largest producer of oilseed rape and has the fifth largest fishing industry. Fingal alone produces 55% of Ireland's fresh produce, including soft fruits and berries, apples, lettuces, peppers, asparagus, potatoes, onions, and carrots. As of 2020, the Irish Farmers' Association estimates that the total value of Dublin's agricultural produce is €205 million. According to the CSO, fish landings in the county are worth a further €20 million.

Approximately 41% of the county's land area (38,576 ha) is farmed. Of this, is under tillage, the 9th highest in the country, and is dedicated to fruit & horticulture, the 4th highest. Rural County Dublin is considered a peri-urban region, where an urban environment transitions into a rural one. Due to the growth of Dublin city and its commuter towns in the north of the county, the region is considered to be under significant pressure from urban sprawl. Between 1991 and 2010, the amount of agricultural land within the county decreased by 22.9%. In 2015, the local authorities of Fingal, South Dublin and Dún Laoghaire–Rathdown developed a joint Dublin Rural Local Development Strategy aimed at enhancing the region's agricultural output, while also managing and minimising the impact of urbanisation on biodiversity and the identity and culture of rural Dublin.

The county has a small forestry industry that is based almost entirely in the upland areas of south County Dublin. According to the 2017 National Forestry Inventory, of the county was under forest, of which was private forestry. The majority of Dublin's forests are owned by the national forestry company, Coillte. In the absence of increased private planting, the county's commercial timber capacity is expected to decrease in the coming decades, as Coillte intends to convert much of their holdings in the Dublin Mountains into non-commercial mixed forests.

Dublin has 810 individual farms with an average size of , the largest average farm size of any county in Ireland. Roughly 9,400 people within the county are directly employed in either agriculture or the food and drink processing industry. Numerous Irish and multinational food and drink companies are either based in Dublin or have facilities within the county, including Mondelez, Coca-Cola, Mars, Diageo, Kellogg's, Danone, Ornua, Pernod Ricard and Glanbia. In 1954, Tayto Crisps were established in Coolock and developed into cultural phenomenon throughout much of the Republic of Ireland. Its operations and headquarters have since moved to neighbouring County Meath. Another popular crisp brand, Keogh's, are based in Oldtown, Fingal.

In Ireland, spending on education is controlled by the government and the allocation of funds is decided each year in the annual budget. Local authorities retain limited responsibilities such as funding for school meals, service supports costs and the upkeep of libraries.

There are hundreds of primary and secondary schools within County Dublin, most of which are English-language schools. Several international schools are based in Dublin, such as St Kilian's German School and Lycée Français d'Irlande, which teach in foreign languages. There is also a large minority of students attending gaelscoileanna (Irish-language primary schools). There are 34 gaelscoileanna and 10 gaelcholáistí (Irish-language secondary schools) in the county, with a total of 12,950 students as of 2018. In terms of college acceptance rates, gaelcholáistí are consistently the best performing schools in Dublin, and among the best performing in Ireland.

Although the government pays for a large majority of school costs, including teachers' salaries, the Roman Catholic Church is the largest owner of schools in Dublin, and preference is given to Catholic students over non-Catholic students in oversubscribed areas. This has resulted in a growing movement towards non-denominational and co-educational schools in the county.

The majority of private secondary schools in Dublin are still single sex, and continue to have religious patronages with either congregations of the Catholic Church (Spiritans, Sisters of Loreto, Jesuits) or Protestant denominations (Church of Ireland, Presbyterian). Newer private schools which cater for the Leaving Cert cycle such as the Institute of Education and Ashfield College are generally non-denominational and co-educational. In 2018, Nord Anglia International School Dublin opened in Leopardstown, becoming the most expensive private school in Ireland.

As of 2023–24, four of Dublin's third level institutions are listed in the Top 500 of either the Times Higher Education Rankings or the QS World Rankings, placing them amongst the top 5% of all third level institutions in the world. TCD (81), UCD (171) and DCU (436) are within the Top 500 of the QS rankings; and TCD (161), RCSI (201–250), UCD (201–250) and DCU (451-500) and are within the Top 500 of the Times rankings. Newly amalgamated TUD also placed within the world's Top 1,000 universities in the QS rankings, and within the Top 500 for Engineering and Electronics.

County Dublin has four public universities, as well as numerous other colleges, institutes of technology and institutes of further education. Several of Dublin's largest third level institutions and their associated abbreviations are listed below:

For elections to Dáil Éireann, the area of the county is currently divided into eleven constituencies: Dublin Bay North, Dublin Bay South, Dublin Central, Dublin Fingal, Dublin Mid-West, Dublin North-West, Dublin Rathdown, Dublin South-Central, Dublin South-West, Dublin West, and Dún Laoghaire. Together they return 45 deputies (TDs) to the Dáil.

The first Irish Parliament convened in the small village of Castledermot, County Kildare on 18 June 1264. Representatives from seven constituencies were present, one of which was the constituency of Dublin City. Dublin was historically represented in the Irish House of Commons through the constituencies of Dublin City and County Dublin. Three smaller constituencies had been created by the 17th century: Swords; which was created sometime between 1560 and 1585, with Walter Fitzsimons and Thomas Taylor being its first recorded MPs; Newcastle in the west of the county, created in 1613; and Dublin University, which was a university constituency covering Trinity College, also created in 1613. While proceedings of the Irish Parliament were well-documented, many of the records from this time were lost during the shelling of the Four Courts in July 1922.

Following the Acts of Union 1800, Dublin was represented in Westminster through three constituencies from 1801 to 1885: Dublin City, County Dublin and the Dublin University. A series of local government and electoral reforms in the late 19th century radically alerted the county's political map, and by 1918 there were twelve constituencies within County Dublin.

Throughout the twentieth century the representation in Dublin expanded as the population grew. In the Electoral Act 1923, the first division of constituencies arranged by Irish legislation, geographical constituencies in Dublin were 23 of the 147 TDs in geographical constituencies; this contrasts with 45 of 160 at the most recent division.

Twenty-three Dáil Éireann constituencies have been created and abolished within the county since independence, the most recent being the constituencies of Dublin South, Dublin North, Dublin North-Central, Dublin North-East and Dublin South-East, which were abolished in 2016.

Of the fifteen people to have held the office of Taoiseach since 1922, more than half were either born or raised within County Dublin: W. T. Cosgrave, John A. Costello, Seán Lemass, Liam Cosgrave, Charles Haughey (born in County Mayo but raised in Dublin), Garret FitzGerald, Bertie Ahern and Leo Varadkar (Cosgrave held the office of President of the Executive Council; by convention, Taoisigh are numbered to include this position). Conversely, just one of Ireland's nine presidents have hailed from the county, namely Seán T. O'Kelly, who served as president from 1945 to 1959.

The four local government areas in County Dublin form the 4-seat constituency of Dublin in European Parliament elections.

As the capital city, Dublin is the seat of the national parliament of Ireland, the Oireachtas. It is composed of the President of Ireland, Dáil Éireann as a house of representatives, and Seanad Éireann as an upper house. Both houses of the Oireachtas meet in Leinster House, a former ducal palace on Kildare Street. It has been the home of the Irish government since the creation of the Irish Free State. The First Dáil of the revolutionary Irish Republic met in the Round Room of the Mansion House, the present-day residence of the Lord Mayor of Dublin, in January 1919. The former Irish Parliament, which was abolished in 1801, was located at College Green; Parliament House now holds a branch of Bank of Ireland. Government Buildings, located on Merrion Street, houses the Department of the Taoiseach, the Council Chamber, the Department of Finance, and the Office of the Attorney General.

The president resides in Áras an Uachtaráin in Phoenix Park, a stately ranger's lodge built in 1757. The house was bought by the Crown in 1780 to be used as the summer residence of the Lord Lieutenant of Ireland, the British viceroy in the Kingdom of Ireland. Following independence, the lodge was earmarked as the potential home of the Governor-General, but this was highly controversial as it symbolised continued British rule over Ireland, so it was left empty for many years. President Douglas Hyde "temporarily" occupied the building in 1938, as Taoiseach Éamon de Valera intended to demolish it and build a more modest presidential bungalow on the site. Those plans were scrapped during The Emergency and the lodge became the president's permanent residence.

Much like Áras an Uachtaráin, many of the grand estate homes of the former aristocracy were re-purposed for State use in the 20th century. The Deerfield Residence, also in Phoenix Park, is the official residence of the United States Ambassador to Ireland, while Glencairn House in south Dublin is used as the British Ambassador's residence. Farmleigh House, one of the Guinness family residences, was acquired by the government in 1999 for use as the official Irish state guest house.

Many other prominent judicial and political organs are located within Dublin, including the Four Courts, which is the principal seat of the Supreme Court, the Court of Appeal, the High Court and the Dublin Circuit Court; and the Custom House, which houses the Department of Housing, Local Government and Heritage. Once the centuries-long seat of the British government's administration in Ireland, Dublin Castle is now only used for ceremonial purposes, such as policy launches, hosting of State visits, and the inauguration of the president.

Dublin is among the most socially liberal places in Ireland, and popular sentiment on issues such as LGBT rights, abortion and divorce has often foreran the rest of the island. Referendums held on these issues have consistently received much stronger support within Dublin, particularly the south of the county, than the majority of the country. While over 66% of voters nationally voted in favour of the Eighth Amendment in 1983, 58% of voters in Dún Laoghaire and 55% in Dublin South voted against it. In 2018, over 75.5% of voters in County Dublin voted to repeal the amendment, compared with 66.4% nationally.

In 1987, Dublin Senator David Norris took the Irish government to the European Court of Human Rights (see "Norris v. Ireland") over the criminalisation of homosexual acts. In 1988, the Court ruled that the law criminalising same sex activities was contrary to the European Convention on Human Rights, in particular Article 8 which protects the right to respect for private life. The law was held to infringe on the right of adults to engage in acts of their own choice. This led directly to the repeal of the law in 1993. Numerous LGBT events and venues are now located within the county. Dublin Pride is an annual pride parade held on the last Saturday of June and is Ireland's largest public LGBT event. In 2018, an estimated 60,000 people attended. During the 2015 vote to allow same-sex marriage, 71% of County Dublin voted in favour, compared with 62% nationally.

In general, the south-eastern coastal regions of the county such as Dún Laoghaire and Dublin Bay South are a stronghold for the liberal-conservative Fine Gael party. Since the late-2000s the Green Party has also developed a strong support base in these areas. The democratic socialist Sinn Féin party generally performs well in south-central and west Dublin, in areas like Tallaght and Crumlin. In recent elections Sinn Féin have increasingly taken votes in traditional Labour Party areas, whose support has been on the decline since 2016. As a result of the economic crisis, centre-right Fianna Fáil failed to gain a single seat in Dublin in the 2011 general election. This was a first for the long-time dominant party of Irish politics. The party regained a footing in 7 of the 11 Dublin constituencies in 2020, and were also the largest party in Dublin City, Fingal and South Dublin in the 2019 local elections.

Dublin is a dual county in Gaelic games, and it competes at a similar level in both hurling/camogie and Gaelic football. The Dublin county board is the governing body for Gaelic games within the county. The county's current GAA crest, adopted in 2004, represents Dublin's four constituent areas. The castle represents Dublin city, the raven represents Fingal, the Viking longboat represents Dún Laoghaire–Rathdown and the book of Saint Tamhlacht in the centre represents South Dublin.

In Gaelic football, the Dublin county team competes annually in Division 1 of the National Football League and the provincial Leinster Senior Football Championship. Dublin is the dominant force of Leinster football, with 62 Leinster Senior Championship wins. Nationally, the county is second only to Kerry for All-Ireland Senior Football Championship titles. The two counties are fierce rivals, and a meeting between them is considered the biggest game in Gaelic football. Dublin has won the All-Ireland on 31 occasions, including a record 6 in a row from 2015 to 2020.

In hurling, the Dublin hurling team currently compete in Division 1B of the National Hurling League and in the Leinster Senior Hurling Championship. Dublin is the second most successful hurling county in Leinster after Kilkenny, albeit a distant second, with 24 Leinster hurling titles. The county has seen less success in the All-Ireland Senior Hurling Championship, ranking joint-fifth alongside Wexford. Dublin has been in 21 All-Ireland hurling finals, winning just 6, the most recent of which was in 1938.

Within the county, Gaelic football and hurling clubs compete in the Dublin Senior Football Championship and the Dublin Senior Hurling Championship, which were both established in 1887. St Vincents based in Marino and Faughs based in Templeogue are by far the most successful clubs in Dublin their respective sports. Four Dublin football teams have won the All-Ireland Senior Club Football Championship; St Vincents, Kilmacud Crokes, UCD and Ballyboden St Enda's. Despite their historic dominance in Dublin, Faughs have never won an All-Ireland Senior Club Hurling Championship. Since the early 2010s, Dalkey's Cuala have been the county's main hurling force, and the club won back-to-back All-Ireland's in 2017 and 2018.

Association football (soccer) is one of the most popular sports within the county. While Gaelic games are the most watched sport in Dublin, association football is the most widely played, and there are over 200 amateur football clubs in County Dublin. Dalymount Park in Phibsborough is known as the "home of Irish football", as it is both the country's oldest stadium and the former home ground for the national team from 1904 until 1990. The Republic of Ireland national football team is currently based in the 52,000 seater Aviva Stadium, which was built on the site of the old Lansdowne Road stadium in 2010. Shortly after its completion, the Aviva Stadium hosted the 2011 UEFA Europa League Final. Five League of Ireland football clubs are based within County Dublin; Bohemians F.C., Shamrock Rovers, St Patrick's Athletic, University College Dublin and Shelbourne.

Shamrock Rovers, formerly of Milltown but now based in Tallaght, are the most successful club in the country, with 21 League of Ireland titles. They were also the first Irish side to reach the group stages of a European competition when they qualified for the 2011–12 UEFA Europa League group stage. The Dublin University Football Club, founded in 1854, are technically the world's oldest extant football club. However, the club currently only plays rugby union. Bohemians are Ireland's third oldest club currently playing football, after Belfast's Cliftonville F.C. and Athlone Town A.F.C. The Bohemians–Shamrock Rovers rivalry not only involves Dublin's two biggest clubs, but it is also a Northside-Southside rivalry, making it the most intense derby match in the county.

Rugby Union is the county's third most popular sport, after Gaelic games and football. Leinster Rugby play their competitive home games in the RDS Arena & the Aviva Stadium. Donnybrook Stadium hosts Leinster's friendlies and A games, as well as the Ireland A and Women's teams, Leinster Schools and Youths and the home club games of All Ireland League sides Old Wesley and Bective Rangers. County Dublin is home to 13 of the senior rugby union clubs in Ireland, including 5 of the 10 sides in the top division 1A.

Other popular sports in the county include: cricket, hockey, golf, tennis, athletics and equestrian activities. Dublin has two ODI cricket grounds in Castle Avenue and Malahide Cricket Club Ground, and the Phoenix Cricket Club, founded in 1830, is the oldest in Ireland. As with many other sporting organisations in the county, the Fitzwilliam Lawn Tennis Club is one of the world's oldest. It hosted the now-discontinued Irish Open from 1879 until 1983. Field hockey, particularly women's field hockey, is becoming increasingly popular within the county. The Ireland women's national field hockey team made it to the 2018 World Cup final, and many of the players on that team were from Dublin clubs, such as UCD, Old Alex, Loreto, Monkstown, Muckross and Railway Union.

The Dublin Horse Show takes place at the RDS, which hosted the Show Jumping World Championships in 1982, and the county has a horse racing track at Leopardstown which hosts the Irish Champion Stakes every September. Dublin houses the national stadium for both boxing (National Stadium) and basketball (National Basketball Arena), and the city hosted the 2003 Special Olympics. Although a small county in size, Dublin contains one third of Leinster's 168 golf courses, and three-time major winner Pádraig Harrington is from Rathfarnham.

Local radio stations include 98FM, FM104, Dublin City FM, Q102, SPIN 1038, Sunshine 106.8, Raidió Na Life and Radio Nova.

Local newspapers include "The Echo", and the "Liffey Champion".

Most of the area can receive the five main UK television channels as well as the main Irish channels, along with Sky TV and Virgin Media Ireland cable television.




Cosmological argument

A cosmological argument, in natural theology, is an argument which claims that the existence of God can be inferred from facts concerning causation, explanation, change, motion, contingency, dependency, or finitude with respect to the universe or some totality of objects. A cosmological argument can also sometimes be referred to as an argument from universal causation, an argument from first cause, the causal argument, or prime mover argument. Whichever term is employed, there are two basic variants of the argument, each with subtle yet important distinctions: "in esse" (essentiality), and "in fieri" (becoming).

The basic premises of all of these arguments involve the concept of causation. The conclusion of these arguments is that there exists a first cause, subsequently analysed to be God. The history of this argument goes back to Aristotle or earlier, was developed in Neoplatonism and early Christianity and later in medieval Islamic theology during the 9th to 12th centuries, and was re-introduced to medieval Christian theology in the 13th century by Thomas Aquinas. The cosmological argument is closely related to the principle of sufficient reason as addressed by Gottfried Leibniz and Samuel Clarke, itself a modern exposition of the claim that "nothing comes from nothing" attributed to Parmenides.

Contemporary defenders of cosmological arguments include William Lane Craig, Robert Koons, and Alexander Pruss.

Plato (c. 427–347 BC) and Aristotle (c. 384–322 BC) both posited first cause arguments, though each had certain notable caveats. In "The Laws" (Book X), Plato posited that all movement in the world and the Cosmos was "imparted motion". This required a "self-originated motion" to set it in motion and to maintain it. In "Timaeus", Plato posited a "demiurge" of supreme wisdom and intelligence as the creator of the Cosmos.

Aristotle argued "against" the idea of a first cause, often confused with the idea of a "prime mover" or "unmoved mover" ( or "primus motor") in his "Physics" and "Metaphysics". Aristotle argued in "favor" of the idea of several unmoved movers, one powering each celestial sphere, which he believed lived beyond the sphere of the fixed stars, and explained why motion in the universe (which he believed was eternal) had continued for an infinite period of time. Aristotle argued the atomist's assertion of a non-eternal universe would require a first uncaused cause – in his terminology, an efficient first cause – an idea he considered a nonsensical flaw in the reasoning of the atomists.

Like Plato, Aristotle believed in an eternal cosmos with no beginning and no end (which in turn follows Parmenides' famous statement that "nothing comes from nothing"). In what he called "first philosophy" or metaphysics, Aristotle "did" intend a theological correspondence between the prime mover and a deity; functionally, however, he provided an explanation for the apparent motion of the "fixed stars" (now understood as the daily rotation of the Earth). According to his theses, immaterial unmoved movers are eternal unchangeable beings that constantly think about thinking, but being immaterial, they are incapable of interacting with the cosmos and have no knowledge of what transpires therein. From an "aspiration or desire", the celestial spheres, "imitate" that purely intellectual activity as best they can, by uniform circular motion. The unmoved movers "inspiring" the planetary spheres are no different in kind from the prime mover, they merely suffer a dependency of relation to the prime mover. Correspondingly, the motions of the planets are subordinate to the motion inspired by the prime mover in the sphere of fixed stars. Aristotle's natural theology admitted no creation or capriciousness from the immortal pantheon, but maintained a defense against dangerous charges of impiety.

Plotinus, a third-century Platonist, taught that the One transcendent absolute caused the universe to exist simply as a consequence of its existence ("creatio ex deo"). His disciple Proclus stated "The One is God".

Centuries later, the Islamic philosopher Avicenna (c. 980–1037) inquired into the question of being, in which he distinguished between essence ("māhiyya") and existence ("wuǧūd"). He argued that the fact of existence could not be inferred from or accounted for by the essence of existing things, and that form and matter by themselves could not originate and interact with the movement of the Universe or the progressive actualization of existing things. Thus, he reasoned that existence must be due to an agent cause that necessitates, imparts, gives, or adds existence to an essence. To do so, the cause must coexist with its effect and be an existing thing.

Steven Duncan writes that it "was first formulated by a Greek-speaking Syriac Christian neo-Platonist, John Philoponus, who claims to find a contradiction between the Greek pagan insistence on the eternity of the world and the Aristotelian rejection of the existence of any actual infinite". Referring to the argument as the "'Kalam' cosmological argument", Duncan asserts that it "received its fullest articulation at the hands of [medieval] Muslim and Jewish exponents of "Kalam" ("the use of reason by believers to justify the basic metaphysical presuppositions of the faith").

Thomas Aquinas (c. 1225–1274) adapted and enhanced the argument he found in his reading of Aristotle, Avicenna (the Proof of the Truthful), and Maimonides to form one of the most influential versions of the cosmological argument. His conception of first cause was the idea that the Universe must be caused by something that is itself uncaused, which he claimed is that which we call God:

Importantly, Aquinas' Five Ways, given the second question of his "Summa Theologica", are not the entirety of Aquinas' demonstration that the Christian God exists. The Five Ways form only the beginning of Aquinas' Treatise on the Divine Nature.

In the scholastic era, Aquinas formulated the "argument from contingency", following Aristotle in claiming that there must be something to explain why the Universe exists. Since the Universe could, under different circumstances, conceivably "not" exist (contingency), its existence must have a cause – not merely another contingent thing, but something that exists by necessity (something that "must" exist in order for anything else to exist). In other words, even if the Universe has always existed, it still owes its existence to an uncaused cause, Aquinas further said: "... and this we understand to be God."

Aquinas's argument from contingency allows for the possibility of a Universe that has no beginning in time. It is a form of argument from universal causation. Aquinas observed that, in nature, there were things with contingent existences. Since it is possible for such things not to exist, there must be some time at which these things did not in fact exist. Thus, according to Aquinas, there must have been a time when nothing existed. If this is so, there would exist nothing that could bring anything into existence. Contingent beings, therefore, are insufficient to account for the existence of contingent beings: there must exist a "necessary" being whose non-existence is an impossibility, and from which the existence of all contingent beings is ultimately derived.

Aquinas' argument from contingency may also be formulated like this: if each contingently existing being considers himself Bn, then, because he exists contingently, he depends for his existence on a prior being Bn-1. Now, Bn-1 likewise, if it is contingent, depends on Bn-2. Nevertheless, this series cannot go on until Infinity. At a certain time, we will arrive at a B1, the First Being in existence, and since there is no "zeroth" Being or B0, B1 exists Necessarily, i.e. is not a contingent being. This was Aquinas' Third Way, under Question 2, Article 3 in the Summa Theologica

The German philosopher Gottfried Leibniz made a similar argument with his principle of sufficient reason in 1714. "There can be found no fact that is true or existent, or any true proposition," he wrote, "without there being a sufficient reason for its being so and not otherwise, although we cannot know these reasons in most cases." He formulated the cosmological argument succinctly: "Why is there something rather than nothing? The sufficient reason ... is found in a substance which ... is a necessary being bearing the reason for its existence within itself."

Leibniz's argument from contingency is one of the most popular cosmological arguments in philosophy of religion. It attempts to prove the existence of a necessary being and infer that this being is God. Alexander Pruss formulates the argument as follows:


Premise 1 is a form of the principle of sufficient reason stating that all contingently true sentences (i.e. contingent facts) have a sufficient explanation as to why they are the case. Premise 2 refers to what is known as the Big Conjunctive Contingent Fact (abbreviated BCCF), and the BCCF is generally taken to be the logical conjunction of all contingent facts. It can be thought about as the sum total of all contingent reality. Premise 3 then concludes that the BCCF has an explanation, as every contingency does (in virtue of the PSR). It follows that this explanation is non-contingent (i.e. necessary); no contingency can explain the BCCF, because every contingent fact is a "part" of the BCCF. Statement 5, which is either seen as a premise or a conclusion, infers that the necessary being which explains the totality of contingent facts is God. Several philosophers of religion, such as Joshua Rasmussen and T. Ryan Byerly, have argued for the inference from (4) to (5).

The difference between the arguments from causation "in fieri" and "in esse" is a fairly important one. "In fieri" is generally translated as "becoming", while "in esse" is generally translated as "in essence". "In fieri", the process of becoming, is similar to building a house. Once it is built, the builder walks away, and it stands on its own accord; compare the watchmaker analogy. (It may require occasional maintenance, but that is beyond the scope of the first cause argument.)

"In esse" (essence) is more akin to the light from a candle or the liquid in a vessel. George Hayward Joyce, SJ, explained that, "where the light of the candle is dependent on the candle's continued existence, not only does a candle produce light in a room in the first instance, but its continued presence is necessary if the illumination is to continue. If it is removed, the light ceases. Again, a liquid receives its shape from the vessel in which it is contained; but were the pressure of the containing sides withdrawn, it would not retain its form for an instant." This form of the argument is far more difficult to separate from a purely first cause argument than is the example of the house's maintenance above, because here the first cause is insufficient without the candle's or vessel's continued existence.

The philosopher Robert Koons has stated a new variant on the cosmological argument. He says that to deny causation is to deny all empirical ideas – for example, if we know our own hand, we know it because of the chain of causes including light being reflected upon one's eyes, stimulating the retina and sending a message through the optic nerve into your brain. He summarised the purpose of the argument as "that if you don't buy into theistic metaphysics, you're undermining empirical science. The two grew up together historically and are culturally and philosophically inter-dependent ... If you say I just don't buy this causality principle – that's going to be a big big problem for empirical science." This "in fieri" version of the argument therefore does not intend to prove God, but only to disprove objections involving science, and the idea that contemporary knowledge disproves the cosmological argument.

William Lane Craig, who was principally responsible for re-popularizing this argument in Western philosophy, presents it in the following general form:

Craig analyses this cause in "The Blackwell Companion to Natural Theology" and says that this cause must be uncaused, beginningless, changeless, timeless, spaceless, extraordinarily powerful, and personal.

Duns Scotus, the influential Medieval Christian theologian, created a metaphysical argument for the existence of God. Though it was inspired by Aquinas' argument from motion, he, like other philosophers and theologians, believed that his statement for God's existence could be considered separate to Aquinas'. His explanation for God's existence is long, and can be summarised as follows:

Scotus deals immediately with two objections he can see: first, that there cannot be a first, and second, that the argument falls apart when 1) is questioned. He states that infinite regress is impossible, because it provokes unanswerable questions, like, in modern English, "What is infinity minus infinity?" The second he states can be answered if the question is rephrased using modal logic, meaning that the first statement is instead "It is possible that something can be produced."

Depending on its formulation, the cosmological argument is an example of a "positive infinite regress argument". An "infinite regress" is an infinite series of entities governed by a recursive principle that determines how each entity in the series depends on or is produced by its predecessor. An "infinite regress argument" is an argument against a theory based on the fact that this theory leads to an infinite regress. A "positive infinite regress argument" employs the regress in question to argue in support of a theory by showing that its alternative involves a vicious regress. The regress relevant for the cosmological argument is the "regress of causes": an event occurred because it was caused by another event that occurred before it, which was itself caused by a previous event, and so on. For an infinite regress argument to be successful, it has to demonstrate not just that the theory in question entails an infinite regress but also that this regress is vicious. Once the viciousness of the regress of causes is established, the cosmological argument can proceed to its positive conclusion by holding that it is necessary to posit a first cause in order to avoid it.

A regress can be vicious due to "metaphysical impossibility", "implausibility" or "explanatory failure". It is sometimes held that the "regress of causes" is vicious because it is "metaphysically impossible", i.e. that it involves an outright contradiction. But it is difficult to see where this contradiction lies unless an additional assumption is accepted: that actual infinity is impossible. But this position is opposed to infinity in general, not just specifically to the "regress of causes". A more promising view is that the "regress of causes" is to be rejected because it is "implausible". Such an argument can be based on empirical observation, e.g. that, to the best of our knowledge, our universe had a beginning in the form of the Big Bang (albeit the possibility that it existed for eternity before the Big Bang is also not strictly excluded on physics grounds alone). But it can also be based on more abstract principles, like Ockham's razor (parsimony), which posits that we should avoid ontological extravagance by not multiplying entities without necessity. A third option is to see the "regress of causes" as vicious due to "explanatory failure", i.e. that it does not solve the problem it was formulated to solve or that it assumes already in disguised form what it was supposed to explain. According to this position, we seek to explain one event in the present by citing an earlier event that caused it. But this explanation is incomplete unless we can come to understand why this earlier event occurred, which is itself explained by its own cause and so on. At each step, the occurrence of an event has to be assumed. So it fails to explain why anything at all occurs, why there is a chain of causes to begin with.

One objection to the argument asks why first cause is unique in that it does not require any causes. Proponents argue that the first cause is exempt from having a cause, as this is part of what it is to be the first cause, while opponents argue that this is special pleading or otherwise untrue. Critics often press that arguing for the first cause's exemption raises the question of why the first cause is indeed exempt, whereas defenders maintain that this question has been answered by the various arguments, emphasizing that none of the major cosmological arguments rests on the premise that everything has a cause, and so the question does not address the actual premises of an argument and rests on a misunderstanding of them. 

William Lane Craig, who popularized and is notable for defending the Kalam cosmological argument, argues that the infinite is impossible, whichever perspective the viewer takes, and so there must always have been one unmoved thing to begin the universe. He uses Hilbert's paradox of the Grand Hotel and the question "What is infinity minus infinity?" to illustrate the idea that the infinite is metaphysically, mathematically, and even conceptually impossible. Other reasons include the fact that it is impossible to count down from infinity, and that, had the universe existed for an infinite amount of time, every possible event, including the final end of the universe, would already have occurred. He therefore states his argument in three points: firstly, everything that begins to exist has a cause of its existence; secondly, the universe began to exist; so, thirdly, therefore, the universe has a cause of its existence. Craig argues in the Blackwell Companion to Natural Theology that there cannot be an infinite regress of causes and thus there must be a first uncaused cause, even if one posits a plurality of causes of the universe. He argues Occam's razor may be employed to remove unneeded further causes of the universe to leave a single uncaused cause.

Secondly, it is argued that the premise of causality has been arrived at via "a posteriori" (inductive) reasoning, which is dependent on experience. David Hume highlighted this problem of induction and argued that causal relations were not true "a priori". However, as to whether inductive or deductive reasoning is more valuable remains a matter of debate, with the general conclusion being that neither is prominent. Opponents of the argument tend to argue that it is unwise to draw conclusions from an extrapolation of causality beyond experience. Andrew Loke replies that, according to the Kalam cosmological argument, only things which begin to exist require a cause. On the other hand, something that is without beginning has always existed and therefore does not require a cause. The Kalam and the Thomistic cosmological argument posit that there cannot be an actual infinite regress of causes, therefore there must be an uncaused first cause that is beginningless and does not require a cause.

According to this objection, the basic cosmological argument merely establishes that a first cause exists, not that it has the attributes of a theistic god, such as omniscience, omnipotence, and omnibenevolence. This is why the argument is often expanded to assert that at least some of these attributes are necessarily true, for instance in the modern Kalam argument given above. 

Defenders of the cosmological arguments also reply that theologians of note are aware of the need to additionally prove other attributes of the first cause beyond that one exists. One notable example of this is found in Aquinas' "Summa Theologiae" in which much of the first part ("Prima Pars") is devoted to establishing the attributes of this first cause, such as its uniqueness, perfection, and intelligence. Thus defenders of cosmological arguments would reply that while it is true that the cosmological argument only establishes a first cause, this is merely the first step which then allows for the demonstration of the other theistic attributes.

A causal loop is a form of predestination paradox arising where traveling backwards in time is deemed a possibility. A sufficiently powerful entity in such a world would have the capacity to travel backwards in time to a point before its own existence, and to then create itself, thereby initiating everything which follows from it.

The usual reason given to refute the possibility of a causal loop is that it requires that the loop as a whole be its own cause. Richard Hanley argues that causal loops are not logically, physically, or epistemically impossible: "[In timed systems,] the only possibly objectionable feature that all causal loops share is that coincidence is required to explain them." However, Andrew Loke argues that causal loop of the type that is supposed to avoid a first cause suffers from the problem of vicious circularity and thus it would not work.

David Hume and later Paul Edwards have invoked a similar principle in their criticisms of the cosmological argument. William L. Rowe has called this the Hume-Edwards principle:

Nevertheless, David White argues that the notion of an infinite causal regress providing a proper explanation is fallacious. Furthermore, in Hume's "Dialogues Concerning Natural Religion", the character Demea states that even if the succession of causes is infinite, the whole chain still requires a cause. To explain this, suppose there exists a causal chain of infinite contingent beings. If one asks the question, "Why are there any contingent beings at all?", it does not help to be told that "There are contingent beings because other contingent beings caused them." That answer would just presuppose additional contingent beings. An adequate explanation of why some contingent beings exist would invoke a different sort of being, a necessary being that is "not" contingent. A response might suppose each individual is contingent but the infinite chain as a whole is not, or the whole infinite causal chain is its own cause.

Severinsen argues that there is an "infinite" and complex causal structure. White tried to introduce an argument "without appeal to the principle of sufficient reason and without denying the possibility of an infinite causal regress". A number of other arguments have been offered to demonstrate that an actual infinite regress cannot exist, viz. the argument for the impossibility of concrete actual infinities, the argument for the impossibility of traversing an actual infinite, the argument from the lack of capacity to begin to exist, and various arguments from paradoxes.

Other defenders of cosmological arguments such as Ed Feser argue that the type of series in which causes are hierarchically dependent (essentially ordered or "per se" series) one on the other, cannot regress to infinity, even if it may be possible for causal series which are extended backward through time (accidentally ordered or "per accidens" series) to regress infinitely. The rationale for this is that in a hierarchical "per se" causal series, each member cannot so much as act without the concurrent actualization or causation of more fundamental members of the series; thus an infinite hierarchical series would mean that the entire series is composed of members none of which can act of itself, which is impossible. An example of such a series would be the composition of water, which depends on the simultaneous composition of hydrogen and oxygen atoms, which in turn depend on the simultaneous composition of protons, neutrons, and electrons, etc. into deeper levels of the hierarchy of physical reality. This is contrasted with an accidentally ordered or linear series - parents causing their children to begin to exist, who in turn cause their children to begin to exist - in which one member in the series may continue to act even if whatever caused it has ceased to exist, and so there is seemingly no issue if this type of series regresses infinitely; the impossibility of the infinite regress in an essentially ordered causal series would suffice for at least some varieties of cosmological arguments. Further discussion on this point can be found under essential and accidental causal chains.

Some cosmologists and physicists argue that a challenge to the cosmological argument is the nature of time: "One finds that time just disappears from the Wheeler–DeWitt equation" (Carlo Rovelli). The Big Bang theory states that it is the point in which all dimensions came into existence, the start of both space and time. Then, the question "What was there before the Universe?" makes no sense; the concept of "before" becomes meaningless when considering a situation without time. This has been put forward by J. Richard Gott III, James E. Gunn, David N. Schramm, and Beatrice Tinsley, who said that asking what occurred before the Big Bang is like asking what is north of the North Pole. However, some cosmologists and physicists do attempt to investigate causes for the Big Bang, using such scenarios as the collision of membranes.

Philosopher Edward Feser argues that most of the classical philosophers' cosmological arguments for the existence of God do not depend on the Big Bang or whether the universe had a beginning. The question is not about what got things started, or how long they have been going, but rather what keeps them going.

Clutch

A clutch is a mechanical device that allows the output shaft to be disconnected from the rotating input shaft. The clutch's input shaft is typically attached to a motor, while the clutch's output shaft is connected to the mechanism that does the work.

In a motor vehicle, the clutch acts as a mechanical linkage between the engine and transmission. By disengaging the clutch, the engine speed (RPM) is no longer determined by the speed of the driven wheels.

Another example of clutch usage is in electric drills. The clutch's input shaft is driven by a motor and the output shaft is connected to the drill bit (via several intermediate components). The clutch allows the drill bit to either spin at the same speed as the motor (clutch engaged), spin at a lower speed as the motor (clutch slipping) or remain stationary while the motor is spinning (clutch disengaged).

A "dry clutch" uses dry friction to transfer power from the input shaft to the output shaft, for example a "friction disk" pressing on a car engine's flywheel. The majority of clutches are dry clutches, especially in vehicles with manual transmissions. Slippage of a friction clutch (where the clutch is partially engaged but the shafts are rotating at different speeds) is sometimes required, such as when a motor vehicle accelerates from a standstill; however the slippage should be minimised to avoid increased wear rates.

In a "pull-type" clutch, pressing the pedal pulls the release bearing to disengage the clutch. In a "push-type" clutch, pressing the pedal pushes the release bearing to disengage the clutch.

A "multi-plate clutch" consists of several friction plates arranged concentrically. In some cases, it is used instead of a larger diameter clutch. Drag racing cars use multi-plate clutches to control the rate of power transfer to the wheels as the vehicle accelerates from a standing start.

Some clutch disks include springs designed to change the natural frequency of the clutch disc, in order to reduce NVH within the vehicle. Also, some clutches for manual transmission cars use a clutch delay valve to avoid abrupt engagements of the clutch.

In a "wet clutch", the friction material sits in an oil bath (or has flow-through oil) which cools and lubricates the clutch. This can provide smoother engagement and a longer lifespan of the clutch, however wet clutches can have a lower efficiency due to some energy being transferred to the oil. Since the surfaces of a wet clutch can be slippery (as with a motorcycle clutch bathed in engine oil), stacking multiple clutch discs can compensate for the lower coefficient of friction and so eliminate slippage under power when fully engaged.

Wet clutches often use a composite paper material.

A centrifugal clutch automatically engages as the speed of the input shaft increases and disengages as the input shaft speed decreases. Applications include small motorcycles, motor scooters, chainsaws, and some older automobiles.

A cone clutch is similar to dry friction plate clutch, except the friction material is applied to the outside of a conical shaped object.
A common application for cone clutches is the synchronizer ring in a manual transmission.

A dog clutch is a non-slip design of clutch which is used in non-synchronous transmissions.

The "single-revolution clutch" was developed in the 19th century to power machinery such as shears or presses where a single pull of the operating lever or (later) press of a button would trip the mechanism, engaging the clutch between the power source and the machine's crankshaft for exactly one revolution before disengaging the clutch. When the clutch is disengaged, the driven member is stationary. Early designs were typically dog clutches with a cam on the driven member used to disengage the dogs at the appropriate point.

Greatly simplified single-revolution clutches were developed in the 20th century, requiring much smaller operating forces and in some variations, allowing for a fixed fraction of a revolution per operation. Fast action friction clutches replaced dog clutches in some applications, eliminating the problem of impact loading on the dogs every time the clutch engaged.

In addition to their use in heavy manufacturing equipment, single-revolution clutches were applied to numerous small machines. In tabulating machines, for example, pressing the operate key would trip a single revolution clutch to process the most recently entered number. In typesetting machines, pressing any key selected a particular character and also engaged a single rotation clutch to cycle the mechanism to typeset that character. Similarly, in teleprinters, the receipt of each character tripped a single-revolution clutch to operate one cycle of the print mechanism.

In 1928, Frederick G. Creed developed a single-turn spring clutch (see above) that was particularly well suited to the repetitive start-stop action required in teleprinters. In 1942, two employees of Pitney Bowes Postage Meter Company developed an improved single turn spring clutch. In these clutches, a coil spring is wrapped around the driven shaft and held in an expanded configuration by the trip lever. When tripped, the spring rapidly contracts around the power shaft engaging the clutch. At the end of one revolution, if the trip lever has been reset, it catches the end of the spring (or a pawl attached to it), and the angular momentum of the driven member releases the tension on the spring. These clutches have long operating lives—many have performed tens and perhaps hundreds of millions of cycles without the need of maintenance other than occasional lubrication.

"Cascaded-pawl single-revolution clutches" superseded wrap-spring single-revolution clutches in page printers, such as teleprinters, including the Teletype Model 28 and its successors, using the same design principles. IBM Selectric typewriters also used them. These are typically disc-shaped assemblies mounted on the driven shaft. Inside the hollow disc-shaped drive drum are two or three freely floating pawls arranged so that when the clutch is tripped, the pawls spring outward much like the shoes in a drum brake. When engaged, the load torque on each pawl transfers to the others to keep them engaged. These clutches do not slip once locked up, and they engage very quickly, on the order of milliseconds. A trip projection extends out from the assembly. If the trip lever engaged this projection, the clutch was disengaged. When the trip lever releases this projection, internal springs and friction engage the clutch. The clutch then rotates one or more turns, stopping when the trip lever again engages the trip projection.


Most cars and trucks with a manual transmission use a dry clutch, which is operated by the driver using the left-most pedal. The motion of the pedal is transferred to the clutch using hydraulics (master and slave cylinders) or a cable. The clutch is only disengaged at times when the driver is pressing on the clutch pedal, therefore the default state is for the transmission to be connected to the engine. A "neutral" gear position is provided, so that the clutch pedal can be released with the vehicle remaining stationary.

The clutch is required for standing starts and is usually (but not always) used to assist in synchronising the speeds of the engine and transmission during gear changes, i.e. while reducing the engine speed (RPM) during upshifts and increasing the engine speed during downshifts.

The clutch is usually mounted directly to the face of the engine's flywheel, as this already provides a convenient large-diameter steel disk that can act as one driving plate of the clutch. Some racing clutches use small multi-plate disk packs that are not part of the flywheel. Both clutch and flywheel are enclosed in a conical bellhousing for the gearbox. The friction material used for the clutch disk varies, with a common material being an organic compound resin with a copper wire facing or a ceramic material.

In an automatic transmission, the role of the clutch is performed by a torque converter. However, the transmission itself often includes internal clutches, such as a "lock-up clutch" to prevent slippage of the torque converter, in order to reduce the energy loss through the transmission and therefore improve fuel economy.

Older belt-driven engine cooling fans often use a heat-activated clutch, in the form of a bimetallic strip. When the temperature is low, the spring winds and closes the valve, which lets the fan spin at about 20% to 30% of the crankshaft speed. As the temperature of the spring rises, it unwinds and opens the valve, allowing fluid past the valve, making the fan spin at about 60% to 90% of crankshaft speed.

A vehicle's air-conditioning compressor often uses magnetic clutches to engage the compressor as required.

Motorcycles typically employ a wet clutch with the clutch riding in the same oil as the transmission. These clutches are usually made up of a stack of alternating friction plates and steel plates. The friction plates have lugs on their outer diameters that lock them into a basket that is turned by the crankshaft. The steel plates have lugs on their inner diameters that lock them to the transmission input shaft. A set of coil springs or a diaphragm spring plate force the plates together when the clutch is engaged.

On motorcycles the clutch is operated by a hand lever on the left handlebar. No pressure on the lever means that the clutch plates are engaged (driving), while pulling the lever back towards the rider disengages the clutch plates through cable or hydraulic actuation, allowing the rider to shift gears or coast. Racing motorcycles often use slipper clutches to eliminate the effects of engine braking, which, being applied only to the rear wheel, can cause instability.

Cow tipping

Cow tipping is the purported activity of sneaking up on any unsuspecting or sleeping upright cow and pushing them over for entertainment. The practice of cow tipping is generally considered an urban legend and stories of such feats viewed as tall tales. The implication that rural citizens seek such entertainment due to lack of alternatives is viewed as a stereotype. The concept of cow tipping apparently developed in the 1970s, though tales of animals that cannot rise if they fall has historical antecedents dating to the Roman Empire.

Cows routinely lie down and can easily regain their footing unless sick or injured. Scientific studies have been conducted to determine if cow tipping is theoretically possible, with varying conclusions. All agree that cows are large animals that are difficult to surprise and will generally resist attempts to be tipped. Estimates suggest a force of between is needed, and that at least four and possibly as many as fourteen people would be required to achieve this. In real-life situations where cattle have to be laid on the ground, or "cast", such as for branding, hoof care or veterinary treatment, either rope restraints are required or specialized mechanical equipment is used that confines the cow and then tips it over. On rare occasions, cattle can lie down or fall down in proximity to a ditch or hill that restricts their normal ability to rise without help. Cow tipping has many references in popular culture and is also used as a figure of speech.

Some versions of the urban legend suggest that because cows sleep standing up, it is possible to approach them and push them over without the animals reacting. However, cows only sleep lightly while standing up, and they are easily awakened. They lie down to sleep deeply. Furthermore, numerous sources have questioned the practice's feasibility, since most cows weigh over and easily resist any lesser force.

A 2005 study led by Margo Lillie, a zoologist at the University of British Columbia, and her student Tracy Boechler, concluded that tipping a cow would require a force of nearly and is therefore impossible to accomplish by a single person. Her calculations found that it would require more than four people to apply enough force to push over a cow, based on an estimate that a single person could exert of force. However, since a cow can brace itself, Lillie and Boechler suggested that five or six people would, most likely, be needed. Further, cattle are well aware of their surroundings and are very difficult to surprise, due to excellent senses of both smell and hearing. Lillie and Boechler's analysis found that if a cow did not move, the principles of static physics suggest that two people might be able to tip a cow if its centre of mass were pushed over its hooves before the cow could react. However, cows are not rigid or unresponsive, and the faster humans have to move, the less force they can exert. Thus Lillie and Boechler concluded that it is unlikely that cows can actually be tipped over in this way. Lillie stated, "It just makes the physics of it all, in my opinion, impossible."

Although biologist Steven Vogel agrees that it would take a force of about 3,000 newtons to push over a standing cow, he thinks that the study by Lillie and Boechler overestimates the pushing ability of an individual human. Using data from Cotterell and Kamminga, who estimated that humans exert a pushing force of 280 newtons, Vogel suggests that someone applying force at the requisite height to topple a cow might generate a maximum push of no more than 300 newtons. By this calculation, at least 10 people would be needed to tip over a non-reacting cow. However, this combined force requirement, he says, might not be the greatest impediment to such a prank. Standing cows are not asleep and, like other animals, have ever-vigilant reflexes. "If the cow does no more than modestly widen its stance without an overall shift of its center of gravity", he says, "about 4,000 newtons or 14 pushers would be needed—quite a challenge to deploy without angering the cow."

The belief that certain animals cannot rise if pushed over has historical antecedents. Julius Caesar recorded a belief that a European elk had no knee joints and could not get up if it fell. Pliny said the same about the hind legs of an animal he called the achlis, which Pliny's 19th-century translators Bostock and Riley said was merely another name for the elk. They also noted that Pliny's belief about the jointless back legs of the achlis (elk) was false.

In 1255, Louis IX of France gave an elephant to Henry III of England for his menagerie in the Tower of London. A drawing by the historian Matthew Paris for his "Chronica Majora" can be seen in his bestiary at Parker Library of Corpus Christi College, Cambridge. An accompanying text cites elephant lore suggesting that elephants did not have knees and were unable to get up if they fell.

Journalist Jake Steelhammer believes the American urban myth of cow tipping originated in the 1970s. It "stampeded into the '80s", he says, "when movies like "Tommy Boy" and "Heathers" featured cow tipping expeditions." Stories about cow tipping tend to be second-hand, he says, told by someone who does not claim to have tipped a cow but who knows someone else who says they did.

Cattle may need to be deliberately thrown or tipped over for certain types of husbandry practices and medical treatment. When done for medical purposes, this is often called "casting", and when performed without mechanical assistance requires the attachment of of rope around the body and legs of the animal. After the rope is secured by non-slip bowline knots, it is pulled to the rear until the animal is off-balance. Once the cow is forced to lie down in sternal recumbency (on its chest), it can be rolled onto its side and its legs tied to prevent kicking.

A calf table or calf cradle, also called a "tipping table" or a "throw down", is a relatively modern invention designed to be used on calves that are being branded. A calf is run into a chute, confined, and then tipped by the equipment onto its side for easier branding and castration.

Hydraulic tilt tables for adult cattle have existed since the 1970s and are designed to lift and tip cattle onto their sides to enable veterinary care, particularly of the animals' genitalia, and for hoof maintenance. (Unlike horses, cows generally do not cooperate with a farrier when standing.) A Canadian veterinarian explained, "Using the table is much safer and easier than trying to get underneath to examine the animal", and noted that cows tipped over on a padded table usually stop struggling and become calm fairly quickly. One design, developed at the Western College of Veterinary Medicine in Saskatoon, Saskatchewan, included "cow comfort" as a unique aspect of care using this type of apparatus.

Cows may inadvertently tip themselves. Due to their bulk and relatively short legs, cattle cannot roll over. Those that lie down and roll to their sides with their feet pointing uphill may become stuck and unable to rise without assistance, with potentially fatal results. In such cases, two humans can roll or flip a cow onto its other side, so that its feet are aimed downhill, thus allowing it to rise on its own. In one documented case of "real-life cow tipping", a pregnant cow rolled into a gully in New Hampshire and became trapped in an inverted state until rescued by volunteer fire fighters. The owner of the cow commented that he had seen this happen "once or twice" before.

Trauma or illness may also result in a cow unable to rise to its feet. Such animals are sometimes called "downers." Sometimes this occurs as a result of muscle and nerve damage from calving or a disease such as mastitis. Leg injuries, muscle tears, or a massive infection of some sort may also be causes. Downer cows are encouraged to get to their feet and have a much greater chance of recovery if they do. If unable to rise, some have survived—with medical care—as long as 14 days and were ultimately able to get back on their feet. Appropriate medical treatment for a downer cow to prevent further injury includes rolling from one side to the other every three hours, careful and frequent feeding of small amounts of fodder, and access to clean water.

Dead animals may appear to have been tipped over, but this is actually the process of rigor mortis, which stiffens the muscles of the carcass, beginning six to eight hours after death and lasting for one to two days. It is particularly noticeable in the limbs, which stick out straight. Post-mortem bloat also occurs because of gas formation inside the body. The process may result in cattle carcasses that wind up on their back with all four feet in the air.

Assorted individuals have claimed to have performed cow tipping, often while under the influence of alcohol. These claims, to date, cannot be reliably verified, with Jake Swearingen of "Modern Farmer" noting in 2013 that YouTube, a popular source of videos of challenges and stunts, "fails to deliver one single actual cow-tipping video".

Pranksters have sometimes pushed over artificial cows. Along Chicago's Michigan Avenue in 1999, two "apparently drunk" men felled six fiberglass cows that were part of a Cows on Parade public art exhibit. Four other vandals removed a "Wow cow" sculpture from its lifeguard chair at Oak Street Beach and abandoned it in a pedestrian underpass. A year later, New York City anchored its CowParade art cows, including "A Streetcow Named Desire", to concrete bases "to prevent the udder disrespect of cow-tippers and thieves."

Cow tipping has been featured in films from the 1980s and later, such as "Heathers" (1988), "Tommy Boy" (1995), " Barnyard" (2006), and "I Love You, Beth Cooper" (2009). It was also used in the title of a 1992 documentary film by Randy Redroad, "Cow Tipping—The Militant Indian Waiter". 

Variants of cow tipping have also been seen in popular media such as the film "Cars" (2006), which features a vehicular variant called tractor-tipping, and the video game "", which allows the character to sneak up on and tip over a Brahmin, the game's two-headed cow-like animal. The board game "Battle Cattle" is based on the practice, with heavily armed cows having "Tipping Defense Numbers."

In The Little Willies song "Lou Reed" from their 2006 self-titled debut album, Norah Jones sings about a fictional event during which musician Lou Reed tips cows in Texas. In another medium, "The Big Bang Theory", a television show, uses cow tipping lore as an element to establish the nature of a rural character, Penny.

The term "cow tipping" is sometimes used as a figure of speech for pushing over something big. In "A Giant Cow-Tipping by Savages", author John Weir Close uses the term to describe contemporary mergers and acquisitions. "Tipping sacred cows" has been used as a deliberate mixed metaphor in titles of books on Christian ministry and business management.



Cassandra

Cassandra or Kassandra (; Ancient Greek: Κασσάνδρα, , also , and sometimes referred to as Alexandra) in Greek mythology was a Trojan priestess dedicated to the god Apollo and fated by him to utter true prophecies but never to be believed. In modern usage her name is employed as a rhetorical device to indicate a person whose accurate prophecies, generally of impending disaster, are not believed.

Cassandra was a daughter of King Priam and Queen Hecuba of Troy. Her elder brother was Hector, the hero of the Greek-Trojan war. The older and most common versions of the myth state that she was admired by the god Apollo, who sought to win her love by means of the gift of seeing the future. According to Aeschylus, she promised him her favours, but after receiving the gift, she went back on her word. As the enraged Apollo could not revoke a divine power, he added to it the curse that nobody would believe her prophecies. In other sources, such as Hyginus and Pseudo-Apollodorus, Cassandra broke no promise to Apollo but rather the power of foresight was given to her as an enticement to enter into a romantic engagement, the curse being added only when it failed to produce the result desired by the god.

Later versions on the contrary describe her falling asleep in a temple, where snakes licked (or whispered into) her ears which enabled her to hear the future.

Hjalmar Frisk ("Griechisches Etymologisches Wörterbuch", Heidelberg, 1960–1970) notes "unexplained etymology", citing "various hypotheses" found in Wilhelm Schulze, Edgar Howard Sturtevant, J. Davreux, and . R. S. P. Beekes cites García Ramón's derivation of the name from the Proto-Indo-European root *"(s)kend-" "raise". The Online Etymology Dictionary states "though the second element looks like a fem. form of Greek "andros" "of man, male human being." Watkins suggests PIE "*(s)kand-" "to shine" as source of second element. The name also has been connected to "kekasmai" "to surpass, excel."

Cassandra was described by the chronicler Malalas in his account of the "Chronography" as "shortish, round-faced, white, mannish figure, good nose, good eyes, dark pupils, blondish, curly, good neck, bulky breasts, small feet, calm, noble, priestly, an accurate prophet foreseeing everything, practicing hard, virgin". Meanwhile, in the account of Dares the Phrygian, she was illustrated as ". . .of moderate stature, round-mouthed, and auburn-haired. Her eyes flashed. She knew the future."

Cassandra was one of the many children born to the king and queen of Troy, Priam and Hecuba. She is the fraternal twin sister of Helenus, as well as the sister to Hector and Paris. One of the oldest and most common versions of her myth states that Cassandra was admired for her beauty and intelligence by the god Apollo, who sought to win her with the gift to see the future. According to Aeschylus, Cassandra promised Apollo favors, but, after receiving the gift, went back on her word and refused Apollo. Since the enraged Apollo could not revoke a divine power, he added a curse that nobody would believe Cassandra's prophecies.

Cassandra appears in texts written by Homer, Virgil, Aeschylus and Euripides. Each author depicts her prophetic powers differently.

In Homer's work, Cassandra is mentioned a total of four times "as a virgin daughter of Priam, as bewailing Hector's death, as chosen by Agamemnon as his slave mistress after the sack of Troy, and is killed by Clytemnestra over Agamemnon's corpse after Clytemnestra murders him on his return home."

In Virgil's work, Cassandra appears in book two of his epic poem titled "Aeneid," with her powers of prophecy restored. In Book 2 of the Aeneid, unlike Homer, Virgil presents Cassandra as having fallen into a mantic state and her prophecies reflect it. 
Likewise Seneca the Younger, in his play "Agamemnon", has her prophesize why Agamemnon deserves the death he got:"Quid me vocatis sospitem solam e meis, umbrae meorum? te sequor, tota pater Troia sepulte; frater, auxilium Phrygum terrorque Danaum, non ego antiquum decus video aut calentes ratibus ambustis manus, sed lacera membra et saucios vinclo gravi illos lacertos. te sequor… (Ag. 741–747)"

"Why do you call me, the lone survivor of my family, My shades? I follow you, father buried with all of Troy; Brother, bulwark of Trojans, terrorizer of Greeks, I do not see your beauty of old or hands warmed by burnt ships, But your lacerated limbs and those famous shoulders savaged By heavy chains. I follow you…"Later on in Seneca's work, this behavior is reflected in acts 4 and 5 as "Her mantic vision in act 4 will be supplemented by a further (in)sight into what is going on inside the palace in act 5 when she becomes a quasi-messenger and provides a meticulous account of Agamemnon's murder in the bath: 'I see and I am there and I enjoy it, no false vision deceives my eyes: let's watch' ("video et intersum et fruor, / imago visus dubia non fallit meos: / spectemus".)"

Cassandra was given the gift of prophecy, but was also cursed by the god Apollo so that her true prophecies would not be believed. Many versions of the myth relate that she incurred the god's wrath by refusing him sexual favours after promising herself to him in exchange for the power of prophecy. In Aeschylus' "Agamemnon", she bemoans her relationship with Apollo:
<poem>Apollo, Apollo!
God of all ways, but only Death's to me,
Once and again, O thou, Destroyer named,
Thou hast destroyed me, thou, my love of old!</poem>

And she acknowledges her fault:
<poem>I consented [marriage] to Loxias [Apollo] but broke my word. ... Ever since that fault I could persuade no one of anything.</poem>

Latin author Hyginus in Fabulae says:
Louise Bogan, an American poet, writes that another way Cassandra, as well as her twin brother Helenus, had earned their prophetic powers: ""she and her brother Helenus were left overnight in the temple of the Thymbraean Apollo. No reason has been advanced for this night in the temple; perhaps it was a ritual routinely performed by everyone. When their parents looked in on them the next morning, the children were entwined with serpents, which flicked their tongues into the children's ears. This enabled Cassandra and Helenus to divine the future."" It would not be until Cassandra is much older that Apollo appears in the same temple and tried to seduce Cassandra, who rejects his advances, and curses her by making her prophecies not be believed.

Her cursed gift from Apollo became an endless pain and frustration to her. She was seen as a liar and a madwoman by her family and by the Trojan people. Because of this, her father, Priam, had locked her away in a chamber and guarded her like the madwoman she was believed to be. Though Cassandra made many predictions that went unbelieved, the one prophecy that was believed was that of Paris being her abandoned brother.

Before the fall of Troy took place, Cassandra foresaw that if Paris goes to Sparta and brings Helen back as his wife, the arrival of Helen will spark the downfall and destruction of Troy during the Trojan War. Despite the prophecy and ignoring Cassandra's warning, Paris still went to Sparta and returned with Helen. While the people of Troy rejoiced, Cassandra, angry with Helen's arrival, furiously snatched away Helen's golden veil and tore at her hair.

In Virgil's epic poem, the Aeneid, Cassandra warned the Trojans about the Greeks hiding inside the Trojan Horse, Agamemnon's death, her own demise at the hands of Aegisthus and Clytemnestra, her mother Hecuba's fate, Odysseus's ten-year wanderings before returning to his home, and the murder of Aegisthus and Clytemnestra by the latter's children Electra and Orestes. Cassandra predicted that her cousin Aeneas would escape during the fall of Troy and found a new nation in Rome.

Coroebus and Othronus came to the aid of Troy during the Trojan War out of love for Cassandra and in exchange for her hand in marriage, but both were killed. According to one account, Priam offered Cassandra to Telephus's son Eurypylus, in order to induce Eurypylus to fight on the side of the Trojans. Cassandra was also the first to see the body of her brother Hector being brought back to the city.In "The Fall of Troy", told by Quintus Smyrnaeus, Cassandra attempted to warn the Trojan people that Greek warriors were hiding in the Trojan Horse while they were celebrating their victory over the Greeks with feasting. Disbelieving Cassandra, the Trojans resorted to calling her names and hurling insults at her. Attempting to prove herself right, Cassandra took an axe in one hand and a burning torch in the other, and ran towards the Trojan Horse, intent on destroying the Greeks herself, but the Trojans stopped her. The Greeks hiding inside the Horse were relieved, but alarmed by how clearly she had divined their plan.

At the fall of Troy, Cassandra sought shelter in the temple of Athena. There she embraced the wooden statue of Athena in supplication for her protection, but was abducted and brutally raped by Ajax the Lesser. Cassandra clung so tightly to the statue of the goddess that Ajax knocked it from its stand as he dragged her away. The actions of Ajax were a sacrilege because Cassandra was a supplicant at the sanctuary, and under the protection of the goddess Athena and Ajax further defiled the temple by raping Cassandra. In Apollodorus chapter 6, section 6, Ajax's death comes at the hands of both Athena and Poseidon "Athena threw a thunderbolt at the ship of Ajax; and when the ship went to pieces he made his way safe to a rock, and declared that he was saved in spite of the intention of Athena. But Poseidon smote the rock with his trident and split it, and Ajax fell into the sea and perished; and his body, being washed up, was buried by Thetis in Myconos".
In some versions, Cassandra intentionally left a chest behind in Troy, with a curse on whichever Greek opened it first. Inside the chest was an image of Dionysus, made by Hephaestus and presented to the Trojans by Zeus. It was given to the Greek leader Eurypylus as a part of his share of the victory spoils of Troy. When he opened the chest and saw the image of the god, he went mad.

Once Troy had fallen, Cassandra was taken as a "pallake" (concubine) by King Agamemnon of Mycenae. While he was away at war, Agamemnon's wife, Clytemnestra, had taken Aegisthus as her lover. Cassandra and Agamemnon were later killed by either Clytemnestra or Aegisthus. Various sources state that Cassandra and Agamemnon had twin boys, Teledamus and Pelops, who were murdered by Aegisthus.

The final resting place of Cassandra is either in Amyclae or Mycenae. Statues of Cassandra exist both in Amyclae and across the Peleponnese peninsula from Mycenae in Leuctra. In Mycenae, German business man and pioneer archeologist Heinrich Schliemann discovered in Grave Circle A the graves of Cassandra and Agamemnon and telegraphed back to King George of Greece:"With great joy I announce to Your Majesty that I have discovered the tombs which the tradition proclaimed by Pausanias indicates to be the graves of Agamemnon, Cassandra, Eurymedon and their companions, all slain at a banquet by Clytemnestra and her lover Aegisthos."However, it was later discovered that the graves predated the Trojan War by at least 300 years.

The play "Agamemnon" from Aeschylus's trilogy "Oresteia" depicts the king treading the scarlet cloth laid down for him, and walking offstage to his death. After the chorus's ode of foreboding, time is suspended in Cassandra's "mad scene". She has been onstage, silent and ignored. Her madness that is unleashed now is not the physical torment of other characters in Greek tragedy, such as in Euripides' "Heracles" or Sophocles' "Ajax".

According to author Seth Schein, two further familiar descriptions of her madness are that of Heracles in "The Women of Trachis" or Io in "Prometheus Bound". She speaks, disconnectedly and transcendent, in the grip of her psychic possession by Apollo, witnessing past and future events. Schein says, "She evokes the same awe, horror and pity as do schizophrenics". Cassandra is one of those "who often combine deep, true insight with utter helplessness, and who retreat into madness."

Eduard Fraenkel remarked on the powerful contrasts between declaimed and sung dialogue in this scene. The frightened and respectful chorus are unable to comprehend her. She goes to her inevitable offstage murder by Clytemnestra with full knowledge of what is to befall her.





Couplet

In poetry, a couplet is a pair of successive lines that rhyme and have the same metre. A couplet may be formal (closed) or run-on (open). In a formal (closed) couplet, each of the two lines is end-stopped, implying that there is a grammatical pause at the end of a line of verse. In a run-on (open) couplet, the meaning of the first line continues to the second.

The word "couplet" comes from the French word meaning "two pieces of iron riveted or hinged together". The term "couplet" was first used to describe successive lines of verse in Sir P. Sidney's "Arcadia "in 1590: "In singing some short coplets, whereto the one halfe beginning, the other halfe should answere."

While couplets traditionally rhyme, not all do. Poems may use white space to mark out couplets if they do not rhyme. Couplets in iambic pentameter are called "heroic couplets". John Dryden in the 17th century and Alexander Pope in the 18th century were both well known for their writing in heroic couplets. The Poetic epigram is also in the couplet form. Couplets can also appear as part of more complex rhyme schemes, such as sonnets.

Rhyming couplets are one of the simplest rhyme schemes in poetry. Because the rhyme comes so quickly, it tends to call attention to itself. Good rhyming couplets tend to "explode" as both the rhyme and the idea come to a quick close in two lines. Here are some examples of rhyming couplets where the sense as well as the sound "rhymes":

On the other hand, because rhyming couplets have such a predictable rhyme scheme, they can feel artificial and plodding. Here is a Pope parody of the predictable rhymes of his era:

Regular rhyme was not originally a feature of English poetry: Old English verse came in metrically paired units somewhat analogous to couplets, but constructed according to alliterative verse principles. The rhyming couplet entered English verse in the early Middle English period through the imitation of medieval Latin and Old French models. The earliest surviving examples are a metrical paraphrase of the Lord's Prayer in short-line couplets, and the "Poema Morale" in septenary (or "heptameter") couplets, both dating from the twelfth century.

Rhyming couplets were often used in Middle English and early modern English poetry. Chaucer's "Canterbury Tales", for instance, is predominantly written in rhyming couplets, and Chaucer also incorporated a concluding couplet into his rhyme royal stanza. Similarly, Shakespearean sonnets often employ rhyming couplets at the end to emphasize the theme. Take one of Shakespeare's most famous sonnets, Sonnet 18, for example (the rhyming couplet is shown in italics):

In the late seventeenth century and early eighteenth-century English rhyming couplets achieved the zenith of their prestige in English verse, in the popularity of heroic couplets. The heroic couplet was used by famous poets for ambitious translations of revered Classical texts, for instance, in John Dryden's translation of the "Aeneid" and in Alexander Pope's translation of the "Iliad".

Though poets still sometimes write in couplets, the form fell somewhat from favour in English in the twentieth century; contemporary poets writing in English sometimes prefer unrhymed couplets, distinguished by layout rather than by matching sounds.

Couplets called duilian may be seen on doorways in Chinese communities worldwide. Duilian displayed as part of the Chinese New Year festival, on the first morning of the New Year, are called chunlian (春联). These are usually purchased at a market a few days before and glued to the doorframe. The text of the couplets is often traditional and contains hopes for prosperity. Other chunlian reflect more recent concerns. For example, the CCTV New Year's Gala usually promotes couplets reflecting current political themes in mainland China.

Some duilian may consist of two lines of four characters each. Duilian are read from top to bottom where the first line starts from the right.

Tamil literature contains some of the notable examples of ancient couplet poetry. The Tamil language has a rich and refined grammar for couplet poetry, and distichs in Tamil poetry follow the venpa metre. One of the most notable examples of Tamil couplet poetry is the ancient Tamil moral text of the Tirukkural, which contains a total of 1330 couplets written in the kural venpa metre from which the title of the work was derived centuries later. Each Kural couplet is made of exactly 7 words—4 in the first line and 3 in the second. The first word may rhyme with the fourth or the fifth word. Below is an example of a couplet:

In Hindi, a couplet is called a "doha", while in Urdu, it is called a "sher".

Couplets were the most common form of poetry between the 12th and 18th Centuries, in Hidustani. Famous poets include Kabir, Tulsidas and Rahim Khan-i-Khanan.

Kabir (also known as Kabirdas) is thought to be one of the greatest composers of Hindustani couplets.

The American poet J. V. Cunningham was noted for many distichs included in the various forms of epigrams included in his poetry collections, as exampled here:

Deep summer, and time passes. Sorrow wastes<br>To a new sorrow. While Time heals time hastes

Charlotte Brontë

Charlotte Brontë (, ; 21 April 1816 – 31 March 1855) was an English novelist and poet, the eldest of the three Brontë sisters who survived into adulthood and whose novels became classics of English literature. She is best known for her novel "Jane Eyre", which she published under the gender neutral pen name Currer Bell. "Jane Eyre" went on to become a success in publication, and is widely held in high regard in the gothic fiction genre of literature.

She enlisted in school at Roe Head, Mirfield, in January 1831, aged 14 years. She left the year after to teach her sisters, Emily and Anne, at home, returning in 1835 as a governess. In 1839, she undertook the role of governess for the Sidgwick family, but left after a few months to return to Haworth, where the sisters opened a school but failed to attract pupils. Instead, they turned to writing and they each first published in 1846 under the pseudonyms of Currer, Ellis, and Acton Bell. Although her first novel, "The Professor", was rejected by publishers, her second novel, "Jane Eyre", was published in 1847. The sisters admitted to their Bell pseudonyms in 1848, and by the following year were celebrated in London literary circles.

Charlotte Brontë was the last to die of all her siblings. She became pregnant shortly after her wedding in June 1854 but died on 31 March 1855, almost certainly from hyperemesis gravidarum, a complication of pregnancy which causes excessive nausea and vomiting.

Charlotte Brontë was born on 21 April 1816 in Market Street, Thornton (in a house now known as the Brontë Birthplace), west of Bradford in the West Riding of Yorkshire, the third of the six children of Maria (née Branwell) and Patrick Brontë (formerly surnamed Brunty), an Irish Anglican clergyman. In 1820 her family moved a few miles to the village of Haworth, on the edge of the moors, where her father had been appointed perpetual curate of St Michael and All Angels Church. Maria died of cancer on 15 September 1821, leaving five daughters, Maria, Elizabeth, Charlotte, Emily and Anne, and a son, Branwell, to be taken care of by her sister, Elizabeth Branwell.

In August 1824, Patrick sent Charlotte, Emily, Maria, and Elizabeth to the Clergy Daughters' School at Cowan Bridge in Lancashire. Charlotte maintained that the school's poor conditions permanently affected her health and physical development, and hastened the deaths of Maria (born 1814) and Elizabeth (born 1815), who both died of tuberculosis in June 1825. After the deaths of his older daughters, Patrick removed Charlotte and Emily from the school. Charlotte used the school as the basis for Lowood School in "Jane Eyre," which is similarly affected by tuberculosis that is exacerbated by the poor conditions.

At home in Haworth Parsonage, Brontë acted as "the motherly friend and guardian of her younger sisters". Brontë wrote her first known poem at the age of 13 in 1829, and was to go on to write more than 200 poems in the course of her life. Many of her poems were "published" in their homemade magazine "Branwell's Blackwood's Magazine", and concerned the fictional world of "Glass Town". She and her surviving siblings – Branwell, Emily and Anne – created this shared world, and began chronicling the lives and struggles of the inhabitants of their imaginary kingdom in 1827. Charlotte, in private letters, called "Glass Town" "her 'world below', a private escape where she could act out her desires and multiple identities". Charlotte's "predilection for romantic settings, passionate relationships, and high society is at odds with Branwell's obsession with battles and politics and her young sisters' homely North Country realism, none the less at this stage there is still a sense of the writings as a family enterprise".

However, from 1831 onwards, Emily and Anne 'seceded' from the "Glass Town Confederacy" to create a 'spin-off' called "Gondal", which included many of their poems. After 1831, Charlotte and Branwell concentrated on an evolution of the "Glass Town Confederacy" called "Angria". Christine Alexander, a Brontë juvenilia historian, wrote "both Charlotte and Branwell ensured the consistency of their imaginary world. When Branwell exuberantly kills off important characters in his manuscripts, Charlotte comes to the rescue and, in effect, resurrects them for the next stories [...]; and when Branwell becomes bored with his inventions, such as the Glass Town magazine he edits, Charlotte takes over his initiative and keeps the publication going for several more years". The sagas the siblings created were episodic and elaborate, and they exist in incomplete manuscripts, some of which have been published as juvenilia. They provided them with an obsessive interest during childhood and early adolescence, which prepared them for literary vocations in adulthood.

Between 1831 and 1832, Brontë continued her education at a boarding school twenty miles away in Mirfield, Roe Head (now part of Hollybank Special School), where she met her lifelong friends and correspondents Ellen Nussey and Mary Taylor. In 1833 she wrote a novella, "The Green Dwarf", using the name Wellesley. Around about 1833, her stories shifted from tales of the supernatural to more realistic stories. She returned to Roe Head as a teacher from 1835 to 1838. Unhappy and lonely as a teacher at Roe Head, Brontë took out her sorrows in poetry, writing a series of melancholic poems. In "We wove a Web in Childhood" written in December 1835, Brontë drew a sharp contrast between her miserable life as a teacher and the vivid imaginary worlds she and her siblings had created. In another poem "Morning was its freshness still" written at the same time, Brontë wrote "Tis bitter sometimes to recall/Illusions once deemed fair". Many of her poems concerned the imaginary world of Angria, often concerning Byronic heroes, and in December 1836 she wrote to the Poet Laureate Robert Southey asking him for encouragement of her career as a poet. Southey replied, famously, that "Literature cannot be the business of a woman's life, and it ought not to be. The more she is engaged in her proper duties, the less leisure will she have for it even as an accomplishment and a recreation." This advice she respected but did not heed.

In 1839, she took up the first of many positions as governess to families in Yorkshire, a career she pursued until 1841. In particular, from May to July 1839 she was employed by the Sidgwick family at their summer residence, Stone Gappe, in Lothersdale, where one of her charges was John Benson Sidgwick (1835–1927), an unruly child who on one occasion threw the Bible at Charlotte, an incident that may have been the inspiration for a part of the opening chapter of "Jane Eyre" in which John Reed throws a book at the young Jane. Brontë did not enjoy her work as a governess, noting her employers treated her almost as a slave, constantly humiliating her.

Brontë was of slight build and was less than five feet tall.

In 1842 Charlotte and Emily travelled to Brussels to enrol at the boarding school run by Constantin Héger (1809–1896) and his wife Claire Zoé Parent Héger (1804–1887). During her time in Brussels, Brontë, who favoured the Protestant ideal of an individual in direct contact with God, objected to the stern Catholicism of Madame Héger, which she considered a tyrannical religion that enforced conformity and submission to the Pope. In return for board and tuition Charlotte taught English and Emily taught music. Their time at the school was cut short when their aunt Elizabeth Branwell, who had joined the family in Haworth to look after the children after their mother's death, died of internal obstruction in October 1842. Charlotte returned alone to Brussels in January 1843 to take up a teaching post at the school. Her second stay was not happy: she was homesick and deeply attached to Constantin Héger. She returned to Haworth in January 1844 and used the time spent in Brussels as the inspiration for some of the events in "The Professor" and "Villette".

After returning to Haworth, Charlotte and her sisters made headway with opening their own boarding school in the family home. It was advertised as "The Misses Brontë's Establishment for the Board and Education of a limited number of Young Ladies" and inquiries were made to prospective pupils and sources of funding. But none were attracted and in October 1844, the project was abandoned.

In May 1846 Charlotte, Emily, and Anne self-financed the publication of a joint collection of poems under their assumed names Currer, Ellis and Acton Bell. The pseudonyms veiled the sisters' sex while preserving their initials; thus Charlotte was Currer Bell. "Bell" was the middle name of Haworth's curate, Arthur Bell Nicholls whom Charlotte later married, and "Currer" was the surname of Frances Mary Richardson Currer who had funded their school (and maybe their father). Of the decision to use "noms de plume", Charlotte wrote:

Although only two copies of the collection of poems were sold, the sisters continued writing for publication and began their first novels, continuing to use their "noms de plume" when sending manuscripts to potential publishers.

Brontë's first manuscript, 'The Professor', did not secure a publisher, although she was heartened by an encouraging response from Smith, Elder & Co. of Cornhill, who expressed an interest in any longer works Currer Bell might wish to send. Brontë responded by finishing and sending a second manuscript in August 1847. Six weeks later, "Jane Eyre" was published. It tells the story of a plain governess, Jane, who, after difficulties in her early life, falls in love with her employer, Mr Rochester. They marry, but only after Rochester's insane first wife, of whom Jane initially has no knowledge, dies in a dramatic house fire. The book's style was innovative, combining Romanticism, naturalism with gothic melodrama, and broke new ground in being written from an intensely evoked first-person female perspective. Brontë believed art was most convincing when based on personal experience; in "Jane Eyre" she transformed the experience into a novel with universal appeal.

"Jane Eyre" had immediate commercial success and initially received favourable reviews. G. H. Lewes wrote that it was "an utterance from the depths of a struggling, suffering, much-enduring spirit", and declared that it consisted of ""suspiria de profundis"!" (sighs from the depths). Speculation about the identity and gender of the mysterious Currer Bell heightened with the publication of "Wuthering Heights" by Ellis Bell (Emily) and "Agnes Grey" by Acton Bell (Anne). Accompanying the speculation was a change in the critical reaction to Brontë's work, as accusations were made that the writing was "coarse", a judgement more readily made once it was suspected that Currer Bell was a woman. However, sales of "Jane Eyre" continued to be strong and may even have increased as a result of the novel developing a reputation as an "improper" book. A talented amateur artist, Brontë personally did the drawings for the second edition of "Jane Eyre" and in the summer of 1834 two of her paintings were shown at an exhibition by the Royal Northern Society for the Encouragement of the Fine Arts in Leeds.

In 1848 Brontë began work on the manuscript of her second novel, "Shirley". It was only partially completed when the Brontë family suffered the deaths of three of its members within eight months. In September 1848 Branwell died of chronic bronchitis and marasmus, exacerbated by heavy drinking, although Brontë believed that his death was due to tuberculosis. Branwell may have had a laudanum addiction. Emily became seriously ill shortly after his funeral and died of pulmonary tuberculosis in December 1848. Anne died of the same disease in May 1849. Brontë was unable to write at this time.

After Anne's death Brontë resumed writing as a way of dealing with her grief, and "Shirley", which deals with themes of industrial unrest and the role of women in society, was published in October 1849. Unlike "Jane Eyre", which is written in the first person, "Shirley" is written in the third person and lacks the emotional immediacy of her first novel, and reviewers found it less shocking. Brontë, as her late sister's heir, suppressed the republication of Anne's second novel, "The Tenant of Wildfell Hall", an action which had a deleterious effect on Anne's popularity as a novelist and has remained controversial among the sisters' biographers ever since.

In view of the success of her novels, particularly "Jane Eyre", Brontë was persuaded by her publisher to make occasional visits to London, where she revealed her true identity and began to move in more exalted social circles, becoming friends with Elizabeth Gaskell and Harriet Martineau whose sister Rachel had taught Gaskell's daughters. Brontë sent an early copy of "Shirley" to Martineau whose home at Ambleside she visited. The two friends shared an interest in racial relations and the abolitionist movement; recurrent themes in their writings. Brontë was also acquainted with William Makepeace Thackeray and G.H. Lewes. She never left Haworth for more than a few weeks at a time, as she did not want to leave her ageing father. Thackeray's daughter, writer Anne Isabella Thackeray Ritchie, recalled a visit to her father by Brontë:

Brontë's friendship with Elizabeth Gaskell, while not particularly close, was significant in that Gaskell wrote the first biography of Brontë after her death in 1855.

Brontë's third novel, the last published in her lifetime, was "Villette", which appeared in 1853. Its main themes include isolation, how such a condition can be borne, and the internal conflict brought about by social repression of individual desire. Its main character, Lucy Snowe, travels abroad to teach in a boarding school in the fictional town of Villette, where she encounters a culture and religion different from her own and falls in love with a man (Paul Emanuel) whom she cannot marry. Her experiences result in a breakdown but eventually, she achieves independence and fulfilment through running her own school. A substantial amount of the novel's dialogue is in the French language. "Villette" marked Brontë's return to writing from a first-person perspective (that of Lucy Snowe), the technique she had used in "Jane Eyre". Another similarity to "Jane Eyre" lies in the use of aspects of her own life as inspiration for fictional events, in particular her reworking of the time she spent at the "pensionnat" in Brussels. "Villette" was acknowledged by critics of the day as a potent and sophisticated piece of writing although it was criticised for "coarseness" and for not being suitably "feminine" in its portrayal of Lucy's desires.

Before the publication of "Villette", Brontë received an expected proposal of marriage from Irishman Arthur Bell Nicholls, her father's curate, who had long been in love with her. She initially refused him and her father objected to the union at least partly because of Nicholls's poor financial status. Elizabeth Gaskell, who believed that marriage provided "clear and defined duties" that were beneficial for a woman, encouraged Brontë to consider the positive aspects of such a union and tried to use her contacts to engineer an improvement in Nicholls's finances. According to James Pope-Hennessy in "The Flight of Youth," it was the generosity of Richard Monckton Milnes that made the marriage possible. Brontë, meanwhile, was increasingly attracted to Nicholls and by January 1854, she had accepted his proposal. They gained the approval of her father by April and married in June. Her father Patrick had intended to give Charlotte away, but at the last minute decided he could not, and Charlotte had to make her way to the church without him. The married couple took their honeymoon in Banagher, County Offaly, Ireland. By all accounts, her marriage was a success and Brontë found herself very happy in a way that was new to her.

Brontë became pregnant soon after her wedding, but her health declined rapidly and, according to Gaskell, she was attacked by "sensations of perpetual nausea and ever-recurring faintness". She died, with her unborn child, on 31 March 1855, three weeks before her 39th birthday. Her death certificate gives the cause of death as phthisis, but biographers including Claire Harman and others suggest that she died from dehydration and malnourishment due to vomiting caused by severe morning sickness or hyperemesis gravidarum. Brontë was buried in the family vault in the Church of St Michael and All Angels at Haworth.

"The Professor", the first novel Brontë had written, was published posthumously in 1857. The fragment of a new novel she had been writing in her last years has been twice completed by recent authors, the more famous version being "Emma Brown: A Novel from the Unfinished Manuscript by Charlotte Brontë" by Clare Boylan in 2003. Most of her writings about the imaginary country Angria have also been published since her death. In 2018, "The New York Times" published a belated obituary for her.

The daughter of an Irish Anglican clergyman, Brontë was herself an Anglican. In a letter to her publisher, she claims to "love the Church of England. Her Ministers indeed, I do not regard as infallible personages, I have seen too much of them for that – but to the Establishment, with all her faults – the profane Athanasian Creed excluded – I am sincerely attached."

In a letter to Ellen Nussey she wrote: 

Elizabeth Gaskell's biography "The Life of Charlotte Brontë" was published in 1857. It was an important step for a leading female novelist to write a biography of another, and Gaskell's approach was unusual in that, rather than analysing her subject's achievements, she concentrated on private details of Brontë's life, emphasising those aspects that countered the accusations of "coarseness" that had been levelled at her writing. The biography is frank in places, but omits details of Brontë's love for Héger, a married man, as being too much of an affront to contemporary morals and a likely source of distress to Brontë's father, widower, and friends. Mrs. Gaskell also provided doubtful and inaccurate information about Patrick Brontë, claiming that he did not allow his children to eat meat. This is refuted by one of Emily Brontë's diary papers, in which she describes preparing meat and potatoes for dinner at the parsonage. It has been argued that Gaskell's approach transferred the focus of attention away from the 'difficult' novels, not just Brontë's, but all the sisters', and began a process of sanctification of their private lives.

Brontë held lifelong correspondence with her former schoolmate Ellen Nussey. 350 of the some 500 letters sent by Brontë to Nussey survive, whereas all of Nussey's letters to Brontë were burned at Nicholls's request. The surviving letters provide most of the information known on Charlotte Brontë's life and are the backbone of her autobiographies.

Brontë's letters to Nussey seem to have romantic undertones:

Some scholars believe it is possible that Charlotte Brontë was in a romantic or sexual relationship with Ellen Nussey. Brontë would certainly have been aware of female same-sex attraction as she lived near Anne Lister.

On 29 July 1913 "The Times" of London printed four letters Brontë had written to Constantin Héger after leaving Brussels in 1844. Written in French except for one postscript in English, the letters broke the prevailing image of Brontë as an angelic martyr to Christian and female duties that had been constructed by many biographers, beginning with Gaskell. The letters, which formed part of a larger and somewhat one-sided correspondence in which Héger frequently appears not to have replied, reveal that she had been in love with a married man, although they are complex and have been interpreted in numerous ways, including as an example of literary self-dramatisation and an expression of gratitude from a former pupil.

In 1980 a commemorative plaque was unveiled at the Centre for Fine Arts, Brussels, on the site of the Madam Heger's school, in honour of Charlotte and Emily.


"The Green Dwarf, A Tale of the Perfect Tense" was written in 1833 under the pseudonym Lord Charles Albert Florian Wellesley. It shows the influence of Walter Scott, and Brontë's modifications to her earlier gothic style have led Christine Alexander to comment that, in the work, "it is clear that Brontë was becoming tired of the gothic mode "per se"".

"At the end of 1839, Brontë said goodbye to her fantasy world in a manuscript called Farewell to Angria. More and more, she was finding that she preferred to escape to her imagined worlds over remaining in reality – and she feared that she was going mad. So she said goodbye to her characters, scenes and subjects. [...] She wrote of the pain she felt at wrenching herself from her 'friends' and venturing into lands unknown".









Charles Williams (British writer)

Charles Walter Stansby Williams (20 September 1886 – 15 May 1945) was an English poet, novelist, playwright, theologian and literary critic. Most of his life was spent in London, where he was born, but in 1939 he moved to Oxford with the university press for which he worked and was buried there following his early death.

Charles Williams was born in London in 1886, the only son of (Richard) Walter Stansby Williams (1848–1929) and Mary (née Wall). His father Walter was a journalist and foreign business correspondent for an importing firm, writing in French and German, who was a 'regular and valued' contributor of verse, stories and articles to many popular magazines. His mother Mary, the sister of the ecclesiologist and historian J. Charles Wall, was a former milliner (hatmaker), of Islington. He had one sister, Edith, born in 1889. The Williams family lived in 'shabby-genteel' circumstances, owing to Walter's increasing blindness and the decline of the firm by which he was employed, in Holloway. In 1894 the family moved to St Albans in Hertfordshire, where Williams lived until his marriage in 1917.

Educated at St Albans School, Williams was awarded a scholarship to University College London, but he left in 1904 without attempting to gain a degree due to an inability to pay tuition fees.

Williams began work in 1904 in a Methodist bookroom. He was employed by the Oxford University Press (OUP) as a proofreading assistant in 1908 and quickly climbed to the position of editor. He continued to work at the OUP in various positions of increasing responsibility until his death in 1945. One of his greatest editorial achievements was the publication of the first major English-language edition of the works of Søren Kierkegaard. His work was part of the literature event in the art competition at the 1924 Summer Olympics.

Although chiefly remembered as a novelist, Williams also published poetry, works of literary criticism, theology, drama, history, biography, and a voluminous number of book reviews. Some of his best known novels are "War in Heaven" (1930), "Descent into Hell" (1937), and "All Hallows' Eve" (1945). T. S. Eliot, who wrote an introduction for the last of these, described Williams's novels as "supernatural thrillers" because they explore the sacramental intersection of the physical with the spiritual while also examining the ways in which power, even spiritual power, can corrupt as well as sanctify.

All of Williams's fantasies, unlike those of J. R. R. Tolkien and most of those of C. S. Lewis, are set in the contemporary world. Williams has been described by Colin Manlove as one of the three main writers of "Christian fantasy" in the twentieth century (the other two being C.S. Lewis and T. F. Powys). More recent writers of fantasy novels with contemporary settings, notably Tim Powers, cite Williams as a model and inspiration.

W. H. Auden, one of Williams's greatest admirers, reportedly re-read Williams's extraordinary and highly unconventional history of the church, "The Descent of the Dove" (1939), every year. Williams's study of Dante entitled "The Figure of Beatrice" (1944) was very highly regarded at its time of publication and continues to be consulted by Dante scholars today. His work inspired Dorothy L. Sayers to undertake her translation of "The Divine Comedy". Williams, however, regarded his most important work to be his extremely dense and complex Arthurian poetry, of which two books were published, "Taliessin through Logres" (1938) and "The Region of the Summer Stars" (1944), and more remained unfinished at his death. Some of Williams's essays were collected and published posthumously in "Image of the City and Other Essays" (1958), edited by Anne Ridler.

Williams gathered many followers and disciples during his lifetime. He was, for a period, a member of the Salvator Mundi Temple of the Fellowship of the Rosy Cross. He met fellow Anglican Evelyn Underhill in 1937 and would later write the introduction to her published "Letters" in 1943.

When World War II broke out in 1939, Oxford University Press moved its offices from London to Oxford. Williams was reluctant to leave his beloved city, and his wife Florence refused to go. From the nearly 700 letters he wrote to his wife during the war years, a generous selection has been published — "primarily… love letters," the editor calls them.

But the move to Oxford did allow him to participate regularly in Lewis's literary society known as the Inklings. In this setting Williams was able to read (and improve) his final published novel, "All Hallows' Eve", as well as to hear J. R. R. Tolkien read aloud to the group some of his early drafts of "The Lord of the Rings". In addition to meeting in Lewis's rooms at Oxford, they also regularly met at The Eagle and Child pub in Oxford (better known by its nickname "The Bird and Baby"). During this time Williams also gave lectures at Oxford on John Milton, William Wordsworth, and other authors, and received an honorary M.A. degree.

Williams is buried in Holywell Cemetery in Oxford. His headstone bears the word "poet" followed by the words "Under the Mercy", a phrase often used by Williams himself.

In 1917 Williams married his first sweetheart, Florence Conway, following a long courtship during which he presented her with a sonnet sequence that would later become his first published book of poetry, "The Silver Stair". Their son Michael was born in 1922.

Williams was an unswerving and devoted member of the Church of England, reputedly with a tolerance of the scepticism of others and a firm belief in the necessity of a "doubting Thomas" in any apostolic body.

Although Williams attracted the attention and admiration of some of the most notable writers of his day, including T. S. Eliot and W. H. Auden, his greatest admirer was probably C. S. Lewis, whose novel "That Hideous Strength" (1945) has been regarded as partially inspired by his acquaintance with both the man and his novels and poems. Williams came to know Lewis after reading Lewis's then-recently published study "The Allegory of Love"; he was so impressed he jotted down a letter of congratulation and dropped it in the mail. Coincidentally, Lewis had just finished reading Williams's novel "The Place of the Lion" and had written a similar note of congratulation. The letters crossed in the mail and led to an enduring and fruitful friendship.

Williams developed the concept of co-inherence and gave rare consideration to the theology of romantic love. Falling in love for Williams was a form of mystical envisioning in which one saw the beloved as he or she was seen through the eyes of God. Co-inherence was a term used in Patristic theology to describe the relationship between the human and divine natures of Jesus Christ and the relationship between the persons of the blessed Trinity. Williams extended the term to include the ideal relationship between the individual parts of God's creation, including human beings. It is our mutual indwelling: Christ in us and we in Christ, interdependent. It is also the web of interrelationships, social and economic and ecological, by which the social fabric and the natural world function. But especially for Williams, co-inherence is a way of talking about the Body of Christ and the communion of saints. For Williams, salvation was not a solitary affair: "The thread of the love of God was strong enough to save you and all the others, but not strong enough to save you alone." He proposed an order, the Companions of the Co-inherence, who would practice substitution and exchange, living in love-in-God, truly bearing one another's burdens, being willing to sacrifice and to forgive, living from and for one another in Christ. According to Gunnar Urang, co-inherence is the focus of all Williams's novels.










Celery

Celery (Apium graveolens) is a marshland plant in the family Apiaceae that has been cultivated as a vegetable since ancient times. Celery has a long fibrous stalk tapering into leaves. Depending on location and cultivar, either its stalks, leaves or hypocotyl are eaten and used in cooking. Celery seed powder is used as a spice.

Celery leaves are pinnate to bipinnate with rhombic leaflets long and broad. The flowers are creamy-white, in diameter, and are produced in dense compound umbels. The seeds are broad ovoid to globose, long and wide. Modern cultivars have been selected for either solid petioles, leaf stalks, or a large hypocotyl. A celery stalk readily separates into "strings" which are bundles of angular collenchyma cells exterior to the vascular bundles.

Wild celery, "Apium graveolens" var. "graveolens", grows to tall.
Celery is a biennial plant that occurs around the globe. It produces flowers and seeds only during its second year. The first cultivation is thought to have happened in the Mediterranean region, where the natural habitats were salty and wet, or marshy soils near the coast where celery grew in agropyro-rumicion-plant communities.

North of the Alps, wild celery is found only in the foothill zone on soils with some salt content. It prefers moist or wet, nutrient rich, muddy soils. It cannot be found in Austria and is increasingly rare in Germany.
First attested and printed in English as "sellery" by John Evelyn in 1664, the modern English word "celery" derives from the French "céleri", in turn from Italian "seleri", the plural of "selero", which comes from Late Latin "selinon", the latinisation of the , "celery". The earliest attested form of the word is the Mycenaean Greek "se-ri-no", written in Linear B syllabic script.

Celery was described by Carl Linnaeus in Volume One of his "Species Plantarum" in 1753.

The plants are raised from seed, sown either in a hot bed or in the open garden according to the season of the year, and, after one or two thinnings and transplantings, they are, on attaining a height of , planted out in deep trenches for convenience of blanching, which is effected by earthing up to exclude light from the stems. Development of self-blanching varieties of celery, which do not need to be earthed up, dominate both the commercial and amateur market.

Celery was first grown as a winter and early spring vegetable. It was considered a cleansing tonic to counter the deficiencies of a winter diet based on salted meats without fresh vegetables. By the 19th century, the season for celery in England had been extended, to last from the beginning of September to late in April.

In North America, commercial production of celery is dominated by the cultivar called 'Pascal' celery. Gardeners can grow a range of cultivars, many of which differ from the wild species, mainly in having stouter leaf stems. They are ranged under two classes, white and red. The stalks grow in tight, straight, parallel bunches, and are typically marketed fresh that way. They are sold without roots and only a small amount of green leaf remaining.

The stalks can be eaten raw, or as an ingredient in salads, or as a flavoring in soups, stews, and pot roasts.

In Europe, the variety called celeriac (also known as "celery root"), "Apium graveolens" var. "rapaceum", is also popular. It is grown because its hypocotyl forms a large bulb, white on the inside, which can be kept for months in winter and mostly serves as a key ingredient in soup. It can also be shredded and used in salads. The leaves are used as seasoning; the small, fibrous stalks find only marginal use.

Leaf celery (Chinese celery, "Apium graveolens var. secalinum") is a cultivar from East Asia that grows in marshlands. Leaf celery has characteristically thin skin stalks and a stronger taste and smell compared to other cultivars. It is used as a flavoring in soups and sometimes pickled as a side dish.

The wild form of celery is known as "smallage". It has a furrowed stalk with wedge-shaped leaves, the whole plant having a coarse, earthy taste, and a distinctive smell. The stalks are not usually eaten (except in soups or stews in French cuisine), but the leaves may be used in salads, and its seeds are those sold as a spice. With cultivation and blanching, the stalks lose their acidic qualities and assume the mild, sweetish, aromatic taste particular to celery as a salad plant.

Because wild celery is rarely eaten, yet susceptible to the same diseases as more well-used cultivars, it is often removed from fields to help prevent transmission of viruses like celery mosaic virus.

Harvesting occurs when the average size of celery in a field is marketable; due to extremely uniform crop growth, fields are harvested only once. The petioles and leaves are removed and harvested; celery is packed by size and quality (determined by color, shape, straightness and thickness of petiole, stalk and midrib length and absence of disease, cracks, splits, insect damage and rot). During commercial harvesting, celery is packaged into cartons which contain between 36 and 48 stalks and weigh up to . Under optimal conditions, celery can be stored for up to seven weeks from . Inner stalks may continue growing if kept at temperatures above . Shelf life can be extended by packaging celery in anti-fogging, micro-perforated shrink wrap. Freshly cut petioles of celery are prone to decay, which can be prevented or reduced through the use of sharp blades during processing, gentle handling, and proper sanitation.

Celery stalk may be preserved through pickling by first removing the leaves, then boiling the stalks in water before finally adding vinegar, salt, and vegetable oil.

In the past, restaurants used to store celery in a container of water with powdered vegetable preservative, but it was found that the sulfites in the preservative caused allergic reactions in some people. In 1986, the U.S. Food and Drug Administration banned the use of sulfites on fruits and vegetables intended to be eaten raw.

Celery is eaten around the world as a vegetable. In North America and Europe the crisp petiole (leaf stalk) is used. In Europe the hypocotyl is also used as a root vegetable. The leaves are strongly flavored and are used less often, either as a flavoring in soups and stews or as a dried herb. Celery, onions, and bell peppers are the "holy trinity" of Louisiana Creole and Cajun cuisine. Celery, onions, and carrots make up the French mirepoix, often used as a base for sauces and soups. Celery is a staple in many soups. It is used in the Iranian stew "khoresh karafs".

Celery leaves are frequently used in cooking to add a mild spicy flavor to foods, similar to, but milder than black pepper. Celery leaves are suitable dried and sprinkled on baked, fried or roasted fish or meats, or as part of a blend of fresh seasonings suitable for use in soups and stews. They may also be eaten raw, mixed into a salad or as a garnish.

In temperate countries, celery is also grown for its seeds. Actually very small fruit, these "seeds" yield a valuable essential oil that is used in the perfume industry. The oil contains the chemical compound apiole. Celery seeds can be used as flavoring or spice, either as whole seeds or ground.

Celery seeds can be ground and mixed with salt to produce celery salt. Celery salt can be made from an extract of the roots or by using dried leaves. Celery salt is used as a seasoning, in cocktails (commonly to enhance the flavor of Bloody Mary cocktails), on the Chicago-style hot dog, and in Old Bay Seasoning. Similarly, combinations of celery powder and salt are used to flavor and preserve cured pork and other processed meats as an alternative to industrial curing salt. The naturally occurring nitrites in celery work synergistically with the added salt to cure food.

In 2019, a trend of drinking celery juice was reported in the United States, based on "detoxification" claims posted on a blog. The claims have no scientific basis, but the trend caused a sizable spike in celery prices.

Raw celery is 95% water, 3% carbohydrates, 0.7% protein, and contains negligible fat (table). A reference serving provides 16 calories of food energy, and is a rich source of vitamin K, providing 28% of the Daily Value, with no other micronutrients in significant content (table).

Celery is among a small group of foods that may provoke allergic reactions; for people with celery allergy, exposure can cause potentially fatal anaphylactic shock. Cases of allergic reaction to ingestion of celery root have also been reported in pollen-sensitive individuals resulting in gastrointestinal disorders and other symptoms, although in most cases, celery sensitivity is not considered clinically significant. In the European Union and the United Kingdom, foods that contain or may contain celery, even in trace amounts, must be clearly marked.

The "Apium graveolens" plant has an OPALS allergy scale rating of 4 out of 10, indicating moderate potential to cause allergic reactions, exacerbated by over-use of the same plant throughout a garden. Celery has caused skin rashes and cross-reactions with carrots and ragweed.

The main chemicals responsible for the aroma and taste of celery are butylphthalide and sedanolide.

Daniel Zohary and Maria Hopf note that celery leaves and inflorescences were part of the garlands found in the tomb of pharaoh Tutankhamun (died 1323 BC), and celery mericarps dated to the seventh century BC were recovered in the Heraion of Samos. However, they note "A. graveolens" grows wild in these areas, it is hard to decide whether these remains represent wild or cultivated forms." Only by classical antiquity is it thought that celery was cultivated.

M. Fragiska mentions an archeological find of celery dating to the 9th century BC, at Kastanas; however, the literary evidence for ancient Greece is far more abundant. In Homer's "Iliad", the horses of the Myrmidons graze on wild celery that grows in the marshes of Troy, and in "Odyssey", there is mention of the meadows of violet and wild celery surrounding Calypso's Cave.

In the "Capitulary" of Charlemagne, compiled c. 800, "apium" appears, as does "olisatum", or alexanders, among medicinal herbs and vegetables the Frankish emperor desired to see grown. At some later point in medieval Europe, celery displaced alexanders.

The name "celery" retraces the plant's route of successive adoption in European cooking, as the English "celery" (1664) is derived from the French "céleri" coming from the Lombard term, "seleri", from the Latin "selinon", borrowed from Greek.

Celery's late arrival in the English kitchen is an end-product of the long tradition of seed selection needed to reduce the sap's bitterness and increase its sugars. By 1699, John Evelyn could recommend it in his "Acetaria. A Discourse of Sallets": "Sellery, apium Italicum, (and of the Petroseline Family) was formerly a stranger with us (nor very long since in Italy) is a hot and more generous sort of Macedonian Persley or Smallage... and for its high and grateful Taste is ever plac'd in the middle of the Grand Sallet, at our Great Men's tables, and Praetors feasts, as the Grace of the whole Board".

Celery makes a minor appearance in colonial American gardens; its culinary limitations are reflected in the observation by the author of "A Treatise on Gardening, by a Citizen of Virginia" that it is "one of the species of parsley". Its first extended treatment in print was in Bernard M'Mahon's "American Gardener's Calendar" (1806).

After the mid-19th century, continued selections for refined crisp texture and taste brought celery to American tables, where it was served in celery vases to be salted and eaten raw. Celery was so popular in the United States during the 19th century and early 20th century that the New York Public Library's historical menu archive shows that it was the third most popular dish in New York City menus during that time, behind only coffee and tea. In those days, celery cost more than caviar, as it was difficult to cultivate. There were also many varieties of celery back then that are no longer around because they are difficult to grow and do not ship well.

A chthonian symbol among the ancient Greeks, celery was said to have sprouted from the blood of Kadmilos, father of the Cabeiri, chthonian divinities celebrated in Samothrace, Lemnos, and Thebes. The spicy odor and dark leaf color encouraged this association with the cult of death. In classical Greece, celery leaves were used as garlands for the dead, and the wreaths of the winners at the Isthmian Games were first made of celery before being replaced by crowns made of pine. According to Pliny the Elderin Achaea, the garland worn by the winners of the sacred Nemean Games was also made of celery. The Ancient Greek colony of Selinous (, "Selinous"), on Sicily, was named after wild parsley that grew abundantly there; Selinountian coins depicted a parsley leaf as the symbol of the city.



Celestines

The Celestines were a Roman Catholic monastic order, a branch of the Benedictines, founded in 1244. At the foundation of the new rule, they were called Hermits of St Damiano, or Moronites (or Murronites), and did not assume the appellation of Celestines until after the election of their founder, Peter of Morone (Pietro Murrone), to the Papacy as Celestine V. They used the post-nominal initials O.S.B. Cel. The last house closed in 1785. 

The fame of the holy life and the austerities practised by Pietro Morone in his solitude on the Mountain of Majella, near Sulmona, attracted many visitors, several of whom were moved to remain and share his mode of life. They built a small convent on the spot inhabited by the holy hermit, which became too small for the accommodation of those who came to share their life of privations. Peter of Morone (later Pope Celestine V), their founder, built a number of other small oratories in that neighborhood.

Around the year 1254, Peter of Morone gave the order a rule formulated in accordance with his own practices. In 1264 the new institution was approved as a branch of the Benedictines by Urban IV; however, the next pope Pope Gregory X had commanded that all orders founded since the prior Lateran Council should not be further multiplied. Hearing a rumor that the order was to be suppressed, the reclusive Peter traveled to Lyon, where the Pope was holding a council. There he persuaded Gregory to approve his new order, making it a branch of the Benedictines and following the rule of Saint Benedict, but adding to it additional severities and privations. Gregory took it under the Papal protection, assured to it the possession of all property it might acquire, and endowed it with exemption from the authority of the ordinary. Nothing more was needed to ensure the rapid spread of the new association and Peter the hermit of Morone lived to see himself "Superior-General" to thirty-six monasteries and more than six hundred monks. 

As soon as he had seen his new order thus consolidated he gave up the government of it to a certain Robert, and retired once again to an even more remote site to devote himself to solitary penance and prayer. Shortly afterwards, in a chapter of the order held in 1293, the original monastery of Majella being judged to be too desolate and exposed to too rigorous a climate, it was decided that the Abbey of the Holy Spirit at Monte Morrone, located in Sulmona, should be the headquarters of the order and the residence of the General-Superior, where it continued for centuries. The next year Peter of Morrone, despite his reluctance, was elected Pope by the name of Celestine V. From there on, the order he had founded took the name of Celestines. During his short reign as Pope, the former hermit confirmed the rule of the order, which he had himself composed, and conferred on the society a variety of special graces and privileges. In the only creation of cardinals promoted by him, among the twelve raised to the purple, there were two monks of his order. He also visited personally the Benedictine monastery on Monte Cassino, where he persuaded the monks to accept his more rigorous rule. He sent fifty monks of his order to introduce it, who remained there, however, for only a few months.

After the death of the founder the order was favoured and privileged by Benedict XI, and rapidly spread through Italy, Germany, Flanders, and France, where they were received by Philip the Fair in 1300.

The administration of the order was carried on somewhat after the pattern of Cluny, that is all monasteries were subject to the Abbey of the Holy Ghost at Sulmona, and these dependent houses were divided into provinces. The Celestines had ninety-six houses in Italy, twenty-one in France, and a few in Germany.

Subsequently, the French Celestines, with the consent of the Italian superiors of the order, and of Pope Martin V in 1427, obtained the privilege of making new constitutions for themselves, which they did in the 17th century in a series of regulations accepted by the provincial chapter in 1667. At that time the French congregation of the order was composed of twenty-one monasteries, the head of which was that of Paris, and was governed by a Provincial with the authority of General. Paul V was a notable benefactor of the order. The order became extinct in the eighteenth century.

According to their special constitutions the Celestines were bound to say matins in the choir at two o'clock in the morning, and always to abstain from eating meat, save in illness. The distinct rules of their order with regard to fasting are numerous, but not more severe than those of similar congregations, though much more so than is required by the old Benedictine rule. In reading their minute directions for divers degrees of abstinence on various days, it is impossible to avoid being struck by the conviction that the great object of the framers of these rules was the general purpose of ensuring an ascetic mode of life.

The Celestines wore a white woollen cassock bound with a linen band, and a leathern girdle of the same colour, with a scapular unattached to the body of the dress, and a black hood. It was not permitted to them to wear any shirt save of serge. Their dress in short was very like that of the Cistercians. But it is a tradition in the order that in the time of the founder they wore a coarse brown cloth. The church and monastery of San Pietro in Montorio originally belonged to the Celestines in Rome; but they were turned out of it by Sixtus IV to make way for Franciscans, receiving from the Pope in exchange the Church of St Eusebius of Vercelli with the adjacent mansion for a monastery.


Cessna

Cessna () is an American brand of general aviation aircraft owned by Textron Aviation since 2014, headquartered in Wichita, Kansas. Originally, it was a brand of the Cessna Aircraft Company, an American general aviation aircraft manufacturing corporation also headquartered in Wichita. The company produced small, piston-powered aircraft, as well as business jets. For much of the mid-to-late 20th century, Cessna was one of the highest-volume and most diverse producers of general aviation aircraft in the world. It was founded in 1927 by Clyde Cessna and Victor Roos and was purchased by General Dynamics in 1985, then by Textron, Inc. in 1992. In March 2014, when Textron purchased the Beechcraft and Hawker Aircraft corporations, Cessna ceased operations as a subsidiary company, and joined the others as one of the three distinct brands produced by Textron Aviation.

Throughout its history, and especially in the years following World War II, Cessna became best-known for producing high-wing, small piston aircraft. Its most popular and iconic aircraft is the Cessna 172, delivered since 1956 (with a break from 1986–1996), with more sold than any other aircraft in history. Since the first model was delivered in 1972, the brand has also been well known for its Citation family of low-wing business jets which vary in size.

Clyde Cessna, a farmer in Rago, Kansas, built his own aircraft and flew it in June 1911. He was the first person to do so between the Mississippi River and the Rocky Mountains. Cessna started his wood-and-fabric aircraft ventures in Enid, Oklahoma, testing many of his early planes on the salt flats. When bankers in Enid refused to lend him more money to build his planes, he moved to Wichita.
Cessna Aircraft was formed when Clyde Cessna and Victor Roos became partners in the Cessna-Roos Aircraft Company in 1927. Roos resigned just one month into the partnership, selling back his interest to Cessna. Shortly afterward, Roos's name was dropped from the company name.

The Cessna DC-6 earned certification on the same day as the stock market crash of 1929, October 29, 1929.

In 1932, the Cessna Aircraft Company closed due to the Great Depression.

However, the Cessna CR-3 custom racer made its first flight in 1933. The plane won the 1933 American Air Race in Chicago and later set a new world speed record for engines smaller than 500 cubic inches by averaging .

Cessna's nephews, brothers Dwane and Dwight Wallace, bought the company from Cessna in 1934. They reopened it and began the process of building it into what would become a global success.
The Cessna C-37 was introduced in 1937 as Cessna's first seaplane when equipped with Edo floats. In 1940, Cessna received their largest order to date, when they signed a contract with the U.S. Army for 33 specially equipped Cessna T-50s, their first twin engine plane. Later in 1940, the Royal Canadian Air Force placed an order for 180 T-50s.

Cessna returned to commercial production in 1946, after the revocation of wartime production restrictions (L-48), with the release of the Model 120 and Model 140. The approach was to introduce a new line of all-metal aircraft that used production tools, dies and jigs, rather than the hand-built tube-and-fabric construction process used before the war.

The Model 140 was named by the US Flight Instructors Association as the "Outstanding Plane of the Year" in 1948.

Cessna's first helicopter, the Cessna CH-1, received FAA type certification in 1955.

Cessna introduced the Cessna 172 in 1956. It became the most produced airplane in history. During the post-World War II era, Cessna was known as one of the "Big Three" in general aviation aircraft manufacturing, along with Piper and Beechcraft.

In 1959, Cessna acquired Aircraft Radio Corporation (ARC), of Boonton, New Jersey, a leading manufacturer of aircraft radios. During these years, Cessna expanded the ARC product line, and rebranded ARC radios as "Cessna" radios, making them the "factory option" for avionics in new Cessnas. However, during this time, ARC radios suffered a severe decline in quality and popularity. Cessna kept ARC as a subsidiary until 1983, selling it to avionics-maker Sperry.

In 1960, Cessna acquired McCauley Industrial Corporation, of Ohio, a leading manufacturer of propellers for light aircraft. McCauley became the world's leading producer of general aviation aircraft propellers, largely through their installation on Cessna airplanes.

In 1960, Cessna affiliated itself with Reims Aviation of Reims, France. In 1963, Cessna produced its 50,000th airplane, a Cessna 172.

Cessna's first business jet, the Cessna Citation I, performed its maiden flight on September 15, 1969.

Cessna produced its 100,000th single-engine airplane in 1975.

In 1985, Cessna ceased to be an independent company. It was purchased by General Dynamics Corporation and became a wholly owned subsidiary. Production of the Cessna Caravan began. General Dynamics in turn sold Cessna to Textron in 1992.
Late in 2007, Cessna purchased the bankrupt Columbia Aircraft company for US$26.4M and would continue production of the Columbia 350 and 400 as the Cessna 350 and Cessna 400 at the Columbia factory in Bend, Oregon. However, production of both aircraft had ended by 2018.

On November 27, 2007, Cessna announced the then-new Cessna 162 would be built in the People's Republic of China by Shenyang Aircraft Corporation, which is a subsidiary of the China Aviation Industry Corporation I (AVIC I), a Chinese government-owned consortium of aircraft manufacturers. Cessna reported that the decision was made to save money and also that the company had no more plant capacity in the United States at the time. Cessna received much negative feedback for this decision, with complaints centering on the recent quality problems with Chinese production of other consumer products, China's human rights record, exporting of jobs and China's less than friendly political relationship with the United States. The customer backlash surprised Cessna and resulted in a company public relations campaign. In early 2009, the company attracted further criticism for continuing plans to build the 162 in China while laying off large numbers of workers in the United States. In the end, the Cessna 162 was not a commercial success and only a small number were delivered before production was cancelled.

The company's business suffered notably during the late-2000s recession, laying off more than half its workforce between January 2009 and September 2010.
On November 4, 2008, Cessna's parent company, Textron, indicated that Citation production would be reduced from the original 2009 target of 535 "due to continued softening in the global economic environment" and that this would result in an undetermined number of lay-offs at Cessna.

On November 8, 2008, at the Aircraft Owners and Pilots Association (AOPA) Expo, CEO Jack Pelton indicated that sales of Cessna aircraft to individual buyers had fallen, but piston and turboprop sales to businesses had not. "While the economic slowdown has created a difficult business environment, we are encouraged by brisk activity from new and existing propeller fleet operators placing almost 200 orders for 2009 production aircraft," Pelton stated.

Beginning in January 2009, a total of 665 jobs were cut at Cessna's Wichita and Bend, Oregon plants. The Cessna factory at Independence, Kansas, which builds the Cessna piston-engined aircraft and the Cessna Mustang, did not see any layoffs, but one third of the workforce at the former Columbia Aircraft facility in Bend was laid off. This included 165 of the 460 employees who built the Cessna 350 and 400. The remaining 500 jobs were eliminated at the main Cessna Wichita plant.

In January 2009, the company laid off an additional 2,000 employees, bringing the total to 4,600. The job cuts included 120 at the Bend, Oregon, facility reducing the plant that built the Cessna 350 and 400 to fewer than half the number of workers that it had when Cessna bought it. Other cuts included 200 at the Independence, Kansas, plant that builds the single-engined Cessnas and the Mustang, reducing that facility to 1,300 workers.

On April 29, 2009, the company suspended the Citation Columbus program and closed the Bend, Oregon, facility. The Columbus program was finally cancelled in early July 2009. The company reported, "Upon additional analysis of the business jet market related to this product offering, we decided to formally cancel further development of the Citation Columbus". With the 350 and 400 production moving to Kansas, the company indicated that it would lay off 1,600 more workers, including the remaining 150 employees at the Bend plant and up to 700 workers from the Columbus program.

In early June 2009, Cessna laid off an additional 700 salaried employees, bringing the total number of lay-offs to 7,600, which was more than half the company's workers at the time.

The company closed its three Columbus, Georgia, manufacturing facilities between June 2010 and December 2011. The closures included the new facility that was opened in August 2008 at a cost of US$25M, plus the McCauley Propeller Systems plant. These closures resulted in total job losses of 600 in Georgia. Some of the work was relocated to Cessna's Independence, Kansas, or Mexican facilities.

Cessna's parent company, Textron, posted a loss of US$8M in the first quarter of 2010, largely driven by continuing low sales at Cessna, which were down 44%. Half of Cessna's workforce remained laid-off and CEO Jack Pelton stated that he expected the recovery to be long and slow.

In September 2010, a further 700 employees were laid off, bringing the total to 8,000 jobs lost. CEO Jack Pelton indicated this round of layoffs was due to a "stalled [and] lackluster economy" and noted that while the number of orders cancelled for jets had been decreasing, new orders had not met expectations. Pelton added, "our strategy is to defend and protect our current markets while investing in products and services to secure our future, but we can do this only if we succeed in restructuring our processes and reducing our costs."

On May 2, 2011, CEO Jack J. Pelton retired. The new CEO, Scott A. Ernest, started on May 31, 2011. Ernest joined Textron after 29 years at General Electric, where he had most recently served as vice president and general manager, global supply chain for GE Aviation. Ernest previously worked for Textron CEO Scott Donnelly when both worked at General Electric.

In September 2011, the Federal Aviation Administration (FAA) proposed a US$2.4 million fine against the company for its failure to follow quality assurance requirements while producing fiberglass components at its plant in Chihuahua, Mexico. Excess humidity meant that the parts did not cure correctly and quality assurance did not detect the problems. The failure to follow procedures resulted in the delamination in flight of a section of one Cessna 400's wing skin from the spar while the aircraft was being flown by an FAA test pilot. The aircraft was landed safely. The FAA also discovered 82 other aircraft parts that had been incorrectly made and not detected by the company's quality assurance. The investigation resulted in an emergency Airworthiness Directive that affected 13 Cessna 400s.

Since March 2012, Cessna has been pursuing building business jets in China as part of a joint venture with Aviation Industry Corporation of China (AVIC). The company stated that it intends to eventually build all aircraft models in China, saying "The agreements together pave the way for a range of business jets, utility single-engine turboprops and single-engine piston aircraft to be manufactured and certified in China."

In late April 2012, the company added 150 workers in Wichita as a result of anticipated increased demand for aircraft production. Overall, they have cut more than 6000 jobs in the Wichita plant since 2009.

In March 2014, Cessna ceased operations as a company and instead became a brand of Textron Aviation.

During the 1950s and 1960s, Cessna's marketing department followed the lead of Detroit automakers and came up with many unique marketing terms in an effort to differentiate its product line from their competitors.

Other manufacturers and the aviation press widely ridiculed and spoofed many of the marketing terms, but Cessna built and sold more aircraft than any other manufacturer during the boom years of the 1960s and 1970s.

Generally, the names of Cessna models do not follow a theme, but there is usually logic to the numbering: the 100 series are the light singles, the 200s are the heftier, the 300s are light to medium twins, the 400s have "wide oval" cabin-class accommodation and the 500s are jets. Many Cessna models have names starting with C for the sake of alliteration (e.g. Citation, Crusader, Chancellor).

Cessna marketing terminology includes:



In October 2020, Textron Aviation was producing the following Cessna-branded models:


Czesław Miłosz

Czesław Miłosz ( , , ; 30 June 1911 – 14 August 2004) was a Polish-American poet, prose writer, translator, and diplomat. He primarily wrote his poetry in Polish. Regarded as one of the great poets of the 20th century, he won the 1980 Nobel Prize in Literature. In its citation, the Swedish Academy called Miłosz a writer who "voices man's exposed condition in a world of severe conflicts".

Miłosz survived the German occupation of Warsaw during World War II and became a cultural attaché for the Polish government during the postwar period. When communist authorities threatened his safety, he defected to France and ultimately chose exile in the United States, where he became a professor at the University of California, Berkeley. His poetry—particularly about his wartime experience—and his appraisal of Stalinism in a prose book, "The Captive Mind", brought him renown as a leading "émigré" artist and intellectual.

Throughout his life and work, Miłosz tackled questions of morality, politics, history, and faith. As a translator, he introduced Western works to a Polish audience, and as a scholar and editor, he championed a greater awareness of Slavic literature in the West. Faith played a role in his work as he explored his Catholicism and personal experience. He wrote in Polish and English.

Miłosz died in Kraków, Poland, in 2004. He is interred in Skałka, a church known in Poland as a place of honor for distinguished Poles.

Czesław Miłosz was born on 30 June 1911, in the village of Šeteniai (), Kovno Governorate, Russian Empire (now Kėdainiai district, Kaunas County, Lithuania). He was the son of Aleksander Miłosz (1883–1959), a Polish civil engineer, and his wife, Weronika (née Kunat; 1887–1945).

Miłosz was born into a prominent family. On his mother's side, his grandfather was Zygmunt Kunat, a descendant of a Polish family that traced its lineage to the 13th century and owned an estate in Krasnogruda (in present-day Poland). Having studied agriculture in Warsaw, Zygmunt settled in Šeteniai after marrying Miłosz's grandmother, Jozefa, a descendant of the noble Syruć family, which was of Lithuanian origin. One of her ancestors, , had been personal secretary to Stanisław I, King of Poland and Grand Duke of Lithuania. Miłosz's paternal grandfather, Artur Miłosz, was also from a noble family and fought in the 1863 January Uprising for Polish independence. Miłosz's grandmother, Stanisława, was a doctor's daughter from Riga, Latvia, and a member of the German-Polish von Mohl family. The Miłosz estate was in Serbiny, a name that Miłosz's biographer has suggested could indicate Serbian origin; it is possible the Miłosz family originated in Serbia and settled in present-day Lithuania after being expelled from Germany centuries earlier. Miłosz's father was born and educated in Riga. Miłosz's mother was born in Šeteniai and educated in Kraków.

Despite this noble lineage, Miłosz's childhood on his maternal grandfather's estate in Šeteniai lacked the trappings of wealth or the customs of the upper class. He memorialized his childhood in a 1955 novel, "", and a 1959 memoir, "." In these works, he described the influence of his Catholic grandmother, Jozefa, his burgeoning love for literature, and his early awareness, as a member of the Polish gentry in Lithuania, of the role of class in society.

Miłosz's early years were marked by upheaval. When his father was hired to work on infrastructure projects in Siberia, he and his mother traveled to be with him. After World War I broke out in 1914, Miłosz's father was conscripted into the Russian army, tasked with engineering roads and bridges for troop movements. Miłosz and his mother were sheltered in Vilnius when the German army captured it in 1915. Afterward, they once again joined Miłosz's father, following him as the front moved further into Russia, where, in 1917, Miłosz's brother, Andrzej, was born. Finally, after moving through Estonia and Latvia, the family returned to Šeteniai in 1918. But the Polish–Soviet War broke out in 1919, during which Miłosz's father was involved in a failed attempt to incorporate the newly independent Lithuania into the Second Polish Republic, resulting in his expulsion from Lithuania and the family's move to what was then known as Wilno, which had come under Polish control after the Polish–Lithuanian War of 1920. The Polish-Soviet War continued, forcing the family to move again. At one point during the conflict, Polish soldiers fired at Miłosz and his mother, an episode he recounted in "Native Realm." The family returned to Wilno after the war ended in 1921.

Despite the interruptions of wartime wanderings, Miłosz proved to be an exceptional student with a facility for languages. He ultimately learned Polish, Lithuanian, Russian, English, French, and Hebrew. After graduation from Sigismund Augustus Gymnasium in Wilno, he entered Stefan Batory University in 1929 as a law student. While at university, Miłosz joined a student group called and a student poetry group called , along with the young poets Jerzy Zagórski, Teodor Bujnicki, , Jerzy Putrament, and . His first published poems appeared in the university's student magazine in 1930.

In 1931, he visited Paris, where he first met his distant cousin, Oscar Milosz, a French-language poet of Lithuanian descent who had become a Swedenborgian. Oscar became a mentor and inspiration. Returning to Wilno, Miłosz's early awareness of class difference and sympathy for those less fortunate than himself inspired his defense of Jewish students at the university who were being harassed by an anti-Semitic mob. Stepping between the mob and the Jewish students, Miłosz fended off attacks. One student was killed when a rock was thrown at his head.

Miłosz's first volume of poetry, ", was published in Polish in 1933. In the same year, he publicly read his poetry at an anti-racist "Poetry of Protest" event in Wilno, occasioned by Hitler's rise to power in Germany. In 1934, he graduated with a law degree, and the poetry group Żagary disbanded. Miłosz relocated to Paris on a scholarship to study for one year and write articles for a newspaper back in Wilno. In Paris, he frequently met with his cousin Oscar.

By 1936, he had returned to Wilno, where he worked on literary programs at Polish Radio Wilno. His second poetry collection, ", was published that same year, eliciting from one critic a comparison to Adam Mickiewicz. After only one year at Radio Wilno, Miłosz was dismissed due to an accusation that he was a left-wing sympathizer: as a student, he had adopted socialist views from which, by then, he had publicly distanced himself, and he and his boss, , had produced programming that included performances by Jews and Byelorussians, which angered right-wing nationalists. After Byrski made a trip to the Soviet Union, an anonymous complaint was lodged with the management of Radio Wilno that the station housed a communist cell, and Byrski and Miłosz were dismissed. In summer 1937, Miłosz moved to Warsaw, where he found work at Polish Radio and met his future wife, (née Dłuska; 1909–1986), who was at the time married to another man.

Miłosz was in Warsaw when it was bombarded as part of the German invasion of Poland in September 1939. Along with colleagues from Polish Radio, he escaped the city, making his way to Lwów. But when he learned that Janina had remained in Warsaw with her parents, he looked for a way back. The Soviet invasion of Poland thwarted his plans, and, to avoid the incoming Red Army, he fled to Bucharest. There he obtained a Lithuanian identity document and Soviet visa that allowed him to travel by train to Kyiv and then Wilno. After the Red Army invaded Lithuania, he procured fake documents that he used to enter the part of German-occupied Poland the Germans had dubbed the "General Government". It was a difficult journey, mostly on foot, that ended in summer 1940. Finally back in Warsaw, he reunited with Janina.

Like many Poles at the time, to evade notice by German authorities, Miłosz participated in underground activities. For example, with higher education officially forbidden to Poles, he attended underground lectures by Władysław Tatarkiewicz, the Polish philosopher and historian of philosophy and aesthetics. He translated Shakespeare's "As You Like It" and T. S. Eliot's "The Waste Land" into Polish. Along with his friend the novelist Jerzy Andrzejewski, he also arranged for the publication of his third volume of poetry, "", under a pseudonym in September 1940. The pseudonym was "Jan Syruć" and the title page said the volume had been published by a fictional press in Lwów in 1939; in fact, it may have been the first clandestine book published in occupied Warsaw. In 1942, Miłosz arranged for the publication of an anthology of Polish poets, "Invincible Song: Polish Poetry of War Time", by an underground press.

Miłosz's riskiest underground wartime activity was aiding Jews in Warsaw, which he did through an underground socialist organization called Freedom. His brother, Andrzej, was also active in helping Jews in Nazi-occupied Poland; in 1943, Andrzej transported the Polish Jew Seweryn Tross and his wife from Vilnius to Warsaw. Miłosz took in the Trosses, found them a hiding place, and supported them financially. The Trosses ultimately died during the Warsaw Uprising. Miłosz helped at least three other Jews in similar ways: Felicja Wołkomińska and her brother and sister.

Despite his willingness to engage in underground activity and vehement opposition to the Nazis, Miłosz did not join the Polish Home Army. In later years, he explained that this was partly out of an instinct for self-preservation and partly because he saw its leadership as right-wing and dictatorial. He also did not participate in the planning or execution of the Warsaw Uprising. According to Polish literary historian , he saw the uprising as a "doomed military effort" and lacked the "patriotic elation" for it. He called the uprising "a blameworthy, lightheaded enterprise", but later criticized the Red Army for failing to support it when it had the opportunity to do so.

As German troops began torching Warsaw buildings in August 1944, Miłosz was captured and held in a prisoner transit camp; he was later rescued by a Catholic nun—a stranger to him—who pleaded with the Germans on his behalf. Once freed, he and Janina escaped the city, ultimately settling in a village outside Kraków, where they were staying when the Red Army swept through Poland in January 1945, after Warsaw had been largely destroyed.

In the preface to his 1953 book "The Captive Mind", Miłosz wrote, "I do not regret those years in Warsaw, which was, I believe, the most agonizing spot in the whole of terrorized Europe. Had I then chosen emigration, my life would certainly have followed a very different course. But my knowledge of the crimes which Europe has witnessed in the twentieth century would be less direct, less concrete than it is". Immediately after the war, Miłosz published his fourth poetry collection, "Rescue"; it focused on his wartime experiences and contains some of his most critically praised work, including the 20-poem cycle "The World," composed like a primer for naïve schoolchildren, and the cycle "Voices of Poor People". The volume also contains some of his most frequently anthologized poems, including "A Song on the End of the World", "", and "A Poor Christian Looks at the Ghetto".

From 1945 to 1951, Miłosz served as a cultural attaché for the newly formed People's Republic of Poland. It was in this capacity that he first met Jane Zielonko, the future translator of "The Captive Mind", with whom he had a brief relationship. He moved from New York City to Washington, D.C., and finally to Paris, organizing and promoting Polish cultural occasions such as musical concerts, art exhibitions, and literary and cinematic events. Although he was a representative of Poland, which had become a Soviet satellite country behind the Iron Curtain, he was not a member of any communist party. In "The Captive Mind", he explained his reasons for accepting the role:My mother tongue, work in my mother tongue, is for me the most important thing in life. And my country, where what I wrote could be printed and could reach the public, lay within the Eastern Empire. My aim and purpose was to keep alive freedom of thought in my own special field; I sought in full knowledge and conscience to subordinate my conduct to the fulfillment of that aim. I served abroad because I was thus relieved from direct pressure and, in the material which I sent to my publishers, could be bolder than my colleagues at home. I did not want to become an émigré and so give up all chance of taking a hand in what was going on in my own country.Miłosz did not publish a book while he was a representative of the Polish government. Instead, he wrote articles for various Polish periodicals introducing readers to American writers like Eliot, William Faulkner, Ernest Hemingway, Norman Mailer, Robert Lowell, and W. H. Auden. He also translated into Polish Shakespeare's "Othello" and the work of Walt Whitman, Carl Sandburg, Pablo Neruda, and others.

In 1947, Miłosz's son, Anthony, was born in Washington, D.C.

In 1948, Miłosz arranged for the Polish government to fund a Department of Polish Studies at Columbia University. Named for Adam Mickiewicz, the department featured lectures by Manfred Kridl, Miłosz's friend who was then on the faculty of Smith College, and produced a scholarly book about Mickiewicz. Mickiewicz's granddaughter wrote a letter to Dwight D. Eisenhower, then the president of Columbia University, to express her approval, but the Polish American Congress, an influential group of Polish émigrés, denounced the arrangement in a letter to Eisenhower that they shared with the press, which alleged a communist infiltration at Columbia. Students picketed and called for boycotts. One faculty member resigned in protest. Despite the controversy, the department was established, the lectures took place, and the book was produced, but the department was discontinued in 1954 when funding from Poland ceased.

In 1949, Miłosz visited Poland for the first time since joining its diplomatic corps and was appalled by the conditions he saw, including an atmosphere of pervasive fear of the government. After returning to the U.S., he began to look for a way to leave his post, even soliciting advice from Albert Einstein, whom he met in the course of his duties.

As the Polish government, influenced by Joseph Stalin, became more oppressive, his superiors began to view Miłosz as a threat: he was outspoken in his reports to Warsaw and met with people not approved by his superiors. Consequently, his superiors called him "an individual who ideologically is totally alien". Toward the end of 1950, when Janina was pregnant with their second child, Miłosz was recalled to Warsaw, where in December 1950 his passport was confiscated, ostensibly until it could be determined that he did not plan to defect. After intervention by Poland's foreign minister, Zygmunt Modzelewski, Miłosz's passport was returned. Realizing that he was in danger if he remained in Poland, Miłosz left for Paris in January 1951.

Upon arriving in Paris, Miłosz went into hiding, aided by the staff of the Polish émigré magazine "Kultura." With his wife and son still in the United States, he applied to enter the U.S. and was denied. At the time, the U.S. was in the grip of McCarthyism, and influential Polish émigrés had convinced American officials that Miłosz was a communist. Unable to leave France, Miłosz was not present for the birth of his second son, John Peter, in Washington, D.C., in 1951.

With the United States closed to him, Miłosz requested—and was granted—political asylum in France. After three months in hiding, he announced his defection at a press conference and in a "Kultura" article, "No", that explained his refusal to live in Poland or continue working for the Polish regime. He was the first artist of note from a communist country to make public his reasons for breaking ties with his government. His case attracted attention in Poland, where his work was banned and he was attacked in the press, and in the West, where prominent individuals voiced criticism and support. For example, the future Nobel laureate Pablo Neruda, then a supporter of the Soviet Union, attacked him in a communist newspaper as "The Man Who Ran Away". On the other hand, Albert Camus, another future Nobel laureate, visited Miłosz and offered his support. Another supporter during this period was the Swiss philosopher Jeanne Hersch, with whom Miłosz had a brief romantic affair.

Miłosz was finally reunited with his family in 1953, when Janina and the children joined him in France. That same year saw the publication of "The Captive Mind", a nonfiction work that uses case studies to dissect the methods and consequences of Soviet communism, which at the time had prominent admirers in the West. The book brought Miłosz his first readership in the United States, where it was credited by some on the political left (such as Susan Sontag) with helping to change perceptions about communism. The German philosopher Karl Jaspers described it as a "significant historical document". It became a staple of political science courses and is considered a classic work in the study of totalitarianism.

Miłosz's years in France were productive. In addition to "The Captive Mind", he published two poetry collections ("Daylight" (1954) and "A Treatise on Poetry" (1957)), two novels ("" (1955) and "The Issa Valley" (1955)), and a memoir ("Native Realm" (1959)). All were published in Polish by an émigré press in Paris.

Andrzej Franaszek has called "A Treatise on Poetry" Miłosz's magnum opus, while the scholar Helen Vendler compared it to "The Waste Land", a work "so powerful that it bursts the bounds in which it was written—the bounds of language, geography, epoch". A long poem divided into four sections, "A Treatise on Poetry" surveys Polish history, recounts Miłosz's experience of war, and explores the relationship between art and history.

In 1956, Miłosz and Janina were married.

In 1960, Miłosz was offered a position as a visiting lecturer at the University of California, Berkeley. With this offer, and with the climate of McCarthyism abated, he was able to move to the United States. He proved to be an adept and popular teacher, and was offered tenure after only two months. The rarity of this, and the degree to which he had impressed his colleagues, are underscored by the fact that Miłosz lacked a PhD and teaching experience. Yet his deep learning was obvious, and after years of working administrative jobs that he found stifling, he told friends that he was in his element in a classroom. With stable employment as a tenured professor of Slavic languages and literatures, Miłosz was able to secure American citizenship and purchase a home in Berkeley.

Miłosz began to publish scholarly articles in English and Polish on a variety of authors, including Fyodor Dostoevsky. But despite his successful transition to the U.S., he described his early years at Berkeley as frustrating, as he was isolated from friends and viewed as a political figure rather than a great poet. (In fact, some of his Berkeley faculty colleagues, unaware of his creative output, expressed astonishment when he won the Nobel Prize.) His poetry was not available in English, and he was not able to publish in Poland.

As part of an effort to introduce American readers to his poetry, as well as to his fellow Polish poets' work, Miłosz conceived and edited the anthology "", which was published in English in 1965. American poets like W.S. Merwin, and American scholars like Clare Cavanagh, have credited it with a profound impact. It was many English-language readers' first exposure to Miłosz's poetry, as well as that of Polish poets like Wisława Szymborska, Zbigniew Herbert, and Tadeusz Różewicz. (In the same year, Miłosz's poetry also appeared in the first issue of "Modern Poetry in Translation," an English-language journal founded by prominent literary figures Ted Hughes and Daniel Weissbort. The issue also featured Miroslav Holub, Yehuda Amichai, Ivan Lalić, Vasko Popa, Zbigniew Herbert, and Andrei Voznesensky.) In 1969, Miłosz's textbook "The History of Polish Literature" was published in English. He followed this with a volume of his own work, "Selected Poems" (1973), some of which he translated into English himself. This was his first anthology of poetry published in English language.

At the same time, Miłosz continued to publish in Polish with an émigré press in Paris. His poetry collections from this period include "King Popiel and Other Poems" (1962), "Bobo’s Metamorphosis" (1965), "City Without a Name" (1969), and "From the Rising of the Sun" (1974).

During Miłosz's time at Berkeley, the campus became a hotbed of student protest, notably as the home of the Free Speech Movement, which has been credited with helping to "define a generation of student activism" across the United States. Miłosz's relationship to student protesters was sometimes antagonistic: he called them "spoiled children of the bourgeoisie" and their political zeal naïve. At one campus event in 1970, he mocked protesters who claimed to be demonstrating for peace and love: "Talk to me about love when they come into your cell one morning, line you all up, and say 'You and you, step forward—it’s your time to die—unless any of your friends loves you so much he wants to take your place!'" Comments like these were in keeping with his stance toward American counterculture of the 1960s in general. For example, in 1968, when Miłosz was listed as a signatory of an open letter of protest written by poet and counterculture figure Allen Ginsberg and published in "The New York Review of Books", Miłosz responded by calling the letter "dangerous nonsense" and insisting that he had not signed it.

After 18 years, Miłosz retired from teaching in 1978. To mark the occasion, he was awarded a "Berkeley Citation", the University of California's equivalent of an honorary doctorate. But when his wife, Janina, fell ill and required expensive medical treatment, Miłosz returned to teaching seminars. This year also marked the publication of his second English-language poetry anthology, "Bells in Winter".

On 9 October 1980, the Swedish Academy announced that Miłosz had won the Nobel Prize in Literature. The award catapulted him to global fame. On the day the prize was announced, Miłosz held a brief press conference and then left to teach a class on Dostoevsky. In his Nobel lecture, Miłosz described his view of the role of the poet, lamented the tragedies of the 20th century, and paid tribute to his cousin Oscar.

Many Poles became aware of Miłosz for the first time when he won the Nobel Prize. After a 30-year ban in Poland, his writing was finally published there in limited selections. He was also able to visit Poland for the first time since fleeing in 1951 and was greeted by crowds with a hero's welcome. He met with leading Polish figures like Lech Wałęsa and Pope John Paul II. At the same time, his early work, until then only available in Polish, began to be translated into English and many other languages.

In 1981, Miłosz was appointed the Norton Professor of Poetry at Harvard University, where he was invited to deliver the Charles Eliot Norton Lectures. He used the opportunity, as he had before becoming a Nobel laureate, to draw attention to writers who had been unjustly imprisoned or persecuted. The lectures were published as "" (1983).

Miłosz continued to publish work in Polish through his longtime publisher in Paris, including the poetry collections "Hymn of the Pearl" (1981) and "Unattainable Earth" (1986), and the essay collection "Beginning with My Streets" (1986).

In 1986, Miłosz's wife, Janina, died.

In 1988, Miłosz's "Collected Poems" appeared in English; it was the first of several attempts to collect all his poetry into a single volume. After the fall of communism in Poland, he split his time between Berkeley and Kraków, and he began to publish his writing in Polish with a publisher based in Kraków. When Lithuania broke free from the Soviet Union in 1991, Miłosz visited for the first time since 1939. In 2000, he moved to Kraków.

In 1992, Miłosz married Carol Thigpen, an academic at Emory University in Atlanta, Georgia. They remained married until her death in 2002. His work from the 1990s includes the poetry collections "Facing the River" (1994) and ' (1997), and the collection of short prose "Miłosz’s ABC’s" (1997). Miłosz's last stand-alone volumes of poetry were ' (2000), and "The Second Space" (2002). Uncollected poems written afterward appeared in English in "New and Selected Poems" (2004) and, posthumously, in "Selected and Last Poems" (2011).

Czesław Miłosz died on 14 August 2004, at his Kraków home, aged 93. He was given a state funeral at the historic Mariacki Church in Kraków. Polish Prime Minister Marek Belka attended, as did the former president of Poland, Lech Wałęsa. Thousands of people lined the streets to witness his coffin moved by military escort to his final resting place at Skałka Roman Catholic Church, where he was one of the last to be commemorated. In front of that church, the poets Seamus Heaney, Adam Zagajewski, and Robert Hass read Miłosz's poem "In Szetejnie" in Polish, French, English, Russian, Lithuanian, and Hebrew—all the languages Miłosz knew. Media from around the world covered the funeral.

Protesters threatened to disrupt the proceedings on the grounds that Miłosz was anti-Polish, anti-Catholic, and had signed a petition supporting gay and lesbian freedom of speech and assembly. Pope John Paul II, along with Miłosz's confessor, issued public messages confirming that Miłosz had received the sacraments, which quelled the protest.

Miłosz's brother, Andrzej Miłosz (1917–2002), was a Polish journalist, translator, and documentary film producer. His work included Polish documentaries about his brother.

Miłosz's son, Anthony, is a composer and software designer. He studied linguistics, anthropology, and chemistry at the University of California at Berkeley, and neuroscience at the University of California Medical Center in San Francisco. In addition to releasing recordings of his own compositions, he has translated some of his father's poems into English.

In addition to the Nobel Prize in Literature, Miłosz received the following awards:


Miłosz was named a distinguished visiting professor or fellow at many institutions, including the University of Michigan and University of Oklahoma, where he was a Puterbaugh Fellow in 1999. He was an elected member of the American Academy of Arts and Sciences, the American Academy of Arts and Letters, and the Serbian Academy of Sciences and Arts. He received honorary doctorates from Harvard University, the University of Michigan, the University of California at Berkeley, Jagiellonian University, Catholic University of Lublin, and Vytautas Magnus University in Lithuania. Vytautas Magnus University and Jagiellonian University have academic centers named for Miłosz.

In 1992, Miłosz was made an honorary citizen of Lithuania, where his birthplace was made into a museum and conference center. In 1993, he was made an honorary citizen of Kraków.

His books also received awards. His first, "A Poem on Frozen Time", won an award from the Union of Polish Writers in Wilno. "The Seizure of Power" received the Prix Littéraire Européen (European Literary Prize). The collection "Roadside Dog" received a Nike Award in Poland.

In 1989, Miłosz was named one of the "Righteous Among the Nations" at Israel's Yad Vashem memorial to the Holocaust, in recognition of his efforts to save Jews in Warsaw during World War II.

Miłosz has also been honored posthumously. The Polish Parliament declared 2011, the centennial of his birth, the "Year of Miłosz". It was marked by conferences and tributes throughout Poland, as well as in New York City, at Yale University, and at the Dublin Writers Festival, among many other locations. The same year, he was featured on a Lithuanian postage stamp. Streets are named for him near Paris, Vilnius, and in the Polish cities of Kraków, Poznań, Gdańsk, Białystok, and Wrocław. In Gdańsk there is a Czesław Miłosz Square. In 2013, a primary school in Vilnius was named for Miłosz, joining schools in Mierzecice, Poland, and Schaumburg, Illinois, that bear his name.

In 1978, the Russian-American poet Joseph Brodsky called Miłosz "one of the great poets of our time; perhaps the greatest". Miłosz has been cited as an influence by numerous writers—contemporaries and succeeding generations. For example, scholars have written about Miłosz's influence on the writing of Seamus Heaney, and Clare Cavanagh has identified the following poets as having benefited from Miłosz's influence: Robert Pinsky, Edward Hirsch, Rosanna Warren, Robert Hass, Charles Simic, Mary Karr, Carolyn Forché, Mark Strand, Ted Hughes, Joseph Brodsky, and Derek Walcott.

By being smuggled into Poland, Miłosz's writing was a source of inspiration to the anti-communist Solidarity movement there in the early 1980s. Lines from his poem "" are inscribed on the Monument to the Fallen Shipyard Workers of 1970 in Gdańsk, where Solidarity originated.

Of the effect of Miłosz's edited volume "Postwar Polish Poetry" on English-language poets, Merwin wrote, "Miłosz’s book had been a talisman and had made most of the literary bickering among the various ideological encampments, then most audible in the poetic doctrines in English, seem frivolous and silly". Similarly, the British poet and scholar Donald Davie argued that, for many English-language writers, Miłosz's work encouraged an expansion of poetry to include multiple viewpoints and an engagement with subjects of intellectual and historical importance: "I have suggested, going for support to the writings of Miłosz, that no concerned and ambitious poet of the present day, aware of the enormities of twentieth-century history, can for long remain content with the privileged irresponsibility allowed to, or imposed on, the lyric poet".

Miłosz's writing continues to be the subject of academic study, conferences, and cultural events. His papers, including manuscripts, correspondence, and other materials, are housed at the Beinecke Rare Book and Manuscript Library at Yale University.

Miłosz's birth in a time and place of shifting borders and overlapping cultures, and his later naturalization as an American citizen, have led to competing claims about his nationality. Although his family identified as Polish and Polish was his primary language, and although he frequently spoke of Poland as his country, he also publicly identified himself as one of the last citizens of the multi-ethnic Grand Duchy of Lithuania. Writing in a Polish newspaper in 2000, he claimed, "I was born in the very center of Lithuania and so have a greater right than my great forebear, Mickiewicz, to write 'O Lithuania, my country.'" But in his Nobel lecture, he said, "My family in the 16th century already spoke Polish, just as many families in Finland spoke Swedish and in Ireland English, so I am a Polish, not a Lithuanian, poet". Public statements such as these, and numerous others, inspired discussion about his nationality, including a claim that he was "arguably the greatest spokesman and representative of a Lithuania that, in Miłosz’s mind, was bigger than its present incarnation". Others have viewed Miłosz as an American author, hosting exhibitions and writing about him from that perspective and including his work in anthologies of American poetry. 

But in "The New York Review of Books" in 1981, the critic John Bayley wrote, "nationality is not a thing [Miłosz] can take seriously; it would be hard to imagine a greater writer more emancipated from even its most subtle pretensions". Echoing this notion, the scholar and diplomat Piotr Wilczek argued that, even when he was greeted as a national hero in Poland, Miłosz "made a distinct effort to remain a universal thinker". Speaking at a ceremony to celebrate his birth centenary in 2011, Lithuanian President Dalia Grybauskaitė stressed that Miłosz's works "unite the Lithuanian and Polish people and reveal how close and how fruitful the ties between our people can be".

Though raised Catholic, Miłosz as a young man came to adopt a "scientific, atheistic position mostly", though he later returned to the Catholic faith. He translated parts of the Bible into Polish, and allusions to Catholicism pervade his poetry, culminating in a long 2001 poem, "A Theological Treatise". For some critics, Miłosz's belief that literature should provide spiritual fortification was outdated: Franaszek suggests that Miłosz's belief was evidence of a "beautiful naïveté", while David Orr, citing Miłosz's dismissal of "poetry which does not save nations or people", accused him of "pompous nonsense".

Miłosz expressed some criticism of both Catholicism and Poland (a majority-Catholic country), causing furor in some quarters when it was announced that he would be interred in Kraków's historic Skałka church. Cynthia Haven writes that, to some readers, Miłosz's embrace of Catholicism can seem surprising and complicates the understanding of him and his work.

Miłosz's body of work comprised multiple literary genres: poetry, fiction (particularly the novel), autobiography, scholarship, personal essay, and lectures. His letters are also of interest to scholars and lay readers; for example, his correspondence with writers such as Jerzy Andrzejewski, Witold Gombrowicz, and Thomas Merton have been published.

At the outset of his career, Miłosz was known as a "catastrophist" poet—a label critics applied to him and other poets from the Żagary poetry group to describe their use of surreal imagery and formal inventiveness in reaction to a Europe beset by extremist ideologies and war. While Miłosz evolved away from the apocalyptic view of catastrophist poetry, he continued to pursue formal inventiveness throughout his career. As a result, his poetry demonstrates a wide-ranging mastery of form, from long or epic poems (e.g., "A Treatise on Poetry") to poems of just two lines (e.g., "On the Death of a Poet" from the collection "This"), and from prose poems and free verse to classic forms such as the ode or elegy. Some of his poems use rhyme, but many do not. In numerous cases, Miłosz used form to illuminate meaning in his poetry; for example, by juxtaposing variable stanzas to accentuate ideas or voices that challenge each other.

Miłosz's work is known for its complexity; according to the scholars Leonard Nathan and Arthur Quinn, Miłosz "prided himself on being an esoteric writer accessible to a mere handful of readers". Nevertheless, some common themes are readily apparent throughout his body of work.

The poet, critic, and frequent Miłosz translator Robert Hass has described Miłosz as "a poet of great inclusiveness", with a fidelity to capturing life in all of its sensuousness and multiplicities. According to Hass, Miłosz's poems can be viewed as "dwelling in contradiction", where one idea or voice is presented only to be immediately challenged or changed. According to English poet Donald Davie, this allowance for contradictory voices—a shift from the solo lyric voice to a chorus—is among the most important aspects of Miłosz's work.

The poetic chorus is deployed not just to highlight the complexity of the modern world but also to search for morality, another of Miłosz's recurrent themes. Nathan and Quinn write, "Miłosz’s work is devoted to unmasking man’s fundamental duality; he wants to make his readers admit the contradictory nature of their own experience" because doing so "forces us to assert our preferences as preferences". That is, it forces readers to make conscious choices, which is the arena of morality. At times, Miłosz's exploration of morality was explicit and concrete, such as when, in "The Captive Mind", he ponders the right way to respond to three Lithuanian women who were forcibly moved to a Russian communal farm and wrote to him for help, or when, in the poems "Campo Dei Fiori" and "A Poor Christian Looks at the Ghetto", he addresses survivor's guilt and the morality of writing about another's suffering.

Miłosz's exploration of morality takes place in the context of history, and confrontation with history is another of his major themes. Vendler wrote, "for Miłosz, the person is irrevocably a person in history, and the interchange between external event and the individual life is the matrix of poetry". Having experienced both Nazism and Stalinism, Miłosz was particularly concerned with the notion of "historical necessity", which, in the 20th century, was used to justify human suffering on a previously unheard-of scale. Yet Miłosz did not reject the concept entirely. Nathan and Quinn summarize Miłosz's appraisal of historical necessity as it appears in his essay collection ": "Some species rise, others fall, as do human families, nations, and whole civilizations. There may well be an internal logic to these transformations, a logic that when viewed from sufficient distance has its own elegance, harmony, and grace. Our reason tempts us to be enthralled by this superhuman splendor; but when so enthralled we find it difficult to remember, except perhaps as an element in an abstract calculus, the millions of individuals, the millions upon millions, who unwillingly paid for this splendor with pain and blood".

Miłosz's willingness to accept a form of logic in history points to another recurrent aspect of his writing: his capacity for wonder, amazement, and, ultimately, faith—not always religious faith, but "faith in the objective reality of a world to be known by the human mind but not constituted by that mind". At other times, Miłosz was more explicitly religious in his work. According to scholar and translator Michael Parker, "crucial to any understanding of Miłosz’s work is his complex relationship to Catholicism". His writing is filled with allusions to Christian figures, symbols, and theological ideas, though Miłosz was closer to Gnosticism, or what he called Manichaeism, in his personal beliefs, viewing the universe as ruled by an evil whose influence human beings must try to escape. From this perspective, "he can at once admit that the world is ruled by necessity, by evil, and yet still find hope and sustenance in the beauty of the world. History reveals the pointlessness of human striving, the instability of human things; but time also is the moving image of eternity". According to Hass, this viewpoint left Miłosz "with the task of those heretical Christians…to suffer time, to contemplate being, and to live in the hope of the redemption of the world".

Miłosz had numerous literary and intellectual influences, although scholars of his work—and Miłosz himself, in his writings—have identified the following as significant: Oscar Miłosz (who inspired Miłosz's interest in the metaphysical) and, through him, Emanuel Swedenborg; Lev Shestov; Simone Weil (whose work Miłosz translated into Polish); Dostoevsky; William Blake (whose concept of "Ulro" Miłosz borrowed for his book "), and Eliot.












Carnivore

A carnivore , or meat-eater (Latin, "caro", genitive "carnis", meaning meat or "flesh" and "vorare" meaning "to devour"), is an animal or plant whose food and energy requirements derive from the consumption of animal tissues (mainly muscle, fat and other soft tissues) whether through hunting or scavenging.

The technical term for mammals in the order Carnivora is "carnivoran", and they are so-named because most member species in the group have a carnivorous diet, but the similarity of the name of the order and the name of the diet causes confusion.

Many but not all carnivorans are meat eaters; a few, such as the large and small cats (felidae) are "obligate" carnivores (see below). Other classes of carnivore are highly variable. The Ursids, for example: While the Arctic polar bear eats meat almost exclusively (more than 90% of its diet is meat), almost all other bear species are omnivorous, and one species, the giant panda, is nearly exclusively herbivorous.

Dietary carnivory is not a distinguishing trait of the order. Many mammals with highly carnivorous diets are "not" members of the order Carnivora. Cetaceans, for example, all eat other animals, but are paradoxically members of the almost exclusively plant-eating hooved mammals.

Animals that depend solely on animal flesh for their nutrient requirements are called "hypercarnivores" or "obligate carnivores", whilst those which also consume non-animal food are called "mesocarnivores", or "facultative carnivores", or "omnivores" (there are no clear distinctions). A carnivore at the top of the food chain (adults not preyed upon by other animals) is termed an apex predator, regardless of whether it is an "obligate" or "facultative" carnivore.
Outside the animal kingdom, there are several genera containing carnivorous plants (predominantly insectivores) and several phyla containing carnivorous fungi (preying mostly on microscopic invertebrates, such as nematodes, amoebae, and springtails).

Carnivores are sometimes characterized by their type of prey. For example, animals that eat mainly insects and similar invertebrates are called "insectivores", while those that eat mainly fish are called "piscivores".

Carnivores may alternatively be classified according to the percentage of meat in their diet. The diet of a hypercarnivore consists of more than 70% meat, that of a mesocarnivore 30–70%, and that of a hypocarnivore less than 30%, with the balance consisting of non-animal foods, such as fruits, other plant material, or fungi.

Omnivores also consume both animal and non-animal food, and apart from their more general definition, there is no clearly defined ratio of plant vs. animal material that distinguishes a "facultative carnivore" from an "omnivore".

Obligate or "true" carnivores are those whose diet requires nutrients found only in animal flesh. While obligate carnivores might be able to ingest small amounts of plant matter, they lack the necessary physiology required to fully digest it. Some obligate carnivorous mammals will ingest vegetation as an emetic, a food that upsets their stomachs, to self-induce vomiting.

Obligate carnivores are diverse. The amphibian axolotl consumes mainly worms and larvae in its environment, but if necessary will consume algae. All felids, including the domestic cat, require a diet of primarily animal flesh and organs. Specifically, cats have high protein requirements and their metabolisms appear unable to synthesize essential nutrients such as retinol, arginine, taurine, and arachidonic acid; thus, in nature, they must consume flesh to supply these nutrients.

Characteristics commonly associated with carnivores include strength, speed, and keen senses for hunting, as well as teeth and claws for capturing and tearing prey. However, some carnivores do not hunt and are scavengers, lacking the physical characteristics to bring down prey; in addition, most hunting carnivores will scavenge when the opportunity arises. Carnivores have comparatively short digestive systems, as they are not required to break down the tough cellulose found in plants.

Many hunting animals have evolved eyes facing forward, enabling depth perception. This is almost universal among mammalian predators, while most reptile and amphibian predators have eyes facing sideways.

"Predation" (the eating of one living creature by another for nutrition) predates the rise of commonly recognized carnivores by hundreds of millions (perhaps billions) of years. Indeed: It began with single-celled organisms, before multicellular creatures, and so carnivory predates the clear distinction between plants and animals (herbivory / carnivory).

The earliest predators were microbial organisms, which engulfed or grazed on others. Because the earliest fossil record is the poorest, these first predators could date back anywhere between 1 and over 2.7 Gya (billion years ago).

The rise of eukaryotic cells at around 2.7 Gya, the rise of multicellular organisms at about 2 Gya, and the rise of mobile predators (around 600 Mya – 2 Gya, probably around 1 Gya) have all been attributed to early predatory behavior, and many very early remains show evidence of boreholes or other markings attributed to small predator species.

Among more familiar species, the first vertebrate carnivores were fish, and then amphibians that moved on to land. Early tetrapods were large amphibious piscivores.
The first tetrapods, or land-dwelling vertebrates, were piscivorous amphibians called "labyrinthodonts". They gave rise to insectivorous vertebrates and, later, to predators of other tetrapods.

Some scientists assert that "Dimetrodon" "was the first terrestrial vertebrate to develop the curved, serrated teeth that enable a predator to eat prey much larger than itself." While amphibians continued to feed on fish and later insects, reptiles began exploring two new food types: tetrapods (carnivory) and then plants (herbivory). Carnivory was a natural transition from insectivory for medium and large tetrapods, requiring minimal adaptation; in contrast, a complex set of adaptations was necessary for feeding on highly fibrous plant materials.

In the Mesozoic, some theropod dinosaurs such as "Tyrannosaurus rex" are thought probably to have been obligate carnivores.

Though the theropods were the larger carnivores, several carnivorous mammal groups were already present. Most notable are the gobiconodontids, the triconodontid "Jugulator", the deltatheroidans and "Cimolestes". Many of these, such as "Repenomamus", "Jugulator" and "Cimolestes", were among the largest mammals in their faunal assemblages, capable of attacking dinosaurs.

In the early-to-mid-Cenozoic, the dominant predator forms were mammals: hyaenodonts, oxyaenids, entelodonts, ptolemaiidans, arctocyonids and mesonychians, representing a great diversity of eutherian carnivores in the northern continents and Africa. In South America, sparassodonts were dominant, while Australia saw the presence of several marsupial predators, such as the dasyuromorphs and thylacoleonids. From the Miocene to the present, the dominant carnivorous mammals have been carnivoramorphs.

Most carnivorous mammals, from dogs to "deltatheridiums", share several dental adaptations, such as carnassialiforme teeth, long canines and even similar tooth replacement patterns. Most aberrant are thylacoleonids, with a diprodontan dentition completely unlike that of any other mammal; and eutriconodonts like gobiconodontids and "Jugulator", with a three-cusp anatomy which nevertheless functioned similarly to carnassials.


Celts

The Celts (, see pronunciation for different usages) or Celtic peoples () were a collection of Indo-European peoples in Europe and Anatolia, identified by their use of Celtic languages and other cultural similarities. Major Celtic groups included the Gauls; the Celtiberians and Gallaeci of Iberia; the Britons, Picts, and Gaels of Britain and Ireland; the Boii; and the Galatians. The relation between ethnicity, language and culture in the Celtic world is unclear and debated; for example over the ways in which the Iron Age people of Britain and Ireland should be called Celts. In current scholarship, 'Celt' primarily refers to 'speakers of Celtic languages' rather than to a single ethnic group.

The history of pre-Celtic Europe and Celtic origins is debated. The traditional "Celtic from the East" theory, says the proto-Celtic language arose in the late Bronze Age Urnfield culture of central Europe, named after grave sites in southern Germany, which flourished from around 1200 BC. This theory links the Celts with the Iron Age Hallstatt culture which followed it (c. 1200–500 BC), named for the rich grave finds in Hallstatt, Austria, and with the following La Tène culture (c. 450 BC onward), named after the La Tène site in Switzerland. It proposes that Celtic culture spread westward and southward from these areas by diffusion or migration. A newer theory, "Celtic from the West", suggests proto-Celtic arose earlier, was a "lingua franca" in the Atlantic Bronze Age coastal zone, and spread eastward. Another newer theory, "Celtic from the Centre", suggests proto-Celtic arose between these two zones, in Bronze Age Gaul, then spread in various directions. After the Celtic settlement of Southeast Europe in the 3rd century BC, Celtic culture reached as far east as central Anatolia, Turkey.

The earliest undisputed examples of Celtic language are the Lepontic inscriptions from the 6th century BC. Continental Celtic languages are attested almost exclusively through inscriptions and place-names. Insular Celtic languages are attested from the 4th century AD in Ogham inscriptions, though they were clearly being spoken much earlier. Celtic literary tradition begins with Old Irish texts around the 8th century AD. Elements of Celtic mythology are recorded in early Irish and early Welsh literature. Most written evidence of the early Celts comes from Greco-Roman writers, who often grouped the Celts as barbarian tribes. They followed an ancient Celtic religion overseen by druids.

The Celts were often in conflict with the Romans, such as in the Roman–Gallic wars, the Celtiberian Wars, the conquest of Gaul and conquest of Britain. By the 1st century AD, most Celtic territories had become part of the Roman Empire. By c. 500, due to Romanisation and the migration of Germanic tribes, Celtic culture had mostly become restricted to Ireland, western and northern Britain, and Brittany. Between the 5th and 8th centuries, the Celtic-speaking communities in these Atlantic regions emerged as a reasonably cohesive cultural entity. They had a common linguistic, religious and artistic heritage that distinguished them from surrounding cultures.

Insular Celtic culture diversified into that of the Gaels (Irish, Scots and Manx) and the Celtic Britons (Welsh, Cornish, and Bretons) of the medieval and modern periods. A modern Celtic identity was constructed as part of the Romanticist Celtic Revival in Britain, Ireland, and other European territories such as Galicia. Today, Irish, Scottish Gaelic, Welsh, and Breton are still spoken in parts of their former territories, while Cornish and Manx are undergoing a revival.

The first recorded use of the name 'Celts' – as () in Ancient Greek – was by Greek geographer Hecataeus of Miletus in 517 BC, when writing about a people living near Massilia (modern Marseille), southern Gaul. In the fifth century BC, Herodotus referred to living around the source of the Danube and in the far west of Europe. The etymology of is unclear. Possible roots include Indo-European *"kʲel" 'to hide' (seen also in Old Irish , and Modern Welsh ), *"kʲel" 'to heat' or *"kel" 'to impel'. It may come from the Celtic language. Linguist Kim McCone supports this view and notes that "Celt-" is found in the names of several ancient Gauls such as Celtillus, father of Vercingetorix. He suggests it meant the people or descendants of "the hidden one", noting the Gauls claimed descent from an underworld god (according to "Commentarii de Bello Gallico"), and linking it with the Germanic "Hel". Others view it as a name coined by Greeks; among them linguist Patrizia de Bernardo Stempel, who suggests it meant "the tall ones".

In the first century BC, Roman leader Julius Caesar reported that the Gauls called themselves 'Celts', , in their own tongue. Thus whether it was given to them by others or not, it was used by the Celts themselves. Greek geographer Strabo, writing about Gaul towards the end of the first century BC, refers to the "race which is now called both "Gallic" and "Galatic"", though he also uses "Celtica" as another name for Gaul. He reports Celtic peoples in Iberia too, calling them "Celtiberi" and "Celtici". Pliny the Elder noted the use of "Celtici" in Lusitania as a tribal surname, which epigraphic findings have confirmed.

A Latin name for the Gauls, (), may come from a Celtic ethnic name, perhaps borrowed into Latin during the Celtic expansion into Italy from the early fifth century BC. Its root may be Proto-Celtic "*galno", meaning "power, strength" (whence Old Irish "gal" "boldness, ferocity", Welsh "gallu" "to be able, power"). The Greek name Γαλάται (, Latinized "Galatae") most likely has the same origin, referring to the Gauls who invaded southeast Europe and settled in Galatia. The suffix "-atai" might be a Greek inflection. Linguist Kim McCone suggests it comes from Proto-Celtic "*galatis" ("ferocious, furious"), and was not originally an ethnic name but a name for young warrior bands. He says "If the Gauls' initial impact on the Mediterranean world was primarily a military one typically involving fierce young "*galatīs", it would have been natural for the Greeks to apply this name for the type of "Keltoi" that they usually encountered".

Because Classical writers did not call the inhabitants of Britain and Ireland () or , some scholars prefer not to use the term for the Iron Age inhabitants of those islands. However, they spoke Celtic languages, shared other cultural traits, and Roman historian Tacitus says the Britons resembled the Gauls in customs and religion.

For at least 1,000 years the name Celt was not used at all, and nobody called themselves Celts or Celtic, until from about 1700, after the word 'Celtic' was rediscovered in classical texts, it was applied for the first time to the distinctive culture, history, traditions, language of the modern Celtic nations – Ireland, Scotland, Wales, Cornwall, Brittany, and the Isle of Man. 'Celt' is a modern English word, first attested in 1707 in the writing of Edward Lhuyd, whose work, along with that of other late 17th-century scholars, brought academic attention to the languages and history of the early Celtic inhabitants of Great Britain. The English words Gaul, Gauls () and Gaulish (first recorded in the 16–17th centuries) come from French and , a borrowing from Frankish , "Roman land" (see Gaul: Name), the root of which is Proto-Germanic "*walha-", "foreigner, Roman, Celt", whence the English word 'Welsh' (Old English "wælisċ"). Proto-Germanic "*walha" comes from the name of the Volcae, a Celtic tribe who lived first in southern Germany and central Europe, then migrated to Gaul. This means that English Gaul, despite its superficial similarity, is not actually derived from Latin (which should have produced in French), though it does refer to the same ancient region.

Celtic refers to a language family and, more generally, means "of the Celts" or "in the style of the Celts". Several archaeological cultures are considered Celtic, based on unique sets of artefacts. The link between language and artefact is aided by the presence of inscriptions. The modern idea of a Celtic cultural identity or "Celticity" focuses on similarities among languages, works of art, and classical texts, and sometimes also among material artefacts, social organisation, homeland and mythology. Earlier theories held that these similarities suggest a common "racial" ("race" is contemporarily an invalid epistemolical and genetic concept) origin for the various Celtic peoples, but more recent theories hold that they reflect a common cultural and linguistic heritage more than a genetic one. Celtic cultures seem to have been diverse, with the use of a Celtic language being the main thing they had in common.

Today, the term 'Celtic' generally refers to the languages and cultures of Ireland, Scotland, Wales, Cornwall, the Isle of Man, and Brittany; also called the Celtic nations. These are the regions where Celtic languages are still spoken to some extent. The four are Irish, Scottish Gaelic, Welsh, and Breton; plus two recent revivals, Cornish (a Brittonic language) and Manx (a Goidelic language). There are also attempts to reconstruct Cumbric, a Brittonic language of northern Britain. Celtic regions of mainland Europe are those whose residents claim a Celtic heritage, but where no Celtic language survives; these include western Iberia, i.e. Portugal and north-central Spain (Galicia, Asturias, Cantabria, Castile and León, Extremadura).

Continental Celts are the Celtic-speaking people of mainland Europe and Insular Celts are the Celtic-speaking people of the British and Irish islands, and their descendants. The Celts of Brittany derive their language from migrating Insular Celts from Britain and so are grouped accordingly.

The Celtic languages are a branch of the Indo-European languages. By the time Celts are first mentioned in written records around 400 BC, they were already split into several language groups, and spread over much of western mainland Europe, the Iberian Peninsula, Ireland and Britain. The languages developed into Celtiberian, Goidelic and Brittonic branches, among others.

The mainstream view during most of the twentieth century is that the Celts and the proto-Celtic language arose out of the Urnfield culture of central Europe around 1000 BC, spreading westward and southward over the following few hundred years. The Urnfield culture was preeminent in central Europe during the late Bronze Age, circa 1200 BC to 700 BC. The spread of iron-working led to the Hallstatt culture (c. 800 to 500 BC) developing out of the Urnfield culture in a wide region north of the Alps. The Hallstatt culture developed into the La Tène culture from about 450 BC, which came to be identified with Celtic art.

In 1846, Johann Georg Ramsauer unearthed an ancient grave field with distinctive grave goods at Hallstatt, Austria. Because the burials "dated to roughly the time when Celts are mentioned near the Danube by Herodotus, Ramsauer concluded that the graves were Celtic". Similar sites and artifacts were found over a wide area, which were named the 'Hallstatt culture'. In 1857, the archaeological site of La Tène was discovered in Switzerland. The huge collection of artifacts had a distinctive style. Artifacts of this 'La Tène style' were found elsewhere in Europe, "particularly in places where people called Celts were known to have lived and early Celtic languages are attested. As a result, these items quickly became associated with the Celts, so much so that by the 1870s scholars began to regard finds of the La Tène as 'the archaeological expression of the Celts'". This cultural network was overrun by the Roman Empire, though traces of La Tène style were still seen in Gallo-Roman artifacts. In Britain and Ireland, the La Tène style survived precariously to re-emerge in Insular art.

The Urnfield-Hallstatt theory began to be challenged in the latter 20th century, when it was accepted that the oldest known Celtic-language inscriptions were those of Lepontic from the 6th century BC and Celtiberian from the 2nd century BC. These were found in northern Italy and Iberia, neither of which were part of the 'Hallstatt' nor 'La Tène' cultures at the time. The Urnfield-Hallstatt theory was partly based on ancient Greco-Roman writings, such as the "Histories" of Herodotus, which placed the Celts at the source of the Danube. However, Stephen Oppenheimer shows that Herodotus seemed to believe the Danube rose near the Pyrenees, which would place the Ancient Celts in a region which is more in agreement with later classical writers and historians (i.e. in Gaul and Iberia). The theory was also partly based on the abundance of inscriptions bearing Celtic personal names in the Eastern Hallstatt region (Noricum). However, Patrick Sims-Williams notes that these date to the later Roman era, and says they suggest "relatively late settlement by a Celtic-speaking elite".

In the late 20th century, the Urnfield-Hallstatt theory began to fall out of favour with some scholars, which was influenced by new archaeological finds. 'Celtic' began to refer primarily to 'speakers of Celtic languages' rather than to a single culture or ethnic group. A new theory suggested that Celtic languages arose earlier, along the Atlantic coast (including Britain, Ireland, Armorica and Iberia), long before evidence of 'Celtic' culture is found in archaeology. Myles Dillon and Nora Kershaw Chadwick argued that "Celtic settlement of the British Isles" might date to the Bell Beaker culture of the Copper and Bronze Age (from c. 2750 BC). Martín Almagro Gorbea (2001) also proposed that Celtic arose in the 3rd millennium BC, suggesting that the spread of the Bell Beaker culture explained the wide dispersion of the Celts throughout western Europe, as well as the variability of the Celtic peoples. Using a multidisciplinary approach, Alberto J. Lorrio and Gonzalo Ruiz Zapatero reviewed and built on Almagro Gorbea's work to present a model for the origin of Celtic archaeological groups in Iberia and proposing a rethinking of the meaning of "Celtic".

John T. Koch and Barry Cunliffe have developed this 'Celtic from the West' theory. It proposes that the proto-Celtic language arose along the Atlantic coast and was the "lingua franca" of the Atlantic Bronze Age cultural network, later spreading inland and eastward. More recently, Cunliffe proposes that proto-Celtic had arisen in the Atlantic zone even earlier, by 3000 BC, and spread eastwards with the Bell Beaker culture over the following millennium. His theory is partly based on glottochronology, the spread of ancient Celtic-looking placenames, and thesis that the Tartessian language was Celtic. However, the proposal that Tartessian was Celtic is widely rejected by linguists, many of whom regard it as unclassified.

Celticist Patrick Sims-Williams (2020) notes that in current scholarship, 'Celt' is primarily a linguistic label. In his 'Celtic from the Centre' theory, he argues that the proto-Celtic language did not originate in central Europe nor the Atlantic, but in-between these two regions. He suggests that it "emerged as a distinct Indo-European dialect around the second millennium BC, probably somewhere in Gaul [centered in modern France] [...] whence it spread in various directions and at various speeds in the first millennium BC". Sims-Williams says this avoids the problematic idea "that Celtic was spoken over a vast area for a very long time yet somehow avoided major dialectal splits", and "it keeps Celtic fairly close to Italy, which suits the view that Italic and Celtic were in some way linked".

The Proto-Celtic language is usually dated to the Late Bronze Age. The earliest records of a Celtic language are the Lepontic inscriptions of Cisalpine Gaul (Northern Italy), the oldest of which pre-date the La Tène period. Other early inscriptions, appearing from the early La Tène period in the area of Massilia, are in Gaulish, which was written in the Greek alphabet until the Roman conquest. Celtiberian inscriptions, using their own Iberian script, appear later, after about 200 BC. Evidence of Insular Celtic is available only from about 400 AD, in the form of Primitive Irish Ogham inscriptions.

Besides epigraphic evidence, an important source of information on early Celtic is toponymy (place names).

Arnaiz-Villena et al. (2017) demonstrated that Celtic-related populations of the European Atlantic (Orkney Islands, Scottish, Irish, British, Bretons, Basques, Galicians) shared a common HLA system.

Other genetic research does not support the notion of a significant genetic link between these populations, beyond the fact that they are all West Europeans. Early European Farmers did settle Britain (and all of Northern Europe) in the Neolithic; however, recent genetics research has found that, between 2400 and 2000 BC, over 90% of British DNA was overturned by European Steppe Herders in a migration that brought large amounts of Steppe DNA (including the R1b haplogroup) to western Europe. Modern autosomal genetic clustering is testament to this fact, as both modern and Iron Age British and Irish samples cluster genetically very closely with other North Europeans, and less so with Galicians, Basques or those from the south of France.

The concept that the Hallstatt and La Tène cultures could be seen not just as chronological periods but as "Culture Groups", entities composed of people of the same ethnicity and language, had started to grow by the end of the 19th century. At the beginning of the 20th century the belief that these "Culture Groups" could be thought of in racial or ethnic terms was held by Gordon Childe, whose theory was influenced by the writings of Gustaf Kossinna. As the 20th century progressed, the ethnic interpretation of La Tène culture became more strongly rooted, and any findings of La Tène culture and flat inhumation cemeteries were linked to the Celts and the Celtic language.
In various academic disciplines the Celts were considered a Central European Iron Age phenomenon, through the cultures of Hallstatt and La Tène. However, archaeological finds from the Halstatt and La Tène culture were rare in Iberia, southwestern France, northern and western Britain, southern Ireland and Galatia and did not provide enough evidence for a culture like that of Central Europe. It is equally difficult to maintain that the origin of the Iberian Celts can be linked to the preceding Urnfield culture. This has resulted in a newer theory that introduces a 'proto-Celtic' substratum and a process of Celticisation, having its initial roots in the Bronze Age Bell Beaker culture.

The La Tène culture developed and flourished during the late Iron Age (from 450 BC to the Roman conquest in the 1st century BC) in eastern France, Switzerland, Austria, southwest Germany, the Czech Republic, Slovakia and Hungary. It developed out of the Hallstatt culture without any definite cultural break, under the impetus of considerable Mediterranean influence from Greek, and later Etruscan civilisations. A shift of settlement centres took place in the 4th century. The western La Tène culture corresponds to historical Celtic Gaul. Whether this means that the whole of La Tène culture can be attributed to a unified Celtic people is difficult to assess; archaeologists have repeatedly concluded that language and material culture do not necessarily run parallel. Frey notes that in the 5th century, "burial customs in the Celtic world were not uniform; rather, localised groups had their own beliefs, which, in consequence, also gave rise to distinct artistic expressions". Thus, while the La Tène culture is certainly associated with the Gauls, the presence of La Tène artefacts may be due to cultural contact and does not imply the permanent presence of Celtic speakers.

The Greek historian Ephorus of Cyme in Asia Minor, writing in the 4th century BC, believed the Celts came from the islands off the mouth of the Rhine and were "driven from their homes by the frequency of wars and the violent rising of the sea". Polybius published a history of Rome about 150 BC in which he describes the Gauls of Italy and their conflict with Rome. Pausanias in the 2nd century AD says that the Gauls "originally called Celts", "live on the remotest region of Europe on the coast of an enormous tidal sea". Posidonius described the southern Gauls about 100 BC. Though his original work is lost, later writers such as Strabo used it. The latter, writing in the early 1st century AD, deals with Britain and Gaul as well as Hispania, Italy and Galatia. Caesar wrote extensively about his Gallic Wars in 58–51 BC. Diodorus Siculus wrote about the Celts of Gaul and Britain in his 1st-century history.

Diodorus Siculus and Strabo both suggest that the heartland of the people they call Celts was in southern Gaul. The former says that the Gauls were to the north of the Celts, but that the Romans referred to both as Gauls (linguistically the Gauls were certainly Celts). Before the discoveries at Hallstatt and La Tène, it was generally considered that the Celtic heartland was southern Gaul, see Encyclopædia Britannica for 1813.

The Romans knew the Celts then living in present-day France as Gauls. The territory of these peoples probably included the Low Countries, the Alps and present-day northern Italy. Julius Caesar in his "Gallic Wars" described the 1st-century BC descendants of those Gauls.

Eastern Gaul became the centre of the western La Tène culture. In later Iron Age Gaul, the social organisation resembled that of the Romans, with large towns. From the 3rd century BC the Gauls adopted coinage. Texts with Greek characters from southern Gaul have survived from the 2nd century BC.

Greek traders founded Massalia about 600 BC, with some objects (mostly drinking ceramic vessels) being traded up the Rhône valley. But trade became disrupted soon after 500 BC and re-oriented over the Alps to the Po valley in the Italian peninsula. The Romans arrived in the Rhone valley in the 2nd century BC and encountered a mostly Celtic-speaking Gaul. Rome wanted land communications with its Iberian provinces and fought a major battle with the Saluvii at Entremont in 124–123 BC. Gradually Roman control extended, and the Roman province of Gallia Transalpina developed along the Mediterranean coast. The Romans knew the remainder of Gaul as Gallia Comata – "Long-haired Gaul."

In 58 BC the Helvetii planned to migrate westward but Julius Caesar forced them back. He then became involved in fighting the various tribes in Gaul, and by 55 BC had overrun most of Gaul. In 52 BC Vercingetorix led a revolt against Roman occupation but was defeated at the Battle of Alesia and surrendered.

Following the Gallic Wars of 58–51 BC, Caesar's "Celtica" formed the main part of Roman Gaul, becoming the province of Gallia Lugdunensis. This territory of the Celtic tribes was bounded on the south by the Garonne and on the north by the Seine and the Marne. The Romans attached large swathes of this region to neighbouring provinces Belgica and Aquitania, particularly under Augustus.

Place- and personal-name analysis and inscriptions suggest that Gaulish was spoken over most of what is now France.

Until the end of the 19th century, traditional scholarship dealing with the Celts did acknowledge their presence in the Iberian Peninsula as a material culture relatable to the Hallstatt and La Tène cultures. However, since according to the definition of the Iron Age in the 19th century Celtic populations were supposedly rare in Iberia and did not provide a cultural scenario that could easily be linked to that of Central Europe, the presence of Celtic culture in that region was generally not fully recognised. Modern scholarship, however, has clearly proven that Celtic presence and influences were most substantial in what is today Spain and Portugal (with perhaps the highest settlement saturation in Western Europe), particularly in the central, western and northern regions.

In addition to Gauls infiltrating from the north of the Pyrenees, the Roman and Greek sources mention Celtic populations in three parts of the Iberian Peninsula: the eastern part of the "Meseta" (inhabited by the Celtiberians), the southwest (Celtici, in modern-day Alentejo) and the northwest (Gallaecia and Asturias). A modern scholarly review found several archaeological groups of Celts in Spain:

The origins of the Celtiberians might provide a key to understanding the Celticisation process in the rest of the Peninsula. The process of Celticisation of the southwestern area of the peninsula by the Keltoi and of the northwestern area is, however, not a simple Celtiberian question. Recent investigations about the Callaici and Bracari in northwestern Portugal are providing new approaches to understanding Celtic culture (language, art and religion) in western Iberia.

John T. Koch of Aberystwyth University suggested that Tartessian inscriptions of the 8th century BC might be classified as Celtic. This would mean that Tartessian is the earliest attested trace of Celtic by a margin of more than a century.

In Germany by the late Bronze Age, the Urnfield culture () had replaced the Bell Beaker, Unetice and Tumulus cultures in central Europe, whilst the Nordic Bronze Age had developed in Scandinavia and northern Germany. The Hallstatt culture, which had developed from the Urnfield culture, was the predominant Western and Central European culture from the 12th to 8th centuries BC and during the early Iron Age (8th to 6th centuries BC). It was followed by the La Tène culture (5th to 1st centuries BC).

The people who had adopted these cultural characteristics in central and southern Germany are regarded as Celts. Celtic cultural centres developed in central Europe during the late Bronze Age ( until 700 BC). Some, like the Heuneburg, the oldest city north of the Alps, grew to become important cultural centres of the Iron Age in Central Europe, that maintained trade routes to the Mediterranean. In the 5th century BC the Greek historian Herodotus mentioned a Celtic city at the Danube – "Pyrene", that historians attribute to the Heuneburg. Beginning around 700 BC (or later), Germanic peoples (Germanic tribes) from southern Scandinavia and northern Germany expanded south and gradually replaced the Celtic peoples in Central Europe.

The Canegrate culture represented the first migratory wave of the proto-Celtic population from the northwest part of the Alps that, through the Alpine passes, had already penetrated and settled in the western Po valley between Lake Maggiore and Lake Como (Scamozzina culture). It has also been proposed that a more ancient proto-Celtic presence can be traced back to the beginning of the Middle Bronze Age, when North Westwern Italy appears closely linked regarding the production of bronze artefacts, including ornaments, to the western groups of the Tumulus culture. La Tène cultural material appeared over a large area of mainland Italy, the southernmost example being the Celtic helmet from Canosa di Puglia.

Italy is home to Lepontic, the oldest attested Celtic language (from the 6th century BC). Anciently spoken in Switzerland and in Northern-Central Italy, from the Alps to Umbria. According to the "Recueil des Inscriptions Gauloises", more than 760 Gaulish inscriptions have been found throughout present-day France – with the notable exception of Aquitaine – and in Italy, which testifies the importance of Celtic heritage in the peninsula.

In 391 BC, Celts "who had their homes beyond the Alps streamed through the passes in great strength and seized the territory that lay between the Apennine Mountains and the Alps" according to Diodorus Siculus. The Po Valley and the rest of northern Italy (known to the Romans as Cisalpine Gaul) was inhabited by Celtic-speakers who founded cities such as Milan. Later the Roman army was routed at the battle of Allia and Rome was sacked in 390 BC by the Senones.

At the battle of Telamon in 225 BC, a large Celtic army was trapped between two Roman forces and crushed.

The defeat of the combined Samnite, Celtic and Etruscan alliance by the Romans in the Third Samnite War sounded the beginning of the end of the Celtic domination in mainland Europe, but it was not until 192 BC that the Roman armies conquered the last remaining independent Celtic kingdoms in Italy.

The Celts also expanded down the Danube river and its tributaries. One of the most influential tribes, the Scordisci, established their capital at Singidunum (present-day Belgrade, Serbia) in the 3rd century BC. The concentration of hill-forts and cemeteries shows a dense population in the Tisza valley of modern-day Vojvodina, Serbia, Hungary and into Ukraine. Expansion into Romania was however blocked by the Dacians.

The Serdi were a Celtic tribe inhabiting Thrace. They were located around and founded Serdika (, , ), now Sofia in Bulgaria, which reflects their ethnonym. They would have established themselves in this area during the Celtic migrations at the end of the 4th century BC, though there is no evidence for their existence before the 1st century BC. "Serdi" are among traditional tribal names reported into the Roman era. They were gradually Thracianized over the centuries but retained their Celtic character in material culture up to a late date. According to other sources they may have been simply of Thracian origin, according to others they may have become of mixed Thraco-Celtic origin. Further south, Celts settled in Thrace (Bulgaria), which they ruled for over a century, and Anatolia, where they settled as the Galatians "(see also: Gallic Invasion of Greece)". Despite their geographical isolation from the rest of the Celtic world, the Galatians maintained their Celtic language for at least 700 years. St Jerome, who visited Ancyra (modern-day Ankara) in 373 AD, likened their language to that of the Treveri of northern Gaul.

For Venceslas Kruta, Galatia in central Turkey was an area of dense Celtic settlement.

The Boii tribe gave their name to Bohemia, Bologna and possibly Bavaria, and Celtic artefacts and cemeteries have been discovered further east in what is now Poland and Slovakia. A Celtic coin (Biatec) from Bratislava's mint was displayed on the old Slovak 5-crown coin.

As there is no archaeological evidence for large-scale invasions in some of the other areas, one current school of thought holds that Celtic language and culture spread to those areas by contact rather than invasion. However, the Celtic invasions of Italy and the expedition in Greece and western Anatolia, are well documented in Greek and Latin history.

There are records of Celtic mercenaries in Egypt serving the Ptolemies. Thousands were employed in 283–246 BC and they were also in service around 186 BC. They attempted to overthrow Ptolemy II.

All living Celtic languages today belong to the Insular Celtic languages, derived from the Celtic languages spoken in Iron Age Britain and Ireland. They separated into a Goidelic and a Brittonic branch early on. By the time of the Roman conquest of Britain in the 1st century AD, the Insular Celts were made up of the Celtic Britons, the Gaels (or Scoti), and the Picts (or Caledonians).

Linguists have debated whether a Celtic language came to the British Isles and then split, or whether the two branches arrived separately. The older view was that Celtic influence in the Isles was the result of successive migrations or invasions from the European mainland by diverse Celtic-speaking peoples over several centuries, accounting for the P-Celtic vs. Q-Celtic isogloss. This view has been challenged by the hypothesis that the islands' Celtic languages form an Insular Celtic dialect group. In the 19th and 20th centuries, scholars often dated the "arrival" of Celtic culture in Britain (via an invasion model) to the 6th century BC, corresponding to archaeological evidence of Hallstatt influence and the appearance of chariot burials in what is now England. Cunliffe and Koch propose in their newer 'Celtic from the West' theory that Celtic languages reached the Isles earlier, with the Bell Beaker culture c.2500 BC, or even before this. More recently, a major archaeogenetics study uncovered a migration into southern Britain in the Bronze Age from 1300 to 800 BC. The newcomers were genetically most similar to ancient individuals from Gaul. From 1000 BC, their genetic marker swiftly spread through southern Britain, but not northern Britain. The authors see this as a "plausible vector for the spread of early Celtic languages into Britain". There was much less immigration during the Iron Age, so it is likely that Celtic reached Britain before then. Cunliffe suggests that a branch of Celtic was already spoken in Britain, and the Bronze Age migration introduced the Brittonic branch.

Like many Celtic peoples on the mainland, the Insular Celts followed an Ancient Celtic religion overseen by druids. Some of the southern British tribes had strong links with Gaul and Belgica, and minted their own coins. During the Roman occupation of Britain, a Romano-British culture emerged in the southeast. The Britons and Picts in the north, and the Gaels of Ireland, remained outside the empire. During the end of Roman rule in Britain in the 400s AD, there was significant Anglo-Saxon settlement of eastern and southern Britain, and some Gaelic settlement of its western coast. During this time, some Britons migrated to the Armorican peninsula, where their culture became dominant. Meanwhile, much of northern Britain (Scotland) became Gaelic. By the 10th century AD, the Insular Celtic peoples had diversified into the Brittonic-speaking Welsh (in Wales), Cornish (in Cornwall), Bretons (in Brittany) and Cumbrians (in the Old North); and the Gaelic-speaking Irish (in Ireland), Scots (in Scotland) and Manx (on the Isle of Man).

Classical writers did not call the inhabitants of Britain and Ireland or "" (), leading some scholars to question the use of the term 'Celt' for the Iron Age inhabitants of those islands. The first historical account of the islands was by the Greek geographer Pytheas, who sailed around what he called the "Pretannikai nesoi" (the "Pretannic isles") around 310–306 BC. In general, classical writers referred to the Britons as "Pretannoi" (in Greek) or "Britanni" (in Latin). Strabo, writing in Roman times, distinguished between the Celts and Britons. However, Roman historian Tacitus says the Britons resembled the Celts of Gaul in customs and religion.

Under Caesar the Romans conquered Celtic Gaul, and from Claudius onward the Roman empire absorbed parts of Britain. Roman local government of these regions closely mirrored pre-Roman tribal boundaries, and archaeological finds suggest native involvement in local government.

The native peoples under Roman rule became Romanised and keen to adopt Roman ways. Celtic art had already incorporated classical influences, and surviving Gallo-Roman pieces interpret classical subjects or keep faith with old traditions despite a Roman overlay.

The Roman occupation of Gaul, and to a lesser extent of Britain, led to Roman-Celtic syncretism. In the case of the continental Celts, this eventually resulted in a language shift to Vulgar Latin, while the Insular Celts retained their language.

There was also considerable cultural influence exerted by Gaul on Rome, particularly in military matters and horsemanship, as the Gauls often served in the Roman cavalry. The Romans adopted the Celtic cavalry sword, the spatha, and Epona, the Celtic horse goddess.

To the extent that sources are available, they depict a pre-Christian Iron Age Celtic social structure based formally on class and kingship, although this may only have been a particular late phase of organisation in Celtic societies. Patron-client relationships similar to those of Roman society are also described by Caesar and others in the Gaul of the 1st century BC.

In the main, the evidence is of tribes being led by kings, although some argue that there is also evidence of oligarchical republican forms of government eventually emerging in areas which had close contact with Rome. Most descriptions of Celtic societies portray them as being divided into three groups: a warrior aristocracy; an intellectual class including professions such as druid, poet, and jurist; and everyone else. In historical times, the offices of high and low kings in Ireland and Scotland were filled by election under the system of tanistry, which eventually came into conflict with the feudal principle of primogeniture in which succession goes to the first-born son.

Little is known of family structure among the Celts. Patterns of settlement varied from decentralised to urban. The popular stereotype of non-urbanised societies settled in hillforts and duns, drawn from Britain and Ireland (there are about 3,000 hill forts known in Britain) contrasts with the urban settlements present in the core Hallstatt and La Tène areas, with the many significant "oppida" of Gaul late in the first millennium BC, and with the towns of Gallia Cisalpina.

Slavery, as practised by the Celts, was very likely similar to the better documented practice in ancient Greece and Rome. Slaves were acquired from war, raids, and penal and debt servitude. Slavery was hereditary, though manumission was possible. The Old Irish and Welsh words for 'slave', "cacht" and "caeth" respectively, are cognate with Latin "captus" 'captive' suggesting that the slave trade was an early means of contact between Latin and Celtic societies. In the Middle Ages, slavery was especially prevalent in the Celtic countries. Manumissions were discouraged by law and the word for "female slave", "cumal", was used as a general unit of value in Ireland.

There are only very limited records from pre-Christian times written in Celtic languages. These are mostly inscriptions in the Roman and sometimes Greek alphabets. The Ogham script, an Early Medieval alphabet, was mostly used in early Christian times in Ireland and Scotland (but also in Wales and England), and was only used for ceremonial purposes such as inscriptions on gravestones. The available evidence is of a strong oral tradition, such as that preserved by bards in Ireland, and eventually recorded by monasteries. Celtic art also produced a great deal of intricate and beautiful metalwork, examples of which have been preserved by their distinctive burial rites.

In some regards the Atlantic Celts were conservative: for example, they still used chariots in combat long after they had been reduced to ceremonial roles by the Greeks and Romans. However, despite being outdated, Celtic chariot tactics were able to repel the invasions of Britain attempted by Julius Caesar.

According to Diodorus Siculus:

During the later Iron Age the Gauls generally wore long-sleeved shirts or tunics and long trousers (called "braccae" by the Romans). Clothes were made of wool or linen, with some silk being used by the rich. Cloaks were worn in the winter. Brooches and armlets were used, but the most famous item of jewellery was the torc, a neck collar of metal, sometimes gold. The horned Waterloo Helmet in the British Museum, which long set the standard for modern images of Celtic warriors, is in fact a unique survival, and may have been a piece for ceremonial rather than military wear.

Archaeological evidence suggests that the pre-Roman Celtic societies were linked to the network of overland trade routes that spanned Eurasia. Archaeologists have discovered large prehistoric trackways crossing bogs in Ireland and Germany. Due to their substantial nature, these are believed to have been created for wheeled transport as part of an extensive roadway system that facilitated trade. The territory held by the Celts contained tin, lead, iron, silver and gold. Celtic smiths and metalworkers created weapons and jewellery for international trade, particularly with the Romans.

The myth that the Celtic monetary system consisted of wholly barter is a common one, but is in part false. The monetary system was complex and is still not understood (much like the late Roman coinages), and due to the absence of large numbers of coin items, it is assumed that "proto-money" was used. This included bronze items made from the early La Tène period and onwards, which were often in the shape of axeheads, rings, or bells. Due to the large number of these present in some burials, it is thought they had a relatively high monetary value, and could be used for "day to day" purchases. Low-value coinages of potin, a bronze alloy with high tin content, were minted in most Celtic areas of the continent and in South-East Britain prior to the Roman conquest of these lands. Higher-value coinages, suitable for use in trade, were minted in gold, silver, and high-quality bronze. Gold coinage was much more common than silver coinage, despite being worth substantially more, as while there were around 100 mines in Southern Britain and Central France, silver was more rarely mined. This was due partly to the relative sparsity of mines and the amount of effort needed for extraction compared to the profit gained. As the Roman civilisation grew in importance and expanded its trade with the Celtic world, silver and bronze coinage became more common. This coincided with a major increase in gold production in Celtic areas to meet the Roman demand, due to the high value Romans put on the metal. The large number of gold mines in France is thought to be a major reason why Caesar invaded.

Very few reliable sources exist regarding Celtic views on gender roles, though some archaeological evidence suggests their views may have differed from those of the Greco-Roman world, which tended to be less egalitarian. Some Iron Age burials in northeastern Gaul suggest women may have had roles in warfare during the earlier "La Tène" period, but the evidence is far from conclusive. Celtic individuals buried with both female jewellery and weaponry have been found, such as the Vix Grave in northeastern Gaul, and there are questions about the gender of some individuals buried with weaponry. However, it has been suggested that the weapons indicate high social rank rather than masculinity.

Most written accounts of the Ancient Celts are from the Romans and Greeks, though it is not clear how accurate these are. Roman historians Ammianus Marcellinus and Tacitus mentioned Celtic women inciting, participating in, and leading battles. Plutarch reports that Celtic women acted as ambassadors to avoid a war among Celtic chiefdoms in the Po valley during the 4th century BC. Posidonius' anthropological comments on the Celts had common themes, primarily primitivism, extreme ferocity, cruel sacrificial practices, and the strength and courage of their women. Cassius Dio suggests there was great sexual freedom among women in Celtic Britain:

Barry Cunliffe writes that such references are "likely to be ill-observed" and meant to portray the Celts as outlandish "barbarians". Historian Lisa Bitel argues the descriptions of Celtic women warriors are not credible. She says some Roman and Greek writers wanted to show that the barbarian Celts lived in "an upside-down world [...] and a standard ingredient in such a world was the manly warrior woman".

The Greek philosopher Aristotle wrote in his "Politics" that the Celts of southeastern Europe approved of male homosexuality. Greek historian Diodorus Siculus wrote in his "Bibliotheca historica" that although Gaulish women were beautiful, the men had "little to do with them" and it was a custom for men to sleep on animal skins with two younger males. He further claimed that "the young men will offer themselves to strangers and are insulted if the offer is refused". His claim was later repeated by Greco-Roman writers Athenaeus and Ammianus. David Rankin, in "Celts and the Classical World", suggests some of these claims refer to bonding rituals in warrior groups, which required abstinence from women at certain times, and says it probably reflects "the warlike character of early contacts between the Celts and the Greeks".

Under Brehon Law, which was written down in early Medieval Ireland after conversion to Christianity, a woman had the right to divorce her husband and gain his property if he was unable to perform his marital duties due to impotence, obesity, homosexual inclination or preference for other women.

Celtic art is generally used by art historians to refer to art of the La Tène period across Europe, while the Early Medieval art of Britain and Ireland, that is what "Celtic art" evokes for much of the general public, is called Insular art in art history. Both styles absorbed considerable influences from non-Celtic sources, but retained a preference for geometrical decoration over figurative subjects, which are often extremely stylised when they do appear; narrative scenes only appear under outside influence. Energetic circular forms, triskeles and spirals are characteristic. Much of the surviving material is in precious metal, which no doubt gives a very unrepresentative picture, but apart from Pictish stones and the Insular high crosses, large monumental sculpture, even with decorative carving, is very rare; possibly it was originally common in wood. Celts were also able to create developed musical instruments such as the carnyces, these famous war trumpets used before the battle to frighten the enemy, as the best preserved found in Tintignac (Gaul) in 2004 and which were decorated with a boar head or a snake head.

The interlace patterns that are often regarded as typical of "Celtic art" were characteristic of the whole of the British Isles, a style referred to as Insular art, or Hiberno-Saxon art. This artistic style incorporated elements of La Tène, Late Roman, and, most importantly, animal Style II of Germanic Migration Period art. The style was taken up with great skill and enthusiasm by Celtic artists in metalwork and illuminated manuscripts. Equally, the forms used for the finest Insular art were all adopted from the Roman world: Gospel books like the Book of Kells and Book of Lindisfarne, chalices like the Ardagh Chalice and Derrynaflan Chalice, and penannular brooches like the Tara Brooch and Roscrea Brooch. These works are from the period of peak achievement of Insular art, which lasted from the 7th to the 9th centuries, before the Viking attacks sharply set back cultural life.

In contrast the less well known but often spectacular art of the richest earlier Continental Celts, before they were conquered by the Romans, often adopted elements of Roman, Greek and other "foreign" styles (and possibly used imported craftsmen) to decorate objects that were distinctively Celtic. After the Roman conquests, some Celtic elements remained in popular art, especially Ancient Roman pottery, of which Gaul was actually the largest producer, mostly in Italian styles, but also producing work in local taste, including figurines of deities and wares painted with animals and other subjects in highly formalised styles. Roman Britain also took more interest in enamel than most of the Empire, and its development of champlevé technique was probably important to the later Medieval art of the whole of Europe, of which the energy and freedom of Insular decoration was an important element. Rising nationalism brought Celtic revivals from the 19th century.

The Coligny calendar, which was found in 1897 in Coligny, Ain, was engraved on a bronze tablet, preserved in 73 fragments, that originally was wide and high (Lambert p. 111). Based on the style of lettering and the accompanying objects, it probably dates to the end of the 2nd century. It is written in Latin inscriptional capitals, and is in Gaulish. The restored tablet contains 16 vertical columns, with 62 months distributed over 5 years.

French archaeologist J. Monard speculated that it was recorded by druids wishing to preserve their tradition of timekeeping in a time when the Julian calendar was imposed throughout the Roman Empire. However, the general form of the calendar suggests the public peg calendars (or "parapegmata") found throughout the Greek and Roman world.

Tribal warfare appears to have been a regular feature of Celtic societies. While epic literature depicts this as more of a sport focused on raids and hunting rather than organised territorial conquest, the historical record is more of tribes using warfare to exert political control and harass rivals, for economic advantage, and in some instances to conquer territory.

The Celts were described by classical writers such as Strabo, Livy, Pausanias, and Florus as fighting like "wild beasts", and as hordes. Dionysius said that their Such descriptions have been challenged.

Polybius (2.33) indicates that the principal Celtic weapon was a long bladed sword which was used for hacking edgewise rather than stabbing. Celtic warriors are described by Polybius and Plutarch as frequently having to cease fighting in order to straighten their sword blades. This claim has been questioned by some archaeologists, who note that Noric steel, steel produced in Celtic Noricum, was famous in the Roman Empire period and was used to equip the Roman military. However, Radomir Pleiner, in "The Celtic Sword" (1993) argues that "the metallographic evidence shows that Polybius was right up to a point", as around one third of surviving swords from the period might well have behaved as he describes. In addition to these long bladed slashing swords, spears and specialized javelins were also used.

Polybius also asserts that certain of the Celts fought naked, "The appearance of these naked warriors was a terrifying spectacle, for they were all men of splendid physique and in the prime of life." According to Livy, this was also true of the Celts of Asia Minor.

Celts had a reputation as head hunters. Paul Jacobsthal says, "Amongst the Celts the human head was venerated above all else, since the head was to the Celt the soul, centre of the emotions as well as of life itself, a symbol of divinity and of the powers of the other-world." Writing in the first century BC, Greek historians Posidonius and Diodorus Siculus said Celtic warriors cut off the heads of enemies slain in battle, hung them from the necks of their horses, then nailed them up outside their homes. Strabo wrote in the same century that Celts embalmed the heads of their most esteemed enemies in cedar oil and put them on display. Roman historian Livy wrote that the Boii beheaded a defeated Roman general after the Battle of Silva Litana, covered his skull in gold, and used it as a ritual cup. Archaeologists have found evidence that heads were embalmed and displayed by the southern Gauls.
In another example, at the southern Gaulish site of Entremont, there stood a pillar carved with skulls, within which were niches where human skulls were kept, nailed into position. Roquepertuse nearby has similar carved heads and skull niches. Many lone carved heads have been found in Celtic regions, some with two or three faces. Examples include the Mšecké Žehrovice Head and the Corleck Head.

Severed heads are a common motif in Insular Celtic myths, and there are many tales in which 'living heads' preside over feasts or speak prophecies. The beheading game is a motif in Irish myth and Arthurian legend, most famously in the tale "Sir Gawain and the Green Knight", where the Green Knight picks up his own severed head after Gawain has struck it off. There are also many legends in Celtic regions of saints who carry their own severed heads. In Irish myth, the severed heads of warriors are called the mast or nuts of the goddess Macha.

Like other European Iron Age societies, the Celts practised a polytheistic religion and believed in an afterlife. Celtic religion varied by region and over time, but had "broad structural similarities", and there was "a basic religious homogeneity" among the Celtic peoples. Because the ancient Celts did not have writing, evidence about their religion is gleaned from archaeology, Greco-Roman accounts, and literature from the early Christian period.

The names of over two hundred Celtic deities have survived (see list of Celtic deities), although it is likely that many of these were alternative names, regional names or titles for the same deity. Some deities were venerated only in one region, but others were more widely known. According to Miranda Aldhouse-Green, the Celts were also animists, believing that every part of the natural world had a spirit.

The Celts seem to have had a father god, who was often a god of the tribe and of the dead (Toutatis probably being one name for him); and a mother goddess who was associated with the land, earth and fertility (Dea Matrona probably being one name for her). The mother goddess could also take the form of a war goddess as protectress of her tribe and its land. There also seems to have been a male celestial god—identified with Taranis—associated with thunder, the wheel, and the bull. There were gods of skill and craft, such as the pan-regional god Lugus, and the smith god Gobannos. Celtic healing deities were often associated with sacred springs, such as Sirona and Borvo. Other pan-regional deities include the horned god Cernunnos, the horse and fertility goddess Epona, the divine son Maponos, as well as Belenos, Ogmios, and Sucellos. Caesar says the Gauls believed they all descended from a god of the dead and underworld. Triplicity is a common theme in Celtic cosmology, and a number of deities were seen as threefold, for example the Three Mothers.

Celtic religious ceremonies were overseen by priests known as druids, who also served as judges, teachers, and lore-keepers. Other classes of druids performed sacrifices for the perceived benefit of the community. There is evidence that ancient Celtic peoples sacrificed animals, almost always livestock or working animals. It appears some were offered wholly to the gods (by burying or burning), while some were shared between gods and humans (part eaten and part offered). There is also some evidence that ancient Celts sacrificed humans, and some Greco-Roman sources claim the Gauls sacrificed criminals by burning them in a wicker man.

The Romans said the Celts held ceremonies in sacred groves and other natural shrines, called nemetons. Some Celtic peoples built temples or ritual enclosures of varying shapes (such as the Romano-Celtic temple and viereckschanze), though they also maintained shrines at natural sites. Celtic peoples often made votive offerings: treasured items deposited in water and wetlands, or in ritual shafts and wells, often in the same place over generations. Modern clootie wells might be a continuation of this.

Most surviving Celtic mythology belongs to the Insular Celtic peoples: Irish mythology has the largest written body of myths, followed by Welsh mythology. These were written down in the early Middle Ages, mainly by Christian scribes.

The supernatural race called the Tuatha Dé Danann are believed to represent the main Celtic gods of Ireland. Their traditional rivals are the Fomóire, whom they defeat in the "Battle of Mag Tuired". Barry Cunliffe says the underlying structure in Irish myth was a dualism between the male tribal god and the female goddess of the land. The Dagda seems to have been the chief god and the Morrígan his consort, each of whom had other names. One common motif is the sovereignty goddess, who represents the land and bestows sovereignty on a king by marrying him. The goddess Brigid was linked with nature as well as poetry, healing and smithing.

Some figures in medieval Insular Celtic myth have ancient continental parallels: Irish Lugh and Welsh Lleu are cognate with Lugus, Goibniu and Gofannon with Gobannos, Macán and Mabon with Maponos, while Macha and Rhiannon may be counterparts of Epona.

In Insular Celtic myth, the Otherworld is a parallel realm where the gods dwell. Some mythical heroes visit it by entering ancient burial mounds or caves, by going under water or across the western sea, or after being offered a silver apple branch by an Otherworld resident. Irish myth says that the spirits of the dead travel to the house of Donn ("Tech Duinn"), a legendary ancestor; this echoes Caesar's comment that the Gauls believed they all descended from a god of the dead and underworld.

Insular Celtic peoples celebrated four seasonal festivals, known to the Gaels as Beltaine (1 May), Lughnasa (1 August), Samhain (1 November) and Imbolc (1 February).

The Roman invasion of Gaul brought a great deal of Celtic peoples into the Roman Empire. Roman culture had a profound effect on the Celtic tribes which came under the empire's control. Roman influence led to many changes in Celtic religion, the most noticeable of which was the weakening of the druid class, especially religiously; the druids were to eventually disappear altogether. Romano-Celtic deities also began to appear: these deities often had both Roman and Celtic attributes, combined the names of Roman and Celtic deities, or included couples with one Roman and one Celtic deity. Other changes included the adaptation of the Jupiter Column, a sacred column set up in many Celtic regions of the empire, primarily in northern and eastern Gaul. Another major change in religious practice was the use of stone monuments to represent gods and goddesses. The Celts had probably only created wooden cult images (including monuments carved into trees, which were known as sacred poles) before the Roman conquest.

While the regions under Roman rule adopted Christianity along with the rest of the Roman empire, unconquered areas of Ireland and Scotland began to move from Celtic polytheism to Christianity in the 5th century. Ireland was converted by missionaries from Britain, such as Saint Patrick. Later missionaries from Ireland were a major source of missionary work in Scotland, Anglo-Saxon parts of Britain, and central Europe (see Hiberno-Scottish mission). Celtic Christianity, the forms of Christianity that took hold in Britain and Ireland at this time, had for some centuries only limited and intermittent contact with Rome and continental Christianity, as well as some contacts with Coptic Christianity. Some elements of Celtic Christianity developed, or retained, features that made them distinct from the rest of Western Christianity, most famously their conservative method of calculating the date of Easter. In 664, the Synod of Whitby began to resolve these differences, mostly by adopting the current Roman practices, which the Gregorian Mission from Rome had introduced to Anglo-Saxon England.

Genetic studies on the limited amount of material available suggest continuity between Iron Age people from areas considered Celtic and the earlier Bell Beaker culture of Bronze Age Western Europe. Like the Bell Beakers, ancient Celts carried a substantial amount of Western Steppe Herders ancestry, which is derived from Yamnaya pastoralists who expanded westwards from the Pontic–Caspian steppe during the late Neolithic and the early Bronze Age and associated with the initial spread of Indo-European languages. This ancestry was particularly prevalent among Celts of Northwest Europe. Examined individuals overwhelmingly carry types of the paternal haplogroup R-M269, while the maternal haplogroups H and U are frequent. These lineages are associated with steppe ancestry. The spread of Celts into Iberia and the emergence of the Celtiberians is associated with an increase in north-central European ancestry in Iberia, and may be connected to the expansion of the Urnfield culture. The paternal haplogroup haplogroup I2a1a1a has been detected among Celtiberians. There appears to have been significant gene flow among Celtic peoples of Western Europe during the Iron Age. While the Gauls of southern France display genetic links with the Celtiberians, the Gauls of northern France display links with Great Britain and Sweden. Modern populations of Western Europe, particularly those who still speak Celtic languages, display substantial genetic continuity with the Iron Age populations of the same areas.



Geography

Organisations

Conductor

Conductor or conduction may refer to:





Claude Monet

Oscar-Claude Monet (, , ; 14 November 1840 – 5 December 1926) was a French painter and founder of impressionist painting who is seen as a key precursor to modernism, especially in his attempts to paint nature as he perceived it. During his long career, he was the most consistent and prolific practitioner of impressionism's philosophy of expressing one's perceptions of nature, especially as applied to "plein air" (outdoor) landscape painting. The term "Impressionism" is derived from the title of his painting "Impression, soleil levant", exhibited in 1874 (the "exhibition of rejects") initiated by Monet and his associates as an alternative to the Salon.

Monet was raised in Le Havre, Normandy, and became interested in the outdoors and drawing from an early age. Although his mother, Louise-Justine Aubrée Monet, supported his ambitions to be a painter, his father, Claude-Adolphe, disapproved and wanted him to pursue a career in business. He was very close to his mother, but she died in January 1857 when he was sixteen years old, and he was sent to live with his childless, widowed but wealthy aunt, Marie-Jeanne Lecadre. He went on to study at the Académie Suisse, and under the academic history painter Charles Gleyre, where he was a classmate of Auguste Renoir. His early works include landscapes, seascapes, and portraits, but attracted little attention. A key early influence was Eugène Boudin who introduced him to the concept of "plein air" painting. From 1883, Monet lived in Giverny, also in northern France, where he purchased a house and property and began a vast landscaping project, including a water-lily pond.

Monet's ambition to document the French countryside led to a method of painting the same scene many times so as to capture the changing of light and the passing of the seasons. Among the best-known examples are his series of haystacks (1890–1891), paintings of Rouen Cathedral (1892–1894), and the paintings of water lilies in his garden in Giverny that occupied him continuously for the last 20 years of his life.

Frequently exhibited and successful during his lifetime, Monet's fame and popularity soared in the second half of the 20th century when he became one of the world's most famous painters and a source of inspiration for a burgeoning group of artists.

Claude Monet was born on 14 November 1840 on the fifth floor of 45 rue Laffitte, in the 9th arrondissement of Paris. He was the second son of Claude Adolphe Monet and Louise Justine Aubrée Monet, both of them second-generation Parisians. On 20 May 1841, he was baptised in the local Paris church, Notre-Dame-de-Lorette, as Oscar-Claude, but his parents called him simply Oscar. Despite being baptised Catholic, Monet later became an atheist.

In 1845, his family moved to Le Havre in Normandy. His father, a wholesale merchant, wanted him to go into the family's ship-chandling and grocery business, but Monet wanted to become an artist. His mother was a singer, and supported Monet's desire for a career in art.

On 1 April 1851, he entered Le Havre secondary school of the arts. He was an apathetic student who, after showing skill in art from young age, began drawing caricatures and portraits of acquaintances at age 15 for money. He began his first drawing lessons from Jacques-François Ochard, a former student of Jacques-Louis David. In around 1858, he met fellow artist Eugène Boudin, who would encourage Monet to develop his techniques, teach him the "en plein air" (outdoor) techniques for painting and take Monet on painting excursions. Monet thought of Boudin as his master, whom "he owed everything to" for his later success.

In 1857, his mother died. He lived with his father and aunt, Marie-Jeanne Lecadre; Lecadre would be a source of support for Monet in his early art career.

From 1858 to 1860, Monet continued his studies in Paris, where he enrolled in Académie Suisse and met Camille Pissarro in 1859. He was called for military service and served under the Chasseurs d'Afrique (African Hunters), in Algeria, from 1861 to 1862. His time in Algeria had a powerful effect on Monet, who later said that the light and vivid colours of North Africa "contained the gem of my future researches". Illness forced his return to Le Havre, where he bought out his remaining service and met Johan Barthold Jongkind, who together with Boudin was an important mentor to Monet.
Upon his return to Paris, with the permission of his father, he divided his time between his childhood home and the countryside and enrolled in Charles Gleyre's studio, where he met Pierre-Auguste Renoir and Frédéric Bazille. Bazille eventually became his closest friend. In search of motifs, they traveled to Honfleur where Monet painted several "studies" of the harbor and the mouth of the Seine. Monet often painted alongside Renoir and Alfred Sisley, both of whom shared his desire to articulate new standards of beauty in conventional subjects.

During this time he painted "Women in Garden", his first successful large-scale painting, and , the "most important painting of Monet's early period". Having debuted at the Salon in 1865 with "La Pointe de la Hève at Low Tide" and "Mouth of the Seine at Honfleur" to large praise, he hoped would help him break through into the Salon of 1866. He could not finish it in a timely manner and instead submitted "The Woman in the Green Dress" and "Pavé de Chailly" to acceptance. Thereafter, he submitted works to the Salon annually until 1870, but they were accepted by the juries only twice, in 1866 and 1868. He sent no more works to the Salon until his single, final attempt in 1880. His work was considered radical, "discouraged at all official levels".

In 1867 his then-mistress, Camille Doncieux—whom he had met two years earlier as a model for his paintings—gave birth to their first child, Jean. Monet had a strong relationship with Jean, claiming that Camille was his lawful wife so Jean would be considered legitimate. Monet's father stopped financially supporting him as a result of the relationship. Earlier in the year Monet had been forced to move to his aunt's house in Sainte-Adresse. There he immersed himself in his work, although a temporary problem with his eyesight, probably related to stress, prevented him from working in sunlight. Monet loved his family dearly, painting many portraits of them such as "child with a cup, a portrait of Jean Monet". This painting in particular shows the first signs of Monets' later famous impressionistic work.

With help from the art collector Louis-Joachim Gaudibert, he reunited with Camille and moved to Étretat the following year. Around this time, he was trying to establish himself as a figure painter who depicted the "explicitly contemporary, bourgeois", an intention that continued into the 1870s. He did evolve his painting technique and integrate stylistic experimentation in his plein-air style—as evidenced by "The Beach at Sainte-Adresse" and "On the Bank of the Seine" respectively, the former being his "first sustained campaign of painting that involved tourism".

Several of his paintings had been purchased by Gaudibert, who commissioned a painting of his wife, alongside other projects; the Gaudiberts were for two years "the most supportive of Monet's hometown patrons". Monet would later be financially supported by the artist and art collector Gustave Caillebotte, Bazille and perhaps Gustave Courbet, although creditors still pursued him.

He married Camille on 28 June 1870, just before the outbreak of the Franco-Prussian War. During the war, he and his family lived in London and the Netherlands to avoid conscription. Monet and Charles-François Daubigny lived in self-imposed exile. While living in London, Monet met his old friend Pissarro, the American painter James Abbott McNeill Whistler, and befriended his first and primary art dealer Paul Durand-Ruel; an encounter that would be decisive for his career. There he saw and admired the works of John Constable and J. M. W. Turner and was impressed by Turner's treatment of light, especially in the works depicting the fog on the Thames. He repeatedly painted the Thames, Hyde Park and Green Park. In the spring of 1871, his works were refused authorisation for inclusion in the Royal Academy exhibition and police suspected him of revolutionary activities. That same year he learned of his father's death.

The family moved to Argenteuil in 1871, where he, influenced by his time with Dutch painters, mostly painted the Seine's surrounding area. He acquired a sailboat to paint on the river. In 1874, he signed a six-and-a-half year lease and moved into a newly built "rose-colored house with green shutters" in Argenteuil, where he painted fifteen paintings of his garden from a panoramic perspective. Paintings such as "Gladioli" marked what was likely the first time Monet had cultivated a garden for the purpose of his art. The house and garden became the "single most important" motif of his final years in Argenteuil. For the next four years, he painted mostly in Argenteuil and took an interest in the colour theories of chemist Michel Eugène Chevreul. For three years of the decade, he rented a large villa in Saint-Denis for a thousand francs per year. "Camille Monet on a Garden Bench" displays the garden of the villa, and what some have argued to be Camille's grief upon learning of her father's death.

Monet and Camille were often in financial straits during this period—they were unable to pay their hotel bill during the summer of 1870 and likely lived on the outskirts of London as a result of insufficient funds. An inheritance from his father, together with sales of his paintings, did, however, enable them to hire two servants and a gardener by 1872. Following the successful exhibition of some maritime paintings and the winning of a silver medal at Le Havre, Monet's paintings were seized by creditors, from whom they were bought back by a shipping merchant, Gaudibert, who was also a patron of Boudin.

When Durand-Ruel's previous support of Monet and his peers began to decline, Monet, Renoir, Pissarro, Sisley, Paul Cézanne, Edgar Degas, and Berthe Morisot exhibited their work independently; they did so under the name the Anonymous Society of Painters, Sculptors and Engravers for which Monet was a leading figure in its formation. He was inspired by the style and subject matter of his slightly older contemporaries, Pissarro and Édouard Manet. The group, whose title was chosen to avoid association with any style or movement, were unified in their independence from the Salon and rejection of the prevailing academicism. Monet gained a reputation as the foremost landscape painter of the group.

At the first exhibition, in 1874, Monet displayed, among others, "Impression, Sunrise", "The Luncheon" and "Boulevard des Capucines". The art critic Louis Leroy wrote a hostile review. Taking particular notice of "Impression, Sunrise" (1872), a hazy depiction of Le Havre port and stylistic detour, he coined the term "Impressionism". Conservative critics and the public derided the group, with the term initially being ironic and denoting the painting as unfinished. More progressive critics praised the depiction of modern life—Louis Edmond Duranty called their style a "revolution in painting". Leroy later regretted inspiring the name, as he believed that they were a group "whose majority had nothing impressionist".

The total attendance is estimated at 3500. Monet priced "Impression: Sunrise" at 1000 francs but failed to sell it. The exhibition was open to anyone prepared to pay 60 francs and gave artists the opportunity to show their work without the interference of a jury. Another exhibition was held in 1876, again in opposition to the Salon. Monet displayed 18 paintings, including "The Beach at Sainte-Adresse" which showcased multiple Impressionist characteristics.

For the third exhibition, on 5 April 1877, he selected seven paintings from the dozen he had made of Gare Saint-Lazare in the past three months, the first time he had "synced as many paintings of the same site, carefully coordinating their scenes and temporalities". The paintings were well received by critics, who especially praised the way he captured the arrival and departures of the trains. By the fourth exhibition his involvement was by means of negotiation on Caillebotte's part. His last time exhibiting with the Impressionists was in 1882—four years before the final Impressionist exhibition.

Monet, Renoir, Pissarro, Morisot, Cézanne and Sisley proceeded to experiment with new methods of depicting reality. They rejected the dark, contrasting lighting of romantic and realist paintings, in favour of the pale tones of their peers' paintings such as those by Jean-Baptiste-Camille Corot and Boudin. After developing methods for painting transient effects, Monet would go on to seek more demanding subjects, new patrons and collectors; his paintings produced in the early 1870s left a lasting impact on the movement and his peers—many of whom moved to Argenteuil as a result of admiring his depiction.

In 1875, he returned to figure painting with "Woman with a Parasol - Madame Monet and Her Son," after effectively abandoning it with "The Luncheon". His interest in the figure continued for the next four years—reaching its crest in 1877 and concluding altogether in 1890. In an "unusually revealing" letter to Théodore Duret, Monet discussed his revitalised interest: "I am working like never before on a new endeavour figures in plein air, as I understand them. This is an old dream, one that has always obsessed me and that I would like to master once and for all. But it is all so difficult! I am working very hard, almost to the point of making myself ill".

In 1876, Camille Monet became seriously ill. Their second son, Michel, was born in 1878, after which Camille's health deteriorated further. In the autumn of that year, they moved to the village of Vétheuil where they shared a house with the family of Ernest Hoschedé, a wealthy department store owner and patron of the arts who had commissioned four paintings from Monet. In 1878, Camille was diagnosed with uterine cancer. She died the next year. Her death, alongside financial difficulties—once having to leave his house to avoid creditors—afflicted Monet's career; Hoschedé had recently purchased several paintings but soon went bankrupt, leaving for Paris in hopes of regaining his fortune, as interest in the Impressionists dwindled.
Monet made a study in oils of his late wife. Many years later, he confessed to his friend Georges Clemenceau that his need to analyse colours was both a joy and a torment to him. He explained: "I one day found myself looking at my beloved wife's dead face and just systematically noting the colours according to an automatic reflex". John Berger describes the work as "a blizzard of white, grey, purplish paint ... a terrible blizzard of loss which will forever efface her features. In fact there can be very few death-bed paintings which have been so intensely felt or subjectively expressive."

Monet's study of the Seine continued. He submitted two paintings to the Salon in 1880, one of which was accepted. He began to abandon Impressionist techniques as his paintings utilised darker tones and displayed environments, such as the Seine river, in harsh weather. For the rest of the decade, he focused on the elemental aspect of nature. His personal life influenced his distancing from the Impressionists. He returned to Étretat and expressed in letters to Alice Hoschedé—who he would marry in 1892, following her husband's death the preceding year—a desire to die. In 1881, he moved with Alice and her children to Poissy and again sold his paintings to Durand-Ruel. Alice's third daughter, Suzanne, would become Monet's "preferred model", after Camille.

In April 1883, looking out the window of the train between Vernon and Gasny, he discovered Giverny in Normandy. That same year his first major retrospective show was held.

Monet's struggles with creditors ended following prosperous trips; he went to Bordighera in 1884, and brought back 50 landscapes. He travelled to the Netherlands in 1886 to paint the tulips. He soon met and became friends with Gustave Geffroy, who published an article on Monet. Despite his qualms, Monet's paintings were sold in America and contributed towards his financial security. In contrast to the last two decades of his career, Monet favoured working alone—and felt that he was always better when he did, having regularly "long[ed] for solitude, away from crowded tourist resorts and sophisticated urban settings". Such a desire was recurrent in his letters to Alice.

In 1883, Monet and his family rented a house and gardens in Giverny, which provided him domestic stability he had not yet enjoyed. The house was situated near the main road between the towns of Vernon and Gasny at Giverny. There was a barn that doubled as a painting studio, orchards and a small garden. The house was close enough to the local schools for the children to attend, and the surrounding landscape provided numerous natural areas for Monet to paint.

The family worked and built up the gardens, and Monet's fortunes began to change for the better as Durand-Ruel had increasing success in selling his paintings. The gardens were Monet's greatest source of inspiration for 40 years. In 1890, Monet purchased the house. During the 1890s, Monet built a greenhouse and a second studio, a spacious building well lit with skylights.

Monet wrote daily instructions to his gardener, precise designs and layouts for plantings, and invoices for his floral purchases and his collection of botany books. As Monet's wealth grew, his garden evolved. He remained its architect, even after he hired seven gardeners. Monet purchased additional land with a water meadow. White water lilies local to France were planted along with imported cultivars from South America and Egypt, resulting in a range of colours including yellow, blue and white lilies that turned pink with age. In 1902, he increased the size of his water garden by nearly 4000 square metres; the pond was enlarged in 1901 and 1910 with easels installed all around to allow different perspectives to be captured.

Dissatisfied with the limitations of Impressionism, Monet began to work on series of paintings displaying single subjects—haystacks, poplars and the Rouen Cathedral—to resolve his frustration. These series of paintings provided widespread critical and financial success; in 1898, 61 paintings were exhibited at the Petit gallery. He also begun a series of "Mornings on the Seine," which portrayed the dawn hours of the river."" In 1887 and 1889 he displayed a series of paintings of Belle Île to rave reviews by critics. Monet chose the location in the hope of finding a "new aesthetic language that bypassed learned formulas, one that would be both true to nature and unique to him as an individual, not like anyone else."
In 1899, he began painting the water lilies that would occupy him continuously for the next 20 years of his life, being his last and "most ambitious" sequence of paintings. He had exhibited this first group of pictures of the garden, devoted primarily to his Japanese bridge, in 1900. He returned to London—now residing at the prestigious Savoy Hotel—in 1899 to produce a series that included 41 paintings of Waterloo bridge, 34 of Charing Cross bridge and 19 of the House of Parliament. Monet's final journey would be to Venice, with Alice in 1908.

Depictions of the water lilies, with alternating light and mirror-like reflections, became an integral part of his work. By the mid-1910s Monet had achieved "a completely new, fluid, and somewhat audacious style of painting in which the water-lily pond became the point of departure for an almost abstract art". Claude Roger-Marx noted in a review of Monet's successful 1909 exhibition of the first "Water Lilies" series that he had "reached the ultimate degree of abstraction and imagination joined to the real". This exhibition, entitled "Waterlilies, a Series of Waterscape," consisted of 42 canvases, his "largest and most unified series to date""." He would ultimately make over 250 paintings of the "Waterlilies".

At his house, Monet met with artists, writers, intellectuals and politicians from France, England, Japan and the United States. In the summer of 1887 he met John Singer Sargent whose experimentation with figure painting out of doors intrigued him; the pair went on to frequently influence each other.

Monet's second wife, Alice, died in 1911, and his oldest son, Jean, who had married Alice's daughter, Blanche, Monet's particular favourite, died in 1914. Their deaths left Monet depressed, as Blanche cared for him. It was during this time that Monet began to develop the first signs of possible cataracts. In 1913, Monet travelled to London to consult the German ophthalmologist Richard Liebreich. He was prescribed new glasses and rejected cataract surgery for the right eye. The next year, Monet, encouraged by Clemenceau, made plans to construct a new, large studio that he could use to create a "decorative cycle of paintings devoted to the water garden".

In the following years, his perception of colour suffered; his broad strokes were broader and his paintings were increasingly darker. To achieve his desired outcome, he began to label his tubes of paint, kept a strict order on his palette and wore a straw hat to negate glare. He approached painting by formulating the ideas and features in his mind, taking the "motif in large masses" and transcribing them through memory and imagination. This was due to him being "insensitive" to the "finer shades of tonalities and colors seen close up".

Monet's output decreased as he became withdrawn, although he did produce several panel paintings for the French Government, from 1914 to 1918 to great financial success and he would later create works for the state. His work on the "cycle of paintings" mostly occurred around 1916 to 1921. Cataract surgery was once again recommended, this time by Clemenceau. Monet—who was apprehensive, following Honoré Daumier and Mary Cassatt's botched surgeries—stated that he would rather have poor sight and perhaps abandon painting than forego "a little of these things that I love". In 1919, Monet began a series of landscape paintings, "in full force" although he was not pleased with the outcome. By October the weather caused Monet to cease plein air painting and the next month he sold four of the eleven "Water Lilies" paintings, despite his then-reluctance to relinquish his work. The series inspired praise from his peers; his later works were well received by dealers and collectors, and he received 200,000 francs from one collector.

In 1922 a prescription of mydriatics provided short-lived relief. He eventually underwent cataract surgery in 1923. Persistent cyanopsia and aphakic spectacles proved to be a struggle. Now "able to see the real colours", he began to destroy canvases from his pre-operative period. Upon receiving tinted Zeiss lenses, Monet was laudatory, although his left eye soon had to be entirely covered by a black lens. By 1925, his visual impairment was improved and he began to retouch some of his pre-operative works, with bluer water lilies than before.

During World War I, in which his younger son, Michel, served, Monet painted a "Weeping Willow" series as homage to the French fallen soldiers. He became deeply dedicated to the decorations of his garden during the war.

Monet has been described as "the driving force behind Impressionism". Crucial to the art of the Impressionist painters was the understanding of the effects of light on the local colour of objects, and the effects of the juxtaposition of colours with each other. His free flowing style and use of colour have been described as "almost ethereal" and the "[epitome] of impressionist style"; "Impression, Sunrise" is an example of the "fundamental" Impressionist principle of depicting only that which is purely visible. Monet was fascinated with the effects of light, and painting en plein air—he believed that his only "merit lies in having painted directly in front of nature, seeking to render my impressions of the most fleeting effects" Wanting to "paint the air", he often combined modern life subjects in outdoor light.
Monet made light the central focus of his paintings. To capture its variations, he would sometimes complete a painting in one sitting, often without preparation. He wished to demonstrate how light altered colour and perception of reality. His interest in light and reflection began in the late 1860s and lasted throughout his career. During his first time in London, he developed an admiration for the relationship between the artist and motifs—for what he deemed the "envelope". He utilised pencil drawings to quickly note subjects and motifs for future reference.

Monet's portrayal of landscapes emphasised industrial elements such as railways and factories; his early seascapes featured brooding nature depicted with muted colours and local residents. Critic, and friend of Monet, Théodore Duret noted, in 1874, that he was "little attracted by rustic scenes...He [felt] particularly drawn towards nature when it is embellished and towards urban scenes and for preference he paint[ed] flowery gardens, parks and groves." When depicting figures and landscapes in tandem, Monet wished for the landscape to not be a mere backdrop and the figures not to be dominate the composition. His dedication to such a portrayal of landscapes resulted in Monet reprimanding Renoir for defying it. He often depicted the suburban and rural leisure activities of Paris and as a young artist experimented with still lifes. From the 1870s onwards, he gradually moved away from suburban and urban landscapes—when they were depicted it was to further his study of light. Contemporary critics—and later academics—felt that with his choice of showcasing Belle Île, he had indicated a desire to move away from the modern culture of Impressionist paintings and instead towards primitive nature.

After meeting Boudin, Monet dedicated himself to searching for new and improved methods of painterly expression. To this end, as a young man, he visited the Salon and familiarised himself with the works of older painters, and made friends with other young artists. The five years that he spent at Argenteuil, spending much time on the River Seine in a little floating studio, were formative in his study of the effects of light and reflections. He began to think in terms of colours and shapes rather than scenes and objects. He used bright colours in dabs and dashes and squiggles of paint. Having rejected the academic teachings of Gleyre's studio, he freed himself from theory, saying "I like to paint as a bird sings." Boudin, Daubigny, Jongkind, Courbet, and Corot were among Monet's influences and he would often work in accordance with developments in avant-garde art.

In 1877 a series of paintings at St-Lazare Station had Monet looking at smoke and steam and the way that they affected colour and visibility, being sometimes opaque and sometimes translucent. He was to further use this study in the painting of the effects of mist and rain on the landscape. The study of the effects of atmosphere was to evolve into a number of series of paintings in which Monet repeatedly painted the same subject (such as his water lilies series) in different lights, at different hours of the day, and through the changes of weather and season. This process began in the 1880s and continued until the end of his life in 1926. In his later career, Monet "transcended" the Impressionist style and begun to push the boundaries of art.
Monet refined his palette in the 1870s, consciously minimising the use of darker tones and favouring pastel colours. This coincided with his softer approach, using smaller and more varied brush strokes. His palette would again undergo change in the 1880s, with more emphasis than before on harmony between warm and cold hues. Following his optical operation in 1923, Monet returned to his style from before a decade ago. He forwent garish colours or "coarse application" for emphasised colour schemes of blue and green. Whilst suffering from cataracts, his paintings were more broad and abstract—from the late 1880s onwards, he had simplified his compositions and sought subjects which could offer broad colour and tone. He increasingly used red and yellow tones, a trend that first started following his trip to Venice. Monet often travelled alone at this time—from France to Normandy to London; to the Rivera and Rouen—in search of new and more challenging subjects.The stylistic change was likely a by-product of the disorder and not an intentional choice. Monet would often work on large canvases due to the deterioration of his eyesight and by 1920 he admitted that he had grown too accustomed to broad painting to return to small canvases. The influence of his cataracts on his output has been a topic of discussion among academics; Lane et al. (1997) argues the occurrence of a deterioration from the late 1860s onwards led to a diminishing of sharp lines. Gardens were a focus throughout his art, becoming prominent in his later work, especially during the last decade of his life. Daniel Wildenstein noted a "seamless" continuity in his paintings that was "enriched by innovation".

From the 1880s onwards—and particularly in the 1890s—Monet's series of paintings of specific subjects sought to document the different conditions of light and weather. As light and weather changed throughout the day, he switched between canvases—sometimes working on as many as eight at one time—usually spending an hour on each. In 1895, he exhibited 20 paintings of "Rouen Cathedral," showcasing the façade in different conditions of light, weather and atmosphere. The paintings do not focus on the grand Medieval building, but on the play of light and shade across its surface, transforming the solid masonry. For this series, he experimented with creating his own frames.

His first series exhibited was of haystacks, painted from different points of view and at different times of the day. Fifteen of the paintings were exhibited at the Galerie Durand-Ruel in 1891. In 1892 he produced twenty-six views of "Rouen Cathedral". Between 1883 and 1908, Monet travelled to the Mediterranean, where he painted landmarks, landscapes, and seascapes, including a series of paintings in Venice. In London he painted four series: "the Houses of Parliament, London", "Charing Cross Bridge", "Waterloo Bridge", and "Views of Westminster Bridge". Helen Gardner writes:

Following his return from London, Monet painted mostly from nature, in his own garden; its water lilies, its pond and its bridge. From 22 November to 15 December 1900, another exhibition dedicated to him was held at the Durand-Ruel gallery, with around ten versions of the "Water Lilies" exhibited. This same exhibition was organized in February 1901 in New York City, where it was met with great success.

In 1901, Monet enlarged the pond of his home by buying a meadow located on the other side of the Ru, the local watercourse. He then divided his time between work on nature and work in his studio.

The canvases dedicated to the water lilies evolved with the changes made to his garden. In addition, around 1905, Monet gradually modified his aesthetics by abandoning the perimeter of the body of water and therefore modifying perspective. He also changed the shape and size of his canvases by moving from rectangular stretchers to square and then circular stretchers.

These canvases were created with great difficulty: Monet spent a significant amount of time reworking them in order to find the perfect effects and impressions. When he deemed them unsuccessful he did not hesitate to destroy them. He continually postponed the Durand-Ruel exhibition until he was satisfied with the works. After several postponements dating back to 1906, the exhibition titled "Les Nymphéas" ended up opening on 6 May 1909. Comprising forty-eight paintings dating from 1903 to 1908, representing a series of landscapes and water lily scenes, this exhibition was once again a success.

Monet died of lung cancer on 5 December 1926 at the age of 86 and is buried in the Giverny church cemetery. Monet had insisted that the occasion be simple; thus, only about fifty people attended the ceremony. At his funeral, Clemenceau removed the black cloth draped over the coffin, stating: "No black for Monet!" and replaced it with a flower-patterned cloth. At the time of his death, "Waterlilies" was "technically unfinished".

Monet's home, garden, and water lily pond were bequeathed by Michel to the French Academy of Fine Arts (part of the Institut de France) in 1966. Through the Fondation Claude Monet, the house and gardens were opened for visits in 1980, following restoration. In addition to souvenirs of Monet and other objects of his life, the house contains his collection of Japanese woodcut prints. The house and garden, along with the Museum of Impressionism, are major attractions in Giverny, which hosts tourists from all over the world.

Speaking of Monet's body of work, Wildenstein said that it is "so extensive that its very ambition and diversity challenges our understanding of its importance". His paintings produced at Giverny and under the influence of cataracts have been said to create a link between Impressionism and twentieth-century art and modern abstract art, respectively. His later works were a "major" inspiration to Objective abstraction. Ellsworth Kelly, following a formative experience at Giverny, paid homage to Monet's works created there with "Tableau Vert" (1952). Monet has been called an "intermediary" between tradition and modernism—his work has been examined in relation to postmodernism—and was an influence to Bazille, Sisley, Renoir and Pissarro. Monet is now the most famous of the Impressionists; as a result of his contributions to the movement, he "exerted a huge influence on late 19th-century art".
In May 1927, 27 panel paintings were displayed in the Musée de l'Orangerie, following lengthy negotiations with the French government. Due to his later works being ignored by artists, art historians, critics and the public few attended the showing. In the 1950s, Monet's later works were "rediscovered" by the Abstract Expressionists, and those adjacent like Clement Greenberg, who used a similar canvases and held a disinterest in the blunt and ideological art of the war. A 1952 essay by André Masson helped change the perception of the paintings and inspire appreciation that begin to take shape in 1956–1957. The next year, a fire in the Museum of Modern Art would see the "Water Lilies" paintings acquired by them burn. The large scale nature of Monet's later paintings proved to be difficult for some museums, which resulted in them altering the framing.

In 1978, Monet's garden in Giverny—which had grown decrepit over fifty years—was restored and opened to the public. In 2004, "London, the Parliament, Effects of Sun in the Fog" ("Londres, le Parlement, trouée de soleil dans le brouillard"; 1904), sold for US$20.1 million. In 2006, the journal "Proceedings of the Royal Society" published a paper providing evidence that these were painted in situ at St Thomas' Hospital over the river Thames. In 1981, Ronald Pickvance noted that Monet's works after 1880 were increasingly receiving scholarly attention.

"Falaises près de Dieppe" ("Cliffs Near Dieppe") has been stolen on two occasions: once in 1998 (in which the museum's curator was convicted of the theft and jailed for five years and two months along with two accomplices) and most recently in August 2007. It was recovered in June 2008.

On 14 November 2001, a Google Doodle was made for Claude Monet's 161st birthday, depicting the Google logo in Monet's signature style. It was the first Google Doodle made for someone's birthday.

Monet's "Le Pont du chemin de fer à Argenteuil", an 1873 painting of a railway bridge spanning the Seine near Paris, was bought by an anonymous telephone bidder for a record $41.4 million at Christie's auction in New York on 6 May 2008. The previous record for his painting stood at $36.5 million. A few weeks later, "Le bassin aux nymphéas" (from the water lilies series) sold at Christie's 24 June 2008 auction in London for £40,921,250 ($80,451,178), nearly doubling the record for the artist. This purchase represented one of the top 20 highest prices paid for a painting at the time.

In October 2013, Monet's paintings "L'Eglise de Vétheuil" and "Le Bassin aux Nympheas" became subjects of a legal case in New York against New York-based Vilma Bautista, one-time aide to Imelda Marcos, wife of dictator Ferdinand Marcos, after she sold "Le Bassin aux Nympheas" for $32 million to a Swiss buyer. The said Monet paintings, along with two others, were acquired by Imelda during her husband's presidency and allegedly bought using the nation's funds. Bautista's lawyer claimed that the aide sold the painting for Imelda but did not have a chance to give her the money. The Philippine government seeks the return of the painting. "Le Bassin aux Nympheas", also known as "Japanese Footbridge over the Water-Lily Pond at Giverny", is part of Monet's famed Water Lilies series.

Under the Nazi regime, both in Germany from 1933 and in German-occupied countries until 1945, Jewish art collectors of Monet were looted by Nazis and their agents. Several of the stolen artworks have been returned to their rightful owners, while others have been the object of court battles. In 2014, during the spectacular discovery of a hidden trove of art in Munich, a Monet that had belonged to a Jewish retail magnate was found in the suitcase of Cornelius Gurlitt, the son of one of Hitler's official art dealers of looted art, Hildebrand Gurlitt.

Examples of Nazi-looted Monet works include:

Monet's "Le Palais Ducal," and his 1880 work, "Poppy Field near Vétheui"l, formerly in the collection of Max Emden, have been the object of restitution claims. "La Mare, Snow Effect" ("La Mare, effect de neige") was the object of a settlement with the heirs of Richard Semmel.




Carthage

Carthage was an ancient city on the eastern side of the Lake of Tunis in what is now Tunisia. Carthage was one of the most important trading hubs of the Ancient Mediterranean and one of the most affluent cities of the classical world. It became the capital city of the civilisation of Ancient Carthage and later Roman Carthage.

The city developed from a Phoenician colony into the capital of a Punic empire which dominated large parts of the Southwest Mediterranean during the first millennium BC. The legendary Queen Elissa, Alyssa or Dido, originally from Tyre, is regarded as the founder of the city, though her historicity has been questioned. In the myth, Dido asked for land from a local tribe, which told her that she could get as much land as an oxhide could cover. She cut the oxhide into strips and laid out the perimeters of the new city. As Carthage prospered at home, the polity sent colonists abroad as well as magistrates to rule the colonies.

The ancient city was destroyed in the nearly-three year siege of Carthage by the Roman Republic during the Third Punic War in 146 BC. It was re-developed a century later as Roman Carthage, which became the major city of the Roman Empire in the province of Africa. The question of Carthaginian decline and demise has remained a subject of literary, political, artistic, and philosophical debates in both ancient and modern histories.

Late antique and medieval Carthage continued to play an important cultural and economic role in the Byzantine period. The city was sacked and destroyed by Umayyad forces after the Battle of Carthage in 698 to prevent it from being reconquered by the Byzantine Empire. It remained occupied during the Muslim period and was used as a fort by the Muslims until the Hafsid period when it was taken by the Crusaders with its inhabitants massacred during the Eighth Crusade. The Hafsids decided to destroy its defenses so it could not be used as a base by a hostile power again. It also continued to function as an episcopal see.

The regional power had shifted to Kairouan and the Medina of Tunis in the medieval period, until the early 20th century, when it began to develop into a coastal suburb of Tunis, incorporated as Carthage municipality in 1919. The archaeological site was first surveyed in 1830, by Danish consul Christian Tuxen Falbe. Excavations were performed in the second half of the 19th century by Charles Ernest Beulé and by Alfred Louis Delattre. The Carthage National Museum was founded in 1875 by Cardinal Charles Lavigerie. Excavations performed by French archaeologists in the 1920s first attracted an extraordinary amount of attention because of the evidence they produced for child sacrifice. There has been considerable disagreement among scholars concerning whether child sacrifice was practiced by ancient Carthage. The open-air Carthage Paleo-Christian Museum has exhibits excavated under the auspices of UNESCO from 1975 to 1984. The site of the ruins is a UNESCO World Heritage Site.
The name "Carthage" ( ) is the Early Modern anglicisation of Middle French , from Latin and (cf. Greek () and Etruscan ) from the Punic "new city", implying it was a "new Tyre". The Latin adjective , meaning "Phoenician", is reflected in English in some borrowings from Latinnotably the Punic Wars and the Punic language.

The Modern Standard Arabic form "" () is an adoption of French , replacing an older local toponym reported as "Cartagenna" that directly continued the Latin name.

Carthage was built on a promontory with sea inlets to the north and the south. The city's location made it master of the Mediterranean's maritime trade. All ships crossing the sea had to pass between Sicily and the coast of Tunisia, where Carthage was built, affording it great power and influence. Two large, artificial harbors were built within the city, one for harboring the city's prodigious navy of 220 warships and the other for mercantile trade. A walled tower overlooked both harbors. The city had massive walls, long, which was longer than the walls of comparable cities. Most of the walls were on the shore and so could be less impressive, as Carthaginian control of the sea made attack from that direction difficult. The of wall on the isthmus to the west were truly massive and were never penetrated.

Carthage was one of the largest cities of the Hellenistic period and was among the largest cities in preindustrial history. Whereas by AD 14, Rome had at least 750,000 inhabitants and in the following century may have reached 1 million, the cities of Alexandria and Antioch numbered only a few hundred thousand or less. According to the history of Herodian, Carthage rivaled Alexandria for second place in the Roman empire.

The Punic Carthage was divided into four equally sized residential areas with the same layout. The Punic had religious areas, market places, council house, towers, a theater, and a huge necropolis; roughly in the middle of the city stood a high citadel called the Byrsa. Surrounding Carthage were walls "of great strength" said in places to rise above 13 m, being nearly 10 m thick, according to ancient authors. To the west, three parallel walls were built. The walls altogether ran for about to encircle the city. The heights of the Byrsa were additionally fortified; this area being the last to succumb to the Romans in 146 BC. Originally the Romans had landed their army on the strip of land extending southward from the city.

Outside the city walls of Carthage is the "Chora" or farm lands of Carthage. "Chora" encompassed a limited area: the north coastal "tell", the lower Bagradas river valley (inland from Utica), Cape Bon, and the adjacent "sahel" on the east coast. Punic culture here achieved the introduction of agricultural sciences first developed for lands of the eastern Mediterranean, and their adaptation to local African conditions.

The "urban landscape" of Carthage is known in part from ancient authors, augmented by modern digs and surveys conducted by archeologists. The "first urban nucleus" dating to the seventh century, in area about , was apparently located on low-lying lands along the coast (north of the later harbors). As confirmed by archaeological excavations, Carthage was a "creation "ex nihilo"", built on 'virgin' land, and situated at what was then the end of a peninsula. Here among "mud brick walls and beaten clay floors" (recently uncovered) were also found extensive cemeteries, which yielded evocative grave goods like clay masks. "Thanks to this burial archaeology we know more about archaic Carthage than about any other contemporary city in the western Mediterranean." Already in the eighth century, fabric dyeing operations had been established, evident from crushed shells of murex (from which the 'Phoenician purple' was derived). Nonetheless, only a "meager picture" of the cultural life of the earliest pioneers in the city can be conjectured, and not much about housing, monuments or defenses. The Roman poet Virgil (70–19 BC) imagined early Carthage, when his legendary character Aeneas had arrived there:
"Aeneas found, where lately huts had been,
marvelous buildings, gateways, cobbled ways,
and din of wagons. There the Tyrians
were hard at work: laying courses for walls,
rolling up stones to build the citadel,
while others picked out building sites and plowed
a boundary furrow. Laws were being enacted,
magistrates and a sacred senate chosen.
Here men were dredging harbors, there they laid
the deep foundations of a theatre,
and quarried massive pillars... ."

The two inner harbors, named "cothon" in Punic, were located in the southeast; one being commercial, and the other for war. Their definite functions are not entirely known, probably for the construction, outfitting, or repair of ships, perhaps also loading and unloading cargo. Larger anchorages existed to the north and south of the city. North and west of the "cothon" were located several industrial areas, e.g., metalworking and pottery (e.g., for amphora), which could serve both inner harbors, and ships anchored to the south of the city.

About the Byrsa, the citadel area to the north, considering its importance our knowledge of it is patchy. Its prominent heights were the scene of fierce combat during the fiery destruction of the city in 146 BC. The Byrsa was the reported site of the Temple of Eshmun (the healing god), at the top of a stairway of sixty steps. A temple of Tanit (the city's queen goddess) was likely situated on the slope of the 'lesser Byrsa' immediately to the east, which runs down toward the sea. Also situated on the Byrsa were luxury homes.

South of the citadel, near the "cothon" was the "tophet", a special and very old cemetery, which when begun lay outside the city's boundaries. Here the "Salammbô" was located, the "Sanctuary of Tanit", not a temple but an enclosure for placing stone stelae. These were mostly short and upright, carved for funeral purposes. The presence of infant skeletons from here may indicate the occurrence of child sacrifice, as claimed in the Bible, although there has been considerable doubt among archeologists as to this interpretation and many consider it simply a cemetery devoted to infants. Probably the "tophet" burial fields were "dedicated at an early date, perhaps by the first settlers." Recent studies, on the other hand, indicate that child sacrifice was practiced by the Carthaginians.

Between the sea-filled "cothon" for shipping and the Byrsa heights lay the "agora" [Greek: "market"], the city-state's central marketplace for business and commerce. The "agora" was also an area of public squares and plazas, where the people might formally assemble, or gather for festivals. It was the site of religious shrines, and the location of whatever were the major municipal buildings of Carthage. Here beat the heart of civic life. In this district of Carthage, more probably, the ruling suffets presided, the council of elders convened, the tribunal of the 104 met, and justice was dispensed at trials in the open air.

Early residential districts wrapped around the Byrsa from the south to the north east. Houses usually were whitewashed and blank to the street, but within were courtyards open to the sky. In these neighborhoods multistory construction later became common, some up to six stories tall according to an ancient Greek author. Several architectural floorplans of homes have been revealed by recent excavations, as well as the general layout of several city blocks. Stone stairs were set in the streets, and drainage was planned, e.g., in the form of soakaways leaching into the sandy soil. Along the Byrsa's southern slope were located not only fine old homes, but also many of the earliest grave-sites, juxtaposed in small areas, interspersed with daily life.

Artisan workshops were located in the city at sites north and west of the harbors. The location of three metal workshops (implied from iron slag and other vestiges of such activity) were found adjacent to the naval and commercial harbors, and another two were further up the hill toward the Byrsa citadel. Sites of pottery kilns have been identified, between the "agora" and the harbors, and further north. Earthenware often used Greek models. A fuller's shop for preparing woolen cloth (shrink and thicken) was evidently situated further to the west and south, then by the edge of the city. Carthage also produced objects of rare refinement. During the 4th and 3rd centuries, the sculptures of the sarcophagi became works of art. "Bronze engraving and stone-carving reached their zenith."

The elevation of the land at the promontory on the seashore to the north-east (now called Sidi Bou Saïd), was twice as high above sea level as that at the Byrsa (100 m and 50 m). In between runs a ridge, several times reaching 50 m; it continues northwestward along the seashore, and forms the edge of a plateau-like area between the Byrsa and the sea. Newer urban developments lay here in these northern districts.

Due to the Roman's leveling of the city, the original Punic urban landscape of Carthage was largely lost. Since 1982, French archaeologist Serge Lancel excavated a residential area of the Punic Carthage on top of Byrsa hill near the Forum of the Roman Carthage. The neighborhood can be dated back to early second century BC, and with its houses, shops, and private spaces, is significant for what it reveals about daily life of the Punic Carthage.

The remains have been preserved under embankments, the substructures of the later Roman forum, whose foundation piles dot the district. The housing blocks are separated by a grid of straight streets about wide, with a roadway consisting of clay; "in situ" stairs compensate for the slope of the hill. Construction of this type presupposes organization and political will, and has inspired the name of the neighborhood, "Hannibal district", referring to the legendary Punic general or sufet (consul) at the beginning of the second century BC. The habitat is typical, even stereotypical. The street was often used as a storefront/shopfront; cisterns were installed in basements to collect water for domestic use, and a long corridor on the right side of each residence led to a courtyard containing a sump, around which various other elements may be found. In some places, the ground is covered with mosaics called punica pavement, sometimes using a characteristic red mortar.

Punic culture and agricultural sciences, after arriving at Carthage from the eastern Mediterranean, gradually adapted to the local conditions. The merchant harbor at Carthage was developed after settlement of the nearby Punic town of Utica, and eventually the surrounding African countryside was brought into the orbit of the Punic urban centers, first commercially, then politically. Direct management over cultivation of neighbouring lands by Punic owners followed. A 28-volume work on agriculture written in Punic by Mago, a retired army general (), was translated into Latin and later into Greek. The original and both translations have been lost; however, some of Mago's text has survived in other Latin works. Olive trees (e.g., grafting), fruit trees (pomegranate, almond, fig, date palm), viniculture, bees, cattle, sheep, poultry, implements, and farm management were among the ancient topics which Mago discussed. As well, Mago addresses the wine-maker's art (here a type of sherry).

In Punic farming society, according to Mago, the small estate owners were the chief producers. They were, two modern historians write, not absent landlords. Rather, the likely reader of Mago was "the master of a relatively modest estate, from which, by great personal exertion, he extracted the maximum yield." Mago counselled the rural landowner, for the sake of their own 'utilitarian' interests, to treat carefully and well their managers and farm workers, or their overseers and slaves. Yet elsewhere these writers suggest that rural land ownership provided also a new power base among the city's nobility, for those resident in their country villas. By many, farming was viewed as an alternative endeavour to an urban business. Another modern historian opines that more often it was the urban merchant of Carthage who owned rural farming land to some profit, and also to retire there during the heat of summer. It may seem that Mago anticipated such an opinion, and instead issued this contrary advice (as quoted by the Roman writer Columella):

The man who acquires an estate must sell his house, lest he prefer to live in the town rather than in the country. Anyone who prefers to live in a town has no need of an estate in the country." "One who has bought land should sell his town house, so that he will have no desire to worship the household gods of the city rather than those of the country; the man who takes greater delight in his city residence will have no need of a country estate.

The issues involved in rural land management also reveal underlying features of Punic society, its structure and stratification. The hired workers might be considered 'rural proletariat', drawn from the local Berbers. Whether there remained Berber landowners next to Punic-run farms is unclear. Some Berbers became sharecroppers. Slaves acquired for farm work were often prisoners of war. In lands outside Punic political control, independent Berbers cultivated grain and raised horses on their lands. Yet within the Punic domain that surrounded the city-state of Carthage, there were ethnic divisions in addition to the usual quasi feudal distinctions between lord and peasant, or master and serf. This inherent instability in the countryside drew the unwanted attention of potential invaders. Yet for long periods Carthage was able to manage these social difficulties.

The many amphorae with Punic markings subsequently found about ancient Mediterranean coastal settlements testify to Carthaginian trade in locally made olive oil and wine. Carthage's agricultural production was held in high regard by the ancients, and rivaled that of Romethey were once competitors, e.g., over their olive harvests. Under Roman rule, however, grain production (wheat and barley) for export increased dramatically in 'Africa'; yet these later fell with the rise in Roman Egypt's grain exports. Thereafter olive groves and vineyards were re-established around Carthage. Visitors to the several growing regions that surrounded the city wrote admiringly of the lush green gardens, orchards, fields, irrigation channels, hedgerows (as boundaries), as well as the many prosperous farming towns located across the rural landscape.

Accordingly, the Greek author and compiler Diodorus Siculus (fl. 1st century BC), who enjoyed access to ancient writings later lost, and on which he based most of his writings, described agricultural land near the city of Carthage c. 310 BC:

It was divided into market gardens and orchards of all sorts of fruit trees, with many streams of water flowing in channels irrigating every part. There were country homes everywhere, lavishly built and covered with stucco. ... Part of the land was planted with vines, part with olives and other productive trees. Beyond these, cattle and sheep were pastured on the plains, and there were meadows with grazing horses.

Greek cities contested with Carthage for the Western Mediterranean culminating in the Sicilian Wars and the Pyrrhic War over Sicily, while the Romans fought three wars against Carthage, known as the Punic Wars, from the Latin "Punic" meaning "Phoenician", as Carthage was a Phoenician colony grown into an empire.

The Carthaginian republic was one of the longest-lived and largest states in the ancient Mediterranean. Reports relay several wars with Syracuse and finally, Rome, which eventually resulted in the defeat and destruction of Carthage in the Third Punic War. The Carthaginians were Phoenician settlers originating in the Mediterranean coast of the Near East. They spoke Canaanite, a Semitic language, and followed a local variety of the ancient Canaanite religion, the Punic religion. The Carthaginians travelled widely across the seas and set up numerous colonies. Unlike Greek, Phoenician, and Tyrian colonizers who "only required colonies to pay due respect for their home-cities", Carthage is said to have "sent its own magistrates to govern overseas settlements".

The fall of Carthage came at the end of the Third Punic War in 146 BC at the Battle of Carthage. Despite initial devastating Roman naval losses and Hannibal's 15-year occupation of much of Roman Italy, who was on the brink of defeat but managed to recover, the end of the series of wars resulted in the end of Carthaginian power and the complete destruction of the city by Scipio Aemilianus. The Romans pulled the Phoenician warships out into the harbor and burned them before the city, and went from house to house, capturing and enslaving the people. About 50,000 Carthaginians were sold into slavery. The city was set ablaze and razed to the ground, leaving only ruins and rubble. After the fall of Carthage, Rome annexed the majority of the Carthaginian colonies, including other North African locations such as Volubilis, Lixus, Chellah. Today a "Carthaginian peace" can refer to any brutal peace treaty demanding total subjugation of the defeated side.

Since at least 1863, it has been claimed that Carthage was sown with salt after being razed, but there is no evidence for this.

When Carthage fell, its nearby rival Utica, a Roman ally, was made capital of the region and replaced Carthage as the leading center of Punic trade and leadership. It had the advantageous position of being situated on the outlet of the Medjerda River, Tunisia's only river that flowed all year long. However, grain cultivation in the Tunisian mountains caused large amounts of silt to erode into the river. This silt accumulated in the harbor until it became useless, and Rome was forced to rebuild Carthage.

By 122 BC, Gaius Gracchus founded a short-lived colony, called "Colonia Iunonia", after the Latin name for the Punic goddess Tanit, "Iuno Caelestis". The purpose was to obtain arable lands for impoverished farmers. The Senate abolished the colony some time later, to undermine Gracchus' power.

After this ill-fated effort, a new city of Carthage was built on the same land by Julius Caesar in the period from 49 to 44 BC, and by the first century, it had grown to be the second-largest city in the western half of the Roman Empire, with a peak population of 500,000. It was the center of the province of Africa, which was a major breadbasket of the Empire. Among its major monuments was an amphitheater.

Carthage also became a center of early Christianity (see Carthage (episcopal see)). In the first of a string of rather poorly reported councils at Carthage a few years later, no fewer than 70 bishops attended. Tertullian later broke with the mainstream that was increasingly represented in the West by the primacy of the Bishop of Rome, but a more serious rift among Christians was the Donatist controversy, against which Augustine of Hippo spent much time and parchment arguing. At the Council of Carthage (397), the biblical canon for the western Church was confirmed. The Christians at Carthage conducted persecutions against the pagans, during which the pagan temples, notably the famous Temple of Juno Caelesti, were destroyed.
The Vandals under Gaiseric invaded Africa in 429. They relinquished the facade of their allied status to Rome and defeated the Roman general Bonifacius to seize Carthage, the once most treasured province of Rome. The 5th-century Roman bishop Victor Vitensis mentions in his "Historia Persecutionis Africanae Provincia" that the Vandals destroyed parts of Carthage, including various buildings and churches. Once in power, the ecclesiastical authorities were persecuted, the locals were aggressively taxed, and naval raids were routinely launched on Romans in the Mediterranean.

After a failed attempt to recapture the city in the fifth century, the Eastern Roman Empire finally subdued the Vandals in the Vandalic War in 533–534 and made Carthage capital of Byzantine North Africa. Thereafter, the city became the seat of the praetorian prefecture of Africa, which was made into an exarchate during the emperor Maurice's reign, as was Ravenna on the Italian Peninsula. These two exarchates were the western bulwarks of the Byzantine Empire, all that remained of its power in the West. In the early seventh century Heraclius the Elder, the exarch of Carthage, overthrew the Byzantine emperor Phocas, whereupon his son Heraclius succeeded to the imperial throne.

The Roman Exarchate of Africa was not able to withstand the seventh-century Muslim conquest of the Maghreb. The Umayyad Caliphate under Abd al-Malik ibn Marwan in 686 sent a force led by Zuhayr ibn Qays, who won a battle over the Romans and Berbers led by King Kusaila of the Kingdom of Altava on the plain of Kairouan, but he could not follow that up. In 695, Hassan ibn al-Nu'man captured Carthage and advanced into the Atlas Mountains. An imperial fleet arrived and retook Carthage, but in 698, Hasan ibn al-Nu'man returned and defeated Emperor Tiberios III at the 698 Battle of Carthage. Roman imperial forces withdrew from all of Africa except Ceuta. Fearing that the Byzantine Empire might reconquer it, they decided to destroy Roman Carthage in a scorched earth policy and establish their headquarters somewhere else. Its walls were torn down, the water supply from its aqueducts cut off, the agricultural land was ravaged and its harbors made unusable.

The destruction of the Exarchate of Africa marked a permanent end to the Byzantine Empire's influence in the region.

It is clear from archaeological evidence that the town of Carthage continued to be occupied, as did the neighborhood of Bjordi Djedid. The Baths of Antoninus continued to function in the Arab period and the eleventh-century historian Al-Bakri stated that they were still in good condition at that time. They also had production centers nearby. It is difficult to determine whether the continued habitation of some other buildings belonged to Late Byzantine or Early Arab period. The Bir Ftouha church may have continued to remain in use although it is not clear when it became uninhabited. Constantine the African was born in Carthage.

The Medina of Tunis, originally a Berber settlement, was established as the new regional center under the Umayyad Caliphate in the early 8th century. Under the Aghlabids, the people of Tunis revolted numerous times, but the city profited from economic improvements and quickly became the second most important in the kingdom. It was briefly the national capital, from the end of the reign of Ibrahim II in 902, until 909, when the Shi'ite Berbers took over Ifriqiya and founded the Fatimid Caliphate.

Carthage remained a residential see until the high medieval period, and is mentioned in
two letters of Pope Leo IX dated 1053, written in reply to consultations regarding a conflict between the bishops of Carthage and Gummi. In each of the two letters, Pope Leo declares that, after the Bishop of Rome, the first archbishop and chief metropolitan of the whole of Africa is the bishop of Carthage.
Later, an archbishop of Carthage named Cyriacus was imprisoned by the Arab rulers because of an accusation by some Christians. Pope Gregory VII wrote Cyriacus a letter of consolation, repeating the hopeful assurances of the primacy of the Church of Carthage, "whether the Church of Carthage should still lie desolate or rise again in glory".
By 1076, Cyriacus was set free, but there was only one other bishop in the province. These are the last of whom there is mention in that period of the history of the see.

The fortress of Carthage was used by the Muslims until Hafsid era and was captured by the Crusaders during the Eighth Crusade. The inhabitants of Carthage were slaughtered by the Crusaders after they took it, and it was used as a base of operations against the Hafsids. After repelling them, Muhammad I al-Mustansir decided to raze Cathage's defenses in order to prevent a repeat.

Carthage is some east-northeast of Tunis; the settlements nearest to Carthage were the town of Sidi Bou Said to the north and the village of Le Kram to the south. Sidi Bou Said was a village which had grown around the tomb of the eponymous sufi saint (d. 1231), which had been developed into a town under Ottoman rule in the 18th century. Le Kram was developed in the late 19th century under French administration as a settlement close to the port of La Goulette.

In 1881, Tunisia became a French protectorate, and in the same year Charles Lavigerie, who was archbishop of Algiers, became apostolic administrator of the vicariate of Tunis. In the following year, Lavigerie became a cardinal. He "saw himself as the reviver of the ancient Christian Church of Africa, the Church of Cyprian of Carthage", and, on 10 November 1884, was successful in his great ambition of having the metropolitan see of Carthage restored, with himself as its first archbishop. In line with the declaration of Pope Leo IX in 1053, Pope Leo XIII acknowledged the revived Archdiocese of Carthage as the primatial see of Africa and Lavigerie as primate.

The Acropolium of Carthage (Saint Louis Cathedral of Carthage) was erected on Byrsa hill in 1884.

The Danish consul Christian Tuxen Falbe conducted a first survey of the topography of the archaeological site (published in 1833).
Antiquarian interest was intensified following the publication of Flaubert's "Salammbô" in 1858. Charles Ernest Beulé performed some preliminary excavations of Roman remains on Byrsa hill in 1860. A more systematic survey of both Punic and Roman-era remains is due to Alfred Louis Delattre, who was sent to Tunis by cardinal Charles Lavigerie in 1875 on both an apostolic and an archaeological mission.
Audollent (1901, p. 203) cites Delattre and Lavigerie to the effect that in the 1880s, locals still knew the area of the ancient city under the name of "Cartagenna" (i.e. reflecting the Latin "n"-stem "Carthāgine").

Auguste Audollent divides the area of Roman Carthage into four quarters, "Cartagenna", "Dermèche", "Byrsa" and "La Malga". Cartagenna and Dermèche correspond with the lower city, including the site of Punic Carthage; Byrsa is associated with the upper city, which in Punic times was a walled citadel above the harbour; and "La Malga" is linked with the more remote parts of the upper city in Roman times.

French-led excavations at Carthage began in 1921, and from 1923 reported finds of a large quantity of urns containing a mixture of animal and children's bones. René Dussaud identified a 4th-century BC stela found in Carthage as depicting a child sacrifice.

A temple at Amman (1400–1250 BC) excavated and reported upon by J.B. Hennessy in 1966, shows the possibility of bestial and human sacrifice by fire. While evidence of child sacrifice in Canaan was the object of academic disagreement, with some scholars arguing that merely children's cemeteries had been unearthed in Carthage, the mixture of children's with animal bones as well as associated epigraphic evidence involving mention of "mlk" led some to believe that, at least in Carthage, child sacrifice was indeed common practice. However, though the animals were surely sacrificed, this does not entirely indicate that the infants were, and in fact the bones indicate the opposite. Rather, the animal sacrifice was likely done to, in some way, honour the deceased.

A study conducted in 1970 by M. Chabeuf, the then Doctor of Science from the University of Paris, showed little difference between 17 modern Tunisians, and 68 Punic remains. An analysis the following year on 42 North-West African skulls dating back to Roman times concluded that they were overall similar to modern Berbers and other Mediterranean populations, especially eastern Iberians. They also noted the presence of one outlier in Tunisia who appears to have inherited mechtoid traits, which led them to hypothesize the persistence of such affinities well into the Punic and Roman era.

M. C. Chamla and D Ferembach (1988) in their entry dealing with the craniometric conclusions of Protohistorical Algerians and Punics in the region of Tunisia, found strong sexual dimorphism with male skulls being robust. Mediterranean elements were dominant, but Mechtoid features, as well as 'Negroid' traits were present in some of the samples. Overall, Punic burials showed affinities with Algerians, Roman Era skulls from Tarragona (Spain), Guanches, and to a lesser extent Abydos (XVIIIth dynasty), Etruscans, Bronze Age Syrians (Euphrates) and skulls from Lozere (France). The anthropological position of the Algerian and Punic people when it comes to populations of the Mediterranean Basin agreed quite well with the geographical situation.

Jehan Desanges stated that "In the Punic burial grounds, negroid remains were not rare and there were black auxiliaries in the Carthaginian army who were certainly not Nilotics".

In 1990, Shomarka Keita, a biological anthropologist, had conducted a craniometric study which featured a set of remains from Northern Africa. He examined a sample of 49 Maghreban crania which included skulls from pre-Roman Carthage and concluded that, although they were heterogeneous, many of them showed physical similarities to crania from equatorial Africa, ancient Egypt, and Kush. S.O.Y. Keita's report in 2018, found the pre-Roman Carthaginian series to be intermediate between the Phoenician and Maghrebian. He noted the findings are consistent with an interpretation that it reflects both local and Levantine ancestry due to specific interactions in the ancient period.

In 2016, an ancient Carthaginian individual, who was excavated from a Punic tomb in Byrsa Hill, was found to belong to the rare U5b2c1 maternal haplogroup. The Young Man of Byrsa specimen dates from the late 6th century BC, and his lineage is believed to represent early gene flow from Iberia to the Maghreb. Craniometric analysis of the young man indicated likely Mediterranean/European ancestry as opposed to African or Asian.

Due to its coastal location, Carthage Archeological Site is vulnerable to sea level rise. In 2022, the IPCC Sixth Assessment Report included it in the list of African cultural sites which would be threatened by flooding and coastal erosion by the end of the century, but only if climate change followed RCP 8.5, which is the scenario of high and continually increasing greenhouse gas emissions associated with the warming of over 4°C., and is no longer considered very likely. The other, more plausible scenarios result in lower warming levels and consequently lower sea level rise: yet, sea levels would continue to increase for about 10,000 years under all of them. Even if the warming is limited to 1.5°C, global sea level rise is still expected to exceed after 2000 years (and higher warming levels will see larger increases by then), consequently exceeding 2100 levels of sea level rise under RCP 8.5 (~ with a range of ) well before the year 4000. Thus, it is a matter of time before the Carthage Archeological Site is threatened by rising water levels, unless it can be protected by adaptation efforts such as sea walls.

The commune of Carthage was created by a decree of the Bey of Tunis on 15 June 1919, during the rule of Naceur Bey.

In 1920, the first seaplane base was built on the Lake of Tunis for the seaplanes of Compagnie Aéronavale. The Tunis Airfield opened in 1938, serving around 5,800 passengers annually on the Paris-Tunis route.
During World War II, the airport was used by the United States Army Air Force Twelfth Air Force as a headquarters and command control base for the Italian Campaign of 1943.
Construction on the Tunis-Carthage Airport, which was fully funded by France, began in 1944, and in 1948 the airport become the main hub for Tunisair.

In the 1950s the Lycée Français de Carthage was established to serve French families in Carthage. In 1961 it was given to the Tunisian government as part of the Independence of Tunisia, so the nearby Collège Maurice Cailloux in La Marsa, previously an annex of the Lycée Français de Carthage, was renamed to the Lycée Français de La Marsa and began serving the "lycée" level. It is currently the Lycée Gustave Flaubert.

After Tunisian independence in 1956, the Tunis conurbation gradually extended around the airport, and Carthage (قرطاج "Qarṭāj") is now a suburb of Tunis, covering the area between Sidi Bou Said and Le Kram. Its population as of January 2013 was estimated at 21,276,
mostly attracting the more wealthy residents. If Carthage is not the capital, it tends to be the political pole, a "place of emblematic power" according to Sophie Bessis, leaving to Tunis the economic and administrative roles. The Carthage Palace (the Tunisian presidential palace) is located in the coast.

The suburb has six train stations of the TGM line between Le Kram and Sidi Bou Said:
Carthage Salammbo (named for the ancient children’s cemetery where it stands), Carthage Byrsa (named for Byrsa hill), Carthage Dermech ("Dermèche"), Carthage Hannibal (named for Hannibal), Carthage Présidence (named for the Presidential Palace) and Carthage Amilcar (named for Hamilcar).

The merchants of Carthage were in part heirs of the Mediterranean trade developed by Phoenicia, and so also heirs of the rivalry with Greek merchants. Business activity was accordingly both stimulated and challenged. Cyprus had been an early site of such commercial contests. The Phoenicians then had ventured into the western Mediterranean, founding trading posts, including Utica and Carthage. The Greeks followed, entering the western seas where the commercial rivalry continued. Eventually it would lead, especially in Sicily, to several centuries of intermittent war. Although Greek-made merchandise was generally considered superior in design, Carthage also produced trade goods in abundance. That Carthage came to function as a manufacturing colossus was shown during the Third Punic War with Rome. Carthage, which had previously disarmed, then was made to face the fatal Roman siege. The city "suddenly organised the manufacture of arms" with great skill and effectiveness. According to Strabo (63 BC – AD 21) in his "Geographica":

[Carthage] each day produced one hundred and forty finished shields, three hundred swords, five hundred spears, and one thousand missiles for the catapults... . Furthermore, [Carthage although surrounded by the Romans] built one hundred and twenty decked ships in two months... for old timber had been stored away in readiness, and a large number of skilled workmen, maintained at public expense.

The textiles industry in Carthage probably started in private homes, but the existence of professional weavers indicates that a sort of factory system later developed. Products included embroidery, carpets, and use of the purple murex dye (for which the Carthaginian isle of Djerba was famous). Metalworkers developed specialized skills, i.e., making various weapons for the armed forces, as well as domestic articles, such as knives, forks, scissors, mirrors, and razors (all articles found in tombs). Artwork in metals included vases and lamps in bronze, also bowls, and plates. Other products came from such crafts as the potters, the glassmakers, and the goldsmiths. Inscriptions on votive stele indicate that many were not slaves but 'free citizens'.

Phoenician and Punic merchant ventures were often run as a family enterprise, putting to work its members and its subordinate clients. Such family-run businesses might perform a variety of tasks: own and maintain the ships, providing the captain and crew; do the negotiations overseas, either by barter or buying and selling, of their own manufactured commodities and trade goods, and native products (metals, foodstuffs, etc.) to carry and trade elsewhere; and send their agents to stay at distant outposts in order to make lasting local contacts, and later to establish a warehouse of shipped goods for exchange, and eventually perhaps a settlement. Over generations, such activity might result in the creation of a wide-ranging network of trading operations. Ancillary would be the growth of reciprocity between different family firms, foreign and domestic.

State protection was extended to its sea traders by the Phoenician city of Tyre and later likewise by the daughter city-state of Carthage. Stéphane Gsell, the well-regarded French historian of ancient North Africa, summarized the major principles guiding the civic rulers of Carthage with regard to its policies for trade and commerce:

Both the Phoenicians and the Carthaginians were well known in antiquity for their secrecy in general, and especially pertaining to commercial contacts and trade routes. Both cultures excelled in commercial dealings. Strabo (63 BC–AD 21) the Greek geographer wrote that before its fall (in 146 BC) Carthage enjoyed a population of 700,000, and directed an alliance of 300 cities. The Greek historian Polybius (–120) referred to Carthage as "the wealthiest city in the world".

A "suffet" (possibly two) was elected by the citizens, and held office with no military power for a one-year term. Carthaginian generals marshalled mercenary armies and were separately elected. From about 550 to 450 the Magonid family monopolized the top military position; later the Barcid family acted similarly. Eventually it came to be that, after a war, the commanding general had to testify justifying his actions before a court of 104 judges.

Aristotle (384–322) discusses Carthage in his work, "Politica"; he begins: "The Carthaginians are also considered to have an excellent form of government." He briefly describes the city as a "mixed constitution", a political arrangement with cohabiting elements of monarchy, aristocracy, and democracy, i.e., a king (Gk: basileus), a council of elders (Gk: gerusia), and the people (Gk: demos). Later Polybius of Megalopolis (–122, Greek) in his "Histories" would describe the Roman Republic in more detail as a mixed constitution in which the Consuls were the monarchy, the Senate the aristocracy, and the Assemblies the democracy.

Evidently Carthage also had an institution of elders who advised the Suffets, similar to a Greek "" or the Roman Senate. We do not have a Punic name for this body. At times its members would travel with an army general on campaign. Members also formed permanent committees. The institution had several hundred members drawn from the wealthiest class who held office for life. Vacancies were probably filled by recruitment from among the elite, i.e., by co-option. From among its members were selected the 104 Judges mentioned above. Later the 104 would come to evaluate not only army generals but other office holders as well. Aristotle regarded the 104 as most important; he compared it to the ephorate of Sparta with regard to control over security. In Hannibal's time, such a Judge held office for life. At some stage there also came to be independent self-perpetuating boards of five who filled vacancies and supervised (non-military) government administration.

Popular assemblies also existed at Carthage. When deadlocked the Suffets and the quasi-senatorial institution of elders might request the assembly to vote; also, assembly votes were requested in very crucial matters in order to achieve political consensus and popular coherence. The assembly members had no "legal" wealth or birth qualification. How its members were selected is unknown, e.g., whether by festival group or urban ward or another method.

The Greeks were favourably impressed by the constitution of Carthage; Aristotle had a separate study of it made which unfortunately is lost. In his "Politica" he states: "The government of Carthage is oligarchical, but they successfully escape the evils of oligarchy by enriching one portion of the people after another by sending them to their colonies." "[T]heir policy is to send some [poorer citizens] to their dependent towns, where they grow rich." Yet Aristotle continues, "[I]f any misfortune occurred, and the bulk of the subjects revolted, there would be no way of restoring peace by legal means." Aristotle remarked also:

Many of the Carthaginian institutions are excellent. The superiority of their constitution is proved by the fact that the common people remain loyal to the constitution; the Carthaginians have never had any rebellion worth speaking of, and have never been under the rule of a tyrant.

The city-state of Carthage, whose citizens were mainly "Libyphoenicians" (of Phoenician ancestry born in Africa), dominated and exploited an agricultural countryside composed mainly of native Berber sharecroppers and farmworkers, whose affiliations to Carthage were open to divergent possibilities. Beyond these more settled Berbers and the Punic farming towns and rural manors, lived the independent Berber tribes, who were mostly pastoralists.

In the brief, uneven review of government at Carthage found in his "Politica" Aristotle mentions several faults. Thus, "that the same person should hold many offices, which is a favorite practice among the Carthaginians." Aristotle disapproves, mentioning the flute-player and the shoemaker. Also, that "magistrates should be chosen not only for their merit but for their wealth." Aristotle's opinion is that focus on pursuit of wealth will lead to oligarchy and its evils.

[S]urely it is a bad thing that the greatest offices... should be bought. The law which allows this abuse makes wealth of more account than virtue, and the whole state becomes avaricious. For, whenever the chiefs of the state deem anything honorable, the other citizens are sure to follow their example; and, where virtue has not the first place, their aristocracy cannot be firmly established.

In Carthage the people seemed politically satisfied and submissive, according to the historian Warmington. They in their assemblies only rarely exercised the few opportunities given them to assent to state decisions. Popular influence over government appears not to have been an issue at Carthage. Being a commercial republic fielding a mercenary army, the people were not conscripted for military service, an experience which can foster the feel for popular political action. But perhaps this misunderstands the society; perhaps the people, whose values were based on small-group loyalty, felt themselves sufficiently connected to their city's leadership by the very integrity of the person-to-person linkage within their social fabric. Carthage was very stable; there were few openings for tyrants. Only after defeat by Rome devastated Punic imperial ambitions did the people of Carthage seem to question their governance and to show interest in political reform.

In 196, following the Second Punic War (218–201), Hannibal, still greatly admired as a Barcid military leader, was elected suffet. When his reforms were blocked by a financial official about to become a judge for life, Hannibal rallied the populace against the 104 judges. He proposed a one-year term for the 104, as part of a major civic overhaul. Additionally, the reform included a restructuring of the city's revenues, and the fostering of trade and agriculture. The changes rather quickly resulted in a noticeable increase in prosperity. Yet his incorrigible political opponents cravenly went to Rome, to charge Hannibal with conspiracy, namely, plotting war against Rome in league with Antiochus the Hellenic ruler of Syria. Although the Roman Scipio Africanus resisted such manoeuvre, eventually intervention by Rome forced Hannibal to leave Carthage. Thus, corrupt city officials efficiently blocked Hannibal in his efforts to reform the government of Carthage.

Mago (6th century) was King of Carthage; the head of state, war leader, and religious figurehead. His family was considered to possess a sacred quality. Mago's office was somewhat similar to that of a pharaoh, but although kept in a family it was not hereditary, it was limited by legal consent. Picard, accordingly, believes that the council of elders and the popular assembly are late institutions. Carthage was founded by the king of Tyre who had a royal monopoly on this trading venture. Thus it was the royal authority stemming from this traditional source of power that the King of Carthage possessed. Later, as other Phoenician ship companies entered the trading region, and so associated with the city-state, the King of Carthage had to keep order among a rich variety of powerful merchants in their negotiations among themselves and over risky commerce across the Mediterranean. Under these circumstance, the office of king began to be transformed. Yet it was not until the aristocrats of Carthage became wealthy owners of agricultural lands in Africa that a council of elders was institutionalized at Carthage.

Most ancient literature concerning Carthage comes from Greek and Roman sources as Carthage's own documents were destroyed by the Romans. Apart from inscriptions, hardly any Punic literature has survived, and none in its own language and script. A brief catalogue would include:

"[F]rom the Greek author Plutarch [(c. 46 – c. 120)] we learn of the 'sacred books' in Punic safeguarded by the city's temples. Few Punic texts survive, however." Once "the City Archives, the Annals, and the scribal lists of "Suffets"" existed, but evidently these were destroyed in the horrific fires during the Roman capture of the city in 146 BC.

Yet some Punic books (Latin: "libri punici") from the libraries of Carthage reportedly did survive the fires. These works were apparently given by Roman authorities to the newly augmented Berber rulers. Over a century after the fall of Carthage, the Roman politician-turned-author Gaius Sallustius Crispus or Sallust (86–34) reported his having seen volumes written in Punic, which books were said to be once possessed by the Berber king, Hiempsal II (r. 88–81). By way of Berber informants and Punic translators, Sallust had used these surviving books to write his brief sketch of Berber affairs.
Probably some of Hiempsal II's "libri punici", that had escaped the fires that consumed Carthage in 146 BC, wound up later in the large royal library of his grandson Juba II (r. 25 BC–AD 24). Juba II not only was a Berber king, and husband of Cleopatra's daughter, but also a scholar and author in Greek of no less than nine works. He wrote for the Mediterranean-wide audience then enjoying classical literature. The "libri punici" inherited from his grandfather surely became useful to him when composing his "Libyka", a work on North Africa written in Greek. Unfortunately, only fragments of "Libyka" survive, mostly from quotations made by other ancient authors. It may have been Juba II who 'discovered' the five-centuries-old 'log book' of Hanno the Navigator, called the "Periplus", among library documents saved from fallen Carthage.

In the end, however, most Punic writings that survived the destruction of Carthage "did not escape the immense wreckage in which so many of Antiquity's literary works perished." Accordingly, the long and continuous interactions between Punic citizens of Carthage and the Berber communities that surrounded the city have no local historian. Their political arrangements and periodic crises, their economic and work life, the cultural ties and social relations established and nourished (infrequently as kin), are not known to us directly from ancient Punic authors in written accounts. Neither side has left us their stories about life in Punic-era Carthage.

Regarding "Phoenician" writings, few remain and these seldom refer to Carthage. The more ancient and most informative are cuneiform tablets, c. 1600–1185, from ancient Ugarit, located to the north of Phoenicia on the Syrian coast; it was a Canaanite city politically affiliated with the Hittites. The clay tablets tell of myths, epics, rituals, medical and administrative matters, and also correspondence. The highly valued works of Sanchuniathon, an ancient priest of Beirut, who reportedly wrote on Phoenician religion and the origins of civilization, are themselves completely lost, but some little content endures twice removed. Sanchuniathon was said to have lived in the 11th century, which is considered doubtful. Much later a "Phoenician History" by Philo of Byblos (64–141) reportedly existed, written in Greek, but only fragments of this work survive. An explanation proffered for why so few Phoenician works endured: early on (11th century) archives and records began to be kept on papyrus, which does not long survive in a moist coastal climate. Also, both Phoenicians and Carthaginians were well known for their secrecy.

Thus, of their ancient writings we have little of major interest left to us by Carthage, or by Phoenicia the country of origin of the city founders. "Of the various Phoenician and Punic compositions alluded to by the ancient classical authors, not a single work or even fragment has survived in its original idiom." "Indeed, not a single Phoenician manuscript has survived in the original [language] or in translation." We cannot therefore access directly the line of thought or the contour of their worldview as expressed in their own words, in their own voice. Ironically, it was the Phoenicians who "invented or at least perfected and transmitted a form of writing [the alphabet] that has influenced dozens of cultures including our own."

As noted, the celebrated ancient books on agriculture written by Mago of Carthage survives only via quotations in Latin from several later Roman works.

The scant remains of what was once a great city are reflected upon in Letitia Elizabeth Landon's poetical illustration, "Carthage", to an engraving of a painting by J. Salmon, published in Fisher's Drawing Room Scrap Book, 1837 with quotes from Sir Grenville Temple's Journal.
The protagonist in Isaac Asimov's 1956 science-fiction short story "The Dead Past" is an academic professor obsessed with debunking historical perceptions of Carthage.




Coprime integers

In number theory, two integers and are coprime, relatively prime or mutually prime if the only positive integer that is a divisor of both of them is 1. Consequently, any prime number that divides does not divide , and vice versa. This is equivalent to their greatest common divisor (GCD) being 1. One says also "is prime to" or "is coprime with" .

The numbers 8 and 9 are coprime, despite the fact that neither—considered individually—is a prime number, since 1 is their only common divisor. On the other hand, 6 and 9 are not coprime, because they are both divisible by 3. The numerator and denominator of a reduced fraction are coprime, by definition.

When the integers and are coprime, the standard way of expressing this fact in mathematical notation is to indicate that their greatest common divisor is one, by the formula or . In their 1989 textbook "Concrete Mathematics", Ronald Graham, Donald Knuth, and Oren Patashnik proposed an alternative notation formula_1 to indicate that and are relatively prime and that the term "prime" be used instead of coprime (as in is "prime" to ).

A fast way to determine whether two numbers are coprime is given by the Euclidean algorithm and its faster variants such as binary GCD algorithm or Lehmer's GCD algorithm.

The number of integers coprime with a positive integer , between 1 and , is given by Euler's totient function, also known as Euler's phi function, .

A set of integers can also be called coprime if its elements share no common positive factor except 1. A stronger condition on a set of integers is pairwise coprime, which means that and are coprime for every pair of different integers in the set. The set is coprime, but it is not pairwise coprime since 2 and 4 are not relatively prime.

The numbers 1 and −1 are the only integers coprime with every integer, and they are the only integers that are coprime with 0.

A number of conditions are equivalent to and being coprime:

As a consequence of the third point, if and are coprime and , then . That is, we may "divide by " when working modulo . Furthermore, if are both coprime with , then so is their product (i.e., modulo it is a product of invertible elements, and therefore invertible); this also follows from the first point by Euclid's lemma, which states that if a prime number divides a product , then divides at least one of the factors .

As a consequence of the first point, if and are coprime, then so are any powers and .

If and are coprime and divides the product , then divides . This can be viewed as a generalization of Euclid's lemma.

The two integers and are coprime if and only if the point with coordinates in a Cartesian coordinate system would be "visible" via an unobstructed line of sight from the origin , in the sense that there is no point with integer coordinates anywhere on the line segment between the origin and . (See figure 1.)

In a sense that can be made precise, the probability that two randomly chosen integers are coprime is , which is about 61% (see , below).

Two natural numbers and are coprime if and only if the numbers and are coprime. As a generalization of this, following easily from the Euclidean algorithm in base :

A set of integers formula_3 can also be called "coprime" or "setwise coprime" if the greatest common divisor of all the elements of the set is 1. For example, the integers 6, 10, 15 are coprime because 1 is the only positive integer that divides all of them.

If every pair in a set of integers is coprime, then the set is said to be "pairwise coprime" (or "pairwise relatively prime", "mutually coprime" or "mutually relatively prime"). Pairwise coprimality is a stronger condition than setwise coprimality; every pairwise coprime finite set is also setwise coprime, but the reverse is not true. For example, the integers 4, 5, 6 are (setwise) coprime (because the only positive integer dividing "all" of them is 1), but they are not "pairwise" coprime (because ).

The concept of pairwise coprimality is important as a hypothesis in many results in number theory, such as the Chinese remainder theorem.

It is possible for an infinite set of integers to be pairwise coprime. Notable examples include the set of all prime numbers, the set of elements in Sylvester's sequence, and the set of all Fermat numbers.

Two ideals and in a commutative ring are called coprime (or "comaximal") if formula_4 This generalizes Bézout's identity: with this definition, two principal ideals () and () in the ring of integers are coprime if and only if and are coprime. If the ideals and of are coprime, then formula_5 furthermore, if is a third ideal such that contains , then contains . The Chinese remainder theorem can be generalized to any commutative ring, using coprime ideals.

Given two randomly chosen integers and , it is reasonable to ask how likely it is that and are coprime. In this determination, it is convenient to use the characterization that and are coprime if and only if no prime number divides both of them (see Fundamental theorem of arithmetic).

Informally, the probability that any number is divisible by a prime (or in fact any integer) is for example, every 7th integer is divisible by 7. Hence the probability that two numbers are both divisible by is and the probability that at least one of them is not is Any finite collection of divisibility events associated to distinct primes is mutually independent. For example, in the case of two events, a number is divisible by primes and if and only if it is divisible by ; the latter event has probability If one makes the heuristic assumption that such reasoning can be extended to infinitely many divisibility events, one is led to guess that the probability that two numbers are coprime is given by a product over all primes,

Here refers to the Riemann zeta function, the identity relating the product over primes to is an example of an Euler product, and the evaluation of as is the Basel problem, solved by Leonhard Euler in 1735.

There is no way to choose a positive integer at random so that each positive integer occurs with equal probability, but statements about "randomly chosen integers" such as the ones above can be formalized by using the notion of "natural density". For each positive integer , let be the probability that two randomly chosen numbers in formula_7 are coprime. Although will never equal exactly, with work one can show that in the limit as formula_8 the probability approaches .

More generally, the probability of randomly chosen integers being coprime is 

All pairs of positive coprime numbers (with ) can be arranged in two disjoint complete ternary trees, one tree starting from (for even–odd and odd–even pairs), and the other tree starting from (for odd–odd pairs). The children of each vertex are generated as follows:

This scheme is exhaustive and non-redundant with no invalid members. This can be proved by remarking that, if formula_12 is a coprime pair with formula_13 then 
In all cases formula_21 is a "smaller" coprime pair with formula_22 This process of "computing the father" can stop only if either formula_23 or formula_24 In these cases, coprimality, implies that the pair is either formula_25 or formula_26

In machine design, an even, uniform gear wear is achieved by choosing the tooth counts of the two gears meshing together to be relatively prime. When a 1:1 gear ratio is desired, a gear relatively prime to the two equal-size gears may be inserted between them.

In pre-computer cryptography, some Vernam cipher machines combined several loops of key tape of different lengths. Many rotor machines combine rotors of different numbers of teeth. Such combinations work best when the entire set of lengths are pairwise coprime.

This concept can be extended to other algebraic structures than for example, polynomials whose greatest common divisor is 1 are called coprime polynomials.



Control unit

The control unit (CU) is a component of a computer's central processing unit (CPU) that directs the operation of the processor. A CU typically uses a binary decoder to convert coded instructions into timing and control signals that direct the operation of the other units (memory, arithmetic logic unit and input and output devices, etc.).

Most computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the Von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.

The simplest computers use a multicycle microarchitecture. These were the earliest designs. They are still popular in the very smallest computers, such as the embedded systems that operate machinery.

In a computer, the control unit often steps through the instruction cycle successively. This consists of fetching the instruction, fetching the operands, decoding the instruction, executing the instruction, and then writing the results back to memory. When the next instruction is placed in the control unit, it changes the behavior of the control unit to complete the instruction correctly. So, the bits of the instruction directly control the control unit, which in turn controls the computer.

The control unit may include a binary counter to tell the control unit's logic what step it should do.

Multicycle control units typically use both the rising and falling edges of their square-wave timing clock. They operate a step of their operation on each edge of the timing clock, so that a four-step operation completes in two clock cycles. This doubles the speed of the computer, given the same logic family.

Many computers have two different types of unexpected events. An interrupt occurs because some type of input or output needs software attention in order to operate correctly. An exception is caused by the computer's operation. One crucial difference is that the timing of an interrupt cannot be predicted. Another is that some exceptions (e.g. a memory-not-available exception) can be caused by an instruction that needs to be restarted.

Control units can be designed to handle interrupts in one of two typical ways. If a quick response is most important, a control unit is designed to abandon work to handle the interrupt. In this case, the work in process will be restarted after the last completed instruction. If the computer is to be very inexpensive, very simple, very reliable, or to get more work done, the control unit will finish the work in process before handling the interrupt. Finishing the work is inexpensive, because it needs no register to record the last finished instruction. It is simple and reliable because it has the fewest states. It also wastes the least amount of work.

Exceptions can be made to operate like interrupts in very simple computers. If virtual memory is required, then a memory-not-available exception must retry the failing instruction.

It is common for multicycle computers to use more cycles. Sometimes it takes longer to take a conditional jump, because the program counter has to be reloaded. Sometimes they do multiplication or division instructions by a process, something like binary long multiplication and division. Very small computers might do arithmetic, one or a few bits at a time. Some other computers have very complex instructions that take many steps.

Many medium-complexity computers pipeline instructions. This design is popular because of its economy and speed.

In a pipelined computer, instructions flow through the computer. This design has several stages. For example, it might have one stage for each step of the Von Neumann cycle. A pipelined computer usually has "pipeline registers" after each stage. These store the bits calculated by a stage so that the logic gates of the next stage can use the bits to do the next step.

It is common for even numbered stages to operate on one edge of the square-wave clock, while odd-numbered stages operate on the other edge. This speeds the computer by a factor of two compared to single-edge designs.

In a pipelined computer, the control unit arranges for the flow to start, continue, and stop as a program commands. The instruction data is usually passed in pipeline registers from one stage to the next, with a somewhat separated piece of control logic for each stage. The control unit also assures that the instruction in each stage does not harm the operation of instructions in other stages. For example, if two stages must use the same piece of data, the control logic assures that the uses are done in the correct sequence.

When operating efficiently, a pipelined computer will have an instruction in each stage. It is then working on all of those instructions at the same time. It can finish about one instruction for each cycle of its clock. When a program makes a decision, and switches to a different sequence of instructions, the pipeline sometimes must discard the data in process and restart. This is called a "stall." When two instructions could interfere, sometimes the control unit must stop processing a later instruction until an earlier instruction completes. This is called a "pipeline bubble" because a part of the pipeline is not processing instructions. Pipeline bubbles can occur when two instructions operate on the same register.

Interrupts and unexpected exceptions also stall the pipeline. If a pipelined computer abandons work for an interrupt, more work is lost than in a multicycle computer. Predictable exceptions do not need to stall. For example, if an exception instruction is used to enter the operating system, it does not cause a stall.

Speed? For the same speed of electronic logic, it can do more instructions per second than a multicycle computer. Also, even though the electronic logic has a fixed maximum speed, a pipelined computer can be made faster or slower by varying the number of stages in the pipeline. With more stages, each stage does less work, and so the stage has fewer delays from the logic gates.

Economy? A pipelined model of a computer often has the least logic gates per instruction per second, less than either a multicycle or out-of-order computer. Why? The average stage is less complex than a multicycle computer. An out-of-order computer usually has large amounts of idle logic at any given instant. Similar calculations usually show that a pipelined computer uses less energy per instruction.

However, a pipelined computer is usually more complex and more costly than a comparable multicycle computer. It typically has more logic gates, registers and a more complex control unit. In a like way, it might use more total energy, while using less energy per instruction. Out-of-order CPUs can usually do more instructions per second because they can do several instructions at once.

Control units use many methods to keep a pipeline full and avoid stalls. For example, even simple control units can assume that a backwards branch, to a lower-numbered, earlier instruction, is a loop, and will be repeated. So, a control unit with this design will always fill the pipeline with the backwards branch path. If a compiler can detect the most frequently-taken direction of a branch, the compiler can just produce instructions so that the most frequently taken branch is the preferred direction of branch. In a like way, a control unit might get hints from the compiler: Some computers have instructions that can encode hints from the compiler about the direction of branch.

Some control units do branch prediction: A control unit keeps an electronic list of the recent branches, encoded by the address of the branch instruction. This list has a few bits for each branch to remember the direction that was taken most recently.

Some control units can do speculative execution, in which a computer might have two or more pipelines, calculate both directions of a branch, and then discard the calculations of the unused direction.

Results from memory can become available at unpredictable times because very fast computers cache memory. That is, they copy limited amounts of memory data into very fast memory. The CPU must be designed to process at the very fast speed of the cache memory. Therefore, the CPU might stall when it must access main memory directly. In modern PCs, main memory is as much as three hundred times slower than cache.

To help this, out-of-order CPUs and control units were developed to process data as it becomes available. (See next section)

But what if all the calculations are complete, but the CPU is still stalled, waiting for main memory? Then, a control unit can switch to an alternative thread of execution whose data has been fetched while the thread was idle. A thread has its own program counter, a stream of instructions and a separate set of registers. Designers vary the number of threads depending on current memory technologies and the type of computer. Typical computers such as PCs and smart phones usually have control units with a few threads, just enough to keep busy with affordable memory systems. Database computers often have about twice as many threads, to keep their much larger memories busy. Graphic processing units (GPUs) usually have hundreds or thousands of threads, because they have hundreds or thousands of execution units doing repetitive graphic calculations.

When a control unit permits threads, the software also has to be designed to handle them. In general-purpose CPUs like PCs and smartphones, the threads are usually made to look very like normal time-sliced processes. At most, the operating system might need some awareness of them. In GPUs, the thread scheduling usually cannot be hidden from the application software, and is often controlled with a specialized subroutine library.

A control unit can be designed to finish what it can. If several instructions can be completed at the same time, the control unit will arrange it. So, the fastest computers can process instructions in a sequence that can vary somewhat, depending on when the operands or instruction destinations become available. Most supercomputers and many PC CPUs use this method. The exact organization of this type of control unit depends on the slowest part of the computer.

When the execution of calculations is the slowest, instructions flow from memory into pieces of electronics called "issue units." An issue unit holds an instruction until both its operands and an execution unit are available. Then, the instruction and its operands are "issued" to an execution unit. The execution unit does the instruction. Then the resulting data is moved into a queue of data to be written back to memory or registers. If the computer has multiple execution units, it can usually do several instructions per clock cycle.

It is common to have specialized execution units. For example, a modestly priced computer might have only one floating-point execution unit, because floating point units are expensive. The same computer might have several integer units, because these are relatively inexpensive, and can do the bulk of instructions.
One kind of control unit for issuing uses an array of electronic logic, a "scoreboard"" that detects when an instruction can be issued. The "height" of the array is the number of execution units, and the "length" and "width" are each the number of sources of operands. When all the items come together, the signals from the operands and execution unit will cross. The logic at this intersection detects that the instruction can work, so the instruction is "issued" to the free execution unit. An alternative style of issuing control unit implements the Tomasulo algorithm, which reorders a hardware queue of instructions. In some sense, both styles utilize a queue. The scoreboard is an alternative way to encode and reorder a queue of instructions, and some designers call it a queue table.

With some additional logic, a scoreboard can compactly combine execution reordering, register renaming and precise exceptions and interrupts. Further it can do this without the power-hungry, complex content-addressable memory used by the Tomasulo algorithm.

If the execution is slower than writing the results, the memory write-back queue always has free entries. But what if the memory writes slowly? Or what if the destination register will be used by an "earlier" instruction that has not yet issued? Then the write-back step of the instruction might need to be scheduled. This is sometimes called "retiring" an instruction. In this case, there must be scheduling logic on the back end of execution units. It schedules access to the registers or memory that will get the results.

Retiring logic can also be designed into an issuing scoreboard or a Tomasulo queue, by including memory or register access in the issuing logic.

Out of order controllers require special design features to handle interrupts. When there are several instructions in progress, it is not clear where in the instruction stream an interrupt occurs. For input and output interrupts, almost any solution works. However, when a computer has virtual memory, an interrupt occurs to indicate that a memory access failed. This memory access must be associated with an exact instruction and an exact processor state, so that the processor's state can be saved and restored by the interrupt. A usual solution preserves copies of registers until a memory access completes.

Also, out of order CPUs have even more problems with stalls from branching, because they can complete several instructions per clock cycle, and usually have many instructions in various stages of progress. So, these control units might use all of the solutions used by pipelined processors.

Some computers translate each single instruction into a sequence of simpler instructions. The advantage is that an out of order computer can be simpler in the bulk of its logic, while handling complex multi-step instructions. x86 Intel CPUs since the Pentium Pro translate complex CISC x86 instructions to more RISC-like internal micro-operations.

In these, the "front" of the control unit manages the translation of instructions. Operands are not translated. The "back" of the CU is an out-of-order CPU that issues the micro-operations and operands to the execution units and data paths.

Many modern computers have controls that minimize power usage. In battery-powered computers, such as those in cell-phones, the advantage is longer battery life. In computers with utility power, the justification is to reduce the cost of power, cooling or noise.

Most modern computers use CMOS logic. CMOS wastes power in two common ways: By changing state, i.e. "active power," and by unintended leakage. The active power of a computer can be reduced by turning off control signals. Leakage current can be reduced by reducing the electrical pressure, the voltage, making the transistors with larger depletion regions or turning off the logic completely.

Active power is easier to reduce because data stored in the logic is not affected. The usual method reduces the CPU's clock rate. Most computer systems use this method. It is common for a CPU to idle during the transition to avoid side-effects from the changing clock.

Most computers also have a "halt" instruction. This was invented to stop non-interrupt code so that interrupt code has reliable timing. However, designers soon noticed that a halt instruction was also a good time to turn off a CPU's clock completely, reducing the CPU's active power to zero. The interrupt controller might continue to need a clock, but that usually uses much less power than the CPU.

These methods are relatively easy to design, and became so common that others were invented for commercial advantage. Many modern low-power CMOS CPUs stop and start specialized execution units and bus interfaces depending on the needed instruction. Some computers even arrange the CPU's microarchitecture to use transfer-triggered multiplexers so that each instruction only utilises the exact pieces of logic needed.

One common method is to spread the load to many CPUs, and turn off unused CPUs as the load reduces. The operating system's task switching logic saves the CPUs' data to memory. In some cases, one of the CPUs can be simpler and smaller, literally with fewer logic gates. So, it has low leakage, and it is the last to be turned off, and the first to be turned on. Also it then is the only CPU that requires special low-power features. A similar method is used in most PCs, which usually have an auxiliary embedded CPU that manages the power system. However, in PCs, the software is usually in the BIOS, not the operating system.

Theoretically, computers at lower clock speeds could also reduce leakage by reducing the voltage of the power supply. This affects the reliability of the computer in many ways, so the engineering is expensive, and it is uncommon except in relatively expensive computers such as PCs or cellphones.

Some designs can use very low leakage transistors, but these usually add cost. The depletion barriers of the transistors can be made larger to have less leakage, but this makes the transistor larger and thus both slower and more expensive. Some vendors use this technique in selected portions of an IC by constructing low leakage logic from large transistors that some processes provide for analog circuits. Some processes place the transistors above the surface of the silicon, in "fin fets", but these processes have more steps, so are more expensive. Special transistor doping materials (e.g. hafnium) can also reduce leakage, but this adds steps to the processing, making it more expensive. Some semiconductors have a larger band-gap than silicon. However, these materials and processes are currently (2020) more expensive than silicon.

Managing leakage is more difficult, because before the logic can be turned-off, the data in it must be moved to some type of low-leakage storage.

Some CPUs make use of a special type of flip-flop (to store a bit) that couples a fast, high-leakage storage cell to a slow, large (expensive) low-leakage cell. These two cells have separated power supplies. When the CPU enters a power saving mode (e.g. because of a halt that waits for an interrupt), data is transferred to the low-leakage cells, and the others are turned off. When the CPU leaves a low-leakage mode (e.g. because of an interrupt), the process is reversed.

Older designs would copy the CPU state to memory, or even disk, sometimes with specialized software. Very simple embedded systems sometimes just restart.

All modern CPUs have control logic to attach the CPU to the rest of the computer. In modern computers, this is usually a bus controller. When an instruction reads or writes memory, the control unit either controls the bus directly, or controls a bus controller. Many modern computers use the same bus interface for memory, input and output. This is called "memory-mapped I/O". To a programmer, the registers of the I/O devices appear as numbers at specific memory addresses. x86 PCs use an older method, a separate I/O bus accessed by I/O instructions.

A modern CPU also tends to include an interrupt controller. It handles interrupt signals from the system bus. The control unit is the part of the computer that responds to the interrupts.

There is often a cache controller to cache memory. The cache controller and the associated cache memory is often the largest physical part of a modern, higher-performance CPU. When the memory, bus or cache is shared with other CPUs, the control logic must communicate with them to assure that no computer ever gets out-of-date old data.

Many historic computers built some type of input and output directly into the control unit. For example, many historic computers had a front panel with switches and lights directly controlled by the control unit. These let a programmer directly enter a program and debug it. In later production computers, the most common use of a front panel was to enter a small bootstrap program to read the operating system from disk. This was annoying. So, front panels were replaced by bootstrap programs in read-only memory.

Most PDP-8 models had a data bus designed to let I/O devices borrow the control unit's memory read and write logic. This reduced the complexity and expense of high speed I/O controllers, e.g. for disk.

The Xerox Alto had a multitasking microprogrammable control unit that performed almost all I/O. This design provided most of the features of a modern PC with only a tiny fraction of the electronic logic. The dual-thread computer was run by the two lowest-priority microthreads. These performed calculations whenever I/O was not required. High priority microthreads provided (in decreasing priority) video, network, disk, a periodic timer, mouse, and keyboard. The microprogram did the complex logic of the I/O device, as well as the logic to integrate the device with the computer. For the actual hardware I/O, the microprogram read and wrote shift registers for most I/O, sometimes with resistor networks and transistors to shift output voltage levels (e.g. for video). To handle outside events, the microcontroller had microinterrupts to switch threads at the end of a thread's cycle, e.g. at the end of an instruction, or after a shift-register was accessed. The microprogram could be rewritten and reinstalled, which was very useful for a research computer.

Thus a program of instructions in memory will cause the CU to configure a CPU's data flows to manipulate the data correctly between instructions. This results in a computer that could run a complete program and require no human intervention to make hardware changes between instructions (as had to be done when using only punch cards for computations before stored programmed computers with CUs were invented).

Hardwired control units are implemented through use of combinational logic units, featuring a finite number of gates that can generate specific results based on the instructions that were used to invoke those responses. Hardwired control units are generally faster than the microprogrammed designs.

This design uses a fixed architecture—it requires changes in the wiring if the instruction set is modified or changed. It can be convenient for simple, fast computers.

A controller that uses this approach can operate at high speed; however, it has little flexibility. A complex instruction set can overwhelm a designer who uses ad hoc logic design.

The hardwired approach has become less popular as computers have evolved. Previously, control units for CPUs used ad hoc logic, and they were difficult to design.

The idea of microprogramming was introduced by Maurice Wilkes in 1951 as an intermediate level to execute computer program instructions. Microprograms were organized as a sequence of "microinstructions" and stored in special control memory. The algorithm for the microprogram control unit, unlike the hardwired control unit, is usually specified by flowchart description. The main advantage of a microprogrammed control unit is the simplicity of its structure. Outputs from the controller are by microinstructions. The microprogram can be debugged and replaced similarly to software.

A popular variation on microcode is to debug the microcode using a software simulator. Then, the microcode is a table of bits. This is a logical truth table, that translates a microcode address into the control unit outputs. This truth table can be fed to a computer program that produces optimized electronic logic. The resulting control unit is almost as easy to design as microprogramming, but it has the fast speed and low number of logic elements of a hard wired control unit. The practical result resembles a Mealy machine or Richards controller.


Cello

The cello ( ), or violoncello ( , ), is a bowed (sometimes plucked and occasionally hit) string instrument of the violin family. Its four strings are usually tuned in perfect fifths: from low to high, C, G, D and A. The viola's four strings are each an octave higher. Music for the cello is generally written in the bass clef, with tenor clef, and treble clef used for higher-range passages.

Played by a "cellist" or "violoncellist", it enjoys a large solo repertoire with and without accompaniment, as well as numerous concerti. As a solo instrument, the cello uses its whole range, from bass to soprano, and in chamber music such as string quartets and the orchestra's string section, it often plays the bass part, where it may be reinforced an octave lower by the double basses. Figured bass music of the Baroque era typically assumes a cello, viola da gamba or bassoon as part of the basso continuo group alongside chordal instruments such as organ, harpsichord, lute, or theorbo. Cellos are found in many other ensembles, from modern Chinese orchestras to cello rock bands.

The name "cello" is derived from the ending of the Italian "violoncello", which means "little violone". Violone ("big viola") was a large-sized member of viol (viola da gamba) family or the violin (viola da braccio) family. The term "violone" today usually refers to the lowest-pitched instrument of the viols, a family of stringed instruments that went out of fashion around the end of the 17th century in most countries except England and, especially, France, where they survived another half-century before the louder violin family came into greater favour in that country as well. In modern symphony orchestras, it is the second largest stringed instrument (the double bass is the largest). Thus, the name "violoncello" contained both the augmentative ""-one"" ("big") and the diminutive ""-cello"" ("little"). By the turn of the 20th century, it had become common to shorten the name to 'cello, with the apostrophe indicating the missing stem. It is now customary to use "cello" without apostrophe as the full designation. "Viol" is derived from the root "viola", which was derived from Medieval Latin , meaning stringed instrument.

Cellos are tuned in fifths, starting with C (two octaves below middle C), followed by G, D, and then A. It is tuned in the exact same intervals and strings as the viola, but an octave lower. Similar to the double bass, the cello has an endpin that rests on the floor to support the instrument's weight. The cello is most closely associated with European classical music. The instrument is a part of the standard orchestra, as part of the string section, and is the bass voice of the string quartet (although many composers give it a melodic role as well), as well as being part of many other chamber groups.

Among the most well-known Baroque works for the cello are Johann Sebastian Bach's six unaccompanied Suites. Other significant works include Sonatas and Concertos by Antonio Vivaldi, and solo sonatas by Francesco Geminiani and Giovanni Bononcini. Domenico Gabrielli was one of the first composers to treat the cello as a solo instrument. As a basso continuo instrument the cello may have been used in works by Francesca Caccini (1587–1641), Barbara Strozzi (1619–1677) with pieces such as "Il primo libro di madrigali, per 2–5 voci e basso continuo, op. 1" and Elisabeth Jacquet de La Guerre (1665–1729) who wrote six sonatas for violin and basso continuo. The earliest known manual for learning the cello, Francesco Supriani's "Principij da imparare a suonare il violoncello e con 12 Toccate a solo" (before 1753), dates from this era. As the title of the work suggests, it contains 12 toccatas for solo cello, which along with Johann Sebastian Bach's Cello Suites, are some of the first works of that type. 

From the Classical era, the two concertos by Joseph Haydn in C major and D major stand out, as do the five sonatas for cello and pianoforte of Ludwig van Beethoven, which span the important three periods of his compositional evolution. Other outstanding examples include the three Concerti by Carl Philipp Emanuel Bach, Capricci by dall'Abaco, and Sonatas by Flackton, Boismortier, and Luigi Boccherini. A "Divertimento for Piano, Clarinet, Viola and Cello" is among the surviving works by Duchess Anna Amalia of Brunswick-Wolfenbüttel (1739–1807). Wolfgang Amadeus Mozart supposedly wrote a Cello Concerto in F major, K. 206a in 1775, but this has since been lost. His Sinfonia Concertante in A major, K. 320e includes a solo part for cello, along with the violin and viola, although this work is incomplete and only exists in fragments, therefore it is given an Anhang number (Anh. 104).

Well-known works of the Romantic era include the Robert Schumann Concerto, the Antonín Dvořák Concerto, the first Camille Saint-Saëns Concerto, as well as the two sonatas and the Double Concerto by Johannes Brahms. A review of compositions for cello in the Romantic era must include the German composer Fanny Mendelssohn (1805–1847) who wrote Fantasia in G Minor for cello and piano and a Capriccio in A-flat for cello. 

Compositions from the late-19th and early 20th century include three cello sonatas (including the Cello Sonata in C Minor written in 1880) by Dame Ethel Smyth (1858–1944), Edward Elgar's Cello Concerto in E minor, Claude Debussy's Sonata for Cello and Piano, and unaccompanied cello sonatas by Zoltán Kodály and Paul Hindemith. Pieces including cello were written by American Music Center founder Marion Bauer (1882–1955) (two trio sonatas for flute, cello, and piano) and Ruth Crawford Seeger (1901–1953) (Diaphonic suite No. 2 for bassoon and cello).

The cello's versatility made it popular with many composers in this era, such as Sergei Prokofiev, Dmitri Shostakovich, Benjamin Britten, György Ligeti, Witold Lutoslawski and Henri Dutilleux. Polish composer Grażyna Bacewicz (1909–1969) was writing for cello in the mid 20th century with Concerto No. 1 for Cello and Orchestra (1951), Concerto No. 2 for Cello and Orchestra (1963) and in 1964 composed her Quartet for four cellos. 

In the 2010s, the instrument is found in popular music, but was more commonly used in 1970s pop and disco music. Today it is sometimes featured in pop and rock recordings, examples of which are noted later in this article. The cello has also appeared in major hip-hop and R & B performances, such as singers Rihanna and Ne-Yo's 2007 performance at the American Music Awards. The instrument has also been modified for Indian classical music by Nancy Lesh and Saskia Rao-de Haas.

The violin family, including cello-sized instruments, emerged as a family of instruments distinct from the viola da gamba family. The earliest depictions of the violin family, from Italy , show three sizes of instruments, roughly corresponding to what we now call violins, violas, and cellos. Contrary to a popular misconception, the cello did not evolve from the viola da gamba, but existed alongside it for about two and a half centuries. The violin family is also known as the viola da braccio (meaning viola for the arm) family, a reference to the primary way the members of the family are held. This is to distinguish it from the viola da gamba (meaning viola for the leg) family, in which all the members are all held with the legs. The likely predecessors of the violin family include the lira da braccio and the rebec. The earliest surviving cellos are made by Andrea Amati, the first known member of the celebrated Amati family of luthiers.

The direct ancestor to the violoncello was the bass violin. Monteverdi referred to the instrument as "basso de viola da braccio" in "Orfeo" (1607). Although the first bass violin, possibly invented as early as 1538, was most likely inspired by the viol, it was created to be used in consort with the violin. The bass violin was actually often referred to as a ""violone"", or "large viola", as were the viols of the same period. Instruments that share features with both the bass violin and the "viola da gamba" appear in Italian art of the early 16th century.

The invention of wire-wound strings (fine wire around a thin gut core), in Bologna, allowed for a finer bass sound than was possible with purely gut strings on such a short body. Bolognese makers exploited this new technology to create the cello, a somewhat smaller instrument suitable for solo repertoire due to both the timbre of the instrument and the fact that the smaller size made it easier to play virtuosic passages. This instrument had disadvantages as well, however. The cello's light sound was not as suitable for church and ensemble playing, so it had to be doubled by organ, theorbo, or violone.

Around 1700, Italian players popularized the cello in northern Europe, although the bass violin (basse de violon) continued to be used for another two decades in France. Many existing bass violins were literally cut down in size to convert them into cellos according to the smaller pattern developed by Stradivarius, who also made a number of old pattern large cellos (the 'Servais'). The sizes, names, and tunings of the cello varied widely by geography and time. The size was not standardized until .

Despite similarities to the viola da gamba, the cello is actually part of the viola da braccio family, meaning "viol of the arm", which includes, among others, the violin and viola. Though paintings like Bruegel's "The Rustic Wedding", and Jambe de Fer in his "Epitome Musical" suggest that the bass violin had alternate playing positions, these were short-lived and the more practical and ergonomic "a gamba" position eventually replaced them entirely.

Baroque-era cellos differed from the modern instrument in several ways. The neck has a different form and angle, which matches the baroque bass-bar and stringing. The fingerboard is usually shorter than that of the modern cello, as the highest notes are not often called for in baroque music. Modern cellos have an endpin at the bottom to support the instrument (and transmit some of the sound through the floor), while Baroque cellos are held only by the calves of the player. Modern bows curve in and are held at the frog; Baroque bows curve out and are held closer to the bow's point of balance. Modern strings normally have a metal core, although some use a synthetic core; Baroque strings are made of gut, with the G and C strings wire-wound. Modern cellos often have fine tuners connecting the strings to the tailpiece, which makes it much easier to tune the instrument, but such pins are rendered ineffective by the flexibility of the gut strings used on Baroque cellos. Overall, the modern instrument has much higher string tension than the Baroque cello, resulting in a louder, more projecting tone, with fewer overtones. In addition, the instrument was less standardized in size and number of strings; a smaller, five-string variant (the violoncello piccolo) was commonly used as a solo instrument and five-string instruments are occasionally specified in the Baroque repertoire.

Few educational works specifically devoted to the cello existed before the 18th century and those that do exist contain little value to the performer beyond simple accounts of instrumental technique. One of the earliest cello manuals is Michel Corrette's "Méthode, thèorique et pratique pour apprendre en peu de temps le violoncelle dans sa perfection" (Paris, 1741).

Cellos are part of the standard symphony orchestra, which usually includes eight to twelve cellists. The cello section, in standard orchestral seating, is located on stage left (the audience's right) in the front, opposite the first violin section. However, some orchestras and conductors prefer switching the positioning of the viola and cello sections. The "principal" cellist is the section leader, determining bowings for the section in conjunction with other string principals, playing solos, and leading entrances (when the section begins to play its part). Principal players always sit closest to the audience.

The cellos are a critical part of orchestral music; all symphonic works involve the cello section, and many pieces require cello soli or solos. Much of the time, cellos provide part of the low-register harmony for the orchestra. Often, the cello section plays the melody for a brief period, before returning to the harmony role. There are also cello concertos, which are orchestral pieces that feature a solo cellist accompanied by an entire orchestra.

There are numerous cello concertos – where a solo cello is accompanied by an orchestra – notably 25 by Vivaldi, 12 by Boccherini, at least three by Haydn, three by C. P. E. Bach, two by Saint-Saëns, two by Dvořák, and one each by Robert Schumann, Lalo, and Elgar. There were also some composers who, while not otherwise cellists, did write cello-specific repertoire, such as Nikolaus Kraft who wrote six cello concertos. Beethoven's Triple Concerto for Cello, Violin and Piano and Brahms' Double Concerto for Cello and Violin are also part of the concertante repertoire although in both cases the cello shares solo duties with at least one other instrument. Moreover, several composers wrote large-scale pieces for cello and orchestra, which are concertos in all but name. Some familiar "concertos" are Richard Strauss' tone poem "Don Quixote", Tchaikovsky's "Variations on a Rococo Theme", Bloch's "Schelomo" and Bruch's "Kol Nidrei".

In the 20th century, the cello repertoire grew immensely. This was partly due to the influence of virtuoso cellist Mstislav Rostropovich, who inspired, commissioned, and premiered dozens of new works. Among these, Prokofiev's "Symphony-Concerto", Britten's "Cello Symphony", the concertos of Shostakovich and Lutosławski as well as Dutilleux's "Tout un monde lointain..." have already become part of the standard repertoire. Other major composers who wrote concertante works for him include Messiaen, Jolivet, Berio, and Penderecki. In addition, Arnold, Barber, Glass, Hindemith, Honegger, Ligeti, Myaskovsky, Penderecki, Rodrigo, Villa-Lobos and Walton also wrote major concertos for other cellists, notably for Gaspar Cassadó, Aldo Parisot, Gregor Piatigorsky, Siegfried Palm and Julian Lloyd Webber.

There are also many sonatas for cello and piano. Those written by Beethoven, Mendelssohn, Chopin, Brahms, Grieg, Rachmaninoff, Debussy, Fauré, Shostakovich, Prokofiev, Poulenc, Carter, and Britten are particularly well known.

Other important pieces for cello and piano include Schumann's five "Stücke im Volkston" and transcriptions like Schubert's "Arpeggione Sonata" (originally for arpeggione and piano), César Franck's Cello Sonata (originally a violin sonata, transcribed by Jules Delsart with the composer's approval), Stravinsky's "Suite italienne" (transcribed by the composer – with Gregor Piatigorsky – from his ballet "Pulcinella") and Bartók's first rhapsody (also transcribed by the composer, originally for violin and piano).

There are pieces for cello solo, Johann Sebastian Bach's six Suites for Cello (which are among the best-known solo cello pieces), Kodály's Sonata for Solo Cello and Britten's three Cello Suites. Other notable examples include Hindemith's and Ysaÿe's Sonatas for Solo Cello, Dutilleux's "Trois Strophes sur le Nom de Sacher", Berio's "Les Mots Sont Allés", Cassadó's Suite for Solo Cello, Ligeti's Solo Sonata, Carter's two "Figment"s and Xenakis' "Nomos Alpha" and "Kottos".

There are also modern solo pieces written for cello. Such as Julie-O by Mark Summer.

The cello is a member of the traditional string quartet as well as string quintets, sextet or trios and other mixed ensembles.
There are also pieces written for two, three, four, or more cellos; this type of ensemble is also called a "cello choir" and its sound is familiar from the introduction to Rossini's William Tell Overture as well as Zaccharia's prayer scene in Verdi's Nabucco. Tchaikovsky's 1812 Overture also starts with a cello ensemble, with four cellos playing the top lines and two violas playing the bass lines. As a self-sufficient ensemble, its most famous repertoire is Heitor Villa-Lobos' first of his Bachianas Brasileiras for cello ensemble (the fifth is for soprano and 8 cellos). Other examples are Offenbach's cello duets, quartet, and sextet, Pärt's Fratres for eight cellos and Boulez' "Messagesquisse" for seven cellos, or even Villa-Lobos' rarely played "Fantasia Concertante" (1958) for 32 cellos. The 12 cellists of the Berlin Philharmonic Orchestra (or "the Twelve" as they have since taken to being called) specialize in this repertoire and have commissioned many works, including arrangements of well-known popular songs.

The cello is less common in popular music than in classical music. Several bands feature a cello in their standard line-up, including Hoppy Jones of the Ink Spots and Joe Kwon of the Avett Brothers. The more common use in pop and rock is to bring the instrument in for a particular song. In the 1960s, artists such as the Beatles and Cher used the cello in popular music, in songs such as The Beatles' "Yesterday", "Eleanor Rigby" and "Strawberry Fields Forever", and Cher's "Bang Bang (My Baby Shot Me Down)". "Good Vibrations" by the Beach Boys includes the cello in its instrumental ensemble, which includes a number of instruments unusual for this sort of music. Bass guitarist Jack Bruce, who had originally studied music on a performance scholarship for cello, played a prominent cello part in "As You Said" on Cream's "Wheels of Fire" studio album (1968).

In the 1970s, the Electric Light Orchestra enjoyed great commercial success taking inspiration from so-called "Beatlesque" arrangements, adding the cello (and violin) to the standard rock combo line-up and in 1978 the UK based rock band, Colosseum II, collaborated with cellist Julian Lloyd Webber on the recording "Variations". Most notably, Pink Floyd included a cello solo in their 1970 epic instrumental "Atom Heart Mother". Bass guitarist Mike Rutherford of Genesis was originally a cellist and included some cello parts in their "Foxtrot" album.

Established non-traditional cello groups include Apocalyptica, a group of Finnish cellists best known for their versions of Metallica songs; Rasputina, a group of cellists committed to an intricate cello style intermingled with Gothic music; the Massive Violins, an ensemble of seven singing cellists known for their arrangements of rock, pop and classical hits; Von Cello, a cello-fronted rock power trio; Break of Reality, who mix elements of classical music with the more modern rock and metal genre; Cello Fury, a cello rock band that performs original rock/classical crossover music; and Jelloslave, a Minneapolis-based cello duo with two percussionists. These groups are examples of a style that has become known as cello rock. The crossover string quartet Bond also includes a cellist. Silenzium and Cellissimo Quartet are Russian (Novosibirsk) groups playing rock and metal and having more and more popularity in Siberia. Cold Fairyland from Shanghai, China is using a cello along with a pipa as the main solo instrument to create East meets West progressive (folk) rock.

More recent bands who have used the cello include Clean Bandit, Aerosmith, The Auteurs, Nirvana, Oasis, Ra Ra Riot, Smashing Pumpkins, James, Talk Talk, Phillip Phillips, OneRepublic, Electric Light Orchestra and the baroque rock band Arcade Fire. An Atlanta-based trio, King Richard's Sunday Best, also uses a cellist in their lineup. So-called "chamber pop" artists like Kronos Quartet, The Vitamin String Quartet and Margot and the Nuclear So and So's have also recently made cello common in modern alternative rock. Heavy metal band System of a Down has also made use of the cello's rich sound. The indie rock band The Stiletto Formal are known for using a cello as a major staple of their sound; similarly, the indie rock band Canada employs two cello players in their lineup. The orch-rock group The Polyphonic Spree, which has pioneered the use of stringed and symphonic instruments, employs the cello in creative ways for many of their "psychedelic-esque" melodies. The first-wave screamo band I Would Set Myself On Fire For You featured a cello as well as a viola to create a more folk-oriented sound. The band Panic! at the Disco uses a cello in their song "Build God, Then We'll Talk", with lead vocalist Brendon Urie recording the cello solo himself. The Lumineers added cellist Nela Pekarek to the band in 2010. Radiohead makes frequent use of cello in their music, notably for the songs "Burn The Witch" and "Glass Eyes" in 2016.

In jazz, bassists Oscar Pettiford and Harry Babasin were among the first to use the cello as a solo instrument; both tuned their instruments in fourths, an octave above the double bass. Fred Katz (who was not a bassist) was one of the first notable jazz cellists to use the instrument's standard tuning and arco technique. Contemporary jazz cellists include Abdul Wadud, Diedre Murray, Ron Carter, Dave Holland, David Darling, Lucio Amanti, Akua Dixon, Ernst Reijseger, Fred Lonberg-Holm, Tom Cora and Erik Friedlander. Modern musical theatre pieces like Jason Robert Brown's "The Last Five Years", Duncan Sheik's "Spring Awakening", Adam Guettel's "Floyd Collins", and Ricky Ian Gordon's "My Life with Albertine" use small string ensembles (including solo cellos) to a prominent extent.

In Indian classical music, Saskia Rao-de Haas is a well-established soloist as well as playing duets with her sitarist husband, Pt. Shubhendra Rao. Other cellists performing Indian classical music are Nancy Lesh (Dhrupad) and Anup Biswas. Both Rao and Lesh play the cello sitting cross-legged on the floor.

The cello can also be used in bluegrass and folk music, with notable players including Ben Sollee of the Sparrow Quartet and the "Cajun cellist" Sean Grissom, as well as Vyvienne Long, who, in addition to her own projects, has played for those of Damien Rice. Cellists such as Natalie Haas, Abby Newton, and Liz Davis Maxfield have contributed significantly to the use of cello playing in Celtic folk music, often with the cello featured as a primary melodic instrument and employing the skills and techniques of traditional fiddle playing. Lindsay Mac is becoming well known for playing the cello like a guitar, with her cover of The Beatles' "Blackbird".

The cello is typically made from carved wood, although other materials such as carbon fiber or aluminum may be used. A traditional cello has a spruce top, with maple for the back, sides, and neck. Other woods, such as poplar or willow, are sometimes used for the back and sides. Less expensive cellos frequently have tops and backs made of laminated wood. Laminated cellos are widely used in elementary and secondary school orchestras and youth orchestras, because they are much more durable than carved wood cellos (i.e., they are less likely to crack if bumped or dropped) and they are much less expensive.

The top and back are traditionally hand-carved, though less expensive cellos are often machine-produced. The sides, or ribs, are made by heating the wood and bending it around forms. The cello body has a wide top bout, narrow middle formed by two C-bouts, and wide bottom bout, with the bridge and F holes just below the middle. The top and back of the cello have a decorative border inlay known as purfling. While purfling is attractive, it is also functional: if the instrument is struck, the purfling can prevent cracking of the wood. A crack may form at the rim of the instrument but spreads no further. Without purfling, cracks can spread up or down the top or back. Playing, traveling and the weather all affect the cello and can increase a crack if purfling is not in place. Less expensive instruments typically have painted purfling.

The fingerboard and pegs on a cello are generally made from ebony, as it is strong and does not wear out easily.

In the late 1920s and early 1930s, the Aluminum Company of America (Alcoa) as well as German luthier G.A. Pfretzschner produced an unknown number of aluminum cellos (in addition to aluminum double basses and violins). Cello manufacturer Luis & Clark constructs cellos from carbon fibre. Carbon fibre instruments are particularly suitable for outdoor playing because of the strength of the material and its resistance to humidity and temperature fluctuations. Luis & Clark has produced over 1000 cellos, some of which are owned by cellists such as Yo-Yo Ma and Josephine van Lier.

Above the main body is the carved neck. The neck has a curved cross-section on its underside, which is where the player's thumb runs along the neck during playing. The neck leads to a pegbox and the scroll, which are all normally carved out of a single piece of wood, usually maple. The fingerboard is glued to the neck and extends over the body of the instrument. The fingerboard is given a curved shape, matching the curve on the bridge. Both the fingerboard and bridge need to be curved so that the performer can bow individual strings. If the cello were to have a flat fingerboard and bridge, as with a typical guitar, the performer would only be able to bow the leftmost and rightmost two strings or bow all the strings. The performer would not be able to play the inner two strings alone.

The nut is a raised piece of wood, fitted where the fingerboard meets the pegbox, in which the strings rest in shallow slots or grooves to keep them the correct distance apart. The pegbox houses four tapered tuning pegs, one for each string. The pegs are used to tune the cello by either tightening or loosening the string. The pegs are called "friction pegs", because they maintain their position by friction. The scroll is a traditional ornamental part of the cello and a feature of all other members of the violin family. Ebony is usually used for the tuning pegs, fingerboard, and nut, but other hardwoods, such as boxwood or rosewood, can be used. Black fittings on low-cost instruments are often made from inexpensive wood that has been blackened or "ebonized" to look like ebony, which is much harder and more expensive. Ebonized parts such as tuning pegs may crack or split, and the black surface of the fingerboard will eventually wear down to reveal the lighter wood underneath.

Historically, cello strings had cores made out of catgut, which, despite its name, is made from sheep or goat intestines. Most modern strings used in the 2010s are wound with metallic materials like aluminum, titanium and chromium. Cellists may mix different types of strings on their instruments. The pitches of the open strings are C, G, D, and A (black note heads in the playing range figure above), unless alternative tuning (scordatura) is specified by the composer. Some composers (e.g. Ottorino Respighi in the final movement of ‘’The Pines of Rome’’) ask that the low C be tuned down to a B-flat so that the performer can play a different low note on the lowest open string.

The tailpiece and endpin are found in the lower part of the cello. The tailpiece is the part of the cello to which the "ball ends" of the strings are attached by passing them through holes. The tailpiece is attached to the bottom of the cello. The tailpiece is traditionally made of ebony or another hardwood, but can also be made of plastic or steel on lower-cost instruments. It attaches the strings to the lower end of the cello and can have one or more fine tuners. The fine tuners are used to make smaller adjustments to the pitch of the string. The fine tuners can increase the tension of each string (raising the pitch) or decrease the tension of the string (lowering the pitch). When the performer is putting on a new string, the fine tuner for that string is normally reset to a middle position, and then the peg is turned to bring the string up to pitch. The fine turners are used for subtle, minor adjustments to pitch, such as tuning a cello to the oboe's 440 Hz A note or tuning the cello to a piano.

The endpin or spike is made of wood, metal, or rigid carbon fiber and supports the cello in playing position. The endpin can be retracted into the hollow body of the instrument when the cello is being transported in its case. This makes the cello easier to move about. When the performer wishes to play the cello, the endpin is pulled out to lengthen it. The endpin is locked into the player's preferred length with a screw mechanism. The adjustable nature of endpins enables performers of different ages and body sizes to adjust the endpin length to suit them. In the Baroque period, the cello was held between the calves, as there was no endpin at that time. The endpin was "introduced by Adrien Servais 1845 to give the instrument greater stability". Modern endpins are retractable and adjustable; older ones were removed when not in use. (The word "endpin" sometimes also refers to the button of wood located at this place in all instruments in the violin family, but this is usually called "tailpin".) The sharp tip of the cello's endpin is sometimes capped with a rubber tip that protects the tip from dulling and prevents the cello from slipping on the floor. Many cellists use a rubber pad with a metal cup to keep the tip from slipping on the floor. A number of accessories exist to keep the endpin from slipping; these include ropes that attach to the chair leg and other devices.

The bridge holds the strings above the cello and transfers their vibrations to the top of the instrument and the soundpost inside (see below). The bridge is not glued but rather held in place by the tension of the strings. The bridge is usually positioned by the cross point of the "f-hole" (i.e., where the horizontal line occurs in the "f"). The f-holes, named for their shape, are located on either side of the bridge and allow air to move in and out of the instrument as part of the sound-production process. The f-holes also act as access points to the interior of the cello for repairs or maintenance. Sometimes a small length of rubber hose containing a water-soaked sponge, called a Dampit, is inserted through the f-holes and serves as a humidifier. This keeps the wood components of the cello from drying out.

Internally, the cello has two important features: a bass bar, which is glued to the underside of the top of the instrument, and a round wooden sound post, a solid wooden cylinder which is wedged between the top and bottom plates. The bass bar, found under the bass foot of the bridge, serves to support the cello's top and distribute the vibrations from the strings to the body of the instrument. The soundpost, found under the treble side of the bridge, connects the back and front of the cello. Like the bridge, the soundpost is not glued but is kept in place by the tensions of the bridge and strings. Together, the bass bar and sound post transfer the strings' vibrations to the top (front) of the instrument (and to a lesser extent the back), acting as a diaphragm to produce the instrument's sound.

Cellos are constructed and repaired using hide glue, which is strong but reversible, allowing for disassembly when needed. Tops may be glued on with diluted glue since some repairs call for the removal of the top. Theoretically, hide glue is weaker than the body's wood, so as the top or back shrinks side-to-side, the glue holding it lets go and the plate does not crack. Cellists repairing cracks in their cello do not use regular wood glue, because it cannot be steamed open when a repair has to be made by a luthier.

Traditionally, bows are made from pernambuco or brazilwood. Both come from the same species of tree ("Caesalpinia echinata"), but Pernambuco, used for higher-quality bows, is the heartwood of the tree and is darker in color than brazilwood (which is sometimes stained to compensate). Pernambuco is a heavy, resinous wood with great elasticity, which makes it an ideal wood for instrument bows. Horsehair is stretched out between the two ends of the bow. The taut horsehair is drawn over the strings, while being held roughly parallel to the bridge and perpendicular to the strings, to produce sound. A small knob is twisted to increase or decrease the tension of the horsehair. The tension on the bow is released when the instrument is not being used. The amount of tension a cellist puts on the bow hair depends on the preferences of the player, the style of music being played, and for students, the preferences of their teacher.

Bows are also made from other materials, such as carbon fibre—stronger than wood—and fiberglass (often used to make inexpensive, lower-quality student bows). An average cello bow is long (shorter than a violin or viola bow) high (from the frog to the stick) and wide. The frog of a cello bow typically has a rounded corner like that of a viola bow, but is wider. A cello bow is roughly heavier than a viola bow, which in turn is roughly heavier than a violin bow. 

Bow hair is traditionally horsehair, though synthetic hair, in varying colors, is also used. Prior to playing, the musician tightens the bow by turning a screw to pull the frog (the part of the bow under the hand) back and increase the tension of the hair. Rosin is applied by the player to make the hair sticky. Bows need to be re-haired periodically. Baroque style (1600–1750) cello bows were much thicker and were formed with a larger outward arch when compared to modern cello bows. The inward arch of a modern cello bow produces greater tension, which in turn gives off a louder sound.

The cello bow has also been used to play electric guitars. Jimmy Page pioneered its application on tracks such as "Dazed and Confused". The post-rock Icelandic band Sigur Rós's lead singer often plays guitar using a cello bow.

In 1989, the German cellist Michael Bach began developing a curved bow, encouraged by John Cage, Dieter Schnebel, Mstislav Rostropovich and Luigi Colani: and since then many pieces have been composed especially for it. This curved bow ("BACH.Bow") is a convex curved bow which, unlike the ordinary bow, renders possible polyphonic playing on the various strings of the instrument. The solo repertoire for violin and cello by J. S. Bach the BACH.Bow is particularly suited to it: and it was developed with this in mind, polyphonic playing being required, as well as monophonic.

When a string is bowed or plucked, it vibrates and moves the air around it, producing sound waves. Because the string is quite thin, not much air is moved by the string itself, and consequently, if the string was not mounted on a hollow body, the sound would be weak. In acoustic stringed instruments such as the cello, this lack of volume is solved by mounting the vibrating string on a larger hollow wooden body. The vibrations are transmitted to the larger body, which can move more air and produce a louder sound. Different designs of the instrument produce variations in the instrument's vibrational patterns and thus change the character of the sound produced. A string's fundamental pitch can be adjusted by changing its stiffness, which depends on tension and length. Tightening a string stiffens it by increasing both the outward forces along its length and the net forces it experiences during a distortion. A cello can be tuned by adjusting the tension of its strings, by turning the tuning pegs mounted on its pegbox and tension adjusters (fine tuners) on the tailpiece.

A string's length also affects its fundamental pitch. Shortening a string stiffens it by increasing its curvature during a distortion and subjecting it to larger net forces. Shortening the string also reduces its mass, but does not alter the mass per unit length, and it is the latter ratio rather than the total mass which governs the frequency. The string vibrates in a standing wave whose speed of propagation is given by formula_1, where is the tension and is the mass per unit length; there is a node at either end of the vibrating length, and thus the vibrating length is half a wavelength. Since the frequency of any wave is equal to the speed divided by the wavelength, we have formula_2. (Some writers, including Muncaster (cited below) use the Greek letter in place of .) Thus shortening a string increases the frequency, and thus the pitch. Because of this effect, you can raise and change the pitch of a string by pressing it against the fingerboard in the cello's neck and effectively shortening it. Likewise strings with less mass per unit length, if under the same tension, will have a higher frequency and thus higher pitch than more massive strings. This is a prime reason why the different strings on all string instruments have different fundamental pitches, with the lightest strings having the highest pitches.

A played note of E or F-sharp has a frequency that is often very close to the natural resonating frequency of the body of the instrument, and if the problem is not addressed this can set the body into near resonance. This may cause an unpleasant sudden amplification of this pitch, and additionally a loud beating sound results from the interference produced between these nearby frequencies; this is known as the “wolf tone” because it is an unpleasant growling sound. The wood resonance appears to be split into two frequencies by the driving force of the sounding string. These two periodic resonances beat with each other. This wolf tone must be eliminated or significantly reduced for the cello to play the nearby notes with a pleasant tone. This can be accomplished by modifying the cello front plate, attaching a wolf eliminator (a metal cylinder or a rubber cylinder encased in metal), or moving the soundpost.

When a string is bowed or plucked to produce a note, the fundamental note is accompanied by higher frequency overtones. Each sound has a particular recipe of frequencies that combine to make the total sound.

Playing the cello is done while seated with the instrument supported on the floor by the endpin. The right hand bows (or sometimes plucks) the strings to sound the notes. The left-hand fingertips stop the strings along their length, determining the pitch of each fingered note. Stopping the string closer to the bridge results in a higher-pitched sound because the vibrating string length has been shortened. On the contrary, a string stopped closer to the tuning pegs produces a lower sound. In the "neck" positions (which use just less than half of the fingerboard, nearest the top of the instrument), the thumb rests on the back of the neck, some people use their thumb as a marker of their position; in "thumb position" (a general name for notes on the remainder of the fingerboard) the thumb usually rests alongside the fingers on the string. Then, the side of the thumb is used to play notes. The fingers are normally held curved with each knuckle bent, with the fingertips in contact with the string. If a finger is required on two (or more) strings at once to play perfect fifths (in double stops or chords), it is used flat. The contact point can move slightly away from the nail to the finger's pad in slower or more expressive playing, allowing a fuller vibrato.

Vibrato is a small oscillation in the pitch of a note, usually considered an expressive technique. The closer towards the bridge the note is, the smaller the oscillation needed to create the effect. Harmonics played on the cello fall into two classes; natural and artificial. Natural harmonics are produced by lightly touching (but not depressing) the string at certain places and then bowing (or, rarely, plucking) the string. For example, the halfway point of the string will produce a harmonic that is one octave above the unfingered (open) string. Natural harmonics only produce notes that are part of the harmonic series on a particular string. Artificial harmonics (also called false harmonics or stopped harmonics), in which the player depresses the string fully with one finger while touching the same string lightly with another finger, can produce any note above middle C. 
Glissando (Italian for "sliding") is an effect achieved by sliding the finger up or down the fingerboard without releasing the string. This causes the pitch to rise and fall smoothly, without separate, discernible steps.

In cello playing, the bow is much like the breath of a wind instrument player. Arguably, it is a major factor in the expressiveness of the playing. The right hand holds the bow and controls the duration and character of the notes. In general, the bow is drawn across the strings roughly halfway between the end of the fingerboard and the bridge, in a direction perpendicular to the strings; however, the player may wish to move the bow's point of contact higher or lower depending on the desired sound. The bow is held and manipulated with all five fingers of the right hand, with the thumb opposite the fingers and closer to the cellist's body. Tone production and volume of sound depend on a combination of several factors. The four most important ones are "weight" applied to the string, the "angle" of the bow on the string, bow "speed", and the "point of contact" of the bow hair with the string (sometimes abbreviated WASP).

Double stops involve the playing of two notes simultaneously. Two strings are fingered at once, and the bow is drawn to sound them both. Often, in pizzicato playing, the string is plucked directly with the fingers or thumb of the right hand. However, the strings may be plucked with a finger of the left hand in certain advanced pieces, either so that the cellist can play bowed notes on another string along with pizzicato notes or because the speed of the piece would not allow the player sufficient time to pluck with the right hand. In musical notation, pizzicato is often abbreviated as "pizz." The position of the hand in pizzicato is commonly slightly over the fingerboard and away from the bridge.

A player using the col legno technique strikes or rubs the strings with the wood of the bow rather than the hair. In spiccato playing, the bow still moves in a horizontal motion on the string but is allowed to bounce, generating a lighter, somewhat more percussive sound. In staccato, the player moves the bow a small distance and stops it on the string, making a short sound, the rest of the written duration being taken up by silence. 
Legato is a technique in which notes are smoothly connected without breaks. It is indicated by a slur (curved line) above or below – depending on their position on the staff – the notes of the passage that is to be played legato.

"Sul ponticello" ("on the bridge") refers to bowing closer to (or nearly on) the bridge, while "sul tasto" ("on the fingerboard") calls for bowing nearer to (or over) the end of the fingerboard. At its extreme, sul ponticello produces a harsh, shrill sound with emphasis on overtones and high harmonics. In contrast, sul tasto produces a more flute-like sound that emphasizes the note's fundamental frequency and produces softened overtones. Composers have used both techniques, particularly in an orchestral setting, for special sounds and effects.

Standard-sized cellos are referred to as "full-size" or "" but are also made in smaller (fractional) sizes, including , , , , , , and . The fractions refer to volume rather than length, so a 1/2 size cello is much longer than half the length of a full size. The smaller cellos are identical to standard cellos in construction, range, and usage, but are simply scaled-down for the benefit of children and shorter adults.

Cellos in sizes larger than do exist, and cellists with unusually large hands may require such a non-standard instrument. Cellos made before tended to be considerably larger than those made and commonly played today. Around 1680, changes in string-making technology made it possible to play lower-pitched notes on shorter strings. The cellos of Stradivari, for example, can be clearly divided into two models: the style made before 1702, characterized by larger instruments (of which only three exist in their original size and configuration), and the style made during and after 1707, when Stradivari began making smaller cellos. This later model is the design most commonly used by modern luthiers. The scale length of a cello is about . The new size offered fuller tonal projection and a greater range of expression. The instrument in this form was able to contribute to more pieces musically and offered the possibility of greater physical dexterity for the player to develop technique.

There are many accessories for the cello.


Cellos are made by luthiers, specialists in building and repairing stringed instruments, ranging from guitars to violins. The following luthiers are notable for the cellos they have produced:

A person who plays the cello is called a "cellist". For a list of notable cellists, see the list of cellists and .

Specific instruments are famous (or become famous) for a variety of reasons. An instrument's notability may arise from its age, the fame of its maker, its physical appearance, its acoustic properties, and its use by notable performers. The most famous instruments are generally known for all of these things. The most highly prized instruments are now collector's items and are priced beyond the reach of most musicians. These instruments are typically owned by some kind of organization or investment group, which may loan the instrument to a notable performer. For example, the Davidov Stradivarius, which is currently in the possession of one of the most widely known living cellists, Yo-Yo Ma, is actually owned by the Vuitton Foundation.

Some notable cellos:




Control store

A control store is the part of a CPU's control unit that stores the CPU's microprogram. It is usually accessed by a microsequencer. A control store implementation whose contents are unalterable is known as a Read Only Memory (ROM) or Read Only Storage (ROS); one whose contents are alterable is known as a Writable Control Store (WCS).

Early control stores were implemented as a diode-array accessed via address decoders, a form of read-only memory. This tradition dates back to the "program timing matrix" on the MIT Whirlwind, first described in 1947. Modern VLSI processors instead use matrices of field-effect transistors to build the ROM and/or PLA structures used to control the processor as well as its internal sequencer in a microcoded implementation. IBM System/360 used a variety of techniques: CCROS (Card Capacitor Read-Only Storage) on the Model 30, TROS (Transformer Read-Only Storage) on the Model 40, and BCROS (Balanced Capacitor Read-Only Storage) on Models 50, 65 and 67.

Some computers are built using "writable microcode" — rather than storing the microcode in ROM or hard-wired logic, the microcode is stored in a RAM called a "writable control store" or "WCS". Such a computer is sometimes called a "Writable Instruction Set Computer" or "WISC". Many of these machines were experimental laboratory prototypes, such as the WISC CPU/16 and the RTX 32P.

The original System/360 models have read-only control store, but later System/360, System/370 and successor models load part or all of their microprograms from floppy disks or other DASD into a writable control store consisting of ultra-high speed random-access read–write memory. The System/370 architecture includes a facility called Initial-Microprogram Load (IML or IMPL) that can be invoked from the console, as part of Power On Reset (POR) or from another processor in a tightly coupled multiprocessor complex. This permitted IBM to easily repair microprogramming defects in the field. Even when the majority of the control store is stored in ROM, computer vendors would often sell writable control store as an option, allowing the customers to customize the machine's microprogram. Other vendors, e.g., IBM, use the WCS to run microcode for emulator features and hardware diagnostics.

Other commercial machines that use writable microcode include the Burroughs Small Systems (1970s and 1980s), the Xerox processors in their Lisp machines and Xerox Star workstations, the DEC VAX 8800 ("Nautilus") family, and the Symbolics L- and G-machines (1980s). Some DEC PDP-10 machines store their microcode in SRAM chips (about 80 bits wide x 2 Kwords), which is typically loaded on power-on through some other front-end CPU. Many more machines offer user-programmable writable control stores as an option (including the HP 2100, DEC PDP-11/60 and Varian Data Machines V-70 series minicomputers).
The Mentec M11 and Mentec M1 store its microcode in SRAM chips, loaded on power-on through another CPU.
The Data General Eclipse MV/8000 ("Eagle") has a SRAM writable control store, loaded on power-on through another CPU.

WCS offers several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs could provide. User-programmable WCS allow the user to optimize the machine for specific purposes. However, it also had the disadvantage of making it harder to debug programs, and making it possible for malicious users to negatively affect the system and data.

Some CPU designs compile the instruction set to a writable RAM or FLASH inside the CPU (such as the Rekursiv processor and the Imsys Cjip), or an FPGA (reconfigurable computing).

Several Intel CPUs in the x86 architecture family have writable microcode, starting with the Pentium Pro in 1995.
This has allowed bugs in the Intel Core 2 microcode and Intel Xeon microcode to be fixed in software, rather than requiring the entire chip to be replaced.
Such fixes can be installed by Linux, FreeBSD, Microsoft Windows, or the motherboard BIOS.

The control store usually has a register on its outputs. The outputs that go back into the sequencer to determine the next address have to go through some sort of register to prevent the creation of a race condition.
In most designs all of the other bits also go through a register. This is because the machine will work faster if the execution of the next microinstruction is delayed by one cycle. This register is known as a pipeline register. Very often the execution of the next microinstruction is dependent on the result of the current microinstruction, which will not be stable until the end of the current microcycle. It can be seen that either way, all of the outputs of the control store go into one big register. Historically it used to be possible to buy EPROMs with these register bits on the same chip.

The clock signal determining the clock rate, which is the cycle time of the system, primarily clocks this register.


Columba

Columba () or Colmcille (7 December 521 – 9 June 597 AD) was an Irish abbot and missionary evangelist credited with spreading Christianity in what is today Scotland at the start of the Hiberno-Scottish mission. He founded the important abbey on Iona, which became a dominant religious and political institution in the region for centuries. He is the patron saint of Derry. He was highly regarded by both the Gaels of Dál Riata and the Picts, and is remembered today as a Catholic saint and one of the Twelve Apostles of Ireland.

Columba studied under some of Ireland's most prominent church figures and founded several monasteries in the country. Around 563 AD he and his twelve companions crossed to Dunaverty near Southend, Argyll, in Kintyre before settling in Iona in Scotland, then part of the Ulster kingdom of Dál Riata, where they founded a new abbey as a base for spreading Celtic Christianity among the pagan Northern Pictish kingdoms. He remained active in Irish politics, though he spent most of the remainder of his life in Scotland. Three surviving early medieval Latin hymns are attributed to him.

Columba was born to Fedlimid and Eithne of the "Cenél Conaill" in Gartan, a district beside Lough Gartan, in Tír Chonaill (mainly modern County Donegal) in what is now Ulster, the northern province in Ireland. On his father's side, he is claimed as being the great-great-grandson of Niall of the Nine Hostages, a pseudo-historical Irish high king of the 5th century. He was baptised in Temple-Douglas, in the County Donegal parish of Conwal (midway between Gartan and Letterkenny), by his teacher and foster-uncle Cruithnechán.

Columba lived in the remote district of what is now Glencolmcille for roughly 5 years, which was named after him. It is not known for sure if his name at birth was Colmcille or if he adopted this name later in life; Adomnán (Eunan) of Iona thought it was his birth name but other Irish sources have claimed his name at birth was Crimthann (meaning 'fox'). In the Irish language his name means 'dove', which is the same name as the Prophet Jonah (Jonah in Hebrew is also 'dove'), which Adomnán of Iona, as well as other early Irish writers, were aware of, although it is not clear if he was deliberately named after Jonah or not. "Columba" is also Latin for dove. (See also the bird genus "Columba".)

When sufficiently advanced in letters he entered the monastic school of Movilla, at Newtownards, under Finnian of Movilla who had studied at Ninian's "Magnum Monasterium" on the shores of Galloway. He was about twenty, and a deacon when, having completed his training at Movilla, he travelled southwards into Leinster, where he became a pupil of an aged bard named Gemman. On leaving him, Columba entered the monastery of Clonard, governed at that time by Finnian, noted for sanctity and learning. Here he imbibed the traditions of the Welsh Church, for Finnian had been trained in the schools of David.

In early Christian Ireland, the druidic tradition collapsed due to the spread of the new Christian faith. The study of Latin learning and Christian theology in monasteries flourished. Columba became a pupil at the monastic school at Clonard Abbey, situated on the River Boyne in modern County Meath. During the sixth century, some of the most significant names in the history of Celtic Christianity studied at the Clonard monastery. The average number of scholars under instruction at Clonard was said to be 300. Columba was one of twelve students of Finnian of Clonard who became known as the Twelve Apostles of Ireland. He became a monk and eventually was ordained a priest.

Another preceptor of Columba was Mobhí Clárainech, whose monastery at Glasnevin was frequented by such famous men as Cainnech of Aghaboe, Comgall, and Ciarán. A pestilence which devastated Ireland in 544 caused the dispersion of Mobhi's disciples, and Columba returned to Ulster, the land of his kindred. He was a striking figure of great stature and powerful build, with a loud, melodious voice which could be heard from one hilltop to another.

The foundation of several important monasteries marked the following years: Derry, at the southern edge of Inishowen; Durrow, County Offaly; Kells, County Meath; and Swords. While at Derry it is said that he planned a pilgrimage to Rome and Jerusalem, but did not proceed farther than Tours. From Tours, he brought a copy of those gospels that had lain on the bosom of Martin for 100 years. This relic was deposited in Derry. St Colmcille is also believed to have established a Church on Inishkea North, County Mayo which is named St Colmcille's Church.

Some traditions assert that sometime around 560 Columba became involved in a quarrel with Finnian of Moville of Movilla Abbey over a psalter. Columba copied the manuscript at the scriptorium under Finnian, intending to keep the copy. Finnian disputed his right to keep it. There is a suggestion that this conflict resulted in the Battle of Cúl Dreimhne in Cairbre Drom Cliabh (now in County Sligo) in 561, during which many men were killed. Richard Sharpe, translator of Adomnán's Life of St. Columba (referenced in the bibliography below) makes a stern caution at this point against accepting the many references that link the battle and Columba's leaving of Ireland, even though there is evidence in the annals that Columba supported his own king against the high king. Political conflicts that had existed for some time resulted in the clan Neill's battle against King Diarmait at Cooldrevny in 561. An issue, for example, was the king's violation of the right of sanctuary belonging to Columba's person as a monk on the occasion of the murder of Prince Curnan, Columba's kinsman.

Prince Curnan of Connacht, who had fatally injured a rival in a hurling match and had taken refuge with Columba, was dragged from his protector's arms and slain by Diarmaid's men, in defiance of the rights of sanctuary.

A synod of clerics and scholars threatened to excommunicate him for these deaths, but Brendan of Birr spoke on his behalf. Eventually, the process was deemed a miscarriage of justice. Columba's own conscience was uneasy, and on the advice of an aged hermit, Molaise, he resolved to expiate his sense of offence by departing Ireland. The term "exile" is used in some references. This, too, can be disputed, for the term "pilgrimage" is used more frequently in the literature about him. A marker at Stroove Beach on the Inishowen Peninsula commemorates the place where Columba set sail for Scotland. He left Ireland, but through the following years, he returned several times in relationships with the communities he had founded there.

Columba's copy of the psalter has been traditionally associated with the Cathach of St. Columba. In 574/575, during his return for the Synod of Drum Ceat, he founded the monastery of Drumcliff in Cairbre, now County Sligo, near the battlefield.

In 563, he travelled to Scotland with twelve companions (said to include Odran of Iona) in a wicker currach covered with leather. According to legend he first landed on the Kintyre Peninsula, near Southend. However, being still in sight of his native land, he moved farther north up the west coast of Scotland. The island of Iona was made over to him by his kinsman Conall mac Comgaill King of Dál Riata, who perhaps had invited him to come to Scotland in the first place. However, there is a sense in which he was not leaving his native people, as the Ulster Gaels had been colonising the west coast of Scotland for the previous couple of centuries. Aside from the services he provided guiding the only centre of literacy in the region, his reputation as a holy man led to his role as a diplomat among the tribes.

There are also many stories of miracles which he performed during his work to convert the Picts, the most famous being his encounter with an unidentified animal that some have equated with the Loch Ness Monster in 565. It is said that he banished a ferocious "water beast" to the depths of the River Ness after it had killed a Pict and then tried to attack Columba's disciple, Lugne (see Vita Columbae Book 2 below). He visited the pagan King Bridei, King of Fortriu, at his base in Inverness, winning Bridei's respect, although not his conversion. He subsequently played a major role in the politics of the country.

He was also very energetic in his work as a missionary, and, in addition to founding several churches in the Hebrides, he worked to turn his monastery at Iona into a school for missionaries. He was a renowned man of letters, having written several hymns and being credited with having transcribed 300 books. One of the few, if not the only, times he left Scotland was towards the end of his life, when he returned to Ireland to found the monastery at Durrow.

According to traditional sources, Columba died in Iona on Sunday, 9 June 597, and was buried by his monks in the abbey he created. However, Dr. Daniel P. Mc Carthy disputes this and assigns a date of 593 to Columba's death. The Annals record the first raid made upon Iona in 795, with further raids occurring in 802, 806, and 825. Columba's relics were finally removed in 849 and divided between Scotland and Ireland.

In Ireland, the saint is commonly known as Colmcille.

Colmcille is one of the three patron saints of Ireland, after Patrick and Brigid of Kildare.

Colmcille is the patron saint of the city of Derry, where he founded a monastic settlement in c. 540. The name of the city in Irish is "Doire Cholm Cille" and is derived from the native oak trees in the area and the city's association with Colmcille. The Catholic Church of Saint Colmcille's Long Tower, and the Church of Ireland St Augustine's Church both claim to stand at the spot of this original settlement. The Church of Ireland Cathedral, St Columb's Cathedral, and the largest park in the city, St. Columb's Park, are named in his honour. The Catholic Boys' Grammar School, St Columb's College, has him as Patron and namesake.

St. Columba's National School in Drumcondra is a girls' school named after the saint.

St. Colmcille's Primary School and St. Colmcille's Community School are two schools in Knocklyon, Dublin, named after him, with the former having an annual day dedicated to the saint on 9 June.

The town of Swords, Dublin was reputedly founded by Colmcille in 560 AD. St Colmcille's Boys' National School and St. Colmcille's Girls' National School, both located in the town of Swords, are also named after the Saint as is one of the local gaelic teams, Naomh Colmcille.

The Columba Press, a religious and spiritual book company based in Dublin, is named after Colmcille.

Aer Lingus, Ireland's national flag carrier has named one of its Airbus A330 aircraft in commemoration of the saint (reg: EI-DUO).

Columba is credited as being a leading figure in the revitalisation of monasticism. The Clan Malcolm/Clan McCallum claims its name from Columba and was reputedly founded by the descendants of his original followers. It is also said that Clan Robertson Clan Donnachaidh / Duncan are heirs of Columba. Clan MacKinnon may also have some claim to being spiritual descendants of St Columba as after he founded his monastery on Isle Iona, the MacKinnons were the abbots of the church for centuries. Sir Iain Moncreiffe of that Ilk speculated that Clan MacKinnon belonged to the kindred of Columba, noting the MacKinnon Arms bore the hand of Columba holding the Cross, and the several Mackinnon abbots of Iona.

The cathedral of the Catholic Diocese of Argyll and the Isles is placed under the patronage of Saint Columba, as are numerous Catholic schools and parishes throughout the nation. The Scottish Episcopal Church, the Church of Scotland,
and the Evangelical Lutheran Church of England also have parishes dedicated to him. The village of Kilmacolm in Renfrewshire is also derived from Colmcille's name.

St Columba's Hospice, a prominent hospice in Edinburgh, is named after the saint.

Columba currently has two poems attributed to him: "Adiutor Laborantium" and "Altus Prosator". Both poems are examples of Abecedarian hymns in Latin written while Columba was at the Iona Abbey.

The shorter of the two poems, "Adiutor Laborantium" consists of twenty-seven lines of eight syllables each, with each line following the format of an Abecedarian hymn using the Classical Latin alphabet save for lines 10–11 and 25–27. The content of the poem addresses God as a helper, ruler, guard, defender and lifter for those who are good and an enemy of sinners whom he will punish.

"Altus Prosator" consists of twenty-three stanzas sixteen syllables long, with the first containing seven lines and six lines in each subsequent stanza. It uses the same format and alphabet as "Adiutor Laborantium" except with each stanza starting with a different letter rather than each line. The poem tells a story over three parts split into the beginning of time, history of Creation, and the Apocalypse or end of time.

As of 2011, Canadians who are of Scottish ancestry are the third largest ethnic group in the country and thus Columba's name is to be found attached to Catholic, Anglican and Presbyterian parishes. This is particularly the case in eastern Canada, apart from French-speaking Quebec.

Throughout the US there are numerous parishes within the Catholic and Episcopalian denominations dedicated to Columba. Within the Protestant tradition the Presbyterian Church (which has its roots in Scottish Presbyterianism) also has parishes named in honour of Columba. Columba is the patron saint of the Roman Catholic Diocese of Youngstown, Ohio. The Cathedral there is named for him.

Iona University, a small Catholic liberal arts college whose main campus is located in New Rochelle, New York, is named after the island on which Columba established his first monastery in Scotland, as is Iona College in Windsor, Ontario, Iona Presentation College, Perth, and Iona College Geelong in Charlemont, Victoria.

There are at least four pipe bands named for him; one each from Tullamore, Ireland, from Derry, Northern Ireland, from Kearny, New Jersey, and from Cape Cod, Massachusetts.

St. Columba's School, one of the most prominent English-Medium schools in India, run by the Irish Christian Brothers, is also named after the saint.

The Munich GAA is named München Colmcilles.

Saint Columba's Feast Day, 9 June, has been designated as International Celtic Art Day. The Book of Kells and the Book of Durrow, great medieval masterpieces of Celtic art, are associated with Columba.

Benjamin Britten composed "A Hymn of St Columba" for choir and organ in 1962, setting a poem by the saint, on the occasion of the 1,400th anniversary of his voyage to Iona.

Columba is honored in the Anglican communion as well, including the Church of England and the Episcopal Church, on 9 June.

The main source of information about Columba's life is the "Life of Columba" (), a hagiography written by Adomnán, one of Columba's successors at Iona, in the style of "saint's lives" narratives that had become widespread throughout medieval Europe. Both the "Life of Columba" and Bede (672/673–735) record Columba's visit to Bridei. Whereas Adomnán just tells us that Columba visited Bridei, Bede relates a later, perhaps Pictish tradition, whereby Columba actually converts the Pictish king. Another early source is a poem in praise of Columba, most probably commissioned by Columba's kinsman, the King of the Uí Néill clan. It was almost certainly written within three or four years of Columba's death and is the earliest vernacular poem in European history. It consists of 25 stanzas of four verses of seven syllables each, called the Amra Coluim Chille.

Through the reputation of its venerable founder and its position as a major European centre of learning, Columba's Iona became a place of pilgrimage. Columba is historically revered as a warrior saint and was often invoked for victory in battle.
Some of his relics were removed in 849 and divided between Alba and Ireland. Relics of Columba were carried before Scottish armies in the reliquary made at Iona in the mid-8th century, called the Brecbennoch. Legend has it that the Brecbennoch was carried to the Battle of Bannockburn (24 June 1314) by the vastly outnumbered Scots army and the intercession of Columba helped them to victory. Since the 19th century the "Brecbennoch of St. Columba" has been identified with the Monymusk Reliquary, although this is now doubted by scholars.

In the Antiphoner of Inchcolm Abbey, the "Iona of the East" (situated on an island in the Firth of Forth), a 14th-century prayer begins "O Columba spes Scotorum..." "O Columba, hope of the Scots".




Conditional proof

A conditional proof is a proof that takes the form of asserting a conditional, and proving that the antecedent of the conditional necessarily leads to the consequent.

The assumed antecedent of a conditional proof is called the conditional proof assumption (CPA). Thus, the goal of a conditional proof is to demonstrate that if the CPA were true, then the desired conclusion necessarily follows. The validity of a conditional proof does not require that the CPA be true, only that "if it were true" it would lead to the consequent.

Conditional proofs are of great importance in mathematics. Conditional proofs exist linking several otherwise unproven conjectures, so that a proof of one conjecture may immediately imply the validity of several others. It can be much easier to show a proposition's truth to follow from another proposition than to prove it independently.

A famous network of conditional proofs is the NP-complete class of complexity theory. There is a large number of interesting tasks (see "List of NP-complete problems"), and while it is not known if a polynomial-time solution exists for any of them, it is known that if such a solution exists for some of them, one exists for all of them. Similarly, the Riemann hypothesis has many consequences already proven.

As an example of a conditional proof in symbolic logic, suppose we want to prove A → C (if A, then C) from the first two premises below:



Conjunction introduction

Conjunction introduction (often abbreviated simply as conjunction and also called and introduction or adjunction) is a valid rule of inference of propositional logic. The rule makes it possible to introduce a conjunction into a logical proof. It is the inference that if the proposition formula_1 is true, and the proposition formula_2 is true, then the logical conjunction of the two propositions formula_1 and formula_2 is true. For example, if it is true that "it is raining", and it is true that "the cat is inside", then it is true that "it is raining and the cat is inside". The rule can be stated:

where the rule is that wherever an instance of "formula_1" and "formula_2" appear on lines of a proof, a "formula_8" can be placed on a subsequent line.

The "conjunction introduction" rule may be written in sequent notation:

where formula_1 and formula_2 are propositions expressed in some formal system, and formula_12 is a metalogical symbol meaning that formula_8 is a syntactic consequence if formula_1 and formula_2 are each on lines of a proof in some logical system;

English in the Commonwealth of Nations

The use of the English language in current and former member countries of the Commonwealth of Nations was largely inherited from British colonization, with some exceptions. English serves as the medium of inter-Commonwealth relations.

Commonwealth English (CE or CwE) is very diverse, and many regions (notably Australia, Brunei, Canada, Hong Kong, India, Ireland, Malaysia, New Zealand, Pakistan, Singapore, South Africa, Sri Lanka, and the Caribbean) have developed their own local varieties of the language. In Cyprus, it does not have official status but is widely used as a . English is spoken as a first or second language in most of the Commonwealth.

Written English in the current and former Commonwealth generally favours British English spelling as opposed to American English, with some exceptions, particularly in Canada, where there are strong influences from neighbouring American English. Few Commonwealth countries besides Australia, Canada, and the United Kingdom have produced their own English dictionaries and style guides, and may rely on those produced in other countries.

Southern Hemisphere native varieties of English began to develop during the 18th century, with the colonisation of Australasia and South Africa. Australian English and New Zealand English are closely related to each other, and share some similarities with South African English (though it has unique influences from indigenous African languages, and Dutch influences it inherited along with the development of Afrikaans from Dutch).

Canadian English contains elements of British English and American English, as well as many Canadianisms and some French influences. It is the product of several waves of immigration and settlement, from Britain, Ireland, France, the United States, and around the world, over a period of more than two centuries. Modern Canadian English has taken significant vocabulary and spelling from the shared political and social institutions of Commonwealth countries.

Caribbean English is influenced by the English-based Creole varieties spoken, but they are not one and the same. There is a great deal of variation in the way English is spoken, with a "Standard English" at one end of a bipolar linguistic continuum and Creole languages at the other. These dialects have roots in 17th-century British and Irish English, and African languages, plus localised influences from other colonial languages including French, Spanish, and Dutch; unlike most native varieties of English, West Indian dialects often tend to be syllable-timed rather than stress-timed.

Second-language varieties of English in Africa and Asia have often undergone "indigenisation"; that is, each English-speaking community has developed (or is in the process of developing) its own standards of usage, often under the influence of local languages. These dialects are sometimes referred to as "New Englishes" (McArthur, p. 36); most of them inherited non-rhoticity from Southern British English.

Several dialects of West African English exist, with a lot of regional variation and some influence from indigenous languages. West African English tends to be syllable-timed, and its phoneme inventory is much simpler than that of Received Pronunciation; this sometimes affects mutual intelligibility with native varieties of English. A distinctive North African English, often with significant influences from Bantu languages such as Swahili, is spoken in countries such as Kenya or Tanzania, particularly in Nairobi and other cities where there is an expanding middle class, for whom English is increasingly being used in the home as the first language.

Small communities of native English speakers can be found in Zimbabwe, Botswana, and Namibia; the dialects spoken are similar to native South African English.

English was introduced into the subcontinent by the British Raj. Among the partitioned post-independent countries, India has the largest English-speaking population in the Commonwealth, although comparatively very few speakers of Indian English are first-language speakers. The same is true of English spoken in other parts of South Asia, e.g. Pakistani English, Sri Lankan English, Bangladeshi English and Myanmar English. South Asian English phonology is highly variable; stress, rhythm and intonation are generally different from those of native varieties. There are also several peculiarities at the levels of morphology, syntax and usage, some of which can also be found among educated speakers.

Southeast Asian English comprises Singapore English, Malaysian English, and Brunei English; it features some influence from Malay and Chinese languages, as well as Indian English.

Hong Kong ceased to be part of the Commonwealth in 1997. Nonetheless, the English language there still enjoys status as an official language.


Other languages:


Charles McCarry

Charles McCarry (June 14, 1930 – February 26, 2019) was an American writer, primarily of spy fiction, and a former undercover operative for the Central Intelligence Agency.

McCarry's family came from The Berkshires area of western Massachusetts. He was born in Pittsfield, and lived in Virginia. He graduated from Dalton High School.

McCarry began his writing career in the United States Army as a correspondent for "Stars and Stripes". He served from 1948 to 1951 and achieved the rank of sergeant. He received initial training at Fort Benning, Georgia, and was stationed in Germany for almost two years and at Camp Pickett, Virginia for about a year.

After his army service, he was a speechwriter in the early Administration of President Dwight D. Eisenhower. In 1958, at the invitation of Cord Meyer, he accepted a post with the CIA, for whom he traveled the globe as a deep cover operative. He took a leave of absence to work for the 1960 Nixon campaign, writing for vice-presidential candidate Henry Cabot Lodge. He left the CIA for the last time in 1967, becoming a writer of spy novels.

McCarry was also an editor-at-large for "National Geographic" and contributed pieces to "The New York Times", "The Wall Street Journal", "The Washington Post", the "Saturday Evening Post", and other national publications.

McCarry believed that "the best novels are about ordinary things: love, betrayal, death, trust, loneliness, marriage, fatherhood." In 1988 McCarry described the themes of his novels to date as "ordinary things – love, death, betrayal and the American dream."

McCarry wrote that: "After I resigned [from the CIA], intending to spend the rest of my life writing fiction and knowing what tricks the mind can play when the gates are thrown wide open, as they are by the act of writing, between the imagination and that part of the brain in which information is stored, I took the precaution of writing a closely remembered narrative of my clandestine experiences. After correcting the manuscript, I burned it. What I kept for my own use was the atmosphere of secret life: How it worked on the five senses and what it did to the heart and mind. All the rest went up in flames, setting me free henceforth to make it all up. In all important matters, such as the creation of characters and the invention of plots, with rare and minor exceptions, that is what I have done. And, as might be expected, when I have been weak enough to use something that really happened as an episode in a novel, it is that piece of scrap, buried in a landfill of the imaginary, readers invariably refuse to believe."

McCarry was an admirer of the work of Eric Ambler and W. Somerset Maugham, especially the latter's stories. He was also an admirer of Richard Condon, author of "The Manchurian Candidate" (1959).

Ten of McCarry's novels involve the life story of a fictional character named Paul Christopher, who grew up in pre-Nazi Germany, and later served in the Marines and became an operative for a U.S. government entity known as "the Outfit", meant to represent the Central Intelligence Agency.

These books are, in order of publication:

Alternately, in chronological order of events depicted:

"The Wall Street Journal" described McCarry in 2013 as "the dean of American spy writers". "The New Republic" magazine called him "poet laureate of the CIA"; and Otto Penzler described him as "the greatest espionage writer that America has ever produced." Jonathan Yardley, Pulitzer Prize-winning critic for the "Washington Post", calls him a "'serious' novelist" whose work may include "the best novel ever written about life in high-stakes Washington, D.C." In 2004 P. J. O'Rourke called him "the best modern writer on the subject of intrigue."

The film "Wrong is Right" (1982), starring Sean Connery, was loosely based on McCarry's novel, "The Better Angels" (1979).



Otto Penzler, editor:





Cimbri

The Cimbri (Greek Κίμβροι, "Kímbroi"; Latin "Cimbri") were an ancient tribe in Europe. Ancient authors described them variously as a Celtic people (or Gaulish), Germanic people, or even Cimmerian. Several ancient sources indicate that they lived in Jutland, which in some classical texts was called the Cimbrian peninsula. There is no direct evidence for the language they spoke, though some scholars argue that it was a Germanic language, while others argue that it was Celtic.

Together with the Teutones and the Ambrones, they fought the Roman Republic between 113 and 101 BC during the Cimbrian War. The Cimbri were initially successful, particularly at the Battle of Arausio, in which a large Roman army was routed. They then raided large areas in Gaul and Hispania. In 101 BC, during an attempted invasion of the Italian peninsula, the Cimbri were decisively defeated at the Battle of Vercellae by Gaius Marius, and their king, Boiorix, was killed. Some of the surviving captives are reported to have been among the rebellious gladiators in the Third Servile War.

The origin of the name "Cimbri" is unknown. One etymology is PIE ' "inhabitant", from ' "home" (> English "home"), itself a derivation from "" "live" (> Greek , Latin "sinō"); then, the Germanic "*himbra-" finds an exact cognate in Slavic "sębrъ" "farmer" (> Croatian, Serbian "sebar", Russian сябёр "syabyor").

The name has also been related to the word "kimme" meaning "rim", i.e., "the people of the coast". Finally, since Antiquity, the name has been related to that of the Cimmerians.

The name of the Danish region Himmerland (Old Danish "Himbersysel") has been proposed to be a derivative of their name. According to such proposals, the word "Cimbri" with a "c" would be an older form before Grimm's law (PIE "k" > Germanic "h"). Alternatively, Latin "c-" represents an attempt to render the unfamiliar Proto-Germanic "h" = (Latin "h" was but was becoming silent in common speech at the time), perhaps due to Celtic-speaking interpreters (a Celtic intermediary would also explain why Germanic "*Þeuðanōz" became Latin "Teutones").

Because of the similarity of the names, the Cimbri have been at times associated with Cymry, the Welsh name for themselves. However, "Cymry" is derived from Brittonic "*Kombrogi", meaning "compatriots", and is linguistically unrelated to Cimbri.

Scholars generally see the Cimbri as a Germanic tribe originating in Jutland, but archaeologists have found no clear indications of any mass migration from Jutland in the early Iron Age. The Gundestrup Cauldron, which was deposited in a bog in Himmerland in the 2nd or 1st century BC, shows that there was some sort of contact with southeastern Europe, but it is uncertain if this contact can be associated with the Cimbrian militia expeditions against Rome of the 1st Century BC. It is known that the peoples of Northern Europe and the British Isles participated in annual partial population seasonal Winter migrations southward to what is now central Iberia and southern France where goods and resources were traded and cross-culture marriages were arranged.

Advocates for a northern homeland point to Greek and Roman sources that associate the Cimbri with the Jutland peninsula. According to the "Res gestae" (ch. 26) of Augustus, the Cimbri were still found in the area around the turn of the 1st century AD:

The contemporary Greek geographer Strabo testified that the Cimbri still existed as a Germanic tribe, presumably in the "Cimbric peninsula" (since they are said to live by the North Sea and to have paid tribute to Augustus):

On the map of Ptolemy, the "Kimbroi" are placed on the northernmost part of the peninsula of Jutland, i.e., in the modern landscape of Himmerland south of Limfjorden (since Vendsyssel-Thy north of the fjord was at that time a group of islands).

Some time before 100 BC many of the Cimbri, as well as the Teutons and Ambrones, migrated south-east. After several unsuccessful battles with the Boii and other Celtic tribes, they appeared 113 BC in Noricum, where they invaded the lands of one of Rome's allies, the Taurisci.

On the request of the Roman consul Gnaeus Papirius Carbo, sent to defend the Taurisci, they retreated, only to find themselves deceived and attacked at the Battle of Noreia, where they defeated the Romans. Only a storm, which separated the combatants, saved the Roman forces from complete annihilation.

Now the road to Italy was open, but they turned west towards Gaul. They came into frequent conflict with the Romans, who usually came out the losers. In Commentarii de Bello Gallico the Aduatuci —Belgians of Cimbrian origin—repeatedly sided with Rome's enemies. In 109 BC, they defeated a Roman army under the consul Marcus Junius Silanus, who was the commander of Gallia Narbonensis. In 107 BC they defeated another Roman army under the consul Gaius Cassius Longinus, who was killed at the Battle of Burdigala (modern day Bordeaux) against the Tigurini, who were allies of the Cimbri.

It was not until 105 BC that they planned an attack on the Roman Republic itself. At the Rhône, the Cimbri clashed with the Roman armies. Discord between the Roman commanders, the proconsul Quintus Servilius Caepio and the consul Gnaeus Mallius Maximus, hindered Roman coordination and so the Cimbri succeeded in first defeating the legate Marcus Aurelius Scaurus and later inflicted a devastating defeat on Caepio and Maximus at the Battle of Arausio. The Romans lost as many as 80,000 men, according to Livy; Mommsen (in his "History of Rome") thought that excluded auxiliary cavalry and non-combatants who brought the total loss closer to 112,000. Other estimates are much smaller, but by any account a large Roman army was routed.

Rome was in panic, and the "terror cimbricus" became proverbial. Everyone expected to soon see the "new Gauls" outside of the gates of Rome. Desperate measures were taken: contrary to the Roman constitution, Gaius Marius, who had defeated Jugurtha, was elected consul and supreme commander for five years in a row (104–100 BC).

In 104–103 BC, the Cimbri had turned to the Iberian Peninsula where they pillaged far and wide, until they were confronted by a coalition of Celtiberians. Defeated, the Cimbri returned to Gaul, where they joined their allies, the Teutons. During this time, C. Marius had the time to prepare and, in 102 BC, he was ready to meet the Teutons and the Ambrones at the Rhône. These two tribes intended to pass into Italy through the western passes, while the Cimbri and the Tigurines were to take the northern route across the Rhine and later across the Central Eastern Alps.

At the estuary of the Isère, the Teutons and the Ambrones met Marius, whose well-defended camp they did not manage to overrun. Instead, they pursued their route, and Marius followed them. At Aquae Sextiae, the Romans won two battles and took the Teuton king Teutobod prisoner.

The Cimbri had penetrated through the Alps into northern Italy. The consul Quintus Lutatius Catulus had not dared to fortify the passes, but instead he had retreated behind the river Po, and so the land was open to the invaders. The Cimbri did not hurry, and the victors of Aquae Sextiae had the time to arrive with reinforcements. At the Battle of Vercellae, at the confluence of the river Sesia with the Po, in 101 BC, the long voyage of the Cimbri also came to an end.

It was a devastating defeat. Two chieftains, Lugius and Boiorix, died on the field, while the other chieftains Caesorix and Claodicus were captured. The women killed both themselves and their children in order to avoid slavery. The Cimbri were annihilated, although some may have survived to return to the homeland where a population with this name was residing in northern Jutland in the 1st century AD, according to the sources quoted above. Some of the surviving captives are reported to have been among the rebelling gladiators in the Third Servile War.

However, Justin's epitome of Trogus, 38.4, has Mithridates the Great state that the Cimbri are ravaging Italy while the Social War is going on, i.e. at some time in 90–88 BC, thus more than a decade later, after sending ambassadors to the Cimbri to request military aid; judging from the context they must have been living in North Eastern Europe at the time.

According to Julius Caesar, the Belgian tribe of the Atuatuci "was descended from the Cimbri and Teutoni, who, upon their march into our province and Italy, set down such of their stock and stuff as they could not drive or carry with them on the near (i.e. west) side of the Rhine, and left six thousand men of their company there as guard and garrison" ("Gall." 2.29, trans. Edwards). They founded the city of Atuatuca in the land of the Belgic Eburones, whom they dominated. Thus Ambiorix king of the Eburones paid tribute and gave his son and nephew as hostages to the Atuatuci ("Gall." 6.27). In the first century AD, the Eburones were replaced or absorbed by the Germanic Tungri, and the city was known as Atuatuca Tungrorum, i.e. the modern city of Tongeren.

The population of modern-day Himmerland claims to be the heirs of the ancient Cimbri. The adventures of the Cimbri are described by the Danish Nobel Prize–winning author Johannes V. Jensen, himself born in Himmerland, in the novel "Cimbrernes Tog" (1922), included in the epic cycle "Den lange Rejse" (English "The Long Journey", 1923). The so-called Cimbrian bull ("Cimbrertyren"), a sculpture by Anders Bundgaard, was erected on 14 April 1937 in a central town square in Aalborg, the capital of the region of North Jutland.

A German ethnic minority speaking the Cimbrian language, having settled in the mountains between Vicenza, Verona, and Trento in Italy (also known as Seven Communities), is also called the . For hundreds of years this isolated population and its present 4,400 inhabitants have claimed to be the direct descendants of the Cimbri retreating to this area after the Roman victory over their tribe. However, it is more likely that Bavarians settled here in the Middle Ages. Most linguists remain committed to the hypothesis of a medieval (11th to 12th century AD) immigration to explain the presence of small German-speaking communities in the north of Italy. Some genetic studies seem to prove a Celtic, not Germanic, descent for most inhabitants in the region that is reinforced by Gaulish toponyms such as those ending with the suffix "-ago" < Celtic "-*ako(n)" (e.g. Asiago is clearly the same place name as the numerous variants – Azay, Aisy, Azé, Ezy – in France, all of which derive from "*Asiacum" < Gaulish "*Asiāko(n)"). On the other hand, the original place names in the region, from the specifically localized language known as 'Cimbro' are still in use alongside the more modern names today. These indicate a different origin (e.g., Asiago is known also by its original Cimbro name of "Sleghe"). The Cimbrian origin myth was popularized by humanists in the 14th century.

Despite these connections to southern Germany, belief in a Himmerland origin persisted well into modern times. On one occasion in 1709, for instance, Frederick IV of Denmark paid the region's inhabitants a visit and was greeted as their king. The population, which kept its independence during the time of the Venice Republic, was later severely devastated by World War I. As a result, many Cimbri have left this mountainous region of Italy, effectively forming a worldwide diaspora.

The Cimbri are depicted as ferocious warriors who did not fear death. The host was followed by women and children on carts. Aged women, priestesses, dressed in white sacrificed the prisoners of war and sprinkled their blood, the nature of which allowed them to see what was to come.

Strabo gives this vivid description of the Cimbric folklore:

If the Cimbri did in fact come from Jutland, evidence that they practiced ritualistic sacrifice may be found in the Haraldskær Woman discovered in Jutland in the year 1835. Noosemarks and skin piercing were evident and she had been thrown into a bog rather than buried or cremated. Furthermore, the Gundestrup cauldron, found in Himmerland, may be a sacrificial vessel like the one described in Strabo's text. In style, the work looks like Thracian silver work, while many of the engravings are Celtic objects.

A major problem in determining whether the Cimbri were speaking a Celtic language or a Germanic language is that, at that time, the Greeks and Romans tended to refer to all groups to the north of their sphere of influence as Gauls, Celts, or Germani rather indiscriminately. Caesar seems to be one of the first authors to distinguish the two groups, and he had a political motive for doing so (it was an argument in favour of the Rhine border). Yet, one cannot always trust Caesar and Tacitus when they ascribe individuals and tribes to one or the other category, although Caesar made clear distinctions between the two cultures. Some ancient sources categorize the Cimbri as a Germanic tribe, but some ancient authors include the Cimbri among the Celts.

There are few direct testimonies to the language of the Cimbri: referring to the Northern Ocean (the Baltic or the North Sea), Pliny the Elder states: "Philemon says that it is called Morimarusa, i.e. the Dead Sea, by the Cimbri, until the promontory of Rubea, and after that Cronium." The contemporary Gaulish terms for "sea" and "dead" appear to have been "mori" and "*maruo-"; compare their well-attested modern Insular Celtic cognates "muir" and "marbh" (Irish), "môr" and "marw" (Welsh), and "mor" and "marv" (Breton). The same word for "sea" is also known from Germanic, but with an "a" (*"mari-"), whereas a cognate of "marbh" is unknown in all dialects of Germanic. Yet, given that Pliny had not heard the word directly from a Cimbric informant, it cannot be ruled out that the word is in fact Gaulish instead.

The known Cimbri chiefs have Celtic names, including Boiorix (which may mean "King of the Boii" or, more literally, "King of Strikers"), Gaesorix (which means "Spear King"), and Lugius (which may be named after the Celtic god Lugus). Other evidence to the language of the Cimbri is circumstantial: thus, we are told that the Romans enlisted Gaulish Celts to act as spies in the Cimbri camp before the final showdown with the Roman army in 101 BC.

Jean Markale wrote that the Cimbri were associated with the Helvetii, and more especially with the indisputably Celtic Tigurini. These associations may link to a common ancestry, recalled from two hundred years previous, but that is not certain. Henri Hubert states "All these names are Celtic, and they cannot be anything else". Some authors take a different perspective.

Countering the argument of a Celtic origin is the literary evidence that the Cimbri originally came from northern Jutland, an area with no Celtic placenames, instead only Germanic ones. This does not rule out Cimbric Gallicization during the period when they lived in Gaul. Boiorix, who may have had a Celtic if not a Celticized Germanic name, was king of the Cimbri after they moved away from their ancestral home of northern Jutland. Boiorix and his tribe lived around Celtic peoples during his era as J. B. Rives points out in his introduction to Tacitus' "Germania"; furthermore, the name "Boiorix" can be seen as having either Proto-Germanic or Celtic roots.

The science fiction story "Delenda Est" by Poul Anderson depicts an alternate history in which Hannibal won the Second Punic War and destroyed Rome, but Carthage proved unable to rule Italy – which fell into utter chaos. Thus, there was no one to stop the Cimbri two hundred years later. They filled the vacuum, conquered Italy, assimilated the local population to their own culture and by the equivalent of the 20th century had made of Italy a flourishing, technologically advanced kingdom speaking a Germanic language.

Cimbri is referenced in Italo Calvino's novel "If on a Winter's Night a Traveller" as a fictional country that warred with a similarly fictionalised version of Cimmeria, thus imposing its own written language onto the Cimmerians.

Jeff Hein’s historical fiction series The Cimbrian War tells the story of the Cimbri and their migration across Iron-Age Europe.



Cleveland Browns

The Cleveland Browns are a professional American football team based in Cleveland. Named after original coach and co-founder Paul Brown, they compete in the National Football League (NFL) as a member club of the American Football Conference (AFC) North division. The Browns play their home games at Cleveland Browns Stadium, which opened in 1999, with administrative offices and training facilities in Berea, Ohio. The Browns' official club colors are brown, orange, and white. They are unique among the 32 member franchises of the NFL in that they do not have a logo on their helmets.

The franchise was founded in 1944 by Brown and businessman Arthur B. McBride as a charter member of the All-America Football Conference (AAFC), and began play in 1946. The Browns dominated the AAFC, compiling a 47–4–3 record in the league's four seasons and winning its championship in each. When the AAFC folded after the 1949 season, the Browns joined the NFL along with the San Francisco 49ers and the original Baltimore Colts. The Browns won a championship in their inaugural NFL season, as well as in the 1954, 1955, and 1964 seasons, and in a feat unequaled in any of the North American major professional sports, played in their league championship game in each of their first 10 years of existence, winning seven of those games. From 1965 to 1995, they qualified to play in the NFL playoffs 14 times, but did not win another championship or play in the Super Bowl during that period. 

In 1995, owner Art Modell, who had purchased the Browns in 1961, announced plans to move the team to Baltimore. After threats of legal action from the city of Cleveland and fans, a compromise was reached in early 1996 that allowed Modell to establish the Baltimore Ravens as a new franchise while retaining the contracts of all Browns personnel. The Browns' intellectual property, including team name, logos, training facility, and history, were kept in trust and the franchise was regarded by the NFL as suspended for three seasons. While several of the then-30 existing franchises considered re-locating to Cleveland, in 1998 it was confirmed that the NFL would field 31 teams when the Browns resumed play in 1999. Thus, while the 1999 Browns were not technically considered to be an expansion franchise, with the relocated and renamed team being considered the expansion team instead, the club's roster was re-stocked via an expansion draft.

Since resuming operations in 1999, the Browns have struggled to find success, especially during the 2010s when they did not post one winning season throughout that decade. They have had only four winning seasons (2002, 2007, 2020, and 2023), three playoff appearances (2002, 2020, and 2023), and one playoff win (2020), winning less than one third of their games in total, and in 2017 were only the second team in NFL history to have a 0–16 season after the 2008 Detroit Lions. The franchise has also been noted for a lack of stability with head coaches (10 full time - including two who were fired after only one season - and two interim since 1999) and quarterbacks (38 different starters since 1999). From 2003 to 2019, the Browns had a 17-season playoff drought, which ended during the 2020 season. They are one of four teams to have never appeared in a Super Bowl.

The Cleveland Browns were founded in 1944 when taxi-cab magnate Arthur B. "Mickey" McBride secured a Cleveland franchise in the newly formed All-America Football Conference (AAFC). Paul Brown was the team's namesake and first coach. The Browns began play in 1946 in the AAFC. The Browns won each of the league's four championship games before the league dissolved in 1949. The team then moved to the more established National Football League (NFL), where it continued to dominate. Between 1950 and 1955, Cleveland reached the NFL championship game every year, winning three times.

McBride and his partners sold the team to a group of Cleveland businessmen in 1953 for a then-unheard-of $600,000. Eight years later, the team was sold again, this time to a group led by New York advertising executive Art Modell. Modell fired Brown before the 1963 season, but the team continued to win behind running back Jim Brown. The Browns won the championship in 1964 and reached the title game the following season, losing to the Green Bay Packers.

When the AFL and NFL merged before the 1970 season, Cleveland became part of the new American Football Conference (AFC). While the Browns made it back to the playoffs in 1971 and 1972, they fell into mediocrity through the mid-1970s. A revival of sorts took place in 1979 and 1980, when quarterback Brian Sipe engineered a series of last-minute wins and the Browns came to be called the "Kardiac Kids". Under Sipe, however, the Browns did not make it past the first round of the playoffs. Quarterback Bernie Kosar, whom the Browns drafted in 1985, led the team to three AFC Championship games in the late 1980s but lost each time to the Denver Broncos.

In 1995, Modell announced he was relocating the Browns to Baltimore, sowing a mix of outrage and bitterness among Cleveland's dedicated fan base. Negotiations and legal battles led to an agreement where Modell would be allowed to take his personnel to Baltimore as an expansion franchise, called the Baltimore Ravens, but would leave Cleveland the Browns' colors, logos and heritage for a reactivated Browns franchise that would take the field no later than 1999.

After three years of inactivity while Cleveland Stadium was demolished and Cleveland Browns Stadium was built on its site, the Browns were reactivated and started play again in 1999 under new owner Al Lerner. The Browns struggled throughout the 2000s and 2010s, posting a record of 101–234–1 () since their 1999 return. The Browns have only posted four winning seasons and three playoff appearances (2002, 2020, 2023) since returning to the NFL. The team's struggles have been magnified since 2012, when the Lerner family sold the team to businessman Jimmy Haslam. In six seasons under Haslam's ownership, the Browns went through four head coaches and four general managers, none of whom had found success. In 2016 and 2017 under head coach Hue Jackson, the Browns went 1–31 (, including a winless 0–16 season in 2017), the worst two-year stretch in NFL history, and received the number one overall draft pick in both of those years. In 2020, the Browns secured their first playoff berth since 2002 by defeating the Pittsburgh Steelers in week 17 and finishing the season 11–5.

The Browns are the only National Football League team without a helmet logo. The logoless helmet serves as the Browns' official logo. The organization has used several promotional logos throughout the years; players' numbers were painted on the helmets from 1957 to 1960; and an unused "CB" logo was created in 1965. But for much of their history, the Browns' helmets have been an unadorned burnt orange color with a top stripe of dark brown (officially called "seal brown") divided by a white stripe.

The team has had various promotional logos throughout the years, such as the "Brownie Elf" mascot or a Brown "B" in a white football. While Art Modell did away with the elf in the mid-1960s (believing it to be too childish), its use has been revived since the team's return in 1999. The popularity of the Dawg Pound section at First Energy Stadium has led to a brown and orange dog being used for various Browns functions. But overall, the orange, logo-less helmet continues as the primary trademark of the Cleveland Browns. The Browns have used special commemorative logos during individual seasons, such as the 1999 logo to celebrate the team's return to the NFL, a 60th-anniversary logo for the 2006 season, and a 75th-anniversary logo in 2021.
The current logos and wordmarks were introduced on February 24, 2015, with the helmet design remaining largely as is, the only differences being minor color changes to the shade of orange used on the helmet and the facemask being changed from gray to brown. A new secondary "dawg" logo was introduced in 2023. The logo, featuring a bull mastiff dog, was created by graphic designer Houston Mark and was the winning entry of a fan vote. It features numerous small homages to the city of Cleveland, state of Ohio, and the team's history.

For the 2022 season, by virtue of a fan poll, a version of the Brownie elf logo was featured at midfield at FirstEnergy Stadium.

The original designs of the jerseys, pants, and socks remained mostly the same, but the helmets went through many significant revisions throughout the years. The Browns uniforms saw their first massive change prior to the 2015 season.

Jerseys:

Pants:

Socks:

Helmet: Solid white (1946–1949); solid white for day games and solid orange for night games (1950–1951); orange with a single white stripe (1952–1956); orange with a single white stripe and brown numerals on the sides (1957–1959); orange with a brown-white-brown stripe sequence and brown numerals on the sides (1960); orange with a brown-white-brown stripe sequence (1961–1995 and 1999–present).

Over the years, the Browns have had on-and-off periods of wearing white for their home games, particularly in the 1970s and 80s, as well as in the early 2000s after the team returned to the league. Until recently, when more NFL teams have started to wear white at home at least once a season, the Browns were the only non-subtropical team north of the Mason-Dixon line to wear white at home on a regular basis.

Secondary numerals (called "TV numbers") first appeared on the jersey sleeves in 1961. Over the years, there have been minor revisions to the sleeve stripes, the first occurring in 1968 (brown jerseys worn in early season) and 1969 (white and brown jerseys) when stripes began to be silkscreened onto the sleeves and separated from each other to prevent color bleeding. However, the basic five-stripe sequence has remained intact (with the exception of the 1984 season). A recent revision was the addition of the initials "AL" to honor team owner Al Lerner who died in 2002; this was removed in 2013 upon Jimmy Haslam assuming ownership of the team.

Orange pants with a brown-white-brown stripe sequence were worn from 1975 to 1983 and become symbolic of the "Kardiac Kids" era. The orange pants were worn again occasionally in 2003 and 2004.

Other than the helmet, the uniform was completely redesigned for the 1984 season. New striping patterns appeared on the white jerseys, brown jerseys and pants. Solid brown socks were worn with brown jerseys and solid orange socks were worn with white jerseys. Brown numerals on the white jerseys were outlined in orange. White numerals on the brown jerseys were double outlined in brown and orange. (Orange numerals double outlined in brown and white appeared briefly on the brown jerseys in one pre-season game.) However, this particular uniform set was not popular with the fans, and in 1985 the uniform was returned to a look similar to the original design. It remained that way until 1995.

In 1999, the expansion Browns adopted the traditional design with two exceptions: first, the TV numbers, previously on the sleeves, were moved to the shoulders; and second, the orange-brown-orange pants stripes were significantly widened.

Experimentation with the uniform design began in 2002. An alternate orange jersey was introduced that season as the NFL encouraged teams to adopt a third jersey, and a major design change was made when solid brown socks appeared for the first time since 1984 and were used with white, brown and orange jerseys. Other than 1984, striped socks (matching the jersey stripes) had been a signature design element in the team's traditional uniform. The white striped socks appeared occasionally with the white jerseys in 2003–2005 and 2007.

Experimentation continued in 2003 and 2004 when the traditional orange-brown-orange stripes on the white pants were replaced by two variations of a brown-orange-brown sequence, one in which the stripes were joined (worn with white jerseys) and the other in which they were separated by white (worn with brown jerseys). The joined sequence was used exclusively with both jerseys in 2005. In 2006, the traditional orange-brown-orange sequence returned.

Additionally in 2006, the team reverted to an older uniform style, featuring gray face masks; the original stripe pattern on the brown jersey sleeves (The white jersey has had that sleeve stripe pattern on a consistent basis since the 1985 season.) and the older, darker shade of brown.

The Browns wore brown pants for the first time in team history on August 18, 2008, preseason game against the New York Giants. The pants contain no stripes or markings. The team had the brown pants created as an option for their away uniform when they integrated the gray facemask in 2006. They were not worn again until the Browns "family" scrimmage on August 9, 2009 with white-striped socks. The Browns have continued to wear the brown pants throughout the 2009 season. Browns quarterback Brady Quinn supported the team's move to wearing the brown pants full-time, claiming that the striped pattern on the white pants "prohibit[ed] mobility".
However, the fans generally did not like the brown pants, and after being used for only one season, the team returned to their white shirt-on-white pants in 2010. Coach Eric Mangini told "The Plain Dealer" the Browns won't use the brown pants anymore. "It wasn't very well-received," Mangini said. "I hope we can get to the point where we can wear fruit on our heads and people wouldn't notice." At the time, the brown pants weren't officially dropped by the team, but simply not used.

The Browns chose to wear white at home for the 2011 season, and wound up wearing white for all 16 games as when they were on the road, the home team would wear their darker colored uniform.

The Browns brought back the brown pants in their home game against the Buffalo Bills on October 3, 2013, on "Thursday Night Football", pairing them with the brown jerseys. It marked the first time the team wore an all-brown combination in team history.

On April 14, 2015, the Cleveland Browns unveiled their new uniform combinations, consisting of the team's colors of orange, brown and white.

The Browns brought back the all-brown look for the NFL Color Rush program in 2016, minus the white elements. In 2018, despite the Color Rush program being discontinued, the uniform was worn at home three times. For the 2019 season, the Browns promoted this uniform to their primary home uniform and donned it for six home games as well as any away game in which the home team wore white.

The club unveiled a new uniform design for the 2020 season. The new uniform design pays homage to the Browns' classic uniform design from years past.

In 2023, the Browns introduced new "White Out" uniforms, an all-white uniform, featuring a white helmet, that will be worn during select home games. This will mark the first time since 1950 the Browns will have non-orange helmets. The white helmets, which will feature an orange stripe down the middle flanked by two brown stripes (basically inverting the regular helmet's colors), are an homage to the early years of the franchise.

The Browns have rivalries with all three of their AFC North opponents. In addition, the team has had historical rivalries with the Denver Broncos, Detroit Lions, San Francisco 49ers, and Houston Oilers/Tennessee Titans.

Often called the "Turnpike Rivalry", the Browns' biggest rival has long been the Pittsburgh Steelers. Former Browns owner Art Modell scheduled home games against the Steelers on Saturday nights from 1964 to 1970 to help fuel the rivalry. The rivalry has also been fueled by the proximity of the two teams, number of championships both teams have won, players and personnel having played and/or coached for both sides, and personal bitterness. The teams have played twice annually since 1950, making it the oldest rivalry in the AFC and the fifth-oldest rivalry in the NFL. Though the Browns dominated this rivalry early in the series (winning the first eight meetings and posting a 31–9 record in the 1950s and 1960s), the Steelers went 15–5 in the 1970s and 36–9–1 since the Browns returned to the league in 1999. The Steelers have been particularly dominant in Pittsburgh, posting a 44–7 record when hosting the Browns since 1970, including to winning streaks of 16 games (1970–85) and 17 games (2004–20).

The Steelers currently hold a 79–61–1 lead. The Browns and Steelers met in the playoffs in , , and 2020, with the Steelers holding a 2–1 lead in the postseason series. Though the rivalry has cooled in Pittsburgh due to the Modell move as well as the Browns' poor play since 1999, the Steelers still remain the top rival for Cleveland.

Originally conceived due to the personal animosity between Paul Brown and Art Modell, the "Battle of Ohio" between the Browns and the Cincinnati Bengals has been fueled by the sociocultural differences between Cincinnati and Cleveland, a shared history between the two teams, and similar team colors, as Brown used the exact shade of orange for the Bengals that he used for the Browns. (Though this has changed since then, as the Bengals now use a brighter shade of orange.) Modell, in fact, moved the Browns to the AFC after the AFL–NFL merger in order to have a rivalry with the Bengals. The rivalry has also produced two of the eleven highest-scoring games in NFL history. Cincinnati has the all-time edge 52–48. While the Bengals have a 28–21 edge since the Browns returned to the NFL in 1999, this series has been more competitive than the Browns' series with their other division rivals, and the Browns have won 9 of the last 11 meetings.

Created as a result of the Browns' relocation controversy, the rivalry between the Browns and Baltimore Ravens was more directed at Art Modell than the team itself, and is simply considered a divisional game in Baltimore. This matchup is more bitter for Cleveland than the others due to the fact that the draft picks for 1995 to 1998 resulted in the rosters that won the Super Bowl for the Ravens in 2000. Had Modell not moved the team, these teams, drafted by general manager and former Browns tight end Ozzie Newsome, might have given the Browns a title after a 35-year drought. The Ravens lead the overall series 34–12. The two teams have not met in the playoffs.

The Browns' rivalry with the Detroit Lions began in the 1950s, when the Browns and Lions played each other in four NFL Championship Games. The Lions won three of those championships, while the Browns won one. This was arguably one of the NFL's best rivalries in the 1950s. Since the NFL-AFL merger of 1970, the teams have met much less frequently with the Browns' move to the AFC. From 2002 to 2014, the two teams played an annual preseason game known as the "Great Lakes Classic".

The Browns had a brief rivalry with the Denver Broncos that arose from three AFC Championship Games from 1986 to 1989. In the 1986 AFC Championship, quarterback John Elway led The Drive to secure a tie in the waning moments at Cleveland Municipal Stadium; the Broncos went on to win in 23–20 in overtime. One year later, the two teams met again in the 1987 AFC Championship game at Mile High Stadium. Denver took a 21–3 lead, but Browns' quarterback Bernie Kosar threw four touchdown passes to tie the game at 31–31 halfway through the 4th quarter. After a long drive, John Elway threw a 20-yard touchdown pass to running back Sammy Winder to give Denver a 38–31 lead. Cleveland advanced to Denver's 8-yard line with 1:12 left, but Broncos' safety Jeremiah Castille stripped Browns' running back Earnest Byner of the football at the 2-yard line—a play that has been called The Fumble by Browns' fans. The Broncos recovered it, gave Cleveland an intentional safety, and went on to win 38–33. The two teams met yet again in the 1989 AFC Championship at Mile High Stadium, which the Broncos easily won by a score of 37–21. 

This short-lived rivalry also featured a controversial 16–13 Browns' win at Cleveland Municipal Stadium in the 1989 regular season. The game was decided by a Matt Bahr 48-yard field goal as time expired - a kick that barely cleared the crossbar. Bahr's field goal came after referee Tom Dooley ordered the teams to switch ends of the field midway through the 4th quarter, thanks to rowdy Dawg Pound fans who pelted the Broncos with dog biscuits, eggs and other debris. The switch gave the Browns a small, timely wind advantage to finish the game.

More recently, the rivalry has cooled off as the Broncos won 11 straight meetings from 1991 to 2015 before Cleveland broke that streak with a narrow 17–16 win in . Denver leads the overall series, 24–7. 

The most competitive team in the AAFC era for the Browns was the San Francisco 49ers. San Francisco finished second to the Browns in each of the four seasons that the league played. Two of the Browns' four losses in that era were to the 49ers (including a loss that ended the Browns' 29-game unbeaten streak); the rivalry did not last into the NFL years, particularly after the teams were placed in opposite conferences in . The rivalry has turned into a friendly relationship as many 49ers personnel helped the Browns relaunch in 1999, specifically former 49ers president and CEO Carmen Policy and vice president/director of football operations Dwight Clark, who were hired by the expansion Browns in the same roles. In addition, 49ers owners John York and Denise DeBartolo York reside in Youngstown, southeast of Cleveland. Long-time Browns placekicker and fan favorite Phil Dawson and backup quarterback Colt McCoy signed with the 49ers in 2014.

The Browns' rivalry with the Houston Oilers/Tennessee Titans dates back to the Browns and then-Oilers being placed in the AFC Central after the AFL-NFL merger in 1970. As such, the teams played each other twice annually from 1970 until 2002 when divisional realignment placed the Browns in the AFC North and the now-Titans in the AFC South (excluding 1996-98 when the Browns were inactive). The teams have met much less frequently since 2002. The Browns lead the overall series 37–32, and the 69 meetings with the Oilers/Titans are the third-most of any Cleveland opponent, trailing only the Steelers and Bengals.

The height of this rivalry was during the 1980s. Oilers head coach Jerry Glanville and Marty Schottenheimer shared several bitter exchanges during the decade and the Browns and Oilers had their only playoff meeting in the , in which the Oilers came away with a narrow 24–23 victory. There have been a few memorable games in recent years. In , the Browns erased a 28–3 deficit to come away with a 29–28 win. In a December contest with playoff implications for both teams, the Browns jumped to a 38–7 halftime lead, setting a franchise record for points in the first half. However, Tennessee rallied in the second half but came up just short as the Browns hung on for a 41–35 win.

A 2006 study conducted by "Bizjournal" determined that Browns fans are the most loyal fans in the NFL. The study, while not scientific, was largely based on fan loyalty during winning and losing seasons, attendance at games, and challenges confronting fans (such as inclement weather or long-term poor performance of their team). The study noted that Browns fans filled 99.8% of the seats at Cleveland Browns Stadium during the last seven seasons, despite a combined record of 36–76 over that span.

Perhaps the most visible Browns fans are those that can be found in the Dawg Pound. Originally the name for the bleacher section located in the open (east) end of old Cleveland Municipal Stadium, the current incarnation is likewise located in the east end of FirstEnergy Stadium and still features hundreds of orange and brown clad fans sporting various canine-related paraphernalia. The fans adopted that name in 1984 after members of the Browns defense used it to describe the team's defense.

Retired cornerback Hanford Dixon, who played his entire career for the Browns (1981–1989), is credited with naming the Cleveland Browns defense 'The Dawgs' in the mid-1980s. Dixon and teammates Frank Minnifield and Eddie Johnson would bark at each other and to the fans in the bleachers at the Cleveland Stadium to fire them up. It was from Dixon's naming that the "Dawg Pound" subsequently took its title. The fans adopted that name in the years after. Due to this nickname, since the team's revival the Browns have used a bulldog as an alternate logo.
The most prominent organization of Browns fans is the "Browns Backers Worldwide" (BBW). The organization has approximately 305,000 members and Browns Backers clubs can be found in every major city in the United States, and in a number of military bases throughout the world, with the largest club being in Phoenix, Arizona. In addition, the organization has a sizable foreign presence in places as far away as Egypt, Australia, Japan, Sri Lanka, and McMurdo Station in Antarctica. According to The Official Fan Club of the Cleveland Browns, the two largest international fan clubs are in Alon Shvut, West Bank and Niagara, Canada, with Alon Shvut having 129 members and Niagara having 310.

Following former Browns owner Randy Lerner's acquisition of English soccer club Aston Villa, official Villa outlets started selling Cleveland Browns goods such as jerseys and NFL footballs. This has raised interest in England and strengthened the link between the two sporting clubs. Aston Villa supporters have set up an organization known as the Aston (Villa) Browns Backers of Birmingham. 

The Cleveland Browns have the fourth-largest number of players enshrined in the Pro Football Hall of Fame with a total of 17 enshrined players elected based on their performance with the Browns, and nine more players or coaches elected who spent at least one year with the Browns franchise. No Browns players were inducted in the inaugural induction class of 1963. Otto Graham was the first Browns player to be enshrined as a member of the class of 1965, and the most recent Browns player to be included in the Pro Football Hall of Fame is Joe Thomas, who was a member of the class of 2023, who is the first member inducted that played in the 21st century. 

The Cleveland Browns legends program honors former Browns who made noteworthy contributions to the history of the franchise. In addition to all the Hall of Famers listed above, the Legends list includes:

Beginning in 2010, the Browns established a Ring of Honor, honoring the greats from the past by having their names displayed around the upper deck of FirstEnergy Stadium. The inaugural class in the Browns Ring of Honor was unveiled during the home opener on September 19, 2010, and featured the 16 Hall of Famers listed above who went into the Hall of Fame as Browns. In 2018, Joe Thomas was entered into the Ring of Honor with the number 10,363 – commemorating his NFL record of consecutive snaps played on offense. In 2019, four-time Pro Bowl linebacker Clay Matthews Jr. was entered into the Ring of Honor.

Numerous Browns players and staff have had statues made in their honor:
In and around First Energy Stadium

In and around Cleveland

Browns players featured on murals in downtown Cleveland include:


Radio

WKNR (850 AM), WKRK-FM (92.3 FM), and WNCX (98.5 FM) serve as co-flagship stations for the Cleveland Browns Radio Network. 

Jim Donovan serves as play-by-play announcer, calling games on-site alongside commentator Nathan Zegura – who made news when he had to serve an eight-game suspension due to arguing with officials during a game in 2018 when he was sideline reporter, and former NFL player and current WKNR host Je'Rod Cherry, who serves as sideline analyst/reporter. Cherry, WKRK's Ken Carman and Andy Baskin, and Cleveland area native/former NFL player Tyvis Powell host the network pregame show, while WKRK's Jeff Phelps and Powell host the network postgame show.

Spanish language broadcasts are heard on WNZN 89.1 FM with announcers Rafa Hernández-Brito and Octavio Sequera.

TV

Cleveland ABC affiliate WEWS-TV 5 serves as the broadcast TV home of the Browns, airing year-round team programming as well as all non-network preseason games, with the broadcast team of Chris Rose (play-by-play), former Browns left tackle Joe Thomas (analyst), and Aditi Kinkhabwala (sidelines). Bally Sports Great Lakes is the cable outlet for the team, airing various Browns related programming during the season.

Honors

The Browns in-house production team won a pair of Lower Great Lakes Emmy Awards in 2005. One was for a primetime special honoring the 1964 NFL Championship team ("The 1964 Championship Show") and one was for a commercial spot ("The Paperboy").
The Browns have (either directly or indirectly) been featured in various movies and TV shows over the years. Notable examples include:



Carbine

A carbine ( or ) is a long gun that has a barrel shortened from its original length. Most modern carbines are rifles that are compact versions of a longer rifle or are rifles chambered for less powerful cartridges.

The smaller size and lighter weight of carbines make them easier to handle. They are typically issued to high-mobility troops such as special operations soldiers and paratroopers, as well as to mounted, artillery, logistics, or other non-infantry personnel whose roles do not require full-sized rifles, although there is a growing tendency for carbines to be issued to front-line soldiers to offset the increasing weight of other issued equipment. An example of this is the M4 carbine, the standard issue carbine of the United States Armed Forces.

The name comes from its first users — cavalry troopers called "carabiniers", from the French "carabine", from Old French "carabin" (soldier armed with a musket), whose origin is unclear. One theory connects it to an "ancient engine of war" called a "calabre"; another connects it to Medieval Latin "Calabrinus" 'Calabrian'; yet another, less likely, to "escarrabin", gravedigger, from the scarab beetle.

In 1432, the Joseon dynasty under the reign of Sejong the Great introduced the world's first handgun, named (). The chongtong has a total length of 13.8 cm, inner diameter of 0.9 cm, and outer diameter of 1.4 cm. It is held by its cheolheumja (, iron tong-handle), which allows a quick change of barrel for the next shot, and fires chase-jeon (, a contemporary type of standardized arrow) with a maximum fatal range of around 250 meters. Initially, Joseon considered the gun a failure due to its short effective range, but the chongtong quickly saw use after fielding to the frontier provinces starting in June 1437. chongtong was used by both soldiers of different units and by civilians, including women and children, as a personal defense weapon. The gun was notably used by chetamja (, special reconnaissance), whose mission was to infiltrate enemy territory, and by carabiniers carrying multiple guns, who benefited from its compact size.

The carbine was originally developed for cavalry. The start of early modern warfare about the 16th century had infantry armed with firearms, prompting cavalry to do the same, even though reloading muzzle loading firearms while moving mounted was highly impractical. Some cavalry, such as the German Reiters, added one or more pistols, while other cavalry, such as harquebusiers, tried various shorter, lightened versions of the infantry arquebus weapons – the first carbines. But these weapons were still difficult to reload while mounted, and the saber often remained main weapon of such cavalry. Dragoons and other mounted infantry that dismounted for battles usually adopted standard infantry firearms, though some favored versions that were less encumbering when riding – something that could be arranged to hang clear of the rider's elbows and horse's legs.

While more portable, carbines had the general disadvantages of less accuracy and power than the longer guns of the infantry. During Napoleonic warfare, pistol and carbine-armed cavalry generally transitioned into traditional melee cavalry or dragoons. Carbines found increased use outside of standard cavalry and infantry, such as support and artillery troops, who might need to defend themselves from attack but would be hindered by keeping full-sized weapons with them continuously; a common title for many short rifles in the late 19th century was "artillery carbine".

As the rifled musket replaced the smoothbore firearms for infantry in the mid 19th century, carbine versions were also developed; this was often developed separately from the infantry rifles and, in many cases, did not even use the same ammunition, which made for supply difficulties.

A notable weapon developed towards the end of the American Civil War by the Union was the Spencer carbine, one of the first breechloading, repeating weapons. It had a spring-powered, removable tube magazine in the buttstock which held seven rounds and could be reloaded by inserting spare tubes. It was intended to give the cavalry a replacement weapon which could be fired from horseback without the need for awkward reloading after each shot – although it saw service mostly with dismounted troopers, as was typical of cavalry weapons during that war.

In the late 19th century, it became common for a number of nations to make bolt-action rifles in both full-length and carbine versions. One of the most popular and recognizable carbines were the lever-action Winchester carbines, with several versions available firing revolver cartridges. This made it an ideal choice for cowboys and explorers, as well as other inhabitants of the American West, who could carry a revolver and a carbine, both using the same ammunition.

The Lee Enfield Cavalry Carbine, a shortened version of the standard British Army infantry rifle was introduced in 1896, although it did not become the standard British cavalry weapon until 1903.

In late 1918, France developed the Chauchat-Ribeyrolles for tank crews to defend themselves. Developed from the Fusil Automatique Modèle 1917, the stock was replaced with a pistol grip, and the barrel is significantly shorter at resulting in an overall length of .

In the decades following World War I, the standard battle rifle used by armies around the world had been growing shorter, either by redesign or by the general issue of carbine versions instead of full-length rifles. This move was initiated by the U.S. Model 1903 Springfield, which was originally produced in 1907 with a short barrel, providing a short rifle that was longer than a carbine but shorter than a typical rifle, so it could be issued to all troops without need for separate versions. Other nations followed suit after World War I, when they learned that their traditional long-barreled rifles provided little benefit in the trenches and merely proved a hindrance to the soldiers. Examples include the Russian Model 1891 rifle, originally with an barrel, later shortened to in 1930, and to in 1938, the German Mauser Gewehr 98 rifles went from in 1898 to in 1935 as the "Karabiner 98k" (K98k or Kar98k), or "short carbine".

The barrel lengths in rifles used by the United States did not change between the bolt-action M1903 rifle of World War I and the World War II M1 Garand rifle, because the barrel on the M1903 was still shorter than even the shortened versions of the Model 1891 and Gewehr 98. The U.S. M1 carbine was more of a traditional carbine in that it was significantly shorter and lighter, with a barrel, than the M1 Garand rifle, and that it was intended for rear-area troops who could not be hindered with full-sized rifles but needed something more powerful and accurate than a Model 1911 pistol (although this did not stop soldiers from using them on the front line). Contrary to popular belief, and even what some books claim, in spite of both being designated "M1", the M1 Carbine was "not" a shorter version of the .30-06 M1 Garand, as is typical for most rifles and carbines, but it was a wholly different design, firing a smaller, less-powerful cartridge. The "M1" designates each as the first model in the new U.S. designation system, which no longer used the year of introduction but a sequential series of numbers starting at "1": the M1 "Carbine" and M1 "Rifle".

The United Kingdom developed a "Jungle Carbine" version of their Lee–Enfield service rifle, featuring a shorter barrel, flash suppressor, and manufacturing modifications designed to decrease the rifle's weight Officially titled "Rifle, No. 5 Mk I", it was introduced in the closing months of World War II, but it did not see widespread service until the Korean War, the Mau Mau Uprising, and the Malayan Emergency as well as the Vietnam War.

A shorter weapon was more convenient when riding in a truck, armored personnel carrier, helicopter, or aircraft, and also when engaged in close-range combat. Based on the combat experience of World War II, the criteria used for selecting infantry weapons began to change. Unlike previous wars, which were often fought mainly from fixed lines and trenches, World War II was a highly mobile war, often fought in cities, forests, or other areas where mobility and visibility were restricted. In addition, improvements in artillery made moving infantry in open areas even less practical than it had been.

The majority of enemy contacts were at ranges of less than , and the enemy was exposed to fire for only short periods of time as they moved from cover to cover. Most rounds fired were not aimed at an enemy combatant but instead fired in the enemy's direction to keep them from moving and from firing back. These situations did not require a heavy rifle, firing full-power rifle bullets with long-range accuracy. A less-powerful weapon would still produce casualties at the shorter ranges encountered in actual combat, and the reduced recoil would allow more shots to be fired in the short amount of time an enemy was visible. The lower-powered round would also weigh less, allowing a soldier to carry more ammunition. With no need of a long barrel to fire full-power ammunition, a shorter barrel could be used. A shorter barrel made the weapon weigh less, was easier to handle in tight spaces, and was easier to shoulder quickly to fire a shot at an unexpected target. Full-automatic fire was also considered a desirable feature, allowing the soldier to fire short bursts of three to five rounds, increasing the probability of a hit on a moving target.

The Germans had experimented with selective-fire carbines firing rifle cartridges during the early years of World War II. These were determined to be less than ideal, as the recoil of full-power rifle cartridges caused the weapon to be uncontrollable in full-automatic fire. They then developed an intermediate-power cartridge round, which was accomplished by reducing the power and the length of the standard 7.92×57mm Mauser rifle cartridge to create the 7.92×33mm (short) cartridge. A selective-fire weapon was developed to fire this shorter cartridge, eventually resulting in the Sturmgewehr 44, later translated as "assault rifle" (also frequently called "machine carbines" by Allied intelligence, a quite accurate assessment, in fact). Very shortly after World War II, the USSR adopted a similar weapon, the ubiquitous AK-47, the first model in the famed Kalashnikov-series, which became the standard Soviet infantry weapon and which has been produced and exported in extremely large numbers up through the present day.

Although the United States had developed the M2 Carbine, a selective-fire version of the M1 Carbine during WW2, the .30 Carbine cartridge was closer to a pistol round in power, making it more of a submachine gun than an assault rifle. It was also adopted only in very small numbers and issued to few troops (the semi-automatic M1 carbine was produced in a 10-to-1 ratio to the M2), while the AK47 was produced by the millions and was standard-issue to all Soviet troops, as well as those of many other nations. The U.S. was slow to follow suit, insisting on retaining a full-power, 7.62×51mm NATO rifle, the M14 (although this "was" selective fire).

In the 1950s, the British developed the .280 British, an intermediate cartridge, and a select-fire bullpup assault rifle to fire it, the EM-2. They pressed for the U.S. to adopt it so it could become a NATO-standard round, but the U.S. insisted on retaining a full-power, .30 caliber round. This forced NATO to adopt the 7.62×51mm NATO round (which in reality is only slightly different ballistically from the .308 Winchester), to maintain commonality. The British eventually adopted the 7.62mm FN FAL, and the U.S. adopted the 7.62mm M14 rifle. These rifles are both what is known as "battle rifles" and were a few inches shorter than the standard-issue rifles they replaced ( barrel as opposed to for the M1 Garand), although they were still full-powered rifles, with selective fire capability. These can be compared to the even shorter, less-powerful assault rifle, which might be considered the "carbine branch of weapons development", although indeed, there are now carbine variants of many of the assault rifles which had themselves seemed quite small and light when adopted.
By the 1960s, after becoming involved in war in Vietnam, the U.S. did an abrupt about-face and decided to standardize on the intermediate 5.56×45mm round (based on the .223 Remington varmint cartridge) fired from the new, lightweight M16 rifle, leaving NATO to hurry and catch up. Many of the NATO countries could not afford to re-equip so soon after the recent 7.62mm standardization, leaving them armed with full-power 7.62mm battle rifles for some decades afterwards, although by this point, the 5.56mm has been adopted by almost all NATO countries and many non-NATO nations as well. This 5.56mm NATO round was even lighter and smaller than the Soviet 7.62×39mm AK-47 cartridge but possessed higher velocity. In U.S. service, the M16 assault rifle replaced the M14 as the standard infantry weapon, although the M14 continued to be used by designated marksmen. Although at , the barrel of the M16 was shorter than that of the M14, it was still designated a "rifle" rather than a "carbine", and it was still longer than the AK-47, which used a barrel. (The SKS – an interim, semi-automatic, weapon adopted a few years before the AK-47 was put into service – was designated a carbine, even though its barrel was significantly longer than the AK series' . This is because of the Kalashnikov's revolutionary nature, which altered the old paradigm. Compared to previous rifles, particularly the Soviets' initial attempts at semi-automatic rifles, such as the SVT-40, the SKS was significantly shorter. The Kalashnikov altered traditional notions and ushered in a change in what was considered a "rifle" in military circles.)

In 1974, shortly after the introduction of the 5.56mm NATO, the USSR began to issue a new Kalashnikov variant, the AK-74, chambered in the small-bore 5.45×39mm cartridge, which was a standard 7.62×39mm necked down to take a smaller, lighter, faster bullet. It soon became standard issue in Soviet nations, although many of the nations with export Kalashnikovs retained the larger 7.62×39mm round. In 1995, the People's Republic of China adopted a new 5.8×42mm cartridge to match the modern trend in military ammunition, replacing the previous 7.62×39mm and 5.45×39mm round as standard.

Later, even lighter carbine variants of many of these short-barreled assault rifles came to be adopted as the standard infantry weapon. In much modern tactical thinking, only a certain number of soldiers need to retain longer-range weapons, serving as designated marksmen. The rest can carry lighter, shorter-ranged weapons for close quarters combat and suppressive fire. This is basically a more extreme extension of the idea that brought the original assault rifle. Another factor is that with the increasing weight of technology, sighting systems, ballistic armor, etc., the only way to reduce the burden on the modern soldier was to equip them with a smaller, lighter weapon. Also, modern soldiers rely a great deal on vehicles and helicopters to transport them around the battle area, and a longer weapon can be a serious hindrance to entering and exiting these vehicles. Development of lighter assault rifles continued, matched by developments in even lighter carbines. In spite of the short barrels of the new assault rifles, carbine variants like the 5.45×39mm AKS-74U and Colt Commando were being developed for use when mobility was essential and a submachine gun was not sufficiently powerful. The AKS-74U featured an extremely short barrel which necessitated redesigning and shortening the gas-piston and integrating front sights onto the gas tube; the Colt Commando was a bit longer, at . Neither was adopted as standard issue, although the U.S. did later adopt the somewhat longer M4 carbine, with a barrel.

In 1994, the U.S. had adopted the M4 carbine, a derivative of the M16 family which fired the same 5.56mm cartridge but was lighter and shorter (in overall length and barrel length), resulting in marginally reduced range and power, although offering better mobility and lighter weight to offset the weight of equipment and armor that a modern soldier has to carry.

In spite of the benefits of the modern carbine, many armies are experiencing a certain backlash against the universal equipping of soldiers with carbines and lighter rifles in general, and are equipping selected soldiers, usually designated marksmen, with higher-powered rifles. Another problem comes from the loss of muzzle velocity caused by the shorter barrel, which when coupled with the typical small, lightweight bullets, causes effectiveness to be diminished; a 5.56mm gets its lethality from its high velocity, and when fired from the M4 carbine, its power, penetration, and range are diminished. Thus, there has been a move towards adopting a slightly more powerful cartridge tailored for high performance from both long and short barrels. The U.S. has experimented with a new, slightly larger and heavier caliber such as the 6.5mm Grendel or 6.8mm Remington SPC, which are heavier and thus retain more effectiveness at lower muzzle velocities.

While the U.S. Army adopted the M4 carbine in 1994, the U.S. Marine Corps retained their barrel M16A4 rifles long afterwards, citing the increased range and effectiveness over the carbine version; officers were required to carry an M4 carbine rather than an M9 pistol, as Army officers do. Because the Marine Corps emphasizes "every Marine a rifleman", the lighter carbine was considered a suitable compromise between a rifle and a pistol. Marines with restricted mobility such as vehicle operators, or a greater need for mobility such as squad leaders, were issued M4 carbines. In 2015, the Marine Corps approved the M4 carbine for standard issue to front-line Marines, replacing the M16A4 rifle. The rifles are issued to support troops while the carbines go to the front-line Marines, in a reversal of the traditional roles of "rifles for the front line, carbines for the rear".

Special forces need to perform fast, decisive operations, frequently airborne or boat-mounted. A pistol, though light and quick to operate, is viewed as not having enough power, firepower, or range. A submachine gun has selective fire, but firing a pistol cartridge and having a short barrel and sight radius, it is not accurate or powerful enough at longer ranges. Submachine guns also tend to have poorer armor and cover penetration than rifles and carbines firing rifle ammunition. Consequently, carbines have gained wide acceptance among United States Special Operations Command, United Kingdom Special Forces, and other communities, having relatively light weight, large magazine capacity, selective fire, and much better range and penetration than a submachine gun.

The smaller size and relative lighter weight of carbines makes them easier to handle in close-quarter situations such as urban engagements, when deploying from military vehicles, or in any situation where space is confined. The disadvantages of carbines relative to rifles include inferior long-range accuracy and a shorter effective range. These comparisons refer to carbines (short-barreled rifles) of the same power and class as the regular full-sized rifles.

Compared to submachine guns, carbines have a greater effective range and are capable of penetrating helmets and body armor when used with armor-piercing ammunition. However, submachine guns are still used by military special forces and police SWAT teams for close quarters battle because they are "a pistol caliber weapon that's easy to control, and less likely to over-penetrate the target." Also, carbines are harder to maneuver in tight encounters where superior range and stopping power at distance are not great considerations.

Firing the same ammunition as standard-issue rifles or pistols gives carbines the advantage of standardization over those personal defense weapons that require proprietary cartridges.

The modern usage of the term carbine covers much the same scope as it always had, namely lighter weapons (generally rifles) with barrels up to in length. These weapons can be considered carbines, while rifles with barrels longer than are generally not considered carbines unless specifically named so. Conversely, many rifles have barrels "shorter" than 20 inches, yet are not considered carbines. The AK series rifles has an almost universal barrel length of , well within carbine territory, yet has always been considered a rifle, perhaps because it was designed as such and not shortened from a longer weapon. Modern carbines use ammunition ranging from that used in light pistols up to powerful rifle cartridges, with the usual exception of high-velocity magnum cartridges. In the more powerful cartridges, the short barrel of a carbine has significant disadvantages in velocity, and the high residual pressure, and frequently still-burning powder and gases, when the bullet exits the barrel results in substantially greater muzzle blast. Flash suppressors are a common, partial solution to this problem, although even the best flash suppressors are hard put to deal with the excess flash from the still-burning powder leaving the short barrel (and they also add several inches to the length of the barrel, diminishing the purpose of having a short barrel in the first place).

The typical carbine is the pistol-caliber carbine. These first appeared soon after metallic cartridges became common. These were developed as "companions" to the popular revolvers of the day, firing the same cartridge but allowing more velocity and accuracy than the revolver. These were carried by cowboys, lawmen, and others in the Old West. The classic combination would be a Winchester lever-action carbine and a Colt Single Action Army revolver in .44-40 or .38-40. During the 20th century, this trend continued with more modern and powerful smokeless revolver cartridges, in the form of Winchester and Marlin lever action carbines chambered in .38 Special/.357 Magnum and .44 Special/.44 Magnum.

Modern equivalents include the Ruger Police Carbine and Ruger PC Carbine, which uses the same magazine as the Ruger pistols of the same caliber, and the (discontinued) Marlin Camp Carbine, which, in .45 ACP, used M1911 magazines. The Ruger Model 44 and Ruger Deerfield Carbine were both carbines chambered in .44 Magnum. The Beretta Cx4 Storm shares magazines with many Beretta pistols and is designed to be complementary to the Beretta Px4 Storm pistol. The Hi-Point 995TS are popular, economical and reliable alternatives to other pistol caliber carbines in the United States, and their magazines can be used in the Hi-Point C-9 pistol. Another example is the Kel-Tec SUB-2000 series chambered in either 9mm Parabellum or .40 S&W, which can be configured to accept Glock, Beretta, S&W, or SIG pistol magazines. The SUB-2000 also has the somewhat unusual (although not unique) ability to fold in half.

The primary advantage of a carbine over a pistol using the same ammunition is controllability. The combination of firing from the shoulder, longer sight-radius, three points of contact (firing hand, support hand, and shoulder), and precision offer a significantly more user-friendly platform. Carbines like the Kel-Tec SUB-2000, Hi Point 995TS, Just Right Carbines (JR Carbine), and Beretta Cx4 Storm have the ability to mount user-friendly optics, lights and lasers thanks to them having accessory rails, which make target acquisition and engagement much easier.
The longer barrel can offer increased velocity and, with it, greater energy and effective range due to the propellant having more time to burn. However, loss in bullet velocity can happen where the propellant is utilised before the bullet reaches the muzzle, combined with the friction from the barrel on the bullet. As long guns, pistol-caliber carbines may be less legally restricted than handguns in some jurisdictions. Compared to carbines chambered in intermediate or rifle calibers, such as .223 Remington and 7.62×54mmR, pistol-caliber carbines generally experience less of an increase in external ballistic properties as a result of the propellant. The drawback is that one loses the primary benefits of a handgun, i.e. portability and concealability, resulting in a weapon almost the size of, but less accurate than, a long-gun, but not much more powerful than a pistol.

Also widely produced are semi-automatic and typically longer-barreled derivatives of select-fire submachine guns, such as the FN PS90, HK USC, KRISS Vector, Thompson carbine, CZ Scorpion S1 Carbine, and the Uzi carbine. In order to be sold legally in many countries, the barrel must meet a minimum length ( in the United States). So the original submachine gun is given a legal-length barrel and made into a semi-automatic firearm, transforming it into a carbine. Though less common, pistol-caliber conversions of centerfire rifles like the AR-15 are commercially available.

Some handguns used to come from the factory with mounting lugs for a shoulder stock, notably including the "Broomhandle" Mauser C96, Luger P.08, and Browning Hi-Power. In the case of the first two, the pistol could come with a hollow wooden stock that doubled as a holster.

Carbine conversion kits are commercially available for many other pistols, including M1911, and most Glocks. These can either be simple shoulder stocks fitted to a pistol or full carbine conversion kits, which are at least long and replace the pistol's barrel with one at least long for compliance with the United States law. In the United States, fitting a shoulder stock to a handgun with a barrel less than long possibly turns said firearm into a short-barreled rifle, which may be in violation of the National Firearms Act; this is currently being adjudicated by the courts.

Under the National Firearms Act of 1934, firearms with shoulder stocks or originally manufactured as a rifle and barrels less than in length are classified as short-barreled rifles. Short-barreled rifles are restricted similarly to short-barreled shotguns, requiring a $200 tax paid prior to manufacture or transfer – a process which can take several months. Because of this, firearms with barrels of less than and a shoulder stock are uncommon. A list of firearms not covered by the NFA due to their antique status may be found here or due to their "Curio and Relic" status may be found here; these lists includes a number of carbines with barrels less than the minimum legal length and firearms that are "primarily collector's items and are not likely to be used as weapons and, therefore, are excluded from the provisions of the National Firearms Act." Machine guns, as their own class of firearm, are not subject to requirements of other class firearms.

Distinct from simple shoulder stock kits, full carbine conversion kits are not classified as short-barreled rifles. By replacing the pistol barrel with one at least in length and having an overall length of at least , a carbine converted pistol may be treated as a standard rifle under Title I of the Gun Control Act of 1968 (GCA). However, certain "Broomhandle" Mauser C96, Luger, and Browning Hi-Power Curio & Relic pistols with their originally issued stock attached only may retain their pistol classification.

Carbines without a stock and not originally manufactured as a rifle are not classified as rifles or short barreled rifles. A carbine manufactured under in length without a forward vertical grip will be a pistol and, state law notwithstanding, can be carried concealed without creating an unregistered Any Other Weapon. A nearly identical carbine with an overall length of or greater is simply an unclassified firearm under Title I of the Gun Control Act of 1968, as the Any Other Weapon catch-all only applies to firearms under or that have been concealed. However, a modification intending to fire from the shoulder and bypass the regulation of short-barreled rifles is considered the unlawful possession and manufacture of an unregistered short-barreled rifle.

In some historical cases, the term "machine carbine" was the official title for submachine guns, such as the British Sten and Australian Owen guns. The semiautomatic-only version of the Sterling submachine gun was also officially called a "carbine". The original Sterling semi-auto would be classed a "short barrel rifle" under the U.S. National Firearms Act, but fully legal long-barrel versions of the Sterling have been made for the U.S. collector market.



Chinese cuisine

Chinese cuisine comprises cuisines originating from China, as well as from Chinese people from other parts of the world. Because of the Chinese diaspora and historical power of the country, Chinese cuisine has profoundly influenced many other cuisines in Asia and beyond, with modifications made to cater to local palates. Chinese food staples such as rice, soy sauce, noodles, tea, chili oil, and tofu, and utensils such as chopsticks and the wok, can now be found worldwide. 

The world's earliest eating establishments recognizable as restaurants in the modern sense first emerged in Song dynasty China during the 11th and 12th centuries. Street food became an integral aspect of Chinese food culture during the Tang dynasty, and the street food culture of much of Southeast Asia was established by workers imported from China during the late 19th century. 

The preferences for seasoning and cooking techniques of Chinese provinces depend on differences in social class, religion, historical background, and ethnic groups. Geographic features including mountains, rivers, forests, and deserts also have a strong effect on the local available ingredients, considering that the climate of China varies from tropical in the south to subarctic in the northeast. Imperial royal and noble preference also plays a role in the change of Chinese cuisine. Because of imperial expansion, immigration, and trading; ingredients and cooking techniques from other cultures have been integrated into Chinese cuisines over time, and Chinese culinary influences have also spread across the world.

There are numerous regional, religious, and ethnic styles of Chinese cuisine found within China and abroad. Chinese cuisine is highly diverse and most frequently categorised into provincial divisions, although these province-level classifications consist of many more styles within themselves. During the Qing dynasty, the most praised Four Great Traditions in Chinese cuisine were Chuan, Lu, Yue, and Huaiyang, representing cuisines of West, North, South, and East China, respectively. In 1980, a modern grouping from Chinese journalist Wang Shaoquan's article published in the People's Daily newspaper identified the Eight Cuisines of China as Anhui (), Guangdong (), Fujian (), Hunan (), Jiangsu (), Shandong (), Sichuan (), and Zhejiang ().

Chinese cuisine is deeply intertwined with traditional Chinese medicine, such as in the practise of Chinese food therapy. Color, scent and taste are the three traditional aspects used to describe Chinese food, as well as the meaning, appearance, and nutrition of the food. Cooking should be appraised with respect to the ingredients used, knife work, cooking time, and seasoning. 

Chinese society greatly valued gastronomy, and developed an extensive study of the subject based on its traditional medical beliefs. Chinese culture initially centered around the North China Plain. The first domesticated crops seem to have been the foxtail and broomcorn varieties of millet, while rice was cultivated in the south. By 2000 BC, wheat had arrived from western Asia. These grains were typically served as warm noodle soups instead of baked into bread as in Europe. Nobles hunted various wild game and consumed mutton, pork and dog as these animals were domesticated. Grain was stored against famine and flood and meat was preserved with salt, vinegar, curing, and fermenting. The flavor of the meat was enhanced by cooking it in animal fats though this practice was mostly restricted to the wealthy.

By the time of Confucius in the late Zhou, gastronomy had become a high art. Confucius discussed the principles of dining: The Lüshi chunqiu notes: "Only if one is chosen as the Son of Heaven will the tastiest delicacies be prepared [for him]."

The Zhaohun (4-3rd c. BC) gives some examples: turtle ragout, honey cakes and beer (chilled with ice).

During Shi Huangdi's Qin dynasty, the empire expanded into the south. By the time of the Han dynasty, the different regions and cuisines of China's people were linked by major canals and leading to greater complexity in the different regional cuisines. Not only is food seen as giving "qi", energy, but the food is also about maintaining yin and yang. The philosophy behind it was rooted in the "I Ching" and Chinese traditional medicine: food was judged for color, aroma, taste, and texture and a good meal was expected to balance the Four Natures ('hot', warm, cool, and 'cold') and the Five Tastes (pungent, sweet, sour, bitter, and salty). Salt was used as a preservative from early times, but in cooking was added in the form of soy sauce, and not at the table.

By the Later Han period (2nd century), writers frequently complained of lazy aristocrats who did nothing but sit around all day eating smoked meats and roasts.

During the Han dynasty, the Chinese developed methods of food preservation for military rations during campaigns such as drying meat into jerky and cooking, roasting, and drying grain.
Chinese legends claim that the roasted, flat bread shaobing was brought back from the "Xiyu" (the Western Regions, a name for Central Asia) by the Han dynasty General Ban Chao, and that it was originally known as hubing (, lit. "barbarian bread"). The shaobing is believed to be descended from the hubing. Shaobing is believed to be related to the Persian "nan" and Central Asian "nan", as well as the Middle Eastern pita. Foreign westerners made and sold sesame cakes in China during the Tang dynasty.

During the Southern and Northern dynasties non-Han people like the Xianbei of Northern Wei introduced their cuisine to northern China, and these influences continued up to the Tang dynasty, popularizing meat like mutton and dairy products like goat milk, yogurts, and Kumis among even Han people. It was during the Song dynasty that Han Chinese developed an aversion to dairy products and abandoned the dairy foods introduced earlier.

The Han Chinese rebel Wang Su who received asylum in the Xianbei Northern Wei after fleeing from Southern Qi, at first could not stand eating dairy products like goat's milk and meat like mutton and had to consume tea and fish instead, but after a few years he was able to eat yogurt and lamb, and the Xianbei Emperor asked him which of the foods of China (Zhongguo) he preferred, fish vs mutton and tea vs yogurt.

The great migration of Chinese people south during the invasions preceding and during the Song dynasty increased the relative importance of southern Chinese staples such as rice and congee. Su Dongpo has improved the red braised pork as Dongpo pork. The dietary and culinary habits also changed greatly during this period, with many ingredients such as soy sauce and Central Asian influenced foods becoming widespread and the creation of important cookbooks such as the "Shanjia Qinggong" () and the "Wushi Zhongkuilu" () showing the respective esoteric foods and common household cuisine of the time.

The Yuan and Qing dynasties introduced Mongolian and Manchu cuisine, warm northern dishes that popularized hot pot cooking. During the Yuan dynasty many Muslim communities emerged in China, who practiced a porkless cuisine now preserved by Hui restaurants throughout the country. Yunnan cuisine is unique in China for its cheeses like Rubing and Rushan cheese made by the Bai people, and its yogurt, the yogurt may have been due to a combination of Mongolian influence during the Yuan dynasty, the Central Asian settlement in Yunnan, and the proximity and influence of India and Tibet on Yunnan.

As part of the last leg of the Columbian Exchange, Spanish and Portuguese traders began introducing foods from the New World to China through the port cities of Canton and Macau. Mexican chili peppers became essential ingredients in Sichuan cuisine and calorically dense potatoes and corn became staple foods across the northern plains.

During the Qing dynasty, Chinese gastronomes such as Yuan Mei focused upon the primary goal of extracting the maximum flavour of each ingredient. As noted in his culinary work the "Suiyuan shidan", however, the fashions of cuisine at the time were quite varied and in some cases were flamboyantly ostentatious, especially when the display served also a formal ceremonial purpose, as in the case of the Manchu Han Imperial Feast.

As the pace of life increases in modern China, fast food like fried noodles, fried rice and "gaifan" (dish over rice) become more and more popular.

There are a variety of styles of cooking in China, but most Chinese chefs classified eight regional cuisines according to their distinct tastes and local characteristics. A number of different styles contribute to Chinese cuisine but perhaps the best known and most influential are Cantonese cuisine, Shandong cuisine, Jiangsu cuisine (specifically Huaiyang cuisine) and Sichuan cuisine. These styles are distinctive from one another due to factors such as availability of resources, climate, geography, history, cooking techniques and lifestyle. One style may favour the use of garlic and shallots over chili and spices, while another may favour preparing seafood over other meats and fowl. Jiangsu cuisine favours cooking techniques such as braising and stewing, while Sichuan cuisine employs baking. Zhejiang cuisine focuses more on serving fresh food and shares some traits in common with Japanese food. Fujian cuisine is famous for its seafood and soups and the use of spices. Hunan cuisine is famous for its hot and sour taste. Anhui cuisine incorporates wild food for an unusual taste and is wilder than Fujian cuisine.

Based on the raw materials and ingredients used, the method of preparation and cultural differences, a variety of foods with different flavors and textures are prepared in different regions of the country. Many traditional regional cuisines rely on basic methods of preservation such as drying, salting, pickling and fermentation.

In addition, the "rice theory" attempts to describe cultural differences between north and south China; in the north, noodles are more consumed due to wheat being widely grown whereas in the south, rice is more preferred as it has historically been more cultivated there.

Chinese ancestors successfully planted millet, rice, and other grains about 8,000 to 9,000 years ago. Wheat, another staple, took another three or four thousand years. For the first time, grains provided people with a steady supply of food. Because of the lack of various foods, Chinese people had to adapt to new eating habits. Meat was scarce, and so people cooked with small amounts of meat and rice or noodles.

Rice is a primary staple food for people from rice farming areas in southern China. Steamed rice, usually white rice, is the most commonly eaten form. People in South China also like to use rice to make congee as breakfast. Rice is also used to produce beer, baijiu and vinegar. Glutinous rice ("sticky rice") is a variety of rice used in special dishes such as lotus leaf rice and glutinous rice balls.

In wheat-farming areas in Northern China, people largely rely on flour-based food, such as noodles, "bing" (bread), "jiaozi" (a kind of Chinese dumplings), and "mantou" (a type of steamed buns). Wheat likely "appeared in the lower Yellow River around 2600 Before Common Era (BCE), followed by Gansu and Xinjiang around 1900 BCE and finally occurred in the middle Yellow River and Tibet regions by 1600 BCE".

Chinese noodles come dry or fresh in a variety of sizes, shapes and textures and are often served in soups or fried as toppings. Some varieties, such as Shou Mian (寿面, literally noodles of longevity), is an avatar of long life and good health according to Chinese traditions. Noodles can be served hot or cold with different toppings, with broth, and occasionally dry (as is the case with mi-fen). Noodles are commonly made with rice flour or wheat flour, but other flours such as soybean are also used in minor groups. Some noodles names describe their methods of creation, such as the hand-pulled noodle.

Tofu is made of soybeans and is another popular food product that supplies protein. The production process of tofu varies from region to region, resulting in different kinds of tofu with a wide range of texture and taste. Other products such as soy milk, soy paste, soy oil, and fermented soy sauce are also important in Chinese cooking.

There are many kinds of soybean products, including tofu skin, smoked tofu, dried tofu, and fried tofu.

Stinky tofu is fermented tofu. Like blue cheese or durian, it has a very distinct, potent and strong smell, and is an acquired taste. Hard stinky tofu is often deep-fried and paired with soy sauce or salty spice. Soft stinky tofu is usually used as a spread on steamed buns.

Doufuru is another type of fermented tofu that has a salty taste. Doufuru can be pickled together with soy beans, red yeast rice or chili to create different color and flavor. This is more of a pickled type of tofu and is not as strongly scented as stinky tofu. Doufuru has the consistency of slightly soft blue cheese, and a taste similar to Japanese miso paste, but less salty. Doufuru can be used as a spread on steamed buns, or paired with rice congee.

Sufu is one other type of fermented tofu that goes through ageing process. The color (red, white, green) and flavor profile can determine the type of sufu it is. This kind of tofu is usually eaten alongside breakfast rice.

Soybean milk is soybean-based milk. It is a morning beverage, and it has many benefits to human health.

Apart from vegetables that can be commonly seen, some unique vegetables used in Chinese cuisine include baby corn, bok choy, snow peas, Chinese eggplant, Chinese broccoli, and straw mushrooms. Other vegetables, including bean sprouts, pea vine tips, watercress, lotus roots, chestnuts, water chestnuts, and bamboo shoots, are also used in different cuisines of China.

Because of different climate and soil conditions, cultivars of green beans, peas, and mushrooms can be found in rich variety.

A variety of dried or pickled vegetables are also processed, especially in drier or colder regions where fresh vegetables were hard to get out of season.

Seasonings such as fresh ginger root, garlic, scallion, cilantro and sesame are widely used in many regional cuisines. Sichuan peppercorns, star anise, cinnamon, fennel, cloves and white peppers and smart weed are also used in different regions.

To add extra flavor to the dishes, many Chinese cuisines also contain dried Chinese mushrooms, dried baby shrimp, dried tangerine peel, and dried Sichuan chillies.

When it comes to sauces, China is home to soy sauce, which is made from fermented soybeans and wheat. A number of sauces are also based on fermented soybeans, including hoisin sauce, ground bean sauce and yellow bean sauce. There are also different sauces preferred by regional cuisines, oyster sauce, fish sauce and furu (fermented tofu) are also widely used. Vinegar also has a variety with different flavors: clear rice vinegar, Chinkiang black rice vinegar, Shanxi vinegar, Henghe vinegar etc.

Generally, seasonal fruits serve as the most common form of dessert consumed after dinner.

Dim sum (点心), originally means a small portion of food, can refer to dessert, or pastries. Later to avoid disambiguation, tian dian (甜点) and gao dian (糕点) are used to describe desserts and pastries.

Traditionally, Chinese desserts are sweet foods and dishes that are served with tea, usually during the meal, or at the end of meals in Chinese cuisine.

Besides being served as dim sum along with tea, pastries are used for celebration of traditional festivals. The most famous one is moon cake, used to celebrate the Mid-Autumn Festival.

A wide variety of Chinese desserts are available, mainly including steamed and boiled sweet snacks. Bing is an umbrella term for all breads in Chinese, also including pastries and sweets. These are baked wheat-flour-based confections, with different stuffings including red bean paste, jujube, and a variety of others. Su (酥) is another kind of pastry made with more amount of oil, making the confection more friable. Chinese candies and sweets, called "táng" (糖) are usually made with cane sugar, malt sugar, honey, nuts, and fruit. Gao or Guo are rice-based snacks that are typically steamed and may be made from glutinous or normal rice.

Another cold dessert is called "baobing", which is shaved ice with sweet syrup. Chinese jellies are known collectively in the language as "ices". Many jelly desserts are traditionally set with agar and are flavoured with fruits, known as guodong (果冻), though gelatine based jellies are also common in contemporary desserts.

Chinese dessert soups are typically sweet and served hot.

European pastries are also seen in China, like mille-feuille, crème brûlée, and cheesecake, but they are generally not as popular because the Chinese preference of dessert is mildly sweet and less oily.

Many types of street foods, which vary from region to region, can be eaten as snacks or light dinner. Prawn crackers are an often-consumed snack in Southeast China.

Chinese in earlier dynasties evidently drank milk and ate dairy products, although not necessarily from cows, but perhaps "kumis" (fermented mare's milk) or goat's milk.

Historically, many Chinese chefs tried not to use milk, because of the high rate of lactose intolerance among the Chinese population. However, today, dairy products are increasingly used in Chinese cuisine, such as the "double skin milk" dessert in Guangdong Province, the Rubing (milk cake) cheese in Yunnan, and yoghurt in Qinghai and Xinjiang. China has a wide variety of dairy desserts that are very popular.

Cold dishes are usually served before the main meal. Besides salad and pickles as appetizers, they can range from jelly, beancurd, noodle salad, cooked meat and sausages, to jellyfish or cold soups.

Chinese sausages vary from region to region. The most common sausage is made of pork and pork fat. The flavor is generally salty-sweet in Southern China. In other parts of China, sausages are salted to be preserved. Chinese sausage is prepared in many different ways, including oven-roasting, stir-frying, and steaming.

In some part of South China, soups are served between the cold dishes and the main dishes. In other parts of China, soups are served between the main dish and staple foods, before desserts or fruit salad. There are many traditional Chinese soups, such as wonton soup, herbal chicken soup, hot and sour soup, winter melon soup, and so on.

Tea plays an important role in Chinese dining culture. In China, there are two main types of tea, one is made from dried tea leaves, the other one is made by extracts from tea leaves. Baijiu and huangjiu as strong alcoholic beverages are preferred by many people as well. Wine is not so popular as other drinks in China that are consumed whilst dining, although they are usually available in the menu.

As well as with dim sum, many Chinese drink their tea with snacks such as nuts, plums, dried fruit (in particular jujube), small sweets, melon seeds, and waxberry. China was the earliest country to cultivate and drink tea, which is enjoyed by people from all social classes. Tea processing began after the Qin and Han dynasties.

The different types of Chinese tea include black, white, green, yellow, oolong, and dark tea. Chinese tea is often classified into several different categories according to the species of plant from which it is sourced, the region in which it is grown, and the method of production used. Some of these types are green tea, oolong tea, black tea, scented tea, white tea, and compressed tea. There are four major tea plantation regions: Jiangbei, Jiangnan, Huanan and the southwestern region. Well known types of green tea include Longjing, Huangshan Maofeng, Bilochun, Putuofeng Cha, and Liu'an Guapian. China is the world's largest exporter of green tea.

One of the most ubiquitous accessories in modern China, after a wallet or purse and an umbrella, is a double-walled insulated glass thermos with tea leaves in the top behind a strainer.

The importance of "baijiu" ( "white liquor") in China (99.5% of its alcoholic market) makes it the most-consumed alcoholic spirit in the world. It dates back to the introduction of distilling during the Song dynasty; can be made from wheat, corn, or rice; and is usually around 120 proof (60% ABV). The most ubiquitous brand is the cheap Er guo tou, but Mao Tai is the premium "baijiu". Other popular brands include Kang, Lu Zhou Te Qu, and Wu Liang Ye.

"Huangjiu" ( "yellow liquor") is not distilled and is a strong rice wine (10–15% ABV). Popular brands include Shaoxing Lao Jiu, Shaoxing Hua Diao, and Te Jia Fan.

While fermented grain beverages have been brewed in China for over 9,000 years, it has been long overshadowed by stronger alcohol like Baijiu and Huangjiu.

Chinese herb tea, also known as "medicinal herbal tea", is a kind of tea made from Chinese medicinal herbs.

Soy milk, almond milk, walnut milk and coconut milk are also drunk during the meal in different regions. In some parts of China, hawthorn and jujube juice are preferred. A small shot of fruit vinegar is served as an appetizer in Shanxi.

Where there are historical immigrant Chinese populations, the style of food has evolved and been adapted to local tastes and ingredients, and modified by the local cuisine, to greater or lesser extents. This has resulted in a deep Chinese influence on other national cuisines such as Cambodian cuisine, Filipino cuisine, Singaporean cuisine, Thai cuisine and Vietnamese cuisine. There are also a large number of forms of fusion cuisine, often popular in the country in question. Some, such as ramen (Japanese Chinese cuisine) have become popular internationally.

Deep-fried meat combined with sweet and sour sauce as a cooking style receives an enormous preference outside of China. Therefore, many similar international Chinese cuisines are invented based on sweet and sour sauce, including Sweet and sour chicken (Europe and North America), Manchurian chicken (India) or "tangsuyuk" (South Korea). The Hawaiian pizza was inspired by Chinese sweet and sour flavors.

Apart from the host country, the dishes developed in overseas Chinese cuisines are heavily dependent on the cuisines derived from the origin of the Chinese immigrants. In Korean Chinese cuisine, the dishes derive primarily from Shandong cuisine while Filipino Chinese cuisine is strongly influenced by Fujian cuisine. American Chinese cuisine has distinctive dishes (such as chop suey) originally based on Cantonese cuisine, which are more popular among non-Chinese Americans than with Chinese Americans themselves.

According to the report released by China's largest on-demand service platform in 2018, there are over 600,000 Chinese restaurants overseas. The report also pointed out that hotpot is the most popular food in the foreign market. Sichuan cuisine and some Chinese snacks and fast food followed in second and third place, respectively.
Youths should not begin eating before their elders do. When eating from a bowl, one should not hold it with its bottom part, because it resembles the act of begging. Chopsticks are the main eating utensils for Chinese food, which can be used to cut and pick up food. When someone is taking a break from eating at the table, they should not put the chopstick into the rice vertically, because it resembles the Chinese traditional funeral tribute, which involves putting chopsticks inside a bowl of rice vertically. It is considered inappropriate to use knives on the dining table. Chopsticks should not be waved around in the air or played with. Food should first be taken from the plate in front. It is considered impolite to stare at a plate. Watching TV, using mobile phones or doing other activities while eating is considered in poor taste. If an older person puts food in a younger person's bowl, the younger person should thank them.

Chinese culture has guidelines in how and when food are eaten. Chinese people typically eat three meals a day. Breakfast is served around 6–9am, lunch is served around 12–2pm, and dinner is served around 6–9pm. Within the Chinese culture, families do follow different traditions. In some families, the elderly members and youngsters get their meal first, then the mother and father, and then the children and teenagers. Other families have the male and female eat separately at different seating area. Whatever tradition the family decide to follow, it is intended to show respect to members of the family. Late night meals served from 9pm-4am and similar to the Western concept of supper is known as siu yeh.

Food plays various roles in social and cultural life. In Chinese folk religion, ancestor veneration is conducted by offering food to ancestors and Chinese festivals involve the consumption and preparation of specific foods which have symbolic meanings attached to them. Specific religions in China have their own cuisines such as the Taoist diet, Buddhist cuisine and Chinese Islamic Cuisine. 

The Kaifeng Jews in Henan province once had their own Chinese Jewish cuisine but the community has largely died out in the modern era and not much is known about the specifics of their cuisine but they did influence foods eaten in their region and some of their dishes remain. Chinese dishes with purported Kaifeng Jewish roots include Kaifeng xiao long bao, Mayuxing bucket-shaped chicken, Chrysanthemum hot pot, and Four Treasures.

Food also plays a role in daily life. The formality of the meal setting can signify what kind of relationship people have with one another, and the type of food can indicate ones' social status and their country of origin. In a formal setting, up to sixteen of any combination of hot and cold dishes would be served to respect the guests. On the other hand, in a casual setting, people would eat inexpensive meals such as at food stalls or homemade food. The typical disparity in food in the Chinese society between the wealthy and everyone below that group lies in the rarity and cost of the food or ingredient, such as shark fins and bear paws. 

Depending on whether one chooses to have rice or a meal that is made of wheat flour such as bread or noodles as their main source of food, people within a similar culture or of a different background can make an assumption of the other's country of origin from the south or north of China. Different foods have different symbolic meanings. Mooncakes and dumplings are symbolic of the Mid-autumn festival and the Spring Festival, respectively. Pear symbolizes bad luck due to its similarity in pronunciation of 'away' in the native language and noodle means living a long life for its length.

In Chinese philosophy, food frequently conveys a message. A Chinese philosophy "I Ching" says, "Gentlemen use eating as a way to attain happiness. They should be aware of what they say, and refrain from eating too much."




Constantin Brâncuși

Constantin Brâncuși (; February 19, 1876 – March 16, 1957) was a Romanian sculptor, painter and photographer who made his career in France. Considered one of the most influential sculptors of the 20th century and a pioneer of modernism, Brâncuși is called the patriarch of modern sculpture. As a child, he displayed an aptitude for carving wooden farm tools. Formal studies took him first to Bucharest, then to Munich, then to the École des Beaux-Arts in Paris from 1905 to 1907. His art emphasizes clean geometrical lines that balance forms inherent in his materials with the symbolic allusions of representational art. Brâncuși sought inspiration in non-European cultures as a source of primitive exoticism, as did Paul Gauguin, Pablo Picasso, André Derain, and others. However, other influences emerge from Romanian folk art traceable through Byzantine and Dionysian traditions.

Brâncuși grew up in the village of Hobița, Gorj, near Târgu Jiu, close to Romania's Carpathian Mountains, an area known for its rich tradition of folk crafts, particularly woodcarving. Geometric patterns of the region are seen in his later works such as the Endless Column created in 1918.

His parents Nicolae and Maria Brâncuși were poor peasants who earned a meagre living through back-breaking labor; from the age of seven, Constantin herded the family's flock of sheep. He showed talent for carving objects out of wood and often ran away from home to escape the bullying of his father and older brothers.

At the age of nine, Brâncuși left the village to work in the nearest large town. At the age of eleven, he went into the service of a grocer in Slatina; and then he became a domestic in a public house in Craiova, where he remained for several years. When he was 18, Brâncuși created a violin by hand with materials he found around his workplace. Impressed by Brâncuși's talent for carving, an industrialist enrolled him in the Craiova School of Arts and Crafts ("școala de arte și meserii"), where he pursued his love for woodworking, graduating with honors in 1898.

He then enrolled in the Bucharest School of Fine Arts, where he received academic training in sculpture. He worked hard and quickly distinguished himself as talented. One of his earliest surviving works, under the guidance of his anatomy teacher, Dimitrie Gerota, is a masterfully rendered écorché (statue of a man with skin removed to reveal the muscles underneath) which was exhibited at the Romanian Athenaeum in 1903. Though just an anatomical study, it foreshadowed the sculptor's later efforts to reveal essence rather than merely copy outward appearance.

In 1903, Brâncuși traveled to Munich, and from there to Paris. In Paris, he was welcomed by the community of artists and intellectuals brimming with new ideas. He worked for two years in the workshop of Antonin Mercié of the École des Beaux-Arts and was invited to enter the workshop of Auguste Rodin. Even though he admired the eminent Rodin he left the Rodin studio after only two months, saying, "Nothing can grow under big trees."

After leaving Rodin's workshop, Brâncuși began developing the revolutionary style for which he is known. His first commissioned work, "The Prayer", was part of a gravestone memorial. It depicts a young woman crossing herself as she kneels, and marks the first step toward abstracted, non-literal representation, and shows his drive to depict "not the outer form but the idea, the essence of things." He also began doing more carving, rather than the method popular with his contemporaries, that of modeling in clay or plaster which would be cast in metal, and by 1908 he worked almost exclusively by carving.

In the following few years, he made many versions of "Sleeping Muse" and "The Kiss", further simplifying forms to geometrical and sparse objects.

His works became popular in France, Romania, and the United States. Collectors, notably John Quinn, bought his pieces, and reviewers praised his works. In 1913 Brâncuși's work was displayed at both the Salon des Indépendants and the first exhibition in the U.S. of modern art, the Armory Show.
In 1920, he developed a notorious reputation with the entry of "Princess X" in the Salon. The phallic appearance of this large, gleaming bronze piece scandalized the Salon and, despite Brâncuși's explanation that it was simply meant to represent the essence of womanhood, it was removed from the exhibition. "Princess X" was revealed to be Princess Marie Bonaparte, direct descendant of the younger brother of Napoleon Bonaparte. The sculpture has been interpreted by some as symbolizing her obsession with the penis and her lifelong quest to achieve vaginal orgasm, with the help of Sigmund Freud.

Around this time, Brâncuși began crafting the bases for his sculptures with much care and originality because he considered them important to the works themselves.

One of his major groups of sculptures involved the "Bird in Space" — simple abstract shapes representing a bird in flight. The works are based on his earlier "Măiastra" series. In Romanian folklore the Măiastra is a beautiful golden bird who foretells the future and cures the blind. Over the following 20 years, Brâncuși made multiple versions of "Bird in Space" out of marble or bronze. Athena Tacha Spear's book, "Brâncuși's Birds," (CAA monographs XXI, NYU Press, New York, 1969), first sorted out the 36 versions and their development, from the early "Măiastra", to the "Golden Bird" of the late teens, to the "Bird in Space", which emerged in the early 1920s and which Brâncuși developed throughout his life.

One of these versions caused a major controversy in 1926 when photographer Edward Steichen purchased it and shipped it to the United States. Customs officers did not accept the "Bird" as a work of art and assessed customs duty on its import as an industrial item. After protracted court proceedings, this assessment was overturned, thus confirming the Bird's status as a duty-exempt work of art. The verdict was somewhat influenced by the Judge Justice Waite's personal appreciation of the art calling it 'beautiful', 'symmetrical', and 'ornamental'. The ruling also established the important principle that "art" does not have to involve a realistic representation of nature, and that it was legitimate for it to simply represent an abstract concept – in this case "flight".

His work became increasingly popular in the U.S, where he visited several times during his life. Worldwide fame in 1933 brought him the commission of building a meditation temple, the Temple of Deliverance, in India for the Maharajah of Indore, Yeshwant Rao Holkar. Holkar had commissioned three "L'Oiseau dans l'Espace"—in bronze, black and white marble—previously, but when Brâncuși went to India in 1937 to complete the plans and begin construction, the Mahrajah was away and, supposedly, lost interest in the project which was to be an homage to his wife, the Maharani Margaret Holkar, who had died when he returned. Of the three birds, the bronze one is in the collection of the Norton Simon Museum in Pasadena, California, and the two marble birds are currently in the permanent collection of the National Gallery of Australia in Canberra, Australia.

In 1938, he finished the World War I monument in Târgu-Jiu where he had spent much of his childhood. "Table of Silence", "The Gate of the Kiss", and "Endless Column" commemorate the courage and sacrifice of Romanians who in 1916 defended Târgu Jiu from the forces of the Central Powers. The restoration of this ensemble was spearheaded by the World Monuments Fund and was completed in 2004.

The Târgu Jiu ensemble marks the apex of his artistic career. In his remaining 19 years he created fewer than 15 pieces, mostly reworking earlier themes, and while his fame grew, he withdrew. Brâncuși received his first retrospective in 1955 at the Guggenheim Museum in New York. In 1955 "Life" magazine reported, "Wearing white pajamas and a yellow gnome-like cap, Brâncuși today hobbles about his studio tenderly caring for and communing with the silent host of fish, birds, heads, and endless columns which he created."

Brâncuși was cared for in his later years by a Romanian refugee couple. He became a French citizen in 1952 in order to make the caregivers his heirs, and to bequeath his studio and its contents to the Musée National d'Art Moderne in Paris. In 2021, for IRCAM and Centre Pompidou's Festival Manifeste, the intermedial large-scale installation "Infinite Light Columns / Constellations of The Future, tribute to Constantin Brancusi" by artists duo Arotin & Serghei has been installed on Renzo Piano's IRCAM Tower on Centre Pompidou Square, on the opposite site to Brancusi's Studio.

Brâncuși dressed simply, reflective of his Romanian peasant background. His studio was reminiscent of the houses of the peasants from his native region: there was a big slab of rock as a table and a primitive fireplace, similar to those found in traditional houses in his native Oltenia, while the rest of the furniture was made by him out of wood. Brâncuși would cook his own food, traditional Romanian dishes, with which he would treat his guests.

Brâncuși held a large spectrum of interests, from science to music, and was known to play the violin. He would sing old Romanian folk songs, often expressing his feelings of homesickness. After the installment of communism, the artist never permanently returned to his native Romania, but did visit eight times.

His circle of friends included artists and intellectuals in Paris such as Amedeo Modigliani, Ezra Pound, Henri Pierre Roché, Guillaume Apollinaire, Louise Bourgeois, Pablo Picasso, Man Ray, Marcel Duchamp, Henri Rousseau, Peggy Guggenheim, Tristan Tzara, and Fernand Léger. He was an old friend of Romany Marie, who was also Romanian, and referred Isamu Noguchi to her café in Greenwich Village.
Although surrounded by the Parisian avant-garde, Brâncuși never lost contact with Romania and had friends from the community of Romanian artists and intellectuals living in Paris, including Benjamin Fondane, George Enescu, Theodor Pallady, Camil Ressu, Nicolae Dărăscu, Panait Istrati, Traian Vuia, Eugène Ionesco, Emil Cioran, Natalia Dumitresco, and Paul Celan. Another Romanian scholar wrote on Brâncuși, Mircea Eliade.

Brâncuși held a particular interest in mythology, especially Romanian mythology, folk tales, and traditional art (which also had a strong influence on his works), but he became interested in African and Mediterranean art as well.

A talented handyman, he built his own phonograph and made most of his furniture, utensils, and doorways. His worldview valued "differentiating the essential from the ephemeral," with Plato, Lao-Tzu, and Milarepa as influences. Reportedly, he had a copy of the first ever translation from the Tibetan into French of Jacques Bacot's Le poete tibetain Milarepa: ses crimes, ses épreuves, son Nirvana that he kept by his bedside. He identified closely with Milarepa's mountain existence since Brancusi himself came from the Carpathian Mountains of Romania and he often thought he was a reincarnation of Milarepa. He was a saint-like idealist and near ascetic, turning his workshop into a place where visitors noted the deep spiritual atmosphere. However, particularly through the 1910s and 1920s, he was known as a pleasure seeker and merrymaker in his bohemian circle. He enjoyed cigarettes, good wine, and the company of women. He had one child, John Moore, with the New Zealand pianist Vera Moore. He never acknowledged his son as his own.

Brâncuși died on March 16, 1957, aged 81. He was buried in the Cimetière du Montparnasse in Paris. This cemetery also displays statues that Brâncuși carved for deceased artists.

At his death, Brâncuși left 1200 photographs and 215 sculptures. He bequeathed part of his collection to the French state on condition that his workshop be rebuilt as it was on the day he died. This reconstruction of his studio, adjacent to the Pompidou Centre, is open to the public. Brâncuși's studio inspired Swedish architect Klas Anshelm's design of the Malmö Konsthall, which opened in 1975.

In September 1957, African American sculptor Richard Hunt traveled from Chicago to Paris to view Brancusi's studio. Hunt's visit left an enduring impression on the 22-year-old artist, not only because of the artistic influence of Brancusi and exploration of biomorphic abstraction in sculpture but also because of the way which Hunt chose to live the majority of his life. Like Brancusi, Hunt slept in his own studio surrounded by his art and the tools used in his practice for much of his life. 

Brancusi's "Bird in Space" sculptures inspired the Modernist poet, Ezra Pound, specifically his late "Cantos" which were written in the mid-twentieth century. The literary critic Lucy Jeffery highlights ways in which Brancusi's sculptural form influenced Ezra Pound, analysing Pound's "Canto CXVII et seq., 815". Through close textual analysis and with direct reference to Brancusi's comments on his own creative process, Jeffery highlights how Pound's and Brancusi's sculptural process and resulting style is one of ambiguity and tension between: levity and weight, simplicity and complexity, ease and struggle. As Jeffery remarks: 'Despite their drive towards an holistic artwork, neither Brancusi nor Pound could, to borrow [Albert] Boime's phrasing, "emancipate" their art from the material or social context to which it belonged.' In the article, Jeffery contextualises Brancusi's work in relation to the sculptor Gaudier-Brzeska, photographer Man Ray, and writers such as Mina Loy, Samuel Beckett, and Peter Russell. 

In 1962, Georg Olden used Brâncuși's "Bird in Space" as the inspiration behind his design of the Clio Award statuette.

In November 1971, was established in his birth village Hobița, as a branch of the . 

Brâncuși was elected posthumously to the Romanian Academy in 1990.

Google commemorated his 135th birthday with a Doodle in 2011 consisting of seven of his works.

Brâncuși's works are housed in museums around the world: in Romania at the National Museum of Art and Craiova Art Museum, in the US at the Museum of Modern Art (New York City) and the Philadelphia Museum of Art, the former holding the largest collection of Brâncuși sculptures in the United States.

Constantin Brâncuși University in Târgu Jiu and a metro station in Bucharest are named after him.

In 2015, the Romanian Parliament declared February 19 "The Brâncuși Day", a working holiday in Romania.

Director Mick Davis plans to make a biographical film about Brâncuși called "The Sculptor", and British director Peter Greenaway said in 2017 that he is working on a film called "Walking to Paris", a film which shows Brâncuși's journey from Bucharest to Paris.

Brâncuși's piece "Madame L.R." sold for €29.185 million ($37.2 million) in 2009, setting a record price for a sculpture sold at auction.

In May 2018, "La Jeune Fille Sophistiquée" ("Portrait de Nancy Cunard"), a polished bronze on a carved marble base (1932), sold for US$71 million (with fees) at Christie's New York, setting a world record auction price for the artist.

Both "Bird in Space" and "Sleeping Muse I" are sculptures of animate objects; however, unlike ones from Ancient Greece or Rome, or those from the High Renaissance period, these works of art are more abstract in style.

"Bird in Space" is a series from the 1920s. One of these, constructed in 1925 using wood, stone, and marble (Richler 178) stands around 72 inches tall and consists of a narrow feather standing erect on a wooden base. Similar models, but made from materials such as bronze, were also produced by Brâncuși and placed in exhibitions.

"Sleeping Muse I" has different versions as well; one, from 1909 to 1910, is made of marble and measures 6 ¾ in. in height (Adams 549). This is a model of a head, without a body, with markings to show features such as hair, nose, lips, and closed eyes. In "A History of Western Art", Adams says that the sculpture has "an abstract, curvilinear quality and a smooth contour that create an impression of elegance" (549). The qualities which produce the effect can particularly be seen in the shape of the eyes and in the set of the mouth.



Apeirogon by Colum McCann p212




Claus Sluter

Claus Sluter (1340s in Haarlem – 1405 or 1406 in Dijon) was a Dutch sculptor, living in the Duchy of Burgundy from about 1380. He was the most important northern European sculptor of his age and is considered a pioneer of the "northern realism" of the Early Netherlandish painting that came into full flower with the work of Jan van Eyck and others in the next generation.

The name "Claes de Slutere van Herlam" is inscribed in the Register of the Corporation of Stonemasons and Sculptors of Brussels around the years 1379/1380. He then moved to the Burgundian capital of Dijon, where from 1385 to 1389 he was the assistant of Jean de Marville, court sculptor to Philip the Bold, Duke of Burgundy. From 1389 to his death he was court sculptor himself, with the rank of "valet de chambre". He was succeeded by his nephew Claus de Werve.

Sluter's most significant work is the so-called "Well of Moses" (1395–1403), or the Great Cross. It was created for the Carthusian monastery of Champmol, which was founded by Philip the Bold right outside Dijon in 1383. For many years, the top portion was thought to have included (along with Christ on a cross), sculptures of the Virgin and John the Evangelist. However it was more likely just Christ, with Mary Magdalene kneeling at the foot of the cross. The cross, and whatever was on the terrace below, was destroyed at some point after 1736 and before 1789, probable because the roof of the building protecting the monument collapsed. Some fragments from the original Cross are preserved in the Musée Archéologique de Dijon. Life-sized figures representing Old Testament prophets and kings (Moses, David, Daniel, Jeremiah, Zachariah, and Isaiah) stand around the base, holding phylacteries and books inscribed with verses from their respective texts, which were interpreted in the Middle Ages as typological prefigurations of the sacrifice of Christ. The work's physical structure, in which the Old Testament figures support those of the New Dispensation, literalizes the typological iconography. The pedestal surmounts a hexagonal fountain. The entire monument is executed in limestone quarried from Tonnerre and Asnières.
The portal of the former mortuary chapel of Champmol is positioned a few feet away from the Well of Moses. It consists of three sculptural groups by Sluter: a standing Madonna and Child at the trumeau; the duke and St. John, his patron saint, at the left jamb and the duchess and her patron saint, Catherine, at the right one. Sluter was also responsible for the main part of the work on Philip's tomb, which (restored and partly reconstructed) has been moved to the Museum of Fine Arts which is housed in the former ducal palace in Dijon.

Sluter was one of the sculptors of the pleurants, or mourners, which occupy niches below the tombs of Philip the Bold, his wife Margaret, and John the Fearless.


Cadillac, Michigan

Cadillac ( ) is a city in and county seat of Wexford County in the U.S. state of Michigan. The population was 10,371 at the 2020 census, making it the second most-populated city in the Northern Michigan region, after Traverse City.

Cadillac was settled as early as 1871 and formerly known as the village of Clam Lake before incorporating as a city in 1877. The city is located upon the shores of Lake Cadillac, connected by the Clam Lake Canal to Lake Mitchell. The Clam River, which begins Cadillac, is part of the Muskegon River watershed. Cadillac is the junction of three major highways: US Highway 131, M-55, and M-115. The geographic center of Michigan is approximately north-northwest of Cadillac. Cadillac is the primary city of the Cadillac micropolitan area, which includes all of Wexford County and Missaukee County to the east, and had population of 48,725 at the 2020 census.

European explorers and fur traders visited this area from the 18th century, most of them initially French and French-Canadians who traded with regional Native Americans. More permanent communities were not established until the late 19th century. Initial settlements developed from logging camps and the logging industry.

In 1871, the first sawmill began operations at Clam Lake. Originally called the Pioneer Mill, it was built by John R. Yale. That same year, George A. Mitchell, a prominent local banker and railroad entrepreneur, and Adam Gallinger, a local carpenter, formed the Clam Lake Canal Improvement and Construction Company. Two years later, the Clam Lake Canal was constructed between Big and Little Clam lakes, known as present-day Lakes Mitchell and Lake Cadillac. Sawmill owners used the canal to transport timber from Big Clam Lake to the mills and railroad sites on Little Clam Lake. The Grand Rapids and Indiana Railroad (G.R. & I. Railroad) had reached the area in 1872.

The settlement of Clam Lake was incorporated as a village in 1874. George Mitchell was elected as the first mayor. The village was incorporated as a city in 1877 and renamed Cadillac, after Antoine Laumet de La Mothe, sieur de Cadillac, a French colonist who started the first permanent settlement at Detroit in 1701.

The Wexford County seat of government, originally located in Sherman, was moved to Manton in 1881, as the result of a compromise between the feuding residents of Cadillac and Sherman. Cadillac partisans, however, won the county seat by a county-wide vote in April 1882. The day following the election, a sheriff's posse left the city for Manton by special train to seize the county records. After they arrived and collected a portion of the materials, however, an angry crowd confronted the Cadillac men and drove them out of town.

When the sheriff returned to Cadillac, he encountered a force consisting of several hundred armed men; this group reportedly included a brass band. The Sheriff's force, some of whom may have been intoxicated, traveled back to Manton to seize the remaining records. Although Manton residents confronted the Cadillac men and barricaded the courthouse, the posse successfully seized the documents. They returned to Cadillac in dubious glory.

In 1878, Ephraim Shay perfected his Shay locomotive, which was particularly effective in its ability to climb steep grades, maneuver sharp turns, and accommodate imperfections in railroad tracks. Cadillac was home to the Michigan Iron Works Company, which manufactured the Shay locomotive for a short time in the early 1880s. The lumber industry continued to dominate the city, attracting a large immigrant labor force, most of whom were Swedish. (Later Cadillac made sister city arrangements with Mölnlycke, Sweden, and Rovaniemi, Finland).

In 1899, the Cadillac Club formed, the forerunner of the Cadillac Area Chamber of Commerce.

By the early 20th century, with the lumber depleted, the timber industry was in decline. Today, manufacturers employ 30% of residents. Cadillac's range of industries includes the manufacture of pleasure boats, automotive parts, water-well components, vacuum cleaners, and rubber products.

In 1936, the U.S. Forest Service and the Civilian Conservation Corps developed the Caberfae Ski Area during the Great Depression as an investment in future economic development. This resulted in promotion of this area as a tourist center. Caberfae remains in operation today, as the oldest ski resort in the midwest. Tourism and outdoor recreation have since become an important sector of Cadillac's economy.

In the summer, tourists travel to the city and region for boating, fishing, hiking, mountain biking, and camping. During the fall, hunting and color tours are popular. The winter is possibly the busiest season; the area can be found packed with downhill skiers, cross-country skiers, ice-fishers, snow-shoers and–most of all-snowmobilers. The North American Snowmobile Festival (NASF) is held on frozen Lake Cadillac every winter.

Thirsty's, a gas station on M-55 west of Cadillac, was the home of Samantha or "Sam The Bear" from the 1970s through the late 1990s, when Sam died of old age. Sam was the only brown bear in captivity in the US at the time to hibernate naturally. Sam lived in a large cage in front of the gas station and was fed ice cream cones by tourists every summer.

In October 1975 the rock group Kiss visited Cadillac and performed at the Cadillac High School gymnasium. They played the concert to honor the Cadillac High School football team. In previous years, the team had compiled a record of sixteen consecutive victories, but the 1974 squad opened the season with two losses. The assistant coach, Jim Neff, an English teacher and rock'n'roll fan, thought to inspire the team by playing Kiss music in the locker room. He also connected the team's game plan, K-I-S-S or "Keep It Simple Stupid", with the band. The team went on to win seven straight games and their conference co-championship. After learning of their association with the team's success, the band decided to visit the school and play for the homecoming game.

Cadillac maintains a number of state historic landmarks. Most are marked with a green "Michigan Historical Marker" sign, which includes a description of the landmark. Six sites with the city are marked: Cadillac Carnegie Library, Charles T. Mitchell House, Clam Lake Canal, Cobbs & Mitchell Building, Cobbs & Mitchell No. 1, and the Shay Locomotive (pictured at the right). Two more are in the near Cadillac area: Caberfae Ski Resort and Greenwood Disciples of Christ Church; and another two are in surrounding Wexford County, marking Battle of Manton and the First Wexford County Court House.

According to the U.S. Census Bureau, the city has a total area of , of which is land and is water.

The Lake Cadillac is entirely within the city limits. The larger, Lake Mitchell is nearby on the west side of the city, with of shoreline within the city's municipal boundary. The lakes were connected by a stream which was replaced in 1873 by the Clam Lake Canal. The canal was featured on Ripley's Believe It or Not in the 1970s due to the phenomenon that in winter the canal freezes before the lakes and then after the lakes freeze, the canal thaws and remains unfrozen for the rest of the winter.

Cadillac is located at the eastern edge of what is now managed as the Manistee National Forest. The surrounding area is heavily wooded, with mixed hardwood and conifer forests. Christmas tree farming has been important to the area agricultural industry. Cadillac was chosen in 1988 to donate the holiday tree installed at the lawn of the U.S. Capitol building in Washington, D.C.

The area surrounding Cadillac is primarily rural, and is considered to be part of Northern Michigan. Given the small size of nearby communities, the city is a major commercial and industrial hub of the region.

The commercial center of the city is located on the eastern edge of Lake Cadillac. Most downtown buildings range from two to five stories in height. Many face Mitchell Street, the city's tree-lined main street and traditional corridor of travel through town. The downtown contains a movie theater, gift shops, restaurants, a bookstore, specialty food stores, jewelers, clothing retailers, and various other businesses.

The Courthouse Hill Historic District, recognized in April 2005, lies adjacent to the city's commercial center. The District contains a number of large Victorian-style residences built by the lumber barons and businessmen who helped develop the city in the 1870s. Population and building density is highest in this area.

On the western bank of Lake Cadillac, where M-55 intersects M-115, is what is locally referred to as Cadillac West. This is a small commercial district, bordering Mitchell State Park and the two lakes; it caters mostly to tourists. It contains a number of motels and restaurants.

Along the northern and southern stretches of the lake are the main residential areas of the city. They are generally of low to moderate density, characterized primarily by single-family structures.

Cadillac experiences a typical northern Michigan climate, undergoing temperate seasonal changes, influenced by the presence of Lake Michigan and the inevitable lake effect. Winters are generally cold with large amounts of snowfall. Summers are warm. The average high temperature in July is and the average low in January is . Summer temperatures can exceed , and winter temperatures can drop below . Average annual rainfall is , and average annual snowfall is . Snowfall typically occurs between the months of November and March. According to the Köppen climate classification system, Cadillac has a humid continental climate, abbreviated "Dfb" on climate maps.

Cadillac has two superfund sites, according to the U.S. Environmental Protection Agency. One is located at 1100 Wright Street, the former site of Kysor Industrial Corp, which operations resulted in toxic wastes. The other is located at 1002 6th Street, the former site of Northernaire Plating. Its operations also produced hazardous wastes, which produced contamination.

As of the census of 2010, there were 10,355 people, 4,280 households, and 2,625 families residing in the city. The population density was . There were 4,927 housing units at an average density of . The racial makeup of the city was 95.6% White, 0.5% African American, 0.6% Native American, 1.0% Asian, 0.4% from other races, and 1.8% from two or more races. Hispanic or Latino of any race were 1.8% of the population.

There were 4,280 households, of which 32.9% had children under the age of 18 living with them, 39.2% were married couples living together, 16.4% had a female householder with no husband present, 5.7% had a male householder with no wife present, and 38.7% were non-families. 32.0% of all households were made up of individuals, and 14% had someone living alone who was 65 years of age or older. The average household size was 2.34 and the average family size was 2.90.

The median age in the city was 36.5 years. 24.7% of residents were under the age of 18; 10% were between the ages of 18 and 24; 24.4% were from 25 to 44; 23.8% were from 45 to 64; and 17.1% were 65 years of age or older. The gender makeup of the city was 47.4% male and 52.6% female.

As of the census of 2000, there were 10,000 people, 4,118 households, and 2,577 families residing in the city. The population density was . There were 4,466 housing units at an average density of . The racial makeup of the city was 96.55% White, 0.21% Black or African American, 0.92% Native American, 0.63% Asian, 0.03% Pacific Islander, 0.28% from other races, and 1.38% from two or more races. 1.18% of the population were Hispanic or Latino of any race.

There were 4,118 households, out of which 32.2% had children under the age of 18 living with them, 43.9% were married couples living together, 14.2% had a female householder with no husband present, and 37.4% were non-families. 31.8% of all households were made up of individuals, and 14.4% had someone living alone who was 65 years of age or older. The average household size was 2.37 and the average family size was 2.96.

In the city, the population was spread out, with 26.2% under the age of 18, 9.6% from 18 to 24, 27.9% from 25 to 44, 19.6% from 45 to 64, and 16.7% who were 65 years of age or older. The median age was 36 years. For every 100 females, there were 91.4 males. For every 100 females age 18 and over, there were 84.4 males.

The median income for a household in the city was $29,899, and the median income for a family was $36,825. Males had a median income of $29,773 versus $21,283 for females. The per capita income for the city was $16,801. About 10.9% of families and 13.7% of the population were below the poverty line, including 15.4% of those under age 18 and 13.3% of those age 65 or over.

Cadillac was incorporated as a city in 1877. It is a home rule city with a Council-Manager form of government-one.

Current council members are Shari Spoelman, Antoinette Schippers, Arthur Stevens, James Dean and Carla Filkins (mayor). The present City Manager is Marcus Peccia.

Cadillac is located in Michigan's 2nd congressional district, represented by Republican John Moolenaar.

Manufacturing has been the greatest employer in Cadillac since the logging industry. More than 26% of the city's labor force is employed in manufacturing. Three industrial parks are located within the city limits, comprising 7% of the total land use in Cadillac. Their operations generate 47% of the city's tax base. Much of the city's economic performance is determined by the fortunes of local industry.

The center of the city is generally perceived to have a "small-town-feel". In the summer, the downtown fills with tourists, many from southern Michigan. The city center is one block from Lake Cadillac. For visitors by boat who dock at the public docks, it is nearly as accessible by boat as it is by car. The city's immediate proximity to two lakes, as well as Manistee National Forest, Pere Marquette State Forest, Mitchell State Park and a number of major highways, has established tourism as a significant sector of the local economy.

During the winter months, Lake Cadillac and Lake Mitchell freeze over and the city becomes covered with snow. Cadillac is connected to a number of trail systems popular with winter recreation enthusiasts. The city integrates unusually well into the corridors of travel created by snowmobilers.

Cadillac is also known as Chestnut Town, USA. The local area has a relatively high number of American chestnut trees, planted by pioneers from New York and Pennsylvania who settled in western Michigan. A blight in the early 20th century killed nearly every American Chestnut tree, but those in western Michigan had developed a mysterious resistance and survived.

According to the city's 2019 "Comprehensive Annual Financial Report", the principal employers in the city were:

Cadillac's public education system has a total of 10 schools, with approximately 3,100 students and 166 teachers with a student:teacher ratio of 19.1:1. Cadillac has 4 private primary and secondary schools with approximately 394 students, 20 teachers and a student:teacher ratio of 20:1.

The city has two high schools: Cadillac High School and Innovation High School. The area also has a junior high school, covering grades 7 and 8, located adjacent to the high school, and a middle school, Mackinaw Trail Middle School, covering grades 5 and 6. There are four elementary schools, Forest View Elementary, Franklin Elementary, Kenwood Elementary, and Lincoln Elementary.

Cadillac also has an alternative high school, located in the building that formerly housed Cooley Elementary School. Adult high school and GED courses are offered there as well. As a whole, the programs at Cooley are part of a curriculum that aids individuals in overcoming the exceptional obstacles to their educational and workforce goals.

Vocational career training is available to high school students free of charge in Cadillac and nearby schools at Wexford–Missaukee Intermediate School District (ISD) Career Tech Center (formerly Wexford-Missaukee Vocational Center or Voc-Tech). Students are bussed for part of the day to the Career Tech Center from their respective schools and receive credits toward high school graduation. Students are also able to earn certification in a chosen trade. Courses include:


Cosmetology is offered through the Career Tech Center, but at an off-campus location in downtown Cadillac. Adults can attend the vocational or cosmetology school with tuition or financial aid for certification.

Cadillac hosts the Wexford-Missaukee ISD Special Education for residents of the two counties who are in need of special services. This school is on the same campus as the Career Tech Center.

The class of 2006 was the largest class to go through Cadillac Public Schools.

Cadillac offers several options for private religious education.

Cadillac Heritage Christian offers nondenominational Christian education from pre-K through 12th grade. It is a coed school with 98 students and a teacher:student ratio of 1:11. Graduating classes are typically between 3–12 students.

Northview Adventist School has 16 students in grades 1–10 as of 2020. It is a coed Seventh Day Adventist School. They operate in a one-room format, with one teacher that doubles as the principal, and one or two teachers assistants. They also have a multitude of volunteers that runs a library, band, and physical education, among other things. They do not participate in competitive sports.

Noah's Ark Day School is a small alternative non-denominational Christian school for students in pre-K through first grade only. It is coed with 42 students and 1 teacher.

Cadillac's largest and most well-known private school is St. Ann School, a coed private Roman Catholic school with 236 students in grades pre-K through 7. The teacher:student ratio is 1:26. St. Ann is a member of the National Catholic Education Association. No Catholic high school education is offered at St. Ann School, and students typically attend public school for grades 8–12.

Northwoods Aviation, located at Wexford County Airport, offers training programs for piloting and servicing aircraft. Northwoods Aviation also offers primary instruction for those interested in sport pilot, private, and commercial certificates.

The Cadillac Institute of Cosmetology (formerly Cadillac Academy of Beauty) is a full service teaching salon in downtown Cadillac that offers training for general cosmetologists and specialized technicians to high school students through a partnership with Wexford-Missaukee Intermediate School District. Training is also available to adult students though private courses on a tuition basis. Upon completion of the program, students are qualified to take the state board exam to become a licensed cosmetologist or specialty technician.

The Baker College-Cadillac campus occupies just outside the City of Cadillac. The school has an enrollment of more than 1,300 students and offers Associate's and bachelor's degrees, in addition to professional certifications.

Cadillac is situated as the confluence of three highways: US 131, M-55 and M-115. Prior to 2001, the northern end of the freeway portion of US 131 was located at the southern entrance to Cadillac. With the construction of a bypass, the US 131 freeway was extended around the east side of the city. The former route of the highway through downtown Cadillac was redesignated as BUS US 131. In the city, BUS US 131 is named Mitchell Street, after George Mitchell, but may be referred to as main street.


The city is serviced by rail via the Great Lakes Central Railroad. This is primarily a freight line, although passenger service is expected in the future.


The White Pine Trail's northern terminus is in Cadillac. The trail, which stretches and originates from Comstock Park, follows an abandoned railroad bed into the center of the city. The trail is paved from the village of Leroy 16 miles north to Cadillac.





COINTELPRO

COINTELPRO (a syllabic abbreviation derived from Counter Intelligence Program) was a series of covert and illegal projects conducted between 1956 and 1971 by the United States Federal Bureau of Investigation (FBI) aimed at surveilling, infiltrating, discrediting, and disrupting American political organizations that the FBI perceived as subversive. Groups and individuals targeted by the FBI included feminist organizations, the Communist Party USA, anti–Vietnam War organizers, activists in the civil rights and Black power movements (e.g., Martin Luther King Jr., the Nation of Islam, and the Black Panther Party), environmentalist and animal rights organizations, the American Indian Movement (AIM), Chicano and Mexican-American groups like the Brown Berets and the United Farm Workers, independence movements (including Puerto Rican independence groups such as the Young Lords and the Puerto Rican Socialist Party), a variety of organizations that were part of the broader New Left, and white supremacist groups such as the Ku Klux Klan and the National States' Rights Party.

The FBI has used covert operations against domestic political groups since its inception; however, covert operations under the official COINTELPRO label took place between 1956 and 1971. Many of the tactics used in COINTELPRO are alleged to have seen continued use, including discrediting targets through psychological warfare; smearing individuals and groups using forged documents and by planting false reports in the media; harassment; wrongful imprisonment; illegal violence; and assassination. According to a Senate report, the FBI's motivation was "protecting national security, preventing violence, and maintaining the existing social and political order".

Beginning in 1969, leaders of the Black Panther Party were targeted by the COINTELPRO and "neutralized" by being assassinated, imprisoned, publicly humiliated or falsely charged with crimes. Some of the Black Panthers targeted include Fred Hampton, Mark Clark, Zayd Shakur, Geronimo Pratt, Mumia Abu-Jamal, and Marshall Conway. Common tactics used by COINTELPRO were perjury, witness harassment, witness intimidation, and withholding of exculpatory evidence.

FBI Director J. Edgar Hoover issued directives governing COINTELPRO, ordering FBI agents to "expose, disrupt, misdirect, discredit, or otherwise neutralize" the activities of these movements and especially their leaders. Under Hoover, the official in charge of COINTELPRO was assistant director William C. Sullivan. Attorney General Robert F. Kennedy personally authorized some of the programs, giving written approval for limited wiretapping of Martin Luther King's phones "on a trial basis, for a month or so". Hoover extended the clearance so his men were "unshackled" to look for evidence in any areas of King's life they deemed worthy.

The FBI initiated COINTELPRO, an abbreviation for Counterintelligence Program, in 1956 with the aim of undermining the operations of the Communist Party of the United States. In the 1960s, the scope of the organization was broadened to encompass various additional domestic factions, including the Ku Klux Klan, the Socialist Workers Party, and the Black Panther Party. The cessation of all COINTELPRO operations occurred in 1971. Despite its relatively small scale (constituting approximately 0.2% of the FBI's overall workload during a 15-year timeframe), COINTELPRO was subsequently subject to justified criticism from both Congress and the American public for infringing upon first amendment rights and other grounds.

Tactics included anonymous phone calls, Internal Revenue Service (IRS) audits, and the creation of documents that would divide the American communist organization internally. An October 1956 memo from Hoover reclassified the FBI's ongoing surveillance of black leaders, including it within COINTELPRO, with the justification that the movement was infiltrated by communists. In 1956, Hoover sent an open letter denouncing Dr. T. R. M. Howard, a civil rights leader, surgeon, and wealthy entrepreneur in Mississippi who had criticized FBI inaction in solving recent murders of George W. Lee, Emmett Till, and other African Americans in the South. When the Southern Christian Leadership Conference (SCLC), an African-American civil rights organization, was founded in 1957, the FBI began to monitor and target the group almost immediately, focusing particularly on Bayard Rustin, Stanley Levison, and eventually Martin Luther King Jr.

After the 1963 March on Washington for Jobs and Freedom, Hoover singled out King as a major target for COINTELPRO. Under pressure from Hoover to focus on King, Sullivan wrote:

Soon after, the FBI was systematically bugging King's home and his hotel rooms, as they were now aware that King was growing in stature daily as the most prominent leader of the civil rights movement.

In the mid-1960s, King began to publicly criticize the Bureau for giving insufficient attention to the use of terrorism by white supremacists. Hoover responded by publicly calling King the most "notorious liar" in the United States. In his 1991 memoir, "Washington Post" journalist Carl Rowan asserted that the FBI had sent at least one anonymous letter to King encouraging him to commit suicide. Historian Taylor Branch documents an anonymous "suicide package" sent by the FBI on November 21, 1964, that contained audio recordings obtained through tapping King's phone and placing bugs throughout various hotel rooms over the past two years, and that was created two days after the announcement of King's impending Nobel Peace Prize. The tape, which was prepared by FBI audio technician John Matter, documented a series of sexual indiscretions by King combined with a letter telling him: "There is only one way out for you. You better take it before your filthy, abnormal, fraudulent self is bared to the nation". King was subsequently informed that the audio would be released to the media if he did not acquiesce and commit suicide prior to accepting his Nobel Peace Prize. When King refused to satisfy their coercion tactics, FBI Associate Director, Cartha D. DeLoach, commenced a media campaign offering the surveillance transcript to various news organizations, including "Newsweek" and "Newsday". Even by 1969, as has been noted elsewhere, "[FBI] efforts to 'expose' Martin Luther King Jr. had not slackened even though King had been dead for a year. [The Bureau] furnished ammunition to opponents that enabled attacks on King's memory, and ... tried to block efforts to honor the slain leader."

During the same period the program also targeted Malcolm X. While an FBI spokesman has denied that the FBI was "directly" involved in Malcolm's murder in 1965, it is documented that the Bureau worked to "widen the rift" between Malcolm and Elijah Muhammad through infiltration and the "sparking of acrimonious debates within the organization", rumor-mongering, and other tactics designed to foster internal disputes, which ultimately led to Malcolm's assassination. The FBI heavily infiltrated Malcolm's Organization of Afro-American Unity in the final months of his life. The Pulitzer Prize-winning asserts that most of the men who plotted Malcolm's assassination were never apprehended and that the full extent of the FBI's involvement in his death cannot be known.

Amidst the urban unrest of July–August 1967, the FBI began "COINTELPRO–BLACK HATE", which focused on King and the SCLC, as well as the Student Nonviolent Coordinating Committee (SNCC), the Revolutionary Action Movement (RAM), the Deacons for Defense and Justice, Congress of Racial Equality (CORE), and the Nation of Islam. BLACK HATE established the Ghetto Informant Program and instructed 23 FBI offices to "disrupt, misdirect, discredit, or otherwise neutralize the activities of black nationalist hate type organizations".

A March 1968 memo stated the program's goal was to "prevent the coalition of militant black nationalist groups"; to "Prevent the RISE OF A 'MESSIAH' who could unify ... the militant black nationalist movement"; "to pinpoint potential troublemakers and neutralize them before they exercise their potential for violence [against authorities]"; to "Prevent militant black nationalist groups and leaders from gaining RESPECTABILITY, by discrediting them to ... both the responsible community and to liberals who have vestiges of sympathy"; and to "prevent the long-range GROWTH of militant black organizations, especially among youth". Dr. King was said to have potential to be the "messiah" figure, should he abandon nonviolence and integrationism, and Kwame Ture was noted to have "the necessary charisma to be a real threat in this way" as he was portrayed as someone who espoused a much more militant vision of "black power". While the FBI was particularly concerned with leaders and organizers, they did not limit their scope of target to the heads of organizations. Individuals such as writers were also listed among the targets of operations.

This program coincided with a broader federal effort to prepare military responses for urban riots and began increased collaboration between the FBI, Central Intelligence Agency, National Security Agency, and the Department of Defense. The CIA launched its own domestic espionage project in 1967 called Operation CHAOS. A particular target was the Poor People's Campaign, a national effort organized by King and the SCLC to occupy Washington, DC. The FBI monitored and disrupted the campaign on a national level, while using targeted smear tactics locally to undermine support for the march. The Black Panther Party was another targeted organization, wherein the FBI collaborated to destroy the party from the inside out.

Overall, COINTELPRO encompassed disruption and sabotage of the Socialist Workers Party (1961), the Ku Klux Klan (1964), the Nation of Islam, the Black Panther Party (1967), and the entire New Left social/political movement, which included antiwar, community, and religious groups (1968). A later investigation by the Senate's Church Committee (see below) stated that "COINTELPRO began in 1956, in part because of frustration with Supreme Court rulings limiting the Government's power to proceed overtly against dissident groups." Official congressional committees and several court cases have concluded that COINTELPRO operations against communist and socialist groups exceeded statutory limits on FBI activity and violated constitutional guarantees of freedom of speech and association.

The program was secret until March 8, 1971, when the Citizens' Commission to Investigate the FBI burgled an FBI field office in Media, Pennsylvania, took several dossiers, and exposed the program by passing this material to news agencies. The boxing match known as the Fight of the Century between Muhammad Ali and Joe Frazier in March 1971 provided cover for the activist group to successfully pull off the burglary. Muhammad Ali was a COINTELPRO target because he had joined the Nation of Islam and the anti-war movement.

Many news organizations initially refused to immediately publish the information, with the notable exception of "The Washington Post". After affirming the reliability of the documents, it published them on the front page (in defiance of the Attorney General's request), prompting other organizations to follow suit. Within the year, Director J. Edgar Hoover declared that the centralized COINTELPRO was over, and that all future counterintelligence operations would be handled case by case.

Additional documents were revealed in the course of separate lawsuits filed against the FBI by NBC correspondent Carl Stern, the Socialist Workers Party, and a number of other groups. In 1976 the Select Committee to Study Governmental Operations with Respect to Intelligence Activities of the United States Senate, commonly referred to as the "Church Committee" after its chairman, Senator Frank Church (D-Idaho), launched a major investigation of the FBI and COINTELPRO. Many released documents have been partly or entirely redacted.

The Final Report of the Select Committee castigated the conduct of the intelligence community in its domestic operations (including COINTELPRO) in no uncertain terms:

The Church Committee documented a history of the FBI (initially called BOI until 1936) exercising political repression as far back as World War I, and through the 1920s, when agents were charged with rounding up "anarchists, communists, socialists, reformists and revolutionaries" for deportation. From 1936 through 1976, the domestic operations were increased against political and anti-war groups.

The FBI claimed that the purpose behind COINTELPRO was to "expose, disrupt, misdirect, or otherwise neutralize" groups that the FBI officials believed were "subversive" by instructing FBI field operatives to:

At its inception, the program's main target was the Communist Party.

In an interview with the BBC's Andrew Marr in February 1996, Noam Chomsky—a political activist and MIT professor of linguistics—spoke about the purpose and the targets of COINTELPRO, saying:

According to the Church Committee:

Examples of surveillance, spanning all presidents from FDR to Nixon, both legal and illegal, contained in the Church Committee report:

Groups that were known to be targets of COINTELPRO operations include:

The COINTELPRO operators targeted multiple groups at once and encouraged splintering of these groups from within. In letter-writing campaigns (wherein false letters were sent on behalf of members of parties), the FBI ensured that groups would not unite in their causes. For instance, they launched a campaign specifically to alienate the Black Panther Party from the Mau Maus, Young Lords, Young Patriots and SDS. These racially diverse groups had been building alliances, in part due to charismatic leaders such as Fred Hampton and his attempts to create a "Rainbow Coalition". The FBI was concerned with ensuring that groups could not gain traction through unity, specifically across racial lines. One of the main ways of targeting these groups was to arouse suspicion between the different parties and causes. In this way the bureau took on a divide-and-conquer offensive.

The COINTELPRO documents show numerous cases of the FBI's intentions to prevent and disrupt protests against the Vietnam War. Many techniques were used to accomplish this task. "These included promoting splits among antiwar forces, encouraging red-baiting of socialists, and pushing violent confrontations as an alternative to massive, peaceful demonstrations." One 1966 COINTELPRO operation tried to redirect the Socialist Workers Party from their pledge of support for the antiwar movement.

The FBI has said that it no longer undertakes COINTELPRO or COINTELPRO-like operations. However, critics have claimed that agency programs in the spirit of COINTELPRO targeted groups such as the Committee in Solidarity with the People of El Salvador, the American Indian Movement, Earth First!, and the anti-globalization movement.

According to attorney Brian Glick in his book "War at Home", the FBI used five main methods during COINTELPRO:

The FBI specifically developed tactics intended to heighten tension and hostility between various factions in the black power movement, for example between the Black Panthers and the US Organization. For instance, the FBI sent a fake letter to the US Organization exposing a supposed Black Panther plot to murder the head of the US Organization, Ron Karenga. They then intensified this by spreading falsely attributed cartoons in the black communities pitting the Black Panther Party against the US Organization. This resulted in numerous deaths, among which were San Diego Black Panther Party members John Huggins, Bunchy Carter and Sylvester Bell. Another example of the FBI's anonymous letter writing campaign is how they turned the Blackstone Rangers head, Jeff Fort, against former ally Fred Hampton, by stating that Hampton had a hit on Fort. They also were instrumental in developing the rift between Black Panther Party leaders Eldridge Cleaver and Huey Newton, as executed through false letters inciting the two leaders of the Black Panther Party.

Dhoruba Bin Wahad, a former Black Panther, reflects on how these tactics made him feel, saying he had a combat mentality and felt like he was at war with the government. When asked about why he thinks the Black Panthers were targeted he said, "In the United States, the equivalent of the military was the local police. During the early sixties, at the height of the civil rights movement, and the human rights movement, the police in the United States became increasingly militaristic. They began to train out of military bases in the United States. The Law Enforcement Assistance Act supplied local police with military technology, everything from assault rifles to army personnel carriers. In his opinion, the Counterintelligence Program went hand-in-hand with the militarization of the police in the Black community, with the militarization of police in America."

The FBI also conspired with the police departments of many U.S. cities (San Diego, Los Angeles, San Francisco, Oakland, Philadelphia, Chicago) to encourage repeated raids on Black Panther homes—often with little or no evidence of violations of federal, state, or local laws—which resulted in the police killing many members of the Black Panther Party, most notably Chicago Black Panther Party Chairman Fred Hampton on December 4, 1969. Whether or not the FBI sanctioned his killing remains unproven. Before the death of Hampton, long-term infiltrator, William O'Neal, shared floor plans of his apartment with the COINTELPRO team. He then gave Hampton a dose of secobarbital that rendered Hampton unconscious during the raid on his home.

In order to eliminate black militant leaders whom they considered dangerous, the FBI is believed to have worked with local police departments to target specific individuals, accuse them of crimes they did not commit, suppress exculpatory evidence and falsely incarcerate them. Elmer "Geronimo" Pratt, a Black Panther Party leader, was incarcerated for 27 years before a California Superior Court vacated his murder conviction, ultimately freeing him. Appearing before the court, an FBI agent testified that he believed Pratt had been framed, because both the FBI and the Los Angeles Police Department knew he had not been in the area at the time the murder occurred.

Some sources claim that the FBI conducted more than 200 "black bag jobs", which were warrantless surreptitious entries, against the targeted groups and their members.

In 1969 the FBI special agent in San Francisco wrote Hoover that his investigation of the Black Panther Party had concluded that in his city, at least, the Panthers were primarily engaged in feeding breakfast to children. Hoover fired back a memo implying the agent's career goals would be directly affected by his supplying evidence to support Hoover's view that the Black Panther Party was "a violence-prone organization seeking to overthrow the Government by revolutionary means".

Hoover supported using false claims to attack his political enemies. In one memo he wrote: "Purpose of counterintelligence action is to disrupt the Black Panther Party and it is immaterial whether facts exist to substantiate the charge."

In one particularly controversial 1965 incident, white civil rights worker Viola Liuzzo was murdered by Ku Klux Klansmen, who gave chase and fired shots into her car after noticing that her passenger was a young black man; one of the Klansmen was Gary Thomas Rowe, an acknowledged FBI informant. The FBI spread rumors that Liuzzo was a member of the Communist Party and had abandoned her children to have sexual relationships with African Americans involved in the civil rights movement. FBI records show that J. Edgar Hoover personally communicated these insinuations to President Johnson.

FBI informant Rowe has also been implicated in some of the most violent crimes of the 1960s civil rights era, including attacks on the Freedom Riders and the 1963 Birmingham, Alabama 16th Street Baptist Church bombing.

The ACLU has claimed the FBI supported an extreme right-wing group of former Minutemen, transforming it into a group called the Secret Army Organization that targeted groups, activists, and leaders involved in the Anti-War Movement, using both intimidation and violent acts.

Hoover ordered preemptive action "to pinpoint potential troublemakers and neutralize them before they exercise their potential for violence."

The final report of the Church Committee concluded:

Too many people have been spied upon by too many Government agencies and too much information has been illegally collected. The Government has often undertaken the secret surveillance of citizens on the basis of their political beliefs, even when those beliefs posed no threat of violence or illegal acts on behalf of a hostile foreign power. The Government, operating primarily through secret and biased informants, but also using other intrusive techniques such as wiretaps, microphone "bugs", surreptitious mail opening, and break-ins, has swept in vast amounts of information about the personal lives, views, and associations of American citizens. Investigations of groups deemed potentially dangerous—and even of groups suspected of associating with potentially dangerous organizations—have continued for decades, despite the fact that those groups did not engage in unlawful activity.

Groups and individuals have been assaulted, repressed, harassed and disrupted because of their political views, social beliefs and their lifestyles. Investigations have been based upon vague standards whose breadth made excessive collection inevitable. Unsavory, harmful and vicious tactics have been employed—including anonymous attempts to break up marriages, disrupt meetings, ostracize persons from their professions, and provoke target groups into rivalries that might result in deaths. Intelligence agencies have served the political and personal objectives of presidents and other high officials. While the agencies often committed excesses in response to pressure from high officials in the Executive branch and Congress, they also occasionally initiated improper activities and then concealed them from officials whom they had a duty to inform.

Governmental officials—including those whose principal duty is to enforce the law—have violated or ignored the law over long periods of time and have advocated and defended their right to break the law.

The Constitutional system of checks and balances has not adequately controlled intelligence activities. Until recently the Executive branch has neither delineated the scope of permissible activities nor established procedures for supervising intelligence agencies. Congress has failed to exercise sufficient oversight, seldom questioning the use to which its appropriations were being put. Most domestic intelligence issues have not reached the courts, and in those cases when they have reached the courts, the judiciary has been reluctant to grapple with them.

While COINTELPRO was officially terminated in April 1971, domestic espionage continued. Between 1972 and 1974, it is documented that the Bureau planted over 500 bugs without a warrant and opened over 2,000 pieces of personal mail. More recent targets of covert action include the American Indian Movement (AIM), Earth First!, and Committees in Solidarity with the People of El Salvador. Documents released under the FOIA show that the FBI tracked the late David Halberstam—a Pulitzer Prize-winning journalist and author—for more than two decades. "Counterterrorism" guidelines implemented during the Reagan administration have been described as allowing a return to COINTELPRO tactics. Some radical groups accuse factional opponents of being FBI informants or assume the FBI is infiltrating the movement. COINTELPRO survivor Filiberto Ojeda Rios was killed by the FBI's hostage rescue team in 2005, his death described by a United Nations special committee as an assassination.

Environmentalist Eric McDavid convicted on arson charges was released after documents emerged demonstrating that the FBI informant in his Earth Liberation Front group provided crucial leadership, information, and material without which the crime could not have been committed, repeating the same pattern of behavior of COINTELPRO. It has been claimed these sorts of practices have become widespread in FBI "counter-terrorism" cases targeting Muslims in the 2009 Bronx terrorism plot and others.

Authors such as Ward Churchill, Rex Weyler, and Peter Matthiessen allege that the federal government intended to acquire uranium deposits on the Lakota tribe's reservation land, and that this motivated a larger government conspiracy against AIM activists on the Pine Ridge reservation. Others believe COINTELPRO continues and similar actions are being taken against activist groups. Caroline Woidat says that, with respect to Native Americans, COINTELPRO should be understood within a historical context in which "Native Americans have been viewed and have viewed the world themselves through the lens of conspiracy theory." Other authors argue that while some conspiracy theories related to COINTELPRO are unfounded, the issue of ongoing government surveillance and repression is real.

FBI Agent Richard G. Held is known to have increased FBI support for the Guardians of the Oglala Nation (GOON) squads, who were a private paramilitary group established in 1972 by the elected tribal chairman, Dick Wilson under authority of the Oglala Sioux. AIM accused GOONs of involvement in 300 assaults and 64 homicides of political opponents. Despite this, The Bureau rarely investigated them and instead used its resources overwhelmingly to prosecute AIM. In 2000, the FBI released a report regarding these alleged unsolved violent deaths on Pine Ridge reservation and accounted for most of the deaths and disputed the claims of unsolved murders. The report stated that only four deaths were unsolved and that some deaths were not murders.

In April 2018, the "Atlanta Black Star" characterized the FBI as still engaging in COINTELPRO behavior by surveilling the Black Lives Matter movement. Internal documents dated as late as 2017 showed that the FBI had surveilled the movement. In 2014, the FBI tracked a Black Lives Matter activist using surveillance tactics which "The Intercept" found "reminiscent of a rich American history of targeting black Americans," including COINTELPRO. This practice, along with the imprisonment of black activists for their views, has been associated with the new FBI designation of "Black Identity Extremists".

Defending Rights & Dissent, a civil liberties group, cataloged known instances of First Amendment abuses and political surveillance by the FBI since 2010. The organization found that the FBI devoted disproportionate resources to spy on peaceful left-leaning civil society groups, including Occupy Wall Street, economic justice advocates, racial justice movements, environmentalists, Abolish ICE, and various anti-war movements.

In December 2012, the FBI released redacted documents in response to a Freedom of Information Act request from the Partnership for Civil Justice Fund (PCJF). Mara Verheyden-Hilliard, the executive director of PCJF, said the documents showed that FBI counterterrorism agents had monitored the Occupy movement from its inception in August 2011 and that the FBI acted improperly by collecting "information on people's free-speech actions" and entering it into "unregulated databases, a vast storehouse of information widely disseminated to a range of law-enforcement and, apparently, private entities" (see Domestic Security Alliance Council). The FBI also communicated with the New York Stock Exchange, banks, private businesses and state and local police forces about the movement. In 2014, the PCJF obtained an additional 4,000 pages of unclassified documents through a Freedom of Information Act request, showing "details of the scrutiny of the Occupy protests in 2011 and 2012 by law enforcement officers, federal officials, security contractors and others."

In October 2020, Katie Reiter, chief of staff to Michigan state Senator Rosemary Bayer, had an FBI task force come to her house and aggressively question her about a draft bill she had recently discussed which would have limited the use of tear gas against protesters. Reiter had discussed the proposed ban on tear gas on a private 90-minute Zoom call with Bayer and a handful of other staffers. Reiter says the two officers refused to answer any questions about how they became aware of her private meeting. "The Intercept" reported about the incident: “Reiter said that the FBI’s visit left her confused and fearful. ‘It has impacted my sleep, it has caused me quite a bit of anxiety,’ she said. ‘And it has certainly impacted how we talk. I try not to let it, I’ll just be like, ‘No, we’re going to talk about this.’ But it's in my mind all the time.’” A spokesperson for the FBI declined to comment on the record, as did a spokesperson for Zoom.







Cruise missile

A cruise missile is a guided missile used against terrestrial or naval targets, that remains in the atmosphere and flies the major portion of its flight path at an approximately constant speed. Cruise missiles are designed to deliver a large warhead over long distances with high precision. Modern cruise missiles are capable of traveling at high subsonic, supersonic, or hypersonic speeds, are self-navigating, and are able to fly on a non-ballistic, extremely low-altitude trajectory.

The idea of an "aerial torpedo" was shown in the British 1909 film "The Airship Destroyer" in which flying torpedoes controlled wirelessly are used to bring down airships bombing London.

In 1916, the American aviator Lawrence Sperry built and patented an "aerial torpedo", the Hewitt-Sperry Automatic Airplane, a small biplane carrying a TNT charge, a Sperry autopilot and barometric altitude control. Inspired by the experiments, the United States Army developed a similar flying bomb called the Kettering Bug. Germany had also flown trials with remote-controlled aerial gliders "(Torpedogleiter)" built by Siemens-Schuckert beginning in 1916.

In the Interwar Period, Britain's Royal Aircraft Establishment developed the Larynx (Long Range Gun with Lynx Engine), which underwent a few flight tests in the 1920s.

In the Soviet Union, Sergei Korolev headed the GIRD-06 cruise missile project from 1932 to 1939, which used a rocket-powered boost-glide bomb design. The 06/III (RP-216) and 06/IV (RP-212) contained gyroscopic guidance systems. The vehicle was designed to boost to 28 km altitude and glide a distance of 280 km, but test flights in 1934 and 1936 only reached an altitude of 500 meters.

In 1944, during World War II, Germany deployed the first operational cruise missiles. The V-1, often called a flying bomb, contained a gyroscope guidance system and was propelled by a simple pulsejet engine, the sound of which gave it the nickname of "buzz bomb" or "doodlebug". Accuracy was sufficient only for use against very large targets (the general area of a city), while the range of 250 km was significantly lower than that of a bomber carrying the same payload. The main advantages were speed (although not sufficient to outperform contemporary propeller-driven interceptors) and expendability. The production cost of a V-1 was only a small fraction of that of a V-2 supersonic ballistic missile with a similar-sized warhead. Unlike the V-2, the initial deployments of the V-1 required stationary launch ramps which were susceptible to bombardment. Nazi Germany, in 1943, also developed the Mistel composite aircraft program, which can be seen as a rudimentary air-launched cruise missile, where a piloted fighter-type aircraft was mounted atop an unpiloted bomber-sized aircraft that was packed with explosives to be released while approaching the target. Bomber-launched variants of the V-1 saw limited operational service near the end of the war, with the pioneering V-1's design reverse-engineered by the Americans as the Republic-Ford JB-2 cruise missile.

Immediately after the war, the United States Air Force had 21 different guided missile projects, including would-be cruise missiles. All but four were cancelled by 1948: the Air Materiel Command Banshee, the SM-62 Snark, the SM-64 Navaho, and the MGM-1 Matador. The Banshee design was similar to Operation Aphrodite; like Aphrodite, it failed, and was canceled in April 1949. Concurrently, the US Navy's Operation Bumblebee, was conducted at Topsail Island, North Carolina, from c. 1 June 1946, to 28 July 1948. Bumblebee produced proof-of-concept technologies that influenced the US military's other missile projects.

During the Cold War, both the United States and the Soviet Union experimented further with the concept, of deploying early cruise missiles from land, submarines, and aircraft. The main outcome of the United States Navy submarine missile project was the SSM-N-8 Regulus missile, based upon the V-1.

The United States Air Force's first operational surface-to-surface missile was the winged, mobile, nuclear-capable MGM-1 Matador, also similar in concept to the V-1. Deployment overseas began in 1954, first to West Germany and later to the Republic of China and South Korea. On 7 November 1956, the U.S. Air Force deployed Matador units in West Germany, whose missiles were capable of striking targets in the Warsaw Pact, from their fixed day-to-day sites to unannounced dispersed launch locations. This alert was in response to the crisis posed by the Soviet attack on Hungary which suppressed the Hungarian Revolution of 1956.

Between 1957 and 1961 the United States followed an ambitious and well-funded program to develop a nuclear-powered cruise missile, Supersonic Low Altitude Missile (SLAM). It was designed to fly below the enemy's radar at speeds above Mach 3 and carry hydrogen bombs that it would drop along its path over enemy territory. Although the concept was proven sound and the 500-megawatt engine finished a successful test run in 1961, no airworthy device was ever completed. The project was finally abandoned in favor of ICBM development.

While ballistic missiles were the preferred weapons for land targets, heavy nuclear and conventional weapon tipped cruise missiles were seen by the USSR as a primary weapon to destroy United States naval carrier battle groups. Large submarines (for example, Echo and Oscar classes) were developed to carry these weapons and shadow United States battle groups at sea, and large bombers (for example, Backfire, Bear, and Blackjack models) were equipped with the weapons in their air-launched cruise missile (ALCM) configuration.

Cruise missiles can be categorized by size, speed (subsonic or supersonic), range, and whether launched from land, air, surface ship, or submarine. Often versions of the same missile are produced for different launch platforms; sometimes air- and submarine-launched versions are a little lighter and smaller than land- and ship-launched versions.

Guidance systems can vary across missiles. Some missiles can be fitted with any of a variety of navigation systems (Inertial navigation, TERCOM, or satellite navigation). Larger cruise missiles can carry either a conventional or a nuclear warhead, while smaller ones carry only conventional warheads.

A hypersonic cruise missile travels at least five times the speed of sound (Mach 5).

These missiles travel faster than the speed of sound, usually using ramjet engines. The range is typically 100–500 km, but can be greater. Guidance systems vary.

Examples:


The United States, Russia, North Korea, India, Iran, South Korea, Israel, France, China and Pakistan have developed several long-range subsonic cruise missiles. These missiles have a range of over and fly at about . They typically have a launch weight of about and can carry either a conventional or a nuclear warhead. Earlier versions of these missiles used inertial navigation; later versions use much more accurate TERCOM and DSMAC systems. Most recent versions can use satellite navigation.

Examples:

These missiles are about the same size and weight and fly at similar speeds to the above category. Guidance systems vary.

Examples:

These are subsonic missiles that weigh around and have a range of up to .
Examples:


The most common mission for cruise missiles is to attack relatively high-value targets such as ships, command bunkers, bridges and dams. Modern guidance systems permit accurate attacks.

, the BGM-109 Tomahawk missile model has become a significant part of the United States naval arsenal. It gives ships and submarines a somewhat accurate, long-range, conventional land attack weapon. Each costs about US$1.99 million. Both the Tomahawk and the AGM-86 were used extensively during Operation Desert Storm. On 7 April 2017, during the Syrian Civil War, U.S. warships fired more than 50 cruise missiles into a Syrian airbase in retaliation for a Syrian chemical weapons attack against a rebel stronghold.

The United States Air Force (USAF) deploys an air-launched cruise missile, the AGM-86 ALCM. The Boeing B-52 Stratofortress is the exclusive delivery vehicle for the AGM-86 and AGM-129 ACM. Both missile types are configurable for either conventional or nuclear warheads.

The USAF adopted the AGM-86 for its bomber fleet while AGM-109 was adapted to launch from trucks and ships and adopted by the USAF and Navy. The truck-launched versions, and also the Pershing II and SS-20 Intermediate Range Ballistic Missiles, were later destroyed under the bilateral INF (Intermediate-Range Nuclear Forces) treaty with the USSR.

The British Royal Navy (RN) also operates cruise missiles, specifically the U.S.-made Tomahawk, used by the RN's nuclear submarine fleet. UK conventional warhead versions were first fired in combat by the RN in 1999, during the Kosovo War (the United States fired cruise missiles in 1991). The Royal Air Force uses the Storm Shadow cruise missile on its Typhoon and previously its Tornado GR4 aircraft. It is also used by France, where it is known as SCALP EG, and carried by the Armée de l'Air's Mirage 2000 and Rafale aircraft.

India and Russia have jointly developed the supersonic cruise missile BrahMos. There are three versions of the Brahmos: ship/land-launched, air-launched, and sub-launched. The ship/land-launched version was operational as of late 2007. The Brahmos have the capability to attack targets on land. Russia also continues to operate other cruise missiles: the SS-N-12 Sandbox, SS-N-19 Shipwreck, SS-N-22 Sunburn and SS-N-25 Switchblade. Germany and Spain operate the Taurus missile while Pakistan has made the Babur missile Both the People's Republic of China and the Republic of China (Taiwan) have designed several cruise missile variants, such as the well-known C-802, some of which are capable of carrying biological, chemical, nuclear, and conventional warheads.

China has CJ-10 land attack cruise missile which is capable of carrying a nuclear warhead. Additionally, China appears to have tested a hypersonic cruise missile in August 2021, a claim it denies.

The French Force de Frappe nuclear forces include both land and sea-based bombers with Air-Sol Moyenne Portée (ASMP) high-speed medium-range nuclear cruise missiles. Two models are in use, ASMP and a newer ASMP-Ameliorer Plus (ASMP-A), which was developed in 1999. An estimated 40 to 50 were produced.

India in 2017 successfully flight-tested its indigenous Nirbhay ('Fearless') land-attack cruise missile, which can deliver nuclear warheads to a strike range of 1,000 km. Nirbhay had been flight-tested successfully.

India currently operates 7 varaints of Brahmos cruise missile operational range of 300-1000 km

India is currently developing hypersonic BRAHMOS-II which is going to be the fastest cruise missile.

The Israel Defense Forces reportedly deploy the medium-range air-launched Popeye Turbo ALCM and the Popeye Turbo SLCM medium-long range cruise missile with nuclear warheads on Dolphin class submarines.

Pakistan currently has four cruise missile systems: the air-launched Ra'ad-I and its enhanced version Ra'ad-II; the ground and submarine launched Babur; ship-launched Harbah missile and surface launched Zarb missile. Both, Ra'ad and Babur, can carry nuclear warheads between 10 and 25 kt, and deliver them to targets at a range of up to and respectively. Babur has been in service with the Pakistan Army since 2010, and Pakistan Navy since 2018.

Russia has Kh-55SM cruise missiles, with a range similar to the United States' AGM-129 range of 3000 km, but are able to carry a more powerful warhead of 200 kt. They are equipped with a TERCOM system which allows them to cruise at an altitude lower than 110 meters at subsonic speeds while obtaining a CEP accuracy of 15 meters with an inertial navigation system. They are air-launched from either Tupolev Tu-95s, Tupolev Tu-22Ms, or Tupolev Tu-160s, each able to carry 16 for the Tu-95, 12 for the Tu-160, and 4 for the Tu-22M. A stealth version of the missile, the Kh-101 is in development. It has similar qualities as the Kh-55, except that its range has been extended to 5,000 km, is equipped with a 1,000 kg conventional warhead, and has stealth features which reduce its probability of intercept.

After the collapse of the Soviet Union, the most recent cruise missile developed was the Kalibr missile which entered production in the early 1990s and was officially inducted into the Russian arsenal in 1994. However, it only saw its combat debut on 7 October 2015, in Syria as a part of the Russian military campaign in Syria. The missile has been used 14 more times in combat operations in Syria since its debut.

In the late 1950s and early 1960s, the Soviet Union was attempting to develop cruise missiles. In this short time frame, the Soviet Union was working on nearly ten different types of cruise missiles. However, due to resources, most of the initial types of cruise missiles developed by the Soviet Union were Sea-Launched Cruise Missiles or Submarine-Launched Cruise Missiles (SLCMs). The SS-N-1 cruise missile was developed to have different configurations to be fired from a submarine or a ship. However, as time progressed, the Soviet Union began to work on air-launched cruise missiles as well (ALCM). These ACLM missiles were typically delivered via bombers designated as "Blinders" or "Backfire". The missiles in this configuration were called the AS-1, and AS-2 with eventual new variants with more development time. The main purpose of Soviet-based cruise missiles was to have defense and offensive mechanisms against enemy ships; in other words, most of the Soviet cruise missiles were anti-ship missiles. In the 1980s the Soviet Union had developed an arsenal of cruise missiles nearing 600 platforms which consisted of land, sea, and air delivery systems.

The United States has deployed nine nuclear cruise missiles at one time or another.

Currently, cruise missiles are among the most expensive of single-use weapons, up to several million dollars apiece. One consequence of this is that its users face difficult choices in target allocation, to avoid expending the missiles on targets of low value. For instance, during the 2001 strikes on Afghanistan the United States attacked targets of very low monetary value with cruise missiles, which led many to question the efficiency of the weapon. However, proponents of the cruise missile counter that the weapon can not be blamed for poor target selection, and the same argument applies to other types of UAVs: they are cheaper than human pilots when total training and infrastructure costs are taken into account, not to mention the risk of loss of personnel. As demonstrated in Libya in 2011 and prior conflicts, cruise missiles are much more difficult to detect and intercept than other aerial assets (reduced radar cross-section, infrared and visual signature due to smaller size), suiting them to attacks against static air defense systems.



