Atlanta Falcons

The Atlanta Falcons is a professional American football team based in Atlanta. The Falcons compete in the National Football League (NFL) as a member club of the league's National Football Conference (NFC) South division. The Falcons were founded on June 30, 1965, and joined the NFL in 1966 as an expansion team, after the NFL offered then-owner Rankin Smith a franchise to keep him from joining the rival American Football League (AFL).

In their 57 years of existence, the Falcons have compiled a record of 390–503–6 ( in the regular season and in the playoffs), winning division championships in 1980, 1998, 2004, 2010, 2012, and 2016. The Falcons have appeared in two Super Bowls, the first during the 1998 season in Super Bowl XXXIII, where they lost to the Denver Broncos and the second 18 years later, a overtime loss to the New England Patriots in Super Bowl LI. They are the oldest major professional sports team in America with no championships.

The Falcons' current home field is Mercedes-Benz Stadium, which opened for the 2017 season; the team's headquarters and practice facilities are located at a site in Flowery Branch, northeast of Atlanta in Hall County.

Professional football first came to Atlanta in 1962, when the American Football League (AFL) staged two preseason contests, with one featuring the Denver Broncos vs. the Houston Oilers and the second pitting the Dallas Texans against the Oakland Raiders. Two years later, the AFL held another exhibition, this time with the New York Jets taking on the San Diego Chargers.

In 1965, after the Atlanta–Fulton County Stadium (then known simply as Atlanta Stadium) was built, the city of Atlanta felt the time was right to start pursuing professional football. One independent group which had been active in NFL exhibition promotions in Atlanta applied for franchises in both the AFL and NFL, acting entirely on its own with no guarantee of stadium rights. Another group reported it had deposited earnest money for a team in the AFL.

With everyone running in different directions, some local businessmen (Cox Broadcasting) worked out a deal and were awarded an AFL franchise on contingent upon acquiring exclusive stadium rights from city NFL Commissioner Pete Rozelle, who had been moving slowly in Atlanta matters, was spurred by the AFL interest and headed on the next plane down to Atlanta to block the rival league's claim on the city of Atlanta. He forced the city to make a choice between the two leagues; by June 30, the city picked Rankin Smith and the NFL.

The AFL's original expansion plans in June 1965 were for two new teams in Atlanta and It later evolved into the Miami Dolphins in 1966 and the Cincinnati Bengals in 1968. The NFL had planned to add two teams in ; the competition with the AFL for Atlanta forced the first to be added a year early in . The odd number of teams (15) resulted in one idle team (bye) each week, with each team playing 14 games over 15 weeks (similar to : 12 games over 13 weeks). The second expansion team, the New Orleans Saints, joined the NFL as planned in 1967 as its sixteenth franchise.

The Atlanta Falcons franchise began when it was approved to begin play in 1966 by a unanimous vote of the NFL club owners on June 21, 1965. Rozelle granted ownership nine days later on June 30 to 40-year-old Rankin Smith Sr., an executive vice president of Life Insurance Company of Georgia. He paid $8.5 million, the highest price in NFL history at the time for a franchise. Rozelle and Smith made the deal in about five minutes and the Atlanta Falcons brought the largest and most popular sport to the city of Atlanta. The Atlanta expansion team became the 15th NFL franchise, and they were awarded the first overall pick in the 1966 NFL Draft as well as the final pick in each of the first five rounds. They selected consensus All-American linebacker Tommy Nobis from the University of Texas, making him the first-ever Falcon. The league also held the expansion draft six weeks later in which Atlanta selected unprotected players from the 14 existing franchises. Although the Falcons selected many good players in those drafts, they still were not able to win right away.

The Atlanta team received its nickname on August 29, 1965. Miss Julia Elliott, a school teacher from Griffin, was singled out from many people who suggested "Falcons" as the nickname for the new franchise. She wrote: "the Falcon is proud and dignified, with great courage and fight. It never drops its prey. It is deadly and has a great sporting tradition."

The Falcons' inaugural season was in 1966, and their first preseason game was on August 1, a loss to the Philadelphia Eagles. Under head coach Norb Hecker, Atlanta lost their first nine regular-season games in 1966; their first victory came on the road against the struggling New York Giants on November 20 in Yankee Stadium. Two weeks later, Atlanta won at Minnesota, and beat St. Louis in Atlanta the next week for their first home win. The team finished the 1960s with 12 wins in four seasons.

The Falcons had their first Monday Night Football game in Atlanta during the 1970 season, a 20–7 loss to the Miami Dolphins. The only two winning seasons in their first 12 years were and 
In the 1978 season, the Falcons qualified for the playoffs for the first time and won the Wild Card game against the Eagles 14–13. The following week, they lost to the Dallas Cowboys 27–20 in the Divisional Playoffs.

In the 1980 season, after a nine-game winning streak, the Falcons posted a franchise then-best record of 12–4 and captured their first NFC West division title. The next week, their dream season ended at home with a loss to the Cowboys 30–27 in the divisional playoffs. In the strike-shortened 1982 season, the Falcons made the playoffs but lost to the Minnesota Vikings, 30–24. Falcons coach Leeman Bennett was fired after the loss. The team then had losing seasons for the next eight years.

In the 1989 NFL Draft, the Falcons selected cornerback Deion Sanders in the first round, who helped them for the next four years, setting many records for the franchise. "Neon Deion" (a.k.a. "Prime Time") had a flashy appeal and helped bring media attention to one of the league's most anonymous franchises. Sanders was also famous for playing on major league baseball teams (New York Yankees and the Atlanta Braves) while simultaneously playing in the NFL.

After defeating the New Orleans Saints in the NFC Wild Card game, the Falcons' 1991 season ended in a divisional playoff loss to the Washington Redskins. In the 1991 NFL Draft, the Falcons selected quarterback Brett Favre as the 33rd overall pick. During his rookie season, he played in two games where he amassed a record of four passing attempts with no receptions and two interceptions. The following February, Favre was traded to the Green Bay Packers.

In 1992, the Atlanta Falcons opened a new chapter in their history moving into the newly constructed Georgia Dome, where the team has defeated all 31 other NFL teams at least once during its time there.

In 1998, under recently acquired head coach Dan Reeves, quarterback Chris Chandler and running back Jamal Anderson the "Dirty Bird" Falcons had their greatest season to date. On November 8, they beat the New England Patriots 41–10, ending a streak of 22 losses at cold-weather sites. The team finished with a franchise-best 14–2 regular-season record and the NFC West division championship. On January 17, 1999, the Falcons upset the top-seeded Vikings at the Hubert H. Humphrey Metrodome in the NFC Championship Game 30–27, in an exciting overtime victory. However, in their first-ever Super Bowl appearance, they lost 34–19 to the defending champion Denver Broncos in Super Bowl XXXIII.

In the second game of the Falcons 1999 season, running back Jamal Anderson, who had been a key player in the Falcons' 1998 success, suffered a season-ending knee injury. The Falcons finished the season with a very disappointing 5–11 regular-season record. In 2000, the Falcons suffered through another horrendous season finishing 4–12 and once again missing the playoffs.

In the 2001 NFL draft, the Falcons orchestrated a trade with the San Diego Chargers, acquiring the first overall pick (which was used on quarterback Michael Vick) in exchange for wide receiver-return specialist Tim Dwight and the fifth overall pick (used on running back LaDainian Tomlinson).

The Falcons finished the 2001 season with a record of 7–9 and missed the playoffs. Jessie Tuggle retired following 14 seasons in Atlanta.

On December 6, 2001, Arthur M. Blank reached a preliminary agreement with the Falcons' Taylor Smith to purchase the team. In a special meeting prior to Super Bowl XXXVI in New Orleans on February 2, 2002, NFL owners voted unanimously to approve the purchase.

The 2002 season saw the Falcons return to the playoffs with a regular-season record of 9–6–1, tying the Pittsburgh Steelers. It was Vick's first year as the starter, and the team, with newly acquired running back Warrick Dunn, delivered the Green Bay Packers their first home playoff loss ever. A 20–6 loss to the Donovan McNabb-led Philadelphia Eagles the following week, however, ended the Falcons' season.

On March 19, 2003, the Falcons presented their new logo. During the 2003 preseason Vick broke his leg and missed the first 12 games of the season. After losing 7 straight games, the decision was made to release head coach Dan Reeves. Wade Phillips acted as interim coach for the final 3 games. Although the Falcons won 3 of their last 4 games after the return of Vick, they ended up with a 5–11 record that year. In 2004, a new head coach, Jim L. Mora, was hired and Vick returned for the full season. The Falcons went 11–5, winning their third division title and earning a first-round bye into the playoffs. In the divisional playoffs, the Falcons defeated the St. Louis Rams, 47–17, in the Georgia Dome, advancing to the NFC Championship Game, which they lost to the Eagles, 27–10.

The Falcons again fell short of achieving back-to-back winning seasons in , going 8–8. In , Michael Vick became the first quarterback in league history to rush for more than 1,000 yards in a season, with 1,039. After finishing the season 7–9, however, coach Jim Mora was dismissed and Bobby Petrino, the University of Louisville's football coach, replaced him. Before the 2007 season began, Vick was suspended indefinitely by the NFL after pleading guilty to charges involving dog fighting in the state of Virginia. On December 10, 2007, Vick received a 23-month prison sentence and was officially cut from the Atlanta roster.

For the 2007 season, the Falcons were forced to start Joey Harrington at quarterback. On December 11, 13 games into his first NFL season as head coach, Bobby Petrino resigned without notice to coach at the University of Arkansas, leaving the beleaguered players only a note in the locker room. Secondary Coach Emmitt Thomas was named interim coach for the final three games of the season on December 12. The Falcons ended the year with a dismal 4–12 record.

After the tumultuous and disappointing 2007 season, the Falcons made a number of moves, hiring a new general manager and head coach, drafting a new starting quarterback, and signing a starting running back.

On January 13, 2008, the Falcons named former Patriots director of college football scouting Thomas Dimitroff General Manager. On January 23, Jacksonville Jaguars defensive coach and former linebackers coach for the 2000 Super Bowl champion Baltimore Ravens Mike Smith was named the Falcons' new head coach. Chargers back-up running back Michael Turner agreed to a 6-year, $30 million deal on March 2. On April 26, Matt Ryan (quarterback from Boston College) was drafted third overall in the 2008 NFL draft by the Falcons.

The Falcons finished the 2008 regular season with a record of 11–5, and the #5 seed in the playoffs. On December 21, 2008, Atlanta beat the Minnesota Vikings 24–17 to clinch a wild card spot, earning a trip to the playoffs for the first time since 2004. The Falcons would go on to lose in the wild-card round of the 2008 NFL playoffs to the eventual NFC champion Arizona Cardinals, 30–24.

Matt Ryan started all 16 games in his rookie season and was named the Associated Press Offensive Rookie of the Year. First-year head coach Mike Smith was named 2008 NFL Coach of the Year.

Although they failed to make the playoffs in 2009 the team rallied to win their final three regular-season games to record back-to-back winning seasons for the first time in franchise history. The Falcons defeated the Tampa Bay Buccaneers 20–10 in the final game of the season to improve their record to 9–7.

In 2010, with a regular-season record of 13–3, the Falcons secured a third straight winning season, their fourth overall divisional title, and the top overall seed in the NFC playoffs; however, the Falcons were overpowered by the eventual Super Bowl XLV champion Green Bay Packers in the NFC Divisional Playoffs 48–21. The Falcons scored 414 points – the fifth-most in franchise history. The team sent an NFL-high and franchise-best nine players to the 2011 Pro Bowl.

The Falcons made a surprise trade up with the Cleveland Browns in the 2011 NFL draft to select Alabama wide receiver Julio Jones sixth overall. In exchange, the Falcons gave up their first-, second- and fourth-round draft picks in 2011, and their first and fourth draft picks in 2012. Jones, along with teammates Tony Gonzalez and Roddy White, have since been dubbed Atlanta's "Big Three" (based on their total number of reception yards). On August 30, 2011, Sports Illustrated senior writer Peter King, who correctly predicted the 2011 Super Bowl, made his predictions for the 2011 season and picked the Falcons to defeat the San Diego Chargers in the 2012 Super Bowl. The Falcons finished the season at 10–6, securing the fifth seed after a Week 17 beatdown of Tampa Bay in which the Falcons pulled their starters after leading 42–0 just 23 minutes into the game.

The Falcons then went on to play the New York Giants in a 2011 NFC Wild Card Game at MetLife Stadium in East Rutherford, New Jersey. The first half was a defensive struggle, with the first points coming off of a safety by the Falcons, giving Atlanta a 2–0 lead. In the second quarter, though, Eli Manning connected with Hakeem Nicks for a short touchdown pass to make it 7–2 Giants heading into the second half. Then the Giants took control, as Manning threw for two more touchdown passes to Mario Manningham and Nicks and the defense completed its shutout of the Falcons to give the New York Giants the win, 24–2, and the Falcons their third straight playoff loss with Matt Ryan and Mike Smith. After the season, defensive coordinator Brian VanGorder accepted a coaching job at Auburn University, and the offensive coordinator Mike Mularkey took the head coaching job in Jacksonville.

Atlanta exploded out of the gate, going a franchise-best 8–0 and remaining the last unbeaten team in the NFL that year. Their hopes to get an undefeated season came to an end with a 27–31 loss to the division rival Saints. Julio Jones had a remarkable second year, grabbing 10 touchdowns and 1,198 yards. The Falcons finished the season 13–3, and clinched the number one seed in the NFC playoffs.

The Falcons played the Seattle Seahawks in their first playoff game. Although they went down 28–27 with only 31 seconds left on the clock, Matt Ryan led the team to their first playoff victory, 30–28. It was the only playoff victory in the Mike Smith era.

The Atlanta Falcons then advanced to face the San Francisco 49ers. The Falcons seized control of the game early with a Matt Bryant field goal, a trio of Matt Ryan touchdown passes caught by Julio Jones and Tony Gonzalez coupled with outstanding defensive play. By the end of the half, the score was 24–14. The tides of the game began to shift in the second half as the 49ers rallied back with a pair of Frank Gore touchdown runs. Atlanta's offense attempted to reply but were ultimately shut down by the 49er defense. A few series later, late in the 4th quarter with little time remaining, Atlanta found themselves in a 4th and 4 situation at the 10-yard line. The Falcons needed just 10 more yards to secure victory and advance to their first Super Bowl berth in 14 years. Matt Ryan fired a pass to Roddy White which was ultimately broken up by inside linebacker NaVorro Bowman, resulting in a 28–24 defeat.

Following the success of the previous season, the Falcons were an expected Super Bowl contender. However, injuries hampered the team's performance and the team finished the season 4–12. With that, the streak of consecutive winning seasons came to an end and Mike Smith had his first losing season as a head coach. Tony Gonzalez, in his final season in the NFL, was selected to the 2014 Pro Bowl as a starter representing Team Rice. Following the conclusion of the 2012 season, director of player personnel Les Snead departed the team to join the St. Louis Rams and Dave Caldwell, assistant to general manager Thomas Dimitroff, left the team to join the Jacksonville Jaguars. Scott Pioli, former GM of the Kansas City Chiefs, was announced as the Falcons' new assistant GM. Mike Smith was given a one-year extension on his contract as head coach. The Falcons had the 6th overall pick in the 2014 NFL draft with which they selected Jake Matthews, who played as offensive tackle for Texas A&M.

Despite having another rough season, the Falcons still had an opportunity to qualify for the playoffs at the end of the regular season. The Falcons hosted the Carolina Panthers in their regular season finale, with the winners clinching the NFC South division. Unfortunately, the Falcons lost in a 34–3 blowout as Matt Ryan threw two interceptions that were returned for touchdowns and got sacked six times. The Falcons finished the season 6–10, marking the second consecutive losing season for the team. The following day, Mike Smith was fired after seven seasons as head coach. The Falcons would soon hire Seattle Seahawks defensive coordinator Dan Quinn as the team's 16th head coach. The Falcons had the 8th overall pick in the 2015 NFL draft with which they selected Vic Beasley, a defensive end from Clemson University.

In February 2015, the team was investigated by the NFL for alleged use of artificial crowd noise in the Georgia Dome. The Falcons lost a 2016 NFL Draft selection as a result of the league's investigation.

Dan Quinn's first season saw a 5–0 start, the team's best start in four years. They would then struggle throughout the rest of the season by losing 8 of their last 11 games, resulting in an 8–8 record. They did, however, give the Panthers their only regular-season loss. The Falcons used their first-round pick in the 2016 NFL Draft on safety Keanu Neal from the University of Florida.

In the Falcons' 25th and final season in the Georgia Dome, Atlanta lost their week 1 game to the Buccaneers 24–31. The Falcons would then win their next four including one over the Panthers, when the franchise set new records: Matt Ryan threw for 503 yards, and Julio Jones caught 12 passes for 300 yards. Beating the San Francisco 49ers 41–13 in Week 15, the Falcons improved to 9–5 and secured their first winning season since 2012. One week later, the Falcons defeated the Panthers in Charlotte, North Carolina, and clinched their first NFC South division title since 2012. In their last regular-season game at the Georgia Dome, the Falcons defeated the New Orleans Saints, and secured an 11–5 record and a first-round bye.

In the divisional round of the playoffs, Atlanta defeated the Seahawks 36–20 in the Georgia Dome, and hosted their last game at the Dome against the Green Bay Packers in the NFC Championship Game on January 22, 2017. The Falcons defeated the Packers 44–21 to advance to Super Bowl LI as the NFC champions. Atlanta was up 28–3 late in the third quarter, and the New England Patriots scored 31 unanswered points, with the last 6 in the first-ever overtime in the Super Bowl. The Patriots' 25-point comeback was the largest in Super Bowl history.

In 2016, the Falcons scored 540 points in the regular season, the seventh-most in NFL history, tied with the Greatest Show on Turf (the 2000 St. Louis Rams). However, the Falcons defense gave up 406 points, 27th in the league.

The Falcons moved into their new home, the Mercedes-Benz Stadium, this season. Their first game ever played at the new stadium was a preseason loss to the Arizona Cardinals. The first regular-season game at the new stadium was a rematch of the 2016–17 NFC Championship, with Atlanta defeating Green Bay 34–23. Their first loss of the season was a 23–17 home defeat to the Buffalo Bills in week 4. The team returned to the playoffs with a 10–6 record (albeit with a third-place finish in the NFC South). The Falcons defeated the Los Angeles Rams 26–13 in the Wild Card round, but their 2017 season came to an end a week later in the Divisional Playoff round at the hands of the eventual Super Bowl champion Philadelphia Eagles 15–10.

In their first game with new uniforms, the Falcons lost to the Seattle Seahawks at home 38–25. The Falcons then suffered comebacks made by both the Cowboys on the road (39–40) and then back in Atlanta against the Bears (26–30). On October 11, after the team suffered a 23–16 loss at home against the Carolina Panthers and fell to 0–5, the Falcons announced the firings of Quinn and Dimitroff. Defensive coordinator Raheem Morris took over for the rest of the season, leading the team to a 4–12 record. Morris was not retained after the season, and soon joined the Los Angeles Rams as their defensive coordinator.

On January 15, 2021, the Falcons announced that Tennessee Titans offensive coordinator Arthur Smith had been named the 18th head coach in franchise history. Four days later, New Orleans Saints executive Terry Fontenot was named the Falcons' new general manager. Tight end Kyle Pitts was selected with the 4th pick of the 2021 draft, and longtime star receiver Julio Jones was traded to the Titans, after publicly requesting a trade from Atlanta. The Falcons improved on their record from the prior year, finishing the season with a 7–10 record.

On March 21, 2022, the Falcons traded longtime star quarterback Matt Ryan to the Indianapolis Colts. During the 2022 season, the team finished last place in the NFC South with a 7–10 record.

The Falcons entered the 2023 seasons with heightened expectations after drafting Texas running back Bijan Robinson with the eighth overall pick of the 2023 draft and making significant improvements in free agency, including signing Jessie Bates and Calais Campbell to improve their defense. However, the team finished with a 7–10 record for the third consecutive season, although they had been in contention for a playoff spot until the final day of the season before a 48–17 loss to the New Orleans Saints on January 7, 2024. The following day, Smith was fired after three years as head coach.

On January 25, 2024, the Falcons announced Raheem Morris's return to the organization, this time as the 19th head coach in Falcons history.

The Falcons have called three stadiums home in their 51 years of existence, and its third home in their history opened in the late summer of 2017. The first was the Atlanta–Fulton County Stadium, sharing with the Atlanta Braves Major League Baseball team until 1991. In 1992, the Georgia Dome was built, and the Falcons played there from its opening to the 2016 season. The Dome has been frequently used for college football, including Georgia State football and college bowl games such as the Peach Bowl.

In an effort to replace the aging Georgia Dome and potentially host a future Super Bowl, team owner Arthur Blank proposed a deal with the city of Atlanta to build a new state-of-the-art stadium not far from where the Georgia Dome is located. Blank will contribute $800 million and the city of Atlanta will contribute an additional $200 million via bonds backed by the city's hotel/motel tax towards the construction of a retractable roof stadium. Blank will contribute additional money for cost overruns if it is needed. The team will provide up to $50 million towards infrastructure costs that weren't included in the construction budget and to retire the remaining debt on the Georgia Dome. In addition, Blank's foundation and the city will each provide $15 million for development in surrounding neighborhoods. Though the total cost of the stadium was initially estimated to be around $1 billion, the total cost was revised to $1.5 billion according to Blank. In March 2013, the Atlanta City Council voted 11–4 in favor of building the stadium. The retractable roof Mercedes-Benz Stadium broke ground in May 2014, and became the third home stadium for the Falcons and the first for the new Atlanta United FC Major League Soccer club upon opening in 2017.

The Atlanta Falcons' colors are black, red, silver and white. When the team began play in 1966, the Falcons wore red helmets with a black falcon crest logo. In the center of the helmet was a center black stripe surrounded by two gold stripes and two white stripes. These colors represented the two college rival schools in the state of Georgia; rival schools Georgia Tech Yellow Jackets (white and gold) and the Georgia Bulldogs (red and black). Although the gold was removed after several seasons, the white remains to this day. They wore white pants and either black or white jerseys. At first, the falcon crest logo was also put on the jersey sleeves, but it was replaced by a red and white stripe pattern four years later. They switched from black to red jerseys in 1971, and the club began to wear silver pants in 1978. The facemasks on the helmets were initially gray, becoming white in 1978, and then black in 1984; the team wore black face masks until its 2020 redesign.

A prototype white helmet was developed for the team prior to the 1974 season, but was never worn.

In 1990, the uniform design changed to black helmets, silver pants, and either black or white jerseys. The numbers on the white jerseys were black, but were changed to red in 1997. (The red numerals could be seen on the away jerseys briefly in 1990.)

Both the logo and uniforms changed in 2003. The logo was redesigned with red and silver accents to depict a more powerful, aggressive falcon, which now more closely resembles the capital letter "F".

Although the Falcons still wore black helmets, the new uniforms featured jerseys and pants with red trim down the sides. The uniform design consisted of either black or white jerseys, and either black or white pants. During that same year, a red alternate jersey with black trim was also introduced. The Falcons also started wearing black cleats with these uniforms.

In 2004, the red jerseys became the primary jerseys, and the black ones became the alternate, both worn with white pants. In select road games, the Falcons wear black pants with white jerseys. The Falcons wore an all-black combination for home games against their archrivals, the New Orleans Saints, winning the first two contests (24–21 in and 36–17 in ), but losing 31–13 in . The Falcons wore the all-black combination against the New Orleans Saints for four straight seasons starting in 2004, With the last time being in 2007, losing 34–14. They wore the combination again in 2006, against the Tampa Bay Buccaneers in Week 2. The Falcons won that game, 14–3. The Falcons also wore their all-black uniform in 2007 against the New York Giants, and in 2008 against the Carolina Panthers and against the Tampa Bay Buccaneers (for the second time). After that, the black pants and uniforms were retired and the white pants were now used full-time with the regular uniforms.

In the 1980s, the Falcons wore their white uniforms at home most of the time because of the heat. When the Falcons started playing in a dome, the team switched to their dark uniforms for home games but have worn their white uniforms at home a few times since switching to the dome. It was announced at the 2009 state of the franchise meeting that the Falcons would wear 1966 throwback uniforms for a couple games during the 2009 season. The Atlanta Falcons wore 1966 throwback jerseys for two home games in 2009 – against the Carolina Panthers on September 20 and against the Tampa Bay Buccaneers on November 29. The Falcons won both of those games. They donned the throwbacks again for 2 games in 2010, against Baltimore and San Francisco, winning both of those games as well. The throwbacks were used twice in 2011 and 2012; both times were against the Panthers and Saints. However, the throwbacks were retired following a 2013 NFL rule requiring only one helmet shell per team.

The Falcons unveiled an all-red Color Rush uniform on September 13, 2016; however, due to the fact that the Falcons and the Tampa Bay Buccaneers had similar all-red Color Rush uniforms, the Falcons were unable to wear their Color Rush uniform until the 2017 season.

Also in 2016, the Falcons unveiled a mixed throwback uniform set. The uniform tops, pants and socks closely resembled their 1960s kits. From 2016 to 2021, due to the NFL's one-shell rule, the Falcons wore the black helmets with the original logo decal similar to the design they wore in the 1990s. However, starting in 2022, with the NFL now reinstating the use of alternate helmets, the Falcons brought back the original red helmets to pair with their throwback uniforms.

It was revealed in January 2020 that the Falcons will change uniforms for the 2020 NFL season. The ensuing design featured the return to black as the primary home uniform color for the first time since 2003. Both the primary home and road uniforms featured the "ATL" abbreviation in red above either white or black numbers with red drop shadows. The white and black tops are usually paired with either white or black pants. The alternate uniform featured a red/black gradient design and also featured the "ATL" abbreviation in white above white numbers with black drop shadows. Black pants are only used with this uniform. All three uniforms feature red side stripes. The current throwback uniform was also retained. In addition, the Falcons switched to matte helmets with the enlarged falcon logo and gray facemasks.

The Falcons have shared a heated divisional rivalry with the New Orleans Saints (first the NFC West, and now the NFC South). The two teams were often basement-dwellers in the division; but the rivalry grew as a means of pride between the two cities, as they were the only two NFL teams in the Deep South for multiple decades. The series is the oldest and most iconic rivalry in the NFC South as the two teams have long harbored bad blood against one another. The series is currently tied at 55–55, including the most recent loss to the Saints on January 7, 2024, when the Falcons lost 48–17.

In addition, the Falcons share a similar, rivalry with the Carolina Panthers, with both teams having been in the NFC West from the Panthers' founding in 1995 to the NFL realignment in 2002. Similar to their rivalry with the Saints, the Falcons have often endured several competitive divisional battles with the Panthers for lead of the NFC South, though the two have yet to meet in the postseason. The series is also known as the "I-85 Rivalry" due to Atlanta and Charlotte being only four hours apart on Interstate 85. The Falcons lead the series 27–17.

The Falcons share a less-intense divisional rivalry with the Tampa Bay Buccaneers since the NFL realignment in 2002. The two had been regional opponents but very little had linked any further animosity towards the two as the Buccaneers played in the former NFC Central before the realignment. The two teams would find themselves competing over staff and players alike, particularly during the 2000s after the Falcons had lured general manager Rich McKay after winning Super Bowl XXXVII the season prior. McKay's ties with Tampa extend into his family as his father John McKay was head coach of the Buccaneers for nine seasons.

The Eagles lead the Falcons 21–15–1, with a 3–1 lead in playoff games. The rivalry first emerged after the Falcons upset the Eagles in the , and only intensified further in the 2000s thanks to the rivalry between prominent dual-threat quarterbacks Donovan McNabb, and Michael Vick. Recently, the Falcons lost to the Eagles in the .

The Falcons have also shared a playoff rivalry with the Green Bay Packers as much of the connections between the two teams stems from Atlanta trading future hall-of-fame quarterback Brett Favre to the Green Bay on February 11, 1992, in exchange for a first-round pick. The two teams have met four times in the postseason, most recently during the 2016–17 NFC Championship as it would also be the final game played at the Georgia Dome. The Packers lead the all-time series 19–16, while both teams are tied in the postseason 2–2.

Includes postseason records

Source:
! Total || 385|| 455 || 6 || || || || || 10–14 ()




Humphrey is the only player in the Hall of Fame who spent the majority of his career with the Falcons.

The Atlanta Falcons organization does not officially retire jersey numbers, but considers certain players' jerseys worthy of being honored. The Falcons Ring of Honor honors individual players.

In their history, the Atlanta Falcons have had 18 head coaches.
The Falcons' flagship radio station is WZGC 92.9 The Game. Wes Durham, son of longtime North Carolina Tar Heels voice Woody Durham, is the Falcons' play-by-play announcer, with former Atlanta Falcons QB and pro football veteran, Dave Archer serving as color commentator.

In 2014, The CW affiliate WUPA became the official television station of the Falcons, gaining rights to its preseason games, which are produced by CBS Sports.

In the regular season, the team's games are seen on Fox's O&O affiliate WAGA. When the Falcons challenge an AFC team, CBS affiliate WANF will air those games while Sunday night games are televised on WXIA, the local NBC affiliate.

Source:


Heathenry in the United States

Heathenry is a modern Pagan new religious movement that has been active in the United States since at least the early 1970s. Although the term "Heathenry" is often employed to cover the entire religious movement, different Heathen groups within the United States often prefer the term "Ásatrú" or "Odinism" as self-designations.

Heathenry appeared in the United States during the 1960s, at the same time as the wider emergence of modern Paganism in the United States. Among the earliest American group was the Odinist Fellowship, founded by Danish migrant Else Christensen in 1969.

Ásatrú grew steadily in the United States during the 1960s. In 1969, the Danish Odinist Else Christensen established the Odinist Fellowship from her home in Florida. Heavily influenced by Alexander Rud Mills' writings, she began publication of a magazine, "The Odinist", although this focused to a greater extent on right-wing and racialist ideas than theological ones. Stephen McNallen first founded the Viking Brotherhood in the early 1970s, before creating the Ásatrú Free Assembly (AFA) in 1976, which broke up in 1986 amid widespread political disagreements after McNallen's repudiation of neo-Nazis within the group. In the 1990s, McNallen founded the Ásatrú Folk Assembly (AFA), an ethnically oriented Heathen group headquartered in California.

Meanwhile, Valgard Murray and his kindred in Arizona founded the Ásatrú Alliance (AA) in the late 1980s, which shared the AFA's perspectives on race and which published the "Vor Tru" newsletter. In 1987, Edred Thorsson and James Chisholm founded The Troth, which was incorporated in Texas. Taking an inclusive, non-racialist view, it soon grew into an international organisation.

In English usage, the genitive "" "of Æsir faith" is often used on its own to denote adherents (both singular and plural). This term is favored by practitioners who focus on the deities of Scandinavia, although it is problematic as many Asatruar worship deities and entities other than the Æsir, such as the Vanir, Valkyries, Elves, and Dwarves. Other practitioners term their religion "Vanatrú", meaning "those who honour the Vanir" or "Dísitrú", meaning "those who honour the Goddesses", depending on their particular theological emphasis.

Within the community it is sometimes stated that the term "Ásatrú" pertains to groups which are not racially focused, while "Odinism" is the term preferred by racially oriented groups. However, in practice, there is no such neat division in terminology.

There are notable differences of emphasis between "Ásatrú" as practiced in the US and in Scandinavia. According to , American Asatruar tend to prefer a more devotional form of worship and a more emotional conception of the Nordic gods than Scandinavian practitioners, reflecting the parallel tendency of highly emotional forms of Christianity prevalent in the United States.

Although deeming it impossible to calculate the exact size of the Heathen community in the US, sociologist Jeffrey Kaplan estimated that, in the mid-1990s, there were around 500 active practitioners in the country, with a further thousand individuals on the periphery of the movement. He noted that the overwhelming majority of individuals in the movement were white, male, and young. Most had at least an undergraduate degree, and worked in a mix of white collar and blue collar jobs. From her experience within the community, Snook concurred that the majority of American Heathens were male, adding also that most were also white and middle-aged, but believed that there had been a growth in the proportion of Heathen women in the US since the mid-1990s.

In 2003, the Pagan Census Project led by Helen A. Berger, Evan A. Leach, and Leigh S. Shaffer gained 60 responses from Heathens in the US, noting that 65% were male and 35% female, which they saw as the "opposite" of the rest of the country's Pagan community. The majority had a college education, but were generally less well educated than the wider Pagan community, with a lower median income than the wider Pagan community too.

Ásatrú organizations have memberships which span the entire political and spiritual spectrum. There is a history of political controversy within organized US Ásatrú, mostly surrounding the question of how to deal with such adherents as place themselves in a context of the far right and white supremacy, notably resulting in the fragmentation of the "Asatru Free Assembly" in 1986.

Externally, political activity on the part of Ásatrú organizations has surrounded campaigns against alleged religious discrimination, such as the call for the introduction of an Ásatrú "emblem of belief" by the United States Department of Veterans Affairs to parallel the Wiccan pentacle granted to the widow of Patrick Stewart in 2006. In May 2013, the "Hammer of Thor" was added to the list of United States Department of Veterans Affairs emblems for headstones and markers. It was reported in early 2019 that a Heathenry service was held on the U.S. Navy's USS John C. Stennis

Historically, the main dispute between the national organizations has generally centered on the interpretation of "Nordic heritage" as either something cultural, or as something genetic or racial. In the internal discourse within American Ásatrú, this cultural/racial divide has long been known as "universalist" vs. "folkish" Ásatrú.

The Troth takes the "universalist" position, claiming "Ásatrú" as a synonym for "Northern European Heathenry" taken to comprise "many variations, names, and practices, including Theodism, Irminism, Odinism, and Anglo-Saxon Heathenry". The Asatru Folk Assembly takes the folkish position, claiming that Ásatrú and the Germanic beliefs are ancestral in nature, and as an indigenous religion of the European Folk should only be accessed by the descendants of Europe. In the UK, Germanic Neopaganism is more commonly known as Odinism or as "Heathenry". This is mostly a matter of terminology, and US Ásatrú may be equated with UK Odinism for practical purposes, as is evident in the short-lived International Asatru-Odinic Alliance of folkish Ásatrú/Odinist groups.

Some groups identifying as Ásatrú have been associated with national socialist and white nationalist movements. Wotansvolk, for example, is an explicitly racial form.

More recently, however, many Ásatrú groups have been taking a harder stance against these elements of their community. Declaration 127, so named for the corresponding stanza of the Hávamál: "When you see misdeeds, speak out against them, and give your enemies no frið” is a collective statement denouncing and testifying disassociation with the Asatru Folk Assembly for alleged racial and sexually-discriminatory practices and beliefs signed by over 150 Ásatrú religious organizations from over 15 different nations mainly represented on Facebook.

Inmates of the "Intensive Management Unit" at Washington State Penitentiary who are adherents of Ásatrú in 2001 were deprived of their Thor's Hammer medallions.
In 2007, a federal judge confirmed that Ásatrú adherents in US prisons have the right to possess a Thor's Hammer pendant. An inmate sued the Virginia Department of Corrections after he was denied it while members of other religions were allowed their medallions.

In the Georgacarakos v. Watts case Peter N. Georgacarakos filed a pro se civil-rights complaint in the United States District Court for the District of Colorado against 19 prison officials for "interference with the free exercise of his Ásatrú religion" and "discrimination on the basis of his being Ásatrú".



Ansible

An ansible is a category of fictional devices or technology capable of near-instantaneous or faster-than-light communication. It can send and receive messages to and from a corresponding device over any distance or obstacle whatsoever with no delay, even between star systems. As a name for such a device, the word "ansible" first appeared in a 1966 novel by Ursula K. Le Guin. Since that time, the term has been broadly used in the works of numerous science fiction authors, across a variety of settings and continuities. A related term is ultrawave.

Ursula K. Le Guin coined the word "ansible" in her 1966 novel "Rocannon's World". The word was a contraction of "answerable", as the device would allow its users to receive answers to their messages in a reasonable amount of time, even over interstellar distances.

The ansible was the basis for creating a specific kind of interstellar civilizationone where communications between far-flung stars are instantaneous, but humans can only travel at relativistic speeds. Under these conditions, a full-fledged galactic empire is not possible, but there is a looser interstellar organization, in which several of Le Guin's protagonists are involved.

Although Le Guin invented the name "ansible" for this type of device (fleshed out with specific details in her fictional works), the broader concept of instantaneous or faster-than-light communication had previously existed in science fiction. Similar communication functions were included in a device called an interocitor in the 1952 novel "This Island Earth" by Raymond F. Jones, and the 1955 film based on that novel. Also in the "Dirac Communicator" which first appeared in James Blish's short story "Beep" (1954) and was later expanded into the novel "The Quincunx of Time" (1973). Robert A. Heinlein in "Time for the Stars" (1958) employed instantaneous telepathic communication between identical twin pairs over interstellar distances, and like Le Guin, provided a technical explanation based on a non-Einsteinian principle of simultaneity.

In her subsequent works, Le Guin continued to develop the concept of the ansible:


Any ansible may be used to communicate through any other, by setting its coordinates to those of the receiving ansible. They have a limited bandwidth, which only allows for at most a few hundred characters of text to be communicated in any transaction of a dialog session, and are attached to a keyboard and small display to perform text messaging.

Since Le Guin's conception of the ansible, the name of the device has been borrowed by numerous authors. While Le Guin's ansible was said to communicate "instantaneously", the name has also been adopted for devices capable of communication at finite speeds that are faster than light.

Orson Scott Card, in his 1977 novelette and 1985 novel "Ender's Game" and its sequels, used the term "ansible" as an unofficial name for the Philotic Parallax Instantaneous Communicator, a machine capable of communicating across infinite distances with no time delay. In "Ender's Game", Colonel Graff states that "somebody dredged the name "ansible" out of an old book somewhere". In an answer on the question-and-answer website Quora, Card explained why he chose to reuse the word "ansible" for an FTL communication device instead of developing a new in-universe name for one: "In an ftl universe, you have several levels. I've you can travel hyperfast, but no radio signal can outstrip your ship. Therefore you have to carry the mail with you. It's like the way things were between Europe and America before the laying of the successful transatlantic cable. But once it was laid, messages could be sent long before a ship could make the passage. That is like the ansible universe in which Ursula K. LeGuin‘s early Hainish novels."

In the universe of the "Ender's Game" series, the ansible's functions involved a fictional subatomic particle, the philote. The two quarks inside a pi meson can be separated by an arbitrary distance, while remaining connected by "philotic rays". This concept is similar to quantum teleportation due to entanglement; however, in reality, quark confinement prevents quarks from being separated by any observable distance.

Card's version of the ansible was also featured in the video game "Advent Rising", for which Card helped write the story, and in the movie "Ender's Game", which was based on the book.

Numerous other writers have included faster-than-light communication devices in their fictional works. Notable examples include:

Adalbert of Prague

Adalbert of Prague (, , , , ; 95623 April 997), known in the Czech Republic, Poland and Slovakia by his birth name Vojtěch (), was a Czech missionary and Christian saint. He was the Bishop of Prague and a missionary to the Hungarians, Poles, and Prussians, who was martyred in his efforts to convert the Baltic Prussians to Christianity. He is said to be the composer of the oldest Czech hymn "Hospodine, pomiluj ny" and "Bogurodzica", the oldest known Polish hymn, but his authorship of them has not been confirmed.

Adalbert was later declared the patron saint of the Czech Republic, Poland, and the Duchy of Prussia. He is also the patron saint of the Archdiocese of Esztergom in Hungary.

Born as "Vojtěch" in 952 or in gord Libice, he belonged to the Slavnik clan, one of the two most powerful families in Bohemia. Events from his life were later recorded by a Bohemian priest Cosmas of Prague (1045–1125). Vojtěch's father was Slavník (d. 978–981), a duke ruling a province centred at Libice. His mother was Střezislava (d. 985–987), and according to David Kalhous belonged to the Přemyslid dynasty. He had five brothers: Soběslav, Spytimír, Dobroslav, Pořej, and Čáslav. Cosmas also refers to Radim (later Gaudentius) as a brother; who is believed to have been a half-brother by his father's liaison with another woman. After he survived a grave illness in childhood, his parents decided to dedicate him to the service of God. Adalbert was well educated, having studied for approximately ten years (970-80) in Magdeburg under Adalbert of Magdeburg. The young Vojtěch took his tutor's name "Adalbert" at his Confirmation.

In 981 Adalbert of Magdeburg died, and his young protege Adalbert returned to Bohemia. Later Bishop Dietmar of Prague ordained him a Catholic priest. In 982, Bishop Dietmar died, and Adalbert, despite being under canonical age, was chosen to succeed him as Bishop of Prague. Amiable and somewhat worldly, he was not expected to trouble the secular powers by making excessive claims for the Church. Although Adalbert was from a wealthy family, he avoided comfort and luxury, and was noted for his charity and austerity. After six years of preaching and prayer, he had made little headway in evangelizing the Bohemians, who maintained deeply embedded pagan beliefs.

Adalbert opposed the participation of Christians in the slave trade and complained of polygamy and idolatry, which were common among the people. Once he started to propose reforms he was met with opposition from both the secular powers and the clergy. His family refused to support Duke Boleslaus in an unsuccessful war against Poland. Adalbert was no longer welcome and eventually forced into exile. In 988 he went to Rome. He lived as a hermit at the Benedictine monastery of Saint Alexis. Five years later, Boleslaus requested that the Pope send Adalbert back to Prague, in hopes of securing his family's support. Pope John XV agreed, with the understanding that Adalbert was free to leave Prague if he continued to encounter entrenched resistance. Adalbert returned as bishop of Prague, where he was initially received with demonstrations of apparent joy. Together with a group of Italian Benedictine monks which brought with him, he founded in 14 January 993 a monastery in Břevnov (then situated westward from Prague, now part of the city), the second oldest monastery on Czech territory.

In 995, the Slavniks' former rivalry with the Přemyslids, who were allied with the powerful Bohemian clan of the Vršovids, resulted in the storming of the Slavnik town of Libice nad Cidlinou, which was led by the Přemyslid Boleslaus II the Pious. During the struggle four or five of Adalbert's brothers were killed. The Zlič Principality became part of the Přemyslids' estate. Adalbert unsuccessfully attempted to protect a noblewoman caught in adultery. She had fled to a convent, where she was killed. In upholding the right of sanctuary, Bishop Adalbert responded by excommunicating the murderers. Butler suggests that the incident was orchestrated by enemies of his family.

After this, Adalbert could not safely stay in Bohemia and escaped from Prague. Strachkvas was eventually appointed to be his successor. However, Strachkvas suddenly died during the liturgy at which he was to accede to his episcopal office in Prague. The cause of his death is still ambiguous. The Pope directed Adalbert to resume his see, but believing that he would not be allowed back, Adalbert requested a brief as an itinerant missionary.

Adalbert then traveled to Hungary and probably baptized Géza of Hungary and his son Stephen in Esztergom. Then he went to Poland where he was cordially welcomed by then-Duke Boleslaus I and installed as Bishop of Gniezno.

Adalbert again relinquished his diocese, namely that of Gniezno, and set out as a missionary to preach to the inhabitants near Prussia. Bolesław I, Duke (and, later, King) of Poland, sent soldiers with Adalbert on his mission to the Prussians. The Bishop and his companions, entered Prussian territory and traveled along the coast of the Baltic Sea to Gdańsk. At the borders of the Polish realm, at the mouth of the Vistula River, his half-brother Radim (Gaudentius), Benedict-Bogusza (who was probably a Pole), and at least one interpreter, ventured out into Prussia alone, as Bolesław had only sent his soldiers to escort them to the border.

Adalbert achieved some success upon his arrival, however his arrival mostly caused strain upon the local Prussian populations. Partially this was because of the imperious manner with which he preached, but potentially because he preached utilizing a book. The Prussians had an oral society where communication was face to face. To the locals Adalbert reading from a book may have come off as a manifestation of an evil action. He was forced to leave this first village after being struck in the back of the head by an oar by a local chieftain, causing the pages of his book to scatter upon the ground. He and his companions then fled across a river.

In the next place that Adalbert tried to preach, his message was met with the locals banging their sticks upon the ground, calling for the death of Adalbert and his companions. Retreating once again Adalbert and his companions went to a market place of Truso (near modern-day Elbląg). Here they were met with a similar response as at the previous place. On the 23 April 997, after mass, while Adalbert and his companions lay in the grass while eating a snack, they were set upon by a pagan mob. The mob was led by a man named Sicco, possibly a pagan priest, who delivered the first blow against Adalbert, before the others joined in. They removed Adalbert's head from his body after he was dead, and mounted on a pole while they returned home. This encounter may also have taken place in Tenkitten and Fischhausen (now Primorsk, Kaliningrad Oblast, Russia). It is recorded that his body was bought back for its weight in gold by King Boleslaus I of Poland.

A few years after his martyrdom, Adalbert was canonized as Saint Adalbert of Prague. His life was written in "Vita Sancti Adalberti Pragensis" by various authors, the earliest being traced to imperial Aachen and the Bishop of Liège, Notger von Lüttich, although it was previously assumed that the Roman monk John Canaparius wrote the first "Vita" in 999. Another famous biographer of Adalbert was Bruno of Querfurt who wrote a hagiography of him in 1001–4.

Notably, the Přemyslid rulers of Bohemia initially refused to ransom Adalbert's body from the Prussians who murdered him, and therefore it was purchased by Poles. This fact may be explained by Adalbert's belonging to the Slavniks family which was rival to the Přemyslids. Thus Adalbert's bones were preserved in Gniezno, which assisted Boleslaus I of Poland in increasing Polish political and diplomatic power in Europe.

According to Bohemian accounts, in 1039 the Bohemian Duke Bretislav I looted the bones of Adalbert from Gniezno in a raid and translated them to Prague. According to Polish accounts, however, he stole the wrong relics, namely those of Gaudentius, while the Poles concealed Adalbert's relics which remain in Gniezno. In 1127 his severed head, which was not in the original purchase according to "Roczniki Polskie", was discovered and translated to Gniezno. In 1928, one of the arms of Adalbert, which Bolesław I had given to Holy Roman Emperor Otto III in 1000, was added to the bones preserved in Gniezno. Therefore, today Adalbert has two elaborate shrines in the Prague Cathedral and Royal Cathedral of Gniezno, each of which claims to possess his relics, but which of these bones are his authentic relics is unknown. For example, pursuant to both claims two skulls are attributed to Adalbert. The one in Gniezno was stolen in 1923.

The massive bronze doors of Gniezno Cathedral, dating from around 1175, are decorated with eighteen reliefs of scenes from Adalbert's life. They are the only Romanesque ecclesiastical doors in Europe depicting a cycle illustrating the life of a saint, and therefore are a precious relic documenting Adalbert's martyrdom. We can read that door literally and theologically.

The one thousandth anniversary of Adalbert's martyrdom was on 23 April 1997. It was commemorated in Poland, the Czech Republic, Germany, Russia, and other nations. Representatives of Catholic, Eastern Orthodox, and Evangelical churches traveled on a pilgrimage to Adalbert's tomb located in Gniezno. Pope John Paul II visited the cathedral and celebrated a liturgy there in which heads of seven European nations and approximately one million faithful participated.

A ten-meter cross was erected near the village of Beregovoe (formerly Tenkitten), Kaliningrad Oblast, where Adalbert is thought to have been martyred by the Prussians.

He is also commemorated on 23 April by Evangelical Church in Germany and Eastern Orthodox Church.

The Dagmar and Václav Havel VIZE 97 Foundation Prize, given annually to a distinguished thinker "whose work exceeds the traditional framework of scientific knowledge, contributes to the understanding of science as an integral part of general culture and is concerned with unconventional ways of asking fundamental questions about cognition, being and human existence"
includes a massive replica of Adalbert's crozier by Czech artist Jiří Plieštík.

St. Vojtech Fellowship was established in 1870 by Slovak Catholic priest Andrej Radlinský. It had facilitated Slovak Catholic thinkers and authors, continuing to publish religious original works and translations to this day. It is the official publishing body of Episcopal Conference of Slovakia.




Ælfheah of Canterbury

Ælfheah ( – 19 April 1012), more commonly known today as Alphege, was an Anglo-Saxon Bishop of Winchester, later Archbishop of Canterbury. He became an anchorite before being elected abbot of Bath Abbey. His reputation for piety and sanctity led to his promotion to the episcopate and, eventually, to his becoming archbishop. Ælfheah furthered the cult of Dunstan and also encouraged learning. He was captured by Viking raiders in 1011 during the siege of Canterbury and killed by them the following year after refusing to allow himself to be ransomed. Ælfheah was canonised as a saint in 1078. Thomas Becket, a later Archbishop of Canterbury, prayed to Ælfheah just before his murder in Canterbury Cathedral in 1170.

Ælfheah was born around 953, supposedly in Weston on the outskirts of Bath, and became a monk early in life. He first entered the monastery of Deerhurst, but then moved to Bath, where he became an anchorite. He was noted for his piety and austerity and rose to become abbot of Bath Abbey. The 12th-century chronicler, William of Malmesbury recorded that Ælfheah was a monk and prior at Glastonbury Abbey, but this is not accepted by all historians. Indications are that Ælfheah became abbot at Bath by 982, perhaps as early as around 977. He perhaps shared authority with his predecessor Æscwig after 968.

Probably due to the influence of Dunstan, the Archbishop of Canterbury (959–988), Ælfheah was elected Bishop of Winchester in 984, and was consecrated on 19 October that year. While bishop he was largely responsible for the construction of a large organ in the cathedral, audible from over a mile (1600 m) away and said to require more than 24 men to operate. He also built and enlarged the city's churches, and promoted the cult of Swithun and his predecessor, Æthelwold of Winchester. One act promoting Æthelwold's cult was the translation of Æthelwold's body to a new tomb in the cathedral at Winchester, which Ælfheah presided over on 10 September 996.

Following a Viking raid in 994, a peace treaty was agreed with one of the raiders, Olaf Tryggvason. Besides receiving danegeld, Olaf converted to Christianity and undertook never to raid or fight the English again. Ælfheah may have played a part in the treaty negotiations, and it is certain that he confirmed Olaf in his new faith.

In 1006, Ælfheah succeeded Ælfric as Archbishop of Canterbury, taking Swithun's head with him as a relic for the new location. He went to Rome in 1007 to receive his pallium—symbol of his status as an archbishop—from Pope John XVIII, but was robbed during his journey. While at Canterbury, he promoted the cult of Dunstan, ordering the writing of the second "Life of Dunstan", which Adelard of Ghent composed between 1006 and 1011. He also introduced new practices into the liturgy, and was instrumental in the Witenagemot's recognition of Wulfsige of Sherborne as a saint in about 1012.

Ælfheah sent Ælfric of Eynsham to Cerne Abbey to take charge of its monastic school. He was present at the council of May 1008 at which Wulfstan II, Archbishop of York, preached his "Sermo Lupi ad Anglos" ("The Sermon of the Wolf to the English"), castigating the English for their moral failings and blaming the latter for the tribulations afflicting the country.

In 1011, the Danes again raided England, and from 8–29 September they laid siege to Canterbury. Aided by the treachery of Ælfmaer, whose life Ælfheah had once saved, the raiders succeeded in sacking the city. Ælfheah was taken prisoner and held captive for seven months. Godwine (Bishop of Rochester), Leofrun (abbess of St Mildrith's), and the king's reeve, Ælfweard were captured also, but the abbot of St Augustine's Abbey, Ælfmær, managed to escape. Canterbury Cathedral was plundered and burned by the Danes following Ælfheah's capture.

Ælfheah refused to allow a ransom to be paid for his freedom, and as a result was killed on 19 April 1012 at Greenwich, reputedly on the site of St Alfege's Church. The account of Ælfheah's death appears in the E version of the "Anglo-Saxon Chronicle": 

Ælfheah was the first Archbishop of Canterbury to die a violent death. A contemporary report tells that Thorkell the Tall attempted to save Ælfheah from the mob about to kill him by offering everything he owned except for his ship, in exchange for Ælfheah's life; Thorkell's presence is not mentioned in the "Anglo-Saxon Chronicle", however. Some sources record that the final blow, with the back of an axe, was delivered as an act of kindness by a Christian convert known as "Thrum". Ælfheah was buried in Old St Paul's Cathedral. In 1023, his body was moved by King Cnut to Canterbury, with great ceremony. Thorkell the Tall was appalled at the brutality of his fellow raiders, and switched sides to the English king Æthelred the Unready following Ælfheah's death.

Pope Gregory VII canonised Ælfheah in 1078, with a feast day of 19 April. Lanfranc, the first post-Conquest archbishop, was dubious about some of the saints venerated at Canterbury. He was persuaded of Ælfheah's sanctity, but Ælfheah and Augustine of Canterbury were the only pre-conquest Anglo-Saxon archbishops kept on Canterbury's calendar of saints. Ælfheah's shrine, which had become neglected, was rebuilt and expanded in the early 12th century under Anselm of Canterbury, who was instrumental in retaining Ælfheah's name in the church calendar. After the 1174 fire in Canterbury Cathedral, Ælfheah's remains together with those of Dunstan were placed around the high altar, at which Thomas Becket is said to have commended his life into Ælfheah's care shortly before his martyrdom during the Becket controversy. The new shrine was sealed in lead, and was north of the high altar, sharing the honour with Dunstan's shrine, which was located south of the high altar. A "Life of Saint Ælfheah" in prose and verse was written by a Canterbury monk named Osbern, at Lanfranc's request. The prose version has survived, but the "Life" is very much a hagiography; many of the stories it contains have obvious Biblical parallels, making them suspect as a historical record.

In the late medieval period, Ælfheah's feast day was celebrated in Scandinavia, perhaps because of the saint's connection with Cnut. Few church dedications to him are known, with most of them occurring in Kent and one each in London and Winchester; as well as St Alfege's Church in Greenwich, a nearby hospital (1931–1968) was named after him. In Kent, there are two 12th-century parish churches dedicated to St Alphege at Seasalter and Canterbury. Reputedly his body lay in these churches overnight on his way back to Canterbury Cathedral for burial. In the town of Solihull in the West Midlands, St Alphege Church is dedicated to Ælfheah dating back to approximately 1277. In 1929, a new Roman Catholic church in Bath, the Church of Our Lady & St Alphege, was designed by Giles Gilbert Scott in homage to the ancient Roman church of Santa Maria in Cosmedin, and dedicated to Ælfheah under the name of Alphege. St George the Martyr with St Alphege & St Jude stands in Borough in London.

Artistic representations of Ælfheah often depict him holding a pile of stones in his chasuble, a reference to his martyrdom.


Associative algebra

In mathematics, an associative algebra "A" over a commutative ring (often a field) "K" is a ring "A" together with a ring homomorphism from "K" into the center of "A". This is thus an algebraic structure with an addition, a multiplication, and a scalar multiplication (the multiplication by the image by the ring homomorphism of an element of "K"). The addition and multiplication operations together give "A" the structure of a ring; the addition and scalar multiplication operations together give "A" the structure of a module or vector space over "K". In this article we will also use the term "K"-algebra"' to mean an associative algebra over "K". A standard first example of a "K"-algebra is a ring of square matrices over a commutative ring "K", with the usual matrix multiplication.

A commutative algebra is an associative algebra that has a commutative multiplication, or, equivalently, an associative algebra that is also a commutative ring.

In this article associative algebras are assumed to have a multiplicative identity, denoted 1; they are sometimes called unital associative algebras for clarification. In some areas of mathematics this assumption is not made, and we will call such structures non-unital associative algebras. We will also assume that all rings are unital, and all ring homomorphisms are unital.

Every ring is an associative algebra over its center and over the integers.

Let "R" be a commutative ring (so "R" could be a field). An associative "R"-algebra "A (or more simply, an R"-algebra "A") is a ring "A" 
that is also an "R"-module in such a way that the two additions (the ring addition and the module addition) are the same operation, and scalar multiplication satisfies
for all "r" in "R" and "x", "y" in the algebra. (This definition implies that the algebra, being a ring, is unital, since rings are supposed to have a multiplicative identity.)

Equivalently, an associative algebra "A" is a ring together with a ring homomorphism from "R" to the center of "A". If "f" is such a homomorphism, the scalar multiplication is (here the multiplication is the ring multiplication); if the scalar multiplication is given, the ring homomorphism is given by . (See also "" below).

Every ring is an associative Z-algebra, where Z denotes the ring of the integers.

A is an associative algebra that is also a commutative ring.

The definition is equivalent to saying that a unital associative "R"-algebra is a monoid object in "R"-Mod (the monoidal category of "R"-modules). By definition, a ring is a monoid object in the category of abelian groups; thus, the notion of an associative algebra is obtained by replacing the category of abelian groups with the category of modules.

Pushing this idea further, some authors have introduced a "generalized ring" as a monoid object in some other category that behaves like the category of modules. Indeed, this reinterpretation allows one to avoid making an explicit reference to elements of an algebra "A". For example, the associativity can be expressed as follows. By the universal property of a tensor product of modules, the multiplication (the "R"-bilinear map) corresponds to a unique "R"-linear map
The associativity then refers to the identity:

An associative algebra amounts to a ring homomorphism whose image lies in the center. Indeed, starting with a ring "A" and a ring homomorphism whose image lies in the center of "A", we can make "A" an "R"-algebra by defining
for all and . If "A" is an "R"-algebra, taking , the same formula in turn defines a ring homomorphism whose image lies in the center.

If a ring is commutative then it equals its center, so that a commutative "R"-algebra can be defined simply as a commutative ring "A" together with a commutative ring homomorphism .

The ring homomorphism "η" appearing in the above is often called a structure map. In the commutative case, one can consider the category whose objects are ring homomorphisms for a fixed "R", i.e., commutative "R"-algebras, and whose morphisms are ring homomorphisms that are under "R"; i.e., is (i.e., the coslice category of the category of commutative rings under "R".) The prime spectrum functor Spec then determines an anti-equivalence of this category to the category of affine schemes over Spec "R".

How to weaken the commutativity assumption is a subject matter of noncommutative algebraic geometry and, more recently, of derived algebraic geometry. See also: "Generic matrix ring".

A homomorphism between two "R"-algebras is an "R"-linear ring homomorphism. Explicitly, is an associative algebra homomorphism if

The class of all "R"-algebras together with algebra homomorphisms between them form a category, sometimes denoted "R"-Alg.

The subcategory of commutative "R"-algebras can be characterized as the coslice category "R"/CRing where CRing is the category of commutative rings.

The most basic example is a ring itself; it is an algebra over its center or any subring lying in the center. In particular, any commutative ring is an algebra over any of its subrings. Other examples abound both from algebra and other fields of mathematics.

formula_10
On the other hand, if "A" is a λ-ring, then there is a ring homomorphism
formula_11
giving "G"("A") a structure of an "A"-algebra.






Let "A" be an associative algebra over a commutative ring "R". Since "A" is in particular a module, we can take the dual module "A" of "A". A priori, the dual "A" need not have a structure of an associative algebra. However, "A" may come with an extra structure (namely, that of a Hopf algebra) so that the dual is also an associative algebra.

For example, take "A" to be the ring of continuous functions on a compact group "G". Then, not only "A" is an associative algebra, but it also comes with the co-multiplication formula_25 and co-unit formula_26. The "co-" refers to the fact that they satisfy the dual of the usual multiplication and unit in the algebra axiom. Hence, the dual "A" is an associative algebra. The co-multiplication and co-unit are also important in order to form a tensor product of representations of associative algebras (see "" below).

Given an associative algebra "A" over a commutative ring "R", the enveloping algebra "A" of "A" is the algebra or , depending on authors.

Note that a bimodule over "A" is exactly a left module over "A".

Let "A" be an algebra over a commutative ring "R". Then the algebra "A" is a right module over with the action . Then, by definition, "A" is said to separable if the multiplication map splits as an "A"-linear map, where is an "A"-module by . Equivalently,
"A" is separable if it is a projective module over ; thus, the -projective dimension of "A", sometimes called the bidimension of "A", measures the failure of separability.

Let "A" be a finite-dimensional algebra over a field "k". Then "A" is an Artinian ring.

As "A" is Artinian, if it is commutative, then it is a finite product of Artinian local rings whose residue fields are algebras over the base field "k". Now, a reduced Artinian local ring is a field and thus the following are equivalent

Let formula_34, the profinite group of finite Galois extensions of "k". Then formula_35 is an anti-equivalence of the category of finite-dimensional separable "k"-algebras to the category of finite sets with continuous formula_36-actions.

Since a simple Artinian ring is a (full) matrix ring over a division ring, if "A" is a simple algebra, then "A" is a (full) matrix algebra over a division algebra "D" over "k"; i.e., . More generally, if "A" is a semisimple algebra, then it is a finite product of matrix algebras (over various division "k"-algebras), the fact known as the Artin–Wedderburn theorem.

The fact that "A" is Artinian simplifies the notion of a Jacobson radical; for an Artinian ring, the Jacobson radical of "A" is the intersection of all (two-sided) maximal ideals (in contrast, in general, a Jacobson radical is the intersection of all left maximal ideals or the intersection of all right maximal ideals.)

The Wedderburn principal theorem states: for a finite-dimensional algebra "A" with a nilpotent ideal "I", if the projective dimension of as a module over the enveloping algebra is at most one, then the natural surjection splits; i.e., "A" contains a subalgebra "B" such that formula_37 is an isomorphism. Taking "I" to be the Jacobson radical, the theorem says in particular that the Jacobson radical is complemented by a semisimple algebra. The theorem is an analog of Levi's theorem for Lie algebras.

Let "R" be a Noetherian integral domain with field of fractions "K" (for example, they can be Z, Q). A "lattice" "L" in a finite-dimensional "K"-vector space "V" is a finitely generated "R"-submodule of "V" that spans "V"; in other words, .

Let "A" be a finite-dimensional "K"-algebra. An "order" in "A" is an "R"-subalgebra that is a lattice. In general, there are a lot fewer orders than lattices; e.g., Z is a lattice in Q but not an order (since it is not an algebra).

A "maximal order" is an order that is maximal among all the orders.

An associative algebra over "K" is given by a "K"-vector space "A" endowed with a bilinear map having two inputs (multiplicator and multiplicand) and one output (product), as well as a morphism identifying the scalar multiples of the multiplicative identity. If the bilinear map is reinterpreted as a linear map (i.e., morphism in the category of "K"-vector spaces) (by the universal property of the tensor product), then we can view an associative algebra over "K" as a "K"-vector space "A" endowed with two morphisms (one of the form and one of the form ) satisfying certain conditions that boil down to the algebra axioms. These two morphisms can be dualized using categorial duality by reversing all arrows in the commutative diagrams that describe the algebra axioms; this defines the structure of a coalgebra.

There is also an abstract notion of "F"-coalgebra, where "F" is a functor. This is vaguely related to the notion of coalgebra discussed above.

A representation of an algebra "A" is an algebra homomorphism from "A" to the endomorphism algebra of some vector space (or module) "V". The property of "ρ" being an algebra homomorphism means that "ρ" preserves the multiplicative operation (that is, for all "x" and "y" in "A"), and that "ρ" sends the unit of "A" to the unit of End("V") (that is, to the identity endomorphism of "V").

If "A" and "B" are two algebras, and and are two representations, then there is a (canonical) representation of the tensor product algebra on the vector space . However, there is no natural way of defining a tensor product of two representations of a single associative algebra in such a way that the result is still a representation of that same algebra (not of its tensor product with itself), without somehow imposing additional conditions. Here, by "tensor product of representations", the usual meaning is intended: the result should be a linear representation of the same algebra on the product vector space. Imposing such additional structure typically leads to the idea of a Hopf algebra or a Lie algebra, as demonstrated below.

Consider, for example, two representations and . One might try to form a tensor product representation according to how it acts on the product vector space, so that
However, such a map would not be linear, since one would have
for . One can rescue this attempt and restore linearity by imposing additional structure, by defining an algebra homomorphism , and defining the tensor product representation as
Such a homomorphism Δ is called a comultiplication if it satisfies certain axioms. The resulting structure is called a bialgebra. To be consistent with the definitions of the associative algebra, the coalgebra must be co-associative, and, if the algebra is unital, then the co-algebra must be co-unital as well. A Hopf algebra is a bialgebra with an additional piece of structure (the so-called antipode), which allows not only to define the tensor product of two representations, but also the Hom module of two representations (again, similarly to how it is done in the representation theory of groups).

One can try to be more clever in defining a tensor product. Consider, for example,
so that the action on the tensor product space is given by

This map is clearly linear in "x", and so it does not have the problem of the earlier definition. However, it fails to preserve multiplication:
But, in general, this does not equal
This shows that this definition of a tensor product is too naive; the obvious fix is to define it such that it is antisymmetric, so that the middle two terms cancel. This leads to the concept of a Lie algebra.

Some authors use the term "associative algebra" to refer to structures which do not necessarily have a multiplicative identity, and hence consider homomorphisms which are not necessarily unital.

One example of a non-unital associative algebra is given by the set of all functions whose limit as "x" nears infinity is zero.

Another example is the vector space of continuous periodic functions, together with the convolution product.



Axiom of regularity

In mathematics, the axiom of regularity (also known as the axiom of foundation) is an axiom of Zermelo–Fraenkel set theory that states that every non-empty set "A" contains an element that is disjoint from "A". In first-order logic, the axiom reads:
The axiom of regularity together with the axiom of pairing implies that no set is an element of itself, and that there is no infinite sequence ("a") such that "a" is an element of "a" for all "i". With the axiom of dependent choice (which is a weakened form of the axiom of choice), this result can be reversed: if there are no such infinite sequences, then the axiom of regularity is true. Hence, in this context the axiom of regularity is equivalent to the sentence that there are no downward infinite membership chains.

The axiom is the contribution of ; it was adopted in a formulation closer to the one found in contemporary textbooks by . Virtually all results in the branches of mathematics based on set theory hold even in the absence of regularity; see chapter 3 of . However, regularity makes some properties of ordinals easier to prove; and it not only allows induction to be done on well-ordered sets but also on proper classes that are well-founded relational structures such as the lexicographical ordering on formula_2

Given the other axioms of Zermelo–Fraenkel set theory, the axiom of regularity is equivalent to the axiom of induction. The axiom of induction tends to be used in place of the axiom of regularity in intuitionistic theories (ones that do not accept the law of the excluded middle), where the two axioms are not equivalent.

In addition to omitting the axiom of regularity, non-standard set theories have indeed postulated the existence of sets that are elements of themselves.

Let "A" be a set, and apply the axiom of regularity to {"A"}, which is a set by the axiom of pairing. We see that there must be an element of {"A"} which is disjoint from {"A"}. Since the only element of {"A"} is "A", it must be that "A" is disjoint from {"A"}. So, since formula_3, we cannot have "A" ∈ "A" (by the definition of disjoint).

Suppose, to the contrary, that there is a function, "f", on the natural numbers with "f"("n"+1) an element of "f"("n") for each "n". Define "S" = {"f"("n"): "n" a natural number}, the range of "f", which can be seen to be a set from the axiom schema of replacement. Applying the axiom of regularity to "S", let "B" be an element of "S" which is disjoint from "S". By the definition of "S", "B" must be "f"("k") for some natural number "k". However, we are given that "f"("k") contains "f"("k"+1) which is also an element of "S". So "f"("k"+1) is in the intersection of "f"("k") and "S". This contradicts the fact that they are disjoint sets. Since our supposition led to a contradiction, there must not be any such function, "f".

The nonexistence of a set containing itself can be seen as a special case where the sequence is infinite and constant.

Notice that this argument only applies to functions "f" that can be represented as sets as opposed to undefinable classes. The hereditarily finite sets, V, satisfy the axiom of regularity (and all other axioms of ZFC except the axiom of infinity). So if one forms a non-trivial ultrapower of V, then it will also satisfy the axiom of regularity. The resulting model will contain elements, called non-standard natural numbers, that satisfy the definition of natural numbers in that model but are not really natural numbers. They are "fake" natural numbers which are "larger" than any actual natural number. This model will contain infinite descending sequences of elements. For example, suppose "n" is a non-standard natural number, then formula_4 and formula_5, and so on. For any actual natural number "k", formula_6. This is an unending descending sequence of elements. But this sequence is not definable in the model and thus not a set. So no contradiction to regularity can be proved.

The axiom of regularity enables defining the ordered pair ("a","b") as <nowiki>{</nowiki>"a",<nowiki>{</nowiki>"a","b"<nowiki>}}</nowiki>; see ordered pair for specifics. This definition eliminates one pair of braces from the canonical Kuratowski definition ("a","b") = <nowiki></nowiki>.

This was actually the original form of the axiom in von Neumann's axiomatization.

Suppose "x" is any set. Let "t" be the transitive closure of {"x"}. Let "u" be the subset of "t" consisting of unranked sets. If "u" is empty, then "x" is ranked and we are done. Otherwise, apply the axiom of regularity to "u" to get an element "w" of "u" which is disjoint from "u". Since "w" is in "u", "w" is unranked. "w" is a subset of "t" by the definition of transitive closure. Since "w" is disjoint from "u", every element of "w" is ranked. Applying the axioms of replacement and union to combine the ranks of the elements of "w", we get an ordinal rank for "w", to wit formula_7. This contradicts the conclusion that "w" is unranked. So the assumption that "u" was non-empty must be false and "x" must have rank.

Let "X" and "Y" be sets. Then apply the axiom of regularity to the set {"X","Y"} (which exists by the axiom of pairing). We see there must be an element of {"X","Y"} which is also disjoint from it. It must be either "X" or "Y". By the definition of disjoint then, we must have either "Y" is not an element of "X" or vice versa.

Let the non-empty set "S" be a counter-example to the axiom of regularity; that is, every element of "S" has a non-empty intersection with "S". We define a binary relation "R" on "S" by formula_8, which is entire by assumption. Thus, by the axiom of dependent choice, there is some sequence ("a") in "S" satisfying "aRa" for all "n" in N. As this is an infinite descending chain, we arrive at a contradiction and so, no such "S" exists.

Regularity was shown to be relatively consistent with the rest of ZF by and , meaning that if ZF without regularity is consistent, then ZF (with regularity) is also consistent. For his proof in modern notation see for instance.

The axiom of regularity was also shown to be independent from the other axioms of ZF(C), assuming they are consistent. The result was announced by Paul Bernays in 1941, although he did not publish a proof until 1954. The proof involves (and led to the study of) Rieger-Bernays permutation models (or method), which were used for other proofs of independence for non-well-founded systems ( and ).

Naive set theory (the axiom schema of unrestricted comprehension and the axiom of extensionality) is inconsistent due to Russell's paradox. In early formalizations of sets, mathematicians and logicians have avoided that contradiction by replacing the axiom schema of comprehension with the much weaker axiom schema of separation. However, this step alone takes one to theories of sets which are considered too weak. So some of the power of comprehension was added back via the other existence axioms of ZF set theory (pairing, union, powerset, replacement, and infinity) which may be regarded as special cases of comprehension. So far, these axioms do not seem to lead to any contradiction. Subsequently, the axiom of choice and the axiom of regularity were added to exclude models with some undesirable properties. These two axioms are known to be relatively consistent.

In the presence of the axiom schema of separation, Russell's paradox becomes a proof that there is no set of all sets. The axiom of regularity together with the axiom of pairing also prohibit such a universal set. However, Russell's paradox yields a proof that there is no "set of all sets" using the axiom schema of separation alone, without any additional axioms. In particular, ZF without the axiom of regularity already prohibits such a universal set.

If a theory is extended by adding an axiom or axioms, then any (possibly undesirable) consequences of the original theory remain consequences of the extended theory. In particular, if ZF without regularity is extended by adding regularity to get ZF, then any contradiction (such as Russell's paradox) which followed from the original theory would still follow in the extended theory.

The existence of Quine atoms (sets that satisfy the formula equation "x" = {"x"}, i.e. have themselves as their only elements) is consistent with the theory obtained by removing the axiom of regularity from ZFC. Various non-wellfounded set theories allow "safe" circular sets, such as Quine atoms, without becoming inconsistent by means of Russell's paradox.

In ZF it can be proven that the class formula_9, called the von Neumann universe, is equal to the class of all sets. This statement is even equivalent to the axiom of regularity (if we work in ZF with this axiom omitted). From any model which does not satisfy axiom of regularity, a model which satisfies it can be constructed by taking only sets in formula_9.

In the same paper, Scott shows that an axiomatic system based on the inherent properties of the cumulative hierarchy turns out to be equivalent to ZF, including regularity.

The concept of well-foundedness and rank of a set were both introduced by Dmitry Mirimanoff (1917) cf. and . Mirimanoff called a set "x" "regular" (French: "ordinaire") if every descending chain "x" ∋ "x" ∋ "x" ∋ ... is finite. Mirimanoff however did not consider his notion of regularity (and well-foundedness) as an axiom to be observed by all sets; in later papers Mirimanoff also explored what are now called non-well-founded sets ("extraordinaire" in Mirimanoff's terminology).

Urelements are objects that are not sets, but which can be elements of sets. In ZF set theory, there are no urelements, but in some other set theories such as ZFA, there are. In these theories, the axiom of regularity must be modified. The statement "formula_12" needs to be replaced with a statement that formula_13 is not empty and is not an urelement. One suitable replacement is formula_14, which states that "x" is inhabited.




IBM AIX

AIX (Advanced Interactive eXecutive, pronounced ,) is a series of proprietary Unix operating systems developed and sold by IBM for several of its computer platforms.

Originally released for the IBM RT PC RISC workstation in 1986, AIX has supported a wide variety of hardware platforms, including the IBM RS/6000 series and later Power and PowerPC-based systems, IBM System i, System/370 mainframes, PS/2 personal computers, and the Apple Network Server. It is currently supported on IBM Power Systems alongside IBM i and Linux.

AIX is based on UNIX System V with 4.3BSD-compatible extensions. It is certified to the UNIX 03 and UNIX V7 marks of the Single UNIX Specification, beginning with AIX versions 5.3 and 7.2 TL5 respectively. Older versions were previously certified to the UNIX 95 and UNIX 98 marks.

AIX was the first operating system to have a journaling file system, and IBM has continuously enhanced the software with features such as processor, disk and network virtualization, dynamic hardware resource allocation (including fractional processor units), and reliability engineering ported from its mainframe designs.

Unix started life at AT&T's Bell Labs research center in the early 1970s, running on DEC minicomputers. By 1976, the operating system was in use at various academic institutions, including Princeton, where Tom Lyon and others ported it to the S/370, to run as a guest OS under VM/370. This port would later grow out to become UTS, a mainframe Unix offering by IBM's competitor Amdahl Corporation.
IBM's own involvement in Unix can be dated to 1979, when it assisted Bell Labs in doing its own Unix port to the 370 (to be used as a build host for the 5ESS switch's software). In the process, IBM made modifications to the TSS/370 Resident Supervisor to better support Unix.

It took until 1984 for IBM to offer its own Unix on the S/370 platform, VM/IX, which was developed by Interactive Systems Corporation using Unix System III as its base. VM/IX (and the modified version of VM/370 it required) was not a General Availability product; it was only obtainable as a PRPQ. In 1985, VM/IX was replaced by IBM IX/370, which was a GA product intended by IBM to compete with Amdahl UTS. IX/370 which was based on AT&T's Unix/360 6th Edition port (which only ran on TSS/370 as a time-share application), was updated to Unix System 5 and modified by IBM to run as a VM/370 guest OS. The IX/370 operating system offered special facilities for interoperating with PC/IX, Interactive/IBM's version of Unix for IBM PC compatible hardware, and was licensed at $10,000 per sixteen concurrent users.

AIX Version 1, introduced in 1986 for the IBM RT PC workstation, was based on UNIX System V Releases 1 and 2. In developing AIX, IBM and Interactive Systems Corporation (whom IBM contracted) also incorporated source code from 4.2 and 4.3 BSD UNIX.

Among other variants, IBM later produced AIX Version 2 (also known as AIX/6000), based on AIX Version 1, for their POWER-based RS/6000 platform. Since 1990, AIX has served as the primary operating system for the RS/6000 series (later renamed "IBM eServer pSeries", then "IBM System p", and now "IBM Power Systems"). 

AIX Version 3, introduced in 1988, for the PS/2 and VM/370 systems, developed by Locus Computing Corporation, added the Transparent Computing Facility. AIX Version 4, introduced in 1994, added symmetric multiprocessing with the introduction of the first RS/6000 SMP servers and continued to evolve through the 1990s, culminating with AIX 4.3.3 in 1999. Version 4.1, in a slightly modified form, was also the standard operating system for the Apple Network Server systems sold by Apple Computer to complement the Macintosh line.

In the late 1990s, under Project Monterey, IBM and the Santa Cruz Operation planned to integrate AIX and UnixWare into a single 32-bit/64-bit multiplatform UNIX with particular emphasis on running on Intel IA-64 (Itanium) architecture CPUs. A beta test version of AIX 5L for IA-64 systems was released, but according to documents released in the "SCO v. IBM" lawsuit, less than forty licenses for the finished Monterey Unix were ever sold before the project was terminated in 2002. In 2003, the SCO Group alleged that (among other infractions) IBM had misappropriated licensed source code from UNIX System V Release 4 for incorporation into AIX; SCO subsequently withdrew IBM's license to develop and distribute AIX. IBM maintains that their license was irrevocable, and continued to sell and support the product until the litigation was adjudicated.

AIX was a component of the 2003 "SCO v. IBM" lawsuit, in which the SCO Group filed a lawsuit against IBM, alleging IBM contributed SCO's intellectual property to the Linux codebase. The SCO Group, who argued they were the rightful owners of the copyrights covering the Unix operating system, attempted to revoke IBM's license to sell or distribute the AIX operating system. In March 2010, a jury returned a verdict finding that Novell, not the SCO Group, owns the rights to Unix.

AIX 6 was announced in May 2007, and it ran as an open beta from June 2007 until the general availability (GA) of AIX 6.1 on November 9, 2007. Major new features in AIX 6.1 included full role-based access control, workload partitions (which enable application mobility), enhanced security (Addition of AES encryption type for NFS v3 and v4), and Live Partition Mobility on the POWER6 hardware.

AIX 7.1 was announced in April 2010, and an open beta ran until general availability of AIX 7.1 in September 2010. Several new features, including better scalability, enhanced clustering and management capabilities were added. AIX 7.1 includes a new built-in clustering capability called Cluster Aware AIX. AIX is able to organize multiple LPARs through the multipath communications channel to neighboring CPUs, enabling very high-speed communication between processors. This enables multi-terabyte memory address range and page table access to support global petabyte shared memory space for AIX POWER7 clusters so that software developers can program a cluster as if it were a single system, without using message passing (i.e. semaphore-controlled Inter-process Communication). AIX administrators can use this new capability to cluster a pool of AIX nodes. By default, AIX V7.1 pins kernel memory and includes support to allow applications to pin their kernel stack. Pinning kernel memory and the kernel stack for applications with real-time requirements can provide performance improvements by ensuring that the kernel memory and kernel stack for an application is not paged out.

AIX 7.2 was announced in October 2015, and released in December 2015. The principal feature of AIX 7.2 is the Live Kernel Update capability, which allows OS fixes to replace the entire AIX kernel with no impact to applications, by live migrating workloads to a temporary surrogate AIX OS partition while the original OS partition is patched. AIX 7.2 was also restructured to remove obsolete components. The networking component, bos.net.tcp.client was repackaged to allow additional installation flexibility. Unlike AIX 7.1, AIX 7.2 is only supported on systems based on POWER7 or later processors.

In January 2023, IBM moved development of AIX to its Indian subsidiary.

The original AIX (sometimes called AIX/RT) was developed for the IBM RT PC workstation by IBM in conjunction with Interactive Systems Corporation, who had previously ported UNIX System III to the IBM PC for IBM as PC/IX. According to its developers, the AIX source (for this initial version) consisted of one million lines of code. Installation media consisted of eight 1.2M floppy disks. The RT was based on the IBM ROMP microprocessor, the first commercial RISC chip. This was based on a design pioneered at IBM Research (the IBM 801).

One of the novel aspects of the RT design was the use of a microkernel, called Virtual Resource Manager (VRM). The keyboard, mouse, display, disk drives and network were all controlled by a microkernel. One could "hotkey" from one operating system to the next using the Alt-Tab key combination. Each OS in turn would get possession of the keyboard, mouse and display. Besides AIX v2, the PICK OS also included this microkernel.

Much of the AIX v2 kernel was written in the PL/8 programming language, which proved troublesome during the migration to AIX v3. AIX v2 included full TCP/IP networking, as well as SNA and two networking file systems: NFS, licensed from Sun Microsystems, and Distributed Services (DS). DS had the distinction of being built on top of SNA, and thereby being fully compatible with DS on and on midrange systems running OS/400 through IBM i. For the graphical user interfaces, AIX v2 came with the X10R3 and later the X10R4 and X11 versions of the X Window System from MIT, together with the Athena widget set. Compilers for Fortran and C were available.

AIX PS/2 (also known as AIX/386) was developed by Locus Computing Corporation under contract to IBM. AIX PS/2, first released in October 1988, ran on IBM PS/2 personal computers with Intel 386 and compatible processors.

The product was announced in September 1988 with a baseline tag price of $595, although some utilities like uucp were included in a separate Extension package priced at $250. nroff and troff for AIX were also sold separately in a Text Formatting System package priced at $200. The TCP/IP stack for AIX PS/2 retailed for another $300. The X Window System package was priced at $195, and featured a graphical environment called the AIXwindows Desktop, based on IXI's X.desktop. The C and FORTRAN compilers each had a price tag of $275. Locus also made available their DOS Merge virtual machine environment for AIX, which could run MS DOS 3.3 applications inside AIX; DOS Merge was sold separately for another $250. IBM also offered a $150 AIX PS/2 DOS Server Program, which provided file server and print server services for client computers running PC DOS 3.3.

The last version of PS/2 AIX is 1.3. It was released in 1992 and announced to add support for non-IBM (non-microchannel) computers as well. Support for PS/2 AIX ended in March 1995.

In 1988, IBM announced AIX/370, also developed by Locus Computing. AIX/370 was IBM's fourth attempt to offer Unix-like functionality for their mainframe line, specifically the System/370 (the prior versions were a TSS/370-based Unix system developed jointly with AT&T c.1980, a VM/370-based system named VM/IX developed jointly with Interactive Systems Corporation c.1984, and a VM/370-based version of TSS/370 named IX/370 which was upgraded to be compatible with UNIX System V). AIX/370 was released in 1990 with functional equivalence to System V Release 2 and 4.3BSD as well as IBM enhancements. With the introduction of the ESA/390 architecture, AIX/370 was replaced by AIX/ESA in 1991, which was based on OSF/1, and also ran on the System/390 platform. This development effort was made partly to allow IBM to compete with Amdahl UTS. Unlike AIX/370, AIX/ESA ran both natively as the host operating system, and as a guest under VM. AIX/ESA, while technically advanced, had little commercial success, partially because UNIX functionality was added as an option to the existing mainframe operating system, MVS, as MVS/ESA SP Version 4 Release 3 OpenEdition in 1994, and continued as an integral part of MVS/ESA SP Version 5, OS/390 and z/OS, with the name eventually changing from "OpenEdition" to "Unix System Services". IBM also provided OpenEdition in VM/ESA Version 2 through z/VM.
As part of Project Monterey, IBM released a beta test version of AIX 5L for the IA-64 (Itanium) architecture in 2001, but this never became an official product due to lack of interest.

The Apple Network Server (ANS) systems were PowerPC-based systems designed by Apple Computer to have numerous high-end features that standard Apple hardware did not have, including swappable hard drives, redundant power supplies, and external monitoring capability. These systems were more or less based on the Power Macintosh hardware available at the time but were designed to use AIX (versions 4.1.4 or 4.1.5) as their native operating system in a specialized version specific to the ANS called AIX for Apple Network Servers.

AIX was only compatible with the Network Servers and was not ported to standard Power Macintosh hardware. It should not be confused with A/UX, Apple's earlier version of Unix for 68k-based Macintoshes.

The release of AIX version 3 (sometimes called AIX/6000) coincided with the announcement of the first POWER1-based IBM RS/6000 models in 1990.

AIX v3 innovated in several ways on the software side. It was the first operating system to introduce the idea of a journaling file system, JFS, which allowed for fast boot times by avoiding the need to ensure the consistency of the file systems on disks (see fsck) on every reboot. Another innovation was shared libraries which avoid the need for static linking from an application to the libraries it used. The resulting smaller binaries used less of the hardware RAM to run, and used less disk space to install. Besides improving performance, it was a boon to developers: executable binaries could be in the tens of kilobytes instead of a megabyte for an executable statically linked to the C library. AIX v3 also scrapped the microkernel of AIX v2, a contentious move that resulted in v3 containing no PL/8 code and being somewhat more "pure" than v2.

Other notable subsystems included:


In addition, AIX applications can run in the PASE subsystem under IBM i.

IBM formerly made the AIX for RS/6000 source code available to customers for an additional fee; in 1991, IBM customers could order the AIX 3.0 source code for a one-time charge of US$60,000; subsequently, IBM released the AIX 3.1 source code in 1992, and AIX 3.2 in 1993. These source code distributions excluded certain files (authored by third-parties) which IBM did not have rights to redistribute, and also excluded layered products such as the MS-DOS emulator and the C compiler. Furthermore, in order to be able to license the AIX source code, the customer first had to procure source code license agreements with AT&T and the University of California, Berkeley.






The default shell was Bourne shell up to AIX version 3, but was changed to KornShell (ksh88) in version 4 for XPG4 and POSIX compliance.

The Common Desktop Environment (CDE) is AIX's default graphical user interface. As part of Linux Affinity and the free AIX Toolbox for Linux Applications (ATLA), open-source KDE Plasma Workspaces and GNOME desktop are also available.

SMIT is the System Management Interface Tool for AIX. It allows a user to navigate a menu hierarchy of commands, rather than using the command line. Invocation is typically achieved with the command codice_1. Experienced system administrators make use of the codice_2 function key which generates the command line that SMIT will invoke to complete it.
SMIT also generates a log of commands that are performed in the codice_3 file. The codice_3 file automatically records the commands with the command flags and parameters used. The codice_3 file can be used as an executable shell script to rerun system configuration tasks. SMIT also creates the codice_6 file, which contains additional detailed information that can be used by programmers in extending the SMIT system.

codice_1 and codice_8 refer to the same program, though codice_8 invokes the text-based version, while codice_1 will invoke an X Window System based interface if possible; however, if codice_1 determines that X Window System capabilities are not present, it will present the text-based version instead of failing. Determination of X Window System capabilities is typically performed by checking for the existence of the codice_12 variable.

Object Data Manager (ODM) is a database of system information integrated into AIX, analogous to the registry in Microsoft Windows. A good understanding of the ODM is essential for managing AIX systems.

Data managed in ODM is stored and maintained as objects with associated attributes. Interaction with ODM is possible via application programming interface (API) library for programs, and command-line utilities such as "odmshow", "odmget", "odmadd", "odmchange" and "odmdelete" for shell scripts and users. SMIT and its associated AIX commands can also be used to query and modify information in the ODM. ODM is stored on disk using Berkeley DB files.

Example of information stored in the ODM database are:



AppleTalk

AppleTalk is a discontinued proprietary suite of networking protocols developed by Apple Computer for their Macintosh computers. AppleTalk includes a number of features that allow local area networks to be connected with no prior setup or the need for a centralized router or server of any sort. Connected AppleTalk-equipped systems automatically assign addresses, update the distributed namespace, and configure any required inter-networking routing.

AppleTalk was released in 1985 and was the primary protocol used by Apple devices through the 1980s and 1990s. Versions were also released for the IBM PC and compatibles and the Apple IIGS. AppleTalk support was also available in most networked printers (especially laser printers), some file servers, and a number of routers.

The rise of TCP/IP during the 1990s led to a reimplementation of most of these types of support on that protocol, and AppleTalk became unsupported as of the release of Mac OS X v10.6 in 2009. Many of AppleTalk's more advanced autoconfiguration features have since been introduced in Bonjour, while Universal Plug and Play serves similar needs.

After the release of the Apple Lisa computer in January 1983, Apple invested considerable effort in the development of a local area networking (LAN) system for the machines. Known as AppleNet, it was based on the seminal Xerox XNS protocol stack but running on a custom 1 Mbit/s coaxial cable system rather than Xerox's 2.94 Mbit/s Ethernet. AppleNet was announced early in 1983 with a full introduction at the target price of $500 for plug-in AppleNet cards for the Lisa and the Apple II.

At that time, early LAN systems were just coming to market, including Ethernet, Token Ring, Econet, and ARCNET. This was a topic of major commercial effort at the time, dominating shows like the National Computer Conference (NCC) in Anaheim in May 1983. All of the systems were jockeying for position in the market, but even at this time, Ethernet's widespread acceptance suggested it was to become a "de facto" standard. It was at this show that Steve Jobs asked Gursharan Sidhu a seemingly innocuous question: "Why has networking not caught on?"

Four months later, in October, AppleNet was cancelled. At the time, they announced that "Apple realized that it's not in the business to create a networking system. We built and used AppleNet in-house, but we realized that if we had shipped it, we would have seen new standards coming up." In January, Jobs announced that they would instead be supporting IBM's Token Ring, which he expected to come out in a "few months".

Through this period, Apple was deep in development of the Macintosh computer. During development, engineers had made the decision to use the Zilog 8530 serial controller chip (SCC) instead of the lower-cost and more common UART to provide serial port connections. The SCC cost about $5 more than a UART, but offered much higher speeds of up to 250 kilobits per second (or higher with additional hardware) and internally supported a number of basic networking-like protocols like IBM's Bisync.

The SCC was chosen because it would allow multiple devices to be attached to the port. Peripherals equipped with similar SCCs could communicate using the built-in protocols, interleaving their data with other peripherals on the same bus. This would eliminate the need for more ports on the back of the machine, and allowed for the elimination of expansion slots for supporting more complex devices. The initial concept was known as AppleBus, envisioning a system controlled by the host Macintosh polling "dumb" devices in a fashion similar to the modern Universal Serial Bus.

The Macintosh team had already begun work on what would become the LaserWriter and had considered a number of other options to answer the question of how to share these expensive machines and other resources. A series of memos from Bob Belleville clarified these concepts, outlining the Mac, LaserWriter, and a file server system which would become the Macintosh Office. By late 1983 it was clear that IBM's Token Ring would not be ready in time for the launch of the Mac, and might miss the launch of these other products as well. In the end, Token Ring would not ship until October 1985.

Jobs' earlier question to Sidhu had already sparked a number of ideas. When AppleNet was cancelled in October, Sidhu led an effort to develop a new networking system based on the AppleBus hardware. This new system would not have to conform to any existing preconceptions, and was designed to be worthy of the Mac – a system that was user-installable and required no configuration or fixed network addresses – in short, a true plug-and-play network. Considerable effort was needed, but by the time the Mac was released, the basic concepts had been outlined, and some of the low-level protocols were on their way to completion. Sidhu mentioned the work to Belleville only two hours after the Mac was announced.

The "new" AppleBus was announced in early 1984, allowing direct connection from the Mac or Lisa through a small box that is plugged into the serial port and connected via cables to the next computer upstream and downstream. Adaptors for Apple II and Apple III were also announced. Apple also announced that an AppleBus network could be attached to, and would appear to be a single node within, a Token Ring system. Details of how this would work were sketchy.

Just prior to its release in early 1985, AppleBus was renamed AppleTalk. Initially marketed as AppleTalk Personal Network, it comprised a family of network protocols and a physical layer.

The physical layer had a number of limitations, including a speed of only 230.4 kbit/s, a maximum distance of from end to end, and only 32 nodes per LAN. But as the basic hardware was built into the Mac, adding nodes only cost about $50 for the adaptor box. In comparison, Ethernet or Token Ring cards cost hundreds or thousands of dollars. Additionally, the entire networking stack required only about 6 kB of RAM, allowing it to run on any Mac.

The relatively slow speed of AppleTalk allowed further reductions in cost. Instead of using RS-422's balanced transmit and receive circuits, the AppleTalk cabling used a single common electrical ground, which limited speeds to about 500 kbit/s, but allowed one conductor to be removed. This meant that common three-conductor cables could be used for wiring. Additionally, the adaptors were designed to be "self-terminating", meaning that nodes at the end of the network could simply leave their last connector unconnected. There was no need for the wires to be connected back together into a loop, nor the need for hubs or other devices.

The system was designed for future expansion; the addressing system allowed for expansion to 255 nodes in a LAN (although only 32 could be used at that time), and by using "bridges" (which came to be known as "routers", although technically not the same) one could interconnect LANs into larger collections. "Zones" allowed devices to be addressed within a bridge-connected internet. Additionally, AppleTalk was designed from the start to allow use with any potential underlying physical link, and within a few years, the physical layer would be renamed LocalTalk, so as to differentiate it from the AppleTalk protocols.

The main advantage of AppleTalk was that it was completely maintenance-free. To join a device to a network, a user simply plugged the adaptor into the machine, then connected a cable from it to any free port on any other adaptor. The AppleTalk network stack negotiated a network address, assigned the computer a human-readable name, and compiled a list of the names and types of other machines on the network so the user could browse the devices through the Chooser. AppleTalk was so easy to use that ad hoc networks tended to appear whenever multiple Macs were in the same room. Apple would later use this in an advertisement showing a network being created between two seats in an airplane.

A thriving third-party market for AppleTalk devices developed over the next few years. One particularly notable example was an alternate adaptor designed by BMUG and commercialized by Farallon as PhoneNET in 1987. This was essentially a replacement for Apple's connector that had conventional phone jacks instead of Apple's round connectors. PhoneNet allowed AppleTalk networks to be connected together using normal telephone wires, and with very little extra work, could run analog phones and AppleTalk on a single four-conductor phone cable.

Other companies took advantage of the SCC's ability to read external clocks in order to support higher transmission speeds, up to 1 Mbit/s. In these systems, the external adaptor also included its own clock, and used that to signal the SCC's clock input pins. The best-known such system was Centram's FlashTalk, which ran at 768 kbit/s, and was intended to be used with their TOPS networking system. A similar solution was the 850 kbit/s DaynaTalk, which used a separate box that plugged in between the computer and a normal LocalTalk/PhoneNet box. Dayna also offered a PC expansion card that ran up to 1.7 Mbit/s when talking to other Dayna PC cards. Several other systems also existed with even higher performance, but these often required special cabling that was incompatible with LocalTalk/PhoneNet, and also required patches to the networking stack that often caused problems.

As Apple expanded into more commercial and education markets, they needed to integrate AppleTalk into existing network installations. Many of these organizations had already invested in a very expensive Ethernet infrastructure and there was no direct way to connect a Macintosh to Ethernet. AppleTalk included a protocol structure for interconnecting AppleTalk subnets and so as a solution, EtherTalk was initially created to use the Ethernet as a backbone between LocalTalk subnets. To accomplish this, organizations would need to purchase a LocalTalk-to-Ethernet bridge and Apple left it to third parties to produce these products. A number of companies responded, including Hayes and a few newly formed companies like Kinetics.

By 1987, Ethernet was clearly winning the standards battle over Token Ring, and in the middle of that year, Apple introduced EtherTalk 1.0, an implementation of the AppleTalk protocol over the Ethernet physical layer. Introduced for the newly released Macintosh II computer, one of Apple's first two Macintoshes with expansion slots (the Macintosh SE had one slot of a different type), the operating system included a new Network control panel that allowed the user to select which physical connection to use for networking (from "Built-in" or "EtherTalk"). At introduction, Ethernet interface cards were available from 3Com and Kinetics that plugged into a Nubus slot in the machine. The new networking stack also expanded the system to allow a full 255 nodes per LAN. With EtherTalk's release, AppleTalk Personal Network was renamed LocalTalk, the name it would be known under for the bulk of its life. Token Ring would later be supported with a similar TokenTalk product, which used the same Network control panel and underlying software. Over time, many third-party companies would introduce compatible Ethernet and Token Ring cards that used these same drivers.

The appearance of a Macintosh with a direct Ethernet connection also magnified the Ethernet and LocalTalk compatibility problem: Networks with new and old Macs needed some way to communicate with each other. This could be as simple as a network of Ethernet Mac II's trying to talk to a LaserWriter that only connected to LocalTalk. Apple initially relied on the aforementioned LocalTalk-to-Ethernet bridge products, but contrary to Apple's belief that these would be low-volume products, by the end of 1987, 130,000 such networks were in use. AppleTalk was at that time the most used networking system in the world, with over three times the installations of any other vendor.

1987 also marked the introduction of the AppleShare product, a dedicated file server that ran on any Mac with 512 kB of RAM or more. A common AppleShare machine was the Mac Plus with an external SCSI hard drive. AppleShare was the #3 network operating system in the late 1980s, behind Novell NetWare and Microsoft's MS-Net. AppleShare was effectively the replacement for the failed Macintosh Office efforts, which had been based on a dedicated file server device.

A significant re-design was released in 1989 as AppleTalk Phase II. In many ways, Phase II can be considered an effort to make the earlier version (never called Phase I) more generic. LANs could now support more than 255 nodes, and zones were no longer associated with physical networks but were entirely virtual constructs used simply to organize nodes. For instance, one could now make a "Printers" zone that would list all the printers in an organization, or one might want to place that same device in the "2nd Floor" zone to indicate its physical location. Phase II also included changes to the underlying inter-networking protocols to make them less "chatty", which had previously been a serious problem on networks that bridged over wide-area networks.

By this point, Apple had a wide variety of communications products under development, and many of these were announced along with AppleTalk Phase II. These included updates to EtherTalk and TokenTalk, AppleTalk software and LocalTalk hardware for the IBM PC, EtherTalk for Apple's A/UX operating system allowing it to use LaserWriters and other network resources, and the Mac X.25 and MacX products.

Ethernet had become almost universal by 1990, and it was time to build Ethernet into Macs direct from the factory. However, the physical wiring used by these networks was not yet completely standardized. Apple solved this problem using a single port on the back of the computer into which the user could plug an adaptor for any given cabling system. This FriendlyNet system was based on the industry-standard Attachment Unit Interface or AUI, but deliberately chose a non-standard connector that was smaller and easier to use, which they called "Apple AUI", or AAUI. FriendlyNet was first introduced on the Quadra 700 and Quadra 900 computers, and used across much of the Mac line for some time. As with LocalTalk, a number of third-party FriendlyNet adaptors quickly appeared.

As 10BASE-T became the de facto cabling system for Ethernet, second-generation Power Macintosh machines added a 10BASE-T port in addition to AAUI. The PowerBook 3400c and lower-end Power Macs also added 10BASE-T. The Power Macintosh 7300/8600/9600 were the final Macs to include AAUI, and 10BASE-T became universal starting with the Power Macintosh G3 and PowerBook G3.

From the beginning of AppleTalk, users wanted to connect the Macintosh to TCP/IP network environments. In 1984, Bill Croft at Stanford University pioneered the development of IP packets encapsulated in DDP as part of the SEAGATE (Stanford Ethernet–AppleTalk Gateway) project. SEAGATE was commercialized by Kinetics in their LocalTalk-to-Ethernet bridge as an additional routing option. A few years later, MacIP was separated from the SEAGATE code and became the de facto method for IP packets to be routed over LocalTalk networks. By 1986, Columbia University released the first version of the Columbia AppleTalk Package (CAP) that allowed higher integration of Unix, TCP/IP, and AppleTalk environments. In 1988, Apple released MacTCP, a system that allowed the Mac to support TCP/IP on machines with suitable Ethernet hardware. However, this left many universities with the problem of supporting IP on their many LocalTalk-equipped Macs. It was soon common to include MacIP support in LocalTalk-to-Ethernet bridges. MacTCP would not become a standard part of the Classic Mac OS until 1994, by which time it also supported SNMP and PPP.

For some time in the early 1990s, the Mac was a primary client on the rapidly expanding Internet. Among the better-known programs in wide use were Fetch, Eudora, eXodus, NewsWatcher, and the NCSA packages, especially NCSA Mosaic and its offspring, Netscape Navigator. Additionally, a number of server products appeared that allowed the Mac to host Internet content. Through this period, Macs had about 2 to 3 times as many clients connected to the Internet as any other platform, despite the relatively small overall microcomputer market share.

As the world quickly moved to IP for both LAN and WAN uses, Apple was faced with maintaining two increasingly outdated code bases on an ever-wider group of machines as well as the introduction of the PowerPC-based machines. This led to the Open Transport efforts, which re-implemented both MacTCP and AppleTalk on an entirely new code base adapted from the Unix standard STREAMS. Early versions had problems and did not become stable for some time. By that point, Apple was deep in their ultimately doomed Copland efforts.

With the purchase of NeXT and subsequent development of Mac OS X, AppleTalk was strictly a legacy system. Support was added to Mac OS X in order to provide support for a large number of existing AppleTalk devices, notably laser printers and file shares, but alternate connection solutions common in this era, notably USB for printers, limited their demand. As Apple abandoned many of these product categories, and all new systems were based on IP, AppleTalk became less and less common. AppleTalk support was finally removed from the macOS line in Mac OS X v10.6 in 2009.

However, the loss of AppleTalk did not reduce the desire for networking solutions that combined its ease of use with IP routing. Apple has led the development of many such efforts, from the introduction of the AirPort router to the development of the zero-configuration networking system and their implementation of it, Rendezvous, later renamed "Bonjour".

As of 2020, AppleTalk support has been completely removed from legacy support with macOS 11 Big Sur.

The AppleTalk design rigorously followed the OSI model of protocol layering. Unlike most of the early LAN systems, AppleTalk was not built using the archetypal Xerox XNS system. The intended target was not Ethernet, and it did not have 48-bit addresses to route. Nevertheless, many portions of the AppleTalk system have direct analogs in XNS.

One key differentiation for AppleTalk was it contained two protocols aimed at making the system completely self-configuring. The "AppleTalk address resolution protocol" ("AARP") allowed AppleTalk hosts to automatically generate their own network addresses, and the "Name Binding Protocol" ("NBP") was a dynamic system for mapping network addresses to user-readable names. Although systems similar to AARP existed in other systems, Banyan VINES for instance. Beginning about 2002 Rendezvous (the combination of DNS-based service discovery, Multicast DNS, and link-local addressing) provided capabilities and usability using IP that were similar to those of AppleTalk.

Both AARP and NBP had defined ways to allow "controller" devices to override the default mechanisms. The concept was to allow routers to provide the information or "hardwire" the system to known addresses and names. On larger networks where AARP could cause problems as new nodes searched for free addresses, the addition of a router could reduce "chattiness." Together AARP and NBP made AppleTalk an easy-to-use networking system. New machines were added to the network by plugging them in and optionally giving them a name. The NBP lists were examined and displayed by a program known as the "Chooser" which would display a list of machines on the local network, divided into classes such as file-servers and printers.

An AppleTalk address was a four-byte quantity. This consisted of a two-byte network number, a one-byte node number, and a one-byte socket number. Of these, only the network number required any configuration, being obtained from a router. Each node dynamically chose its own node number, according to a protocol (originally the LocalTalk Link Access Protocol LLAP and later, for Ethernet/EtherTalk, the AppleTalk Address Resolution Protocol, AARP) which handled contention between different nodes accidentally choosing the same number. For socket numbers, a few well-known numbers were reserved for special purposes specific to the AppleTalk protocol itself. Apart from these, all application-level protocols were expected to use dynamically-assigned socket numbers at both the client and server end.

Because of this dynamism, users could not be expected to access services by specifying their address. Instead, all services had "names" which, being chosen by humans, could be expected to be meaningful to users, and also could be sufficiently long to minimize the chance of conflicts.

As NBP names translated to an address, which included a socket number as well as a node number, a name in AppleTalk mapped directly to a "service" being provided by a machine, which was entirely separate from the name of the machine itself. Thus, services could be moved to a different machine and, so long as they kept the same service name, there was no need for users to do anything different in order to continue accessing the service. And the same machine could host any number of instances of services of the same type, without any network connection conflicts.

Contrast this with "A records" in the DNS, in which a name translates to a machine's address, not including the port number that might be providing a service. Thus, if people are accustomed to using a particular machine name to access a particular service, their access will break when the service is moved to a different machine. This can be mitigated somewhat by insistence on using "CNAME records" indicating service rather than actual machine names to refer to the service, but there is no way of guaranteeing that users will follow such a convention. Some newer protocols, such as Kerberos and Active Directory use DNS SRV records to identify services by name, which is much closer to the AppleTalk model.

The AppleTalk Address Resolution Protocol (AARP) resolves AppleTalk addresses to link layer addresses. It is functionally equivalent to ARP and obtains address resolution by a method very similar to ARP.

AARP is a fairly simple system. When powered on, an AppleTalk machine broadcasts an "AARP probe packet" asking for a network address, intending to hear back from controllers such as routers. If no address is provided, one is picked at random from the "base subnet", 0. It then broadcasts another packet saying "I am selecting this address", and then waits to see if anyone else on the network complains. If another machine has that address, the newly connecting machine will pick another address, and keep trying until it finds a free one. On a network with many machines it may take several tries before a free address is found, so for performance purposes the successful address is recorded in NVRAM and used as the default address in the future. This means that in most real-world setups where machines are added a few at a time, only one or two tries are needed before the address effectively becomes constant.

The AppleTalk Data Stream Protocol (ADSP) was a comparatively late addition to the AppleTalk protocol suite, done when it became clear that a TCP-style reliable connection-oriented transport was needed. Significant differences from TCP were that:

The Apple Filing Protocol (AFP), formerly AppleTalk Filing Protocol, is the protocol for communicating with AppleShare file servers. Built on top of AppleTalk Session Protocol (for legacy AFP over DDP) or the Data Stream Interface (for AFP over TCP), it provides services for authenticating users (extensible to different authentication methods including two-way random-number exchange) and for performing operations specific to the Macintosh HFS filesystem. AFP is still in use in macOS, even though most other AppleTalk protocols have been deprecated.

The AppleTalk Session Protocol (ASP) was an intermediate protocol, built on top of AppleTalk Transaction Protocol (ATP), which in turn was the foundation of AFP. It provided basic services for requesting responses to arbitrary "commands" and performing out-of-band status queries. It also allowed the server to send asynchronous "attention" messages to the client.

The AppleTalk Transaction Protocol (ATP) was the original reliable transport-level protocol for AppleTalk, built on top of DDP. At the time it was being developed, a full, reliable connection-oriented protocol like TCP was considered to be too expensive to implement for most of the intended uses of AppleTalk. Thus, ATP was a simple request/response exchange, with no need to set up or tear down connections.

An ATP "request" packet could be answered by up to eight "response" packets. The requestor then sent an "acknowledgement" packet containing a bit mask indicating which of the response packets it received, so the responder could retransmit the remainder.

ATP could operate in either "at-least-once" mode or "exactly-once" mode. Exactly-once mode was essential for operations which were not idempotent; in this mode, the responder kept a copy of the response buffers in memory until successful receipt of a "release" packet from the requestor, or until a timeout elapsed. This way, it could respond to duplicate requests with the same transaction ID by resending the same response data, without performing the actual operation again.

The Datagram Delivery Protocol (DDP) was the lowest-level data-link-independent transport protocol. It provided a datagram service with no guarantees of delivery. All application-level protocols, including the infrastructure protocols NBP, RTMP and ZIP, were built on top of DDP. AppleTalk's DDP corresponds closely to the Network layer of the Open Systems Interconnection (OSI) communication model.

The Name Binding Protocol (NBP) was a dynamic, distributed system for managing AppleTalk names. When a service started up on a machine, it registered a name for itself as chosen by a human administrator. At this point, NBP provided a system for checking that no other machine had already registered the same name. Later, when a client wanted to access that service, it used NBP to query machines to find that service. NBP provided browsability ("what are the names of all the services available?") as well as the ability to find a service with a particular name. Names were human-readable, containing spaces and upper- and lower-case letters, and including support for searching.

The AppleTalk Echo Protocol (AEP) was a transport layer protocol designed to test the reachability of network nodes. AEP generates packets to be sent to the network node and is identified in the Type field of a packet as an AEP packet. The packet is first passed to the source DDP. After it is identified as an AEP packet, it is forwarded to the node where the packet is examined by the DDP at the destination. After the packet is identified as an AEP packet, the packet is then copied and a field in the packet is altered to create an AEP reply packet, and is then returned to the source node.

The Printer Access Protocol (PAP) was the standard way of communicating with PostScript printers. It was built on top of ATP. When a PAP connection was opened, each end sent the other an ATP request which basically meant "send me more data". The client's response to the server was to send a block of PostScript code, while the server could respond with any diagnostic messages that might be generated as a result, after which another "send-more-data" request was sent. This use of ATP provided automatic flow control; each end could only send data to the other end if there was an outstanding ATP request to respond to.

PAP also provided for out-of-band status queries, handled by separate ATP transactions. Even while it was busy servicing a print job from one client, a PAP server could continue to respond to status requests from any number of other clients. This allowed other Macintoshes on the LAN that were waiting to print to display status messages indicating that the printer was busy, and what the job was that it was busy with.

The Routing Table Maintenance Protocol (RTMP) was the protocol by which routers kept each other informed about the topology of the network. This was the only part of AppleTalk that required periodic unsolicited broadcasts: every 10 seconds, each router had to send out a list of all the network numbers it knew about and how far away it thought they were.

The Zone Information Protocol (ZIP) was the protocol by which AppleTalk network numbers were associated with zone names. A "zone" was a subdivision of the network that made sense to humans (for example, "Accounting Department"); but while a network number had to be assigned to a topologically-contiguous section of the network, a zone could include several different discontiguous portions of the network.

The initial default hardware implementation for AppleTalk was a high-speed serial protocol known as "LocalTalk" that used the Macintosh's built-in RS-422 ports at 230.4 kbit/s. LocalTalk used a splitter box in the RS-422 port to provide an upstream and downstream cable from a single port. The topology was a bus: cables were daisy-chained from each connected machine to the next, up to the maximum of 32 permitted on any LocalTalk segment. The system was slow by today's standards, but at the time the additional cost and complexity of networking on PC machines was such that it was common that Macs were the only networked personal computers in an office. Other larger computers, such as UNIX or VAX workstations, would commonly be networked via Ethernet.

Other physical implementations were also available. A very popular replacement for LocalTalk was "PhoneNET", a third-party solution from Farallon Computing, Inc. (renamed Netopia, acquired by Motorola in 2007) that also used the RS-422 port and was indistinguishable from LocalTalk as far as Apple's LocalTalk port drivers were concerned, but ran over very inexpensive standard phone cabling with four-wire, six-position modular connectors, the same cables used to connect landline telephones. Since it used the second pair of wires, network devices could even be connected through existing telephone jacks if a second line was not present. Foreshadowing today's network hubs and switches, Farallon provided solutions for PhoneNet to be used in "star" as well as "bus" configurations, with both "passive" star connections (with the phone wires simply bridged to each other at a central point), and "active" star with "PhoneNet Star Controller" hub hardware. In a star configuration, any wiring issue only affected one device, and problems were easy to pinpoint. PhoneNet's low cost, flexibility, and easy troubleshooting resulted in it being the dominant choice for Mac networks into the early 1990s.

AppleTalk protocols also came to run over Ethernet (first coaxial and then twisted pair) and Token Ring physical layers, labeled by Apple as "EtherTalk" and "TokenTalk", respectively. EtherTalk gradually became the dominant implementation method for AppleTalk as Ethernet became generally popular in the PC industry throughout the 1990s. Besides AppleTalk and TCP/IP, any Ethernet network could also simultaneously carry other protocols such as DECnet and IPX.

When AppleTalk was first introduced, the dominant office computing platform was the PC compatible running MS-DOS. Apple introduced the AppleTalk PC Card in early 1987, allowing PCs to join AppleTalk networks and print to LaserWriter printers. A year later AppleShare PC was released, allowing PCs to access AppleShare file servers.

The "TOPS Teleconnector" MS-DOS networking system over AppleTalk system enabled MS-DOS PCs to communicate over AppleTalk network hardware; it comprised an AppleTalk interface card for the PC and a suite of networking software allowing such functions as file, drive and printer sharing. As well as allowing the construction of a PC-only AppleTalk network, it allowed communication between PCs and Macs with TOPS software installed. (Macs without TOPS installed could use the same network but only to communicate with other Apple machines.) The Mac TOPS software did not match the quality of Apple's own either in ease of use or in robustness and freedom from crashes, but the DOS software was relatively simple to use in DOS terms, and was robust.

The BSD and Linux operating systems support AppleTalk through an open source project called Netatalk, which implements the complete protocol suite and allows them to both act as native file or print servers for Macintosh computers, and print to LocalTalk printers over the network.

The Windows Server operating systems supported AppleTalk starting with Windows NT and ending after Windows Server 2003. Miramar included AppleTalk in its PC MacLAN product which was discontinued by CA in 2007. GroupLogic continues to bundle its AppleTalk protocol with its ExtremeZ-IP server software for Macintosh-Windows integration which supports Windows Server 2008 and Windows Vista as well prior versions. HELIOS Software GmbH offers a proprietary implementation of the AppleTalk protocol stack, as part of their HELIOS UB2 server. This is essentially a File and Print Server suite that runs on a whole range of different platforms.

In addition, Columbia University released the Columbia AppleTalk Package (CAP) which implemented the protocol suite for various Unix flavors including Ultrix, SunOS, BSD and IRIX. This package is no longer actively maintained.



Apple II series

The Apple II series (trademarked with square brackets as "Apple ][" and rendered on later models as "Apple //") is a family of home computers, one of the first highly successful mass-produced microcomputer products, designed primarily by Steve Wozniak, manufactured by Apple Computer (now Apple Inc.), and launched in 1977 with the original Apple II.

In terms of ease of use, features, and expandability, the Apple II was a major advancement over its predecessor, the Apple I, a limited-production bare circuit board computer for electronics hobbyists. Through 1988, a number of models were introduced, with the most popular, the Apple IIe, remaining relatively unchanged into the 1990s.

A model with more advanced graphics and sound and a 16-bit processor, the Apple II, was added in 1986. It remained compatible with earlier Apple II models, but the II had more in common with mid-1980s systems like the Atari ST, Amiga, and Acorn Archimedes.

The Apple II was first sold on June 10, 1977. By the end of production in 1993, somewhere between five and six million Apple II series computers (including about 1.25 million Apple II models) had been produced. The Apple II was one of the longest running mass-produced home computer series, with models in production for just under 17 years.

The Apple II became one of several recognizable and successful computers during the 1980s and early 1990s, although this was mainly limited to the US. It was aggressively marketed through volume discounts and manufacturing arrangements to educational institutions, which made it the first computer in widespread use in American secondary schools, displacing the early leader Commodore PET. The effort to develop educational and business software for the Apple II, including the 1979 release of the popular VisiCalc spreadsheet, made the computer especially popular with business users and families.

Despite the introduction of the Motorola 68000-based Macintosh in 1984, the Apple II series still reportedly accounted for 85% of the company's hardware sales in the first quarter of fiscal 1985. Apple continued to sell Apple II systems alongside the Macintosh until terminating the II in December 1992 and the IIe in November 1993. The last II-series Apple in production, the IIe card for Macintoshes, was discontinued on October 15, 1993. The total Apple II sales of all of its models during its 16-year production run were about 6 million units, with the peak occurring in 1983 when 1 million were sold.

All the machines in the series, except the //c, shared similar overall design elements. The plastic case was designed to look more like a home appliance than a piece of electronic equipment, and the machine could be opened without the use of tools, allowing access to the computer's internals.

The motherboard held eight expansion slots and an array of random access memory (RAM) sockets that could hold up to 48 kilobytes. Over the course of the Apple II series' life, an enormous amount of first- and third-party hardware was made available to extend the capabilities of the machine.

The //c was designed as a compact, portable unit, not intended to be disassembled, and could not use most of the expansion hardware sold for the other machines in the series.

All machines in the Apple II series had a built-in keyboard, with the exception of the IIgs which had a separate keyboard.

Apple IIs had color and high-resolution graphics modes, sound capabilities and a built-in BASIC programming language. The Apple II was targeted for the masses rather than just hobbyists and engineers, and influenced many of the microcomputers that followed it. Unlike preceding home microcomputers, it was sold as a finished consumer appliance rather than as a kit (unassembled or preassembled). The Apple II series eventually supported over 1,500 software programs. 

Apple marketed the machine as a durable product, including a 1981 ad in which an Apple II survived a fire started when a cat belonging to one early user knocked over a lamp.

The original Apple II provided an operating system in ROM along with a BASIC variant called Integer BASIC. The only form of storage available was cassette tape.

When the Disk II floppy disk drive was released in 1978, a new operating system, Apple DOS, was commissioned from Shepardson Microsystems and developed by Paul Laughton, adding support for the disk drive. The final and most popular version of this software was Apple DOS 3.3.

Apple DOS was superseded by ProDOS, which supported a hierarchical filesystem and larger storage devices. With an optional third-party Z80-based expansion card, the Apple II could boot into the CP/M operating system and run WordStar, dBase II, and other CP/M software. With the release of MousePaint in 1984 and the Apple II in 1986, the platform took on the look of the Macintosh user interface, including a mouse.

Apple eventually released Applesoft BASIC, a more advanced variant of the language which users could run instead of Integer BASIC for more capabilities.

Some commercial Apple II software booted directly and did not use standard DOS disk formats. This discouraged the copying or modifying of the software on the disks, and improved loading speed.

The first Apple II computers went on sale on June 10, 1977 with a MOS Technology 6502 (later Synertek) microprocessor running at 1.023 MHz, 4 KB of RAM, an audio cassette interface for loading programs and storing data, and the Integer BASIC programming language built into the ROMs. The video controller displayed 40 columns by 24 lines of monochrome, upper-case-only (the original character set matches ASCII characters 0x20 to 0x5F) text on the screen, with NTSC composite video output suitable for display on a TV monitor, or on a regular TV set by way of a separate RF modulator. The original retail price of the computer was US$1298(with 4 KB of RAM) and US$2638 (with the maximum 48 KB of RAM). To reflect the computer's color graphics capability, the Apple logo on the casing was represented using rainbow stripes, which remained a part of Apple's corporate logo until early 1998. The earliest Apple IIs were assembled in Silicon Valley, and later in Texas; printed circuit boards were manufactured in Ireland and Singapore.

An external -inch floppy disk drive, the Disk II, attached via a controller card that plugged into one of the computer's expansion slots (usually slot 6), was used for data storage and retrieval to replace cassettes. The Disk II interface, created by Steve Wozniak, was regarded as an engineering masterpiece for its economy of electronic components.

Rather than having a dedicated sound-synthesis chip, the Apple II had a toggle circuit that could only emit a click through a built-in speaker or a line out jack; all other sounds (including two, three and, eventually, four-voice music and playback of audio samples and speech synthesis) were generated entirely by software that clicked the speaker at just the right times.

The Apple II's multiple expansion slots permitted a wide variety of third-party devices, including Apple II peripheral cards such as serial controllers, display controllers, memory boards, hard disks, networking components, and realtime clocks. There were plug-in expansion cards – such as the Z-80 SoftCard – that permitted the Apple to use the Z80 processor and run a multitude of programs developed under the CP/M operating system, including the dBase II database and the WordStar word processor. There was also a third-party 6809 card that would allow OS-9 Level One to be run. Third-party sound cards greatly improved audio capabilities, allowing simple music synthesis and text-to-speech functions. Eventually, Apple II accelerator cards were created to double or quadruple the computer's speed.

Rod Holt designed the Apple II's power supply. He employed a switched-mode power supply design, which was far smaller and generated less unwanted heat than the linear power supply some other home computers used.

The original Apple II was discontinued at the start of 1981, having been superseded by the Apple II+. By 1984, over six million machines had been sold.

The Apple II Plus, introduced in June 1979, included the Applesoft BASIC programming language in ROM. This Microsoft-authored dialect of BASIC, which was previously available as an upgrade, supported floating-point arithmetic, and became the standard BASIC dialect on the Apple II series (though it ran at a noticeably slower speed than Steve Wozniak's Integer BASIC).

Except for improved graphics and disk-booting support in the ROM, and the removal of the 2k 6502 assembler to make room for the floating point BASIC, the II+ was otherwise identical to the original II in terms of electronic functionality. There were small differences in the physical appearance and keyboard. RAM prices fell during 1980–81 and all II+ machines came from the factory with a full 48k of memory already installed.

After the success of the first Apple II in the United States, Apple expanded its market to include Europe, Australia and the Far East in 1979, with the Apple II Europlus (Europe, Australia) and the Apple II J-Plus (Japan). In these models, Apple made the necessary hardware, software and firmware changes in order to comply to standards outside of the US.

The Apple II Plus was followed in 1983 by the Apple IIe, a cost-reduced yet more powerful machine that used newer chips to reduce the component count and add new features, such as the display of upper and lowercase letters and a standard 64 KB of RAM.

The IIe RAM was configured as if it were a 48 KB Apple II Plus with a language card. The machine had no slot 0, but instead had an auxiliary slot that could accept a 1 KB memory card to enable the 80-column display. This card contained only RAM; the hardware and firmware for the 80-column display was built into the Apple IIe. An "extended 80-column card" with more memory increased the machine's RAM to 128 KB.

The Apple IIe was the most popular machine in the Apple II series. It has the distinction of being the longest-lived Apple computer of all time—it was manufactured and sold with only minor changes for nearly 11 years. The IIe was the last Apple II model to be sold, and was discontinued in November 1993.

During its lifespan two variations were introduced: the Apple IIe Enhanced (four replacement chips to give it some of the features of the later model Apple IIc) and the Apple IIe Platinum (a modernized case color to match other Apple products of the era, along with the addition of a numeric keypad).

Some of the feature of the IIe were carried over from the less successful "Apple III", among them the ProDOS operating system.

The Apple IIc was released in April 1984, billed as a portable Apple II because it could be easily carried due to its size and carrying handle, which could be flipped down to prop the machine up into a typing position. Unlike modern portables, it lacked a built-in display and battery. It was the first of three Apple II models to be made in the Snow White design language, and the only one that used its unique creamy off-white color.

The Apple IIc was the first Apple II to use the 65C02 low-power variant of the 6502 processor, and featured a built-in 5.25-inch floppy drive and 128 KB RAM, with a built-in disk controller that could control external drives, composite video (NTSC or PAL), serial interfaces for modem and printer, and a port usable by either a joystick or mouse. Unlike previous Apple II models, the IIc had no internal expansion slots at all.

Two different monochrome LC displays were sold for use with the IIc's video expansion port, although both were short-lived due to high cost and poor legibility. The IIc had an external power supply that converted AC power to 15 V DC, though the IIc itself will accept between 12 V and 17 V DC, allowing third parties to offer battery packs and automobile power adapters that connected in place of the supplied AC adapter.

The Apple II, released on September 15, 1986, is the penultimate and most advanced model in the Apple II series, and a radical departure from prior models. It uses a 16-bit microprocessor, the 65C816 operating at 2.8 MHz with 24-bit addressing, allowing expansion up to 8 MB of RAM. The graphics are significantly improved, with 4096 colors and new modes with resolutions of 320×200 and 640×400. The audio capabilities are vastly improved, with a built-in music synthesizer that far exceeded any other home computer.

The Apple II evolved the platform while still maintaining near-complete backward compatibility. Its Mega II chip contains the functional equivalent of an entire Apple IIe computer (sans processor). This, combined with the 65816's ability to execute 65C02 code directly, provides full support for legacy software, while also supporting 16-bit software running under a new OS.

The OS eventually included a Macintosh-like graphical Finder for managing disks and files and opening documents and applications, along with desk accessories. Later, the II gained the ability to read and write Macintosh disks and, through third-party software, a multitasking Unix-like shell and TrueType font support.

The GS includes a 32-voice Ensoniq 5503 DOC sample-based sound synthesizer chip with 64 KB dedicated RAM, 256 KB (or later 1.125 MB) of standard RAM, built-in peripheral ports (switchable between IIe-style card slots and IIc-style onboard controllers for disk drives, mouse, RGB video, and serial devices) and, built-in AppleTalk networking.

The final Apple II model was the Apple IIc Plus introduced in 1988. It was the same size and shape as the IIc that came before it, but the 5.25-inch floppy drive had been replaced with a -inch drive, the power supply was moved inside the case, and the processor was a fast 4 MHz 65C02 processor that actually ran 8-bit Apple II software faster than the II.

The IIc Plus also featured a new keyboard layout that matched the Platinum IIe and II. Unlike the IIe IIc and II, the IIc Plus came only in one version (American) and was not officially sold anywhere outside the US. The Apple IIc Plus ceased production in 1990, with its two-year production run being the shortest of all the Apple II computers.

Although not an extension of the Apple II line, in 1990 the Apple IIe Card, an expansion card for the LC line of Macintosh computers, was released. Essentially a miniaturized Apple IIe computer on a card (using the Mega II chip from the Apple II), it allowed the Macintosh to run 8-bit Apple IIe software through hardware emulation, with an option to run at roughly double the speed of the original IIe (about 1.8Mhz). However, the video output was emulated in software, and, depending on how much of the screen the currently running program was trying to update in a single frame, performance could be much slower compared to a real IIe. This is due to the fact that writes from the 65C02 on the IIe Card to video memory were caught by the additional hardware on the card, so the video emulation software running on the Macintosh side could process that write and update the video display. But, while the Macintosh was processing video updates, execution of Apple II code would be temporarily halted.

With a breakout cable which connected to the back of the card, the user could attach up to two UniDisk or Apple 5.25 Drives, up to one UniDisk 3.5 drive, and a DE-9 Apple II joystick. Many of the LC's built-in Macintosh peripherals could also be "borrowed" by the card when in Apple II mode, including extra RAM, the Mac's internal 3.5-inch floppy drives, AppleTalk networking, any ProDOS-formatted hard disk partitions, the serial ports, mouse, and real-time clock. The IIe card could not, however, run software intended for the 16-bit Apple II.

Mike Markkula, a retired Intel marketing manager, provided the early critical funding for Apple Computer. From 1977 to 1981, Apple used the Regis McKenna agency for its advertisements and marketing. In 1981, Chiat-Day acquired Regis McKenna's advertising operations and Apple used Chiat-Day. At Regis McKenna Advertising, the team assigned to launch the Apple II consisted of Rob Janoff, art director, Chip Schafer, copywriter and Bill Kelley, account executive. Janoff came up with the Apple logo with a bite out of it. The design was originally an olive green with matching company logotype all in lower case. Steve Jobs insisted on promoting the color capability of the Apple II by putting rainbow stripes on the Apple logo. In its letterhead and business card implementation, the rounded "a" of the logotype echoed the "bite" in the logo. This logo was developed simultaneously with an advertisement and a brochure; the latter being produced for distribution initially at the first West Coast Computer Faire.

Since the original Apple II, Apple has paid high attention to its quality of packaging, partly because of Steve Jobs' personal preferences and opinions on packaging and final product appearance. All of Apple's packaging for the Apple II series looked similar, featuring much clean white space and showing the Apple rainbow logo prominently. For several years up until the late 1980s, Apple used the Motter Tektura font for packaging, until changing to the Apple Garamond font.

Apple ran the first advertisement for the Apple II, a two-page spread ad titled "Introducing Apple II", in "BYTE" in July 1977. The first brochure, was entitled "Simplicity" and the copy in both the ad and brochure pioneered "demystifying" language intended to make the new idea of a home computer more "personal." The Apple II introduction ad was later run in the September 1977 issue of "Scientific American".

Apple later aired eight television commercials for the Apple II, emphasizing its benefits to education and students, along with some print ads.

The Apple II was frequently cloned, both in the United States and abroad, in a similar way to the IBM PC. According to some sources (see below), more than 190 different models of Apple II clones were manufactured. Most could not be legally imported into the United States. Apple sued and sought criminal charges against clone makers in more than a dozen countries.

Originally the Apple II used Compact Cassette tapes for program and data storage. A dedicated tape recorder along the lines of the Commodore Datasette was never produced; Apple recommended using the Panasonic RQ309 in some of its early printed documentation. The uses of common consumer cassette recorders and a standard video monitor or television set (with a third party R-F modulator) made the total cost of owning an Apple II less expensive and helped contribute to the Apple II's success.

Cassette storage may have been inexpensive, but it was also slow and unreliable. The Apple II's lack of a disk drive was "a glaring weakness" in what was otherwise intended to be a polished, professional product. Recognizing that the II needed a disk drive to be taken seriously, Apple set out to develop a disk drive and a DOS to run it. Wozniak spent the 1977 Christmas holidays designing a disk controller that reduced the number of chips used by a factor of 10 compared to existing controllers. Still lacking a DOS, and with Wozniak inexperienced in operating system design, Jobs approached Shepardson Microsystems with the project. On April 10, 1978, Apple signed a contract for $13,000 with Sheperdson to develop the DOS.

Even after disk drives made the cassette tape interfaces obsolete they were still used by enthusiasts as simple one-bit audio input-output ports. Ham radio operators used the cassette input to receive slow scan TV (single frame images). A commercial speech recognition Blackjack program was available, after some user-specific voice training it would recognize simple commands (Hit, stand). Bob Bishop's "Music Kaleidoscope" was a simple program that monitored the cassette input port and based on zero-crossings created color patterns on the screen, a predecessor to current audio visualization plug-ins for media players. Music Kaleidoscope was especially popular on projection TV sets in dance halls.

Apple and many third-party developers made software available on tape at first, but after the Disk II became available in 1978, tape-based Apple II software essentially disappeared from the market. The initial price of the Disk II drive and controller was US$595, although a $100 off coupon was available through the Apple newsletter "Contact". The controller could handle two drives and a second drive (without controller) retailed for $495.

The Disk II single-sided floppy drive used 5.25-inch floppy disks; double-sided disks could be used, one side at a time, by turning them over and notching a hole for the write protect sensor. The first disk operating systems for the were and DOS 3.2, which stored 113.75 KB on each disk, organized into 35 tracks of 13 256-byte sectors each. After about two years, DOS 3.3 was introduced, storing 140 KB thanks to a minor firmware change on the disk controller that allowed it to store 16 sectors per track. (This upgrade was user-installable as two PROMs on older controllers.) After the release of DOS 3.3, the user community discontinued use of except for running legacy software. Programs that required DOS 3.2 were fairly rare; however, as DOS 3.3 was not a major architectural change aside from the number of sectors per track, a program called MUFFIN was provided with DOS 3.3 to allow users to copy files from DOS 3.2 disks to DOS 3.3 disks. It was possible for software developers to create a DOS 3.2 disk which would also boot on a system with firmware.

Later, double-sided drives, with heads to read both sides of the disk, became available from third-party companies. (Apple only produced double-sided 5.25-inch disks for the Lisa 1 computer).

On a DOS 3.x disk, tracks 0, 1, and most of track 2 were reserved to store the operating system. (It was possible, with a special utility, to reclaim most of this space for data if a disk did not need to be bootable.) A short ROM program on the disk controller had the ability to seek to track zero which it did without regard for the read/write head's current position, resulting in the characteristic "chattering" sound of a Disk II boot, which was the read/write head hitting the rubber stop block at the end of the rail – and read and execute code from sector 0. The code contained in there would then pull in the rest of the operating system. DOS stored the disk's directory on track 17, smack in the middle of the 35-track disks, in order to reduce the average seek time to the frequently used directory track. The directory was fixed in size and could hold a maximum of 105 files. Subdirectories were not supported.

Most game publishers did not include DOS on their floppy disks, since they needed the memory it occupied more than its capabilities; instead, they often wrote their own boot loaders and read-only file systems. This also served to discourage "crackers" from snooping around in the game's copy-protection code, since the data on the disk was not in files that could be accessed easily.

Some third-party manufacturers produced floppy drives that could write 40 tracks to most 5.25-inch disks, yielding 160 KB of storage per disk, but the format did not catch on widely, and no known commercial software was published on 40-track media. Most drives, even Disk IIs, could write 36 tracks; a two byte modification to DOS to format the extra track was common.

The Apple Disk II stored 140 KB on single-sided, "single-density" floppy disks, but it was very common for Apple II users to extend the capacity of a single-sided floppy disk to 280 KB by cutting out a second write-protect notch on the side of the disk using a "disk notcher" or hole puncher and inserting the disk flipped over. Double-sided disks, with notches on both sides, were available at a higher price, but in practice the magnetic coating on the reverse of nominally single-sided disks was usually of good enough quality to be used (both sides were coated in the same way to prevent warping, although only one side was certified for use). Early on, diskette manufacturers routinely warned that this technique would damage the read/write head of the drives or wear out the disk faster, and these warnings were frequently repeated in magazines of the day. In practice, however, this method was an inexpensive way to store twice as much data for no extra cost, and was widely used for commercially released floppies as well.

Later, Apple IIs were able to use 3.5-inch disks with a total capacity of 800 KB and hard disks. did not support these drives natively; third-party software was required, and disks larger than about 400 KB had to be split up into multiple "virtual disk volumes."

DOS 3.3 was succeeded by ProDOS, a 1983 descendant of the Apple ///'s SOS. It added support for subdirectories and volumes up to 32 MB in size. ProDOS became the DOS of choice; AppleWorks and other newer programs required it.

The Apple II series of computers had an enormous impact on the technology industry and expanded the role of microcomputers in society. The Apple II was the first personal computer many people ever saw. Its price was within the reach of many middle-class families, and a partnership with MECC helped make the Apple II popular in schools. By the end of 1980 Apple had already sold over 100,000 Apple IIs. Its popularity bootstrapped the computer game and educational software markets and began the boom in the word processor and computer printer markets. The first spreadsheet application, VisiCalc, was initially released for the Apple II, and many businesses bought them just to run VisiCalc. Its success drove IBM in part to create the IBM PC, which many businesses purchased to run spreadsheet and word processing software, at first ported from Apple II versions.

The Apple II's slots, allowing any peripheral card to take control of the bus and directly access memory, enabled an independent industry of card manufacturers who together created a flood of hardware products that let users build systems that were far more powerful and useful (at a lower cost) than any competing system, most of which were not nearly as expandable and were universally proprietary. The first peripheral card was a blank prototyping card intended for electronics enthusiasts who wanted to design their own peripherals for the Apple II.

Specialty peripherals kept the Apple II in use in industry and education environments for many years after Apple Computer stopped supporting the Apple II. Well into the 1990s every clean-room (the super-clean facility where spacecraft are prepared for flight) at the Kennedy Space Center used an Apple II to monitor the environment and air quality. Most planetariums used Apple IIs to control their projectors and other equipment.

Even the game port was unusually powerful and could be used for digital and analog input and output. The early manuals included instructions for how to build a circuit with only four commonly available components (one transistor and three resistors) and a software routine to drive a common Teletype Model 33 machine. Don Lancaster used the game I/O to drive a LaserWriter printer.

Today, emulators for various Apple II models are available to run Apple II software on macOS, Linux, Microsoft Windows, homebrew enabled Nintendo DS and other operating systems. Numerous disk images of Apple II software are available free over the Internet for use with these emulators. AppleWin and MESS are among the best emulators compatible with most Apple II images. The MESS emulator supports recording and playing back of Apple II emulation sessions, as does Home Action Replay Page (a.k.a. HARP).

In addition, an active retrocomputing community of vintage Apple II collectors and users, continue to restore, maintain and develop hardware and software for daily use of these original computers. There is still a small annual convention, KansasFest, dedicated to the platform.

In 2017, the band 8 Bit Weapon released the world's first 100% Apple II based music album entitled, "Class Apples." The album featured dance-oriented cover versions of classical music by Bach, Beethoven, and Mozart recorded directly off the Apple II motherboard.



Apple III

The Apple III (styled as apple ///) is a business-oriented personal computer produced by Apple Computer and released in 1980. Running the Apple SOS operating system, it was intended as the successor to the Apple II series, but was largely considered a failure in the market. It was designed to provide key features business users wanted in a personal computer: a true typewriter-style upper/lowercase keyboard (the Apple II only supported uppercase) and an 80-column display.

Work on the Apple III started in late 1978 under the guidance of Dr. Wendell Sander. It had the internal code name of "Sara", named after Sander's daughter. The system was announced on May 19, 1980 and released in late November that year. Serious stability issues required a design overhaul and a recall of the first 14,000 machines produced. The Apple III was formally reintroduced on November 9, 1981.

Damage to the computer's reputation had already been done, however, and it failed to do well commercially. Development stopped, and the Apple III was discontinued on April 24, 1984. Its last successor, the III Plus, was dropped from the Apple product line in September 1985.

An estimated 65,000–75,000 Apple III computers were sold. The Apple III Plus brought this up to approximately 120,000. Apple co-founder Steve Wozniak stated that the primary reason for the Apple III's failure was that the system was designed by Apple's marketing department, unlike Apple's previous engineering-driven projects. The Apple III's failure led Apple to reevaluate its plan to phase out the Apple II, prompting the eventual continuation of development of the older machine. As a result, later Apple II models incorporated some hardware and software technologies of the Apple III.

Steve Wozniak and Steve Jobs expected hobbyists to purchase the Apple II, but because of VisiCalc and Disk II, small businesses purchased 90% of the computers. The Apple III was designed to be a business computer and successor. Though the Apple II contributed to the inspirations of several important business products, such as VisiCalc, Multiplan, and Apple Writer, the computer's hardware architecture, operating system, and developer environment are limited. Apple management intended to clearly establish market segmentation by designing the Apple III to appeal to the 90% business market, leaving the Apple II to home and education users. Management believed that "once the Apple III was out, the Apple II would stop selling in six months", Wozniak said.

The Apple III is powered by a 1.8-megahertz Synertek 6502A or 6502B 8-bit CPU and, like some of the later machines in the Apple II family, uses bank switching techniques to address memory beyond the 6502's traditional 64 KB limit, up to 256 KB in the III's case. Third-party vendors produced memory upgrade kits that allow the Apple III to reach up to 512 KB of random-access memory (RAM). Other Apple III built-in features include an 80-column, 24-line display with upper and lowercase characters, a numeric keypad, dual-speed (pressure-sensitive) cursor control keys, 6-bit (DAC) audio, and a built-in 140-kilobyte 5.25-inch floppy disk drive. Graphics modes include 560x192 in black and white, and 280x192 with 16 colors or shades of gray. Unlike the Apple II, the Disk III controller is part of the logic board.

The Apple III is the first Apple product to allow the user to choose both a screen font and a keyboard layout: either QWERTY or Dvorak. These choices cannot be changed while programs were running, unlike the Apple IIc, which has a keyboard switch directly above the keyboard, allowing the user to switch on the fly.

The Apple III introduced an advanced operating system called Apple SOS, pronounced "apple sauce". Its ability to address resources by name allows the Apple III to be more scalable than the Apple II's addressing by physical location such as codice_1. Apple SOS allows the full capacity of a storage device to be used as a single volume, such as the Apple ProFile hard disk drive, and it supports a hierarchical file system. Some of the features and code base of Apple SOS were later adopted into the Apple II's ProDOS and GS/OS operating systems, as well as Lisa 7/7 and Mac OS.

With a starting price between , the Apple III was more expensive than many of the CP/M-based business computers that were available at the time. Few software applications other than VisiCalc are available for the computer; according to a presentation at KansasFest 2012, fewer than 50 Apple III-specific software packages were ever published, most shipping when the III Plus was released. Because Apple did not view the Apple III as suitable for hobbyists, it did not provide much of the technical software information that accompanies the Apple II. Originally intended as a direct replacement to the Apple II series, it was designed to be backward compatible with Apple II software. However, since Apple did not want to encourage continued development of the II platform, Apple II compatibility exists only in a special Apple II Mode which is limited in its capabilities to the emulation of a basic Apple II Plus configuration with of RAM. Special chips were intentionally added to prevent access from Apple II Mode to the III's advanced features such as its larger amount of memory.

The Apple III has four expansion slots, a number that "inCider" in 1986 called "miserly". Apple II cards are compatible but risk violating government RFI regulations, and require Apple III-specific device drivers; "BYTE" stated that "Apple provides virtually no information on how to write them". As with software, Apple provided little hardware technical information with the computer but Apple III-specific products became available, such as one that made the computer compatible with the Apple IIe. Several new Apple-produced peripherals were developed for the Apple III. The original Apple III has a built-in real-time clock, which is recognized by Apple SOS. The clock was later removed from the "revised" model, and was instead made available as an add-on.

Along with the built-in floppy drive, the Apple III can also handle up to three additional external Disk III floppy disk drives. The Disk III is only officially compatible with the Apple III. The Apple III Plus requires an adaptor from Apple to use the Disk III with its DB-25 disk port.

With the introduction of the revised Apple III a year after launch, Apple began offering the ProFile external hard disk system. Priced at $3,499 for 5 MB of storage, it also required a peripheral slot for its controller card.

The Apple III has the built-in hardware capability to run Apple II software. In order to do so, an emulation boot disk is required that functionally turns the machine into a standard 48-kilobyte Apple II Plus, until it is powered off. The keyboard, internal floppy drive (and one external Disk III), display (color is provided through the 'B/W video' port) and speaker all act as Apple II peripherals. The paddle and serial ports can also function in Apple II mode, however with some limitations and compatibility issues.

Apple engineers added specialized circuitry with the sole purpose of blocking access to its advanced features when running in Apple II emulation mode. This was done primarily to discourage further development and interest in the Apple II line, and to push the Apple III as its successor. For example, no more than of RAM can be accessed, even if the machine has of RAM or higher present. Many Apple II programs require a minimum of of RAM, making them impossible to run on the Apple III. Similarly, access to lowercase support, 80 columns text, or its more advanced graphics and sound are blocked by this hardware circuitry, making it impossible for even skilled software programmers to bypass Apple's lockout. A third-party company, Titan Technologies, sold an expansion board called the III Plus II that allows Apple II mode to access more memory, a standard game port, and with a later released companion card, even emulate the Apple IIe.

Certain Apple II slot cards can be installed in the Apple III and used in native III-mode with custom written SOS device drivers, including Grappler Plus and Liron 3.5 Controller.

After overheating issues were attributed to serious design flaws, a redesigned logic board was introduced in mid-December 1981 – which included a lower power supply requirement, wider circuit traces and better-designed chip sockets. The $3,495 revised model also includes 256 KB of RAM as the standard configuration. The 14,000 units of the original Apple III sold were returned and replaced with the entirely new revised model.

Apple discontinued the III in October 1983 because it violated FCC regulations, and the FCC required the company to change the redesigned computer's name. It introduced the Apple III Plus in December 1983 at a price of US$2,995. This newer version includes a built-in clock, video interlacing, standardized rear port connectors, 55-watt power supply, 256 KB of RAM as standard, and a redesigned, Apple IIe-like keyboard.

Owners of the Apple III could purchase individual III Plus upgrades, like the clock and interlacing feature, and obtain the newer logic board as a service replacement. A keyboard upgrade kit, dubbed "Apple III Plus upgrade kit" was also made available – which included the keyboard, cover, keyboard encoder ROM, and logo replacements. This upgrade had to be installed by an authorized service technician.

According to Wozniak, the Apple III "had 100 percent hardware failures". Former Apple executive Taylor Pohlman stated that:

Jobs insisted on the idea of having no fan or air vents, in order to make the computer run quietly. He would later push this same ideology onto almost all Apple models he had control of, from the Apple Lisa and Macintosh 128K to the iMac. To allow the computer to dissipate heat, the base of the Apple III was made of heavy cast aluminum, which supposedly acts as a heat sink. One advantage to the aluminum case was a reduction in RFI (Radio Frequency Interference), a problem which had plagued the Apple II series throughout its history. Unlike the Apple II series, the power supply was mounted – without its own shell – in a compartment separate from the logic board. The decision to use an aluminum shell ultimately led to engineering issues which resulted in the Apple III's reliability problems. The lead time for manufacturing the shells was high, and this had to be done before the motherboard was finalized. Later, it was realized that there was not enough room on the motherboard for all of the components unless narrow traces were used.

Many Apple IIIs were thought to have failed due to their inability to properly dissipate heat. "inCider" stated in 1986 that "Heat has always been a formidable enemy of the Apple ///", and some users reported that their Apple IIIs became so hot that the chips started dislodging from the board, causing the screen to display garbled data or their disk to come out of the slot "melted". "BYTE" wrote, "the integrated circuits tended to wander out of their sockets". It has been rumored Apple advised customers to tilt the front of the Apple III six inches above the desk and then drop it to reseat the chips as a temporary solution. Other analyses blame a faulty automatic chip insertion process, not heat.

Case designer Jerry Manock denied the design flaw charges, insisting that tests proved that the unit adequately dissipated the internal heat. The primary cause, he claimed, was a major logic board design problem. The logic board used "fineline" technology that was not fully mature at the time, with narrow, closely spaced traces. When chips were "stuffed" into the board and wave-soldered, solder bridges would form between traces that were not supposed to be connected. This caused numerous short circuits, which required hours of costly diagnosis and hand rework to fix. Apple designed a new circuit board with more layers and normal-width traces. The new logic board was laid out by one designer on a huge drafting board, rather than using the costly CAD-CAM system used for the previous board, and the new design worked.

Earlier Apple III units came with a built-in real time clock. The hardware, however, would fail after prolonged use. Assuming that National Semiconductor would test all parts before shipping them, Apple did not perform this level of testing. Apple was soldering chips directly to boards and could not easily replace a bad chip if one was found. Eventually, Apple solved this problem by removing the real-time clock from the Apple III's specification rather than shipping the Apple III with the clock pre-installed, and then sold the peripheral as a level 1 technician add-on.

Microsoft and Apple each developed their own versions of BASIC for the Apple III. Apple III Microsoft BASIC was designed to run on the CP/M platform available for the Apple III. Apple Business BASIC shipped with the Apple III. Donn Denman ported Applesoft BASIC to SOS and reworked it to take advantage of the extended memory of the Apple III.

Both languages introduced a number of new or improved features over Applesoft BASIC. Both languages replaced Applesoft's single-precision floating-point variables using 5-byte storage with the somewhat-reduced-precision 4-byte variables, while also adding a larger numerical format. Apple III Microsoft BASIC provides double-precision floating-point variables, taking 8 bytes of storage, while Apple Business BASIC offers an extra-long integer type, also taking 8 bytes for storage. Both languages also retain 2-byte integers, and maximum 255-character strings.

Other new features common to both languages include:

Some features work differently in each language:

There is no support for graphics provided within the language, nor for reading analog controls or buttons; nor is there a means of defining the active window of the text screen.

Apple Business BASIC eliminates all references to absolute memory addresses. Thus, the POKE command and PEEK() function were not included in the language, and new features replaced the CALL statement and USR() function. The functionality of certain features in Applesoft that had been achieved with various PEEK and POKE locations is now provided by:

External binary subroutines and functions are loaded into memory by a single INVOKE disk-command that loads separately-assembled code modules. A PERFORM statement is then used to call an INVOKEd procedure by name, with an argument-list. INVOKEd functions would be referenced in expressions by EXFN. (floating-point) or EXFN%. (integer), with the function name appended, plus the argument-list for the function.

Graphics are supported with an INVOKEd module, with features including displaying text within graphics in various fonts, within four different graphics modes available on the Apple III.

Despite devoting the majority of its R&D to the Apple III and so ignoring the II that for a while dealers had difficulty in obtaining the latter, the III's technical problems made marketing the computer difficult. Ed Smith, who after designing the APF Imagination Machine worked as a distributor's representative, described the III as "a complete disaster". He recalled that he "was responsible for going to every dealership, setting up the Apple III in their showroom, and then explaining to them the functions of the Apple III, which in many cases didn't really work".

Pohlman reported that Apple was only selling 500 units a month by late 1981, mostly as replacements. The company was able to eventually raise monthly sales to 5,000, but the IBM PC's successful launch had encouraged software companies to develop for it instead, prompting Apple to shift focus to the Lisa and Macintosh. The PC almost ended sales of the Apple III, the most closely comparable Apple computer model. By early 1984, sales were primarily to existing III owners, Apple itself—its 4,500 employees were equipped with some 3,000-4,500 units—and some small businesses. Apple finally discontinued the Apple III series on April 24, 1984, four months after introducing the III Plus, after selling only 65,000-75,000 units and replacing 14,000 defective units.

Jobs said that the company lost "infinite, incalculable amounts" of money on the Apple III. Wozniak estimated that Apple had spent $100 million on the III, instead of improving the II and better competing against IBM. Pohlman claimed that there was a "stigma" at Apple associated with having contributed to the computer. Most employees who worked on the III reportedly left Apple.

The file system and some design ideas from Apple SOS, the Apple III's operating system, were part of Apple ProDOS and Apple GS/OS, the major operating systems for the Apple II series following the demise of the Apple III, as well as the Apple Lisa, which was the de facto business-oriented successor to the Apple III. The hierarchical file system influenced the evolution of the Macintosh: while the original Macintosh File System (MFS) was a flat file system designed for a floppy disk without subdirectories, subsequent file systems were hierarchical. By comparison, the IBM PC's first file system (again designed for floppy disks) was also flat and later versions (designed for hard disks) were hierarchical.

At the start of the Walt Disney Pictures film "TRON", lead character Kevin Flynn (played by Jeff Bridges) is seen hacking into the ENCOM mainframe using an Apple III.



AVL tree

In computer science, an AVL tree (named after inventors Adelson-Velsky and Landis) is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Lookup, insertion, and deletion all take time in both the average and worst cases, where formula_1 is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more tree rotations.

The AVL tree is named after its two Soviet inventors, Georgy Adelson-Velsky and Evgenii Landis, who published it in their 1962 paper "An algorithm for the organization of information". It is the oldest self-balancing binary search tree data structure to be invented. 

AVL trees are often compared with red–black trees because both support the same set of operations and take formula_2 time for the basic operations. For lookup-intensive applications, AVL trees are faster than red–black trees because they are more strictly balanced. Similar to red–black trees, AVL trees are height-balanced. Both are, in general, neither weight-balanced nor formula_3-balanced for any formula_4;

AVL trees are more rigidly balanced than RB trees with an asymptotic relation AVL/RB ≈0.720 of the maximal heights. For insertions and deletions, Ben Pfaff shows in 79 measurements a relation of AVL/RB between 0.677 and 1.077 with median ≈0.947 and geometric mean ≈0.910.




Aliphatic compound

In organic chemistry, hydrocarbons (compounds composed solely of carbon and hydrogen) are divided into two classes: aromatic compounds and aliphatic compounds (; G. "aleiphar", fat, oil). Aliphatic compounds can be saturated (in which all the C-C bonds are single requiring the structure to be completed, or 'saturated', by hydrogen) like hexane, or unsaturated, like hexene and hexyne. Open-chain compounds, whether straight or branched, and which contain no rings of any type, are always aliphatic. Cyclic compounds can be aliphatic if they are not aromatic.

Aliphatic compounds can be saturated, joined by single bonds (alkanes), or unsaturated, with double bonds (alkenes) or triple bonds (alkynes). If other elements (heteroatoms) are bound to the carbon chain, the most common being oxygen, nitrogen, sulfur, and chlorine, it is no longer a hydrocarbon, and therefore no longer an aliphatic compound. However, such compounds may still be referred to as aliphatic if the hydrocarbon portion of the molecule is aliphatic, e.g. aliphatic amines, to differentiate them from aromatic amines.

The least complex aliphatic compound is methane (CH).

Most aliphatic compounds are flammable, allowing the use of hydrocarbons as fuel, such as methane in natural gas for stoves or heating; butane in torches and lighters; various aliphatic (as well as aromatic) hydrocarbons in liquid transportation fuels like petrol/gasoline, diesel, and jet fuel; and other uses such as ethyne (acetylene) in welding.

The most important aliphatic compounds are:


Important examples of low-molecular aliphatic compounds can be found in the list below (sorted by the number of carbon-atoms):

Astrology

Astrology is a range of divinatory practices, recognized as pseudoscientific since the 18th century, that claim to discern information about human affairs and terrestrial events by studying the apparent positions of celestial objects. Different cultures have employed forms of astrology since at least the 2nd millennium BCE, these practices having originated in calendrical systems used to predict seasonal shifts and to interpret celestial cycles as signs of divine communications. Most, if not all, cultures have attached importance to what they observed in the sky, and some—such as the Hindus, Chinese, and the Maya—developed elaborate systems for predicting terrestrial events from celestial observations. Western astrology, one of the oldest astrological systems still in use, can trace its roots to 19th–17th century BCE Mesopotamia, from where it spread to Ancient Greece, Rome, the Islamic world, and eventually Central and Western Europe. Contemporary Western astrology is often associated with systems of horoscopes that purport to explain aspects of a person's personality and predict significant events in their lives based on the positions of celestial objects; the majority of professional astrologers rely on such systems.

Throughout its history, astrology has had its detractors, competitors and skeptics who opposed it for moral, religious, political, and empirical reasons. Nonetheless, prior to the Enlightenment, astrology was generally considered a scholarly tradition and was common in learned circles, often in close relation with astronomy, meteorology, medicine, and alchemy. It was present in political circles and is mentioned in various works of literature, from Dante Alighieri and Geoffrey Chaucer to William Shakespeare, Lope de Vega, and Calderón de la Barca. During the Enlightenment, however, astrology lost its status as an area of legitimate scholarly pursuit. Following the end of the 19th century and the wide-scale adoption of the scientific method, researchers have successfully challenged astrology on both theoretical and experimental grounds, and have shown it to have no scientific validity or explanatory power. Astrology thus lost its academic and theoretical standing in the western world, and common belief in it largely declined, until a continuing resurgence starting in the 1960s.

The word "astrology" comes from the early Latin word "astrologia", which derives from the Greek —from ἄστρον "astron" ("star") and -λογία "-logia", ("study of"—"account of the stars"). The word entered the English language via Latin and medieval French, and its use overlapped considerably with that of "astronomy" (derived from the Latin "astronomia"). By the 17th century, "astronomy" became established as the scientific term, with "astrology" referring to divinations and schemes for predicting human affairs.

Many cultures have attached importance to astronomical events, and the Indians, Chinese, and Maya developed elaborate systems for predicting terrestrial events from celestial observations. A form of astrology was practised in the Old Babylonian period of Mesopotamia, . "Vedāṅga Jyotiṣa" is one of earliest known Hindu texts on astronomy and astrology ("Jyotisha"). The text is dated between 1400 BCE to final centuries BCE by various scholars according to astronomical and linguistic evidences. Chinese astrology was elaborated in the Zhou dynasty (1046–256 BCE). Hellenistic astrology after 332 BCE mixed Babylonian astrology with Egyptian Decanic astrology in Alexandria, creating horoscopic astrology. Alexander the Great's conquest of Asia allowed astrology to spread to Ancient Greece and Rome. In Rome, astrology was associated with "Chaldean wisdom". After the conquest of Alexandria in the 7th century, astrology was taken up by Islamic scholars, and Hellenistic texts were translated into Arabic and Persian. In the 12th century, Arabic texts were imported to Europe and translated into Latin. Major astronomers including Tycho Brahe, Johannes Kepler and Galileo practised as court astrologers. Astrological references appear in literature in the works of poets such as Dante Alighieri and Geoffrey Chaucer, and of playwrights such as Christopher Marlowe and William Shakespeare.

Throughout most of its history, astrology was considered a scholarly tradition. It was accepted in political and academic contexts, and was connected with other studies, such as astronomy, alchemy, meteorology, and medicine. At the end of the 17th century, new scientific concepts in astronomy and physics (such as heliocentrism and Newtonian mechanics) called astrology into question. Astrology thus lost its academic and theoretical standing, and common belief in astrology has largely declined.

Astrology, in its broadest sense, is the search for meaning in the sky. Early evidence for humans making conscious attempts to measure, record, and predict seasonal changes by reference to astronomical cycles, appears as markings on bones and cave walls, which show that lunar cycles were being noted as early as 25,000 years ago. This was a first step towards recording the Moon's influence upon tides and rivers, and towards organising a communal calendar. Farmers addressed agricultural needs with increasing knowledge of the constellations that appear in the different seasons—and used the rising of particular star-groups to herald annual floods or seasonal activities. By the 3rd millennium BCE, civilisations had sophisticated awareness of celestial cycles, and may have oriented temples in alignment with heliacal risings of the stars.

Scattered evidence suggests that the oldest known astrological references are copies of texts made in the ancient world. The Venus tablet of Ammisaduqa is thought to have been compiled in Babylon around 1700 BCE. A scroll documenting an early use of electional astrology is doubtfully ascribed to the reign of the Sumerian ruler Gudea of Lagash ( – 2124 BCE). This describes how the gods revealed to him in a dream the constellations that would be most favourable for the planned construction of a temple. However, there is controversy about whether these were genuinely recorded at the time or merely ascribed to ancient rulers by posterity. The oldest undisputed evidence of the use of astrology as an integrated system of knowledge is therefore attributed to the records of the first dynasty of Mesopotamia (1950–1651 BCE). This astrology had some parallels with Hellenistic Greek (western) astrology, including the zodiac, a norming point near 9 degrees in Aries, the trine aspect, planetary exaltations, and the dodekatemoria (the twelve divisions of 30 degrees each). The Babylonians viewed celestial events as possible signs rather than as causes of physical events.

The system of Chinese astrology was elaborated during the Zhou dynasty (1046–256 BCE) and flourished during the Han dynasty (2nd century BCE to 2nd century CE), during which all the familiar elements of traditional Chinese culture – the Yin-Yang philosophy, theory of the five elements, Heaven and Earth, Confucian morality – were brought together to formalise the philosophical principles of Chinese medicine and divination, astrology, and alchemy.

The ancient Arabs that inhabited the Arabian Peninsula before the advent of Islam used to profess a widespread belief in fatalism ("ḳadar") alongside a fearful consideration for the sky and the stars, which they held to be ultimately responsible for every phenomena that occurs on Earth and for the destiny of humankind. Accordingly, they shaped their entire lives in accordance with their interpretations of astral configurations and phenomena.

The Hellenistic schools of philosophical skepticism criticized the rationality of astrology. Criticism of astrology by academic skeptics such as Cicero, Carneades, and Favorinus; and Pyrrhonists such as Sextus Empiricus has been preserved.

Carneades argued that belief in fate denies free will and morality; that people born at different times can all die in the same accident or battle; and that contrary to uniform influences from the stars, tribes and cultures are all different.

Cicero, in "De Divinatione", leveled a critique of astrology that some modern philosophers consider to be the first working definition of pseudoscience and the answer to the demarcation problem. Philosopher of Science Massimo Pigliucci, building on the work of Historian of Science, Damien Fernandez-Beanato, argues that Cicero outlined a "convincing distinction between astrology and astronomy that remains valid in the twenty-first century." Cicero stated the twins objection (that with close birth times, personal outcomes can be very different), later developed by Augustine. He argued that since the other planets are much more distant from the Earth than the Moon, they could have only very tiny influence compared to the Moon's. He also argued that if astrology explains everything about a person's fate, then it wrongly ignores the visible effect of inherited ability and parenting, changes in health worked by medicine, or the effects of the weather on people.

Favorinus argued that it was absurd to imagine that stars and planets would affect human bodies in the same way as they affect the tides, and equally absurd that small motions in the heavens cause large changes in people's fates.

Sextus Empiricus argued that it was absurd to link human attributes with myths about the signs of the zodiac, and wrote an entire book, "Against the Astrologers" (Πρὸς ἀστρολόγους, "Pros astrologous"), compiling arguments against astrology. "Against the Astrologers" was the fifth section of a larger work arguing against philosophical and scientific inquiry in general, "Against the Professors" (Πρὸς μαθηματικούς, "Pros mathematikous").

Plotinus, a neoplatonist, argued that since the fixed stars are much more distant than the planets, it is laughable to imagine the planets' effect on human affairs should depend on their position with respect to the zodiac. He also argues that the interpretation of the Moon's conjunction with a planet as good when the moon is full, but bad when the moon is waning, is clearly wrong, as from the Moon's point of view, half of its surface is always in sunlight; and from the planet's point of view, waning should be better, as then the planet sees some light from the Moon, but when the Moon is full to us, it is dark, and therefore bad, on the side facing the planet in question.

In 525 BCE, Egypt was conquered by the Persians. The 1st century BCE Egyptian Dendera Zodiac shares two signs – the Balance and the Scorpion – with Mesopotamian astrology.

With the occupation by Alexander the Great in 332 BCE, Egypt became Hellenistic. The city of Alexandria was founded by Alexander after the conquest, becoming the place where Babylonian astrology was mixed with Egyptian Decanic astrology to create Horoscopic astrology. This contained the Babylonian zodiac with its system of planetary exaltations, the triplicities of the signs and the importance of eclipses. It used the Egyptian concept of dividing the zodiac into thirty-six decans of ten degrees each, with an emphasis on the rising decan, and the Greek system of planetary Gods, sign rulership and four elements. 2nd century BCE texts predict positions of planets in zodiac signs at the time of the rising of certain decans, particularly Sothis. The astrologer and astronomer Ptolemy lived in Alexandria. Ptolemy's work the "Tetrabiblos" formed the basis of Western astrology, and, "...enjoyed almost the authority of a Bible among the astrological writers of a thousand years or more."

The conquest of Asia by Alexander the Great exposed the Greeks to ideas from Syria, Babylon, Persia and central Asia. Around 280 BCE, Berossus, a priest of Bel from Babylon, moved to the Greek island of Kos, teaching astrology and Babylonian culture. By the 1st century BCE, there were two varieties of astrology, one using horoscopes to describe the past, present and future; the other, theurgic, emphasising the soul's ascent to the stars. Greek influence played a crucial role in the transmission of astrological theory to Rome.

The first definite reference to astrology in Rome comes from the orator Cato, who in 160 BCE warned farm overseers against consulting with Chaldeans, who were described as Babylonian 'star-gazers'. Among both Greeks and Romans, Babylonia (also known as Chaldea) became so identified with astrology that 'Chaldean wisdom' became synonymous with divination using planets and stars. The 2nd-century Roman poet and satirist Juvenal complains about the pervasive influence of Chaldeans, saying, "Still more trusted are the Chaldaeans; every word uttered by the astrologer they will believe has come from Hammon's fountain."

One of the first astrologers to bring Hermetic astrology to Rome was Thrasyllus, astrologer to the emperor Tiberius, the first emperor to have had a court astrologer, though his predecessor Augustus had used astrology to help legitimise his Imperial rights.

The main texts upon which classical Indian astrology is based are early medieval compilations, notably the "", and "Sārāvalī" by .
The "Horāshastra" is a composite work of 71 chapters, of which the first part (chapters 1–51) dates to the 7th to early 8th centuries and the second part (chapters 52–71) to the later 8th century. The "Sārāvalī" likewise dates to around 800 CE. English translations of these texts were published by N.N. Krishna Rau and V.B. Choudhari in 1963 and 1961, respectively.

Astrology was taken up by Islamic scholars following the collapse of Alexandria to the Arabs in the 7th century, and the founding of the Abbasid empire in the 8th. The second Abbasid caliph, Al Mansur (754–775) founded the city of Baghdad to act as a centre of learning, and included in its design a library-translation centre known as "Bayt al-Hikma" 'House of Wisdom', which continued to receive development from his heirs and was to provide a major impetus for Arabic-Persian translations of Hellenistic astrological texts. The early translators included Mashallah, who helped to elect the time for the foundation of Baghdad, and Sahl ibn Bishr, ("a.k.a." "Zael"), whose texts were directly influential upon later European astrologers such as Guido Bonatti in the 13th century, and William Lilly in the 17th century. Knowledge of Arabic texts started to become imported into Europe during the Latin translations of the 12th century.

In the seventh century, Isidore of Seville argued in his "Etymologiae" that astronomy described the movements of the heavens, while astrology had two parts: one was scientific, describing the movements of the Sun, the Moon and the stars, while the other, making predictions, was theologically erroneous.

The first astrological book published in Europe was the "Liber Planetis et Mundi Climatibus" ("Book of the Planets and Regions of the World"), which appeared between 1010 and 1027 AD, and may have been authored by Gerbert of Aurillac. Ptolemy's second century AD "Tetrabiblos" was translated into Latin by Plato of Tivoli in 1138. The Dominican theologian Thomas Aquinas followed Aristotle in proposing that the stars ruled the imperfect 'sublunary' body, while attempting to reconcile astrology with Christianity by stating that God ruled the soul. The thirteenth century mathematician Campanus of Novara is said to have devised a system of astrological houses that divides the prime vertical into 'houses' of equal 30° arcs, though the system was used earlier in the East. The thirteenth century astronomer Guido Bonatti wrote a textbook, the "Liber Astronomicus", a copy of which King Henry VII of England owned at the end of the fifteenth century.

In "Paradiso", the final part of the "Divine Comedy", the Italian poet Dante Alighieri referred "in countless details" to the astrological planets, though he adapted traditional astrology to suit his Christian viewpoint, for example using astrological thinking in his prophecies of the reform of Christendom.

John Gower in the fourteenth century defined astrology as essentially limited to the making of predictions. The influence of the stars was in turn divided into natural astrology, with for example effects on tides and the growth of plants, and judicial astrology, with supposedly predictable effects on people. The fourteenth-century sceptic Nicole Oresme however included astronomy as a part of astrology in his "Livre de divinacions". Oresme argued that current approaches to prediction of events such as plagues, wars, and weather were inappropriate, but that such prediction was a valid field of inquiry. However, he attacked the use of astrology to choose the timing of actions (so-called interrogation and election) as wholly false, and rejected the determination of human action by the stars on grounds of free will. The friar Laurens Pignon (c. 1368–1449) similarly rejected all forms of divination and determinism, including by the stars, in his 1411 "Contre les Devineurs". This was in opposition to the tradition carried by the Arab astronomer Albumasar (787-886) whose "Introductorium in Astronomiam" and "De Magnis Coniunctionibus" argued the view that both individual actions and larger scale history are determined by the stars.

In the late 15th century, Giovanni Pico della Mirandola forcefully attacked astrology in "Disputationes contra Astrologos", arguing that the heavens neither caused, nor heralded earthly events. His contemporary, Pietro Pomponazzi, a "rationalistic and critical thinker", was much more sanguine about astrology and critical of Pico's attack.

Renaissance scholars commonly practised astrology. Gerolamo Cardano cast the horoscope of king Edward VI of England, while John Dee was the personal astrologer to queen Elizabeth I of England. Catherine de Medici paid Michael Nostradamus in 1566 to verify the prediction of the death of her husband, king Henry II of France made by her astrologer Lucus Gauricus. Major astronomers who practised as court astrologers included Tycho Brahe in the royal court of Denmark, Johannes Kepler to the Habsburgs, Galileo Galilei to the Medici, and Giordano Bruno who was burnt at the stake for heresy in Rome in 1600. The distinction between astrology and astronomy was not entirely clear. Advances in astronomy were often motivated by the desire to improve the accuracy of astrology. Kepler, for example, was driven by a belief in harmonies between Earthly and celestial affairs, yet he disparaged the activities of most astrologers as "evil-smelling dung".

Ephemerides with complex astrological calculations, and almanacs interpreting celestial events for use in medicine and for choosing times to plant crops, were popular in Elizabethan England. In 1597, the English mathematician and physician Thomas Hood made a set of paper instruments that used revolving overlays to help students work out relationships between fixed stars or constellations, the midheaven, and the twelve astrological houses. Hood's instruments also illustrated, for pedagogical purposes, the supposed relationships between the signs of the zodiac, the planets, and the parts of the human body adherents believed were governed by the planets and signs. While Hood's presentation was innovative, his astrological information was largely standard and was taken from Gerard Mercator's astrological disc made in 1551, or a source used by Mercator. Despite its popularity, Renaissance astrology had what historian Gabor Almasi calls "elite debate", exemplified by the polemical letters of Swiss physician Thomas Erastus who fought against astrology, calling it "vanity" and "superstition." Then around the time of the new star of 1572 and the comet of 1577 there began what Almasi calls an "extended epistemological reform" which began the process of excluding religion, astrology and anthropocentrism from scientific debate. By 1679, the yearly publication La Connoissance des temps eschewed astrology as a legitimate topic.

During the Enlightenment, intellectual sympathy for astrology fell away, leaving only a popular following supported by cheap almanacs. One English almanac compiler, Richard Saunders, followed the spirit of the age by printing a derisive "Discourse on the Invalidity of Astrology", while in France Pierre Bayle's "Dictionnaire" of 1697 stated that the subject was puerile. The Anglo-Irish satirist Jonathan Swift ridiculed the Whig political astrologer John Partridge.

In the second half of the 17th century, the Society of Astrologers (1647–1684), a trade, educational, and social organization, sought to unite London's often fractious astrologers in the task of revitalizing astrology. Following the template of the popular "Feasts of Mathematicians" they endeavored to defend their art in the face of growing religious criticism. The Society hosted banquets, exchanged "instruments and manuscripts", proposed research projects, and funded the publication of sermons that depicted astrology as a legitimate biblical pursuit for Christians. They commissioned sermons that argued Astrology was divine, Hebraic, and scripturally supported by Bible passages about the Magi and the sons of Seth. According to historian Michelle Pfeffer, "The society's public relations campaign ultimately failed." Modern historians have mostly neglected the Society of Astrologers in favor of the still extant Royal Society (1660), even though both organizations initially had some of the same members.

Astrology saw a popular revival starting in the 19th century, as part of a general revival of spiritualism and—later, New Age philosophy, and through the influence of mass media such as newspaper horoscopes. Early in the 20th century the psychiatrist Carl Jung developed some concepts concerning astrology, which led to the development of psychological astrology.

Advocates have defined astrology as a symbolic language, an art form, a science, and a method of divination. Though most cultural astrology systems share common roots in ancient philosophies that influenced each other, many use methods that differ from those in the West. These include Hindu astrology (also known as "Indian astrology" and in modern times referred to as "Vedic astrology") and Chinese astrology, both of which have influenced the world's cultural history.

Western astrology is a form of divination based on the construction of a horoscope for an exact moment, such as a person's birth. It uses the tropical zodiac, which is aligned to the equinoctial points.

Western astrology is founded on the movements and relative positions of celestial bodies such as the Sun, Moon and planets, which are analysed by their movement through signs of the zodiac (twelve spatial divisions of the ecliptic) and by their aspects (based on geometric angles) relative to one another. They are also considered by their placement in houses (twelve spatial divisions of the sky). Astrology's modern representation in western popular media is usually reduced to sun sign astrology, which considers only the zodiac sign of the Sun at an individual's date of birth, and represents only 1/12 of the total chart.

The horoscope visually expresses the set of relationships for the time and place of the chosen event. These relationships are between the seven 'planets', signifying tendencies such as war and love; the twelve signs of the zodiac; and the twelve houses. Each planet is in a particular sign and a particular house at the chosen time, when observed from the chosen place, creating two kinds of relationship. A third kind is the aspect of each planet to every other planet, where for example two planets 120° apart (in 'trine') are in a harmonious relationship, but two planets 90° apart ('square') are in a conflicted relationship. Together these relationships and their interpretations are said to form "...the language of the heavens speaking to learned men."

Along with tarot divination, astrology is one of the core studies of Western esotericism, and as such has influenced systems of magical belief not only among Western esotericists and Hermeticists, but also belief systems such as Wicca, which have borrowed from or been influenced by the Western esoteric tradition. Tanya Luhrmann has said that "all magicians know something about astrology," and refers to a table of correspondences in Starhawk's "The Spiral Dance", organised by planet, as an example of the astrological lore studied by magicians.

The earliest Vedic text on astronomy is the "Vedanga Jyotisha"; Vedic thought later came to include astrology as well.

Hindu natal astrology originated with Hellenistic astrology by the 3rd century BCE, though incorporating the Hindu lunar mansions. The names of the signs (e.g. Greek 'Krios' for Aries, Hindi 'Kriya'), the planets (e.g. Greek 'Helios' for Sun, astrological Hindi 'Heli'), and astrological terms (e.g. Greek 'apoklima' and 'sunaphe' for declination and planetary conjunction, Hindi 'apoklima' and 'sunapha' respectively) in Varaha Mihira's texts are considered conclusive evidence of a Greek origin for Hindu astrology. The Indian techniques may also have been augmented with some of the Babylonian techniques.

Chinese astrology has a close relation with Chinese philosophy (theory of the three harmonies: heaven, earth and man) and uses concepts such as yin and yang, the Five phases, the 10 Celestial stems, the 12 Earthly Branches, and shichen (時辰 a form of timekeeping used for religious purposes). The early use of Chinese astrology was mainly confined to political astrology, the observation of unusual phenomena, identification of portents and the selection of auspicious days for events and decisions.

The constellations of the Zodiac of western Asia and Europe were not used; instead the sky is divided into Three Enclosures (三垣 sān yuán), and Twenty-Eight Mansions (二十八宿 èrshíbā xiù) in twelve Ci (). The Chinese zodiac of twelve animal signs is said to represent twelve different types of personality. It is based on cycles of years, lunar months, and two-hour periods of the day (the shichen). The zodiac traditionally begins with the sign of the Rat, and the cycle proceeds through 11 other animal signs: the Ox, Tiger, Rabbit, Dragon, Snake, Horse, Goat, Monkey, Rooster, Dog, and Pig. Complex systems of predicting fate and destiny based on one's birthday, birth season, and birth hours, such as "ziping" and Zi Wei Dou Shu () are still used regularly in modern-day Chinese astrology. They do not rely on direct observations of the stars.

The Korean zodiac is identical to the Chinese one. The Vietnamese zodiac is almost identical to the Chinese, except for second animal being the "Water Buffalo" instead of the "Ox", and the fourth animal the "Cat" instead of the "Rabbit". The Japanese have since 1873 celebrated the beginning of the new year on 1 January as per the Gregorian calendar. The Thai zodiac begins, not at Chinese New Year, but either on the first day of the fifth month in the Thai lunar calendar, or during the Songkran festival (now celebrated every 13–15 April), depending on the purpose of the use.

Augustine (354430) believed that the determinism of astrology conflicted with the Christian doctrines of man's free will and responsibility, and God not being the cause of evil, but he also grounded his opposition philosophically, citing the failure of astrology to explain twins who behave differently although conceived at the same moment and born at approximately the same time.

Some of the practices of astrology were contested on theological grounds by medieval Muslim astronomers such as Al-Farabi (Alpharabius), Ibn al-Haytham (Alhazen) and Avicenna. They said that the methods of astrologers conflicted with orthodox religious views of Islamic scholars, by suggesting that the Will of God can be known and predicted. For example, Avicenna's 'Refutation against astrology', "Risāla fī ibṭāl aḥkām al-nojūm", argues against the practice of astrology while supporting the principle that planets may act as agents of divine causation. Avicenna considered that the movement of the planets influenced life on earth in a deterministic way, but argued against the possibility of determining the exact influence of the stars. Essentially, Avicenna did not deny the core dogma of astrology, but denied our ability to understand it to the extent that precise and fatalistic predictions could be made from it. Ibn Qayyim al-Jawziyya (1292–1350), in his "Miftah Dar al-SaCadah", also used physical arguments in astronomy to question the practice of judicial astrology. He recognised that the stars are much larger than the planets, and argued: And if you astrologers answer that it is precisely because of this distance and smallness that their influences are negligible, then why is it that you claim a great influence for the smallest heavenly body, Mercury? Why is it that you have given an influence to [the head] and [the tail], which are two imaginary points [ascending and descending nodes]?

Martin Luther denounced astrology in his "Table Talk". He asked why twins like Esau and Jacob had two different natures yet were born at the same time. Luther also compared astrologers to those who say their dice will always land on a certain number. Although the dice may roll on the number a couple of times, the predictor is silent for all the times the dice fails to land on that number.

The Catechism of the Catholic Church maintains that divination, including predictive astrology, is incompatible with modern Catholic beliefs such as free will:

The scientific community rejects astrology as having no explanatory power for describing the universe, and considers it a pseudoscience. Scientific testing of astrology has been conducted, and no evidence has been found to support any of the premises or purported effects outlined in astrological traditions. There is no proposed mechanism of action by which the positions and motions of stars and planets could affect people and events on Earth that does not contradict basic and well understood aspects of biology and physics. Those who have faith in astrology have been characterised by scientists including Bart J. Bok as doing so "...in spite of the fact that there is no verified scientific basis for their beliefs, and indeed that there is strong evidence to the contrary".

Confirmation bias is a form of cognitive bias, a psychological factor that contributes to belief in astrology. Astrology believers tend to selectively remember predictions that turn out to be true, and do not remember those that turn out false. Another, separate, form of confirmation bias also plays a role, where believers often fail to distinguish between messages that demonstrate special ability and those that do not. Thus there are two distinct forms of confirmation bias that are under study with respect to astrological belief.

Under the criterion of falsifiability, first proposed by the philosopher of science Karl Popper, astrology is a pseudoscience. Popper regarded astrology as "pseudo-empirical" in that "it appeals to observation and experiment," but "nevertheless does not come up to scientific standards." In contrast to scientific disciplines, astrology has not responded to falsification through experiment.

In contrast to Popper, the philosopher Thomas Kuhn argued that it was not lack of falsifiability that makes astrology unscientific, but rather that the process and concepts of astrology are non-empirical. Kuhn thought that, though astrologers had, historically, made predictions that categorically failed, this in itself does not make astrology unscientific, nor do attempts by astrologers to explain away failures by claiming that creating a horoscope is very difficult. Rather, in Kuhn's eyes, astrology is not science because it was always more akin to medieval medicine; astrologers followed a sequence of rules and guidelines for a seemingly necessary field with known shortcomings, but they did no research because the fields are not amenable to research, and so "they had no puzzles to solve and therefore no science to practise." While an astronomer could correct for failure, an astrologer could not. An astrologer could only explain away failure but could not revise the astrological hypothesis in a meaningful way. As such, to Kuhn, even if the stars could influence the path of humans through life, astrology is not scientific.

The philosopher Paul Thagard asserts that astrology cannot be regarded as falsified in this sense until it has been replaced with a successor. In the case of predicting behaviour, psychology is the alternative. To Thagard a further criterion of demarcation of science from pseudoscience is that the state-of-the-art must progress and that the community of researchers should be attempting to compare the current theory to alternatives, and not be "selective in considering confirmations and disconfirmations." Progress is defined here as explaining new phenomena and solving existing problems, yet astrology has failed to progress having only changed little in nearly 2000 years. To Thagard, astrologers are acting as though engaged in normal science believing that the foundations of astrology were well established despite the "many unsolved problems", and in the face of better alternative theories (psychology). For these reasons Thagard views astrology as pseudoscience.

For the philosopher Edward W. James, astrology is irrational not because of the numerous problems with mechanisms and falsification due to experiments, but because an analysis of the astrological literature shows that it is infused with fallacious logic and poor reasoning.

Astrology has not demonstrated its effectiveness in controlled studies and has no scientific validity. Where it has made falsifiable predictions under controlled conditions, they have been falsified. One famous experiment included 28 astrologers who were asked to match over a hundred natal charts to psychological profiles generated by the California Psychological Inventory (CPI) questionnaire. The double-blind experimental protocol used in this study was agreed upon by a group of physicists and a group of astrologers nominated by the National Council for Geocosmic Research, who advised the experimenters, helped ensure that the test was fair and helped draw the central proposition of natal astrology to be tested. They also chose 26 out of the 28 astrologers for the tests (two more volunteered afterwards). The study, published in "Nature" in 1985, found that predictions based on natal astrology were no better than chance, and that the testing "...clearly refutes the astrological hypothesis."

In 1955, the astrologer and psychologist Michel Gauquelin stated that though he had failed to find evidence that supported indicators like zodiacal signs and planetary aspects in astrology, he did find positive correlations between the diurnal positions of some planets and success in professions that astrology traditionally associates with those planets. The best-known of Gauquelin's findings is based on the positions of Mars in the natal charts of successful athletes and became known as the "Mars effect". A study conducted by seven French scientists attempted to replicate the claim, but found no statistical evidence. They attributed the effect to selective bias on Gauquelin's part, accusing him of attempting to persuade them to add or delete names from their study.

Geoffrey Dean has suggested that the effect may be caused by self-reporting of birth dates by parents rather than any issue with the study by Gauquelin. The suggestion is that a small subset of the parents may have had changed birth times to be consistent with better astrological charts for a related profession. The number of births under astrologically undesirable conditions was also lower, indicating that parents choose dates and times to suit their beliefs. The sample group was taken from a time where belief in astrology was more common. Gauquelin had failed to find the Mars effect in more recent populations, where a nurse or doctor recorded the birth information.

Dean, a scientist and former astrologer, and psychologist Ivan Kelly conducted a large scale scientific test that involved more than one hundred cognitive, behavioural, physical, and other variables—but found no support for astrology. Furthermore, a meta-analysis pooled 40 studies that involved 700 astrologers and over 1,000 birth charts. Ten of the tests—which involved 300 participants—had the astrologers pick the correct chart interpretation out of a number of others that were not the astrologically correct chart interpretation (usually three to five others). When date and other obvious clues were removed, no significant results suggested there was any preferred chart.

Testing the validity of astrology can be difficult, because there is no consensus amongst astrologers as to what astrology is or what it can predict. Most professional astrologers are paid to predict the future or describe a person's personality and life, but most horoscopes only make vague untestable statements that can apply to almost anyone.

Many astrologers claim that astrology is scientific, while some have proposed conventional causal agents such as electromagnetism and gravity. Scientists reject these mechanisms as implausible since, for example, the magnetic field, when measured from Earth, of a large but distant planet such as Jupiter is far smaller than that produced by ordinary household appliances.

Western astrology has taken the earth's axial precession (also called precession of the equinoxes) into account since Ptolemy's "Almagest", so the "first point of Aries", the start of the astrological year, continually moves against the background of the stars. The tropical zodiac has no connection to the stars, and as long as no claims are made that the constellations themselves are in the associated sign, astrologers avoid the concept that precession seemingly moves the constellations. Charpak and Broch, noting this, referred to astrology based on the tropical zodiac as being "...empty boxes that have nothing to do with anything and are devoid of any consistency or correspondence with the stars." Sole use of the tropical zodiac is inconsistent with references made, by the same astrologers, to the Age of Aquarius, which depends on when the vernal point enters the constellation of Aquarius.

Astrologers usually have only a small knowledge of astronomy, and often do not take into account basic principles—such as the precession of the equinoxes, which changes the position of the sun with time. They commented on the example of Élizabeth Teissier, who claimed that, "The sun ends up in the same place in the sky on the same date each year", as the basis for claims that two people with the same birthday, but a number of years apart, should be under the same planetary influence. Charpak and Broch noted that, "There is a difference of about twenty-two thousand miles between Earth's location on any specific date in two successive years", and that thus they should not be under the same influence according to astrology. Over a 40-year period there would be a difference greater than 780,000 miles.

The general consensus of astronomers and other natural scientists is that astrology is a pseudoscience which carries no predictive capability, with many philosophers of science considering it a "paradigm or prime example of pseudoscience." Some scholars in the social sciences have cautioned against categorizing astrology, especially ancient astrology, as "just" a pseudoscience or projecting the distinction backwards into the past. Thagard, while demarcating it as a pseudoscience, notes that astrology "should be judged as not pseudoscientific in classical or Renaissance times...Only when the historical and social aspects of science are neglected does it become plausible that pseudoscience is an unchanging category." Historians of science such as Tamsyn Barton, Roger Beck, Francesca Rochberg, and Wouter J. Hanegraaff argue that such a wholesale description is anachronistic when applied to historical contexts, stressing that astrology was not pseudoscience before the 18th century and the importance of the discipline to the development of medieval science. R. J. Hakinson writes in the context of Hellenistic astrology that "the belief in the possibility of [astrology] was, at least some of the time, the result of careful reflection on the nature and structure of the universe."

Nicholas Campion, both an astrologer and academic historian of astrology, argues that Indigenous astronomy is largely used as a synonym for astrology in academia, and that modern Indian and Western astrology are better understood as modes of cultural astronomy or ethnoastronomy. Roy Willis and Patrick Curry draw a distinction between propositional "" and metaphoric "" in the ancient world, identifying astrology with the latter and noting that the central concern of astrology "is not knowledge (factual, let alone scientific) but (ethical, spiritual and pragmatic)". Similarly, historian of science Justin Niermeier-Dohoney writes that astrology was "more than simply a science of prediction using the stars and comprised a vast body of beliefs, knowledge, and practices with the overarching theme of understanding the relationship between humanity and the rest of the cosmos through an interpretation of stellar, solar, lunar, and planetary movement." Scholars such as Assyriologist Matthew Rutz have begun using the term "astral knowledge" rather than astrology "to better describe a category of beliefs and practices much broader than the term 'astrology' can capture."

In the West, political leaders have sometimes consulted astrologers. For example, the British intelligence agency MI5 employed Louis de Wohl as an astrologer after claims surfaced that Adolf Hitler used astrology to time his actions. The War Office was "...interested to know what Hitler's own astrologers would be telling him from week to week." In fact, de Wohl's predictions were so inaccurate that he was soon labelled a "complete charlatan", and later evidence showed that Hitler considered astrology "complete nonsense". After John Hinckley's attempted assassination of US President Ronald Reagan, first lady Nancy Reagan commissioned astrologer Joan Quigley to act as the secret White House astrologer. However, Quigley's role ended in 1988 when it became public through the memoirs of former chief of staff, Donald Regan.

There was a boom in interest in astrology in the late 1960s. The sociologist Marcello Truzzi described three levels of involvement of "Astrology-believers" to account for its revived popularity in the face of scientific discrediting. He found that most astrology-believers did not claim it was a scientific explanation with predictive power. Instead, those superficially involved, knowing "next to nothing" about astrology's 'mechanics', read newspaper astrology columns, and could benefit from "tension-management of anxieties" and "a cognitive belief-system that transcends science." Those at the second level usually had their horoscopes cast and sought advice and predictions. They were much younger than those at the first level, and could benefit from knowledge of the language of astrology and the resulting ability to belong to a coherent and exclusive group. Those at the third level were highly involved and usually cast horoscopes for themselves. Astrology provided this small minority of astrology-believers with a ""meaningful" view of their universe and [gave] them an "understanding" of their place in it." This third group took astrology seriously, possibly as an overarching religious worldview (a "sacred canopy", in Peter L. Berger's phrase), whereas the other two groups took it playfully and irreverently.

In 1953, the sociologist Theodor W. Adorno conducted a study of the astrology column of a Los Angeles newspaper as part of a project examining mass culture in capitalist society. Adorno believed that popular astrology, as a device, invariably leads to statements that encouraged conformity—and that astrologers who go against conformity, by discouraging performance at work etc., risk losing their jobs. Adorno concluded that astrology is a large-scale manifestation of systematic irrationalism, where individuals are subtly led—through flattery and vague generalisations—to believe that the author of the column is addressing them directly. Adorno drew a parallel with the phrase opium of the people, by Karl Marx, by commenting, "occultism is the metaphysic of the dopes."

A 2005 Gallup poll and a 2009 survey by the Pew Research Center reported that 25% of US adults believe in astrology, while a 2018 Pew survey found a figure of 29%. According to data released in the National Science Foundation's 2014 "Science and Engineering Indicators" study, "Fewer Americans rejected astrology in 2012 than in recent years." The NSF study noted that in 2012, "slightly more than half of Americans said that astrology was 'not at all scientific,' whereas nearly two-thirds gave this response in 2010. The comparable percentage has not been this low since 1983." Astrology apps became popular in the late 2010s, some receiving millions of dollars in Silicon Valley venture capital.

In India, there is a long-established and widespread belief in astrology. It is commonly used for daily life, particularly in matters concerning marriage and career, and makes extensive use of electional, horary and karmic astrology. Indian politics have also been influenced by astrology. It is still considered a branch of the Vedanga. In 2001, Indian scientists and politicians debated and critiqued a proposal to use state money to fund research into astrology, resulting in permission for Indian universities to offer courses in Vedic astrology.

In February 2011, the Bombay High Court reaffirmed astrology's standing in India when it dismissed a case that challenged its status as a science.

In Japan, strong belief in astrology has led to dramatic changes in the fertility rate and the number of abortions in the years of Fire Horse. Adherents believe that women born in "hinoeuma" years are unmarriageable and bring bad luck to their father or husband. In 1966, the number of babies born in Japan dropped by over 25% as parents tried to avoid the stigma of having a daughter born in the hinoeuma year.

The fourteenth-century English poets John Gower and Geoffrey Chaucer both referred to astrology in their works, including Gower's "Confessio Amantis" and Chaucer's "The Canterbury Tales". Chaucer commented explicitly on astrology in his "Treatise on the Astrolabe", demonstrating personal knowledge of one area, judicial astrology, with an account of how to find the ascendant or rising sign.

In the fifteenth century, references to astrology, such as with similes, became "a matter of course" in English literature.

In the sixteenth century, John Lyly's 1597 play, "The Woman in the Moon", is wholly motivated by astrology, while Christopher Marlowe makes astrological references in his plays "Doctor Faustus" and "Tamburlaine" (both c. 1590), and Sir Philip Sidney refers to astrology at least four times in his romance "The Countess of Pembroke's Arcadia" (c. 1580). Edmund Spenser uses astrology both decoratively and causally in his poetry, revealing "...unmistakably an abiding interest in the art, an interest shared by a large number of his contemporaries." George Chapman's play, "Byron's Conspiracy" (1608), similarly uses astrology as a causal mechanism in the drama. William Shakespeare's attitude towards astrology is unclear, with contradictory references in plays including "King Lear", "Antony and Cleopatra", and "Richard II". Shakespeare was familiar with astrology and made use of his knowledge of astrology in nearly every play he wrote, assuming a basic familiarity with the subject in his commercial audience. Outside theatre, the physician and mystic Robert Fludd practised astrology, as did the quack doctor Simon Forman. In Elizabethan England, "The usual feeling about astrology ... [was] that it is the most useful of the sciences."

In seventeenth century Spain, Lope de Vega, with a detailed knowledge of astronomy, wrote plays that ridicule astrology. In his pastoral romance "La Arcadia" (1598), it leads to absurdity; in his novela "Guzman el Bravo" (1624), he concludes that the stars were made for man, not man for the stars. Calderón de la Barca wrote the 1641 comedy "Astrologo Fingido" (The Pretended Astrologer); the plot was borrowed by the French playwright Thomas Corneille for his 1651 comedy "Feint Astrologue".
The most famous piece of music influenced by astrology is the orchestral suite "The Planets". Written by the British composer Gustav Holst (1874–1934), and first performed in 1918, the framework of "The Planets" is based upon the astrological symbolism of the planets. Each of the seven movements of the suite is based upon a different planet, though the movements are not in the order of the planets from the Sun. The composer Colin Matthews wrote an eighth movement entitled "Pluto, the Renewer", first performed in 2000. In 1937, another British composer, Constant Lambert, wrote a ballet on astrological themes, called "Horoscope". In 1974, the New Zealand composer Edwin Carr wrote "The Twelve Signs: An Astrological Entertainment" for orchestra without strings. Camille Paglia acknowledges astrology as an influence on her work of literary criticism "Sexual Personae" (1990).

Astrology features strongly in Eleanor Catton's "The Luminaries", recipient of the 2013 Man Booker Prize.








Abyssinia (disambiguation)

Abyssinia is a historical name for the Ethiopian Empire.

Abyssinia may also refer to:








Algebraic extension

In mathematics, an algebraic extension is a field extension such that every element of the larger field is algebraic over the smaller field ; that is, every element of is a root of a non-zero polynomial with coefficients in . A field extension that is not algebraic, is said to be transcendental, and must contain transcendental elements, that is, elements that are not algebraic.

The algebraic extensions of the field formula_1 of the rational numbers are called algebraic number fields and are the main objects of study of algebraic number theory. Another example of a common algebraic extension is the extension formula_2 of the real numbers by the complex numbers.

All transcendental extensions are of infinite degree. This in turn implies that all finite extensions are algebraic. The converse is not true however: there are infinite extensions which are algebraic. For instance, the field of all algebraic numbers is an infinite algebraic extension of the rational numbers.

Let be an extension field of , and . The smallest subfield of that contains and is commonly denoted formula_3 If is algebraic over , then the elements of can be expressed as polynomials in with coefficients in "K"; that is, is also the smallest ring containing and . In this case, formula_4 is a finite extension of (it is a finite dimensional -vector space), and all its elements are algebraic over . These properties do not hold if is not algebraic. For example, formula_5 and they are both infinite dimensional vector spaces over formula_6

An algebraically closed field "F" has no proper algebraic extensions, that is, no algebraic extensions "E" with "F" < "E". An example is the field of complex numbers. Every field has an algebraic extension which is algebraically closed (called its algebraic closure), but proving this in general requires some form of the axiom of choice.

An extension "L"/"K" is algebraic if and only if every sub "K"-algebra of "L" is a field.

The following three properties hold:

These finitary results can be generalized using transfinite induction:

This fact, together with Zorn's lemma (applied to an appropriately chosen poset), establishes the existence of algebraic closures.

Model theory generalizes the notion of algebraic extension to arbitrary theories: an embedding of "M" into "N" is called an algebraic extension if for every "x" in "N" there is a formula "p" with parameters in "M", such that "p"("x") is true and the set

is finite. It turns out that applying this definition to the theory of fields gives the usual definition of algebraic extension. The Galois group of "N" over "M" can again be defined as the group of automorphisms, and it turns out that most of the theory of Galois groups can be developed for the general case.

Given a field "k" and a field "K" containing "k", one defines the relative algebraic closure of "k" in "K" to be the subfield of "K" consisting of all elements of "K" that are algebraic over "k", that is all elements of "K" that are a root of some nonzero polynomial with coefficients in "k".



Ani DiFranco

Angela Maria "Ani" DiFranco (; born September 23, 1970) is an American-Canadian singer-songwriter. She has released more than 20 albums. DiFranco's music has been classified as folk rock and alternative rock, although it has additional influences from punk, funk, hip hop and jazz. She has released all her albums on her own record label, Righteous Babe.

In February 2024, DiFranco made her Broadway debut in "Hadestown".

DiFranco supports many social and political movements by performing benefit concerts, appearing on benefit albums and speaking at rallies. Through the Righteous Babe Foundation, DiFranco has backed grassroots cultural and political organizations supporting causes including abortion rights and LGBT visibility. She counts American folk singer and songwriter Pete Seeger among her mentors.

DiFranco released a memoir, "No Walls and the Recurring Dream", on May 7, 2019, via Viking Books and made "The New York Times" Best Seller list.

DiFranco was born in Buffalo, New York, on September 23, 1970, the daughter of Elizabeth (Ross) and Dante Americo DiFranco, who had met while attending the Massachusetts Institute of Technology. Her father was of Italian descent, and her mother was from Montreal. DiFranco started playing Beatles covers at local bars and busking with her guitar teacher, Michael Meldrum, at the age of nine. By 14 she was writing her own songs. She played them at bars and coffee houses throughout her teens. DiFranco graduated from the Buffalo Academy for Visual and Performing Arts high school at 16 and began attending classes at Buffalo State College. She was living by herself, having moved out of her mother's apartment after she became an emancipated minor when she was 15.

DiFranco started her own record company, Righteous Babe Records, in 1989 at age 19. She released her self-titled debut album in the winter of 1990, shortly after relocating to New York City. There, she took poetry classes at The New School, where she met poet Sekou Sundiata, who was to become a friend and mentor. She toured steadily for the next 15 years, pausing only to record albums. Appearances at Canadian folk festivals and increasingly larger venues in the U.S. reflected her increasing popularity on the North American folk and roots scene. Throughout the early and mid-1990s DiFranco toured solo and also as a duo with Canadian drummer Andy Stochansky.

In September 1995, DiFranco participated in a concert at the Rock and Roll Hall of Fame in Cleveland Ohio, inaugurating the opening of the Woody Guthrie Archives in New York City. She later released a CD on Righteous Babe of the concert "Til We Outnumber Em" featuring artists such as DiFranco, Billy Bragg, Ramblin' Jack Elliott, Arlo Guthrie, Indigo Girls, Dave Pirner, Tim Robbins, and Bruce Springsteen with 100 percent of proceeds going to the Woody Guthrie Foundation and Archives and the Rock and Roll Hall of Fame Museum educational department.

In 1996, bassist Sara Lee joined the touring group, whose live rapport is showcased on the 1997 album "Living in Clip". DiFranco would later release Lee's solo album "Make It Beautiful" on Righteous Babe. In 1998, Stochansky left to pursue a solo career as a singer-songwriter. A new touring ensemble consisting of Jason Mercer on bass, Julie Wolf on keyboards, and Daren Hahn on drums, augmented at times by a horn section, accompanied DiFranco on tour between 1998 and 2002.

The 1990s were a period of heightened exposure for DiFranco, as she continued playing ever larger venues around the world and attracted international attention of the press, including cover stories in "Spin", "Ms.", and "Magnet", among others, as well as appearances on MTV and VH1. Her playfully ironic cover of the Bacharach/David song "Wishin' and Hopin'" appeared under the opening titles of the film "My Best Friend's Wedding".
She guest starred on a 1998 episode of the Fox sitcom "King of the Hill", as the voice of Peggy's feminist guitar teacher, Emily.
Beginning in 1999, Righteous Babe Records began releasing albums by other artists including Sara Lee, Sekou Sundiata, Arto Lindsay, Bitch and Animal, That One Guy, Utah Phillips, Hamell on Trial, Andrew Bird, Kurt Swinghammer, Buddy Wakefield, Anaïs Mitchell and Nona Hendryx.

On September 11, 2001, DiFranco was in Manhattan and later penned the poem "Self Evident" about the experience. The poem was featured in the book "It's a Free Country: Personal Freedom in America After September 11". The poem's title also became the name of DiFranco's first book of poetry released exclusively in Italy by Minimum Fax. It was later also featured in "Verses", a book of her poetry published in the U.S. by Seven Stories press. DiFranco has written and performed many spoken-word pieces throughout her career and was showcased as a poet on the HBO series "Def Poetry" in 2005.

Since her 2005 release "Knuckle Down" (co-produced by Joe Henry) DiFranco's touring band and recordings have featured bass player Todd Sickafoose and in turns other musicians such as Allison Miller, Andy Borger, Herlin Riley, and Terence Higgins on drums and Mike Dillon on percussion and vibes.

On September 11, 2007, she released the first retrospective of her career, a two-disc compilation entitled "Canon" and simultaneously a retrospective collection of poetry book "Verses". On September 30, 2008, she released "Red Letter Year".
In 2009, DiFranco appeared at Pete Seeger's 90th birthday celebration at Madison Square Garden, debuting her revamped version of the 1930s labor anthem "Which Side Are You On?" in a duet with Bruce Cockburn and also duetting with Kris Kristofferson on the folk classic "There's a Hole in the Bucket".

DiFranco released an album on January 17, 2012, "¿Which Side Are You On?". It includes collaborations with Pete Seeger, Ivan Neville, Cyril Neville, Skerik, Adam Levy, Righteous Babe recording artist Anaïs Mitchell, CC Adcock, and a host of New Orleans-based horn players known for their work in such outfits as Galactic, Bonerama, and Rebirth Brass Band.

In 2014, she released her eighteenth album, "Allergic to Water". In 2017, she released her nineteenth, "Binary".

On May 7, 2019, DiFranco released a memoir, "No Walls and the Recurring Dream", via Viking Books. It is described as a "coming-of-age story".

In 2021, DiFranco released the album "Revolutionary Love" which was largely inspired by Valarie Kaur's book "See No Stranger."

DiFranco signed an October 2023 open letter of artists for ceasefire during the Israeli bombardment of Gaza.

DiFranco came out as bisexual in her twenties, and has written songs about love and sex with women and men. She addressed the controversy about her sexuality in the song "In or Out" on the album "Imperfectly" (1992). However, in 2015 she told the blog GoPride.com that she was ""not so queer anymore, but definitely a woman-centered woman and just a human rights-centered artist." In a 2019 interview with "Jezebel", she stated that she preferred the term "queer" because "bisexual" "always sounded very medical, like something you do to a frog in 9th grade science or something", and further added that "the irony is I'm pretty fuckin' hetero, which is unfortunate for me because many of my deepest connections are with women. But, naw, I just like what's in boys' pants better.". In 1998, she married her sound engineer Andrew Gilchrist in a Unitarian Universalist service in Canada. DiFranco and Gilchrist divorced in 2003.

In 1990, she wrote "Lost Woman Song", which was inspired by her abortions at ages eighteen and twenty.

DiFranco's father died in the summer of 2004. In July 2005, DiFranco developed tendinitis and took a nine-month hiatus from touring. In January 2007 DiFranco gave birth to her first child, a daughter, at her Buffalo home. She married the child's father, Mike Napolitano, also her regular producer, in 2009. In an interview on September 13, 2012, DiFranco mentioned that she was pregnant with her second child. In April 2013, she gave birth to her second child, a son.

DiFranco has resided in the Bywater, New Orleans, neighborhood since 2008.

DiFranco has described herself as an atheist. On the subject of religion, DiFranco has stated:

DiFranco has spoken critically of cancel culture, saying it is "just gonna get us nowhere" and "The human family can't divorce each other". DiFranco herself has received criticism for planning a 2013 songwriting retreat at Nottoway, a former slave plantation. She cancelled the retreat three days after the news broke, writing on her website, "I needed a wake-up call and you gave it to me." In a 2019 interview, she said of her choices at the time, "I should have found the ultimate humility to put down my own hurt, and all of the misconceptions or mis-truths out there. You have to make yourself accountable. There’s a greater pain that’s bigger than me, and it’s more important."

DiFranco wrote in her memoir that she "[sympathized] with both sides" regarding the controversial trans-exclusionary policies of the Michigan Womyn's Music Festival. In a 2019 interview, she elaborated on this statement, discussing her perception that cisgender women were being "asked again ... to move over and make room for somebody else," and later expressed that she understood the difficulty "for anybody outside of a very specific group to experience it the way that group does," saying that "maybe [women's spaces] should be a little more [inclusive]".

DiFranco has been a critical success for much of her career, with a career album average of 72 on Metacritic. "Living in Clip", DiFranco's 1998 double live album, is the only one to achieve gold record status to date. DiFranco was praised by "The Buffalo News" in 2006 as "Buffalo's leading lady of rock music".

Starting in 2003, DiFranco was nominated four consecutive times for Best Recording Package at the Grammy Awards, winning in 2004 for "Evolve".

On July 21, 2006, DiFranco received the Woman of Courage Award at the National Organization for Women (NOW) Conference and Young Feminist Summit in Albany, New York. DiFranco was one of the first musicians to receive the award, given each year to a woman who has set herself apart by her contributions to the feminist movement.

In 2009, DiFranco received the Woody Guthrie Award for being a voice of positive social change.

DiFranco's guitar playing is often characterized by a signature staccato style, rapid fingerpicking and many alternate tunings. She delivers many of her lines in a speaking style notable for its rhythmic variation. Her lyrics, which often include alliteration, metaphor, word play and a more or less gentle irony, have also received praise for their sophistication.
Although DiFranco's music has been classified as both folk rock and alternative rock, she has reached across genres since her earliest albums incorporating first punk, then funk, hiphop, and jazz influences.

While primarily an acoustic guitarist she has used a variety of instruments and styles: brass instrumentation was prevalent in 1998's "Little Plastic Castle"; a simple walking bass in her 1997 cover of Hal David and Burt Bacharach's "Wishin' and Hopin' "; strings on the 1997 live album "Living in Clip" and 2004's "Knuckle Down"; and electronics and synthesizers in 1999's "To the Teeth" and 2006's "Reprieve".

DiFranco has stated that "folk music is not an acoustic guitar – that's not where the heart of it is. I use the word 'folk' in reference to punk music and rap music. It's an attitude, it's an awareness of one's heritage, and it's a community. It's subcorporate music that gives voice to different communities and their struggle against authority."

DiFranco has collaborated with a wide range of artists. In 1997, she appeared on Canadian songwriter Bruce Cockburn's "Charity of Night" album. In 1998, she produced fellow folksinger Dan Bern's album "Fifty Eggs".

She developed a deep association with folksinger and social activist Utah Phillips throughout the mid-1990s, sharing her stage and her audience with the older musician until his death in 2008 and resulting in two collaborative albums: "The Past Didn't Go Anywhere" (1996) and "Fellow Workers" (1999, with liner notes by Howard Zinn). "The Past" is built around Phillips's storytelling, an important part of his art that had not previously been documented on recordings; on the album, DiFranco provides musical settings for his speaking voice. The followup, "Fellow Workers", was recorded live in Daniel Lanois's Kingsway Studio in New Orleans and features Phillips fronting DiFranco's touring band for a collection of songs and stories.

Prince recorded two songs with DiFranco in 1999, "Providence" on her "To the Teeth" album, and "Eye Love U, But Eye Don't Trust U Anymore" on Prince's "Rave Un2 the Joy Fantastic" album. Funk and soul jazz musician Maceo Parker and rapper Corey Parker have both appeared on DiFranco's albums and featured appearances by her on theirs. Parker and DiFranco toured together in 1999.

She has appeared on several compilations of the songs of Pete Seeger and frequented his Hudson Clearwater Revival Festival. In 2001, she appeared on Brazilian artist Lenine's album "Falange Canibal". In 2002, her rendition of Greg Brown's "The Poet Game" appeared on "Going Driftless: An Artist's Tribute to Greg Brown". Also in 2002 she recorded a duet with Jackie Chan of the Irving Gordon song "Unforgettable" for a record of unlikely collaborations, "When Pigs Fly: Songs You Never Thought You'd Hear".

In 2005, she appeared on Dar Williams' record "My Better Self", duetting on William's cover of Pink Floyd's "Comfortably Numb". She performed with Cyndi Lauper on "Sisters of Avalon" a track from Lauper's 2005 "The Body Acoustic" album. In 2006, she produced Hamell on Trial's album "Songs for Parents Who Enjoy Drugs". In 2008, she appeared on Todd Sickafoose's album "Tiny Resisters". In 2010, she co-produced a track with Margaret Cho called "Captain Cameltoe" for the comedian's "Cho Dependant" album. In 2011, she appeared on Rob Wasserman's album "Note of Hope", an exploration of the writings of Woody Guthrie with musical accompaniment, though the track in which she appeared, "Voice", was actually recorded 13 years earlier. Also in 2011 she duetted with Greg Dulli on the Twilight Singers record "Dynamite Steps".

Other artists have covered and sampled DiFranco's work throughout the years. Her spoken word poem "Self Evident" was covered by Public Enemy founder Chuck D's group called Impossebulls. Alana Davis had some commercial success with DiFranco's song "32 Flavors".

Samples from the track "Coming Up" were used by DJ Spooky in his album "Live Without Dead Time", produced for AdBusters Magazine in 2003.

In 2010, DiFranco played Persephone on Anaïs Mitchell's album Hadestown.

DiFranco was approached by Zoe Boekbinder to work on their "Prison Music Project", an album of collaborations between incarcerated and formerly incarcerated writers and musicians on the outside. DiFranco co-produced the project with Boekbinder and co-wrote and performed "Nowhere but Barstow and Prison." The album "Long Time Gone" was released on Righteous Babe Records in 2020 after ten years in the making.

Although much of DiFranco's material is autobiographical, it is often also strongly political. Many of her songs are concerned with contemporary social issues such as racism, sexism, sexual abuse, homophobia, reproductive rights, poverty, and war. In 2008, she donated a song to Aid Still Required's CD to assist with the restoration of the devastation done to Southeast Asia from the 2004 tsunami.
The combination of personal and political is partially responsible for DiFranco's early popularity among politically active college students, particularly those of the left wing, some of whom set up fan pages on the web to document DiFranco's career as early as 1994. DiFranco's rapid rise in popularity in the mid-1990s was fueled mostly by personal contact and word of mouth rather than mainstream media.

Ani cites her anti-corporate ethos for the main reason she decided to start her own label. This has allowed her a considerable degree of creative freedom over the years, including, for example, providing all instrumentals and vocals and recording the album herself at her home on an analog 8-track reel to reel, and handling much of the artwork and packaging design for her 2004 album "Educated Guess". She has referenced this independence from major labels in song more than once, including "The Million You Never Made" ("Not a Pretty Girl"), which discusses the act of turning down a lucrative contract, "The Next Big Thing" ("Not So Soft"), which describes an imagined meeting with a label head-hunter who evaluates the singer based on her looks, and "Napoleon" ("Dilate"), which sympathizes sarcastically with an unnamed friend who did sign with a label.

The business grew organically starting in 1990 with the first cassette tape. Connections were made when women in colleges started duplicating and sharing tapes. Offers to play at colleges started coming in and her popularity grew largely by word of mouth and through women's groups or organizations. Zango and Goldenrod, two music distributors specializing in women's music, started carrying DiFranco's music. In general they sold music to independent music stores and women's book stores. In 1995, Righteous Babe Records signed with Koch International for DiFranco's release of "Not a Pretty Girl". Her records could then be found in large and small record stores alike.

DiFranco has occasionally joined with Prince in discussing publicly the problems associated with major record companies. Righteous Babe Records employs a number of people in her hometown of Buffalo. In a 1997 open letter to "Ms." magazine she expressed displeasure that what she considers a way to ensure her own artistic freedom was seen by others solely in terms of its financial success.

From the earliest days of her career, DiFranco has lent her voice and her name to a broad range of social movements, performing benefit concerts, appearing on benefit albums, speaking at rallies, and offering info table space to organizations at her concerts and the virtual equivalent on her website, among other methods and actions. In 1999, she created her own not-for-profit organization; as the Buffalo News has reported, "Through the Righteous Babe Foundation, DiFranco has backed various grassroots cultural and political organizations, supporting causes ranging from abortion rights to gay visibility."

During the first Gulf War, DiFranco participated in the anti-war movement. In early 1993 she played Pete Seeger's Clearwater Folk Festival for the first time. In 1998, she was a featured performer in the Dead Man Walking benefit concert series raising money for Sister Helen Prejean's "Not in Our Name" anti-death penalty organization. DiFranco's commitment to opposing the death penalty is longstanding; she has also been a long time supporter of the Southern Center for Human Rights.

During the 2000 U.S. presidential election, she actively supported and voted for Green Party candidate Ralph Nader, though in an open letter she made clear that if she lived in a swing state, she would vote for Al Gore to prevent George W. Bush from being elected.

In 2004, DiFranco visited Burma in order to learn about the Burmese resistance movement and the country's fight for democracy. During her travels she met with then-detained resistance leader Aung San Suu Kyi. Her song "In The Way" was later featured on "For the Lady", a benefit CD that donated all proceeds to the United States Campaign for Burma.

During the 2004 presidential primaries, she supported liberal, anti-war Democrat Dennis Kucinich, who appeared on stage with her during several of her concerts. After the primary season ended, and John Kerry was the clear Democratic candidate, DiFranco launched a "Vote Dammit!" tour of swing states encouraging audience members to vote. In 2005, she lobbied Congress against the proliferation of nuclear power in general and the placement of nuclear waste dumps on Indian land in particular. In 2008, she again backed Kucinich in his bid for the presidency.

In 2002, Righteous Babe Records established the "Aiding Buffalo's Children" program in conjunction with members of the local community to raise funds for Buffalo's public school system. To kick off the program, DiFranco donated "a day's pay"—the performance fee from her concert that year at Shea's Performing Arts Center— to ABC and challenged her fans to do the same. Aiding Buffalo's Children has since been folded into the Community Foundation of Greater Buffalo, contributing to a variety of charitable funds.

In 2005, when Hurricane Katrina devastated DiFranco's newly adopted home town of New Orleans, she collected donations from fans around the world through The Righteous Babe Store website for the Katrina Piano Fund, helping musicians replace instruments lost in the hurricane, raising over $47,500 for the cause.

In 2010, after the Deepwater Horizon oil spill, she performed at the "For Our Coast" benefit concert joining Marianne Faithfull, C. C. Adcock and others at the Acadiana Center for the Arts Theater in Lafayette, raising money for Gulf Aid Acadiana, and the Gulf Aid show with Lenny Kravitz, Mos Def, and others at Mardi Gras World River City in New Orleans, both shows raising money to help protect the wetlands, clean up the coast and to assist the fishermen and their families affected by the spill.

DiFranco also sits on the board for The Roots of Music, founded by Rebirth Brass Band drummer Derrick Tabb. The organization provides free marching band instruction to children in the New Orleans area in addition to academic tutoring and mentoring.

DiFranco joined about 500,000 people at the March for Women's Lives in DC in April 2004. As an honored guest she marched in the front row for the three-mile route, along with Margaret Cho, Janeane Garofalo, Whoopi Goldberg, Gloria Steinem and others. Later in the day, Ani played a few songs on the main stage in front of the Capitol, including "Your Next Bold Move".

Scot Fisher, formerly Righteous Babe label president and DiFranco's manager for many years, has been a longtime advocate of the preservation movement in Buffalo. In 1999, he and DiFranco purchased a decaying church on the verge of demolition in downtown Buffalo and began the lengthy process of restoring it. In 2006, the building opened its doors again, first briefly as "The Church" and then as "Babeville," housing two concert venues, the record label's business office, and Hallwalls Contemporary Arts Center.

DiFranco is also a member of the Toronto-based charity Artists Against Racism for which she participated in a radio PSA.

In October 2023, DiFranco signed an open letter to Joe Biden, President of the United States, of artists calling for a ceasefire of the Israeli bombardment of Gaza.











Arene (disambiguation)

An Arene, or aromatic hydrocarbon, is a hydrocarbon with alternating double and single bonds between carbon atoms forming rings.

Arene may also refer to:



Arizona Diamondbacks

The Arizona Diamondbacks (colloquially the D-backs) are an American professional baseball team based in Phoenix, Arizona. The Diamondbacks compete in Major League Baseball (MLB) as a member of the National League (NL) West Division. The franchise was established on March 9, 1995, and began play in 1998 as an expansion team. The team plays its home games at Chase Field. Along with the Tampa Bay Rays, the Diamondbacks are one of the newest teams in MLB and are the youngest team to win the World Series.

After a fifth-place finish in their inaugural season, the Diamondbacks made several off-season acquisitions, including future Hall of Fame pitcher Randy Johnson, who won four consecutive Cy Young Awards in his first four seasons with the team. In 1999, Arizona won 100 games and their first division championship. In 2001, they won the World Series over the three-time defending champion New York Yankees, becoming the fastest expansion team in major league history to win the World Series, and the first and only men's major professional sports team in the State of Arizona to win a championship. 

From 1998 to 2023, the Diamondbacks have an overall record of ().

On March 9, 1995, Phoenix was awarded an expansion franchise to begin play for the season. A $130 million franchise fee was paid to Major League Baseball and on January 16, 1997, the Diamondbacks were voted into the National League. The Diamondbacks' first major league game was played against the Colorado Rockies on March 31, 1998, at Bank One Ballpark. The ballpark was renamed Chase Field in 2005, as a result of Bank One Corporation's merger with JPMorgan Chase & Co.

Since their debut, the Diamondbacks have won two Wild Card Series, five NL West division titles, two NL pennants, and the 2001 World Series.

They later became the fastest expansion franchise in baseball history to win a World Series. The Diamondbacks defeated the Yankees in Game 7 during the 2001 postseason. 

After beating the Philadelphia Phillies 4–2 in Game 7 of the 2023 NLCS at Citizens Bank Park in Philadelphia, the Arizona Diamondbacks returned to the World Series for the first time since 2001 – against the Texas Rangers on October 27, 2023.
They lost the series 4–1 at home in Arizona on November 1, 2023.

The Diamondbacks' original colors were purple, black, teal and copper. Their logo was an italicized block letter "A" with a diamond pattern, with the crossbar represented by a snake's tongue. This period saw the Diamondbacks wear several uniform combinations.

At home, the Diamondbacks wore cream uniforms with purple pinstripes. The primary sleeved uniform, worn from 1998 to 2000, featured the full team name ("Diamond" and "Backs" stacked together) in front and chest numbers. The alternate sleeveless version contained the "A" logo on the right chest, and was paired with purple undershirts. Before the 2001 season, the sleeved uniform was changed to feature the "A" logo. In all three uniforms, player names were teal with purple trim, and numbers were purple with white with teal trim.

The Diamondbacks' primary road gray uniform also had purple pinstripes. The first version featured "Arizona" in purple with white and teal trim along with black drop shadows, with chest numbers added. Player names were in purple with white trim, and numbers were teal with white and purple trim. In 2001, the uniform became sleeveless with black undershirts, and the lettering scheme was changed to purple with white, copper and black accents.

The alternate home purple uniform featured "Arizona" in teal with white and copper trim and black drop shadows. The letters were rendered in teal with copper and white trim, but were changed to copper with teal and white trim after only one season. This set was worn until 2002.

The alternate road black uniform featured the "A" logo on the right chest, while letters were purple with white trim and numbers were teal with white and purple trim. A zigzag pattern of teal, copper and purple was featured on the sleeves. In 2001, the uniform was changed to feature "Arizona" in front. The letters became purple with white and copper trim.

The Diamondbacks initially wore four different cap versions. The primary home cap is all-purple, while the road cap is black with a teal brim. They also wore a cream cap with purple brim, and a teal cap with purple brim. All designs featured the primary "A" logo. In 1999, the road cap became all-black and contained the alternate "D-snake" logo rendered in copper. Also, the teal and cream alternate caps were dropped.

The left sleeve of all four uniforms contained the snake logo with the full team name until the 2004 season, when it became exclusive to the road black uniform.

The franchise unveiled new uniforms and colors of Sedona red, Sonoran sand and black on November 8, 2006. The red shade is named for the sandstone canyon at Red Rock State Park near Sedona, while the beige (sand) shade is named for the Sonoran Desert. A sleeve patch was added featuring a lowercase "d" and "b" configured to look like a snake's head. The team also kept the "D" logo, which was slightly altered and put on an all-red cap to be used as their game cap. They kept the "A" logo with the new colors applied to it, with a solid black cap used as the alternate cap. Arizona's updated color scheme bore a striking resemblance to the Houston Astros' color scheme (brick red, sand and black) that the Astros used until 2012, as well as the NHL's Phoenix Coyotes, whose adoption of those colors predated the Diamondbacks by four years.

The white home uniform featured "D-Backs" in red with sand and black trim. The road gray uniform featured "Arizona" in red with sand and black trim. Player names were red with black trim while numbers were black with red trim.

The alternate red uniform contained "D-Backs" in sand with red and black trim, with player names in sand with black trim and numbers in black with sand trim.

There were two versions of the alternate black uniform. One design has the alternate "A" logo on the right chest, while the other has "Arizona" written in red with black and sand trim. The latter was introduced in 2013 as a tribute to the victims of the Yarnell Hill Fire. On both uniforms, player names were sand with red trim, and numbers in red with sand trim.

Prior to the 2016 season, the Diamondbacks reincorporated teal into its color scheme while keeping Sedona Red, Sonoran Sand and black. They also unveiled eight different uniform combinations, including two separate home white and away grey uniforms. One major difference between the two sets is that the non-teal uniforms feature a snakeskin pattern on the shoulders, while the teal-trimmed uniforms include a charcoal/grey snakeskin pattern on the back. Arizona also kept the throwback pinstriped sleeveless uniforms from their 2001 championship season for use during Thursday home games.

Starting with the 2020 season, the Diamondbacks made slight redesigns to their uniforms. The snakeskin patterns were removed while the teal-trimmed grey uniforms were retired. The team also reverted to a standard grey uniform after wearing a darker shade on the previous set. Two home white uniforms remain in use: the primary Sedona Red and the alternate teal. They would also wear two black uniforms: one with the primary "A" logo on the left chest and the other with "Los D-Backs" trimmed in teal. Three cap designs were also unveiled, all with a black base: the primary "A" cap, the teal-trimmed "snake" cap (paired exclusively on the teal alternates), and the sand-trimmed "snake" cap with red brim (paired exclusively on the Sedona Red alternates). The Nike swoosh logo is also placed on the right chest near the shoulder. In 2022, the Diamondbacks introduced a red "A" cap with black brim.

In 2021, the Diamondbacks were one of seven teams to wear Nike "City Connect" uniforms. The design is primarily sand and has "Serpientes" in black script lettering emblazoned in front. The first "S" in "Serpientes" was shaped to resemble a rattlesnake. The right sleeve has the flag of Arizona patch recolored to the Diamondbacks' red, sand and black scheme, and the left sleeve has the "A" logo recolored to black and sand. Numerals are in red. The cap is primarily sand with black brim and has the "A" logo in black and sand; the regular batting helmet is used with the uniform. Initially, the Diamondbacks wore white pants with this uniform, but has since switched to sand pants.

Before the 2023 season, the Diamondbacks promoted the alternate white uniform with teal accents to its primary home uniform and retired the previous Sedona Red white uniform. This is due to a new Nike rule that limits teams to four regular uniforms plus the "City Connect" uniform.

Coming off their second World Series appearance in , the Diamondbacks unveiled refreshed uniform designs starting in 2024. Teal returned full-time as an accent color on all uniforms along with Sedona red and black, but sand was removed except on the "City Connect" uniform. The home uniform is now a cream base with black piping, featuring the "A" logo on the left chest. The road grey uniform with black piping featured "Arizona" in Sedona red with teal and black outlines. The alternate black uniform with teal piping shared the same features as the home uniform. The alternate Sedona red uniform incorporated the full "Diamondbacks" wordmark in black with teal and Sedona red outlines, along with teal numbers trimmed in black on the left chest. All uniforms featured the snake head alternate logo on either sleeve, with the sponsor logo (Avnet since 2023) on the other sleeve. The home cap is Sedona red with black brim and featured the "A" logo, and its all-black counterpart is worn with both the home and black alternate uniform. The road cap is black with Sedona red trim and featured the return of the "D-snake" logo, and is worn with both the road and alternate Sedona red uniform. The all-Sedona red alternate cap shared the same features as the road cap, and is worn with the Sedona red uniform.

The primary television play-by-play voice for the team's first nine seasons of play was Thom Brennaman, who also broadcast baseball and college football games nationally for Fox Television. Brennaman was the TV announcer for the Chicago Cubs and Cincinnati Reds (along with his father Marty Brennaman) before being hired by Diamondbacks founder Jerry Colangelo in 1996, two years before the team would begin play.

In October 2006, Brennaman left the Diamondbacks to call games with his father for the Reds beginning in 2007, signing a four-year deal.

On November 1, 2006, the team announced that the TV voice of the Milwaukee Brewers since 2002, Daron Sutton, would be hired as the Diamondbacks primary TV play-by-play voice. Sutton was signed to a five-year contract with a team option for three more years. Sutton's signature chants included "let's get some runs" when the D-backs trail in late innings.

Former Diamondbacks and Chicago Cubs first baseman Mark Grace and knuckleballer Tom Candiotti were the Diamondbacks primary color analysts for the 2006 and 2007 seasons. Former Diamondbacks third baseman Matt Williams also did color commentary on occasion, as did Cardinals and NBC broadcast legend Joe Garagiola, a longtime Phoenix-area resident and father of Joe Garagiola Jr., the first GM of the Diamondbacks.

The Diamondbacks announced in July 2007 that for the 2008 season, all regionally broadcast Diamondbacks TV games would be shown exclusively on Fox Sports Arizona (now Bally Sports Arizona) and a few could possibly be shown on the national "MLB on Fox" telecasts. Bally Sports Arizona is seen in 2.8 million households in Arizona and New Mexico. The previous flagship station since the inaugural 1998 season was KTVK (Channel 3), a popular over-the-air independent station (and former longtime ABC affiliate) in Phoenix.

From 2009 to 2012, Grace and Sutton were tagged as the main broadcasters of the Diamondbacks with pre-game and postgame shows on Fox Sports Arizona, being hosted by Joe Borowski.

On June 21, 2012, Sutton was suspended indefinitely amid rumors of insubordination. On August 24, the team announced that Grace had requested an indefinite leave of absence after being arrested for his second DUI in less than two years. Grace was later indicted on four DUI counts.) For the remainder of the 2012 season, Sutton was replaced by Greg Schulte (Jeff Munn replaced Schulte on the radio broadcast) and Grace was replaced by Luis Gonzalez. At the end of the 2012 season, the team announced that neither Sutton nor Grace would return for the 2013 season.

On October 18, 2012, the team announced that Bob Brenly would return as a broadcaster to replace Grace and that he would be joined by then-ESPN personality Steve Berthiaume.

On July 18, 2023, a federal bankruptcy court granted Bally Sports' parent company Diamond Sports Group a motion to decline its contract with the Diamondbacks as part of its chapter 11 bankruptcy. As a result, Major League Baseball assumed production of the Diamondbacks' regional telecasts (maintaining staff such as commentators), and distributed them via local television providers and MLB.tv.

The English language flagship radio station is KTAR. Greg Schulte is the regular radio play-by-play voice, a 25-year veteran of sports radio in the Phoenix market, also well known for his previous work on Phoenix Suns, Arizona Cardinals and Arizona State University (ASU) broadcasts. He calls games with analyst Tom Candiotti.

Jeff Munn served as a backup radio play-by-play announcer until 2016; he served as the regular public address announcer at Chase Field in the early days of the franchise. He previously served as the public address announcer for the Suns in the 1990s at what became Footprint Center. He is also the play-by-play radio voice for ASU women's basketball. Mike Ferrin served in the same role for six years before parting ways with the team, and he was replaced by Chris Garagiola in December 2021.

The flagship Spanish language radio station is KHOV-FM 105.1 with Oscar Soria, Rodrigo López, and Richard Saenz.

Games were televised in Spanish on KPHE-LP—with Oscar Soria and Jerry Romo as the announcers, but this arrangement ended prior to the 2009 season due to the team switching fully to Fox Sports Arizona and the lack of carriage of KPHE-LP on the Cox cable system.


<nowiki>*</nowiki> signifies active Major League player


The rivalry between the Diamondbacks and the Los Angeles Dodgers has been one of the fiercest divisional matchups for several years. Animosity between the two teams began to escalate during the 2010s in multiple incidents involving either team throwing pitches at one another or instigating into large-scale brawls between both benches. After eliminating the Diamondbacks and clinching the division on September 19, 2013, multiple Dodgers players celebrated the win by jumping into the pool at Chase Field. The two sides met during the 2017 National League Division Series as the Diamondbacks were swept 3–0 by the Dodgers en route to their appearance in the World Series that season. The Dodgers led the series 257–191 with a 3–0 lead in the postseason. After clinching the 2023 NL Wild Card berth and defeating the Milwaukee Brewers in the National League Wild Card Series, the Diamondbacks played the Dodgers again in the 2023 NLDS. There, the Diamondbacks emphatically swept the Dodgers to even the all-time postseason record between the two clubs at 3–3.

The Arizona Diamondbacks farm system consists of eight minor league affiliates.


Aesthetics

Aesthetics (also spelled esthetics) is the branch of philosophy concerned with the nature of beauty and the nature of taste; and functions as the philosophy of art. Aesthetics examines the philosophy of aesthetic value, which is determined by critical judgements of artistic taste; thus, the function of aesthetics is the "critical reflection on art, culture and nature".

Aesthetics studies natural and artificial sources of experiences and how people form a judgement about those sources of experience. It considers what happens in our minds when we engage with objects or environments such as viewing visual art, listening to music, reading poetry, experiencing a play, watching a fashion show, movie, sports or exploring various aspects of nature.

The philosophy of art specifically studies how artists imagine, create, and perform works of art, as well as how people use, enjoy, and criticize art. Aesthetics considers why people like some works of art and not others, as well as how art can affect our moods and our beliefs. Both aesthetics and the philosophy of art try to find answers to what exactly is art and what makes good art.

The word "aesthetic" is derived from the Ancient Greek (', "perceptive, sensitive, pertaining to sensory perception"), which in turn comes from (', "I perceive, sense, learn") and is related to ("", "perception, sensation"). Aesthetics in this central sense has been said to start with the series of articles on "The Pleasures of the Imagination", which the journalist Joseph Addison wrote in the early issues of the magazine The Spectator in 1712.

The term "aesthetics" was appropriated and coined with new meaning by the German philosopher Alexander Baumgarten in his dissertation "Meditationes philosophicae de nonnullis ad poema pertinentibus" () in 1735; Baumgarten chose "aesthetics" because he wished to emphasize the experience of art as a means of knowing. Baumgarten's definition of aesthetics in the fragment "Aesthetica" (1750) is occasionally considered the first definition of modern aesthetics.

The term was introduced into the English language by Thomas Carlyle in his "Life of Friedrich Schiller" (1825).

The history of the philosophy of art as aesthetics covering the visual arts, the literary arts, the musical arts and other artists forms of expression can be dated back at least to Aristotle and the ancient Greeks. Aristotle writing of the literary arts in his "Poetics" stated that epic poetry, tragedy, comedy, dithyrambic poetry, painting, sculpture, music, and dance are all fundamentally acts of mimesis, each varying in imitation by medium, object, and manner. Aristotle applies the term "mimesis" both as a property of a work of art and also as the product of the artist's intention and contends that the audience's realisation of the "mimesis" is vital to understanding the work itself.

Aristotle states that "mimesis" is a natural instinct of humanity that separates humans from animals and that all human artistry "follows the pattern of nature". Because of this, Aristotle believed that each of the mimetic arts possesses what Stephen Halliwell calls "highly structured procedures for the achievement of their purposes." For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language.

The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation – through narrative or character, through change or no change, and through drama or no drama. Erich Auerbach has extended the discussion of history of aesthetics in his book titled "Mimesis".

Some distinguish aesthetics from the philosophy of art, claiming that the former is the study of beauty and taste while the latter is the study of works of art. But aesthetics typically considers questions of beauty as well as of art. It examines topics such as art works, aesthetic experience, and aesthetic judgement.

Aesthetic experience refers to the sensory contemplation or appreciation of an object (not necessarily a work of art), while artistic judgement refers to the recognition, appreciation or criticism of art in general or a specific work of art. In the words of one philosopher, "Philosophy of art is about art. Aesthetics is about many things—including art. But it is also about our experience of breathtaking landscapes or the pattern of shadows on the wall opposite your office.

Philosophers of art weigh a culturally contingent conception of art versus one that is purely theoretical. They study the varieties of art in relation to their physical, social, and cultural environments. Aesthetic philosophers sometimes also refer to psychological studies to help understand how people see, hear, imagine, think, learn, and act in relation to the materials and problems of art. Aesthetic psychology studies the creative process and the aesthetic experience.

Aesthetics examines affective domain response to an object or phenomenon. Judgements of aesthetic value rely on the ability to discriminate at a sensory level. However, aesthetic judgements usually go beyond sensory discrimination.

For David Hume, delicacy of taste is not merely "the ability to detect all the ingredients in a composition", but also the sensitivity "to pains as well as pleasures, which escape the rest of mankind." Thus, sensory discrimination is linked to capacity for pleasure.

For Immanuel Kant ("Critique of Judgment", 1790), "enjoyment" is the result when pleasure arises from sensation, but judging something to be "beautiful" has a third requirement: sensation must give rise to pleasure by engaging reflective contemplation. Judgements of beauty are sensory, emotional and intellectual all at once. Kant observed of a man "if he says that 'Canary wine is pleasant,' he is quite content if someone else corrects his expression and remind him that he ought to say instead: 'It is pleasant "to me",'" because "every one has his own [sense of] taste". The case of "beauty" is different from mere "pleasantness" because "if he gives out anything as beautiful, he supposes in others the same satisfaction—he judges not merely for himself, but for every one, and speaks of beauty as if it were a property of things." 

Viewer interpretations of beauty may on occasion be observed to possess two concepts of value: aesthetics and taste. Aesthetics is the philosophical notion of beauty. Taste is a result of an education process and awareness of elite cultural values learned through exposure to mass culture. Bourdieu examined how the elite in society define the aesthetic values like taste and how varying levels of exposure to these values can result in variations by class, cultural background, and education. According to Kant, beauty is subjective and universal; thus certain things are beautiful to everyone. In the opinion of Władysław Tatarkiewicz, there are six conditions for the presentation of art: beauty, form, representation, reproduction of reality, artistic expression and innovation. However, one may not be able to pin down these qualities in a work of art.

The question of whether there are facts about aesthetic judgments belongs to the branch of metaphilosophy known as meta-aesthetics.

Aesthetic judgement is closely tied to disgust. Responses like disgust show that sensory detection is linked in instinctual ways to facial expressions including physiological responses like the gag reflex. Disgust is triggered largely by dissonance; as Darwin pointed out, seeing a stripe of soup in a man's beard is disgusting even though neither soup nor beards are themselves disgusting. Aesthetic judgements may be linked to emotions or, like emotions, partially embodied in physical reactions. For example, the awe inspired by a sublime landscape might physically manifest with an increased heart-rate or pupil dilation.

As seen, emotions are conformed to 'cultural' reactions, therefore aesthetics is always characterized by 'regional responses', as Francis Grose was the first to affirm in his "Rules for Drawing Caricaturas: With an Essay on Comic Painting" (1788), published in W. Hogarth, The Analysis of Beauty, Bagster, London s.d. (1791? [1753]), pp. 1–24. Francis Grose can therefore be claimed to be the first critical 'aesthetic regionalist' in proclaiming the anti-universality of aesthetics in contrast to the perilous and always resurgent dictatorship of beauty. 'Aesthetic Regionalism' can thus be seen as a political statement and stance which vies against any universal notion of beauty to safeguard the counter-tradition of aesthetics related to what has been considered and dubbed un-beautiful just because one's culture does not contemplate it, e.g. Edmund Burke's sublime, what is usually defined as 'primitive' art, or un-harmonious, non-cathartic art, camp art, which 'beauty' posits and creates, dichotomously, as its opposite, without even the need of formal statements, but which will be 'perceived' as ugly.

Likewise, aesthetic judgments may be culturally conditioned to some extent. Victorians in Britain often saw African sculpture as ugly, but just a few decades later, Edwardian audiences saw the same sculptures as beautiful. Evaluations of beauty may well be linked to desirability, perhaps even to sexual desirability. Thus, judgments of aesthetic value can become linked to judgments of economic, political, or moral value. In a current context, a Lamborghini might be judged to be beautiful partly because it is desirable as a status symbol, or it may be judged to be repulsive partly because it signifies over-consumption and offends political or moral values.

The context of its presentation also affects the perception of artwork; artworks presented in a classical museum context are liked more and rated more interesting than when presented in a sterile laboratory context. While specific results depend heavily on the style of the presented artwork, overall, the effect of context proved to be more important for the perception of artwork than the effect of genuineness (whether the artwork was being presented as original or as a facsimile/copy).

Aesthetic judgments can often be very fine-grained and internally contradictory. Likewise aesthetic judgments seem often to be at least partly intellectual and interpretative. What a thing means or symbolizes is often what is being judged. Modern aestheticians have asserted that will and desire were almost dormant in aesthetic experience, yet preference and choice have seemed important aesthetics to some 20th-century thinkers. The point is already made by Hume, but see Mary Mothersill, "Beauty and the Critic's Judgment", in "The Blackwell Guide to Aesthetics", 2004. Thus aesthetic judgments might be seen to be based on the senses, emotions, intellectual opinions, will, desires, culture, preferences, values, subconscious behaviour, conscious decision, training, instinct, sociological institutions, or some complex combination of these, depending on exactly which theory is employed.

A third major topic in the study of aesthetic judgments is how they are unified across art forms. For instance, the source of a painting's beauty has a different character to that of beautiful music, suggesting their aesthetics differ in kind. The distinct inability of language to express aesthetic judgment and the role of social construction further cloud this issue.

The philosopher Denis Dutton identified six universal signatures in human aesthetics:


Artists such as Thomas Hirschhorn have indicated that there are too many exceptions to Dutton's categories. For example, Hirschhorn's installations deliberately eschew technical virtuosity. People can appreciate a Renaissance Madonna for aesthetic reasons, but such objects often had (and sometimes still have) specific devotional functions. "Rules of composition" that might be read into Duchamp's "Fountain" or John Cage's "4′33″" do not locate the works in a recognizable style (or certainly not a style recognizable at the time of the works' realization). Moreover, some of Dutton's categories seem too broad: a physicist might entertain hypothetical worlds in his/her imagination in the course of formulating a theory. Another problem is that Dutton's categories seek to universalize traditional European notions of aesthetics and art forgetting that, as André Malraux and others have pointed out, there have been large numbers of cultures in which such ideas (including the idea "art" itself) were non-existent.

Aesthetic ethics refers to the idea that human conduct and behaviour ought to be governed by that which is beautiful and attractive. John Dewey has pointed out that the unity of aesthetics and ethics is in fact reflected in our understanding of behaviour being "fair"—the word having a double meaning of attractive and morally acceptable. More recently, James Page has suggested that aesthetic ethics might be taken to form a philosophical rationale for peace education.

Beauty is one of the main subjects of aesthetics, together with art and taste. Many of its definitions include the idea that an object is beautiful if perceiving it is accompanied by aesthetic pleasure. Among the examples of beautiful objects are landscapes, sunsets, humans and works of art. Beauty is a positive aesthetic value that contrasts with ugliness as its negative counterpart.

Different intuitions commonly associated with beauty and its nature are in conflict with each other, which poses certain difficulties for understanding it. On the one hand, beauty is ascribed to things as an objective, public feature. On the other hand, it seems to depend on the subjective, emotional response of the observer. It is said, for example, that "beauty is in the eye of the beholder". It may be possible to reconcile these intuitions by affirming that it depends both on the objective features of the beautiful thing and the subjective response of the observer. One way to achieve this is to hold that an object is beautiful if it has the power to bring about certain aesthetic experiences in the perceiving subject. This is often combined with the view that the subject needs to have the ability to correctly perceive and judge beauty, sometimes referred to as "sense of taste". Various conceptions of how to define and understand beauty have been suggested. "Classical conceptions" emphasize the objective side of beauty by defining it in terms of the relation between the beautiful object as a whole and its parts: the parts should stand in the right proportion to each other and thus compose an integrated harmonious whole. "Hedonist conceptions", on the other hand, focus more on the subjective side by drawing a necessary connection between pleasure and beauty, e.g. that for an object to be beautiful is for it to cause disinterested pleasure. Other conceptions include defining beautiful objects in terms of their value, of a loving attitude towards them or of their function.

During the first half of the twentieth century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other. This resulted in the rise of the New Criticism school and debate concerning "the intentional fallacy". At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist.

In 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled "The Intentional Fallacy", in which they argued strongly against the relevance of an author's intention, or "intended meaning" in the analysis of a literary work. For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting.

In another essay, "The Affective Fallacy," which served as a kind of sister essay to "The Intentional Fallacy", Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text. This fallacy would later be repudiated by theorists from the reader-response school of literary theory. One of the leading theorists from this school, Stanley Fish, was himself trained by New Critics. Fish criticizes Wimsatt and Beardsley in his essay "Literature in the Reader" (1970).

As summarized by Berys Gaut and Livingston in their essay "The Creation of Art": "Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms' assumption that the artist's activities and experience were a privileged critical topic." These authors contend that: "Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art. So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work."

Gaut and Livingston define the intentionalists as distinct from formalists stating that: "Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works." They quote Richard Wollheim as stating that, "The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself."

A large number of derivative forms of aesthetics have developed as contemporary and transitory forms of inquiry associated with the field of aesthetics which include the post-modern, psychoanalytic, scientific, and mathematical among others.

Early-twentieth-century artists, poets and composers challenged existing notions of beauty, broadening the scope of art and aesthetics. In 1941, Eli Siegel, American philosopher and poet, founded Aesthetic Realism, the philosophy that reality itself is aesthetic, and that "The world, art, and self explain each other: each is the aesthetic oneness of opposites."

Various attempts have been made to define Post-Modern Aesthetics. The challenge to the assumption that beauty was central to art and aesthetics, thought to be original, is actually continuous with older aesthetic theory; Aristotle was the first in the Western tradition to classify "beauty" into types as in his theory of drama, and Kant made a distinction between beauty and the sublime. What was new was a refusal to credit the higher status of certain types, where the taxonomy implied a preference for tragedy and the sublime to comedy and the Rococo.

Croce suggested that "expression" is central in the way that beauty was once thought to be central. George Dickie suggested that the sociological institutions of the art world were the glue binding art and sensibility into unities. Marshall McLuhan suggested that art always functions as a "counter-environment" designed to make visible what is usually invisible about a society. Theodor Adorno felt that aesthetics could not proceed without confronting the role of the culture industry in the commodification of art and aesthetic experience. Hal Foster attempted to portray the reaction against beauty and Modernist art in "The Anti-Aesthetic: Essays on Postmodern Culture". Arthur Danto has described this reaction as "kalliphobia" (after the Greek word for beauty, κάλλος "kallos"). André Malraux explains that the notion of beauty was connected to a particular conception of art that arose with the Renaissance and was still dominant in the eighteenth century (but was supplanted later). The discipline of aesthetics, which originated in the eighteenth century, mistook this transient state of affairs for a revelation of the permanent nature of art. Brian Massumi suggests to reconsider beauty following the aesthetical thought in the philosophy of Deleuze and Guattari. Walter Benjamin echoed Malraux in believing aesthetics was a comparatively recent invention, a view proven wrong in the late 1970s, when Abraham Moles and Frieder Nake analyzed links between beauty, information processing, and information theory. Denis Dutton in "The Art Instinct" also proposed that an aesthetic sense was a vital evolutionary factor.

Jean-François Lyotard re-invokes the Kantian distinction between taste and the sublime. Sublime painting, unlike kitsch realism, "... will enable us to see only by making it impossible to see; it will please only by causing pain."

Sigmund Freud inaugurated aesthetical thinking in Psychoanalysis mainly via the "Uncanny" as aesthetical affect. Following Freud and Merleau-Ponty, Jacques Lacan theorized aesthetics in terms of sublimation and the Thing.

The relation of Marxist aesthetics to post-modern aesthetics is still a contentious area of debate.

The field of experimental aesthetics was founded by Gustav Theodor Fechner in the 19th century. Experimental aesthetics in these times had been characterized by a subject-based, inductive approach. The analysis of individual experience and behaviour based on experimental methods is a central part of experimental aesthetics. In particular, the perception of works of art, music, or modern items such as websites or other IT products is studied. Experimental aesthetics is strongly oriented towards the natural sciences. Modern approaches mostly come from the fields of cognitive psychology (aesthetic cognitivism) or neuroscience (neuroaesthetics).

Mathematical considerations, such as symmetry and complexity, are used for analysis in theoretical aesthetics. This is different from the aesthetic considerations of applied aesthetics used in the study of mathematical beauty. Aesthetic considerations such as symmetry and simplicity are used in areas of philosophy, such as ethics and theoretical physics and cosmology to define truth, outside of empirical considerations. Beauty and Truth have been argued to be nearly synonymous, as reflected in the statement "Beauty is truth, truth beauty" in the poem "Ode on a Grecian Urn" by John Keats, or by the Hindu motto "Satyam Shivam Sundaram" (Satya (Truth) is Shiva (God), and Shiva is Sundaram (Beautiful)). The fact that judgments of beauty and judgments of truth both are influenced by processing fluency, which is the ease with which information can be processed, has been presented as an explanation for why beauty is sometimes equated with truth. Recent research found that people use beauty as an indication for truth in mathematical pattern tasks. However, scientists including the mathematician David Orrell and physicist Marcelo Gleiser have argued that the emphasis on aesthetic criteria such as symmetry is equally capable of leading scientists astray.

Computational approaches to aesthetics emerged amid efforts to use computer science methods "to predict, convey, and evoke emotional response to a piece of art. It this field, aesthetics is not considered to be dependent on taste but is a matter of cognition, and, consequently, learning. In 1928, the mathematician George David Birkhoff created an aesthetic measure formula_1 as the ratio of order to complexity.

In the 1960s and 1970s, Max Bense, Abraham Moles and Frieder Nake were among the first to analyze links between aesthetics, information processing, and information theory. Max Bense, for example, built on Birkhoff's aesthetic measure and proposed a similar information theoretic measure formula_2, where formula_3 is the redundancy and formula_4 the entropy, which assigns higher value to simpler artworks.

In the 1990s, Jürgen Schmidhuber described an algorithmic theory of beauty. This theory takes the subjectivity of the observer into account and postulates that among several observations classified as comparable by a given subjective observer, the most aesthetically pleasing is the one that is encoded by the shortest description, following the direction of previous approaches. Schmidhuber's theory explicitly distinguishes between that which is beautiful and that which is interesting, stating that interestingness corresponds to the first derivative of subjectively perceived beauty. He supposes that every observer continually tries to improve the predictability and compressibility of their observations by identifying regularities like repetition, symmetry, and fractal self-similarity.

Since about 2005, computer scientists have attempted to develop automated methods to infer aesthetic quality of images. Typically, these approaches follow a machine learning approach, where large numbers of manually rated photographs are used to "teach" a computer about what visual properties are of relevance to aesthetic quality. A study by Y. Li and C.J. Hu employed Birkhoff's measurement in their statistical learning approach where order and complexity of an image determined aesthetic value. The image complexity was computed using information theory while the order was determined using fractal compression. There is also the case of the Acquine engine, developed at Penn State University, that rates natural photographs uploaded by users.

There have also been relatively successful attempts with regard to chess and music. Computational approaches have also been attempted in film making as demonstrated by a software model developed by Chitra Dorai and a group of researchers at the IBM T.J. Watson Research Center. The tool predicted aesthetics based on the values of narrative elements. A relation between Max Bense's mathematical formulation of aesthetics in terms of "redundancy" and "complexity" and theories of musical anticipation was offered using the notion of Information Rate.

Evolutionary aesthetics refers to evolutionary psychology theories in which the basic aesthetic preferences of "Homo sapiens" are argued to have evolved in order to enhance survival and reproductive success. One example being that humans are argued to find beautiful and prefer landscapes which were good habitats in the ancestral environment. Another example is that body symmetry and proportion are important aspects of physical attractiveness which may be due to this indicating good health during body growth. Evolutionary explanations for aesthetical preferences are important parts of evolutionary musicology, Darwinian literary studies, and the study of the evolution of emotion.

As well as being applied to art, aesthetics can also be applied to cultural objects, such as crosses or tools. For example, aesthetic coupling between art-objects and medical topics was made by speakers working for the US Information Agency. Art slides were linked to slides of pharmacological data, which improved attention and retention by 'simultaneous activation of intuitive right brain with rational left'. It can also be used in topics as diverse as cartography, mathematics, gastronomy, fashion and website design.

Guy Sircello has pioneered efforts in analytic philosophy to develop a rigorous theory of aesthetics, focusing on the concepts of beauty, love and sublimity. In contrast to romantic theorists, Sircello argued for the objectivity of beauty and formulated a theory of love on that basis.

British philosopher and theorist of conceptual art aesthetics, Peter Osborne, makes the point that "'post-conceptual art' aesthetic does not concern a particular type of contemporary art so much as the historical-ontological condition for the production of contemporary art in general ...". Osborne noted that contemporary art is 'post-conceptual' in a public lecture delivered in 2010.

Gary Tedman has put forward a theory of a subjectless aesthetics derived from Karl Marx's concept of alienation, and Louis Althusser's antihumanism, using elements of Freud's group psychology, defining a concept of the 'aesthetic level of practice'.

Gregory Loewen has suggested that the subject is key in the interaction with the aesthetic object. The work of art serves as a vehicle for the projection of the individual's identity into the world of objects, as well as being the irruptive source of much of what is uncanny in modern life. As well, art is used to memorialize individuated biographies in a manner that allows persons to imagine that they are part of something greater than themselves.

The philosophy of aesthetics as a practice has been criticized by some sociologists and writers of art and society. Raymond Williams, for example, argues that there is no unique and or individual aesthetic object which can be extrapolated from the art world, but rather that there is a continuum of cultural forms and experience of which ordinary speech and experiences may signal as art. By "art" we may frame several artistic "works" or "creations" as so though this reference remains within the institution or special event which creates it and this leaves some works or other possible "art" outside of the frame work, or other interpretations such as other phenomenon which may not be considered as "art".

Pierre Bourdieu disagrees with Kant's idea of the "aesthetic". He argues that Kant's "aesthetic" merely represents an experience that is the product of an elevated class habitus and scholarly leisure as opposed to other possible and equally valid "aesthetic" experiences which lay outside Kant's narrow definition.

Timothy Laurie argues that theories of musical aesthetics "framed entirely in terms of appreciation, contemplation or reflection risk idealizing an implausibly unmotivated listener defined solely through musical objects, rather than seeing them as a person for whom complex intentions and motivations produce variable attractions to cultural objects and practices".






Ark of the Covenant

The Ark of the Covenant, also known as the Ark of the Testimony or the Ark of God, is believed to have been the most sacred religious relic of the Israelites. It is described as a wooden chest coated in pure gold and topped off by an elaborate golden lid known as the mercy seat. According to the Book of Exodus and First Book of Kings in the Hebrew Bible, the Ark contained the Tablets of the Law, by which God delivered the Ten Commandments to Moses at Mount Sinai. According to the Epistle to the Hebrews in the New Testament, the Book of Exodus, and the Book of Numbers it also contained Aaron's rod and a pot of manna.

The biblical account relates that approximately one year after the Israelites' exodus from Egypt, the Ark was created according to the pattern that God gave to Moses when the Israelites were encamped at the foot of Mount Sinai. Thereafter, the gold-plated acacia chest's staves were lifted and carried by the Levites approximately 2,000 cubits () in advance of the people while they marched. God spoke with Moses "from between the two cherubim" on the Ark's cover.

In 2023, Thomas Römer argued that “the original Ark contained a statue [i.e. a cult image] of Yhwh” and that it was “brought into the Jerusalem temple under Josiah.” He specifically suggested that it “transported two betyles (sacred stones), or two cult image statues symbolizing Yhwh and his female companion Ashera or a statue representing Yhwh alone.” Noegel and Falk argued that the Ark had Egyptian origins, the former arguing that it was retroactively merged into the Exodus narrative.

According to the Book of Exodus, God instructed Moses to build the Ark during his 40-day stay upon Mount Sinai. He was shown the pattern for the tabernacle and furnishings of the Ark, and told that it would be made of shittim wood (also known as acacia wood) to house the Tablets of Stone. Moses instructed Bezalel and Aholiab to construct the Ark.

The Book of Exodus gives detailed instructions on how the Ark is to be constructed. It is to be cubits in length, cubits breadth, and cubits height (approximately ) of acacia wood. Then it is to be gilded entirely with gold, and a crown or molding of gold is to be put around it. Four rings of gold are to be attached to its four corners, two on each side—and through these rings staves of shittim wood overlaid with gold for carrying the Ark are to be inserted; and these are not to be removed.

The biblical account continues that, after its creation by Moses, the Ark was carried by the Israelites during their 40 years of wandering in the desert. Whenever the Israelites camped, the Ark was placed in a separate room in a sacred tent, called the Tabernacle.

When the Israelites, led by Joshua toward the Promised Land, arrived at the banks of the River Jordan, the Ark was carried in the lead, preceding the people, and was the signal for their advance. During the crossing, the river grew dry as soon as the feet of the priests carrying the Ark touched its waters, and remained so until the priests—with the Ark—left the river after the people had passed over. As memorials, twelve stones were taken from the Jordan at the place where the priests had stood.

During the Battle of Jericho, the Ark was carried around the city once a day for six days, preceded by the armed men and seven priests sounding seven trumpets of rams' horns. On the seventh day, the seven priests sounding the seven trumpets of rams' horns before the Ark compassed the city seven times, and, with a great shout, Jericho's wall fell down flat and the people took the city.

After the defeat at Ai, Joshua lamented before the Ark. When Joshua read the Law to the people between Mount Gerizim and Mount Ebal, they stood on each side of the Ark. The Ark was then kept at Shiloh after the Israelites finished their conquest of Canaan. We next hear of the Ark in Bethel, where it was being cared for by the priest Phinehas, the grandson of Aaron. According to this verse, it was consulted by the people of Israel when they were planning to attack the Benjaminites at the Battle of Gibeah. Later the Ark was kept at Shiloh again, where it was cared for by Hophni and Phinehas, two sons of Eli.

According to the biblical narrative, a few years later the elders of Israel decided to take the Ark onto the battlefield to assist them against the Philistines, having recently been defeated at the battle of Eben-Ezer. They were again heavily defeated, with the loss of 30,000 men. The Ark was captured by the Philistines and Hophni and Phinehas were killed. The news of its capture was at once taken to Shiloh by a messenger "with his clothes rent, and with earth upon his head". The old priest, Eli, fell dead when he heard it; and his daughter-in-law, bearing a son at the time the news of the Ark's capture was received, named him Ichabod—explained as "The glory has departed Israel" in reference to the loss of the Ark. Ichabod's mother died at his birth.

The Philistines took the Ark to several places in their country, and at each place misfortune befell them. At Ashdod it was placed in the temple of Dagon. The next morning Dagon was found prostrate, bowed down, before it; and on being restored to his place, he was on the following morning again found prostrate and broken. The people of Ashdod were smitten with tumors; a plague of rodents was sent over the land. This may have been the bubonic plague. The affliction of tumours was also visited upon the people of Gath and of Ekron, whither the Ark was successively removed.

After the Ark had been among them for seven months, the Philistines, on the advice of their diviners, returned it to the Israelites, accompanying its return with an offering consisting of golden images of the tumors and mice wherewith they had been afflicted. The Ark was set up in the field of Joshua the Beth-shemite, and the Beth-shemites offered sacrifices and burnt offerings. Out of curiosity the men of Beth-shemesh gazed at the Ark; and as a punishment, seventy of them (fifty thousand and seventy in some translations) were struck down by the Lord. The Bethshemites sent to Kirjath-jearim, or Baal-Judah, to have the Ark removed; and it was taken to the house of Abinadab, whose son Eleazar was sanctified to keep it. Kirjath-jearim remained the abode of the Ark for twenty years. Under Saul, the Ark was with the army before he first met the Philistines, but the king was too impatient to consult it before engaging in battle. In 1 Chronicles 13:3 it is stated that the people were not accustomed to consulting the Ark in the days of Saul.

In the biblical narrative, at the beginning of his reign over the United Monarchy, King David removed the Ark from Kirjath-jearim amid great rejoicing. On the way to Zion, Uzzah, one of the drivers of the cart that carried the Ark, put out his hand to steady the Ark, and was struck dead by God for touching it. The place was subsequently named "Perez-Uzzah", literally , as a result. David, in fear, carried the Ark aside into the house of Obed-edom the Gittite, instead of carrying it on to Zion, and it stayed there for three months.

On hearing that God had blessed Obed-edom because of the presence of the Ark in his house, David had the Ark brought to Zion by the Levites, while he himself, "girded with a linen ephod[...] danced before the Lord with all his might" and in the sight of all the public gathered in Jerusalem, a performance which caused him to be scornfully rebuked by his first wife, Saul's daughter Michal. In Zion, David put the Ark in the tent he had prepared for it, offered sacrifices, distributed food, and blessed the people and his own household. David used the tent as a personal place of prayer.

The Levites were appointed to minister before the Ark. David's plan of building a temple for the Ark was stopped on the advice of the prophet Nathan. The Ark was with the army during the siege of Rabbah; and when David fled from Jerusalem at the time of Absalom's conspiracy, the Ark was carried along with him until he ordered Zadok the priest to return it to Jerusalem.

According to the Biblical narrative, when Abiathar was dismissed from the priesthood by King Solomon for having taken part in Adonijah's conspiracy against David, his life was spared because he had formerly borne the Ark. Solomon worshipped before the Ark after his dream in which God promised him wisdom.

During the construction of Solomon's Temple, a special inner room, named ('Holy of Holies'), was prepared to receive and house the Ark; and when the Temple was dedicated, the Ark—containing the original tablets of the Ten Commandments—was placed therein. When the priests emerged from the holy place after placing the Ark there, the Temple was filled with a cloud, "for the glory of the Lord had filled the house of the Lord".

When Solomon married Pharaoh's daughter, he caused her to dwell in a house outside Zion, as Zion was consecrated because it contained the Ark. King Josiah also had the Ark returned to the Temple, from which it appears to have been removed by one of his predecessors (cf. 2 Chronicles 33–34 and 2 Kings 21–23).

King Hezekiah is the last biblical figure mentioned as having seen the Ark. Hezekiah is also known for protecting Jerusalem against the Assyrian Empire by improving the city walls and diverting the waters of the Gihon Spring through a tunnel known today as Hezekiah's Tunnel, which channeled the water inside the city walls to the Pool of Siloam.

In a noncanonical text known as the Treatise of the Vessels, Hezekiah is identified as one of the kings who had the Ark and the other treasures of Solomon's Temple hidden during a time of crisis. This text lists the following hiding places, which it says were recorded on a bronze tablet: (1) a spring named Kohel or Kahal with pure water in a valley with a stopped-up gate; (2) a spring named Kotel (or "wall" in Hebrew); (3) a spring named Zedekiah; (4) an unidentified cistern; (5) Mount Carmel; and (6) locations in Babylon.

To many scholars, Hezekiah is also credited as having written all or some of the Book of Kohelet (Ecclesiastes in the Christian tradition), in particular the famously enigmatic epilogue. Notably, the epilogue appears to refer to the Ark story with references to almond blossoms (i.e., Aaron's rod), locusts, silver, and gold. The epilogue then cryptically refers to a pitcher broken at a fountain and a wheel broken at a cistern.

Although scholars disagree on whether the Pool of Siloam's pure spring waters were used by pilgrims for ritual purification, many scholars agree that a stepped pilgrimage road between the pool and the Temple had been built in the first century CE. This roadway has been partially excavated, but the west side of the Pool of Siloam remains unexcavated.

In 587 BC, when the Babylonians destroyed Jerusalem, an ancient Greek version of the biblical third Book of Ezra, 1 Esdras, suggests that Babylonians took away the vessels of the ark of God, but does not mention taking away the Ark:

In Rabbinic literature, the final disposition of the Ark is disputed. Some rabbis hold that it must have been carried off to Babylon, while others hold that it must have been hidden lest it be carried off into Babylon and never brought back. A late 2nd-century rabbinic work known as the states the opinions of these rabbis that Josiah, the king of Judah, stored away the Ark, along with the jar of manna, and a jar containing the holy anointing oil, the rod of Aaron which budded and a chest given to Israel by the Philistines.

The Kohathites were one of the Levite houses from the Book of Numbers. Theirs was the responsibility to care for "the most holy things" in the tabernacle. When the camp, then wandering the Wilderness, set out the Kohathites would enter the tabernacle with Aaron and cover the ark with the screening curtain and "then they shall put on it a covering of fine leather, and spread over that a cloth all of blue, and shall put its poles in place." The ark was one of the items of the tent of meeting that the Kohathites were responsible for carrying.

Samaritan tradition claims that until the split between Samaritanism and Judaism, which arose when the priest Eli stole the Ark of the Covenant and established a rival cult at Shiloh, the Ark of the Covenant had been kept at the sanctuary of YHWH on Mt. Gerizim.

Archaeological evidence shows strong cultic activity at Kiriath-Jearim in the 8th and 7th centuries BC, well after the ark was supposedly removed from there to Jerusalem. In particular, archaeologists found a large elevated podium, associated with the Northern Kingdom and not the Southern Kingdom, which may have been a shrine. Thomas Römer suggests that this may indicate that the ark was not moved to Jerusalem until much later, possibly during the reign of King Josiah (reigned ). He notes that this might explain why the ark featured prominently in the history before Solomon, but not after. Additionally, 2 Chronicles 35:3 indicates that it was moved during King Josiah's reign. However, Yigal Levin argues that there is no evidence that Kiriath-Jearim was a cultic center in the monarchical era or that it ever housed any "temple of the Ark".

Some scholars believe the story of the Ark was written independently around the 8th century BC in a text referred to as the "Ark Narrative" and then incorporated into the main biblical narrative just before the Babylonian exile.

Römer also suggests that the ark may have originally carried sacred stones "of the kind found in the chests of pre-Islamic Bedouins" and speculates that these may have been either a statue of Yahweh or a pair of statues depicting both Yahweh and his companion goddess Asherah. In contrast, Scott Noegel has argued that the parallels between the ark and these practices "remain unconvincing" in part because the Bedouin objects lack the ark's distinctive structure, function, and mode of transportation. Specifically, unlike the ark, the Bedouin chests "contained no box, no lid, and no poles," they did not serve as the throne or footstool of a god, they were not overlaid with gold, did not have kerubim figures upon them, there were no restrictions on who could touch them, and they were transported on horses or camels. 

Noegel suggests that the ancient Egyptian bark is a more plausible model for the Israelite ark, since Egyptian barks had all the features just mentioned. Noegel adds that the Egyptians also were known to place written covenants beneath the feet of statues, proving a further parallel to the placement of the covenantal tablets inside the ark.

The Ark is first mentioned in the Book of Exodus and then numerous times in Deuteronomy, Joshua, Judges, I Samuel, II Samuel, I Kings, I Chronicles, II Chronicles, Psalms, and Jeremiah.

In the Book of Jeremiah, it is referenced by Jeremiah, who, speaking in the days of Josiah, prophesied a future time, possibly the end of days, when the Ark will no longer be talked about or be made use of again:

Rashi comments on this verse that "The entire people will be so imbued with the spirit of sanctity that God's Presence will rest upon them collectively, as if the congregation itself was the Ark of the Covenant."

According to Second Maccabees, at the beginning of chapter 2:

The "mountain from the top of which Moses saw God's promised land" would be Mount Nebo, located in what is now Jordan.

In the New Testament, the Ark is mentioned in the Letter to the Hebrews and the Revelation to St. John. Hebrews 9:4 states that the Ark contained "the golden pot that had manna, and Aaron's rod that budded, and the tablets of the covenant." says the prophet saw God's temple in heaven opened, "and the ark of his covenant was seen within his temple."

In the Gospel of Luke, the author's accounts of the Annunciation and Visitation are constructed using eight points of literary parallelism to compare Mary to the Ark.

The contents of the ark are seen by theologians such as the Church Fathers and Thomas Aquinas as personified by Jesus Christ: the manna as the Holy Eucharist; Aaron's rod as Jesus' eternal priestly authority; and the tablets of the Law, as the Lawgiver himself.

Catholic scholars connect the pregnant, birthing Woman of the Apocalypse from , with the Blessed Virgin Mary, whom they identify as the "Ark of the New Covenant." Carrying the saviour of mankind within her, she herself became the Holy of Holies. This is the interpretation given in the third century by Gregory Thaumaturgus, and in the fourth century by Saint Ambrose, Saint Ephraem of Syria and Saint Augustine. The Catechism of the Catholic Church teaches that Mary is a metaphorical version of the ark: "Mary, in whom the Lord himself has just made his dwelling, is the daughter of Zion in person, the ark of the covenant, the place where the glory of the Lord dwells. She is 'the dwelling of God[...] with men."

Saint Athanasius, the bishop of Alexandria, is credited with writing about the connections between the Ark and the Virgin Mary: "O noble Virgin, truly you are greater than any other greatness. For who is your equal in greatness, O dwelling place of God the Word? To whom among all creatures shall I compare you, O Virgin? You are greater than them all O (Ark of the) Covenant, clothed with purity instead of gold! You are the Ark in which is found the golden vessel containing the true manna, that is, the flesh in which Divinity resides" ("Homily of the Papyrus of Turin").

The Ark is referred to in the Quran (Surah The Heifer: 248):
According to Uri Rubin, the Ark of the Covenant has a religious basis in Islam (and the Baha'i faith), which gives it special significance.

Since its disappearance from the Biblical narrative, there have been a number of claims of having discovered or of having possession of the Ark, and several possible places have been suggested for its location.

2 Maccabees 2:4–10, written around 100 BC, says that the prophet Jeremiah, "being warned by God" before the Babylonian invasion, took the Ark, the Tabernacle, and the Altar of Incense, and buried them in a cave, informing those of his followers who wished to find the place that it should remain unknown "until the time that God should gather His people again together, and receive them unto mercy."

The Ethiopian Orthodox Tewahedo Church claims to possess the Ark of the Covenant in Axum. The Ark is kept under guard in a treasury near the Church of Our Lady Mary of Zion. Replicas of the tablets within the Ark, or "tabots", are kept in every Ethiopian Orthodox Tewahedo Church. Each tabot is kept in its own holy of holies, each with its own dedication to a particular saint; the most popular of these include Saint Mary, Saint George and Saint Michael.

The "Kebra Nagast" is often said to have been composed to legitimise the Solomonic dynasty, which ruled the Ethiopian Empire following its establishment in 1270, but this is not the case. It was originally composed in some other language (Coptic or Greek), then translated into Arabic, and translated into Geʽez in 1321. It narrates how the Ark of the Covenant was brought to Ethiopia by Menelik I with divine assistance, while a forgery was left in the Temple in Jerusalem. Although the "Kebra Nagast" is the best-known account of this belief, the belief predates the document. Abu al-Makarim, writing in the last quarter of the twelfth century, makes one early reference to this belief that they possessed the Ark. "The Abyssinians possess also the Ark of the Covenant", he wrote, and, after a description of the object, describes how the liturgy is celebrated upon the Ark four times a year, "on the feast of the great nativity, on the feast of the glorious Baptism, on the feast of the holy Resurrection, and on the feast of the illuminating Cross."

In his controversial 1992 book "The Sign and the Seal", British writer Graham Hancock reports on the Ethiopian belief that the ark spent several years in Egypt before it came to Ethiopia via the Nile River, where it was kept on the islands of Lake Tana for about four hundred years and finally taken to Axum. Archaeologist John Holladay of the University of Toronto called Hancock's theory "garbage and hogwash"; Edward Ullendorff, a former professor of Ethiopian Studies at the University of London, said he "wasted a lot of time reading it." In a 1992 interview, Ullendorff says that he personally examined the ark held within the church in Axum in 1941 while an officer in the British Army. Describing the ark there, he says, "They have a wooden box, but it's empty. Middle- to late-medieval construction, when these were fabricated ad hoc."

On 25 June 2009, the patriarch of the Orthodox Church of Ethiopia, Abune Paulos, said he would announce to the world the next day the unveiling of the Ark of the Covenant, which he said had been kept safe and secure in a church in Axum. The following day, he announced that he would not unveil the Ark after all, but that instead he could attest to its current status.

The Lemba people of South Africa and Zimbabwe have claimed that their ancestors carried the Ark south, calling it the "ngoma lungundu" or "voice of God", eventually hiding it in a deep cave in the Dumghe mountains, their spiritual home.

On 14 April 2008, in a UK Channel 4 documentary, Tudor Parfitt, taking a literalist approach to the Biblical story, described his research into this claim. He says that the object described by the Lemba has attributes similar to the Ark. It was of similar size, was carried on poles by priests, was not allowed to touch the ground, was revered as a voice of their God, and was used as a weapon of great power, sweeping enemies aside.

In his book "The Lost Ark of the Covenant" (2008), Parfitt also suggests that the Ark was taken to Arabia following the events depicted in the Second Book of Maccabees, and cites Arabic sources which maintain it was brought in distant times to Yemen. Genetic Y-DNA analyses in the 2000s have established a partially Middle-Eastern origin for a portion of the male Lemba population but no specific Jewish connection. Lemba tradition maintains that the Ark spent some time in a place called Sena, which might be Sena in Yemen. Later, it was taken across the sea to East Africa and may have been taken inland at the time of the Great Zimbabwe civilization. According to their oral traditions, some time after the arrival of the Lemba with the Ark, it self-destructed. Using a core from the original, the Lemba priests constructed a new one. This replica was discovered in a cave by a Swedish German missionary named Harald von Sicard in the 1940s and eventually found its way to the Museum of Human Science in Harare.

The Ark of the Covenant was said to have been kept in the Basilica of St. John Lateran, surviving the pillages of Rome by Alaric I and Gaiseric but lost when the basilica burned.

"Rabbi Eliezer ben José stated that he saw in Rome the mercy-seat of the temple. There was a bloodstain on it. On inquiry he was told that it was a stain from the blood which the high priest sprinkled thereon on the Day of Atonement."

Between 1899 and 1902, the British-Israel Association of London carried out limited excavations of the Hill of Tara in Ireland looking for the Ark of the Covenant. The Irish nationalists including Maud Gonne and the Royal Society of Antiquaries of Ireland (RSAI) campaigned successfully to have them stopped before they destroyed the hill. A non-invasive survey by archaeologist Conor Newman carried out from 1992 until 1995 found no evidence of the Ark.

The British Israelites believed that the Ark was located at the grave of the Egyptian princess Tea Tephi, who according to Irish legend came to Ireland in the 6th century BC and married Irish King Érimón. Because of the historical importance of Tara, Irish nationalists like Douglas Hyde and W. B. Yeats voiced their protests in newspapers and in 1902 Maud Gonne led a protest against the excavations at the site.

Philip Kaufman conceived of the Ark of the Covenant as the main plot device of Steven Spielberg's 1981 adventure film "Raiders of the Lost Ark", where it is found by Indiana Jones in the Egyptian city of Tanis in 1936. In early 2020, a prop version made for the film (which does not actually appear onscreen) was featured on television series "Antiques Roadshow".

In the Danish family film "The Lost Treasure of the Knights Templar" from 2006, the main part of the treasure found in the end is the Ark of the Covenant. The power of the Ark comes from static electricity stored in separated metal plates like a giant Leyden jar.

In Harry Turtledove's novel "Alpha and Omega" (2019) the ark is found by archeologists, and the characters have to deal with the proven existence of God.

The Ark has been depicted many times in art for two thousand years, some examples are in the article above, a few more are here.
Yom HaAliyah (Aliyah Day) () is an Israeli national holiday celebrated annually on the tenth of the Hebrew month of Nisan to commemorate the Israelites crossing the Jordan River into the Land of Israel while carrying the Ark of the Covenant.




Angles (tribe)

The Angles were one of the main Germanic peoples who settled in Great Britain in the post-Roman period. They founded several kingdoms of the Heptarchy in Anglo-Saxon England. Their name, which derives from the Anglia Peninsula, is the root of the name "England" ("land of Ængle"). According to Tacitus, writing around 100 AD, a people known as Angles (Anglii) lived east of the Lombards and Semnones, who lived near the Elbe river.

The term Angles comes from , and . The name of the Angles may have been first recorded in Latinised form, as "Anglii", in the "Germania" of Tacitus. It is thought to derive from the name of the area they originally inhabited, the Anglia Peninsula ("Angeln" in modern German, "Angel" in modern Danish), which is located on the Baltic Sea coast of Schleswig-Holstein.

Multiple theories concerning the etymology of the name have been hypothesised: 


During the fifth century, all Germanic tribes who invaded Britain were referred to as either "Englisc", "Ængle" or "Engle", who were all speakers of Old English (which was known as "Englisc", "Ænglisc", or "Anglisc"). "Englisc" and its descendant, "English", also goes back to Proto-Indo-European "*h₂enǵʰ-", meaning narrow.

Pope Gregory I simplified the Latinised name "Anglii" to "Angli" in a letter and this became standard. The country remained "Anglia" in Latin. Alfred the Great's translation of Orosius's history of the world uses "Angelcynn" (-kin) to describe the English people; Bede uses "Angelfolc" (-folk); also such forms as "Engel", "Englan" (the people), "Englaland", and "Englisc" occur, all showing i-mutation.

The earliest known mention of the Angles may be in chapter 40 of Tacitus's "Germania" written around AD 98. Tacitus describes the "Anglii" as one of the more remote Suebic tribes compared to the Semnones and Langobardi, who lived on the Elbe and were better known to the Romans. He grouped the Angles with several other tribes in that region, the Reudigni, Aviones, Varini, Eudoses, Suarines, and Nuithones. These were all living behind ramparts of rivers and woods, and therefore inaccessible to attack.

He gives no precise indication of their geographical situation but states that, together with the six other tribes, they worshipped Nerthus, or Mother Earth, whose sanctuary was located on "an island in the Ocean". The Eudoses are the Jutes; these names probably refer to localities in Jutland or on the Baltic coast. The coast contains sufficient estuaries, inlets, rivers, islands, swamps, and marshes to have been then inaccessible to those not familiar with the terrain, such as the Romans, who considered it unknown, inaccessible, with a small population and of little economic interest.

The majority of scholars believe that the Anglii lived on the coasts of the Baltic Sea, probably in the southern part of the Jutland peninsula. This view is based partly on Old English and Danish traditions regarding persons and events of the fourth century, and partly because striking affinities to the cult of Nerthus as described by Tacitus are to be found in pre-Christian Scandinavian religion.

Surviving versions of the work of Ptolemy, who wrote around AD 150, in his atlas "Geography" (2.10), describes them in a confusing manner. In one passage, the "Sueboi Angeilloi" (in Greek equivalent to Latin spelling "Suevi Angili"), are living in a stretch of land between the northern Rhine and central Elbe, but apparently not touching either river, with the Suebic Langobardi on the Rhine to their west, and the Suebic Semnones on the Elbe stretching to their east. This is unexpected. However, as pointed out by Gudmund Schütte, the Langobards also appear as the "Laccobardi" in another position near the Elbe and the Saxons, which is considered more likely to be correct, and the Angles probably lived in that region also. Owing to the uncertainty of this passage, much speculation existed regarding the original home of the Anglii.

One theory is that they or part of them dwelt or moved among other coastal people, perhaps confederated up to the basin of the Saale (in the neighbourhood of the ancient canton of Engilin) on the Unstrut valleys below the Kyffhäuserkreis, from which region the "Lex Anglorum et Werinorum hoc est Thuringorum" is believed by many to have come. The ethnic names of Frisians and Warines are also attested in these Saxon districts.

A second possible solution is that these Angles of Ptolemy are not those of Schleswig at all. According to Julius Pokorny, the Angri- in Angrivarii, the -angr in Hardanger and the Angl- in Anglii all come from the same root meaning "bend", but in different senses. In other words, the similarity of the names is strictly coincidental and does not reflect any ethnic unity beyond Germanic. Gudmund Schütte, in his analysis of Ptolemy, believes that the Angles have simply been moved by an error coming from Ptolemy's use of imperfect sources. He points out that Angles are placed correctly just to the northeast of the Langobardi, but that these have been duplicated, so that they appear once, correctly, on the lower Elbe, and a second time, incorrectly, at the northern Rhine.

Bede (died 735) stated that the Anglii, before coming to Great Britain, dwelt in a land called Angulus, "which lies between the province of the Jutes and the Saxons, and remains unpopulated to this day." Similar evidence is given by the 9th-century "Historia Brittonum". King Alfred the Great and the chronicler Æthelweard identified this place with Anglia, in the province of Schleswig (Slesvig; though it may then have been of greater extent), and this identification agrees with the indications given by Bede.

In the Norwegian seafarer Ohthere of Hålogaland's account of a two-day voyage from the Oslo fjord to Schleswig, he reported the lands on his starboard bow, and Alfred appended the note "on these islands dwelt the "Engle" before they came hither". Confirmation is afforded by English and Danish traditions relating to two kings named Wermund and Offa of Angel, from whom the Mercian royal family claimed descent and whose exploits are connected with Anglia, Schleswig, and Rendsburg.

Danish tradition has preserved record of two governors of Schleswig, father and son, in their service, Frowinus (Freawine) and Wigo (Wig), from whom the royal family of Wessex claimed descent. During the fifth century, the Anglii invaded Great Britain, after which time their name does not recur on the continent except in the title of the legal code issued to the Thuringians: "Lex Angliorum et Werinorum hoc est Thuringorum".

The Angles are the subject of a legend about Pope Gregory I, who happened to see a group of Angle children from Deira for sale as slaves in the Roman market. As the story was told by Bede, Gregory was struck by the unusual appearance of the slaves and asked about their background. When told they were called "Anglii" (Angles), he replied with a Latin pun that translates well into English: "Bene, nam et angelicam habent faciem, et tales angelorum in caelis decet esse coheredes" (It is well, for they have an angelic face, and such people ought to be co-heirs of the angels in heaven). Supposedly, this encounter inspired the pope to launch a mission to bring Christianity to their countrymen.

The province of Schleswig has proved rich in prehistoric antiquities that date apparently from the fourth and fifth centuries. A large cremation cemetery has been found at Borgstedt, between Rendsburg and Eckernförde, and it has yielded many urns and brooches closely resembling those found in pagan graves in England. Of still greater importance are the great deposits at Thorsberg moor (in Anglia) and Nydam, which contained large quantities of arms, ornaments, articles of clothing, agricultural implements, etc., and in Nydam, even ships. By the help of these discoveries, Angle culture in the age preceding the invasion of Britannia can be pieced together.

According to sources such as the "History" of Bede, after the invasion of Britannia, the Angles split up and founded the kingdoms of Northumbria, East Anglia, and Mercia. H. R. Loyn has observed in this context that "a sea voyage is perilous to tribal institutions", and the apparently tribe-based kingdoms were formed in England. Early times had two northern kingdoms (Bernicia and Deira) and two midland ones (Middle Anglia and Mercia), which had by the seventh century resolved themselves into two Angle kingdoms, viz., Northumbria and Mercia. 

Northumbria held suzerainty amidst the Teutonic presence in the British Isles in the 7th century, but was eclipsed by the rise of Mercia in the 8th century. Both kingdoms fell in the great assaults of the Danish Viking armies in the 9th century. Their royal houses were effectively destroyed in the fighting, and their Angle populations came under the Danelaw. Further south, the Saxon kings of Wessex withstood the Danish assaults. Then in the late 9th and early 10th centuries, the kings of Wessex defeated the Danes and liberated the Angles from the Danelaw. 

They united their house in marriage with the surviving Angle royalty, and were accepted by the Angles as their kings. This marked the passing of the old Anglo-Saxon world and the dawn of the "English" as a new people. The regions of East Anglia and Northumbria are still known by their original titles. Northumbria once stretched as far north as what is now southeast Scotland, including Edinburgh, and as far south as the Humber estuary and even the river Witham.

The rest of that people stayed at the centre of the Angle homeland in the northeastern portion of the modern German "Bundesland" of Schleswig-Holstein, on the Jutland Peninsula. There, a small peninsular area is still called Anglia today and is formed as a triangle drawn roughly from modern Flensburg on the Flensburger Fjord to the City of Schleswig and then to Maasholm, on the Schlei inlet.

Sources

Attribution:

Aster CT-80

The Aster CT-80 is a 1982 personal computer developed by the small Dutch company MCP (later renamed to Aster Computers), was sold in its first incarnation as a kit for hobbyists. Later it was sold ready to use. It consisted of several Eurocard PCB's with DIN 41612 connectors, and a backplane all based on a 19-inch rack configuration. It was the first commercially available Dutch personal/home computer. The Aster computer could use the software written for the popular Tandy TRS-80 computer while fixing many of the problems of that computer, but it could also run CP/M software, with a large amount of free memory Transient Program Area, (TPA) and a full 80×25 display, and it could be used as a Videotext terminal. Although the Aster was a clone of the TRS-80 Model I it was in fact more compatible with the TRS-80 Model III and ran all the software of these systems including games. It also had a built-in speaker which was compatible with such games software.

Three models were sold. The first model (launched June 1982) looked like the IBM PC, a rectangular base unit with two floppy drives on the front, and a monitor on top with a separate detachable keyboard. The second incarnation was a much smaller unit the width of two 5" floppy drives stacked on top of each other, and the third incarnation looked like a flattened Apple with a built-in keyboard.

All units ran much faster than the original TRS-80, at 4 MHz, (with a software selectable throttle to the original speed for compatibility purposes) and the display supported upper and lower case, hardware snow suppression (video ram bus arbitration logic), and an improved character font set. The floppy disk interface supported dual density, and disk capacities up to 800 KB, more than four times the capacity of the original TRS-80. A special version of NewDos/80, (an improved TRS-DOS compatible Disk operating system) was used to support these disk capacities when using the TRS-80 compatibility mode.

For the educational market a version of the first model was produced with a new plastic enclosure (the First Asters had an all-metal enclosure) that also had an opening on the top in which a cassette recorder could be placed. This model was used in a cluster with one Aster (with disk drives) for the teacher, and eight disk less versions for the pupils. The pupils could download software from the teachers computer through a network based on a fast serial connection, as well as sending back their work to the teachers computer. There was also hardware in place through which the teacher could see the display of each pupils screen on his own monitor.

The Aster used 64 KB of RAM and had the unique feature of supporting two fundamentally different internal architectures: when turned on without a boot floppy or with a TRS-DOS floppy, the Aster would be fully TRS-80 compatible, with 48 KB of RAM. When the boot loader detected a CP/M floppy, the Aster would reconfigure its internal memory architecture on the fly to optimally support CP/M with 60 KB free RAM for programs (TPA) and an 80 x 25 display. This dual-architecture capability only existed on one other TRS-80 clone, the LOBO Max-80.

With a special configuration tool, the CT-80 could reconfigure its floppy drivers to read and write the floppies of about 80 other CP/M systems.

A third mode was entered with a special boot floppy which turned the Aster into a Videotex terminal with a 40x25 display and a Videotex character set, The software used the built in RS-232 interface of the Aster to control a modem through which it could contact a Prestel service provider.

Most Aster CT-80's (about 10 thousand of them) were sold to schools for computer education, in a project first known as the "honderd scholen project" (one hundred schools project), but which later involved many more than just one hundred schools. MCP received this order from the Dutch government because their computer met all the technical and other demands, including the demand that the computers should be of Dutch origin and should be built in the Netherlands. Another important demand was that the computers could be used in a network (Aster developed special software and hardware for that). Later however the Government turned around and gave 50% of the order to Philips and their P2000 homecomputer even though the P2000 did not meet all the technical demands, was made in Austria and did not have network hardware nor software.

Aster computers was based in the small town of Arkel near the town of Gorinchem.
Initially Aster computer b.v. was called MCP (Music print Computer Product), because it was specialized in producing computer assisted printing of sheet music. The director of the company was interested in Microprocessor technology and noticed there was a market for selling kits to computer building amateurs, so they started selling electronic kits to hobbyists, and employed four persons at that time . They also assembled kits for people without soldering skills, especially the "junior Computer" from Elektor (a copy of the KIM-1), and the ZX80 from Sinclair. Among the kits sold there were also alternative floppy disk drives for TRS-80 computers. But these needed the infamous TRS-80 expansion interface, which was very expensive, and had a very unreliable floppy disk controller because it used the WD1771 floppy disk controller chip without an external "data separator". To fix this problem MCP developed a small plugin board which could be plugged into the socket for the WD1771, and which contained a data separator, and a socket for the WD1791 to support dual-density operation. Still, the expansion interface was expensive and due to its design it was also unreliable. So they decided to also develop their own alternative in the form of an improved floppy disk controller and printer interface that could be built right into a floppy disk enclosure. The lack of RAM expansion offered by this solution was solved by a service in which the 16 KB RAM chips inside the base unit would be replaced by 64 KB RAM chips.
While this went on MCP renamed itself to "MCP CHIP" but ran into problems with the German computer magazine "CHIP", and had to return to its former name. At that time MCP did also sell imported home computers like the TRS-80, the Video Genie (another TRS-80 clone), the Luxor ABC 80 and the Apple II.
They also sold the exotic Olivetti M20, a very early 16-bit personal computer that was one of the very few systems to use a Z8000 CPU.

After designing their own fully functional replacement for the TRS-80 expansion interface (which was never commercialized) the company realized that they could do better than just re-designing the expansion interface. They observed that the TRS-80 was a great computer but it lacked in several areas. The display logic and resulting display 'snow' was irritating, as was the missing lower case support, the CPU speed could be improved, the quality and layout of the keyboard was bothersome, and the floppy disk capacity and reliability was low. Also the more interesting software offered for CP/M systems could not run well on a TRS-80. So they decided to design a TRS-80 and CP/M software-compatible computer system, which (following the lead of Apple Computer) they decided to name after a "typical Dutch flower". So they called it the Aster CT-80 (CP/M/Tandy-1980). Why they went with Aster, and not the more well known Tulip is unknown, perhaps they thought it would be to presumptuous, or perhaps the fact that "Aster" is also a Dutch girls' name has something to do with it. Remarkably "Aster" was also the name given to a Dutch Supercomputer much later, in 2002.

The first version of the Aster consisted of four "Eurocards", one Z80 CPU card with 64 KB memory, one Motorola MC6845-based video card, one double density floppy disk controller card and one "keyboard/RS-232/cassette interface" card. Plus a "backplane card", (which connected all the other cards) and a keyboard. And was intended for hobbyists, to be sold as a kit consisting of the parts and the PCB's for the computer and attached keyboard. After selling a few kits, MCP became convinced there was a much bigger market for an improved model sold as a completed working system. However the original kit version lacked many features that prevented its use as a serious computer system. Because the original designer had left the company another employee completely redesigned most of the system, (adding a display snow remover circuit, true 80/64 column text mode support, (with different size letters for TRS-80 and CP/M mode, so that in TRS-80 mode the full screen was also used, not just a 64×16 portion of the 80×25 screen) with an improved font set (adding "gray scale" version of the TRS-80 mozaik graphics and many special PETSCII like characters), and a more flexible and reliable floppy disk controller and keyboard interface plus many other small improvements), also an enclosure was developed for the main computer system, (in the form of a 19-inch rack for the Eurocards) and for two floppy disk drives and the power supply. A software engineer was hired to write the special "dual boot mode" BIOS and the special CP/M BIOS. The "dual boot mode" BIOS actually discovered whether a TRS-DOS, or Aster CP/M disk was placed in the drive, and would, depending on the type of disk, reorganise the internal memory architecture of the system, to either be 100% TRS-80 compatible or optimally support CP/M, with as much "workspace" as possible, and the 80×25 video mode. It also was responsible for switching to ROM BASIC when the system was turned on with the break key pressed, and later supported a primitive LAN system, using the RS-232 port with modified cabling. The very first of the ready made computers were sold with the "kit" versions of the euro cards, the version with redesigned cards came a month or so later.

Soon the little shop became much too small and they moved to a much larger factory building nearby (formerly a window glass factory), and started mass-producing the Aster for a period of a few years, in which time its staff grew twentyfold.

After the Aster having been a few years on the Market Tandy released its own improved model, the TRS-80 Model III computer which solved many of the same problems that the Aster also had solved, but the model 3 still did not fully support CP/M as the Aster did. In the meantime IBM had released its original IBM PC, which incidentally looked remarkably like the Asters base with floppy drives + separate keyboard set-up.

The Aster was chosen for Dutch schools by the Dutch ministry of education, in a set-up with eight disk-less Asters, and one Aster with high-capacity floppy drives all connected by a LAN based on the Aster's high-speed serial port hardware, and special cables that permitted that any single computer on the LAN could broadcast to all other computers. The floppy based system was operated by the teacher who could send programs from his floppy disk, and data, to the student's disk-less systems thanks to the special BIOS in those systems. The students could send programs and data back to the teacher through the same LAN, or could save to a cassette recorder built into the disk-less units. Through a special "video-switch" the teacher was also able to see a copy of each student's display on his own screen. About a thousand of such systems were sold for many hundreds of Dutch schools.

Because of cash flow problems (resulting from growing too fast, insufficient financial backing, technical problems, and a sudden problem with Z80 processor deliveries) the company suddenly folded even before it came to full fruition.

Perhaps the Aster computer inspired another Dutch computer firm to name their computer after another typical Dutch flower—the Tulip's Tulip System-1 which appeared about the same time Aster folded.

Most of the engineers who designed the hardware and software of the Aster went on to design hardware and software for the (then new) MSX system for a company called "Micro Technology b.v.".

To enhance and modernize the Aster CT-80 the company also designed three alternative video display adapters to supplement or replace the TRS-80 compatible video card, (due to the modular nature of the Aster it was simply a matter of changing the video card, and/or CPU card to upgrade the system):

A hard disk interface was also in the works, which would, add a SCSI interface, and the necessary software. A working prototype was developed that added a 40MB hard disk.

On the software front, work was being done to implement the replacement for the aging "user interface" of CP/M, (the Command Console Processor CCP) with the more modern ZCPR.

Finally a replacement for the aging Z80 processor was being developed in the form of an Intel 8086 board, and additional 512K 16 bit memory boards. Such replacements of CPU and memory system components were possible because the Aster CT-80 was designed to use a backplane that was designed to support both 8 and 16 bit processors, and used a modular Eurocard based design with slots to spare for expansion. In theory the system could support the Z80 and the 8086 simultaneously. Plans were formulated to support CP/M-86 and even MS-DOS.

Unfortunately none of these extensions to the system became available because the company folded before any of them could be released.


Arthur Wellesley

Arthur Wellesley may refer to:



Lists of animated television series

These are lists of animated television series. Animated television series are television programs produced by means of animation. Animated series produced for theaters are not included in this lists; for those, see List of animated short film series. These lists include compilation series of theatrical shorts such as "The Bugs Bunny Show" since they often feature some new wrap-around animation.




Atlanta Braves

The Atlanta Braves are an American professional baseball team based in the Atlanta metropolitan area. The Braves compete in Major League Baseball (MLB) as a member club of the National League (NL) East Division. The Braves were founded in Boston, Massachusetts, in 1871, as the Boston Red Stockings. The club was known by various names until the franchise settled on the Boston Braves in 1912. The Braves are the oldest continuously operating professional sports franchise in North America.

After 81 seasons and one World Series title in Boston, the club moved to Milwaukee, Wisconsin, in 1953. With a roster of star players such as Hank Aaron, Eddie Mathews, and Warren Spahn, the Milwaukee Braves won the World Series in 1957. Despite the team's success, fan attendance declined. The club's owners moved the team to Atlanta, Georgia, in 1966.

The Braves did not find much success in Atlanta until 1991. From 1991 to 2005, the Braves were one of the most successful teams in baseball, winning an unprecedented 14 consecutive division titles, making an MLB record eight consecutive National League Championship Series appearances, and producing one of the greatest pitching rotations in the history of baseball including Hall of Famers Greg Maddux, John Smoltz, and Tom Glavine.

The Braves are one of the two remaining National League charter franchises that debuted in 1876. The club has won an MLB record 23 divisional titles, 18 National League pennants, and four World Series championships. The Braves are the only Major League Baseball franchise to have won the World Series in three different home cities. At the end of the 2023 season, the Braves' overall win–loss record is (). Since moving to Atlanta in 1966, the Braves have an overall win–loss record of 4,761–4,388–8 () through the end of 2023.

The Cincinnati Red Stockings, formed in 1869, were the first openly all-professional baseball team but disbanded after the 1870 season. Manager Harry Wright and players moved to Boston, forming the "Boston Red Stockings", a charter team in the National Association of Professional Base Ball Players (NAPBBP). Led by the Wright brothers, Ross Barnes, and Al Spalding, they dominated the National Association, winning four of five championships. The original Boston Red Stockings team and its successors can lay claim to being the oldest continuously playing franchise in American professional sports.

The club was known as the Boston Red Caps when they played the first National League game in 1876, winning against the Philadelphia Athletics. Despite a weaker roster in the league's first year, they rebounded to secure the 1877 and 1878 pennants. Managed by Frank Selee, they were a dominant force in the 19th century, winning eight pennants. By 1898 the team was known as the Beaneaters and they won 102 games, with stars like Hugh Duffy, Tommy McCarthy, and "Slidin'" Billy Hamilton.

In 1901, the American League was introduced, causing many of Beaneaters players including stars Duffy and Jimmy Collins to leave for clubs of the rival league. The team struggled, having only one winning season from 1900 to 1913 and losing 100 games five times. In 1907, they temporarily dropped the red color from their stockings due to infection concerns. The club underwent various nickname changes until becoming the Braves before the 1912 season. The president of the club, John M. Ward named the club after the owner, James Gaffney. Gaffney was called one of the "braves" of New York City's political machine, Tammany Hall, which used a Native American chief as their symbol.

In 1914, the Boston Braves experienced a remarkable turnaround in what would become one of the most memorable seasons in baseball history. Starting with a dismal 4–18 record, the Braves found themselves in last place, trailing the league-leading New York Giants by 15 games after losing a doubleheader to the Brooklyn Robins on July 4. However, the team rebounded with an incredible hot streak, going 41–12 from July 6 to September 5. On August 3, Joseph Lannin the president of the Red Sox, offered Fenway Park to the Braves free of charge for the remainder of the season since their usual home, the South End Grounds, was too small. On September 7 and 8, they defeated the Giants in two out of three games, propelling them into first place. Despite being in last place as late as July 18, the Braves secured the pennant, becoming the only team under the old eight-team league format to achieve this after being in last place on the Fourth of July. They were in last place as late as July 18, but were close to the pack, moving into fourth on July 21 and second place on August 12.

The Braves entered the 1914 World Series led by captain and National League Most Valuable Player, Johnny Evers. The Boston club were slight underdogs against Connie Mack's Philadelphia A's. However, they swept the Athletics and won the world championship. Inspired by their success, owner Gaffney constructed a modern park, Braves Field, which opened in August 1915 and was the largest park in the majors at the time, boasting 40,000 seats and convenient public transportation access.

From 1917 to 1933, the Boston Braves struggled. After a series of different owners, Emil Fuchs bought the team in 1923. Fuchs brought his longtime friend, pitching great Christy Mathewson, back into the game. However, the death of pitching legend in 1925 left Fuchs in control. Despite Fuchs' commitment to success, the team faced challenges overcoming the damage from previous years. It wasn't until 1933 and 1934, under manager Bill McKechnie, that the Braves became competitive, but it did little to help the club's finances.

In an effort to boost fan attendance and finances, Fuchs orchestrated a deal with the New York Yankees to acquire Babe Ruth in 1935. Ruth was appointed team vice president with promises of profit shares and managerial prospects. Initially, Ruth seemed to provide a spark on opening day, but his declining skills became evident. Ruth's inability to run and poor fielding led to internal strife, and it became clear that his titles were symbolic. Ruth retired on June 1, 1935, shortly after hitting his last three home runs. The Braves finished the season with a dismal 38–115 record, marking the franchise's worst season.

Fuchs lost control of the team in August 1935, leading to a rebranding attempt as the Boston Bees, but it did little to alter the team's fortune. Construction magnate Lou Perini took over, eventually restoring the Braves' name. Despite World War II causing a brief setback, the team, led by pitcher Warren Spahn, enjoyed impressive seasons in 1946 and 1947 under Perini's ownership.
In 1948, the team won the pennant, behind the pitching of Spahn and Johnny Sain. The remainder of the rotation was so thin that in September, "Boston Post" writer Gerald Hern wrote this poem about the pair:

The poem received such a wide audience that the sentiment, usually now paraphrased as ""Spahn and Sain and pray for rain"", entered the baseball vocabulary.

The 1948 World Series, which the Braves lost in six games to the Indians, turned out to be the Braves' last hurrah in Boston. On March 13, 1953, Perini announced he was moving the club to Milwaukee. Perini cited advent of television and the lack of enthusiasm for the Braves in Boston as the key factors in deciding to move the franchise.

The Milwaukee Braves' relocation to Wisconsin in 1953 was initially a triumphant success, as they drew a then-National League record of 1.8 million fans and finished their inaugural season second in the National League. Manager Charlie Grimm was named NL Manager of the Year following the Braves improvement.

Throughout the 1950s, the Braves became increasingly competitive; driven by sluggers Eddie Mathews and Hank Aaron, the team won or nearly won four straight pennants between 1956 and 1959. In 1957, Aaron's MVP season led the Braves to their first pennant in nine years, securing a World Series victory against the formidable New York Yankees. Despite a strong start in the World Series rematch the following season, the Braves ultimately lost the last three games and the World Series. The 1959 season ended in a tie with the Los Angeles Dodgers, leading to a playoff loss for the Braves. The ensuing years saw fluctuating success, including the Braves finishing fifth in 1963, their first time in the "second division." The team's owner, Louis Perini, sold the Braves to a Chicago-based group led by William Bartholomay in 1962. Despite plans to move to Atlanta in 1965, legal hurdles kept the Braves in Milwaukee for one more season before they completed the relocation in 1966.

After arriving in Atlanta in 1966, the Braves found success in 1969, with the onset of divisional play by winning the first-ever National League West Division title. In the National League Championship Series the Braves were swept by the "Miracle Mets." They would not be a factor during the next decade, posting only two winning seasons between 1970 and 1981. Fans in Atlanta had to be satisfied with the achievements of Hank Aaron, who by the end of the 1973 season, had hit 713 home runs, one short of Ruth's record. On April 4, opening day of the next season, he hit No. 714 in Cincinnati, and on April 8, in front of his home fans and a national television audience, he finally beat Ruth's mark with a home run to left-center field off left-hander Al Downing of the Los Angeles Dodgers. Aaron spent most of his career as a Milwaukee and Atlanta Brave before being traded to the Milwaukee Brewers on November 2, 1974.

In 1976, the team was purchased by media magnate Ted Turner, owner of superstation WTBS, as a means to keep the team (and one of his main programming staples) in Atlanta. Turner used the Braves as a major programming draw for his fledgling cable network, making the Braves the first franchise to have a nationwide audience and fan base. WTBS marketed the team as "The Atlanta Braves: America's Team", a nickname that still sticks in some areas of the country, especially the South. The financially strapped Turner used money already paid to the team for their broadcast rights as a down-payment. Turner quickly gained a reputation as a quirky, hands-on baseball owner. On May 11, 1977, Turner appointed himself manager, but because MLB passed a rule in the 1950s barring managers from holding a financial stake in their teams, Turner was ordered to relinquish that position after one game (the Braves lost 2–1 to the Pittsburgh Pirates to bring their losing streak to 17 games).

The Braves didn't enjoy much success between 1978 and 1990, however, in the 1982 season, led by manager Joe Torre, the Braves secured their first divisional title since 1969. The team was led by standout performances from key players like Dale Murphy, Bob Horner, Chris Chambliss, Phil Niekro, and Gene Garber. The Braves were swept in the NLCS in three games by the Cardinals. Murphy won the Most Valuable Player award for the National League in 1982 and 1983.

From 1991 to 2005, the Atlanta Braves enjoyed a remarkable era of success in baseball, marked by a record-setting 14 consecutive division titles, five National League pennants, and a World Series championship in 1995. Bobby Cox returned as manager in 1990, leading the team's turnaround after finishing the previous season with the worst record in baseball. Notable developments included the drafting of Chipper Jones in 1990 and the hiring of general manager John Schuerholz from the Kansas City Royals.

The Braves' remarkable journey began in 1991, known as the "Worst to First" season. Overcoming a shaky start, the Braves bounced back led by young pitchers Tom Glavine and John Smoltz. The team secured the NL pennant in a memorable playoff race, ultimately losing a closely contested World Series to the Minnesota Twins. The following year, the Braves won the NLCS in dramatic fashion against the Pirates but fell short in the World Series against the Toronto Blue Jays.

In 1993, the Braves strengthened their pitching staff with the addition of Cy Young Award winner Greg Maddux. Despite posting a franchise-best 104 wins, they lost in the NLCS to the Philadelphia Phillies. The team moved to the Eastern Division in 1994, sparking a heated rivalry with the New York Mets.

The player's strike cut short the 1994 season just before the division championships, but the Braves rebounded in 1995, defeating the Cleveland Indians to win the World Series. With this World Series victory, the Braves became the first team in Major League Baseball to win world championships in three different cities. The Braves reached the World Series in 1996 and 1999 but faced defeat both times against the New York Yankees.

In 1996, Time Warner acquired Ted Turner's Turner Broadcasting System, including the Braves. Despite their continued success with a ninth consecutive division title in 2000, the Braves faced postseason disappointment with a sweep by the St. Louis Cardinals in the NLDS. The team won division titles from 2002 to 2004 but experienced early exits in the NLDS each year.

In December 2005, Time Warner, put the club up for sale, leading to negotiations with Liberty Media. After over a year of talks, a deal was reached in February 2007 for Liberty Media to acquire the Braves for $450 million, a magazine publishing company, and $980 million in cash. The sale, valued at approximately $1.48 billion, was contingent on approval from 75 percent of MLB owners and Commissioner Bud Selig.

Bobby Cox's final year as manager in 2010 saw the Braves return to the postseason for the first time since 2005. The team secured the NL Wild Card but fell to the San Francisco Giants in the National League Division Series in four closely contested games, marking the conclusion of Bobby Cox's managerial career. The following season the Braves suffered a historic September collapse to miss the postseason. The club bounced back in 2012 and returned to the postseason in Chipper Jones' final season. The Braves won 94 games in 2012, but that wasn't enough to win the NL East, so they faced the St. Louis Cardinals in the inaugural Wild Card Game. Chipper Jones last game was a memorable one. The Braves lost the one game playoff 6–3, but the game would be remembered for a controversial infield fly call that helped end a Braves rally in the 8th inning.

In 2017, the Atlanta Braves began playing at Truist Park, replacing Turner Field as their home stadium. Following an MLB investigation into international signing rule violations, general manager John Coppolella resigned and faced a baseball ban. Alex Anthopoulos took over as the new general manager. The team's chairman, Terry McGuirk, apologized for the scandal and expressed confidence in Anthopoulos' integrity. A new on field mascot named Blooper was introduced at a fan event before the 2017 season. Under Anthopoulos, the Braves made the playoffs in six of his first seven seasons. In 2020 the Braves reach the National League Championship Series, but ultimately lost to the Dodgers after leading 3–1.

The following season the Braves got revenge against the Dodgers in the 2021 NLCS to advance to the World Series for the first time since 1999, thereby securing their first pennant in 22 years. They defeated the Houston Astros in six games to win their fourth World Series title.

The Braves logos have evolved over the years, featuring a Native American warrior from 1945 to 1955, followed by a laughing Native American with a mohawk and a feather from 1956 to 1965. The modern logo, introduced in 1987, includes the cursive word "Braves" with a tomahawk below it.
Uniform changes occurred in 1987, with the team adopting uniforms reminiscent of their 1950s classic look. For the 2023 season, the Braves had four uniform combinations, including the classic white home and gray road uniforms, a navy blue road jersey for alternate games, and two alternate uniforms for home games - a Friday night red uniform and a City Connect uniform worn on Saturdays, paying tribute to Hank Aaron. The City Connect uniform features "The A" across the chest, accompanied by a cap with the "A" logo and 1974 uniform colors.

Over the 120 years since the inception of the World Series (119 total World Series played), the Braves franchise has won a total of four World Series Championships, with at least one in each of the three cities they have played in.

The Atlanta Braves home ballpark has been Truist Park since 2017. Truist Park is located approximately 10 miles (16 km) northwest of downtown Atlanta in the unincorporated community of Cumberland, in Cobb County, Georgia. The team played its home games at Atlanta–Fulton County Stadium from 1966 to 1996, and at Turner Field from 1997 to 2016. The Braves opened Truist Park on April 14, 2017, with a four-game sweep of the San Diego Padres. The park received positive reviews. Woody Studenmund of the Hardball Times called the park a "gem" saying that he was impressed with "the compact beauty of the stadium and its exciting approach to combining baseball, business and social activities." J.J. Cooper of Baseball America praised the "excellent sight lines for pretty much every seat."

Since 2019, the Braves have played spring training games at CoolToday Park in North Port, Florida. The ballpark opened on March 24, 2019, with the Braves' 4–2 win over the Tampa Bay Rays. The Braves left Champion Stadium, their previous Spring Training home near Orlando to reduce travel times and to get closer to other teams' facilities. CoolToday Park also serves as the Braves' year round rehabilitation facility.

(*) – There were no fans allowed in any MLB stadium in 2020 due to the COVID-19 pandemic.

Although their first major confrontation occurred when the Mets swept the Braves in the 1969 NLCS, the rivalry did not become especially heated until the 1994 season when division realignment put both the Mets and the Braves in the National League East division. 

The Braves faced the Mets in the 1999 National League Championship Series. The Braves initially took a 3-0 series lead, seemingly on the verge of a sweep, but the Mets rallied in Game 4 and Game 5. Despite the Mets' resilience, the Braves eventually won the series in Game 6 with Andruw Jones securing a dramatic walk-off walk, earning their 5th National League pennant of the decade. In 2022, the Braves and Mets, both finished with 101 wins. The National League East title and a first-round bye came down to a crucial three-game series at Truist Park from September 30 to October 2. The Mets entered with a slight lead but faltered as the Braves swept the series. Atlanta claimed the NL East division title and first-round bye, by winning the season series against the Mets.

In addition to having strong fan support in the Atlanta metropolitan area and the state of Georgia, the Braves are often referred to as "America's Team" in reference to the team's games being broadcast nationally on TBS from the 1970s until 2007, giving the team a nationwide fan base.

The Braves boast heavy support within the Southeastern United States particularly in states such as Mississippi, Alabama, South Carolina, North Carolina, Tennessee and Florida.

In 1991, fans of the Atlanta Braves popularized the "tomahawk chop" during games. The use of foam tomahawks drew criticism from Native American groups, deeming it demeaning. Despite protests, the Braves' public relations director defended it as a "proud expression of unification and family." The controversy resurfaced in 2019 when Cherokee Nation member and St. Louis Cardinals pitcher Ryan Helsley found the chop insulting, prompting the Braves to modify their in-game experience. During the off-season, discussions ensued with Native American representatives, and amid pressure in 2020 to change their name, the Braves announced ongoing talks about the chop but insisted the team name would remain unchanged.

The debate over the tomahawk chop continued into 2021. While some Native American leaders, like Richard Sneed, the Principal Chief of the Eastern Band of Cherokee Indians, expressed personal indifference or tolerance, acknowledging it as an acknowledgment of Native American strength, others vehemently opposed it. Sneed emphasized larger issues facing Native American communities and questioned the focus on the chop. The Eastern Cherokee Band of Indians and the Braves initiated efforts to incorporate Cherokee language and culture into the team's activities, stadium, and merchandise, aiming for greater cultural sensitivity despite differing opinions within the Native American community.

The Braves have retired eleven numbers in the history of the franchise, including most recently Andruw Jones' number 25 in 2023, Chipper Jones' number 10 in 2013, John Smoltz's number 29 in 2012, Bobby Cox's number 6 in 2011, Tom Glavine's number 47 in 2010, and Greg Maddux's number 31 in 2009. Additionally, Hank Aaron's 44, Dale Murphy's 3, Phil Niekro's 35, Eddie Mathews' 41, Warren Spahn's 21 and Jackie Robinson's 42, which is retired for all of baseball with the exception of Jackie Robinson Day, have also been retired. The color and design of the retired numbers reflect the uniform design at the time the person was on the team, excluding Robinson.

Of the eleven Braves whose numbers have been retired, all who are eligible for the National Baseball Hall of Fame have been elected with the exceptions of Dale Murphy and Andruw Jones.

On April 3, 2023, the Braves announced that they will retire number 25 in honor of former centerfielder Andruw Jones on September 9.

The Atlanta Braves farm system consists of six minor league affiliates.

The Braves regional games are exclusively broadcast on Bally Sports Southeast. Brandon Gaudin is the play-by-play announcer for Bally Sports Southeast. Gaudin is joined in the booth by lead analyst C.J. Nitkowski. Jeff Francoeur and Tom Glavine will also join the broadcast for a few games during the season. Peter Moylan, Nick Green, and John Smoltz also appear in the booth for select games as in-game analysts.

The radio broadcast team is led by the tandem of play-by-play announcer Ben Ingram and analyst Joe Simpson. Braves games are broadcast across Georgia and seven other states on at least 172 radio affiliates, including flagship station 680 The Fan in Atlanta and stations as far away as Richmond, Virginia; Louisville, Kentucky; and the US Virgin Islands. The games are carried on at least 82 radio stations in Georgia.



Atari ST

Atari ST is a line of personal computers from Atari Corporation and the successor to the Atari 8-bit family. The initial model, the Atari 520ST, had limited release in April–June 1985 and was widely available in July. It was the first personal computer with a bitmapped color GUI, using a version of Digital Research's GEM from February 1985. The Atari 1040ST, released in 1986 with 1 MB of RAM, was the first home computer with a cost-per-kilobyte of less than US$1.

After Jack Tramiel purchased the assets of the Atari, Inc. consumer division to create Atari Corporation, the 520ST was designed in five months by a small team led by Shiraz Shivji. Alongside the Macintosh, Amiga, Apple IIGS, and Acorn Archimedes, the ST is part of a mid-1980s generation of computers with 16- or 32-bit processors, 256 KB or more of RAM, and mouse-controlled graphical user interfaces. "ST" officially stands for "Sixteen/Thirty-two", referring to the Motorola 68000's 16-bit external bus and 32-bit internals.

The ST was sold with either Atari's color monitor or less expensive monochrome monitor. Color graphics modes are available only on the former while the highest-resolution mode requires the monochrome monitor. Some models can display the color modes on a TV. In Germany and some other markets, the ST gained a foothold for CAD and desktop publishing. With built-in MIDI ports, it was popular for music sequencing and as a controller of musical instruments among amateur and professional musicians. The primary competitor of the Atari ST was the Amiga from Commodore. 

The 520ST and 1040ST were followed by the Mega series, the STE, and the portable STacy. In the early 1990s, Atari released three final evolutions of the ST with significant technical differences from the original models: TT030 (1990), Mega STE (1991), and Falcon (1992). Atari discontinued the entire ST computer line in 1993, shifting the company's focus to the Jaguar video game console.

The Atari ST was born from the rivalry between home computer makers Atari, Inc. and Commodore International. Jay Miner, one of the designers of the custom chips in the Atari 2600 and Atari 8-bit family, tried to convince Atari management to create a new chipset for a video game console and computer. When his idea was rejected, he left Atari to form a small think tank called Hi-Toro in 1982 and began designing the new "Lorraine" chipset.

Amiga ran out of capital to complete Lorraine's development, and Atari, by then owned by Warner Communications, paid Amiga to continue its work. In return, Atari received exclusive use of the Lorraine design for one year as a video game console. After that time, Atari had the right to add a keyboard and market the complete computer, designated the 1850XLD.

After leaving Commodore International in January 1984, Jack Tramiel formed Tramel (without an "i") Technology, Ltd. with his sons and other ex-Commodore employees and, in April, began planning a new computer. Interested in Atari's overseas manufacturing and worldwide distribution network, Tramiel negotiated with Warner in May and June 1984. He secured funding and bought Atari's consumer division, which included the console and home computer departments, in July. As executives and engineers left Commodore to join Tramel Technology, Commodore responded by filing lawsuits against four former engineers for infringement of trade secrets. The Tramiels did not purchase the employee contracts with the assets of Atari, Inc. and re-hired approximately 100 of the 900 former employees. Tramel Technology soon changed its name to Atari Corporation.

Amid rumors that Tramiel was negotiating to buy Atari, Amiga Corp. entered discussions with Commodore. This led to Commodore wanting to purchase Amiga Corporation outright, which Commodore believed would cancel any outstanding contracts, including Atari's. Instead of Amiga Corp. delivering Lorraine to Atari, Commodore delivered a check of $500,000 on Amiga's behalf, in effect returning the funds Atari invested in Amiga for the chipset. Tramiel countered by suing Amiga Corp. on August 13, 1984, seeking damages and an injunction to bar Amiga (and effectively Commodore) from producing anything with its technology.

The lawsuit left the Amiga team in limbo during mid-1984. Commodore eventually moved forward, with plans to improve the chipset and develop an operating system. Commodore announced the Amiga 1000 with the Lorraine chipset in July 1985, but it wasn't available in quantity until 1986. The delay gave Atari time to deliver the Atari 520ST in June 1985. In March 1987, the two companies settled the dispute out of court in a closed decision.

The lead architect of the new computer project at Tramel Technology and Atari Corporation was ex-Commodore employee Shiraz Shivji, who previously worked on the Commodore 64's development. Different CPUs were investigated, including the 32-bit National Semiconductor NS32000, but engineers were disappointed with its performance, and they moved to the Motorola 68000. The Atari ST design was completed in five months in 1984, concluding with it being shown at the January 1985 Consumer Electronics Show.

A custom sound processor called AMY had been in development at Atari, Inc. and was considered for the new ST computer design. The chip needed more time to complete, so AMY was dropped in favor of a commodity Yamaha YM2149F variant of the General Instrument AY-3-8910.

Soon after the Atari buyout, Microsoft suggested to Tramiel that it could port Windows to the platform, but the delivery date was out by two years. Another possibility was Digital Research, which was working on a new GUI-based system then known as Crystal, soon to become GEM. Another option was to write a new operating system, but this was rejected as Atari management was unsure whether the company had the required expertise.

Digital Research was fully committed to the Intel platform, so a team from Atari was sent to the Digital Research headquarters to work with the "Monterey Team", which comprised a mixture of Atari and Digital Research engineers. Atari's Leonard Tramiel was the Atari person overseeing "Project Jason" (also known as The Operating System) for the Atari ST series, named for designer and developer Jason Loveman.

GEM is based on CP/M-68K, a direct port of CP/M to the 68000. By 1985, CP/M was becoming increasingly outdated; it did not support subdirectories, for example. Digital Research was also in the process of building GEMDOS, a disk operating system for GEM, and debated whether a port of it could be completed in time for product delivery in June. The decision was eventually taken to port it, resulting in a GEMDOS file system which became part of Atari TOS (for "The Operating System", colloquially known as the "Tramiel Operating System"). This gave the ST a fast, hierarchical file system, essential for hard drives, and provided programmers with function calls similar to MS-DOS. The Atari ST character set is based on codepage 437.

After six months of intensive effort following Tramiel's takeover, Atari announced the 520ST at the Winter Consumer Electronics Show in Las Vegas in January 1985. "InfoWorld" assessed the prototypes shown at computer shows as follows:Pilot production models of the Atari machine are much slicker than the hand-built models shown at earlier computer fairs; it doesn't look like a typical Commodore 64-style, corner-cutting, low-cost Jack Tramiel product of the past.Atari unexpectedly displayed the ST at Atlanta COMDEX in May. Similarities to the original Macintosh and Tramiel's role in its development resulted in it being nicknamed "Jackintosh". Atari's rapid development of the ST amazed many, but others were skeptical, citing its "cheap" appearance, Atari's uncertain financial health, and poor relations between Tramiel-led Commodore and software developers.

Atari ST print advertisements stated, "America, We Built It For You", and quoted Atari president Sam Tramiel: "We promised. We delivered. With pride, determination, and good old ATARI know how". But Jack Tramiel admitted that sales of its 8-bit family were "very, very slow", Atari was out of cash, and employees feared that he would shut the company down.

In early 1985, the 520ST shipped to the press, developers, and user groups, and in early July 1985 for general retail sales. It saved the company. By November, Atari stated that more than 50,000 520STs had been sold, "with U.S. sales alone well into five figures". The machine had gone from concept to store shelves in a little under one year.

Atari had intended to release the 130ST with 128 KB of RAM and the 260ST with 256 KB. However, the ST initially shipped without TOS in ROM and required booting TOS from floppy, taking 206 KB RAM away from applications. The 260ST was launched in Europe on a limited basis. Early models have six ROM sockets for easy upgrades to TOS. New ROMs were released a few months later and were included in new machines and as an upgrade for older machines.

Atari originally intended to include GEM's GDOS (Graphical Device Operating System), which allows programs to send GEM VDI (Virtual Device Interface) commands to drivers loaded by GDOS. This allows developers to send VDI instructions to other devices simply by pointing to it. However, GDOS was not ready at the time the ST started shipping and was included in software packages and with later ST machines. Later versions of GDOS support vector fonts.

A limited set of GEM fonts were included in the ROMs, including the ST's standard 8x8 pixel graphical character set. It contains four characters which can be placed together in a square, forming the face of J. R. "Bob" Dobbs (the figurehead of the Church of the SubGenius).

The ST was less expensive than most contemporaries, including the Macintosh Plus, and is faster than many. Largely as a result of its price and performance factor, the ST became fairly popular, especially in Europe where foreign-exchange rates amplified prices. The company's English advertising slogan of the era was "Power Without the Price". An Atari ST and terminal emulation software was much cheaper than a Digital VT220 terminal, commonly needed by offices with central computers.

By late 1985, the 520ST added an RF modulator for TV display.

"Computer Gaming World" stated that Tramiel's poor pre-Atari reputation would likely make computer stores reluctant to deal with the company, hurting its distribution of the ST. One retailer said, "If you can believe Lucy when she holds the football for Charlie Brown, you can believe Jack Tramiel"; another said that because of its experience with Tramiel, "our interest in Atari is zero, zilch". Neither Atari nor Commodore could persuade large chains like ComputerLand or BusinessLand to sell its products. Observers criticized Atari's erratic discussion of its stated plans for the new computer, as it shifted between using mass merchandisers, specialty computer stores, and both. When asked at COMDEX, Atari executives could not name any computer stores that would carry the ST. After a meeting with Atari, one analyst said, "We've seen marketing strategies changed before our eyes".

Tramiel's poor reputation influenced potential software developers. One said, "Dealing with Commodore is like dealing with Attila the Hun. I don't know if Tramiel will be following his old habits ... I don't see a lot of people rushing to get software on the machine." Large business-software companies like Lotus, Ashton-Tate, and Microsoft did not promise software for either the ST or Amiga, and the majority of software companies were hesitant to support another platform beyond the IBM PC, Apple, and Commodore 64. Philippe Kahn of Borland said, "These days, if I were a consumer, I'd stick with companies [such as Apple and IBM] I know will be around".

At Las Vegas COMDEX in November 1985, the industry was surprised by more than 30 companies exhibiting ST software while the Amiga had almost none. After Atlanta COMDEX, "The New York Times" reported that "more than 100 software titles will be available for the [ST], most written by small software houses that desperately need work", and contrasted the "small, little-known companies" at Las Vegas with the larger ones like Electronic Arts and Activision, which planned Amiga applications.

Trip Hawkins of Electronic Arts said, "I don't think Atari understands the software business. I'm still skeptical about its resources and its credibility." Although Michael Berlyn of Infocom promised that his company would quickly publish all of its games for the new computer, he doubted many others would soon do so. Spinnaker and Lifetree were more positive, both promising to release ST software. Spinnaker said that "Atari has a vastly improved attitude toward software developers. They are eager to give us technical support and machines". Lifetree said, "We are giving Atari high priority". Some, such as Software Publishing Corporation, were unsure of whether to develop for the ST or the Amiga. John C. Dvorak wrote that the public saw both Commodore and Atari as selling "cheap disposable" game machines, in part because of their computers' sophisticated graphics.

The original 520ST case design was created by Ira Velinsky, Atari's chief Industrial Designer. It is wedge-shaped, with bold angular lines and a series of grilles cut into the rear for airflow. The keyboard has soft tactile feedback and rhomboid-shaped function keys across the top. It is an all-in-one unit, similar to earlier home computers like the Commodore 64, but with a larger keyboard with cursor keys and a numeric keypad. The original has an external floppy drive (SF354) and AC adapter. Starting with the 1040ST, the floppy drive and power supply are integrated into the base unit.

The ports on the 520ST remained largely unchanged over its history. 


Because of its bi-directional design, the Centronics printer port can be used for joystick input, and several games used available adaptors that used the printer socket, providing two additional 9-pin joystick ports.


The ST supports a monochrome or colour monitor. The colour hardware supports two resolutions: 320 × 200 pixels, with 16 of 512 colours; and 640 × 200, with 4 of 512 colours. The monochrome monitor was less expensive and has a single resolution of 640 × 400 at 71.25 Hz. The attached monitor determines available resolutions, so each application either supports both types of monitors or only one. Most ST games require colour with productivity software favouring the monochrome.

Atari initially used single-sided 3.5 inch floppy disk drives that could store up to 360 KB. Later drives were double-sided and stored 720 KB. Some commercial software, particularly games, shipped by default on single-sided disks, even supplying two 360 KB floppies instead of a single double-sided one, to avoid alienating early adopters. 

Some software uses formats which allow the full disk to be read by double-sided drives but still lets single-sided drives access side A of the disk. Many magazine coverdisks (such as the first 30 issues of "ST Format") were designed this way, as were a few games. The music in "Carrier Command" and the intro sequence in "Populous" are not accessible to single-sided drives, for example. 

STs with double-sided drives can read disks formatted by MS-DOS, but IBM PC compatibles can not read Atari disks because of differences in the layout of data on track 0.

Atari upgraded the basic design in 1986 with the 1040STF, stylized as 1040ST: essentially a 520ST with twice the RAM and with the power supply and a double-sided floppy drive built-in instead of external. This adds to the size of the machine, but reduces cable clutter. The joystick and mouse ports, formerly on the right side of the machine, are in a recess underneath the keyboard. An "FM" variant includes an RF modulator allowing a television to be used instead of a monitor.

The trailing "F" and "FM" were often dropped in common usage. In "BYTE" magazine's March 1986 cover photo of the system, the name plate reads 1040ST but in the headline and article it's simply "1040ST". 
The 1040ST is one of the earliest personal computers shipped with a base RAM configuration of 1 MB. With a list price of in the US, "BYTE" hailed it as the first computer to break the $1000 per megabyte price barrier. "Compute!" noted that the 1040ST is the first computer with one megabyte of RAM to sell for less than $2,500.

A limited number of 1040STFs shipped with a single-sided floppy drive.

Initial sales were strong, especially in Europe, where Atari sold 75% of its computers. West Germany became Atari's strongest market, with small business owners using them for desktop publishing and CAD.

To address this growing market segment, Atari introduced the ST1 at Comdex in 1986. Renamed to Mega, it includes a high-quality detached keyboard, a stronger case to support the weight of a monitor, and an internal bus expansion connector. An optional 20 MB hard drive can be placed below or above the main case. Initially equipped with 2 or 4 MB of RAM (a 1 MB version, the Mega 1, followed), the Mega machines can be combined with Atari's laser printer for a low-cost desktop publishing package.

A custom blitter coprocessor improved some graphics performance, but was not included in all models. Developers wanting to use it had to detect its presence in their programs. Properly written applications using the GEM API automatically make use of the blitter.

In late 1989, Atari Corporation released the 520ST and 1040ST (also written STE), enhanced version of the ST with improvements to the multimedia hardware and operating system. It features an increased color palette of 4,096 colors from the ST's 512 (though the maximum displayable palette without programming tricks is still limited to 16 in the lowest 320 × 200 resolution, and even fewer in higher resolutions), genlock support, and a blitter coprocessor (stylized as "BLiTTER") which can quickly move large blocks of data (particularly, graphics data) around in RAM. The STE is the first Atari with PCM audio; using a new chip, it added the ability to play back 8-bit (signed) samples at 6258 Hz, 12517 Hz, 25033 Hz, and even 50066 Hz, via direct memory access (DMA). The channels are arranged as either a mono track or a track of LRLRLRLR... bytes. RAM is now much more simply upgradable via SIMMs.

Two enhanced joystick ports were added (two normal joysticks can be plugged into each port with an adapter), with the new connectors placed in more easily accessed locations on the side of the case. The enhanced joystick ports were re-used in the Atari Jaguar console and are compatible.

The STE models initially had software and hardware conflicts resulting in some applications and video games written for the ST line being unstable or even completely unusable, primarily caused by programming direct hardware calls which bypassed the operating system. Furthermore, even having a joystick plugged in would sometimes cause strange behavior with a few applications (such as the WYSIWYG word-processor application 1st Word Plus). Very little use was made of the extra features of the STE: STE-enhanced and STE-only software was rare.

The last STE machine, the Mega STE, is an STE in a grey Atari TT case that had a switchable 16 MHz, dual-bus design (16-bit external, 32-bit internal), optional Motorola 68881 FPU, built-in 1.44 MB "HD" 3-inch floppy disk drive, VME expansion slot, a network port (very similar to that used by Apple's LocalTalk) and an optional built-in 3" hard drive. It also shipped with TOS 2.00 (better support for hard drives, enhanced desktop interface, memory test, 1.44 MB floppy support, bug fixes). It was marketed as more affordable than a TT but more powerful than an ordinary ST.

In 1990, Atari released the high-end workstation-oriented Atari TT030, based on a 32 MHz Motorola 68030 processor. The "TT" name ("Thirty-two/Thirty-two") continued the nomenclature because the 68030 chip has 32-bit buses both internally and externally. Originally planned with a 68020 CPU, the TT has improved graphics and more powerful support chips. The case has a new design with an integrated hard-drive enclosure.

The final model of ST computer is the Falcon030. Like the TT, it is 68030-based, at 16 MHz, but with improved video modes and an on-board Motorola 56001 audio digital signal processor. Like the Atari STE, it supports sampling frequencies above 44.1 kHz; the sampling master clock is 98340 Hz (which can be divided by a number between 2 and 16 to get the actual sampling frequencies). It can play the STE sample frequencies (up to 50066 Hz) in 8 or 16 bit, mono or stereo, all by using the same DMA interface as the STE, with a few additions. It can both play back and record samples, with 8 mono channels and 4 stereo channels, allowing musicians to use it for recording to hard drive. Although the 68030 microprocessor can use 32-bit memory, the Falcon uses a 16-bit bus, which reduces performance and cost. In another cost-reduction measure, Atari shipped the Falcon in an inexpensive case much like that of the ST and ST. Aftermarket upgrade kits allow it to be put in a desktop or rack-mount case, with the keyboard separate.

Released in 1992, the Falcon was discontinued by Atari the following year. In Europe, C-Lab licensed the Falcon design from Atari and released the C-Lab Falcon Mk I, identical to Atari's Falcon except for slight modifications to the audio circuitry. The Mk II added an internal 500 MB SCSI hard disk; and the Mk X further added a desktop case. C-Lab Falcons were also imported to the US by some Atari dealers.

As with the Atari 8-bit family of computers, software publishers attributed their reluctance to produce Atari ST products in part to—as "Compute!" reported in 1988—the belief in the existence of a "higher-than-normal amount of software piracy". That year, WordPerfect threatened to discontinue the Atari ST version of its word processor because the company discovered that pirate bulletin board systems (BBSs) were distributing it, causing "ST-Log" to warn that "we had better put a stop to piracy "now" ... it can have harmful effects on the longevity and health of your computer". In 1989, magazines published a letter by Gilman Louie, head of Spectrum HoloByte. He stated that he had been warned by competitors that releasing a game like "Falcon" on the ST would fail because BBSs would widely disseminate it. Within 30 days of releasing the non-copy protected ST version, the game was available on BBSs with maps and code wheels. Because the ST market was smaller than that for the IBM PC, it was more vulnerable to piracy which, Louie said, seemed to be better organized and more widely accepted for the ST. He reported that the Amiga version sold in six weeks twice as much as the ST version in nine weeks, and that the Mac and PC versions had four times the sales. "Computer Gaming World" stated "This is certainly the clearest exposition ... we have seen to date" of why software companies produced less software for the ST than for other computers.

Several third-party OSes were developed for, or ported to, the Atari ST. Unix clones include Idris, Minix, and the MiNT OS which was developed specifically for the Atari ST.

Plenty of professional quality MIDI-related software was released. The popular Windows and Macintosh applications Cubase and Logic Pro originated on the Atari ST (the latter as Creator, Notator, Notator-SL, and Notator Logic). Another popular and powerful ST music sequencer application, KCS, contains a "Multi-Program Environment" that allows ST users to run other applications, such as the synthesizer patch editing software XoR (now known as Unisyn on the Macintosh), from within the sequencer application.

Music tracker software became popular on the ST, such as the TCB Tracker, aiding the production of quality music from the Yamaha synthesizer, now called chiptunes.

Due to the ST having comparatively large amounts of memory for the time, sound sampling packages became feasible. Replay Professional features a sound sampler using the ST cartridge port to read in parallel from the cartridge port from the ADC. For output of digital sound, it uses the on-board frequency output, sets it to 128 kHz (inaudible) and then modulates the amplitude of that.

MasterTracks Pro originated on Macintosh, then ST, then IBM PC version. It continued on Windows and macOS, along with the original company's notation applications Encore.

Professional desktop publishing software includes PageStream and Calamus. Word processors include WordPerfect, Microsoft Write, AtariWorks, Signum, Script and First Word (bundled with the machine). Spreadsheets include 3D-Calc, and databases include Zoomracks. Graphics applications include NEOchrome, DEGAS & DEGAS Elite, Deluxe Paint, STAD, and Cyber Paint (which author Jim Kent would later evolve into Autodesk Animator) with advanced features such as 3D design and animation. The Spectrum 512 paint program uses rapid palette switching to expand the on-screen color palette to 512 (up to 46 colors per scan line).

3D computer graphics applications (like Cyber Studio CAD-3D, which author Tom Hudson later developed into Autodesk 3D Studio), brought 3D modelling, sculpting, scripting, and computer animation to the desktop. Video capture and editing applications use dongles connected to the cartridge port for low frame rate, mainly silent and monochrome, but progressed to sound and basic color in still frames. At the end, Spectrum 512 and CAD-3D teamed up to produce realistic 512-color textured 3D renderings, but processing was slow, and Atari's failure to deliver a machine with a math coprocessor had Hudson and Yost looking towards the PC as the future before a finished product could be delivered to the consumer.

Garry Kasparov became the first chess player to register a copy of ChessBase, a popular commercial database program for storing and searching records of chess games. The first version was built for Atari ST with his collaboration in January 1987. In his autobiography "Child of Change", he regards this facility as "the most important development in chess research since printing".

Graphical touchscreen point of sale software for restaurants was originally developed for Atari ST by Gene Mosher under the ViewTouch copyright and trademark. Instead of using GEM, he developed a GUI and widget framework for the application using the NEOchrome paint program.

The 520ST was bundled with both Digital Research Logo and Atari ST BASIC. Third-party BASIC systems with better performance were eventually released: HiSoft BASIC, GFA BASIC, FaST BASIC, DBASIC, LDW BASIC, Omikron BASIC, BASIC 1000D and STOS. In the later years of the Atari ST, Omikron Basic was bundled with it in Germany.

Atari's initial development kit from Atari is a computer and manuals. The cost discouraged development. The later Atari Developer's Kit consists of software and manuals for . It includes a resource kit, C compiler (first Alcyon C, then Mark Williams C), debugger, 68000 assembler, and non-disclosure agreement. The third-party Megamax C development package was .

Other development tools include 68000 assemblers (MadMac from Atari, HiSoft Systems's Devpac, TurboAss, GFA-Assembler), Pascal (OSS Personal Pascal, Maxon Pascal, PurePascal), Modula-2, C compilers (Lattice C, Pure C, Megamax C, GNU C, Aztec C, AHCC), LISP, and Prolog.

The ST had success in gaming due to the low cost, fast performance, and colorful graphics compared to contemporary PCs or 8-bit systems. ST game developers include Peter Molyneux, Doug Bell, Jeff Minter, Éric Chahi, Jez San, and David Braben.

The realtime pseudo-3D role-playing video game "Dungeon Master", was developed and released first on the ST, and is considered to be the best-selling software ever produced for the platform. Simulation games like "Falcon" and "Flight Simulator II" use the ST's graphics hardware, as do many arcade ports. The 1987 first-person shooter, "MIDI Maze", uses the MIDI ports to connect up to 16 machines for networked deathmatch play. The critically acclaimed "Another World" was originally released for ST and Amiga in 1991 with its engine developed on the ST and the rotoscoped animation created on the Amiga. While American developers stopped making ST games around 1991 (The Secret of Monkey Island was the last major American-coded title) it remained popular with many European developers (primarily in France and Britain) until 1993.

The ST's lack of hardware scrolling meant that many games used smaller screen windows, or flick-screen gameplay instead of scrolling. Combined with sound hardware that was not considered the equal of the Amiga or Commodore 64, the machine's performance for 2D arcade games was seen as its weakest point. Games simultaneously released on the Amiga that do not use the Amiga's superior graphics and sound capabilities were often accused by video game magazines of simply being ST ports.. While the ST was often the lead machine, or jointly with the Amiga version, for the 68000 coded versions, later titles such as Lemmings, Cannon Fodder, Turrican II, Sensible Soccer, The Chaos Engine and Civilization were usually coded for the Amiga in 32 colors first and later converted down to the ST in 16 colors.

Spectre GCR emulates the Macintosh. MS-DOS emulators were released in the late 1980s. PC-Ditto has a software-only version, and a hardware version that plugs into the cartridge slot or kludges internally. After running the software, an MS-DOS boot disk is required to load the system. Both run MS-DOS programs in CGA mode, though much more slowly than on an IBM PC. Other options are the PC-Speed (NEC V30), AT-Spee (Intel 80286), and ATonce-386SX (Intel 80386SX) hardware emulator boards.

The ST's low cost, built-in MIDI ports, and fast, low-latency response times made it a favorite with musicians.


All STs are made up of both custom and commercial chips.

As originally released in the 520ST:


Very early machines have the OS on a floppy disk before a final version was burned into ROM. This version of TOS was bootstrapped from a small core boot ROM.

In 1986, most production models became STs, with an integrated single- (520STF) or double-sided (1040STF) double density floppy disk drive built-in, but no other changes. Also in 1986, the "520ST" (or "520STM") added an RF modulator for allowing the low and medium resolution color modes when connected to a TV. Later "F" and "FM" models of the 520 had a built-in double-sided disk drive instead of a single-sided one.

As originally released in the 520ST/1040ST:


The members of the ST family are listed below, in roughly chronological order:


The 130ST was intended to be a 128 KB variant. It was announced at the 1985 CES alongside the 520ST but never produced. The 4160ST was a 1040ST, but with 4 MB of RAM. A small quantity of development units were produced, but the system was never officially released. Atari did produce a quantity of 4160STE metallic case badges which found their way to dealers, so it's not uncommon to find one attached to systems which were originally 520/1040STE. No such labels were produced for the base of the systems.

Atari Transputer Workstation is a standalone machine developed in conjunction with Perihelion Hardware, containing modified ST hardware and up to 17 transputers capable of massively parallel operations for tasks such as ray tracing.

Following Atari's departure from the computer market, both Medusa Computer Systems and Milan Computer manufactured Atari Falcon/TT-compatible machines with 68040 and 68060 processors. The FireBee is an Atari ST/TT clone based on the Coldfire processor. The GE-Soft Eagle is a 32 MHz TT clone.




List of artificial intelligence projects

The following is a list of current and past, non-classified notable artificial intelligence projects.


















Aaliyah

Aaliyah Dana Haughton ( ; January 16, 1979 – August 25, 2001) was an American singer and actress. She has been credited with helping to redefine contemporary R&B, pop, and hip hop, earning her the nicknames the "Princess of R&B" and "Queen of Urban Pop".

Born in Brooklyn and raised in Detroit, she first gained recognition at the age of 10, when she appeared on the television show "Star Search" and performed in concert alongside Gladys Knight. At the age of 12, Aaliyah signed with Jive Records and her uncle Barry Hankerson's Blackground Records. Hankerson introduced her to R. Kelly, who became her mentor, as well as lead songwriter and producer of her debut album, "Age Ain't Nothing but a Number" (1994). The album sold three million copies in the United States and was certified double platinum by the Recording Industry Association of America (RIAA). After allegations of an illegal marriage with Kelly, Aaliyah ended her contract with Jive and signed with Atlantic Records.

Aaliyah worked with record producers Timbaland and Missy Elliott for her second album, "One in a Million" (1996), which sold three million copies in the United States and more than eight million copies worldwide. In 2000, Aaliyah made her acting debut in the film "Romeo Must Die". She contributed to the film's soundtrack, which was supported by her single "Try Again". The song topped the "Billboard" Hot 100 solely through airplay, becoming the first in the chart's history to do so. After completing the film, Aaliyah subsequently filmed her starring role in "Queen of the Damned" (which was released posthumously), and in July 2001, released her eponymous third album, which topped the "Billboard" 200. The album spawned the singles "Rock the Boat", "More Than a Woman", and "We Need a Resolution" (featuring Timbaland).

On August 25, 2001, at the age of 22, Aaliyah was killed in an airplane accident in the Bahamas along with eight other people on board, when the overloaded aircraft she was traveling in crashed shortly after takeoff. The pilot was later found to have traces of cocaine and alcohol in his body and was not qualified to fly the aircraft designated for the flight. Aaliyah's family filed a wrongful death lawsuit against the aircraft's operator, which was settled out of court. In the decades following her death, Aaliyah's music has continued to achieve commercial success, aided by several posthumous releases. She has sold 8.1 million albums in the US and an estimated 24 to 32 million albums worldwide. "Billboard" lists her as the tenth most successful female R&B artist of the past 25 years, and the 27th most successful in history. Her accolades include three American Music Awards and two MTV VMAs, along with five Grammy Award nominations.

Aaliyah Dana Haughton was born on January 16, 1979, in Brooklyn, New York City, New York, the younger child of Diane and Michael "Miguel" Haughton, a warehouse worker. She was of African-American descent. Her name is the feminine form of the Arabic "Ali", meaning "highest, most exalted one, the best." Aaliyah was fond of her name, calling it "beautiful" and saying she was "very proud of it" and strove to live up to her name every day. When she was five years old, her family moved to Detroit, Michigan, where she was raised along with her older brother, Rashad. In Detroit, her father began working in the warehouse business, one of his brother-in-law Barry Hankerson's widening interests. Her mother stayed home and raised her and her brother. Her mother enrolled her in voice lessons at an early age. Eventually, she started performing at weddings, church choir, and charity events. Aaliyah attended a Catholic school, Gesu Elementary, where in first grade she was cast in the stage play "Annie" which inspired her to become an entertainer.

Aaliyah's mother was a vocalist, and her uncle Hankerson was an entertainment lawyer who had been married to Gladys Knight. As a child, Aaliyah traveled with Knight and worked with an agent in New York City to audition for commercials and television programs, including "Family Matters". After failing to land a role on the show she continued her acting through the Gesu Players. In 1989 at age ten she appeared on "Star Search", where she performed "My Funny Valentine". Aaliyah chose to begin auditioning. Her mother made the decision to drop her surname. She auditioned for several record labels and at age 11 appeared in concerts alongside Knight. During her childhood, she had several pet animals including ducks, snakes and iguanas. Her cousin Jomo had a pet alligator, which Aaliyah felt was too much, remarking, "that was something I wasn't going to stroke."

Aaliyah attended Detroit schools and believed she was well-liked, but she was teased for her short stature. By age 15, however, she came to love her height. Her mother told her to be happy she was small and complimented her. Although some children disliked Aaliyah, she determine, "You always have to deal with people who are jealous, but there were so few it didn't even matter. The majority of kids supported me, which was wonderful." Even in her adult life, she considered herself small. She had "learned to accept and love" herself and added: "the most important thing is to think highly of yourself because if you don't, no one else will".

During her audition for acceptance to the Detroit High School for the Fine and Performing Arts, Aaliyah sang "Ave Maria" in Latin. Aaliyah held a 4.0 grade-point average when graduating from high school. She reflected: "I wanted to keep that 4.0. Being in the industry, you know, I don't want kids to think, 'I can just sing and forget about school.' I think it's very important to have an education, and even more important to have something to fall back on." She considered a future career teaching music, music history or drama if she did not make a living as a recording artist because, as she reasoned, "when you pick a career it has to be something you love".

After Hankerson signed a distribution deal with Jive Records, he signed Aaliyah to his Blackground Records label at the age of 12. Hankerson later introduced her to recording artist and producer R. Kelly, who became Aaliyah's mentor, as well as lead songwriter and producer of her first album, recorded when she was 14. Aaliyah's debut album, "Age Ain't Nothing but a Number", was released under her mononym "Aaliyah", by Jive and Blackground Records on May 24, 1994; it debuted at number 24 on the "Billboard" 200 chart, selling 38,000 copies in its first week. It peaked at number 18 on the "Billboard" 200 and it was certified two times Platinum by the RIAA. To date the album has sold over 3 million copies in the US. In Canada, the album was certified gold by Music Canada for 50,000 copies in shipments. In 2014, "Vibe" magazine estimated that the album had sold six million copies globally.

Upon its release, "Age Ain't Nothing But a Number" received generally favorable reviews from music critics. Some writers noted that Aaliyah's "silky vocals" and "sultry voice" blended with Kelly's new jack swing helped define R&B in the 1990s. Her sound was also compared to that of female quartet En Vogue. Christopher John Farley of "Time" magazine called the album a "beautifully restrained work", noting that Aaliyah's "girlish, breathy vocals rode calmly on R. Kelly's rough beats". Stephen Thomas Erlewine of AllMusic felt that the album had its "share of filler", but described the singles as "slyly seductive". He also wrote that the songs on the album were "frequently better" than that of Kelly's second studio album, "12 Play". The single "At Your Best (You Are Love)" was criticized by "Billboard" for being out of place on the album and for its length.

Aaliyah's debut single, "Back & Forth", peaked at number 5 on the Hot 100 and topped the Hot R&B/Hip-Hop Songs chart for three weeks. Two more singles charted: a cover of the Isley Brothers' "At Your Best (You Are Love)" peaked at number 6 on the "Billboard" Hot 100, and the album's title track, "Age Ain't Nothing but a Number", peaked at number 75. Additionally, she released "The Thing I Like" as part of the soundtrack to the 1994 film "A Low Down Dirty Shame".

In 1996, Aaliyah left Jive Records and signed with Atlantic Records. She worked with record producers Timbaland and Missy Elliott, who contributed to her second studio album, "One in a Million". Elliott recalled Timbaland and herself being nervous to work with Aaliyah, since Aaliyah had already released her successful debut album while Elliott and Timbaland were just starting out. Elliott also feared she would be a diva, but reflected that Aaliyah "came in and was so warming; she made us immediately feel like family." The album yielded the lead single "If Your Girl Only Knew", which peaked at number 11 on the "Billboard" Hot 100 and topped the "Billboard" Hot R&B/Hip-Hop Songs for two weeks. It also generated the singles "Hot Like Fire" and "4 Page Letter". "One in a Million" peaked at number 18 on the "Billboard" 200, and was certified double platinum by the RIAA on June 16, 1997, denoting shipments of two million copies. The album went on to sell 3 million copies in the US and over eight million copies worldwide. The year after her album was released, Aaliyah was featured on Timbaland & Magoo's debut single, "Up Jumps da Boogie".

In 1997 Aaliyah graduated with a 4.0 GPA from the Detroit High School for the Fine and Performing Arts, where she majored in drama. The same year, she began her acting career, playing herself in the police drama television series "New York Undercover". During this time, Aaliyah participated in the Children's Benefit Concert, a charity concert at the Beacon Theatre in New York. She also became the spokesperson for the Tommy Hilfiger Corporation. During her campaign with Tommy Hilfiger, the company sold over 2,400 pairs of the red, white and blue baggy jeans she wore in their advertisements. In December 1997, she performed the Christmas carol "What Child Is This?" at the annual Christmas in Washington television special. She also contributed to the soundtrack album for the animated film "Anastasia", performing a cover version of "Journey to the Past" that earned songwriters Lynn Ahrens and Stephen Flaherty a nomination for the Academy Award for Best Original Song. Aaliyah performed the song at the 1998 Academy Awards ceremony, becoming the youngest singer to perform at the event. Also in 1998, she released the song "Are You That Somebody?" which was featured on the "Dr. Dolittle" soundtrack. The song peaked at number 21 on the "Billboard" Hot 100 and earned Aaliyah her first Grammy Award nomination.

In 1999, Aaliyah landed her first big-screen acting role in "Romeo Must Die". She starred opposite martial artist Jet Li, playing a couple who fall in love amid their warring families. Released on March 24, 2000, the movie grossed US$18.6 million in its first weekend, ranking number two at the box office. Aaliyah purposely stayed away from reviews of the film to "make it easier on" herself, but she heard "that people were able to get into me, which is what I wanted." In contrast, some critics felt there was no chemistry between her and Jet Li, as well as viewing the film as too simplistic. This was echoed by Elvis Mitchell of "The New York Times", who wrote that while Aaliyah was "a natural" and the film was conceived as a spotlight for both her and Li, "they have so little chemistry together you'd think they're putting out a fire instead of shooting off sparks.
In addition to acting, Aaliyah served as an executive producer of the film's soundtrack, for which she contributed four songs. "Try Again" was released as a single from the soundtrack; the song topped the "Billboard" Hot 100, making Aaliyah the first artist to top the chart based solely on airplay; this led the song to be released in a 12-inch vinyl and 7-inch single. The music video won the Best Female Video and Best Video from a Film awards at the 2000 MTV Video Music Awards. It also earned her a Grammy Award nomination for Best Female R&B Vocalist. The soundtrack went on to sell 1.5 million copies in the United States.

After completing "Romeo Must Die", Aaliyah began to work on her second film, "Queen of the Damned". She played the role of an ancient vampire, Queen Akasha, which she described as a "manipulative, crazy, sexual being". Filming both Romeo Must Die and "Queen of the Damned" delayed the release of the album. Aaliyah had not intended for her albums to have such a gap between them. "I wanted to take a break after "One in a Million" to just relax, think about how I wanted to approach the next album. Then, when I was ready to start back up, "Romeo" happened, and so I had to take another break and do that film and then do the soundtrack, then promote it. The break turned into a longer break than I anticipated." Ultimately, she filmed "Queen of the Damned" and recorded her third album at the same time so that it could be released in 2001. Aaliyah enjoyed balancing her singing and acting careers. Though she called music a "first" for her, she also had been acting since she was young and had wanted to begin acting "at some point in my career", but "wanted it to be the right time and the right vehicle" and felt "Romeo Must Die" "was it". Connie Johnson of the "Los Angeles Times" argued that Aaliyah having to focus on her film career may have caused her to not give the album "the attention it merited."

During the recording stages for the album, Aaliyah's publicist disclosed that the album's release date was most likely in October 2000. Eventually, she finished recording the album in March 2001; after a year of recording tracks that began in March of the previous year.
"Aaliyah" was released five years after "One in a Million" on July 17, 2001, and it debuted at number two on the "Billboard" 200, selling 187,000 copies in its first week. The first single from the album, "We Need a Resolution", peaked at number 59 on the "Billboard" Hot 100. The week after Aaliyah's death, her third album rose from number 19 to number 1 on the "Billboard" 200. "Rock the Boat" was released as a posthumous single. The music video premiered on BET's "Access Granted", and it became the most viewed and highest rated episode in the history of the show. The song peaked at number 14 on the "Billboard" Hot 100. Promotional posters for "Aaliyah" that had been put up in major cities such as New York and Los Angeles became makeshift memorials for grieving fans. In February 2002, the album was certified double Platinum by the RIAA.

"More than a Woman" and "I Care 4 U" were released as posthumous singles and peaked within the top 25 of the "Billboard" Hot 100. "More than a Woman" reached number one on the UK singles chart making Aaliyah the first female deceased artist to reach number one on the UK singles chart. "More than a Woman" was replaced by George Harrison's "My Sweet Lord" which is the only time in the UK singles chart's history when a dead artist has replaced another dead artist at number one.

Aaliyah was signed to appear in several future films, including a romantic film titled "Some Kind of Blue", and a Whitney Houston–produced remake of the 1976 film "Sparkle". Houston recalled Aaliyah being "so enthusiastic" about the film; the project was shelved after she died. Before her death Aaliyah filmed some scenes for the sequels of "The Matrix" as the character Zee. A portion of her role in "The Matrix Reloaded" was filmed; these unused scenes were included in the tribute section of the "Matrix Ultimate Collection" series.

Aaliyah had the vocal range of a soprano, and with the release of her debut album "Age Ain't Nothing but a Number", writer Dimitri Ehrlich of "Entertainment Weekly" compared her style and sound to R&B group En Vogue. Ehrlich also expressed that Aaliyah's "silky vocals are more agile than those of self-proclaimed queen of hip-hop soul Mary J. Blige." In her review for Aaliyah's second studio album "One in a Million" "Vibe" magazine, music critic Dream Hampton said that Aaliyah's "deliciously feline" voice has the same "pop appeal" as Janet Jackson's. According to "Rolling Stone" "the most remarkable thing about Aaliyah's voice, besides its flexibility and crisp range, was its almost preternatural poise — she always seemed to be holding her power in reserve, to know every side of the scenarios she described". While, Siân Pattenden from "Mixmag" stated that, "She doesn't try to toss the caber with vocal athleticism. There's no shouting, screeching, wailing or jazz-style noodling. Everything is underplayed: Ms Haughton's range is displayed by the slightest high-octave backing and tiniest harmonies".

Aaliyah herself said her vocal styling consisted of her singing softly while utilizing her falsetto. She further explained saying, "My signature style is breathy, tone-y, airy. It's simple but I can ride a crazy track." Although she frequently sang in a softer tone, there were moments when she utilized other facets of her voice. "Never Givin' Up" (1996) and "The One I Gave My Heart To" (1997) are a few stand-out vocal highlights. Daryl Simmons the producer of "The One I Gave My Heart To", recalled Aaliyah doing opera vocal warm-up exercises in preparation for the songs recording. While commenting on her doing opera vocal warm ups Simmons mentioned, "It was the furthest thing I would have ever thought that she could do. It just blew my mind." The song's writer, Diane Warren praised her vocals on the song, saying: "It showed her vocal range, and I know a couple of people thought she wouldn't be able to do that song. I thought, 'No, she'll be able to do that." "Variety" echoed a similar sentiment as warren, saying "The One I Gave My Heart To" "showcased Aaliyah's ability to hit higher notes." Discussing her approach on "Never Givin' Up" producer Craig King said, "lyrically and vocally she just took you to places that you didn't know she could go".

From the very beginning, she opted "for an edgier, more mature sound", and her songs were often uptempo and at the same time often dark, revolving around "matters of the heart". She "easily straddled the hip-hop and pop worlds, never projecting the frilliness of her ingénue peers". In 2001, Aaliyah called her sound "street but sweet", pairing feminine vocals with a gritty urban rhythm track. In another interview she further spoke about her artistry, saying, "I love to fuse other types of music with my own". She explored a wide range of genres such as R&B, pop, hip hop, funk, soul, and dance-pop.

Discussing her lyrical content in "The New Rolling Stone Album Guide" (2004), Keith Harris said "When it came to sexual availability, she was between En Vogue maliciously taunting 'You're never gonna get it' and Tweet blankly cooing 'Oops, there goes my shirt.'" Lyrically, "Her first two albums carefully toed the line between adolescence and adulthood, displaying a woman exploring the terrain of love, trust, and lust; one who exuded a playful innocence while hinting at a more sultry side." Aaliyah did not usually write her own lyrics. The only time she had a hand in writing is on the song "Death of a Playa" from the "Hot Like Fire" single (1997). She co-wrote that song with her brother Rashad Haughton, and "it reflects Aaliyah's dark perspective on romance". Of her role in crafting her music, Aaliyah said, "I like to have the final say but I was trained as a singer, actress and dancer, the interpreter, bringing other people's words to life. I need the songs to reflect me in one way or another". After her R. Kelly–produced debut album, Aaliyah worked with Timbaland and Missy Elliott, whose productions were more electronic. The duo "mixed choppy, nervous rhythms over loops of computer-generated backing tracks, and incorporating harmonies which – within the genre's limited horizons – seemed daring". They also created, the "Freeze-and-stop style of singing on top of bass-heavy instrumentals" which became Aaliyah's signature style.

Aaliyah's songs have been said to have "crisp production" and "staccato arrangements" that "extend genre boundaries" while containing "old-school" soul music. Kelefah Sanneh of "The New York Times" called her "a digital diva who wove a spell with ones and zeroes", and writes that her songs comprised "simple vocal riffs, repeated and refracted to echo the manipulated loops that create digital rhythm", as Timbaland's "computer-programmed beats fitted perfectly with her cool, breathy voice to create a new kind of electronic music." She released "musically risky singles into a notoriously fickle pop market", without being "concerned about conforming to the stereotypes of the marketplace". Her songs "gracefully walk a line between commerciality and experimentation". Reviewing her album, British publication "NME" felt that Aaliyah's "radical" third album was "intended to consolidate her position as U.S. R&B's most experimental artist".

As her albums progressed, writers felt that Aaliyah matured, calling her progress a "near-flawless declaration of strength and independence". ABC News noted that her music was "evolving from the punchy pop-influenced hip hop and R&B to a more mature, introspective sound", on her third album. Stephen Thomas Erlewine of AllMusic described her album "Aaliyah" as "a statement of maturity and a stunning artistic leap forward", and called it one of the strongest urban soul records of its time. She portrayed "unfamiliar sounds, styles and emotions", but managed to please critics with the contemporary sound it contained. Ernest Hardy of "Rolling Stone" felt that Aaliyah was displaying stronger technique, giving her best vocal performances. Altogether, Aaliyah's music can be described as alternative R&B, progressive soul, and neo soul, according to "Time" Farley.

As an artist, Aaliyah said she was inspired by a number of performers. These include Michael Jackson, Stevie Wonder, Sade, Trent Reznor of Nine Inch Nails, Korn, Donnie Hathaway, Johnny Mathis, Janet Jackson, Whitney Houston, and Barbra Streisand. Aaliyah said that Michael Jackson's "Thriller" was her "favorite album" and that "nothing will ever top "Thriller"." She said she had always wanted to work with Janet Jackson, to whom she had often been compared, saying, "I admire her a great deal. She's a total performer ... I'd love to do a duet with Janet Jackson." Jackson reciprocated Aaliyah's affection, saying, "I've loved her from the beginning because she always comes out and does something different, musically." Jackson also said she would have enjoyed collaborating with Aaliyah.

According to director Paul Hunter from day one, "Aaliyah wanted her videos to stand out from clips by other R&B singers". He stated, "You can watch programming all day and see a certain type of video by female artists, "Then when one of hers comes on it's something special, something different to look at. That's what she was about." Christopher John Farley from "Time" stated that Aaliyah's "videos, for the most part, are about mood, not about storylines... Her videos are usually lushly shot and infused with sexual tension, though not in overt and obvious ways".

Alisha Acquaye from "Teen Vogue" felt that, "There's much to gather while watching an Aaliyah music video" in fact, she thinks that watching them is "actually an understatement". Acquaye further explained, "There's a state of hypnosis you submit to as she envelops you through sight and sound, tugging at your heartstrings. Between a sequence of sensual, strong movements, infectious instrumentals, and intuitive lyrics that spark emotions of desire, sex, and empowerment, you are enraptured in Aaliyah's physical presence".

Most of Aaliyah's videos included dance routines. While discussing her video choreography "Billboard" mentioned that she "coined the smooth choreography and tomboyish style that would inspire [R&B]'s future generations for years to come". "Vibe" praised several videos saying, "Looking back on her videos like "Try Again" and "Are You That Somebody," Aaliyah's talent in all of those techniques of dancing are apparent, as she's able to hit every syncopated word and beat with ease as if she's moving on air". Kyann-Sian Williams from "NME" named "Are You That Somebody?" as a visual that, "pushed the boundaries when it came to dance breaks in music videos". Williams declared, "Until that time, dance breaks were usually reserved for boybands like *NSync and the Backstreet Boys, but Aaliyah claimed it for R&B stars too".

Aaliyah focused on her public image while protecting her private life. She felt that it was "important ... to differentiate yourself from the rest of the pack". "USA Today" said, "Her slinky vocal style and eye-popping videos made her a crossover star, while her persistent protection of her privacy added an air of intrigue about her". According to Aaliyah, "I put a lot of pressure on myself to be true to myself and not let anything else influence me to do what someone else is doing. Being a little edgy and sexy is me. My image isn't a put-on. I'm happy to put over that dark edge in my videos, because it's always been there. I used to wear my sunglasses or have my hair over one eye a lot more when I was younger. [Now] I'm happy with all aspects of myself."

She often wore baggy clothes and sunglasses, stating that she wanted to be herself. Aaliyah also wore black clothing, starting a trend for similar fashion among women in United States and Japan. In 1998, she hired a personal trainer to keep in shape, and exercised five days a week and ate diet foods. As her career progressed, "she went through so many fashion revamps". For example, When she changed her hairstyle, Aaliyah took her mother's advice and covered her left eye, much like Veronica Lake. The look has become known as her signature and been referred to as fusion of "unnerving emotional honesty" and "a sense of mystique". In regards to her fashion choices, writer Jeff Lorez described her as a "model of understatement". According to Lorez, "She's beautiful, but hardly in a high-gloss, supermodel way—more like a really good-looking girl next door. And rather than bling-blinging her ice in a ghetto-fabulous manner befitting her Trumped-up surroundings, she blings on the down-low: A subtle bracelet here, a winking pendant there, offset by her simple black jeans and matching sweater. Trés cool". Former TRL host Carson Daly said that she was "cutting edge, always one step ahead of the curve and that the TRL audience looks to her to figure out what's hot and what's new".

Aaliyah was often praised for her "clean-cut image" and "moral values". Robert Christgau of "The Village Voice" wrote of Aaliyah's artistry and image, "she was lithe and dulcet in a way that signified neither jailbait nor hottie—an ingenue whose selling point was sincerity, not innocence and the obverse it implies." Emil Wilbekin, told CNN: "Aaliyah is an excellent role model because she started her career in the public eye at age 15 with a gold album," Age Ain't Nothing but a Number". And then her second album, "One in a Million" went double platinum. She had the leading role in "Romeo Must Die", which was a box office success. She's won numerous awards, several MTV music video awards, and aside from her professional successes, many of her lyrics are very inspirational and uplifting. She also carried herself in a very professional manner. She was well-spoken. She was beautiful, but she didn't use her beauty to sell her music. She used her talent. Many young hip-hop fans greatly admire her."

She was also seen by others as a sex symbol and didn't have a problem with being considered one. "I know that people think I'm sexy and I am looked at as that, and it is cool with me," she stated. "It's wonderful to have sex appeal. If you embrace it, it can be a very beautiful thing. I am totally cool with that. Definitely. I see myself as sexy. If you are comfortable with it, it can be very classy and it can be very appealing." Aaliyah also felt though her image was "risque and sexy", it was important to remain respectable because she wanted to make songs that everyone could relate to without it being vulgar. When she participated in fashion designer Tommy Hilfiger's All America Tour Tommy Jean ads, she wore boxer shorts, baggy jeans and a tube top. Hilfiger's brother, Andy, called it "a whole new look" that was "classy but sexy". The single "We Need a Resolution" was argued to have transformed "the once tomboy into a sexy grown woman".

Aaliyah's family played a major role in the course of her career. Beginning in 1995, Aaliyah's father Michael Haughton served as her personal manager, and her mother assisted him. Aaliyah's brother Rashad Haughton and her cousin Jomo Hankerson were with her when she worked. After her father became ill, her brother Rashad became her manager.

Aaliyah was known to have usually been accompanied by members of her family. Her brother Rashad stated that the filming of "Rock the Boat" was the only time her family was not present during a video shoot. In October 2001, Rashad said: "It really boggles everyone [that] from Day One, every single video she ever shot there's always been myself or my mother or my father there. The circumstances surrounding this last video were really strange because my mother had eye surgery and couldn't fly. That really bothered her because she always traveled. My dad had to take care of my mom at that time. And I went to Australia to visit some friends. We really couldn't understand why we weren't there. You ask yourself maybe we could have stopped it. But you can't really answer the question. There's always gonna be that question of why." Her friend Kidada Jones said in the last year of Aaliyah's life, her parents had given her more freedom and she had spoken about wanting a family.

Aaliyah reportedly developed an intimate relationship with Kelly during the recording of her debut album. She told "Vibe" magazine in 1994 that she and Kelly would "go watch a movie" and "go eat" when she got tired and would then "come back and work". She described the relationship between her and Kelly as "rather close." In December 1994, Aaliyah told the "Chicago Sun-Times" that whenever she was asked about being married to Kelly, she urged them not to believe "all that mess" and that she and Kelly were "close" and "people took it the wrong way".

With the release of "Age Ain't Nothing but a Number", rumors circulated about a relationship between Aaliyah and R. Kelly, including the allegation that they had secretly married without her parents' knowledge. "Vibe" magazine later revealed a marriage certificate that listed the couple married on August 31, 1994, in Sheraton Gateway Suites in Rosemont, Illinois. Aaliyah, who was 15 at the time, was listed as 18 on the certificate; R. Kelly was 27. The marriage was annulled by her parents in February 1995, but the pair denied the allegations, saying that neither was married and that the certificate was a forgery.

Jamie Foster Brown in the 1994 issue of "Sister 2 Sister" wrote that "R. Kelly told me that he and Aaliyah got together, and it was just magic." Brown also reported hearing about a sexual relationship between them. "I've been hearing about Robert and Aaliyah for a while—that she was pregnant. Or that she was coming and going in and out of his house. People would see her walking his dog, 12 Play, with her basketball cap and sunglasses on. Every time I asked the label, they said it was platonic. But I kept hearing complaints from people about her being in the studio with all those men." Brown later added "at 15, you have all those hormones and no brains attached to them".

In his 2011 book "The Man Behind the Man: Looking from the Inside Out", Demetrius Smith Sr., Kelly's former tour manager, revealed that Kelly married Aaliyah after she told him that she was pregnant. In the 2019 documentary "Surviving R. Kelly", Smith described how he helped Aaliyah forge the necessary documents to show she was 18 to marry Kelly. Smith also said he was "not proud" of his role in facilitating their marriage. Additionally, the documentary revealed that Jovante Cunningham, a former backup dancer, claimed to have witnessed Kelly having sex with Aaliyah on his tour bus.

Aaliyah admitted in court documents that she had lied about her age. In May 1997, she filed suit in Cook County seeking to have all records of the marriage expunged because she was not old enough under state law to get married without her parents' consent. It was reported that she cut off all professional and personal ties with Kelly after the marriage was annulled and ceased contact with him. In a 2014 interview, Aaliyah's cousin Jomo Hankerson said that she "got villainized" for her relationship with Kelly and the scandal over the marriage made it difficult to find producers for her second album. "We were coming off of a multi-platinum debut album and except for a couple of relationships with Jermaine Dupri and Puffy, it was hard for us to get producers on the album." Hankerson also expressed confusion over why "they were upset" with Aaliyah given her age at the time.

Aaliyah was known to avoid answering questions about Kelly after the professional split. During an interview with Christopher John Farley, she was asked whether she was still in contact with him and would ever work with him again. Farley said Aaliyah responded with a "firm, frosty 'no to both questions. "Vibe" magazine said Aaliyah changed the subject anytime "you bring up the marriage with her". A spokeswoman for Aaliyah said in 2000 that when "R. Kelly comes up, she doesn't even speak his name, and nobody's allowed to ask about it at all." Kelly later said that Aaliyah had opportunities to address their relationship after they separated professionally but chose not to. In 2019, Damon Dash revealed to "Hip Hop Motivation" that Aaliyah did not even speak of her relationship with Kelly in private; he tried multiple times to discuss it with her, but she would only say that Kelly was a "bad man". Dash said he was unable to watch "Surviving R. Kelly" because its interviews with visibly traumatized girls struggling to discuss their encounters with Kelly reminded him of how Aaliyah behaved when trying to recount her relationship with Kelly. Dash later appeared in "" in 2020.

Other allegations were made about Kelly regarding underage girls in the years after Aaliyah's death, and their marriage was used as an example of his involvement with them. He has refused to discuss his relationship with her, citing her death. "Out of respect for her, and her mom and her dad, I will not discuss Aaliyah. That was a whole other situation, a whole other time, it was a whole other thing, and I'm sure that people also know that." In 2016, Kelly said that he was as in love with Aaliyah as he was with "anybody else." Aaliyah's mother, Diane Haughton, reflected that everything "that went wrong in her life" began with her relationship with Kelly.

After the documentary "Surviving R. Kelly" aired in January 2019, pressure from the public using the Mute R. Kelly hashtag escalated and RCA Records dropped Kelly from the label. In February 2019, Kelly was indicted on ten counts of aggravated criminal sexual abuse. In July 2019, he was arrested on federal charges of sex crimes, human trafficking, child pornography, racketeering, and obstruction of justice. When his trial began in August 2021, Kelly faced 22 federal criminal charges that involved allegedly abusing 11 girls and women between 1994 and 2018. Aaliyah's illegal marriage to Kelly was heavily featured in the court case. On September 27, 2021, a federal court jury found Kelly guilty of nine counts including racketeering, sexual exploitation of a child, kidnapping, bribery, sex trafficking, and a violation of the Mann Act. The judge ordered that Kelly remain in custody pending sentencing, which was set for May 4, 2022. On June 29, 2022, Kelly was sentenced to 30 years in prison.

Aaliyah was dating the co-founder of Roc-A-Fella Records, Damon Dash, at the time of her death. Although they were not formally engaged, Dash claimed the couple had planned to marry in interviews given after Aaliyah's death. In the summer of 2000, Aaliyah was introduced to Dash by his accountant and they formed a friendship. Aaliyah never publicly addressed their relationship as anything but platonic. Due to their hectic work schedules, Aaliyah and Dash were separated for long periods of time. Jay-Z mentioned Aaliyah and Dash in the remix of her song "Miss You", released in 2003. In August 2021, Dash told "Entertainment Tonight" Kevin Frazier, "I was reflecting [that] there hasn't been one day since she's passed, not one in the 20 years, that I haven't either heard her name, heard her record, or seen a picture of her ... Every single day she's present in my life and I feel lucky for that."

On August 25, 2001, at 6:50 p.m. (EDT), Aaliyah and some employees of her record company boarded a twin-engine Cessna 402 light aircraft at the Marsh Harbour Airport in Abaco Islands, the Bahamas, to travel to Opa-Locka Airport in Florida after they completed filming the video for "Rock the Boat". They had a flight scheduled the next day, but with filming finishing early, Aaliyah and her entourage were eager to return to the US and decided to leave immediately. The designated airplane was smaller than the Cessna 404 on which they had originally arrived, but the whole party and all the equipment were accommodated on board. The plane crashed and caught fire shortly after takeoff, about from the end of the runway.

Aaliyah and the eight others on board—pilot Luis Morales III, hair stylist Eric Forman, Anthony Dodd, security guard Scott Gallin, family friend Keith Wallace, make-up stylist Christopher Maldonado, and Blackground Records employees Douglas Kratz and Gina Smith—were killed.

The passengers had grown impatient because the Cessna was supposed to arrive at 4:30 pm. EDT, but did not arrive until 6:15 pm. Another charter pilot, Lewis Key, claimed to have overheard passengers arguing with their pilot, Luis Morales, before takeoff, adding that Morales warned them that there was too much weight for a "safe flight". Key added: "He tried to convince them the plane was overloaded, but they insisted they had chartered the plane and they had to be in Miami Saturday night." Key indicated that Morales gave in to the passengers and that he had trouble starting one of the engines.

According to findings from an inquest conducted by the coroner's office in the Bahamas, Aaliyah had "severe burns and a blow to the head" in addition to severe shock and a weak heart. The coroner theorized that she went into such a state of shock that even if she had survived the crash, her recovery would have been nearly impossible given the severity of her injuries. The bodies were taken to the morgue at Princess Margaret Hospital in Nassau, where they were kept for relatives to help identify them. Some of them were badly burned.

As the subsequent investigation determined, the aircraft was overloaded by when it attempted to take off, and was carrying one more passenger than it was certified for. The National Transportation Safety Board reported, "The airplane was seen lifting off the runway, and then nose down, impacting in a marsh on the south side of the departure end of runway 27." The report indicated that the pilot was not approved to fly the plane. Morales falsely obtained his FAA license by showing hundreds of hours never flown, and he may also have falsified how many hours he had flown to get a job with his employer, Blackhawk International Airways. Additionally, toxicology tests performed on Morales revealed traces of cocaine and alcohol in his system.

Aaliyah's private funeral Mass was held on August 31, 2001, at the Church of St. Ignatius Loyola in Manhattan, following a procession from the Frank E. Campbell Funeral Chapel. Her body was set in a silver-plated copper-deposit casket, which was carried in a horse-drawn, glass hearse. An estimated 800 mourners attended the procession.

Among those in attendance at the private ceremony were Missy Elliott, Timbaland, Gladys Knight, Lil' Kim, and Sean Combs. After the service, 22 white doves were released to symbolize each year of her life.

Aaliyah's brother Rashad delivered the eulogy and described his sister as giving him strength: "Aaliyah, you left, but I'll see you always next to me and I can see you smiling through the sunshine. When our life is over, our book is done. I hope God keeps me strong until I see her again." He read the names of the other victims of the crash and concluded by asking mourners to pray for them as well. As Diane Haughton and the mourners left, they sang Aaliyah's song "One in a Million".

Immediately after Aaliyah's death, there was uncertainty over whether the music video for "Rock the Boat" would ever air. It made its world premiere on BET's "Access Granted" on October 9, 2001. She won two posthumous awards at the American Music Awards of 2002; Favorite Female R&B Artist and Favorite R&B/Soul Album for "Aaliyah". Her second and final film, "Queen of the Damned", was released in February 2002. Before its release, Aaliyah's brother, Rashad, re-dubbed some of her lines during post-production. It grossed US$15.2 million in its first weekend, ranking number one at the box office. On the first anniversary of Aaliyah's death, a candlelight vigil was held in Times Square; millions of fans observed a moment of silence; and throughout the United States, radio stations played her music in remembrance. In December 2002, a collection of both previously unreleased and released material was issued as Aaliyah's first posthumous album, "I Care 4 U". A portion of the proceeds was donated to the Aaliyah Memorial Fund, a program that benefits the Revlon UCLA Women's Cancer Research Program and Harlem's Sloan Kettering Cancer Center. It debuted at number three on the US "Billboard" 200, selling 280,000 copies in its first week. The album's lead single, "Miss You", peaked at number three on the "Billboard" Hot 100 and topped the Hot R&B/Hip-Hop Songs chart for three weeks. In August of the following year, luxury fashion house Dior donated profits from sales in honor of Aaliyah.

In April 2005, Aaliyah's second posthumous album, a double CD+DVD box set titled "Ultimate Aaliyah", was released in the United Kingdom by Blackground Records. A documentary movie "Aaliyah Live in Amsterdam" was released in 2011, shortly before the tenth anniversary of Aaliyah's death. The documentary, by Pogus Caesar, contained previously unseen footage shot of her career beginnings in 1995 when she was appearing in the Netherlands.

In March 2012, music producer Jeffrey "J-Dub" Walker announced that the song "Steady Ground", which he produced for Aaliyah's third album, would be included in a posthumous Aaliyah album. Aaliyah's brother Rashad denied Walker's claim. On August 5, 2012, Blackground Records released the track "Enough Said" which was produced by Noah "40" Shebib and features Canadian rapper Drake. Four days later, Jomo Hankerson claimed a posthumous album was being produced that would feature new production by Timbaland and Missy Elliot, who both later denied being involved with the project. The album was scheduled to be released by the end of 2012 by Blackground Records, but its release was shelved. In June 2013, Aaliyah was featured on the track "Don't Think They Know" by Chris Brown, which appears on Brown's sixth studio album, "X". Timbaland voiced his disapproval for "Enough Said" and "Don't Think They Know" in July 2013, but later apologized to Chris Brown, explaining that Aaliyah and her death were a "very sensitive subject".

In May 2015, Aaliyah was featured on the Tink track "Million", which contained samples from her song "One in a Million". In September 2015, "Aaliyah by Xyrena", an official tribute fragrance, was announced. On December 25, 2015, Timbaland released the mixtape "Kings Stay Kings" which includes the unreleased Aaliyah song "Shakin" featuring rapper Strado. In June 2018, MAC Cosmetics released a cosmetics collection inspired by Aaliyah, The Aaliyah for Mac collection which was priced at $250 and sold out within minutes. MAC and i-D Magazine partnered up to release a short film titled "A-Z of Aaliyah" which coincided with the launch. On August 21, 2019, the Madame Tussauds museum unveiled a wax figure of Aaliyah at their Las Vegas location, modeled on her appearance in the "Try Again" music video. Four days later, Aaliyah's family announced that they were in talks with record companies to discuss the future of her discography.

In March 2021, Funko Pop! released an Aaliyah figurine. In August 2021, Blackground Records announced that Aaliyah's recorded works would be re-released on physical, digital, and streaming services in a deal between the label and Empire Distribution. Aaliyah's estate issued a statement in response to the announcement, denouncing the "unscrupulous endeavor to release Aaliyah's music without any transparency or full accounting to the estate". "One in a Million" was reissued on August 20. After the album's re-release, "One in a Million" re-entered the UK Official Hip Hop and R&B Albums Chart Top 40 at number eight. In the US, the album reached the top ten on the "Billboard" 200 for the first time at number ten, selling 26,000 album-equivalent units in the week ending of August 26.

"Aaliyah" was reissued September 10, 2021. After the album's re-release, "Aaliyah" re-entered the UK Official Hip Hop and R&B Albums Chart Top 40 at number seven and re-entered the US Billboard 200 chart at number 13. In celebration of the reissue, Blackground released an animated commercial titled "It's Been A Long Time" (in a similar style to the album's original 2001 commercial), directed by Takahiro Tanaka, showing Aaliyah resurrecting her music from out of a large underground vault. Compilation albums "I Care 4 U" and "Ultimate Aaliyah" were reissued October 8, 2021. "Ultimate Aaliyah" peaked at number 8 on the UK R&B Albums Chart Top 40 and charted for the first time in the US at number 41 on the "Billboard" 200.

On August 25, 2021, Barry Hankerson revealed in an interview with Big Tigger for WVEE that a posthumous album titled "Unstoppable" would be released in "a matter of weeks". The album will feature Drake, Snoop Dogg, Ne-Yo, Chris Brown, Future and use previously unreleased vocals from before Aaliyah's passing. On December 17, 2021, Background Records released the posthumous Aaliyah single "Poison" featuring The Weeknd. On January 4, 2022, Hankerson confirmed that "Unstoppable" would be released later that month, however, there was no sign of the album, nor any announcement from the label and as of 2024 remains unreleased.

Aaliyah has been credited for helping redefine R&B, pop and hip hop in the 1990s, "leaving an indelible imprint on the music industry as a whole." According to "Billboard", she revolutionized R&B with her sultry mix of pop, soul and hip hop. Peter Piatkowski from "PopMatters", stated, "Much like Janet Jackson's Control set a template of sorts for dance-pop divas in the 1980s, Aaliyah's patented brand of Black pop, which was a mélange of hip-hop, electropop, and soul, set a standard against which other young urban-pop singers were judged". In a 2001 review of her third album, Ernest Hardy from "Rolling Stone" professed that Aaliyah's impact on R&B and pop has been enormous. Steve Huey of AllMusic wrote Aaliyah ranks among the "elite" artists of the R&B genre, as she "played a major role in popularizing the stuttering, futuristic production style that consumed hip-hop and urban soul in the late 1990s." Critic Bruce Britt stated that by combining "schoolgirl charm with urban grit, Aaliyah helped define the teen-oriented sound that has resulted in contemporary pop phenom's like Brandy, Christina Aguilera and Destiny's Child".

Described as one of "R&B's most important artists" during the 1990s, her second studio album, "One in a Million", became one of the most influential R&B albums of the decade. Music critic Simon Reynolds cited "Are You That Somebody?" as "the most radical pop single" of 1998. Kelefah Sanneh of "The New York Times" wrote that rather than being the song's focal point, Aaliyah "knew how to disappear into the music, how to match her voice to the bass line", and consequently "helped change the way popular music sounds; the twitchy, beat-driven songs of Destiny's Child owe a clear debt to 'Are You That Somebody'." Sanneh asserted that by the time of her death in 2001, Aaliyah "had recorded some of the most innovative and influential pop songs of the last five years." Music publication "Popdust" called Aaliyah an unlikely queen of the underground for her influence on the underground alternative music scene. The publication also mentioned that the forward-thinking music Aaliyah made with Timbaland and the experimental music being made by many underground alternative artists are "somewhat cut from the same cloth". While compiling a list of artists that take cues from Aaliyah, MTV Hive stated that it's easy to spot her influence on underground movements like dubstep, strains of indie pop, and lo-fi R&B movements. Erika Ramirez, an associate editor of "Billboard", said at the time of Aaliyah's career "there weren't many artists using the kind of soft vocals the ways she was using it, and now you see a lot of artists doing that and finding success". Ramirez argued that Aaliyah's second album "One in a Million" was "very much ahead of its time, with the bass and electro kind of R&B sounds that they produced", and that the sound, "really stood out" at its time, was being replicated.

There has been continuing belief that Aaliyah would have achieved greater career success had it not been for her death. Emil Wilbekin mentioned the deaths of The Notorious B.I.G. and Tupac Shakur in conjunction with hers and added: "Her just-released third album and scheduled role in a sequel to "The Matrix" could have made her another Janet Jackson or Whitney Houston". Director of "Queen of the Damned" Michael Rymer said of Aaliyah, "God, that girl could have gone so far" and spoke of her having "such a clarity about what she wanted. Nothing was gonna step in her way. No ego, no nervousness, no manipulation. There was nothing to stop her." On July 18, 2014, it was announced that Alexandra Shipp replaced Zendaya for the role of Aaliyah for the Lifetime TV biopic movie "", which premiered on November 15, 2014. Zendaya drew criticism because people felt that she was too light skinned and did not greatly resemble Aaliyah. She voiced her strong respect for Aaliyah before dropping out of the project. She explained her choice to withdraw from the film in videos on Instagram. Aaliyah's family has been vocal in their disapproving of the film. Her cousin Jomo Hankerson stated the family would prefer a "major studio release along the lines" of "What's Love Got to Do with It", the biopic based on the life of Tina Turner. Aaliyah's family has consulted a lawyer to stop Lifetime from using "any of the music, or any of the photographs and videos" they own and Jomo Hankerson claimed the TV network "didn't reach out." On August 9, 2014, it was announced that Chattrisse Dolabaille and Izaak Smith had been cast as Aaliyah's collaborators Missy Elliott and Timbaland. Dolabaille and Smith both received criticism for their appearances in comparison with that of Missy Elliot and Timbaland. Despite negative reviews, the film's premiere drew 3.2 million viewers, becoming the second highest rated television movie of 2014.

On August 17, 2021, Atria Books (an imprint of Simon & Schuster) published Kathy Iandoli's "Baby Girl: Better Known as Aaliyah", a biography that draws on interviews with Aaliyah's friends, mentors and family, and document how her career influenced a new generation of artists. It has not been authorized by the Haughton family. On August 5, 2022, Beyoncé released "The Queens Remix" to her single "Break My Soul", in which she name-drops Aaliyah, along with other cultural icons. On June 14, 2023, Aaliyah was the subject of the documentary "Superstar: Aaliyah", which was broadcast on ABC. The documentary included interviews with Damon Dash, Barry Hankerson, Sevyn Streeter, Will.i.am, Justine Skye, and author Kathy Iandoli, and discussed Aaliyah's life, career and legacy.

Aaliyah has sold 8.1 million albums in the United States and an estimated 24 to 32 million albums worldwide. Throughout the years, she has earned several honorific nicknames, including "Princess of R&B", "Pop Princess", and "Queen of Urban Pop", 
as she "proved she was a muse in her own right". While Ernest Hardy of "Rolling Stone" dubbed her the "undisputed queen of the midtempo come-on". She also has been referred to as a pop and R&B icon for her impact on those genres.

At the 2001 MTV Video Music Awards, Aaliyah was honored by Janet Jackson, Missy Elliott, Timbaland, Ginuwine and her brother, Rashad, who all paid tribute to her. Also during 2001, the United States Social Security Administration ranked the name Aaliyah as one of the 100 most popular names for newborn girls. In 2003 Aaliyah was ranked as one of "The Top 40 Women of the Video Era" in VH1's "The Greatest" series. Also, in 2003 in memory of Aaliyah, the Entertainment Industry Foundation created the Aaliyah Memorial Fund to donate money raised to charities she supported. In 2008, she was ranked at number 18 on BET's "Top 25 Dancers of All Time". In December 2009, "Billboard" ranked Aaliyah at number 70 on its Top Artists of the Decade, while her album "Aaliyah" was ranked at number 181 on the magazine's Top 200 Albums of the Decade. In 2010 "Billboard" listed her as the tenth most successful female R&B artist of the past 25 years, and 27th most successful R&B artist overall. In 2011, Essence ranked her at number 14 on its 50 Most Influential R&B Starts list. In 2012, VH1 ranked her number 48 on their "Greatest Women in Music". In 2014, "NME" ranked her at number 18 on its 100 most influential artist list. In August 2018, "Billboard" ranked Aaliyah at number 47 on their Top 60 Female Artists of All-Time list. In 2020, the publication included her on its list of the 100 Greatest Music Video Artists of All Time. "Rolling Stone" ranked her at number 40 on their 200 Best Singers of All Time list. In September 2023, she was inducted into the National Rhythm & Blues Hall of Fame.






Armour

Armour (Commonwealth English) or armor (American English; see spelling differences) is a covering used to protect an object, individual, or vehicle from physical injury or damage, especially direct contact weapons or projectiles during combat, or from a potentially dangerous environment or activity (e.g. cycling, construction sites, etc.). Personal armour is used to protect soldiers and war animals. Vehicle armour is used on warships, armoured fighting vehicles, and some combat aircraft, mostly ground attack aircraft.

A second use of the term "armour" describes armoured forces, armoured weapons, and their role in combat. After the development of armoured warfare, tanks and mechanised infantry and their combat formations came to be referred to collectively as "armour".

The word "armour" began to appear in the Middle Ages as a derivative of Old French. It is dated from 1297 as a "mail, defensive covering worn in combat". The word originates from the Old French , itself derived from the Latin meaning "arms and/or equipment", with the root meaning "arms or gear".

Armour has been used throughout recorded history. It has been made from a variety of materials, beginning with the use of leathers or fabrics as protection and evolving through chain mail and metal plate into today's modern composites. For much of military history the manufacture of metal personal armour has dominated the technology and employment of armour.

Armour drove the development of many important technologies of the Ancient World, including wood lamination, mining, metal refining, vehicle manufacture, leather processing, and later decorative metal working. Its production was influential in the industrial revolution, and furthered commercial development of metallurgy and engineering. Armour was the single most influential factor in the development of firearms, which in turn revolutionised warfare.

Significant factors in the development of armour include the economic and technological necessities of its production. For instance, plate armour first appeared in Medieval Europe when water-powered trip hammers made the formation of plates faster and cheaper. At times the development of armour has paralleled the development of increasingly effective weaponry on the battlefield, with armourers seeking to create better protection without sacrificing mobility.

Well-known armour types in European history include the lorica hamata, lorica squamata, and the lorica segmentata of the Roman legions, the mail hauberk of the early medieval age, and the full steel plate harness worn by later medieval and renaissance knights, and breast and back plates worn by heavy cavalry in several European countries until the first year of World War I (1914–1915). The samurai warriors of Feudal Japan utilised many types of armour for hundreds of years up to the 19th century.

Cuirasses and helmets were manufactured in Japan as early as the 4th century. "Tankō", worn by foot soldiers and "keikō", worn by horsemen were both pre-samurai types of early Japanese armour constructed from iron plates connected together by leather thongs. Japanese lamellar armour ("keiko") passed through Korea and reached Japan around the 5th century. These early Japanese lamellar armours took the form of a sleeveless jacket, leggings and a helmet.

Armour did not always cover all of the body; sometimes no more than a helmet and leg plates were worn. The rest of the body was generally protected by means of a large shield. Examples of armies equipping their troops in this fashion were the Aztecs (13th to 15th century CE).

In East Asia, many types of armour were commonly used at different times by various cultures, including scale armour, lamellar armour, laminar armour, plated mail, mail, plate armour, and brigandine. Around the dynastic Tang, Song, and early Ming Period, cuirasses and plates (mingguangjia) were also used, with more elaborate versions for officers in war. The Chinese, during that time used partial plates for "important" body parts instead of covering their whole body since too much plate armour hinders their martial arts movement. The other body parts were covered in cloth, leather, lamellar, or Mountain pattern. In pre-Qin dynasty times, leather armour was made out of various animals, with more exotic ones such as the rhinoceros.

Mail, sometimes called "chainmail", made of interlocking iron rings is believed to have first appeared some time after 300 BC. Its invention is credited to the Celts; the Romans are thought to have adopted their design.

Gradually, small additional plates or discs of iron were added to the mail to protect vulnerable areas. Hardened leather and splinted construction were used for arm and leg pieces. The coat of plates was developed, an armour made of large plates sewn inside a textile or leather coat.

Early plate in Italy, and elsewhere in the 13th–15th century, were made of iron. Iron armour could be carburised or case hardened to give a surface of harder steel. Plate armour became cheaper than mail by the 15th century as it required much less labour and labour had become much more expensive after the Black Death, though it did require larger furnaces to produce larger blooms. Mail continued to be used to protect those joints which could not be adequately protected by plate, such as the armpit, crook of the elbow and groin. Another advantage of plate was that a lance rest could be fitted to the breast plate.

The small skull cap evolved into a bigger true helmet, the bascinet, as it was lengthened downward to protect the back of the neck and the sides of the head. Additionally, several new forms of fully enclosed helmets were introduced in the late 14th century.
Probably the most recognised style of armour in the world became the plate armour associated with the knights of the European Late Middle Ages, but continuing to the early 17th century Age of Enlightenment in all European countries.

By 1400, the full harness of plate armour had been developed in armouries of Lombardy. Heavy cavalry dominated the battlefield for centuries in part because of their armour.

In the early 15th century, advances in weaponry allowed infantry to defeat armoured knights on the battlefield. The quality of the metal used in armour deteriorated as armies became bigger and armour was made thicker, necessitating breeding of larger cavalry horses. If during the 14–15th centuries armour seldom weighed more than 15 kg, then by the late 16th century it weighed 25 kg. The increasing weight and thickness of late 16th century armour therefore gave substantial resistance.

In the early years of low velocity firearms, full suits of armour, or breast plates actually stopped bullets fired from a modest distance. Crossbow bolts, if still in use, would seldom penetrate good plate, nor would any bullet unless fired from close range. In effect, rather than making plate armour obsolete, the use of firearms stimulated the development of plate armour into its later stages. For most of that period, it allowed horsemen to fight while being the targets of defending arquebusiers without being easily killed. Full suits of armour were actually worn by generals and princely commanders right up to the second decade of the 18th century. It was the only way they could be mounted and survey the overall battlefield with safety from distant musket fire.

The horse was afforded protection from lances and infantry weapons by steel plate barding. This gave the horse protection and enhanced the visual impression of a mounted knight. Late in the era, elaborate barding was used in parade armour.

Gradually, starting in the mid-16th century, one plate element after another was discarded to save weight for foot soldiers.

Back and breast plates continued to be used throughout the entire period of the 18th century and through Napoleonic times, in many European heavy cavalry units, until the early 20th century. From their introduction, muskets could pierce plate armour, so cavalry had to be far more mindful of the fire. In Japan, armour continued to be used until the late 19th century, with the last major fighting in which armour was used, this occurred in 1868. Samurai armour had one last short lived use in 1877 during the Satsuma Rebellion.

Though the age of the knight was over, armour continued to be used in many capacities. Soldiers in the American Civil War bought iron and steel vests from peddlers (both sides had considered but rejected body armour for standard issue). The effectiveness of the vests varied widely, some successfully deflected bullets and saved lives, but others were poorly made and resulted in tragedy for the soldiers. In any case the vests were abandoned by many soldiers due to their increased weight on long marches, as well as the stigma they got for being cowards from their fellow troops.

At the start of World War I, thousands of the French Cuirassiers rode out to engage the German Cavalry. By that period, the shiny metallic cuirass was covered in a dark paint and a canvas wrap covered their elaborate Napoleonic style helmets, to help mitigate the sunlight being reflected off the surfaces, thereby alerting the enemy of their location. Their armour was only meant for protection against edged weapons such as bayonets, sabres, and lances. Cavalry had to be wary of repeating rifles, machine guns, and artillery, unlike the foot soldiers, who at least had a trench to give them some protection.

Today, ballistic vests, also known as flak jackets, made of ballistic cloth (e.g. kevlar, dyneema, twaron, spectra etc.) and ceramic or metal plates are common among police officers, security guards, corrections officers and some branches of the military.

The US Army has adopted Interceptor body armour, which uses Enhanced Small Arms Protective Inserts (ESAPIs) in the chest, sides, and back of the armour. Each plate is rated to stop a range of ammunition including 3 hits from a 7.62×51 NATO AP round at a range of . Dragon Skin is another ballistic vest which is currently in testing with mixed results. As of 2019, it has been deemed too heavy, expensive, and unreliable, in comparison to more traditional plates, and it is outdated in protection compared to modern US IOTV armour, and even in testing was deemed a downgrade from the IBA.

The British Armed Forces also have their own armour, known as Osprey. It is rated to the same general equivalent standard as the US counterpart, the Improved Outer Tactical Vest, and now the Soldier Plate Carrier System and Modular Tactical Vest.

The Russian Armed Forces also have armour, known as the 6B43, all the way to 6B45, depending on variant. Their armour runs on the GOST system, which, due to regional conditions, has resulted in a technically higher protective level overall.

The first modern production technology for armour plating was used by navies in the construction of the ironclad warship, reaching its pinnacle of development with the battleship. The first tanks were produced during World War I. Aerial armour has been used to protect pilots and aircraft systems since the First World War.

In modern ground forces' usage, the meaning of armour has expanded to include the role of troops in combat. After the evolution of armoured warfare, mechanised infantry were mounted in armoured fighting vehicles and replaced light infantry in many situations. In modern armoured warfare, armoured units equipped with tanks and infantry fighting vehicles serve the historic role of heavy cavalry, light cavalry, and dragoons, and belong to the armoured branch of warfare.

The first ironclad battleship, with iron armour over a wooden hull, , was launched by the French Navy in 1859 prompting the British Royal Navy to build a counter. The following year they launched , which was twice the size and had iron armour over an iron hull. After the first battle between two ironclads took place in 1862 during the American Civil War, it became clear that the ironclad had replaced the unarmoured line-of-battle ship as the most powerful warship afloat.

Ironclads were designed for several roles, including as high seas battleships, coastal defence ships, and long-range cruisers. The rapid evolution of warship design in the late 19th century transformed the ironclad from a wooden-hulled vessel which carried sails to supplement its steam engines into the steel-built, turreted battleships and cruisers familiar in the 20th century. This change was pushed forward by the development of heavier naval guns (the ironclads of the 1880s carried some of the heaviest guns ever mounted at sea), more sophisticated steam engines, and advances in metallurgy which made steel shipbuilding possible.

The rapid pace of change in the ironclad period meant that many ships were obsolete as soon as they were complete, and that naval tactics were in a state of flux. Many ironclads were built to make use of the ram or the torpedo, which a number of naval designers considered the crucial weapons of naval combat. There is no clear end to the ironclad period, but towards the end of the 1890s the term "ironclad" dropped out of use. New ships were increasingly constructed to a standard pattern and designated battleships or armoured cruisers.

Armoured trains saw use from the mid-19th to the mid-20th century, including the American Civil War (1861–1865), the Franco-Prussian War (1870–1871), the First and Second Boer Wars (1880–81 and 1899–1902), the Polish–Soviet War (1919–1921), the First (1914–1918) and Second World Wars (1939–1945) and the First Indochina War (1946–1954). The most intensive use of armoured trains was during the Russian Civil War (1918–1920).

Ancient siege engines were usually protected by wooden armour, often covered with wet hides or thin metal to prevent being easily burned.

Medieval war wagons were horse-drawn wagons that were similarly armoured. These contained guns or crossbowmen that could fire through gun-slits.

The first modern armoured fighting vehicles were armoured cars, developed circa 1900. These started as ordinary wheeled motor-cars protected by iron shields, typically mounting a machine gun.

During the First World War, the stalemate of trench warfare during on the Western Front spurred the development of the tank. It was envisioned as an armoured machine that could advance under fire from enemy rifles and machine guns, and respond with its own heavy guns. It used caterpillar tracks to cross ground broken up by shellfire and trenches.

With the development of effective anti-aircraft artillery in the period before the Second World War, military pilots, once the "knights of the air" during the First World War, became far more vulnerable to ground fire. As a response, armour plating was added to aircraft to protect aircrew and vulnerable areas such as engines and fuel tanks. Self-sealing fuel tanks functioned like armour in that they added protection but also increased weight and cost.

Tank armour has progressed from the Second World War armour forms, now incorporating not only harder composites, but also reactive armour designed to defeat shaped charges. As a result of this, the main battle tank (MBT) conceived in the Cold War era can survive multiple rocket-propelled grenade strikes with minimal effect on the crew or the operation of the vehicle. The light tanks that were the last descendants of the light cavalry during the Second World War have almost completely disappeared from the world's militaries due to increased lethality of the weapons available to the vehicle-mounted infantry.

The armoured personnel carrier (APC) was devised during the First World War. It allows the safe and rapid movement of infantry in a combat zone, minimising casualties and maximising mobility. APCs are fundamentally different from the previously used armoured half-tracks in that they offer a higher level of protection from artillery burst fragments, and greater mobility in more terrain types. The basic APC design was substantially expanded to an infantry fighting vehicle (IFV) when properties of an APC and a light tank were combined in one vehicle.

Naval armour has fundamentally changed from the Second World War doctrine of thicker plating to defend against shells, bombs and torpedoes. Passive defence naval armour is limited to kevlar or steel (either single layer or as spaced armour) protecting particularly vital areas from the effects of nearby impacts. Since ships cannot carry enough armour to completely protect against anti-ship missiles, they depend more on defensive weapons destroying incoming missiles, or causing them to miss by confusing their guidance systems with electronic warfare.

Although the role of the ground attack aircraft significantly diminished after the Korean War, it re-emerged during the Vietnam War, and in the recognition of this, the US Air Force authorised the design and production of what became the A-10 dedicated anti-armour and ground-attack aircraft that first saw action in the Gulf War.

High-voltage transformer fire barriers are often required to defeat ballistics from small arms as well as projectiles from transformer bushings and lightning arresters, which form part of large electrical transformers, per NFPA 850. Such fire barriers may be designed to inherently function as armour, or may be passive fire protection materials "augmented by armour", where care must be taken to ensure that the armour's reaction to fire does not cause issues with regards to the fire barrier being armoured to defeat explosions and projectiles in addition to fire, especially since both functions must be provided simultaneously, meaning they must be fire-tested together to provide realistic evidence of fitness for purpose.

Combat drones use little to no vehicular armour as they are not manned vessels, this results in them being lightweight and small in size.

Body armour for war horses has been used since at least 2000 BC. Cloth, leather, and metal protection covered cavalry horses in ancient civilisations, including ancient Egypt, Assyria, Persia, and Rome. Some formed heavy cavalry units of armoured horses and riders used to attack infantry and mounted archers. Armour for horses is called "barding" (also spelled "bard" or "barb") especially when used by European knights.

During the late Middle Ages as armour protection for knights became more effective, their mounts became targets. This vulnerability was exploited by the Scots at the Battle of Bannockburn in the 14th century, when horses were killed by the infantry, and for the English at the Battle of Crécy in the same century where longbowmen shot horses and the then dismounted French knights were killed by heavy infantry. Barding developed as a response to such events.

Examples of armour for horses could be found as far back as classical antiquity. Cataphracts, with scale armour for both rider and horse, are believed by many historians to have influenced the later European knights, via contact with the Byzantine Empire.

Surviving period examples of barding are rare; however, complete sets are on display at the Philadelphia Museum of Art, the Wallace Collection in London, the Royal Armouries in Leeds, and the Metropolitan Museum of Art in New York City. Horse armour could be made in whole or in part of cuir bouilli (hardened leather), but surviving examples of this are especially rare.

War elephants were first used in ancient times without armour, but armour was introduced because elephants injured by enemy weapons would often flee the battlefield. Elephant armour was often made from hardened leather, which was fitted onto an individual elephant while moist, then dried to create a hardened shell. Alternatively, metal armour pieces were sometimes sewn into heavy cloth. Later lamellar armour (small overlapping metal plates) was introduced. Full plate armour was not typically used due to its expense and the danger of the animal overheating.




Armoured fighting vehicle

An armoured fighting vehicle (British English) or armored fighting vehicle (American English) (AFV) is an armed combat vehicle protected by armour, generally combining operational mobility with offensive and defensive capabilities. AFVs can be wheeled or tracked. Examples of AFVs are tanks, armoured cars, assault guns, self-propelled artilleries, infantry fighting vehicles (IFV), and armoured personnel carriers (APC).

Armoured fighting vehicles are classified according to their characteristics and intended role on the battlefield. The classifications are not absolute; two countries may classify the same vehicle differently, and the criteria change over time. For example, relatively lightly armed armoured personnel carriers were largely superseded by infantry fighting vehicles with much heavier armament in a similar role.

Successful designs are often adapted to a wide variety of applications. For example, the MOWAG Piranha, originally designed as an APC, has been adapted to fill numerous roles such as a mortar carrier, infantry fighting vehicle, and assault gun.

Armoured fighting vehicles began to appear in use in World War I with the armoured car, the tank, the self-propelled gun, and the personnel carrier seeing use. By World War II, armies had large numbers of AFVs, together with other vehicles to carry troops this permitted highly mobile manoeuvre warfare.

The concept of a highly mobile and protected fighting unit has been around for centuries; from Hannibal's war elephants to Leonardo's contraptions, military strategists endeavoured to maximize the mobility and survivability of their soldiers.

Armoured fighting vehicles were not possible until internal combustion engines of sufficient power became available at the start of the 20th century.

Modern armoured fighting vehicles represent the realization of an ancient concept – that of providing troops with mobile protection and firepower. Armies have deployed war machines and cavalries with rudimentary armour in battle for millennia. Use of these animals and engineering designs sought to achieve a balance between the conflicting paradoxical needs of mobility, firepower and protection.

Siege engines, such as battering rams and siege towers, would often be armoured in order to protect their crews from enemy action. Polyidus of Thessaly developed a very large movable siege tower, the "helepolis", as early as 340 BC, and Greek forces used such structures in the Siege of Rhodes (305 BC).

The idea of a protected fighting vehicle has been known since antiquity. Frequently cited is Leonardo da Vinci's 15th-century sketch of a mobile, protected gun-platform; the drawings show a conical, wooden shelter with apertures for cannons around the circumference. The machine was to be mounted on four wheels which would be turned by the crew through a system of hand cranks and cage (or "lantern") gears. Leonardo claimed: "I will build armoured wagons which will be safe and invulnerable to enemy attacks. There will be no obstacle which it cannot overcome." Modern replicas have demonstrated that the human crew would have been able to move it over only short distances.

Hussite forces in Bohemia developed war wagons – medieval horse-drawn wagons that doubled as wagon forts – around 1420 during the Hussite Wars. These heavy wagons were given protective sides with firing slits; their heavy firepower came from either a cannon or from a force of hand-gunners and crossbowmen, supported by light cavalry and infantry using pikes and flails. Heavy arquebuses mounted on wagons were called "arquebus à croc". These carried a ball of about . 

By the end of World War II, most modern armies had vehicles to carry infantry, artillery and anti-aircraft weaponry. Most modern AFVs are superficially similar in design to their World War II counterparts, but with significantly better armour, weapons, engines, electronics, and suspension. The increase in the capacity of transport aircraft makes possible and practicable the transport of AFVs by air. Many armies are replacing some or all of their traditional heavy vehicles with lighter airmobile versions, often with wheels instead of tracks.

The first modern AFVs were armed cars, dating back virtually to the invention of the motor car. The British inventor F. R. Simms designed and built the Motor Scout in 1898. It was the first armed, petrol-engine powered vehicle ever built. It consisted of a De Dion-Bouton quadracycle with a Maxim machine gun mounted on the front bar. An iron shield offered some protection for the driver from the front, but it lacked all-around protective armour.

The armoured car was the first modern fully armoured fighting vehicle. The first of these was the Simms's Motor War Car, also designed by Simms and built by Vickers, Sons & Maxim in 1899. The vehicle had Vickers armour 6 mm thick and was powered by a four-cylinder 3.3-litre 16 hp Cannstatt Daimler engine giving it a maximum speed of around . The armament, consisting of two Maxim guns, was carried in two turrets with 360° traverse.
Another early armoured car of the period was the French Charron, Girardot et Voigt 1902, presented at the "Salon de l'Automobile et du cycle" in Brussels, on 8 March 1902. The vehicle was equipped with a Hotchkiss machine gun, and with 7 mm armour for the gunner. Armoured cars were first used in large numbers on both sides during World War I as scouting vehicles.

In 1903, H. G. Wells published the short story "The Land Ironclads," positing indomitable war machines that would bring a new age of land warfare, the way steam-powered ironclad warships had ended the age of sail.

Wells's literary vision was realized in 1916, when, amidst the pyrrhic standstill of the Great War, the British Landship Committee deployed revolutionary armoured vehicles to break the stalemate. The tank was envisioned as an armoured machine that could cross ground under fire from machine guns and reply with its own mounted machine guns and naval artillery. These first British tanks of World War I moved on caterpillar tracks that had substantially lower ground pressure than wheeled vehicles, enabling them to pass the muddy, pocked terrain and slit trenches of the Battle of the Somme.

The tank eventually proved highly successful and, as technology improved, it became a weapon that could cross large distances at much higher speeds than supporting infantry and artillery. The need to provide the units that would fight alongside the tank led to the development of a wide range of specialised AFVs, especially during the Second World War (1939–1945).

The armoured personnel carrier, designed to transport infantry troops to the frontline, emerged towards the end of World War I. During the first actions with tanks, it had become clear that close contact with infantry was essential in order to secure ground won by the tanks. Troops on foot were vulnerable to enemy fire, but they could not be transported in the tank because of the intense heat and noxious atmosphere. In 1917, Lieutenant G. J. Rackham was ordered to design an armoured vehicle that could fight and carry troops or supplies. The Mark IX tank was built by Armstrong, Whitworth & Co., although just three vehicles had been finished at the time of the Armistice in November 1918, and only 34 were built in total.

Different tank classifications emerged in the interwar period. The tankette was conceived as a mobile, two-man model, mainly intended for reconnaissance. In 1925, Sir John Carden and Vivian Loyd produced the first such design to be adopted – the Carden Loyd tankette. Tankettes saw use in the Royal Italian Army during the Italian invasion of Ethiopia (1935–1936), the Spanish Civil War (1936–1939), and almost everywhere Italian soldiers fought during World War II. The Imperial Japanese Army used tankettes for jungle warfare.

The British Gun Carrier Mark I, the first Self-propelled artillery, was fielded in 1917. It was based on the first tank, the British Mark I, and carried a heavy field-gun. The next major advance was the Birch gun (1925), developed for the British motorised warfare experimental brigade (the Experimental Mechanized Force). This mounted a field gun, capable of the usual artillery trajectories and even anti-aircraft use, on a tank chassis.

During World War II, most major military powers developed self-propelled artillery vehicles. These had guns mounted on a tracked chassis (often that of an obsolete or superseded tank) and provided an armoured superstructure to protect the gun and its crew. The first British design, "Bishop", carried the 25 pdr gun-howitzer in an extemporised mounting on a tank chassis that severely limited the gun's performance. It was replaced by the more effective Sexton. The Germans built many lightly armoured self-propelled anti-tank guns using captured French equipment (for example Marder I), their own obsolete light tank chassis (Marder II), or ex-Czech chassis (Marder III). These led to better-protected tank destroyers, built on a medium-tank chassis such as the Jagdpanzer IV or the Jagdpanther.

The Self-propelled anti-aircraft weapon debuted in WWI. The German 88 mm anti-aircraft gun was truck-mounted and used to great effect against British tanks, and the British QF 3-inch 20 cwt was mounted on trucks for use on the Western Front. Although the Birch gun was a general purpose artillery piece on an armoured tracked chassis, it was capable of elevation for anti-aircraft use. Vickers Armstrong developed one of the first SPAAGs based on the chassis of the Mk.E 6-ton light tank/Dragon Medium Mark IV tractor, mounting a Vickers QF-1 "Pom-Pom" gun of 40 mm. The Germans fielded the Sd.Kfz. 10/4 and 6/2, cargo halftracks mounting single 20 mm or 37 mm AA guns (respectively) by the start of the war.

Rocket launchers such as the Soviet Katyusha originated in the late 1930s. The Wehrmacht fielded self-propelled rocket artillery in World War II – the Panzerwerfer and Wurfrahmen 40 equipped half-track armoured fighting vehicles. Many modern multiple rocket launchers are self propelled by either truck or tank chassis.

The level of armour protection between AFVs varies greatly – a main battle tank will normally be designed to take hits from other tank guns and anti-tank missiles, whilst light reconnaissance vehicles are often only armoured "just in case". Whilst heavier armour provides better protection, it makes vehicles less mobile (for a given engine power), limits its air-transportability, increases cost, uses more fuel and may limit the places it can go – for example, many bridges may be unable to support the weight of a main battle tank. A trend toward composite armour is taking the place of steel – composites are stronger for a given weight, allowing the tank to be lighter for the same protection as steel armour, or better protected for the same weight. Armour is being supplemented with active protection systems on a number of vehicles, allowing the AFV to protect itself from incoming projectiles.
The level of protection also usually varies considerably throughout the individual vehicle too, depending on the role of the vehicle and the likely direction of attack. For example, a main battle tank will usually have the heaviest armour on the hull front and the turret, lighter armour on the sides of the hull and the thinnest armour on the top and bottom of the tank. Other vehicles – such as the MRAP family – may be primarily armoured against the threat from IEDs and so will have heavy, sloped armour on the bottom of the hull.

Weaponry varies by a very wide degree between AFVs – lighter vehicles for infantry carrying, reconnaissance or specialist roles may have only a autocannon or machine gun (or no armament at all), whereas heavy self-propelled artillery will carry howitzers, mortars or rocket launchers. These weapons may be mounted on a pintle, affixed directly to the vehicle or placed in a turret or cupola.

The greater the recoil of the weapon on an AFV, the larger the turret ring needs to be. A larger turret ring necessitates a larger vehicle. To avoid listing to the side, turrets on amphibious vehicles are usually located at the centre of the vehicle.

Grenade launchers provide a versatile launch platform for a plethora of munitions including, smoke, phosphorus, tear gas, illumination, anti-personnel, infrared and radar-jamming rounds.

Turret stabilization is an important capability because it enables firing on the move and prevents crew fatigue.

Modern AFVs have primarily used either petrol (gasoline) or diesel piston engines. More recently, gas turbines have been used. Most early AFVs used petrol engines, as they offer a good power-to-weight ratio. However, they fell out of favour during World War II due to the flammability of the fuel.

Most current AFVs are powered by a diesel engine; modern technology, including the use of turbo-charging, helps to overcome the lower power-to-weight ratio of diesel engines compared to petrol.

Gas turbine (turboshaft) engines offer a very high power-to-weight ratio and were starting to find favour in the late 20th century – however, they offer very poor fuel consumption and as such some armies are switching from gas turbines back to diesel engines (i.e. the Russian T-80 used a gas turbine engine, whereas the later T-90 does not). The US M1 Abrams is a notable example of a gas turbine powered tank.

Notable armoured fighting vehicles extending from post-World War I to today.

The tank is an all terrain AFV incorporating artillery which is designed to fill almost all battlefield roles and to engage enemy forces by the use of direct fire in the frontal assault role. Though several configurations have been tried, particularly in the early experimental "golden days" of tank development, a standard, mature design configuration has since emerged to a generally accepted pattern. This features a main tank gun or artillery gun, mounted in a fully rotating turret atop a tracked automotive hull, with various additional secondary weapon systems throughout.

Philosophically, the tank is, by its very nature, an offensive weapon. Being a protective encasement with at least one gun position, it is essentially a pillbox or small fortress (though these are static fortifications of a purely defensive nature) that can move toward the enemy – hence its offensive utility. Psychologically, the tank is a force multiplier that has a positive morale effect on the infantry it accompanies. It also instills fear in the opposing force who can often hear and even feel their arrival.

Tanks were classified either by size or by role.
Classification by relative size was common, as this also tended to influence the tanks' role.

Over time, tanks tended to be designed with heavier armour and weapons, increasing the weight of all tanks, so these classifications are relative to the average for the nation's tanks for any given period. An older tank design might be reclassified over time, such as a tank being first deployed as a medium tank, but in later years relegated to light tank roles.

Tanks were also classified by roles that were independent of size, such as cavalry tank, cruiser tank, fast tank, infantry tank, "assault" tank, or "breakthrough" tank. Military theorists initially tended to assign tanks to traditional military infantry, cavalry, and artillery roles, but later developed more specialized roles unique to tanks.

In modern use, the heavy tank has fallen out of favour, being supplanted by more heavily armed and armoured descendant of the medium tanks – the universal main battle tank. The light tank has, in many armies, lost favour to cheaper, faster, lighter armoured cars; however, light tanks (or similar vehicles with other names) are still in service with a number of forces as reconnaissance vehicles, most notably the Russian Marines with the PT-76, the British Army with the Scimitar, and the Chinese Army with the Type 63.
Modern main battle tanks or "universal tanks" incorporate recent advances in automotive, artillery, armour, and electronic technology to combine the best characteristics of the historic medium and heavy tanks into a single, all-around type. They are also the most expensive to mass-produce. A main battle tank is distinguished by its high level of firepower, mobility and armour protection relative to other vehicles of its era. It can cross comparatively rough terrain at high speeds, but its heavy dependency on fuel, maintenance, and ammunition makes it logistically demanding. It has the heaviest armour of any AFVs on the battlefield, and carries a powerful precision-guided munition weapon systems that may be able to engage a wide variety of both ground targets and air targets. Despite significant advances in anti-tank warfare, it still remains the most versatile and fearsome land-based weapon-systems of the 21st-century, valued for its shock action and high survivability.

A tankette is a tracked armed and armoured vehicle resembling a small "ultra-light tank" or "super-light tank" roughly the size of a car, mainly intended for light infantry support or scouting. Tankettes were introduced in the mid-1920s as a reconnaissance vehicle and a mobile machine gun position They were one or two-man vehicles armed with a machine gun. Colloquially it may also simply mean a "small tank".

Tankettes were designed and built by several nations between the 1920s and 1940s following the British Carden Loyd tankette which was a successful implementation of "one man tank" ideas from Giffard Le Quesne Martel, a British Army engineer. They were very popular with smaller countries. Some saw some combat (with limited success) in World War II. However, the vulnerability of their light armour eventually caused the concept to be abandoned. However, the German Army uses a modern design of air-transportable armoured weapons carriers, the Wiesel AWC, which resembles the concept of a tankette.

The term "super-heavy tank" has been used to describe armoured fighting vehicles of extreme size, generally over 75 tonnes. Programs have been initiated on several occasions with the aim of creating an invincible siegeworks/breakthrough vehicle for penetrating enemy formations and fortifications without fear of being destroyed in combat. Examples were designed in World War I and World War II (such as the Panzer VIII Maus), along with a few in the Cold War. However, few working prototypes were built and there is no clear evidence any of these vehicles saw combat, as their immense size would have made most designs impractical.

A missile tank is a tank fulfilling the role of a main battle tank, but using only anti-tank surface-to-surface missiles for main armament. Several nations have experimented with prototypes, notably the Soviet Union during the tenure of Nikita Khrushchev (projects Object 167, Object 137Ml, Object 155Ml, Object 287, Object 775),
A flame tank is an otherwise-standard tank equipped with a flamethrower, most commonly used to supplement combined arms attacks against fortifications, confined spaces, or other obstacles. The type only reached significant use in the Second World War, during which the United States, Soviet Union, Germany, Italy, Japan and the United Kingdom (including members of the British Commonwealth) all produced flamethrower-equipped tanks. Usually, the flame projector replaced one of the tank's machineguns, however, some flame projectors replaced the tank's main gun. Fuel for the flame weapon was generally carried inside the tank, although a few designs mounted the fuel externally, such as the armoured trailer used on the Churchill Crocodile.

Flame tanks have been superseded by thermobaric weapons such as the Russian TOS-1.
The idea for this tank was developed during World War I by British and French. The infantry tank was designed to work in concert with infantry in the assault, moving mostly at a walking pace, and carrying heavy armour to survive defensive fire. Its main purpose was to suppress enemy fire, crush obstacles such as barbed-wire entanglements, and protect the infantry on their advance into and through enemy lines by giving mobile overwatch and cover. The French Renault FT was the first iteration of this concept.

The British and French retained the concept between the wars and into the Second World War era. Because infantry tanks did not need to be fast, they could carry heavy armour. One of the best-known infantry tanks was the Matilda II of World War II. Other examples include the French R-35, the British Valentine, and the British Churchill.

A cruiser tank, or cavalry tank, was designed to move fast and exploit penetrations of the enemy front. The idea originated in "Plan 1919", a British plan to break the trench deadlock of World War I in part via the use of high-speed tanks. The first cruiser tank was the British Whippet.

Between the wars, this concept was implemented in the "fast tanks" pioneered by J. Walter Christie. These led to the Soviet BT tank series and the British cruiser tank series.

During World War II, British cruiser tanks were designed to complement infantry tanks, exploiting gains made by the latter to attack and disrupt the enemy rear areas. In order to give them the required speed, cruiser designs sacrificed armour and armament compared to the infantry tanks. Pure British cruisers were generally replaced by more capable medium tanks such as the US Sherman and, to a lesser extent, the Cromwell by 1943.

The Soviet fast tank ("bistrokhodniy tank", or BT tank) classification also came out of the infantry/cavalry concept of armoured warfare and formed the basis for the British cruisers after 1936. The T-34 was a development of this line of tanks as well, though their armament, armour, and all-round capability places them firmly in the medium tank category.

The armoured car is a wheeled, often lightly armoured, vehicle adapted as a fighting machine. Its earliest form consisted of a motorised ironside chassis fitted with firing ports. By World War I, this had evolved into a mobile fortress equipped with command equipment, searchlights, and machine guns for self-defence. It was soon proposed that the requirements for the armament and layout of armoured cars be somewhat similar to those on naval craft, resulting in turreted vehicles. The first example carried a single revolving cupola with a Vickers gun; modern armoured cars may boast heavier armament – ranging from twin machine guns to large calibre cannon.

Some multi-axled wheeled fighting vehicles can be quite heavy, and superior to older or smaller tanks in terms of armour and armament. Others are often used in military marches and processions, or for the escorting of important figures. Under peacetime conditions, they form an essential part of most standing armies. Armoured car units can move without the assistance of transporters and cover great distances with fewer logistical problems than tracked vehicles.

During World War II, armoured cars were used for reconnaissance alongside scout cars. Their guns were suitable for some defence if they encountered enemy armoured fighting vehicles, but they were not intended to engage enemy tanks. Armoured cars have since been used in the offensive role against tanks with varying degrees of success, most notably during the South African Border War, Toyota War, the Invasion of Kuwait, and other lower-intensity conflicts.

An "aerosledge" is a type of propeller-driven snowmobile, running on skis, used for communications, mail deliveries, medical aid, emergency recovery and border patrolling in northern Russia, as well as for recreation. Aerosledges were used by the Soviet Red Army during the Winter War and World War II.

Some early aerosledges were built by young Igor Sikorsky in 1909–10, before he built multi-engine airplanes and helicopters. They were very light plywood vehicles on skis, propelled by old airplane engines and propellers.

A "scout car" is a military armoured reconnaissance vehicle, capable of off-road mobility and often carrying mounted weapons such as machine guns for offensive capabilities and crew protection. They often only carry an operational crew aboard, which differentiates them from wheeled armoured personnel carriers (APCs) and infantry mobility vehicles (IMVs), but early scout cars, such as the open-topped US M3 scout car could carry a crew of seven. The term is often used synonymously with the more general term armoured car, which also includes armoured civilian vehicles. They are also differentiated by being designed and built for purpose, as opposed to improvised "technicals" which might serve in the same role.

A "reconnaissance vehicle", also known as a "scout vehicle", is a military vehicle used for forward reconnaissance. Both tracked and wheeled reconnaissance vehicles are in service. In some countries, light tanks such as the M551 Sheridan and AMX-13 are also used by scout platoons. Reconnaissance vehicles are usually designed with a low profile or small size and are lightly armoured, relying on speed and cover to escape detection. Their armament ranges from a medium machine gun to an autocannon. Modern examples are often fitted with ATGMs and a wide range of sensors.

Some armoured personnel carriers and infantry mobility vehicle, such as the M113, TPz Fuchs, and Cadillac Gage Commando double in the reconnaissance role.
An internal security vehicle (ISV), also known as an armoured security vehicle (ASV), is a combat vehicle used for suppressing civilian unrest. Security vehicles are typically armed with a turreted heavy machine gun and auxiliary medium machine gun. The vehicle is designed to minimize firepower dead space and the vehicles weapons can be depressed to a maximum of 12°. Non-lethal water cannons and tear gas cannons can provide suppressive fire in lieu of unnecessary deadly fire.

The vehicle must be protected against weapons typical of riots. Protection from improvised incendiary devices is achieved though coverage of the air intake and exhaust ports as well as a strong locking mechanism on the fuel opening. Turret and door locks prevent access to the interior of the vehicle by rioters. Vision blocks, ballistic glass and window shutters and outside surveillance cameras allow protected observation from within the vehicle. Wheeled 4x4 and 6x6 configurations are typical of security vehicles. Tracked security vehicles are often cumbersome and leave negative political connotations for being perceived as an imperial invading force.

Military light utility vehicles are the lightest weight class of military vehicles. It refers to light 4x4 military vehicles with light or no armour and all-terrain mobility. This type of vehicle originated in the first half of the 20th century when horses and other draft animals were replaced with mechanization. Light utility vehicles such as the Willys Jeep were frequently mounted with .50-calibre machineguns and other small weapons for hit-and-run tactics in WWII, especially by the British Special Air Service who used Jeeps to raid Axis airfields during the North Africa campaign. After WWII, vehicles like the Toyota Mega Cruiser and Humvee filled this role. In the 21st century, improvised explosive devices continue to pose threat to mobile infantry resulting in light utility vehicles being made heavier and with more armour.

An improvised fighting vehicle is a combat vehicle resulting from modifications to a civilian or military non-combat vehicle in order to give it a fighting capability. Such modifications usually consist of the grafting of armour plating and weapon systems. Various militaries have procured such vehicles, ever since the introduction of the first automobiles into military service.

During the early days, the absence of a doctrine for the military use of automobiles or of an industry dedicated to producing them, lead to much improvisation in the creation of early armoured cars, and other such vehicles. Later, despite the advent of arms industries in many countries, several armies still resorted to using ad hoc contraptions, often in response to unexpected military situations, or as a result of the development of new tactics for which no available vehicle was suitable. The construction of improvised fighting vehicles may also reflect a lack of means for the force that uses them. This is especially true in underdeveloped countries and even in developing countries, where various armies and guerrilla forces have used them, as they are more affordable than military-grade combat vehicles.

Modern examples include military gun truck used by units of regular armies or other official government armed forces, based on a conventional military cargo truck, that is able to carry a large weight of weapons and armour. They have mainly been used by regular armies to escort military convoys in regions subject to ambush by guerrilla forces. "Narco tanks", used by Mexican drug cartels in the Mexican drug war, are built from such trucks, which combines operational mobility, tactical offensive, and defensive capabilities.

Troop-carrying AFVs are divided into three main types – armoured personnel carriers (APCs), infantry fighting vehicles (IFVs) and infantry mobility vehicles (IMV). The main difference between the three is their intended role – the APC is designed purely to transport troops and is armed for self-defence only – whereas the IFV is designed to provide close-quarters and anti-armour fire support to the infantry it carries. IMV is a wheeled armoured personnel carrier serving as a military patrol, reconnaissance or security vehicle.

Armoured personnel carriers (APCs) are intended to carry infantry quickly and relatively safely to the point where they are deployed. In the Battle of Amiens, 8 August 1918, the British Mk V* tank (a lengthened Mark V) carried a small number of machine gunners as an experiment, but the men were debilitated by the conditions inside the vehicle. Later that year the first purpose-built APC, the British Mk IX tank (Mark Nine), appeared. In 1944, the Canadian general Guy Simonds ordered the conversion of redundant armoured vehicles to carry troops (generically named "Kangaroos"). This proved highly successful, even without training, and the concept was widely used in the 21st Army Group. Post-war, specialised designs were built, such as the Soviet BTR-60 and US M113.

An "infantry fighting vehicle" ("IFV"), also known as a "mechanized infantry combat vehicle" ("MICV"), is a type of armoured fighting vehicle used to carry infantry into battle and provide direct fire support. The first example of an IFV was the West German Schützenpanzer Lang HS.30 which served in the Bundeswehr from 1958 until the early 1980s.

IFVs are similar to armoured personnel carriers (APCs) and infantry carrier vehicles (ICVs), designed to transport a section or squad of infantry (generally between five and ten men) and their equipment. They are differentiated from APCswhich are purely "troop-transport" vehicles armed only for self-defencebecause they are designed to give direct fire support to the dismounted infantry and so usually have significantly enhanced armament. IFVs also often have improved armour and some have firing ports (allowing the infantry to fire personal weapons while mounted).

They are typically armed with an autocannon of 20 to 57 mm calibre, 7.62mm machine guns, anti-tank guided missiles (ATGMs) and/or surface-to-air missiles (SAMs). IFVs are usually tracked, but some wheeled vehicles fall into this category. IFVs are generally less heavily armed and armoured than main battle tanks. They sometimes carry anti-tank missiles to protect and support infantry against armoured threats, such as the NATO TOW missile and Soviet Bastion, which offer a significant threat to tanks. Specially equipped IFVs have taken on some of the roles of light tanks; they are used by reconnaissance organizations, and light IFVs are used by airborne units which must be able to fight without the heavy firepower of tanks.

An "infantry mobility vehicle" ("IMV") or "protected patrol vehicle" ("PPV") is a wheeled armoured personnel carrier (APC) serving as a military patrol, reconnaissance or security vehicle. Examples include the ATF Dingo, AMZ Dzik, AMZ Tur, Mungo ESK, and Bushmaster IMV. This term also applies to the vehicles currently being fielded as part of the MRAP program.

IMVs were developed in response to the threats of modern counterinsurgency warfare, with an emphasis on Ambush Protection and Mine-Resistance. Similar vehicles existed long before the term IMV was coined, such as the French VAB and South African Buffel. The term is coming more into use to differentiate light 4x4 wheeled APCs from the traditional 8x8 wheeled APCs. It is a neologism for what might have been classified in the past as an armoured scout car, such as the BRDM, but the IMV is distinguished by having a requirement to carry dismountable infantry. The up-armoured M1114 Humvee variant can be seen as an adaptation of the unarmoured Humvee to serve in the IMV role.

Many modern military vehicles, ranging from light wheeled command and reconnaissance, through armoured personnel carriers and tanks, are manufactured with amphibious capabilities. Contemporary wheeled armoured amphibians include the French Véhicule de l'Avant Blindé and Véhicule Blindé Léger. The latter is a small, lightly armoured 4×4 all-terrain vehicle that is fully amphibious and can swim at 5.4 km/h. The VAB ("Véhicule de l'Avant Blindé" – 'armoured vanguard vehicle') is a fully amphibious armoured personnel carrier powered in the water by two water jets, that entered service in 1976 and produced in numerous configurations, ranging from basic personnel carrier, anti-tank missile platform.

During the Cold War the Soviet bloc states developed a number of amphibious APCs, fighting vehicles and tanks, both wheeled and tracked. Most of the vehicles the Soviets designed were amphibious, or could ford deep water. Wheeled examples are the BRDM-1 and BRDM-2 4x4 armoured scout cars, as well as the BTR-60, BTR-70, BTR-80, BTR-94 and BTR-90 8x8 armoured personnel carriers.

The United States started developing a long line of Landing Vehicle Tracked (LVT) designs from . The US Marine Corps currently uses the AAV7-A1 Assault Amphibious Vehicle, which was to be succeeded by the Expeditionary Fighting Vehicle, which was capable of planing on water and can achieve water speeds of 37–46 km/h. The EFV project has been cancelled.

A significant number of tracked armoured vehicles that are primarily intended for land-use, have some amphibious capability, tactically useful inland, reducing dependence on bridges. They use their tracks, sometimes with added propeller or water jets for propulsion. As long as the banks have a shallow enough slopes to enter or leave the water they can cross rivers and water obstacles.

Some heavy tanks can operate amphibiously with a fabric skirt to add buoyancy. The Sherman DD tank used in the Normandy landings had this setup. When in water the waterproof float screen was raised and propellers deployed. Some modern vehicles use a similar skirt.

Lightweight armoured fighting vehicles designed or modified to be carried by aircraft and delivered by air drop, helicopter lift, glider, or air landing with infantry to provide heavier tactical firepower and mobility. The air-equivalent to amphibious vehicles, the main advantage of airborne forces is their ability to be deployed into combat zones without land passage, as long as the airspace is accessible. Airborne vehicles are limited only by the tonnage capacity of their transport aircraft. Airborne vehicles typically lack the armour and supplies necessary for prolonged combat, so they are utilized for establishing an airhead to bring in larger forces before carrying out other combat objectives. One modern example is the German Wiesel AWC. The USA also created the M22 Locust as a way to aid paratroopers/ being paradropped in as it was very lightly armoured and very small.

Modern engineering AFV's utilize chassis based on main battle tank platforms: these vehicles are as well armoured and protected as tanks, designed to keep up with tanks, breach obstacles to help tanks get to wherever it needs to be, perform utility functions necessary to expedite mission objectives of tanks, and to conduct other earth-moving and engineering work on the battlefield. These vehicles go by different names depending upon the country of use or manufacture. In the United States the term "combat engineer vehicle (CEV)" is used, in the United Kingdom the term "Armoured Vehicle Royal Engineers (AVRE)" is used, while in Canada and other commonwealth nations the term "armoured engineer vehicle (AEV)" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large calibre demolition cannon, augers, winches, excavator arms and cranes, or lifting booms.

Although the term "armoured engineer vehicle" is used specifically to describe these multi-purpose tank-based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank-based engineering vehicles used in the support of mechanized forces. Thus, "armoured engineer vehicle" used generically would refer to AEV, AVLB, Assault Breachers, and so on. Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle.

A breaching vehicle is especially designed to clear pathways for troops and other vehicles through minefields and along roadside bombs and other improvised explosive devices. These vehicles are equipped with mechanical or other means for the breaching of man-made obstacles. Common types of breaching vehicles include mechanical flails, mine plough vehicles, and mine roller vehicles.
The armoured bulldozer is a basic tool of combat engineering. These combat engineering vehicles combine the earth moving capabilities of the bulldozer with armour which protects the vehicle and its operator in or near combat. Most are civilian bulldozers modified by addition of vehicle armour/military equipment, but some are tanks stripped of armament and fitted with a dozer blade. Some tanks have bulldozer blades while retaining their armament, but this does not make them armoured bulldozers as such, because combat remains the primary role – earth moving is a secondary task.

An "armoured recovery vehicle" ("ARV") is a type of vehicle recovery armoured fighting vehicle used to repair battle- or mine-damaged as well as broken-down armoured vehicles during combat, or to tow them out of the danger zone for more extensive repairs. To this end the term "armoured repair and recovery vehicle" ("ARRV") is also used.

ARVs are normally built on the chassis of a main battle tank (MBT), but some are also constructed on the basis of other armoured fighting vehicles, mostly armoured personnel carriers (APCs). ARVs are usually built on the basis of a vehicle in the same class as they are supposed to recover; a tank-based ARV is used to recover tanks, while an APC-based one recovers APCs, but does not have the power to tow a much heavier tank.

An "armoured vehicle-launched bridge" ("AVLB") is a combat support vehicle, sometimes regarded as a subtype of combat engineering vehicle, designed to assist militaries in rapidly deploying tanks and other armoured fighting vehicles across rivers. The AVLB is usually a tracked vehicle converted from a tank chassis to carry a folding metal bridge instead of weapons. The AVLB's job is to allow armoured or infantry units to cross water, when a river too deep for vehicles to wade through is reached, and no bridge is conveniently located (or sufficiently sturdy, a substantial concern when moving 60-ton tanks).

The bridge layer unfolds and launches its cargo, providing a ready-made bridge across the obstacle in only minutes. Once the span has been put in place, the AVLB vehicle detaches from the bridge, and moves aside to allow traffic to pass. Once all of the vehicles have crossed, it crosses the bridge itself and reattaches to the bridge on the other side. It then retracts the span ready to move off again. A similar procedure can be employed to allow crossings of small chasms or similar obstructions. AVLBs can carry bridges of or greater in length. By using a tank chassis, the bridge layer is able to cover the same terrain as main battle tanks, and the provision of armour allows them to operate even in the face of enemy fire. However, this is not a universal attribute: some exceptionally sturdy 6x6 or 8x8 truck chassis have lent themselves to bridge-layer applications.
"Combat engineer section carriers" are used to transport sappers (combat engineers) and can be fitted with bulldozers' blades and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).

An anti-aircraft vehicle, also known as a self-propelled anti-aircraft gun (SPAAG) or self-propelled air defense system (SPAD), is a mobile vehicle with a dedicated anti-aircraft capability.
Specific weapon systems used include machine guns, anti-aircraft autocannons, larger anti-air guns, or surface-to-air-missiles, and some mount both guns and longer-ranged missiles (e.g. the Pantsir-S1). Platforms used include both trucks and heavier combat vehicles such as armored personnel carriers and tanks, which add protection from aircraft, artillery, and small arms fire for front line deployment.
Anti-aircraft guns are usually mounted in a quickly-traversing turret with a high rate of elevation, for tracking fast-moving aircraft. They are often in dual or quadruple mounts, allowing a high rate of fire. In addition, most anti-aircraft guns can be used in a direct-fire role against surface targets to great effect. In the early 21st century, missiles (generally mounted on similar turrets) largely supplanted anti-aircraft guns, though guns have recently shown revived utility against slow, low-flying drones.

Self-propelled artillery vehicles give mobility to artillery. Within the term are covered self-propelled guns (or howitzers) and rocket artillery. They are highly mobile, usually based on tracked chassis carrying either a large howitzer or other field gun or alternatively a mortar or some form of rocket or missile launcher. They are usually used for long-range indirect bombardment support on the battlefield.

In the past, self-propelled artillery has included direct-fire "Gun Motor Carriage" vehicles, such as assault guns and tank destroyers (also known as self-propelled anti-tank guns). These have been heavily armoured vehicles, the former providing danger-close fire-support for infantry and the latter acting as specialized anti-tank vehicles.

Modern self-propelled artillery vehicles may superficially resemble tanks, but they are generally lightly armoured, too lightly to survive in direct-fire combat. However, they protect their crews against shrapnel and small arms and are therefore usually included as armoured fighting vehicles. Many are equipped with machine guns for defence against enemy infantry.

The key advantage of self-propelled over towed artillery is that it can be brought into action much faster. Before towed artillery can be used, it has to stop, unlimber and the guns set up. To move position, the guns must be limbered up again and brought – usually towed – to the new location. By comparison, self-propelled artillery in combination with modern communications, can stop at a chosen location and begin firing almost immediately, then quickly move on to a new position. This ability is very useful in a mobile conflict and particularly on the advance.

Conversely, towed artillery was and remains cheaper to build and maintain. It is also lighter and can be taken to places that self-propelled guns cannot reach, so despite the advantages of the self-propelled artillery, towed guns remain in the arsenals of many modern armies.

An assault gun is a gun or howitzer mounted on a motor vehicle or armoured chassis, designed for use in the direct fire role in support of infantry when attacking other infantry or fortified positions.

Historically, the custom-built fully armoured assault guns usually mounted the gun or howitzer in a fully enclosed casemate on a tank chassis. The use of a casemate instead of a gun turret limited these weapons' field of fire, but allowed a larger gun to be fitted relative to the chassis, more armour to be fitted for the same weight, and provided a cheaper construction. In most cases, these turretless vehicles also presented a lower profile as a target for the enemy.

A siege mortar is a form of self-propelled gun that holds a siege mortar. The only siege mortar ever built was the Karl-Gerät. It could be argued that these could be classified as a Mortar carrier.

A mortar carrier is a self-propelled artillery vehicle carrying a mortar as its primary weapon. Mortar carriers cannot be fired while on the move and some must be dismounted to fire. In U.S. Army doctrine, mortar carriers provide close and immediate indirect fire support for maneuver units while allowing for rapid displacement and quick reaction to the tactical situation. The ability to relocate not only allows fire support to be provided where it is needed faster, but also allows these units to avoid counter-battery fire. Mortar carriers have traditionally avoided direct contact with the enemy. Many units report never using secondary weapons in combat.

Prior to the Iraq War, American 120 mm mortar platoons reorganized from six M1064 mortar carriers and two M577 fire direction centres (FDC) to four M1064 and one FDC. The urban environment of Iraq made it difficult to utilize mortars. New technologies such as mortar ballistic computers and communication equipment and are being integrated. Modern era combat is becoming more reliant on direct fire support from mortar carrier machine guns.

A multiple rocket launcher is a type of unguided rocket artillery system. Like other rocket artillery, multiple rocket launchers are less accurate and have a much lower (sustained) rate of fire than batteries of traditional artillery guns. However, they have the capability of simultaneously dropping many hundreds of kilograms of explosive, with devastating effect.

The Korean Hwacha is an example of an early weapon system with a resemblance to the modern-day multiple rocket launcher. The first self-propelled multiple rocket launchers – and arguably the most famous – were the Soviet BM-13 Katyushas, first used during World War II and exported to Soviet allies afterwards. They were simple systems in which a rack of launch rails was mounted on the back of a truck. This set the template for modern multiple rocket launchers. The first modern multiple rocket launcher was the German "15 cm Nebelwerfer 41" of the 1930s, a small towed artillery piece. Only later in World War II did the British deploy similar weapons in the form of the Land Mattress.The Americans mounted tubular launchers atop M4 Sherman tanks to create the T34 Calliope rocket launching tank, only used in small numbers, as their closest equivalent to the Katyusha.

Missile vehicles are trucks or tractor units designed to carry rockets or missiles. The missile vehicle may be a self-propelled unit, or the missile holder/launcher may be on a trailer towed by a prime mover. They are used in the military forces of a number of countries in the world. Long missiles are commonly transported parallel to the ground on these vehicles, but elevated into an inclined or vertical position for launching.
Tank destroyers and tank hunters are armed with an anti-tank gun or anti-tank missile launcher, and are designed specifically to engage enemy armoured vehicles. Many have been based on a tracked tank chassis, while others are wheeled. Since World War II, main battle tanks have largely replaced gun-armed tank destroyers; although lightly armoured anti-tank guided missile (ATGM) carriers are commonly used for supplementary long-range anti-tank engagements.

In post-Cold War conflict, the resurgence of expeditionary warfare has seen the emergence of gun-armed wheeled vehicles, sometimes called "protected gun systems", which may bear a superficial resemblance to tank destroyers, but are employed as direct fire support units typically providing support in low intensity operations such as Iraq and Afghanistan. These have the advantage of easier deployment, as only the largest air transports can carry a main battle tank, and their smaller size makes them more effective in urban combat.

Many forces' IFVs carry anti-tank missiles in every infantry platoon, and attack helicopters have also added anti-tank capability to the modern battlefield. But there are still dedicated anti-tank vehicles with very heavy long-range missiles, or intended for airborne use. There have also been dedicated anti-tank vehicles built on ordinary armoured personnel carrier or armoured car chassis. Examples include the U.S. M901 ITV (Improved TOW Vehicle) and the Norwegian NM142, both on an M113 chassis, several Soviet ATGM launchers based on the BRDM scout car, the British FV438 Swingfire and FV102 Striker and the German "Raketenjagdpanzer" series built on the chassis of the HS 30 and Mardar IFVs.

An "armoured train" is a railway train protected with armour. They are usually equipped with rail cars armed with artillery, autocannons, machine guns, tank turrets and anti-aircraft guns. They were mostly used during the late 19th to mid-20th century, when they offered an innovative way to quickly move large amounts of firepower. Their use was discontinued in most countries when road vehicles became much more powerful and offered more flexibility, and because armoured trains were too vulnerable to track sabotage and attacks from the air. However, the Russian Federation used improvised armoured trains in the Second Chechen War in the late 1990s and 2000s. Armoured trains carrying ballistic missiles have also been used.

The rail cars on an armoured train were designed for many tasks, such as carrying artillery and machine guns, infantry units, anti-tank and anti-aircraft guns. During World War II, the Germans would sometimes put a "Fremdgerät" (captured AFVs such as the French Somua S-35 or Czech PzKpfw 38(t)), or obsolescent Panzer II light tanks on a flatbed rail car, which could quickly be offloaded by means of a ramp and used away from the railway line to chase down enemy partisans.

Different types of armour were used to protect armoured trains from attack. In addition to various metal plates, concrete and sandbags were used in some cases on armoured trains.

Armoured trains were sometimes escorted by a kind of rail-tank called a draisine. One such example was the Italian 'Littorina' armoured trolley, which had a cab in the front and rear, each with a control set so it could be driven down the tracks in either direction. Littorina mounted two dual 7.92mm MG13 machine gun turrets from Panzer I light tanks.



Anton Drexler

Anton Drexler (13 June 1884 – 24 February 1942) was a German far-right political agitator for the Völkisch movement in the 1920s. He founded the German Workers' Party (DAP), the pan-German and anti-Semitic antecedent of the Nazi Party (NSDAP). Drexler mentored his successor in the NSDAP, Adolf Hitler, during his early years in politics.

Born in Munich, Drexler was a machine-fitter before becoming a railway toolmaker and locksmith in Berlin. He is believed to have been disappointed with his income, and to have played the zither in restaurants to supplement his earnings. Drexler did not serve in the armed forces during World War I because he was deemed physically unfit for service.

During World War I, Drexler joined the German Fatherland Party, a short-lived far-right party active during the last phase of the war, which played a significant role in the emergence of the stab-in-the-back myth and the defamation of certain politicians as the "November Criminals".

In March 1918, Drexler founded a branch of the Free Workers' Committee for a Good Peace ("Der Freie Arbeiterausschuss für einen guten Frieden") league. Karl Harrer, a journalist and member of the Thule Society, convinced Drexler and several others to form the Political Workers' Circle ("Politischer Arbeiter-Zirkel") in 1918. The members met periodically for discussions about nationalism and antisemitism.

Together with Harrer, Drexler founded the German Workers' Party (DAP) in Munich on 5 January 1919. At a DAP meeting in Munich on 12 September 1919, the main speaker was Gottfried Feder, who held a lecture on the subject of 'the breaking of interest slavery'. When Feder's lecture concluded, Adolf Hitler – who attended the meeting as part of his assignment from the German Army to watch political agitators – got involved in a heated political argument with a visitor, Professor Adalbert Baumann, who questioned the soundness of Feder's arguments and in turn spoke in favour of Bavarian separatism. In vehemently attacking the man's arguments, Hitler made an impression on the other party members with his oratorical abilities, and according to him, the professor left the hall defeated. Drexler approached Hitler and gave him a copy of his pamphlet "My Political Awakening". Hitler later claimed the literature reflected the ideals he already held since his own "political awakening". Impressed with Hitler, Drexler encouraged him to join the DAP. On the orders of his army superiors, Hitler applied to join the party.

Once accepted, Hitler began to make the party more public by drawing people in with his speaking abilities, leading up to his organizing the party's biggest meeting yet, which attracted 2,000 people to the Hofbräuhaus in Munich on 24 February 1920. It was in this speech that Hitler, for the first time, enunciated the twenty-five points of the German Worker's Party's manifesto that he had authored with Drexler and Feder. Through these points he gave the organisation a foreign policy, including the abrogation of the Treaty of Versailles, a Greater Germany, Eastern expansion, and exclusion of Jews from citizenship. On the same day the party was renamed the National Socialist German Workers' Party ("Nationalsozialistische Deutsche Arbeiterpartei", NSDAP).

Following an intraparty dispute, Hitler angrily tendered his resignation on 11 July 1921. Drexler and the members of the party's governing committee realised that the resignation of their leading public figure and speaker would mean the end of the party. So Dietrich Eckart was asked by the Party leadership to speak with Hitler and relay the conditions in which he would agree to return. Hitler announced he would rejoin the party on the condition that he would replace Drexler as party chairman, with dictatorial powers and the title of ""Führer"", and that the party headquarters would remain in Munich. The committee agreed and he rejoined the party as member 3,680. Drexler was thereafter moved to the purely symbolic position of honorary president and left the party in 1923.

Drexler was also a member of a "völkisch" political club for affluent members of Munich society known as the Thule Society. His membership in the Nazi Party ended when it was temporarily outlawed in 1923 following the Beer Hall Putsch, although Drexler had not taken part in the coup attempt. In 1924 he was elected to the Bavarian state parliament for the Völkisch-Social Bloc party (VSB), in which he served as vice president until 1928. He played no role in the Nazi Party's re-founding in February 1925 and rejoined only after Hitler ascended to national power in 1933. In May 1925 he founded a group with other VSB deputies, the "Nationalsozialer Volksbund" (National Social People's League), but it was dissolved in 1927–1928. Drexler received the Nazi Party's Blood Order in 1934, and was still occasionally used as a propaganda tool until about 1937, but was never allowed any power within the party.

Drexler died in Munich in February 1942 after a lengthy illness due to alcoholism.



All Quiet on the Western Front

All Quiet on the Western Front () is a novel by Erich Maria Remarque, a German veteran of World War I. The book describes the German soldiers' extreme physical and mental trauma during the war as well as the detachment from civilian life felt by many upon returning home from the war.

The novel was first published in November and December 1928 in the German newspaper and in book form in late January 1929. The book and its sequel, "The Road Back" (1930), were among the books banned and burned in Nazi Germany. "All Quiet on the Western Front" sold 2.5 million copies in 22 languages in its first 18 months in print.

Three film adaptations of the book have been made, each of which was lauded. The 1930 American adaptation, directed by Lewis Milestone, won two Academy Awards. The 1979 British-American adaptation, a television film by Delbert Mann, won a Golden Globe Award and an Emmy Award. The 2022 German adaptation, directed by Edward Berger, won four Academy Awards.

The book entered the public domain in the United States in 2024, with the 1930 film adaptation set to do so in 2026.

The English translation by Arthur Wesley Wheen gives the title as" All Quiet on the Western Front". The literal translation of "" is "Nothing New in the West," with "West" being the Western Front; the phrase refers to the content of an official communiqué at the end of the novel.

Brian Murdoch's 1993 translation rendered the phrase as "there was nothing new to report on the Western Front" within the narrative. However, in the foreword, he explains his retention of the original book title:
Although it does not match the German exactly, Wheen's title has justly become part of the English language and is retained here with gratitude.
The phrase "" has become a colloquial expression meaning stagnation, or lack of visible change, in any context.

Murdoch also explains how, owing to the time it was published, Wheen's translation was obliged to Anglicise some lesser-known German references and lessen the impact of certain passages while omitting others entirely. Murdoch's translation is more accurate to the original text and completely unexpurgated.

The book centers on Paul Bäumer, a German soldier on the Western Front during World War I. Before the war, Paul lived with his parents and sister in a charming German village. He attended school, where the patriotic speeches of his teacher Kantorek led the whole class to volunteer for the Imperial German Army shortly after the start of the Great War. At the training camp, where they meet Himmelstoß, his class is scattered over the platoons amongst Frisian fishermen, peasants and labourers, with whom they soon become friends. Bäumer arrives at the Western Front with his friends and schoolmates (Leer, Müller, Kropp, Kemmerich and a number of other characters). There they meet Stanislaus Katczinsky, an older recalled reservist, nicknamed Kat, who becomes Paul's mentor. 
While fighting at the front, Bäumer and his comrades engage in frequent battles and endure the treacherous and filthy conditions of trench warfare. The battles fought here have no names and seem to have little overall significance, except for the impending possibility of injury or death. Only meager pieces of land are gained, which are often lost again later. Remarque often refers to the living soldiers as old and dead, emotionally drained and shaken. 

Paul visits home, and the contrast with civilian life highlights the cost of the war on his psyche. The town has not changed since he went off to war, but he has: he finds that he does "not belong here any more, it is a foreign world". He feels disconnected from most of the townspeople. His father asks him "stupid and distressing" questions about his war experiences, not understanding "that a man cannot talk of such things". An old schoolmaster lectures him about strategy and advancing to Paris while insisting that Paul and his friends know only their "own little sector" of the war but nothing of the big picture.

Indeed, the only person he remains connected to is his dying mother, with whom he shares a tender yet restrained relationship. The night before he is to return from leave, he stays up with her, exchanging small expressions of love and concern for each other. He thinks to himself, "Ah! Mother, Mother! How can it be that I must part from you? Here I sit and there you are dying; we have so much to say, and we shall never say it." In the end, he concludes that he "ought never to have come [home] on leave".

Paul is glad to return and reunite with his comrades. Soon after, he volunteers to go on a patrol and kills a Frenchman in hand-to-hand combat for the first time. He watches the man die slowly in agony for hours. He is remorseful and devastated, asking for forgiveness from the man's corpse. He later confesses to Kat and Albert, who try to comfort him and reassure him that it is only part of the war. Afterwards they are sent on what Paul calls a "good job". They must guard a supply depot in a village that was evacuated owing to being shelled too heavily. During this time the men are able to feed themselves adequately, unlike the near-starvation conditions in the German trenches. In addition the men enjoy themselves while living off the spoils from the village and officers' luxuries from the supply depot (such as fine cigars). While evacuating the villagers (enemy civilians), Paul and Albert are taken by surprise by artillery fired at the civilian convoy and are wounded by a shell. On the train back home Albert takes a turn for the worse and cannot complete the journey and instead is sent off the train to recuperate in a Catholic hospital. By a combination of bartering and manipulation Paul manages to stay with Albert. Albert eventually has his leg amputated, whilst Paul is deemed fit for service and returned to the front.

By now the war is nearing its end and the German Army is retreating. In despair Paul watches as his friends fall one by one. Kat's death is the last straw that finally causes Paul to lose his will to live. In the final chapter he comments that peace is coming soon but he does not see the future as bright and shining with hope. Paul feels that he has no aims left in life and that their generation will be different and misunderstood.

In October 1918 Paul is finally killed on a remarkably peaceful day. The situation report from the frontline states a simple phrase: "All quiet on the Western Front." Paul's corpse displays a calm expression on its face, "as though almost glad the end had come."

At the beginning of the book, Remarque writes, "This book is to be neither an accusation nor a confession, and least of all an adventure, for death is not an adventure to those who stand face to face with it. It will try simply to tell of a generation of men who, even though they may have escaped (its) shells, were destroyed by the war." The book does not focus on heroic stories of bravery, but rather gives a view of the conditions in which the soldiers find themselves. The monotony between battles, the constant threat of artillery fire and bombardments, the struggle to find food, the lack of training of young recruits (meaning lower chances of survival), and the overarching role of random chance in the lives and deaths of the soldiers are described in detail.

One of the major themes of the novel is the difficulty experienced by former soldiers trying to revert to civilian life after having experienced extreme combat situations. This internal destruction can be found as early as the first chapter as Paul comments that, although all the boys are young, their youth has already left them. In addition, the massive loss of life and negligible gains from the fighting are constantly emphasized. Soldiers' lives are thrown away by their commanding officers who are stationed comfortably away from the front, ignorant of and indifferent to the suffering and terror of the front lines.

Another major theme is the concept of blind nationalism. Remarque often emphasizes that the boys were not forced to join the war effort against their will, but rather by a sense of patriotism and pride. Kantorek called Paul's platoon the "Iron Youth", teaching his students a romanticized version of warfare with glory and duty to the Fatherland. It is only when the boys go to war and have to live and fight in dirty, cramped trenches with little protection from enemy bullets and shells while contending with hunger and sickness that they realize just how dispiriting it is to actually serve in the army.

The main character and central figure of the novel.

Kropp is in Paul's class at school and is described as the clearest thinker of the group as well as the smallest. Kropp is wounded towards the end of the novel and undergoes a leg amputation. Both he and Bäumer end up spending time in a Catholic hospital together, Bäumer suffering from shrapnel wounds to the leg and arm. Although Kropp initially plans to commit suicide if he requires an amputation, he postponed suicide because of the strength of military camaraderie and a lack of a revolver. Kropp and Bäumer part ways when Bäumer is recalled to his regiment after recovering. Paul comments that saying farewell was "very hard, but it is something a soldier learns to deal with."

Haie is tall and strong with a good sense of humor, and a peat-digger by profession. His size and behavior make him seem older than Paul, yet he is the same age as Paul and his school-friends, who are roughly 19 at the start of the book. During combat, he is fatally injured in his back (Chapter 6)—the resulting wound is large enough for Paul to see Haie's breathing lung while Himmelstoß (Himmelstoss) carries him to safety. He later dies of this injury.

Müller is one of Bäumer's classmates, and is 19 when he also volunteers to join the German army. Carrying his old school books with him to the battlefield, he constantly reminds himself of the importance of learning and education. Even while under enemy fire, he "mutters propositions in physics." He takes a liking to Kemmerich's boots and inherits them when Kemmerich dies early in the novel. He is killed later after being shot point-blank in the stomach with a flare gun. As he was dying "quite conscious and in terrible pain", he gave his boots which he inherited from Kemmerich to Paul.

Katczinsky, a recalled reserve militiaman, was a cobbler in civilian life. He is older than Paul Bäumer and his comrades, about 40 years old, and serves as their leadership figure. He also represents a literary model highlighting the differences between the younger and older soldiers. While the older men have already had a life of professional and personal experience before the war, Paul and the men of his age have had little life experience or time for personal growth.

Kat is well known for his ability to scavenge nearly any item needed, especially food. At one point he secures four boxes of lobster. Paul describes Kat as possessing a sixth sense. One night, Paul along with a group of other soldiers are held up in a factory with neither rations nor comfortable bedding. Katczinsky leaves for a short while, returning with straw to put over the bare wires of the beds. Later, to feed the hungry men, Kat brings bread, a bag of horse flesh, a lump of fat, a pinch of salt and a pan in which to cook the food.

Kat is hit by shrapnel near the end of the story, leaving him with a smashed shin. Paul carries him back to camp on his back, only to discover upon their arrival that a stray splinter had hit Kat in the back of the head and killed him on the way. He is thus the last of Paul's close friends to die in battle. It is Kat's death that eventually makes Bäumer indifferent as to whether he survives the war or not, yet certain that he can face the rest of his life without fear. "Let the months and the years come, they can take nothing from me, they can take nothing more. I am so alone, and so without hope that I can confront them without fear."

One of Bäumer's non-schoolmate friends. Before the war, Tjaden was a locksmith. A big eater with a grudge against the former postman-turned-corporal Himmelstoß (thanks to his strict "disciplinary actions"), he manages to forgive Himmelstoß later in the book. Throughout the book, Paul frequently remarks on how much of an eater he is, yet somehow manages to stay as "thin as a rake". He appears in the sequel, "The Road Back".

Kantorek is the schoolmaster of Paul and his friends, including Kropp, Leer, Müller, and Behm. Behaving "in a way that cost [him] nothing," Kantorek is a strong supporter of the war and encourages Bäumer and other students in his class to join the war effort.

Kantorek is a hypocrite, urging the young men he teaches to fight in the name of patriotism, while not voluntarily enlisting himself. In a twist of fate, Kantorek is later drafted. He reluctantly joins the ranks of his former students, where he is drilled and taunted by Mittelstädt, one of the students he had earlier persuaded to enlist.

Leer is an intelligent soldier in Bäumer's company, and one of his classmates. He is very popular with women; when he and his comrades meet three French women, he is the first to seduce one of them. Bäumer describes Leer's ability to attract women by saying "Leer is an old hand at the game". In chapter 11, Leer is hit by a shell fragment, which also hits Bertinck. The shrapnel tears open Leer's hip, causing him to bleed to death quickly. His death causes Paul to ask himself, "What use is it to him now that he was such a good mathematician in school?"

Lieutenant Bertinck is the leader of Bäumer's company. His men have a great respect for him, and Bertinck has great respect for his men. In the beginning of the book, he permits them to eat the rations of the men that had been killed in action, standing up to the chef Ginger who allowed them only their allotted share. Bertinck is genuinely despondent when he learns that few of his men had survived an engagement.

When he and the other characters are trapped in a trench under heavy attack, Bertinck, who has been injured in the firefight, spots a flamethrower team advancing on them. He gets out of cover and takes aim on the flamethrower but misses, and gets hit by enemy fire. With his next shot he kills the flamethrower, and immediately afterwards an enemy shell explodes on his position blowing off his chin. The same explosion also fatally wounds Leer.

Sergeant der Reserve Himmelstoß (which translates as "Heaven-Bound") was a village postman before being mobilised for the war and securing a position as a Sergeant in the Landwehr (Reserves of persons 28-39). Himmelstoß is a power-hungry martinet who compensated for his lack of social standing by abusing his position as the Training NCO for the men under his control, taking sadistic pleasure in punishing the minor infractions of his trainees during their basic training in preparation for their deployment. He had a special contempt for Paul and his friends, because they knew him as their local postman. Paul later figures that the training taught by Himmelstoß made them "hard, suspicious, pitiless, and tough" but most importantly it taught them comradeship. Bäumer and his comrades exact their revenge on Himmelstoß, mercilessly whipping him on the night before they depart for the front.

Himmelstoß later joins them at the front, revealing himself as a coward by pretending to be wounded because of a scratch on his face. Paul Bäumer beats him because of it and when a lieutenant comes along looking for men for a trench charge, Himmelstoß joins and leads the charge. He carries Haie Westhus's body to Bäumer after he is fatally wounded. Matured and repentant through his experiences, Himmelstoß later asks for forgiveness from his previous charges. As he becomes the new staff cook, to prove his friendship he secures two pounds of sugar for Bäumer and half a pound of butter for Tjaden.

In the 1979 film adaptation, he is referred to as "Corporal" and wears a post 1941 shoulderboard for "Unteroffizier", a very junior NCO. In the book, he was a "Sergeant" or "Unterfeldwebel", a rank reserved for long serving "Unteroffiziere" who fulfilled a staff role such as quartermaster, cook, clerks etc.

Detering is a farmer who constantly longs to return to his wife and farm. He is fond of horses and is angered when he sees them used in combat. He says, "It is of the vilest baseness to use horses in the war," when the group hears several wounded horses writhe and scream for a long time before dying during a bombardment. He tries to shoot them to put them out of their misery, but is stopped by Kat to keep their current position hidden. He is driven to desert when he sees a cherry tree in blossom, which reminds him of home. He is found by military police and court-martialed and is never heard from again, presumably executed.

Hamacher is a patient at the Catholic hospital where Paul and Albert Kropp are temporarily stationed. He has an intimate knowledge of the workings of the hospital. He also has a "Special Permit", certifying him as sporadically not responsible for his actions due to a head wound, though he is clearly quite sane and exploiting his permit so he can stay in the hospital and away from the war as long as possible.

Just 19 years old, Franz Kemmerich had enlisted with his best friend and classmate, Bäumer. Kemmerich is shot in the leg early in the story; his injured leg has to be amputated, and he dies shortly after. In anticipation of Kemmerich's imminent death, Müller was eager to get his boots. While in the hospital, someone steals Kemmerich's watch that he intended to give to his mother, causing him great distress and prompting him to ask about his watch every time his friends visit him in the hospital. Paul later finds the watch and hands it over to Kemmerich's mother, and lies to her that Franz died instantly and painlessly.

Youthful and overweight, Behm was the only student in Paul's class that was not quickly influenced by Kantorek's patriotism to join the war. Eventually, after pressure from friends and Kantorek, he joins the war. He is the first of Paul's friends to die. He is blinded in no man's land and believed to be dead by his friends. The next day, when he is seen walking blindly around no man's land, it is discovered that he was only unconscious, but he is killed before he can be rescued.

From November 10 to December 9, 1928, "All Quiet on the Western Front" was published in serial form in "Vossische Zeitung" magazine. It was released in book form the following year to great success, selling one and a half million copies that same year. It was the best-selling work of fiction in America for the year 1929, according to "Publishers Weekly". Although publishers had worried that interest in World War I had waned more than 10 years after the armistice, Remarque's realistic depiction of trench warfare from the perspective of young soldiers struck a chord with the war's survivors—soldiers and civilians alike—and provoked strong reactions, both positive and negative, around the world.

With "All Quiet on the Western Front", Remarque emerged as an eloquent spokesman for a generation that had been, in his own words, "destroyed by war, even though it might have escaped its shells." Remarque's harshest critics, in turn, were his countrymen, many of whom felt the book denigrated the German war effort, and that Remarque had exaggerated the horrors of war to further his pacifist agenda. The strongest voices against Remarque came from the emerging Nazi Party and its ideological allies. In 1933, when the Nazis rose to power, "All Quiet on the Western Front" became one of the first degenerate books to be publicly burnt; in 1930, screenings of the Academy Award-winning film based on the book were met with Nazi-organized protests and mob attacks on both movie theatres and audience members.

Objections to Remarque's portrayal of the World War I German soldiers were not limited to those of the Nazis in 1933. Dr. was concerned about Remarque's depiction of the medical personnel as being inattentive, uncaring, or absent from frontline action. Kroner was specifically worried that the book would perpetuate German stereotypes abroad that had subsided since the First World War. He offered the following clarification: "People abroad will draw the following conclusions: if German doctors deal with their own fellow countrymen in this manner, what acts of inhumanity will they not perpetuate against helpless prisoners delivered up into their hands or against the populations of occupied territory?"

A fellow patient of Remarque's in the military hospital in Duisburg objected to the negative depictions of the nuns and patients and to the general portrayal of soldiers: "There were soldiers to whom the protection of homeland, protection of house and homestead, protection of family were the highest objective, and to whom this will to protect their homeland gave the strength to endure any extremities."

These criticisms suggest that experiences of the war and the personal reactions of individual soldiers to their experiences may be more diverse than Remarque portrays them; however, it is beyond question that Remarque gives voice to a side of the war and its experience that was overlooked or suppressed at the time. This perspective is crucial to understanding the true effects of World War I. The evidence can be seen in the lingering depression that Remarque and many of his friends and acquaintances were suffering a decade later.

The book was also banned in other European countries on the grounds that it was considered anti-war propaganda; Austrian soldiers were forbidden from reading the book in 1929, and Czechoslovakia banned it from its military libraries. The Italian translation was also banned in 1933. When the Nazis were re-militarizing Germany, the book was banned as it was deemed counterproductive to German rearmament. In contrast, "All Quiet on the Western Front" was trumpeted by pacifists as an anti-war book.

Remarque makes a point in the opening statement that the novel does not advocate any political position, but is merely an attempt to describe the experiences of the soldier.

Much of the literary criticism came from Salomo Friedlaender, who wrote a book "Hat Erich Maria Remarque wirklich gelebt?" "Did Erich Maria Remarque really live?" (under the pen name Mynona), which was, in its turn, criticized in: "Hat Mynona wirklich gelebt?" "Did Mynona really live?" by Kurt Tucholsky. Friedlaender's criticism was mainly personal in nature—he attacked Remarque as being egocentric and greedy. Remarque publicly stated that he wrote "All Quiet on the Western Front" for personal reasons, not for profit, as Friedlaender had charged.








African Americans

African Americans, also known as Afro-Americans or Black Americans, are an ethnic group consisting of Americans with partial or total ancestry from any of the Black racial groups of Africa. African Americans constitute the third largest racial or ethnic group in the U.S. after White Americans and Hispanic and Latino Americans. The term "African American" generally denotes descendants of Africans enslaved in the United States. 

Most African Americans are descendants of enslaved people within the boundaries of the present United States. While some Black immigrants or their children may also come to identify as African American, the majority of first-generation immigrants do not, preferring to identify with their nation of origin. The majority of African Americans are of West and Central African ancestry, with some significant Western European and small Native American ancestry.

African-American history began in the 16th century, with Africans from West and Central Africa being sold to European slave traders and transported across the Atlantic to the Western Hemisphere. After arriving in the Americas, they were sold as slaves to European colonists and put to work on plantations, particularly in the southern colonies. A few were able to achieve freedom through manumission or escape and founded independent communities before and during the American Revolution. After the United States was founded in 1783, most Black people continued to be enslaved, being most concentrated in the American South, with four million enslaved only liberated during and at the end of the Civil War in 1865. During Reconstruction, they gained citizenship and adult-males the right to vote; due to the widespread policy and ideology of White supremacy, they were largely treated as second-class citizens and found themselves soon disenfranchised in the South. These circumstances changed due to participation in the military conflicts of the United States, substantial migration out of the South, the elimination of legal racial segregation, and the civil rights movement which sought political and social freedom. However, racism against African Americans remains a problem into the 21st century. In 2008, Barack Obama became the first, and so far only African American to be elected president of the United States.

African-American culture has had a significant influence on worldwide culture, making numerous contributions to visual arts, literature, the English language, philosophy, politics, cuisine, sports, and music. The African-American contributions to popular music is so profound that most American music, including jazz, gospel, blues, rock and roll, funk, disco, hip hop, R&B and soul, has its origins either partially or entirely in the African-American community.

The vast majority of those who were enslaved and transported in the transatlantic slave trade were people from several Central and West Africa ethnic groups, who had been captured directly by the slave traders in coastal raids, or sold by other West Africans, or by half-European "merchant princes" to European slave traders, who brought them to the Americas.

The first African slaves arrived via Santo Domingo to the San Miguel de Gualdape colony (most likely located in the Winyah Bay area of present-day South Carolina), founded by Spanish explorer Lucas Vázquez de Ayllón in 1526. The ill-fated colony was almost immediately disrupted by a fight over leadership, during which the slaves revolted and fled the colony to seek refuge among local Native Americans. De Ayllón and many of the colonists died shortly afterward of an epidemic and the colony was abandoned. The settlers and the slaves who had not escaped returned to Haiti, whence they had come.

The marriage between Luisa de Abrego, a free Black domestic servant from Seville, and Miguel Rodríguez, a White Segovian conquistador in 1565 in St. Augustine (Spanish Florida), is the first known and recorded Christian marriage anywhere in what is now the continental United States.
The first recorded Africans in English America (including most of the future United States) were "20 and odd negroes" who came to Jamestown, Virginia via Cape Comfort in August 1619 as indentured servants. As many Virginian settlers began to die from harsh conditions, more and more Africans were brought to work as laborers.
An indentured servant (who could be White or Black) would work for several years (usually four to seven) without wages. The status of indentured servants in early Virginia and Maryland was similar to slavery. Servants could be bought, sold, or leased and they could be physically beaten for disobedience or running away. Unlike slaves, they were freed after their term of service expired or was bought out, their children did not inherit their status, and on their release from contract they received "a year's provision of corn, double apparel, tools necessary", and a small cash payment called "freedom dues". Africans could legally raise crops and cattle to purchase their freedom. They raised families, married other Africans and sometimes intermarried with Native Americans or European settlers.

By the 1640s and 1650s, several African families owned farms around Jamestown and some became wealthy by colonial standards and purchased indentured servants of their own. In 1640, the Virginia General Court recorded the earliest documentation of lifetime slavery when they sentenced John Punch, a Negro, to lifetime servitude under his master Hugh Gwyn for running away.
In the Spanish Florida some Spanish married or had unions with Pensacola, Creek or African women, both slave and free, and their descendants created a mixed-race population of mestizos and mulattos. The Spanish encouraged slaves from the colony of Georgia to come to Florida as a refuge, promising freedom in exchange for conversion to Catholicism. King Charles II issued a royal proclamation freeing all slaves who fled to Spanish Florida and accepted conversion and baptism. Most went to the area around St. Augustine, but escaped slaves also reached Pensacola. St. Augustine had mustered an all-Black militia unit defending Spanish Florida as early as 1683.

One of the Dutch African arrivals, Anthony Johnson, would later own one of the first Black "slaves", John Casor, resulting from the court ruling of a civil case.

The popular conception of a race-based slave system did not fully develop until the 18th century. The Dutch West India Company introduced slavery in 1625 with the importation of eleven Black slaves into New Amsterdam (present-day New York City). All the colony's slaves, however, were freed upon its surrender to the English.

Massachusetts was the first English colony to legally recognize slavery in 1641. In 1662, Virginia passed a law that children of enslaved women took the status of the mother, rather than that of the father, as under common law. This legal principle was called "partus sequitur ventrum".

By an act of 1699, Virginia ordered all free Blacks deported, virtually defining as slaves all people of African descent who remained in the colony. In 1670, the colonial assembly passed a law prohibiting free and baptized Blacks (and Native Americans) from purchasing Christians (in this act meaning White Europeans) but allowing them to buy people "of their owne nation".
In the Spanish Louisiana although there was no movement toward abolition of the African slave trade, Spanish rule introduced a new law called "coartación", which allowed slaves to buy their freedom, and that of others. Although some did not have the money to buy their freedom, government measures on slavery allowed many free Blacks. That brought problems to the Spaniards with the French Creoles who also populated Spanish Louisiana, French creoles cited that measure as one of the system's worst elements.

First established in South Carolina in 1704, groups of armed White men—slave patrols—were formed to monitor enslaved Black people. Their function was to police slaves, especially fugitives. Slave owners feared that slaves might organize revolts or slave rebellions, so state militias were formed in order to provide a military command structure and discipline within the slave patrols so they could be used to detect, encounter, and crush any organized slave meetings which might lead to revolts or rebellions.

The earliest African American congregations and churches were organized before 1800 in both northern and southern cities following the Great Awakening. By 1775, Africans made up 20% of the population in the American colonies, which made them the second largest ethnic group after English Americans.

During the 1770s, Africans, both enslaved and free, helped rebellious American colonists secure their independence by defeating the British in the American Revolutionary War. Blacks played a role in both sides in the American Revolution. Activists in the Patriot cause included James Armistead, Prince Whipple, and Oliver Cromwell. Around 15,000 Black Loyalists left with the British after the war, most of them ending up as free Black people in England or its colonies, such as the Black Nova Scotians and the Sierra Leone Creole people.

In the Spanish Louisiana, Governor Bernardo de Gálvez organized Spanish free Black men into two militia companies to defend New Orleans during the American Revolution. They fought in the 1779 battle in which Spain captured Baton Rouge from the British. Gálvez also commanded them in campaigns against the British outposts in Mobile, Alabama, and Pensacola, Florida. He recruited slaves for the militia by pledging to free anyone who was seriously wounded and promised to secure a low price for "coartación" (buy their freedom and that of others) for those who received lesser wounds. During the 1790s, Governor Francisco Luis Héctor, baron of Carondelet reinforced local fortifications and recruit even more free Black men for the militia. Carondelet doubled the number of free Black men who served, creating two more militia companies—one made up of Black members and the other of pardo (mixed race). Serving in the militia brought free Black men one step closer to equality with Whites, allowing them, for example, the right to carry arms and boosting their earning power. However, actually these privileges distanced free Black men from enslaved Blacks and encouraged them to identify with Whites.

Slavery had been tacitly enshrined in the U.S. Constitution through provisions such as Article I, Section 2, Clause 3, commonly known as the 3/5 compromise. Because of , Congress was unable to pass an Act Prohibiting Importation of Slaves until 1807. Fugitive slave laws (derived from the Fugitive Slave Clause of the Constitution—) were passed by Congress in 1793 and 1850, guaranteeing the right for a slaveholder to recover an escaped slave within the U.S. Slave owners, who viewed slaves as property, made it a federal crime to assist those who had escaped slavery or to interfere with their capture. Slavery, which by then meant almost exclusively Black people, was the most important political issue in the Antebellum United States, leading to one crisis after another. Among these were the Missouri Compromise, the Compromise of 1850, the Dred Scott decision, and John Brown's raid on Harpers Ferry.
Prior to the Civil War, eight serving presidents owned slaves, a practice protected by the U.S. Constitution. By 1860, there were 3.5 to 4.4 million enslaved Black people in the U.S. due to the Atlantic slave trade, and another 488,000–500,000 Blacks lived free (with legislated limits) across the country. With legislated limits imposed upon them in addition to "unconquerable prejudice" from Whites according to Henry Clay, some Black people who were not enslaved left the U.S. for Liberia in West Africa. Liberia began as a settlement of the American Colonization Society (ACS) in 1821, with the abolitionist members of the ACS believing Blacks would face better chances for freedom and equality in Africa.

The slaves not only constituted a large investment, they produced America's most valuable product and export: cotton. They helped build the United States Capitol, the White House and other Washington, D.C.-based buildings.) Similar building projects existed in the slave states.

By 1815, the domestic slave trade had become a major economic activity in the United States; it lasted until the 1860s. Historians estimate nearly one million in total took part in the forced migration of this new "Middle Passage". The historian Ira Berlin called this forced migration of slaves the "central event" in the life of a slave between the American Revolution and the Civil War, writing that whether slaves were directly uprooted or lived in fear that they or their families would be involuntarily moved, "the massive deportation traumatized black people". Individuals lost their connection to families and clans, and many ethnic Africans lost their knowledge of varying tribal origins in Africa.

The 1863 photograph of Wilson Chinn, a branded slave from Louisiana, like the one of Gordon and his scarred back, served as two early examples of how the newborn medium of photography could encapsulate the cruelty of slavery.

Emigration of free Blacks to their continent of origin had been proposed since the Revolutionary war. After Haiti became independent, it tried to recruit African Americans to migrate there after it re-established trade relations with the United States. The Haitian Union was a group formed to promote relations between the countries. After riots against Blacks in Cincinnati, its Black community sponsored founding of the Wilberforce Colony, an initially successful settlement of African American immigrants to Canada. The colony was one of the first such independent political entities. It lasted for a number of decades and provided a destination for about 200 Black families emigrating from a number of locations in the United States.

In 1863, during the American Civil War, President Abraham Lincoln signed the Emancipation Proclamation. The proclamation declared that all slaves in Confederate-held territory were free. Advancing Union troops enforced the proclamation, with Texas being the last state to be emancipated, in 1865.
Slavery in a few border states continued until the ratification of the Thirteenth Amendment in December 1865. While the Naturalization Act of 1790 limited U.S. citizenship to Whites only, the 14th Amendment (1868) gave Black people citizenship, and the 15th Amendment (1870) gave Black men the right to vote.

African Americans quickly set up congregations for themselves, as well as schools and community/civic associations, to have space away from White control or oversight. While the post-war Reconstruction era was initially a time of progress for African Americans, that period ended in 1876. By the late 1890s, Southern states enacted Jim Crow laws to enforce racial segregation and disenfranchisement. Segregation was now imposed with Jim Crow laws, using signs used to show Blacks where they could legally walk, talk, drink, rest, or eat. For those places that were racially mixed, non-Whites had to wait until all White customers were dealt with. Most African Americans obeyed the Jim Crow laws, to avoid racially motivated violence. To maintain self-esteem and dignity, African Americans such as Anthony Overton and Mary McLeod Bethune continued to build their own schools, churches, banks, social clubs, and other businesses.

In the last decade of the 19th century, racially discriminatory laws and racial violence aimed at African Americans began to mushroom in the United States, a period often referred to as the "nadir of American race relations". These discriminatory acts included racial segregation—upheld by the United States Supreme Court decision in "Plessy v. Ferguson" in 1896—which was legally mandated by southern states and nationwide at the local level of government, voter suppression or disenfranchisement in the southern states, denial of economic opportunity or resources nationwide, and private acts of violence and mass racial violence aimed at African Americans unhindered or encouraged by government authorities.

The desperate conditions of African Americans in the South sparked the Great Migration during the first half of the 20th century which led to a growing African American community in Northern and Western United States. The rapid influx of Blacks disturbed the racial balance within Northern and Western cities, exacerbating hostility between both Blacks and Whites in the two regions. The Red Summer of 1919 was marked by hundreds of deaths and higher casualties across the U.S. as a result of race riots that occurred in more than three dozen cities, such as the Chicago race riot of 1919 and the Omaha race riot of 1919. Overall, Blacks in Northern and Western cities experienced systemic discrimination in a plethora of aspects of life. Within employment, economic opportunities for Blacks were routed to the lowest-status and restrictive in potential mobility. At the 1900 Hampton Negro Conference, Reverend Matthew Anderson said: "...the lines along most of the avenues of wage earning are more rigidly drawn in the North than in the South." Within the housing market, stronger discriminatory measures were used in correlation to the influx, resulting in a mix of "targeted violence, restrictive covenants, redlining and racial steering". While many Whites defended their space with violence, intimidation, or legal tactics toward African Americans, many other Whites migrated to more racially homogeneous suburban or exurban regions, a process known as White flight.
Despite discrimination, drawing cards for leaving the hopelessness in the South were the growth of African American institutions and communities in Northern cities. Institutions included Black oriented organizations (e.g., Urban League, NAACP), churches, businesses, and newspapers, as well as successes in the development in African American intellectual culture, music, and popular culture (e.g., Harlem Renaissance, Chicago Black Renaissance). The Cotton Club in Harlem was a Whites-only establishment, with Blacks (such as Duke Ellington) allowed to perform, but to a White audience. Black Americans also found a new ground for political power in Northern cities, without the enforced disabilities of Jim Crow.

By the 1950s, the civil rights movement was gaining momentum. A 1955 lynching that sparked public outrage about injustice was that of Emmett Till, a 14-year-old boy from Chicago. Spending the summer with relatives in Money, Mississippi, Till was killed for allegedly having wolf-whistled at a White woman. Till had been badly beaten, one of his eyes was gouged out, and he was shot in the head. The visceral response to his mother's decision to have an open-casket funeral mobilized the Black community throughout the U.S. Vann R. Newkirk wrote "the trial of his killers became a pageant illuminating the tyranny of White supremacy". The state of Mississippi tried two defendants, but they were speedily acquitted by an all-White jury. One hundred days after Emmett Till's murder, Rosa Parks refused to give up her seat on the bus in Alabama—indeed, Parks told Emmett's mother Mamie Till that "the photograph of Emmett's disfigured face in the casket was set in her mind when she refused to give up her seat on the Montgomery bus."
The March on Washington for Jobs and Freedom and the conditions which brought it into being are credited with putting pressure on presidents John F. Kennedy and Lyndon B. Johnson. Johnson put his support behind passage of the Civil Rights Act of 1964 that banned discrimination in public accommodations, employment, and labor unions, and the Voting Rights Act of 1965, which expanded federal authority over states to ensure Black political participation through protection of voter registration and elections. By 1966, the emergence of the Black Power movement, which lasted from 1966 to 1975, expanded upon the aims of the civil rights movement to include economic and political self-sufficiency, and freedom from White authority.

During the post-war period, many African Americans continued to be economically disadvantaged relative to other Americans. Average Black income stood at 54 percent of that of White workers in 1947, and 55 percent in 1962. In 1959, median family income for Whites was $5,600 (), compared with $2,900 () for non-White families. In 1965, 43 percent of all Black families fell into the poverty bracket, earning under $3,000 () a year. The 1960s saw improvements in the social and economic conditions of many Black Americans.

From 1965 to 1969, Black family income rose from 54 to 60 percent of White family income. In 1968, 23 percent of Black families earned under $3,000 () a year, compared with 41 percent in 1960. In 1965, 19 percent of Black Americans had incomes equal to the national median, a proportion that rose to 27 percent by 1967. In 1960, the median level of education for Blacks had been 10.8 years, and by the late 1960s, the figure rose to 12.2 years, half a year behind the median for Whites.

Politically and economically, African Americans have made substantial strides during the post–civil rights era. In 1967, Thurgood Marshall became the first African American Supreme Court Justice. In 1968, Shirley Chisholm became the first Black woman elected to the U.S. Congress. In 1989, Douglas Wilder became the first African American elected governor in U.S. history. Clarence Thomas succeeded Marshall to become the second African American Supreme Court Justice in 1991. In 1992, Carol Moseley-Braun of Illinois became the first African American woman elected to the U.S. Senate. There were 8,936 Black officeholders in the United States in 2000, showing a net increase of 7,467 since 1970. In 2001, there were 484 Black mayors.

In 2005, the number of Africans immigrating to the United States, in a single year, surpassed the peak number who were involuntarily brought to the United States during the Atlantic Slave Trade. On November 4, 2008, Democratic Senator Barack Obama defeated Republican Senator John McCain to become the first African American to be elected president. At least 95 percent of African American voters voted for Obama. He also received overwhelming support from young and educated Whites, a majority of Asians, and Hispanics, picking up a number of new states in the Democratic electoral column. Obama lost the overall White vote, although he won a larger proportion of White votes than any previous non-incumbent Democratic presidential candidate since Jimmy Carter. Obama was reelected for a second and final term, by a similar margin on November 6, 2012. In 2021, Kamala Harris became the first woman, the first African American, and the first Asian American to serve as Vice President of the United States. In June 2021, Juneteenth, a day which commemorates the end of slavery in the US, became a federal holiday.

In 1790, when the first U.S. census was taken, Africans (including slaves and free people) numbered about 760,000—about 19.3% of the population. In 1860, at the start of the Civil War, the African American population had increased to 4.4 million, but the percentage rate dropped to 14% of the overall population of the country. The vast majority were slaves, with only 488,000 counted as "freemen". By 1900, the Black population had doubled and reached 8.8 million.

In 1910, about 90% of African Americans lived in the South. Large numbers began migrating north looking for better job opportunities and living conditions, and to escape Jim Crow laws and racial violence. The Great Migration, as it was called, spanned the 1890s to the 1970s. From 1916 through the 1960s, more than 6 million Black people moved north. But in the 1970s and 1980s, that trend reversed, with more African Americans moving south to the Sun Belt than leaving it.

The following table of the African American population in the United States over time shows that the African American population, as a percentage of the total population, declined until 1930 and has been rising since then.
By 1990, the African American population reached about 30 million and represented 12% of the U.S. population, roughly the same proportion as in 1900.

At the time of the 2000 U.S. census, 54.8% of African Americans lived in the South. In that year, 17.6% of African Americans lived in the Northeast and 18.7% in the Midwest, while only 8.9% lived in the Western states. The west does have a sizable Black population in certain areas, however. California, the nation's most populous state, has the fifth largest African American population, only behind New York, Texas, Georgia, and Florida. According to the 2000 Census, approximately 2.05% of African Americans identified as Hispanic or Latino in origin, many of whom may be of Brazilian, Puerto Rican, Dominican, Cuban, Haitian, or other Latin American descent. The only self-reported "ancestral" groups larger than African Americans are the Irish and Germans.

According to the 2010 census, nearly 3% of people who self-identified as Black had recent ancestors who immigrated from another country. Self-reported non-Hispanic Black immigrants from the Caribbean, mostly from Jamaica and Haiti, represented 0.9% of the U.S. population, at 2.6 million. Self-reported Black immigrants from sub-Saharan Africa also represented 0.9%, at about 2.8 million. Additionally, self-identified Black Hispanics represented 0.4% of the United States population, at about 1.2 million people, largely found within the Puerto Rican and Dominican communities. Self-reported Black immigrants hailing from other countries in the Americas, such as Brazil and Canada, as well as several European countries, represented less than 0.1% of the population. Mixed-race Hispanic and non-Hispanic Americans who identified as being part Black, represented 0.9% of the population. Of the 12.6% of United States residents who identified as Black, around 10.3% were "native Black American" or ethnic African Americans, who are direct descendants of West/Central Africans brought to the U.S. as slaves. These individuals make up well over 80% of all Blacks in the country. When including people of mixed-race origin, about 13.5% of the U.S. population self-identified as Black or "mixed with Black". However, according to the U.S. Census Bureau, evidence from the 2000 census indicates that many African and Caribbean immigrant ethnic groups do not identify as "Black, African Am., or Negro". Instead, they wrote in their own respective ethnic groups in the "Some Other Race" write-in entry. As a result, the census bureau devised a new, separate "African American" ethnic group category in 2010 for ethnic African Americans.

Historically, African Americans have been undercounted in the U.S. census due to a number of factors and biases. In the 2020 census, the African American population was undercounted at an estimated rate of 3.3%, up from 2.1% in 2010.

Texas has the largest African American population by state. Followed by Texas is Florida, with 3.8 million, and Georgia, with 3.6 million.

After 100 years of African Americans leaving the south in large numbers seeking better opportunities and treatment in the west and north, a movement known as the Great Migration, there is now a reverse trend, called the New Great Migration. As with the earlier Great Migration, the New Great Migration is primarily directed toward cities and large urban areas, such as Charlotte, Houston, Dallas, Fort Worth, Huntsville, Raleigh, Tampa, San Antonio, New Orleans, Memphis, Nashville, Jacksonville, and so forth. A growing percentage of African Americans from the west and north are migrating to the southern region of the U.S. for economic and cultural reasons. The New York City, Chicago, and Los Angeles metropolitan areas have the highest decline in African Americans, while Atlanta, Dallas, and Houston have the highest increase respectively. Several smaller metro areas also saw sizable gains, including San Antonio; Raleigh and Greensboro, N.C.; and Orlando. Despite recent declines, as of 2020, the New York City metropolitan area still has the largest African American metropolitan population in the United States and the only to have over 3 million African Americans. 

Among cities of 100,000 or more, South Fulton, Georgia had the highest percentage of Black residents of any large U.S. city in 2020, with 93%. Other large cities with African American majorities include Jackson, Mississippi (80%), Detroit, Michigan (80%), Birmingham, Alabama (70%), Miami Gardens, Florida (67%), Memphis, Tennessee (63%), Montgomery, Alabama (62%), Baltimore, Maryland (60%), Augusta, Georgia (59%), Shreveport, Louisiana (58%), New Orleans, Louisiana (57%), Macon, Georgia (56%), Baton Rouge, Louisiana (55%), Hampton, Virginia (53%), Newark, New Jersey (53%), Mobile, Alabama (53%), Cleveland, Ohio (52%), Brockton, Massachusetts (51%), and Savannah, Georgia (51%).

The nation's most affluent community with an African American majority resides in View Park–Windsor Hills, California, with an annual median household income of $159,618. Other largely affluent and African American communities include Prince George's County (namely Mitchellville, Woodmore, Upper Marlboro) and Charles County in Maryland, Dekalb County (namely Stonecrest, Lithonia, Smoke Rise) and South Fulton in Georgia, Charles City County in Virginia, Baldwin Hills in California, Hillcrest and Uniondale in New York, and Cedar Hill, DeSoto, and Missouri City in Texas. Queens County, New York is the only county with a population of 65,000 or more where African Americans have a higher median household income than White Americans.

Seatack, Virginia is currently the oldest African American community in the United States. It survives today with a vibrant and active civic community.

During slavery, anti-literacy laws were enacted in the U.S. that prohibited education for Black people. Slave owners saw literacy as a threat to the institution of slavery. As a North Carolina statute stated, "Teaching slaves to read and write, tends to excite dissatisfaction in their minds, and to produce insurrection and rebellion."

When slavery was finally abolished in 1865, public educational systems were expanding across the country. By 1870, around seventy-four institutions in the south provided a form of advanced education for African American students. By 1900, over a hundred programs at these schools provided training for Black professionals, including teachers. Many of the students at Fisk University, including the young W. E. B. Du Bois, taught school during the summers to support their studies.

African Americans were very concerned to provide quality education for their children, but White supremacy limited their ability to participate in educational policymaking on the political level. State governments soon moved to undermine their citizenship by restricting their right to vote. By the late 1870s, Blacks were disenfranchised and segregated across the American South. White politicians in Mississippi and other states withheld financial resources and supplies from Black schools. Nevertheless, the presence of Black teachers, and their engagement with their communities both inside and outside the classroom, ensured that Black students had access to education despite these external constraints.

During World War II, demands for unity and racial tolerance on the home front provided an opening for the first Black history curriculum in the country. For example, during the early 1940s, Madeline Morgan, a Black teacher in the Chicago public schools, created a curriculum for students in grades one through eight highlighting the contributions of Black people to the history of the United States. At the close of the war, Chicago's Board of Education downgraded the curriculum's status from mandatory to optional.

Predominantly Black schools for kindergarten through twelfth grade students were common throughout the U.S. before the 1970s. By 1972, however, desegregation efforts meant that only 25% of Black students were in schools with more than 90% non-White students. However, since then, a trend towards re-segregation affected communities across the country: by 2011, 2.9 million African American students were in such overwhelmingly minority schools, including 53% of Black students in school districts that were formerly under desegregation orders.

As late as 1947, about one third of African Americans over 65 were considered to lack the literacy to read and write their own names. By 1969, illiteracy as it had been traditionally defined, had been largely eradicated among younger African Americans.

U.S. census surveys showed that by 1998, 89 percent of African Americans aged 25 to 29 had completed a high-school education, less than Whites or Asians, but more than Hispanics. On many college and university entrance exams or on standardized tests and grades, African Americans have historically lagged behind Whites, but some studies suggest that the achievement gap has been closing. Many policy makers have proposed that this gap can and will be eliminated through policies such as affirmative action, desegregation, and multiculturalism.
Between 1995 and 2009, freshmen college enrollment for African Americans increased by 73 percent and only 15 percent for Whites. Black women are enrolled in college more than any other race and gender group, leading all with 9.7% enrolled according to the 2011 U.S. Census Bureau. 
The average high school graduation rate of Blacks in the United States has steadily increased to 71% in 2013. Separating this statistic into component parts shows it varies greatly depending upon the state and the school district examined. 38% of Black males graduated in the state of New York but in Maine 97% graduated and exceeded the White male graduation rate by 11 percentage points. In much of the southeastern United States and some parts of the southwestern United States the graduation rate of White males was in fact below 70% such as in Florida where 62% of White males graduated from high school. Examining specific school districts paints an even more complex picture. In the Detroit school district, the graduation rate of Black males was 20% but 7% for White males. In the New York City school district 28% of Black males graduate from high school compared to 57% of White males. In Newark County 76% of Black males graduated compared to 67% for White males. Further academic improvement has occurred in 2015. Roughly 23% of all Blacks have bachelor's degrees. In 1988, 21% of Whites had obtained a bachelor's degree versus 11% of Blacks. In 2015, 23% of Blacks had obtained a bachelor's degree versus 36% of Whites. Foreign born Blacks, 9% of the Black population, made even greater strides. They exceed native born Blacks by 10 percentage points.

College Board, which runs the official college-level advanced placement (AP) programs in American high schools, have has received criticism in recent years that its curricula have focused too much on Euro-centric history. In 2020, College Board reshaped some curricula among history-based courses to further reflect the African diaspora. In 2021, College Board announced it would be piloting an AP African American Studies course between 2022 and 2024. The course is expected to launch in 2024.

Historically Black colleges and universities (HBCUs), which were founded when segregated institutions of higher learning did not admit African Americans, continue to thrive and educate students of all races today. There are 101 HBCUs representing three percent of the nation's colleges and universities with the majority established in the Southeast. HBCUs have been largely responsible for establishing and expanding the African American middle-class by providing more career opportunities for African Americans.

The economic disparity between the races in the U.S. has marginally improved since the end of slavery. In 1863, two years prior to emancipation, Black people owned 0.5 percent of the national wealth, while in 2019 it is just over 1.5 percent. Racial disparity in poverty rates has narrowed since the civil rights era, with the poverty rate among African Americans decreasing from 24.7% in 2004 to 18.8% in 2020, compared to 10.5% for all Americans. Poverty is associated with higher rates of marital stress and dissolution, physical and mental health problems, disability, cognitive deficits, low educational attainment, and crime.

African Americans have a long and diverse history of business ownership. Although the first African American business is unknown, slaves captured from West Africa are believed to have established commercial enterprises as peddlers and skilled craftspeople as far back as the 17th century. Around 1900, Booker T. Washington became the most famous proponent of African American businesses. His critic and rival W. E. B. DuBois also commended business as a vehicle for African American advancement.

African Americans had a combined buying power of over $1.6 trillion as of 2021, a 171% increase of their buying power in 2000 but lagging significantly in growth behind American Latinos and Asians in the same timer period (with 288% and 383%, respectively; for reference, US growth overall was 144% in the same period); however, African American net worth had shrunk 14% in the previous year despite strong growth in property prices and the S&P 500. In 2002, African American-owned businesses accounted for 1.2 million of the US's 23 million businesses. , African American-owned businesses account for approximately 2 million US businesses. Black-owned businesses experienced the largest growth in number of businesses among minorities from 2002 to 2011.

Twenty-five percent of Blacks had white-collar occupations (management, professional, and related fields) in 2000, compared with 33.6% of Americans overall. In 2001, over half of African American households of married couples earned $50,000 or more. Although in the same year African Americans were over-represented among the nation's poor, this was directly related to the disproportionate percentage of African American families headed by single women; such families are collectively poorer, regardless of ethnicity.

In 2006, the median earnings of African American men was more than Black and non-Black American women overall, and in all educational levels. At the same time, among American men, income disparities were significant; the median income of African American men was approximately 76 cents for every dollar of their European American counterparts, although the gap narrowed somewhat with a rise in educational level.

Overall, the median earnings of African American men were 72 cents for every dollar earned of their Asian American counterparts, and $1.17 for every dollar earned by Hispanic men. On the other hand, by 2006, among American women with post-secondary education, African American women have made significant advances; the median income of African American women was more than those of their Asian-, European- and Hispanic American counterparts with at least some college education.

The U.S. public sector is the single most important source of employment for African Americans. During 2008–2010, 21.2% of all Black workers were public employees, compared with 16.3% of non-Black workers. Both before and after the onset of the Great Recession, African Americans were 30% more likely than other workers to be employed in the public sector. The public sector is also a critical source of decent-paying jobs for Black Americans. For both men and women, the median wage earned by Black employees is significantly higher in the public sector than in other industries.

In 1999, the median income of African American families was $33,255 compared to $53,356 of European Americans. In times of economic hardship for the nation, African Americans suffer disproportionately from job loss and underemployment, with the Black underclass being hardest hit. The phrase "last hired and first fired" is reflected in the Bureau of Labor Statistics unemployment figures. Nationwide, the October 2008 unemployment rate for African Americans was 11.1%, while the nationwide rate was 6.5%. In 2007, the average income for African Americans was approximately $34,000, compared to $55,000 for Whites. African Americans experience a higher rate of unemployment than the general population.

The income gap between Black and White families is also significant. In 2005, employed Blacks earned 65% of the wages of Whites, down from 82% in 1975. "The New York Times" reported in 2006 that in Queens, New York, the median income among African American families exceeded that of White families, which the newspaper attributed to the growth in the number of two-parent Black families. It noted that Queens was the only county with more than 65,000 residents where that was true. In 2011, it was reported that 72% of Black babies were born to unwed mothers. The poverty rate among single-parent Black families was 39.5% in 2005, according to Walter E. Williams, while it was 9.9% among married-couple Black families. Among White families, the respective rates were 26.4% and 6% in poverty.

Collectively, African Americans are more involved in the American political process than other minority groups in the United States, indicated by the highest level of voter registration and participation in elections among these groups in 2004. African Americans also have the highest level of Congressional representation of any minority group in the U.S.

Homeownership in the U.S. is the strongest indicator of financial stability and the primary asset most Americans use to generate wealth. African Americans continue to lag behind other racial groups in becoming homeowners. In the first quarter of 2021, 45.1% of African Americans owned their homes, compared to 65.3% of all Americans. The African American homeownership rate has remained relatively flat since the 1970s despite an increase in anti-discrimination housing laws and protections. The average white high school drop-out still has a slightly better chance of owning a home than the average African American college graduate usually due to unfavorable debt-to-income ratios or credit scores among most African American college graduates. Since 2000, fast-growing housing costs in most cities have made it even more difficult for the U.S. African-American homeownership rate to significantly grow and reach over 50% for the first time in history. From 2000 to 2022, the median home price in the U.S. grew 160%, outpacing average annual household income growth in that same period, which only grew about 30%. South Carolina is the state with the most African American homeownership, with about 55% of African Americans owning their own homes.

Since the mid 20th century, a large majority of African Americans support the Democratic Party. In the 2020 Presidential election, 91% of African American voters supported Democrat Joe Biden, while 8% supported Republican Donald Trump. Although there is an African American lobby in foreign policy, it has not had the impact that African American organizations have had in domestic policy.

Many African Americans were excluded from electoral politics in the decades following the end of Reconstruction. For those that could participate, until the New Deal, African Americans were supporters of the Republican Party because it was Republican President Abraham Lincoln who helped in granting freedom to American slaves; at the time, the Republicans and Democrats represented the sectional interests of the North and South, respectively, rather than any specific ideology, and both conservative and liberal were represented equally in both parties.

The African American trend of voting for Democrats can be traced back to the 1930s during the Great Depression, when Franklin D. Roosevelt's New Deal program provided economic relief to African Americans. Roosevelt's New Deal coalition turned the Democratic Party into an organization of the working class and their liberal allies, regardless of region. The African American vote became even more solidly Democratic when Democratic presidents John F. Kennedy and Lyndon B. Johnson pushed for civil rights legislation during the 1960s. In 1960, nearly a third of African Americans voted for Republican Richard Nixon.

"Lift Every Voice and Sing" is often referred to as the Black national anthem in the United States. In 1919, the National Association for the Advancement of Colored People (NAACP) had dubbed it the "Negro national anthem" for its power in voicing a cry for liberation and affirmation for African American people.

According to a Gallup survey, 4.6% of Black or African Americans self-identified as LGBT in 2016, while the total portion of American adults in all ethnic groups identifying as LGBT was 4.1% in 2016.

The life expectancy for Black men in 2008 was 70.8 years. Life expectancy for Black women was 77.5 years in 2008. In 1900, when information on Black life expectancy started being collated, a Black man could expect to live to 32.5 years and a Black woman 33.5 years. In 1900, White men lived an average of 46.3 years and White women lived an average of 48.3 years. African American life expectancy at birth is persistently five to seven years lower than European Americans. Black men have shorter lifespans than any other group in the US besides Native American men.

Black people have higher rates of obesity, diabetes, and hypertension than the U.S. average. For adult Black men, the rate of obesity was 31.6% in 2010. For adult Black women, the rate of obesity was 41.2% in 2010. African Americans have higher rates of mortality than any other racial or ethnic group for 8 of the top 10 causes of death. In 2013, among men, Black men had the highest rate of getting cancer, followed by White, Hispanic, Asian/Pacific Islander (A/PI), and American Indian/Alaska Native (AI/AN) men. Among women, White women had the highest rate of getting cancer, followed by Black, Hispanic, Asian/Pacific Islander, and American Indian/Alaska Native women. African Americans also have higher prevalence and incidence of Alzheimer's disease compared to the overall average.

Violence has an impact upon African American life expectancy. A report from the U.S. Department of Justice states "In 2005, homicide victimization rates for blacks were 6 times higher than the rates for whites". The report also found that "94% of black victims were killed by blacks." Black boys and men age 15–44 are the only race/sex category for which homicide is a top-five cause of death.

African-Americans are more likely than White Americans to die due to health-related problems developed by alcoholism. Alcohol abuse is the main contributor to the top 3 causes of death among African Americans.

In December 2020, African Americans were less likely to be vaccinated against COVID-19 due to mistrust in the U.S. medical system related to decades of abuses and anti-black treatment. From 2021 to 2022, there was an increase in African Americans who became vaccinated. Still, in 2022, COVID-19 complications became the third leading cause of death for African Americans.

According to the Centers for Disease Control and Prevention, African Americans have higher rates of sexually transmitted infections (STIs) compared to Whites, with 5 times the rates of syphilis and chlamydia, and 7.5 times the rate of gonorrhea.

The disproportionately high incidence of HIV/AIDS among African Americans has been attributed to homophobic influences and lack of proper healthcare. The prevalence of HIV/AIDS among Black men is seven times higher than the prevalence for White men, and Black men are more than nine times as likely to die from HIV/AIDS-related illness than White men.

African Americans have several barriers for accessing mental health services. Counseling has been frowned upon and distant in utility and proximity to many people in the African American community. In 2004, a qualitative research study explored the disconnect with African Americans and mental health. The study was conducted as a semi-structured discussion which allowed the focus group to express their opinions and life experiences. The results revealed a couple key variables that create barriers for many African American communities to seek mental health services such as the stigma, lack of four important necessities; trust, affordability, cultural understanding and impersonal services.

Historically, many African American communities did not seek counseling because religion was a part of the family values. African American who have a faith background are more likely to seek prayer as a coping mechanism for mental issues rather than seeking professional mental health services. In 2015 a study concluded, African Americans with high value in religion are less likely to utilize mental health services compared to those who have low value in religion.

Most counseling approaches are westernized and do not fit within the African American culture. African American families tend to resolve concerns within the family, and it is viewed by the family as a strength. On the other hand, when African Americans seek counseling, they face a social backlash and are criticized. They may be labeled "crazy", viewed as weak, and their pride is diminished. Because of this, many African Americans instead seek mentorship within communities they trust.

Terminology is another barrier in relation to African Americans and mental health. There is more stigma on the term "psychotherapy" versus counseling. In one study, psychotherapy is associated with mental illness whereas counseling approaches problem-solving, guidance and help. More African Americans seek assistance when it is called counseling and not psychotherapy because it is more welcoming within the cultural and community. Counselors are encouraged to be aware of such barriers for the well-being of African American clients. Without cultural competency training in health care, many African Americans go unheard and misunderstood.

In 2021, African Americans had the third highest suicide rate trailing American Indians/Alaska Natives and White Americans. However, African Americans had the second highest increase of its suicide rate from 2011 to 2021, growing 58%. And although suicide is a top-10 cause of death for American men overall, it is not a top-10 cause of death for African American men.

Recent surveys of African Americans using a genetic testing service have found varied ancestries which show different tendencies by region and sex of ancestors. These studies found that on average, African Americans have 73.2–82.1% West African, 16.7%–24% European, and 0.8–1.2% Native American genetic ancestry, with large variation between individuals. Genetics websites themselves have reported similar ranges, with some finding 1 or 2 percent Native American ancestry and Ancestry.com reporting an outlying percentage of European ancestry among African Americans, 29%.

According to a genome-wide study by Bryc et al. (2009), the mixed ancestry of African Americans in varying ratios came about as the result of sexual contact between West/Central Africans (more frequently females) and Europeans (more frequently males). This can be understood as being the result of enslaved African American females being raped by White males. Consequently, the 365 African Americans in their sample have a genome-wide average of 78.1% West African ancestry and 18.5% European ancestry, with large variation among individuals (ranging from 99% to 1% West African ancestry). The West African ancestral component in African Americans is most similar to that in present-day speakers from the non-Bantu branches of the Niger-Congo (Niger-Kordofanian) family.

Correspondingly, Montinaro et al. (2014) observed that around 50% of the overall ancestry of African Americans traces comes from a population similar to the Niger-Congo-speaking Yoruba of southern Nigeria and southern Benin, reflecting the centrality of this West African region in the Atlantic slave trade. The next most frequent ancestral component found among African Americans was derived from Great Britain, in keeping with historical records. It constitutes a little over 10% of their overall ancestry and is most similar to the Northwest European ancestral component also carried by Barbadians. Zakharia et al. (2009) found a similar proportion of Yoruba-like ancestry in their African American samples, with a minority also drawn from Mandenka and Bantu populations. Additionally, the researchers observed an average European ancestry of 21.9%, again with significant variation between individuals. Bryc et al. (2009) note that populations from other parts of the continent may also constitute adequate proxies for the ancestors of some African American individuals; namely, ancestral populations from Guinea Bissau, Senegal and Sierra Leone in West Africa and Angola in Southern Africa. An individual African American person can have over fifteen African ethnic groups in their genetic makeup alone due to the slave trade covering such vast areas.

Altogether, genetic studies suggest that African Americans are a genetically diverse people. According to DNA analysis led in 2006 by Penn State geneticist Mark D. Shriver, around 58 percent of African Americans have at least 12.5% European ancestry (equivalent to one European great-grandparent and their forebears), 19.6 percent of African Americans have at least 25% European ancestry (equivalent to one European grandparent and their forebears), and 1 percent of African Americans have at least 50% European ancestry (equivalent to one European parent and their forebears). According to Shriver, around 5 percent of African Americans also have at least 12.5% Native American ancestry (equivalent to one Native American great-grandparent and their forebears). Research suggests that Native American ancestry among people who identify as African American is a result of relationships that occurred soon after slave ships arrived in the American colonies, and European ancestry is of more recent origin, often from the decades before the Civil War.

Africans bearing the E-V38 (E1b1a) likely traversed across the Sahara, from east to west, approximately 19,000 years ago. E-M2 (E1b1a1) likely originated in West Africa or Central Africa. According to a Y-DNA study by Sims et al. (2007), the majority (≈60%) of African Americans belong to various subclades of the E-M2 (E1b1a1, formerly E3a) paternal haplogroup. This is the most common genetic paternal lineage found today among West/Central African males and is also a signature of the historical Bantu migrations. The next most frequent Y-DNA haplogroup observed among African Americans is the R1b clade, which around 15% of African Americans carry. This lineage is most common today among Northwestern European males. The remaining African Americans mainly belong to the paternal haplogroup I (≈7%), which is also frequent in Northwestern Europe.

According to an mtDNA study by Salas et al. (2005), the maternal lineages of African Americans are most similar to haplogroups that are today especially common in West Africa (>55%), followed closely by West-Central Africa and Southwestern Africa (<41%). The characteristic West African haplogroups L1b, L2b,c,d, and L3b,d and West-Central African haplogroups L1c and L3e in particular occur at high frequencies among African Americans. As with the paternal DNA of African Americans, contributions from other parts of the continent to their maternal gene pool are insignificant.

Formal political, economic and social discrimination against minorities has been present throughout American history. Leland T. Saito, Associate Professor of Sociology and American Studies & Ethnicity at the University of Southern California, writes, "Political rights have been circumscribed by race, class and gender since the founding of the United States, when the right to vote was restricted to White men of property. Throughout the history of the United States race has been used by Whites for legitimizing and creating difference and social, economic and political exclusion."

Although they have gained a greater degree of social equality since the civil rights movement, African Americans have remained stagnant economically, which has hindered their ability to break into the middle class and beyond. As of 2020, the racial wealth gap between Whites and Blacks remains as large as it was in 1968, with the typical net worth of a White household equivalent to that of 11.5 black households. Despite this, African Americans have increased employment rates and gained representation in the highest levels of American government in the post–civil rights era. However, widespread racism remains an issue that continues to undermine the development of social status.

Economically, of all the racially Black ethnic groups on the globe, African Americans are the wealthiest and most successful, with one in every fifty African American families being millionaires. This equates in 2023 to approximately 1.79 million African American millionaires in the United States, which is more than the total amount of millionaires in any racially Black country, and many other countries, around the world.

In 2014, African Americans made up 12% of the U.S. population, while 40% of prison inmates were African American. In the U.S., which has the largest per-capita prison population in the world, African Americans made up the second largest population of prison inmates (38%) in 2023, coming second to Whites who made up 57% of the prison population. According to the National Registry of Exonerations, Blacks are roughly 7.5 times more likely to be wrongfully convicted of murder in the U.S. than Whites. In 2012, the New York City Police Department detained people more than 500,000 times under the city's stop-and-frisk law. Of the total detained, 55% were African-Americans, while Black people made up 20% of the city's population.
African American males are more likely to be killed by police when compared to other races. This is one of the factors that led to the creation of the Black Lives Matter movement in 2013. A historical issue in the U.S. where women have weaponized their White privilege in the country by reporting on Black people, often instigating racial violence, White women calling the police on Black people became widely publicized in 2020. In African American culture there is a long history of calling a meddlesome White woman by a certain name, while "The Guardian" called 2020 "the year of Karen".

Although in the last decade Black youth have had lower rates of cannabis (marijuana) consumption than Whites of the same age, they have disproportionately higher arrest rates than Whites: in 2010, for example, Blacks were 3.73 times as likely to get arrested for using cannabis than Whites, despite not significantly more frequently being users. Even since the legalization of cannabis, there are still more arrests made for Black users than White, wasting taxpayer money, due to many of those cases being abandoned or dropped, with no charges being filed after the trivial, racially-biased arrests.

After over 50 years, marriage rates for all Americans began to decline while divorce rates and out-of-wedlock births have climbed. These changes have been greatest among African Americans. After more than 70 years of racial parity Black marriage rates began to fall behind Whites. Single-parent households have become common, and according to U.S. census figures released in January 2010, only 38 percent of Black children live with both their parents. In 2021, statistics show that over 80 percent marriages in the African American ethnic group marry within their ethnic group.

The first ever anti-miscegenation law was passed by the Maryland General Assembly in 1691, criminalizing interracial marriage. In a speech in Charleston, Illinois in 1858, Abraham Lincoln stated, "I am not, nor ever have been in favor of making voters or jurors of negroes, nor of qualifying them to hold office, nor to intermarry with white people". By the late 1800s, 38 US states had anti-miscegenation statutes. By 1924, the ban on interracial marriage was still in force in 29 states. While interracial marriage had been legal in California since 1948, in 1957 actor Sammy Davis Jr. faced a backlash for his involvement with White actress Kim Novak. Harry Cohn, the president of Columbia Pictures, with whom Novak was under contract, gave in to his concerns that a racist backlash against the relationship could hurt the studio. Davis briefly married Black dancer Loray White in 1958 to protect himself from mob violence. Inebriated at the wedding ceremony, Davis despairingly said to his best friend, Arthur Silber Jr., "Why won't they let me live my life?" The couple never lived together, and commenced divorce proceedings in September 1958. In 1958, officers in Virginia entered the home of Mildred and Richard Loving and dragged them out of bed for living together as an interracial couple, on the basis that "any white person intermarry with a colored person"—or vice versa—each party "shall be guilty of a felony" and face prison terms of five years. In 1967 the law was ruled unconstitutional (via the 14th Amendment adopted in 1868) by the U.S. Supreme Court in "Loving v. Virginia".

In 2008, Democrats overwhelmingly voted 70% against California Proposition 8, African Americans voted 58% in favor of it while 42% voted against Proposition 8. On May 9, 2012, Barack Obama, the first Black president, became the first U.S. president to support same-sex marriage. Since Obama's endorsement there has been a rapid growth in support for same-sex marriage among African Americans. As of 2012, 59% of African Americans support same-sex marriage, which is higher than support among the national average (53%) and White Americans (50%).

Polls in North Carolina, Pennsylvania, Missouri, Maryland, Ohio, Florida, and Nevada have also shown an increase in support for same sex marriage among African Americans. On November 6, 2012, Maryland, Maine, and Washington all voted for approve of same-sex marriage, along with Minnesota rejecting a constitutional amendment banning same-sex marriage. Exit polls in Maryland show about 50% of African Americans voted for same-sex marriage, showing a vast evolution among African Americans on the issue and was crucial in helping pass same-sex marriage in Maryland.

Black Americans hold far more conservative opinions on abortion, extramarital sex, and raising children out of wedlock than Democrats as a whole. On financial issues, however, African Americans are in line with Democrats, generally supporting a more progressive tax structure to provide more government spending on social services.

African Americans have fought in every war in the history of the United States.

The gains made by African Americans in the civil rights movement and in the Black Power movement not only obtained certain rights for African Americans but changed American society in far-reaching and fundamentally important ways. Prior to the 1950s, Black Americans in the South were subject to de jure discrimination, or Jim Crow laws. They were often the victims of extreme cruelty and violence, sometimes resulting in deaths: by the post World War II era, African Americans became increasingly discontented with their long-standing inequality. In the words of Martin Luther King Jr., African Americans and their supporters challenged the nation to "rise up and live out the true meaning of its creed that all men are created equal..."

The civil rights movement marked an enormous change in American social, political, economic and civic life. It brought with it boycotts, sit-ins, nonviolent demonstrations and marches, court battles, bombings and other violence; prompted worldwide media coverage and intense public debate; forged enduring civic, economic and religious alliances; and disrupted and realigned the nation's two major political parties.

Over time, it has changed in fundamental ways the manner in which Blacks and Whites interact with and relate to one another. The movement resulted in the removal of codified, "de jure" racial segregation and discrimination from American life and law, and heavily influenced other groups and movements in struggles for civil rights and social equality within American society, including the Free Speech Movement, the disabled, the women's movement, and migrant workers. It also inspired the Native American rights movement, and in King's 1964 book "Why We Can't Wait" he wrote the U.S. "was born in genocide when it embraced the doctrine that the original American, the Indian, was an inferior race."

African Americans were also involved in the drafting of laws in the United States, such as Frank L. Stanley Sr. who drafted the laws for the Human Rights Commission and the integration of Kentucky schools while his study of how African Americans were segregated was utilized by the government which led to the integration of the military.

Some activists and academics contend that American news media coverage of African American news, concerns, or dilemmas is inadequate, or that the news media present distorted images of African Americans.

To combat this, Robert L. Johnson founded Black Entertainment Television (BET), a network that targets young African Americans and urban audiences in the United States. Over the years, the network has aired such programming as rap and R&B music videos, urban-oriented movies and television series, and some public affairs programs. On Sunday mornings, BET would broadcast Christian programming; the network would also broadcast non-affiliated Christian programs during the early morning hours daily. According to Viacom, BET is now a global network that reaches households in the United States, Caribbean, Canada, and the United Kingdom. The network has gone on to spawn several spin-off channels, including BET Her (originally launched as "BET on Jazz"), which originally showcased jazz music-related programming, and later expanded to include general-interest urban programs as well as some R&B, soul, and world music.

Another network targeting African Americans is TV One. TV One's original programming was formally focused on lifestyle and entertainment-oriented shows, movies, fashion, and music programming. The network also reruns classic series from as far back as the 1970s to current series such as "Empire" and "Sister Circle". TV One is owned by Urban One, founded and controlled by Catherine Hughes. Urban One is one of the nation's largest radio broadcasting companies and the largest African American-owned radio broadcasting company in the United States.

In June 2009, NBC News launched a new website named The Grio in partnership with the production team that created the Black documentary film "Meeting David Wilson". It is the first African American video news site that focuses on underrepresented stories in existing national news. The Grio consists of a broad spectrum of original video packages, news articles, and contributor blogs on topics including breaking news, politics, health, business, entertainment and Black History.

Other Black-owned and oriented media outlets include:

From their earliest presence in North America, African Americans have significantly contributed literature, art, agricultural skills, cuisine, clothing styles, music, language, and social and technological innovation to American culture. The cultivation and use of many agricultural products in the United States, such as yams, peanuts, rice, okra, sorghum, grits, watermelon, indigo dyes, and cotton, can be traced to West African and African American influences. Notable examples include George Washington Carver, who created nearly 500 products from peanuts, sweet potatoes, and pecans. Soul food is a variety of cuisine popular among African Americans. It is closely related to the cuisine of the Southern United States. The descriptive terminology may have originated in the mid-1960s, when "soul" was a common definer used to describe African American culture (for example, soul music). African Americans were the first peoples in the United States to make fried chicken, along with Scottish immigrants to the South. Although the Scottish had been frying chicken before they emigrated, they lacked the spices and flavor that African Americans had used when preparing the meal. The Scottish American settlers therefore adopted the African American method of seasoning chicken. However, fried chicken was generally a rare meal in the African American community and was usually reserved for special events or celebrations.

African-American English is a variety (dialect, ethnolect, and sociolect) of American English, commonly spoken by urban working-class and largely bi-dialectal middle-class African Americans.

African American English evolved during the antebellum period through interaction between speakers of 16th- and 17th-century English of Great Britain and Ireland and various West African languages. As a result, the variety shares parts of its grammar and phonology with the Southern American English dialect. African American English differs from Standard American English (SAE) in certain pronunciation characteristics, tense usage, and grammatical structures, which were derived from West African languages (particularly those belonging to the Niger–Congo family).

Virtually all habitual speakers of African American English can understand and communicate in Standard American English. As with all linguistic forms, AAVE's usage is influenced by various factors, including geographical, educational and socioeconomic background, as well as formality of setting. Additionally, there are many literary uses of this variety of English, particularly in African American literature.

African-American names are part of the cultural traditions of African Americans, most of these cultural names having no connection to Africa but strictly an African American cultural practice that developed in the United States during enslavement. This new evidence became apparent by census records which show African Americans and White Americans, though they spoke the same language, chose to use different names even during times of enslavement, which is where and when the development of African American cultural names began.

Prior to this newer information, it was only thought that before the 1950s, and 1960s, most African-American names closely resembled those used within European-American culture. Babies of that era were generally given a few common names, with children using nicknames to distinguish the various people with the same name. With the rise of 1960s civil rights movement, there was a dramatic increase in names of various origins.

By the 1970s, and 1980s, it had become common among African Americans to invent new names for themselves, although many of these invented names took elements from popular existing names. Prefixes such as La/Le, Da/De, Ra/Re and Ja/Je, and suffixes like -ique/iqua, -isha and -aun/-awn are common, as are inventive spellings for common names. The book "Baby Names Now: From Classic to Cool—The Very Last Word on First Names" places the origins of "La" names in African-American culture in New Orleans.

Even with the rise of inventive names, it is still common for African Americans to use biblical, historical, or traditional European names. Daniel, Christopher, Michael, David, James, Joseph, and Matthew were thus among the most frequent names for African-American boys in 2013.

The name LaKeisha is typically considered American in origin but has elements that were drawn from both French and West/Central African roots. Names such as LaTanisha, JaMarcus, DeAndre, and Shaniqua were created in the same way. Punctuation marks are seen more often within African American names than other American names, such as the names Mo'nique and D'Andre.

The majority of African Americans are Protestant, many of whom follow the historically Black churches. The term Black church refers to churches which minister to predominantly African American congregations. Black congregations were first established by freed slaves at the end of the 17th century, and later when slavery was abolished more African Americans were allowed to create a unique form of Christianity that was culturally influenced by African spiritual traditions. One of these early African American Christian cultural traditions in the Black Church is the Watchnight service, also called Freedom's Eve, where African American congregations all over the nation come together on New Year's Eve through New Years morning in remembrance of the eve and New Year of their emancipation, sharing testimonies, being baptized and partaking in praise and worship.

According to a 2007 survey, more than half of the African American population are part of the historically Black churches. The largest Protestant denomination among African Americans are the Baptists, distributed mainly in four denominations, the largest being the National Baptist Convention, USA and the National Baptist Convention of America. The second largest are the Methodists, the largest denominations are the African Methodist Episcopal Church and the African Methodist Episcopal Zion Church.

Pentecostals are distributed among several different religious bodies, with the Church of God in Christ as the largest among them by far. About 16% of African American Christians are members of White Protestant communions, these denominations (which include the United Church of Christ) mostly have a 2 to 3% African American membership. There are also large numbers of Catholics, constituting 5% of the African American population. Of the total number of Jehovah's Witnesses, 22% are Black.

Some African Americans follow Islam. Historically, between 15 and 30% of enslaved Africans brought to the Americas were Muslims, but most of these Africans were converted to Christianity during the era of American slavery. During the twentieth century, some African Americans converted to Islam, mainly through the influence of Black nationalist groups that preached with distinctive Islamic practices; including the Moorish Science Temple of America, and the largest organization, the Nation of Islam, founded in the 1930s, which attracted at least 20,000 people by 1963. Prominent members included activist Malcolm X and boxer Muhammad Ali.
Malcolm X is considered the first person to start the movement among African Americans towards mainstream Islam, after he left the Nation and made the pilgrimage to Mecca. In 1975, Warith Deen Mohammed, the son of Elijah Muhammad took control of the Nation after his father's death and guided the majority of its members to orthodox Islam.

African American Muslims constitute 20% of the total U.S. Muslim population, the majority are Sunni or orthodox Muslims, some of these identify under the community of W. Deen Mohammed. The Nation of Islam led by Louis Farrakhan has a membership ranging from 20,000 to 50,000 members.

There is also a small but growing group of African American Jews, making up less than 0.5% of African Americans or about 2% of the Jewish population in the United States. The majority of African-American Jews are Ashkenazi, while smaller numbers identify as Sephardi, Mizrahi, or other. Many African-American Jews are affiliated with denominations such as the Reform, Conservative, Reconstructionist, or Orthodox branches of Judaism, but the majority identify as "Jews of no religion", commonly known as secular Jews. A significant number of people who identify themselves as "Black Jews" are affiliated with syncretic religious groups, largely the Black Hebrew Israelites, whose beliefs include the claim that African Americans are descended from the Biblical Israelites. Jews of all races typically do not accept Black Hebrew Israelites as Jews, in part because they are usually not Jewish according to Jewish law, and in part because these groups are sometimes associated with antisemitism. African-American Jews have criticized the Black Hebrew Israelites, regarding the movement as primarily composed of Black non-Jews who have appropriated Black-Jewish identity.

Confirmed atheists are less than one half of one percent, similar to numbers for Hispanics.

African American music is one of the most pervasive African American cultural influences in the United States today and is among the most dominant in mainstream popular music. Hip hop, R&B, funk, rock and roll, soul, blues, and other contemporary American musical forms originated in Black communities and evolved from other Black forms of music, including blues, doo-wop, barbershop, ragtime, bluegrass, jazz, and gospel music.

African American-derived musical forms have also influenced and been incorporated into virtually every other popular music genre in the world, including country and techno. African American genres are the most important ethnic vernacular tradition in America, as they have developed independent of African traditions from which they arise more so than any other immigrant groups, including Europeans; make up the broadest and longest lasting range of styles in America; and have, historically, been more influential, interculturally, geographically, and economically, than other American vernacular traditions.

African Americans have also had an important role in American dance. Bill T. Jones, a prominent modern choreographer and dancer, has included historical African American themes in his work, particularly in the piece "Last Supper at Uncle Tom's Cabin/The Promised Land". Likewise, Alvin Ailey's artistic work, including his "Revelations" based on his experience growing up as an African American in the South during the 1930s, has had a significant influence on modern dance. Another form of dance, Stepping, is an African American tradition whose performance and competition has been formalized through the traditionally Black fraternities and sororities at universities.

Many African American authors have written stories, poems, and essays influenced by their experiences as African Americans. African American literature is a major genre in American literature. Famous examples include Langston Hughes, James Baldwin, Richard Wright, Zora Neale Hurston, Ralph Ellison, Nobel Prize winner Toni Morrison, and Maya Angelou.

African American inventors have created many widely used devices in the world and have contributed to international innovation. Norbert Rillieux created the technique for converting sugar cane juice into white sugar crystals. Moreover, Rillieux left Louisiana in 1854 and went to France, where he spent ten years working with the Champollions deciphering Egyptian hieroglyphics from the Rosetta Stone. Most slave inventors were nameless, such as the slave owned by the Confederate President Jefferson Davis who designed the ship propeller used by the Confederate navy.

By 1913, over 1,000 inventions were patented by Black Americans. Among the most notable inventors were Jan Matzeliger, who developed the first machine to mass-produce shoes, and Elijah McCoy, who invented automatic lubrication devices for steam engines. Granville Woods had 35 patents to improve electric railway systems, including the first system to allow moving trains to communicate. Garrett A. Morgan developed the first automatic traffic signal and gas mask.

Lewis Howard Latimer invented an improvement for the incandescent light bulb. More recent inventors include Frederick McKinley Jones, who invented the movable refrigeration unit for food transport in trucks and trains. Lloyd Quarterman worked with six other Black scientists on the creation of the atomic bomb (code named the Manhattan Project.) Quarterman also helped develop the first nuclear reactor, which was used in the atomically powered submarine called the Nautilus.

A few other notable examples include the first successful open heart surgery, performed by Daniel Hale Williams, and the air conditioner, patented by Frederick McKinley Jones. Mark Dean holds three of the original nine patents on the computer on which all PCs are based. More current contributors include Otis Boykin, whose inventions included several novel methods for manufacturing electrical components that found use in applications such as guided missile systems and computers, and Colonel Frederick Gregory, who was not only the first Black astronaut pilot but the person who redesigned the cockpits for the last three space shuttles. Gregory was also on the team that pioneered the microwave instrumentation landing system.

As part of the preservation of their culture, African Americans have continuously launched their own publications and publishing houses, such as Robert Sengstacke Abbott, founder of the Chicago Defender newspaper, and Carter G. Woodson, the founder of Black History Month who spent over thirty years documenting and publishing African American history in journals and books. The Johnson Publishing Company, founded by John H. Johnson in 1942, is a National Historic Landmark.

The term "African American", popularized by Jesse Jackson in the 1980s, although it was in regular use as far back for the ethnic group in the 18th and 19th centuries, for example, in post-emancipation holidays and conferences, and carries important social implications. Earlier terms also used to describe Americans of African ancestry referred more to skin color than to ancestry. Other terms (such as "colored", "person of color", or "negro") were included in the wording of various laws and legal decisions which some thought were being used as tools of White supremacy and oppression.

A 16-page pamphlet entitled "A Sermon on the Capture of Lord Cornwallis" is notable for the attribution of its authorship to "An "African American"". Published in 1782, the book's use of this phrase predates any other yet identified by more than 50 years.

In the 1980s, the term "African American" was advanced on the model of, for example, German American or Irish American, to give descendants of American slaves, and other American Blacks who lived through the slavery era, a heritage and a cultural base. The term was popularized in Black communities around the country via word of mouth and ultimately received mainstream use after Jesse Jackson publicly used the term in front of a national audience in 1988. Subsequently, major media outlets adopted its use.

In 2023, the government released a new more detailed breakdown due to the rise in racially Black immigration into the US, listing African American as a compound termed ethnicity, distinguished from other racially Black ethnicities such as Nigerian, Jamaican etc.

Surveys show that the majority of Black Americans have no preference for "African American" versus "Black American", although they have a slight preference for the latter in personal settings and the former in more formal settings.

The term "African American" embraces pan-Africanism as earlier enunciated by prominent African thinkers such as Marcus Garvey, W. E. B. Du Bois, and George Padmore. The term "Afro-Usonian", and variations of such, are more rarely used.

Since 1977, in an attempt to keep up with changing social opinion, the United States government has officially classified Black people (revised to "Black" or "African American" in 1997) as "having origins in any of the black racial groups of Africa." Other federal offices, such as the U.S. Census Bureau, adhere to the Office of Management and Budget standards on race in their data collection and tabulation efforts. In preparation for the 2010 U.S. Census, a marketing and outreach plan called "2010 Census Integrated Communications Campaign Plan" (ICC) recognized and defined African Americans as Black people born in the United States. From the ICC perspective, African Americans are one of three groups of Black people in the United States.

The ICC plan was to reach the three groups by acknowledging that each group has its own sense of community that is based on geography and ethnicity. The best way to market the census process toward any of the three groups is to reach them through their own unique communication channels and not treat the entire Black population of the U.S. as though they are all African Americans with a single ethnic and geographical background. The Federal Bureau of Investigation of the U.S. Department of Justice categorizes Black or African American people as "[a] person having origins in any of the black racial groups of Africa" through racial categories used in the UCR Program adopted from the Statistical Policy Handbook (1978) and published by the Office of Federal Statistical Policy and Standards, U.S. Department of Commerce, derived from the 1977 Office of Management and Budget classification.

Historically, "race mixing" between Black and White people was taboo in the United States. So-called anti-miscegenation laws, barring Blacks and Whites from marrying or having sex, were established in colonial America as early as 1691, and endured in many Southern states until the Supreme Court ruled them unconstitutional in "Loving v. Virginia" (1967). The taboo among American Whites surrounding White-Black relations is a historical consequence of the oppression and racial segregation of African Americans. Historian David Brion Davis notes the racial mixing that occurred during slavery was frequently attributed by the planter class to the "lower-class white males" but Davis concludes that "there is abundant evidence that many slaveowners, sons of slaveowners, and overseers took black mistresses or in effect raped the wives and daughters of slave families." A famous example was Thomas Jefferson's mistress, Sally Hemings. Although publicly opposed to race mixing, Jefferson, in his "Notes on the State of Virginia" published in 1785, wrote: "The improvement of the blacks in body and mind, in the first instance of their mixture with the whites, has been observed by every one, and proves that their inferiority is not the effect merely of their condition of life".

Harvard University historian Henry Louis Gates Jr. wrote in 2009 that "African Americans...are a racially mixed or mulatto people—deeply and overwhelmingly so" (see genetics). After the Emancipation Proclamation, Chinese American men married African American women in high proportions to their total marriage numbers due to few Chinese American women being in the United States. African slaves and their descendants have also had a history of cultural exchange and intermarriage with Native Americans, although they did not necessarily retain social, cultural or linguistic ties to Native peoples. There are also increasing intermarriages and offspring between non-Hispanic Blacks and Hispanics of any race, especially between Puerto Ricans and African Americans (American-born Blacks). According to author M.M. Drymon, many African Americans identify as having Scots-Irish ancestry.

Racially mixed marriages have become increasingly accepted in the United States since the civil rights movement and up to the present day. Approval in national opinion polls has risen from 36% in 1978, to 48% in 1991, 65% in 2002, 77% in 2007. A Gallup poll conducted in 2013 found that 84% of Whites and 96% of Blacks approved of interracial marriage, and 87% overall.

At the end of World War II, some African American military men who had been stationed in Japan married Japanese women, who then immigrated to the United States.

In her book "The End of Blackness", as well as in an essay for "Salon", author Debra Dickerson has argued that the term "Black" should refer strictly to the descendants of Africans who were brought to America as slaves, and not to the sons and daughters of Black immigrants who lack that ancestry. Thus, under her definition, President Barack Obama, who is the son of a Kenyan, is not Black. She makes the argument that grouping all people of African descent together regardless of their unique ancestral circumstances would inevitably deny the lingering effects of slavery within the American community of slave descendants, in addition to denying Black immigrants recognition of their own unique ancestral backgrounds. "Lumping us all together", Dickerson wrote, "erases the significance of slavery and continuing racism while giving the appearance of progress."

Similar viewpoints have been expressed by author Stanley Crouch in a "New York Daily News" piece, Charles Steele Jr. of the Southern Christian Leadership Conference and African American columnist David Ehrenstein of the "Los Angeles Times", who accused White liberals of flocking to Blacks who were "Magic Negros", a term that refers to a Black person with no past who simply appears to assist the mainstream White (as cultural protagonists/drivers) agenda. Ehrenstein went on to say "He's there to assuage white 'guilt' they feel over the role of slavery and racial segregation in American history."

The American Descendants of Slavery (ADOS) movement coalesces around this view, arguing that Black descendants of American slavery deserve a separate ethnic category that distinguishes them from other Black groups in the United States. Their terminology has gained popularity in some circles, but others have criticized the movement for a perceived bias against (especially poor and Black) immigrants, and for its often inflammatory rhetoric. Politicians such as Obama and Harris have received especially pointed criticism from the movement, as neither are ADOS and have spoken out at times against policies specific to them.

Many Pan-African movements and organizations that are ideologically Black nationalist, anti-imperialist, anti-Zionist, and Scientific socialist like The All-African People's Revolutionary Party (A-APRP), have argued that African (relating to the diaspora) or New Afrikan should be used instead of African American. Most notably, Malcolm X and Kwame Ture expressed similar views that African Americans are Africans who "happen to be in America", and should not claim or identify as being American if they are fighting for Black (New Afrikan) liberation. Historically, this is due to the enslavement of Africans during the Trans-Atlantic slave trade, ongoing anti-black violence, and structural racism in countries like the United States.

Before the independence of the Thirteen Colonies until the abolition of slavery in 1865, an African American slave was commonly known as a "negro". "Free negro" was the legal status in the territory of an African American person who was not enslaved. In response to the project of the American Colonization Society to transport free Blacks to the future Liberia, a project most Blacks strongly rejected, the Blacks at the time said they were no more African than White Americans were European, and referred to themselves with what they considered a more acceptable term, "colored Americans". The term was used until the second quarter of the 20th century, when it was considered outmoded and generally gave way again to the exclusive use of "negro". By the 1940s, the term was commonly capitalized ("Negro"); but by the mid-1960s, it was considered disparaging. By the end of the 20th century, "negro" had come to be considered inappropriate and was rarely used and perceived as a pejorative. The term is rarely used by younger Black people, but remained in use by many older African Americans who had grown up with the term, particularly in the southern U.S. The term remains in use in some contexts, such as the United Negro College Fund, an American philanthropic organization that funds scholarships for Black students and general scholarship funds for 39 private historically Black colleges and universities.

There are many other deliberately insulting terms, many of which were in common use (e.g., "nigger"), but had become unacceptable in normal discourse before the end of the 20th century. One exception is the use, among the Black community, of the slur "nigger" rendered as "nigga", representing the pronunciation of the word in African American English. This usage has been popularized by American rap and hip-hop music cultures and is used as part of an in-group lexicon and speech. It is not necessarily derogatory and, when used among Black people, the word is often used to mean "homie" or "friend".

Acceptance of intra-group usage of the word "nigga" is still debated, although it has established a foothold among younger generations. The NAACP denounces the use of both "nigga" and "nigger". Mixed-race usage of "nigga" is still considered taboo, particularly if the speaker is White. However, trends indicate that usage of the term in intragroup settings is increasing even among White youth due to the popularity of rap and hip hop culture.


Artistic License

The Artistic License is an open-source license used for certain free and open-source software packages, most notably the standard implementation of the Perl programming language and most CPAN modules, which are dual-licensed under the Artistic License and the GNU General Public License (GPL).

The original Artistic License was written by Larry Wall. The name of the license is a reference to the concept of artistic license.

Whether or not the original Artistic License is a free software license is largely unsettled. The Free Software Foundation explicitly called the original Artistic License a non-free license, criticizing it as being "too vague; some passages are too clever for their own good, and their meaning is not clear". The FSF recommended that the license not be used on its own, but approved the common AL/GPL dual-licensing approach for Perl projects.

In response to this, Bradley Kuhn, who later worked for the Free Software Foundation, made a minimal redraft to clarify the ambiguous passages. This was released as the Clarified Artistic License and was approved by the FSF. It is used by the Paros Proxy, the JavaFBP toolkit and NcFTP.

The terms of the Artistic License 1.0 were at issue in Jacobsen v. Katzer in the initial 2009 ruling by the United States District Court for the Northern District of California declared that FOSS-like licenses could only be enforced through contract law rather than through copyright law, in contexts where contract damages would be difficult to establish. On appeal, a federal appellate court "determined that the terms of the Artistic License are enforceable copyright conditions". The case was remanded to the District Court, which did not apply the superior court's criteria on the grounds that, in the interim, the governing Supreme Court precedent applicable to the case had changed. However, this left undisturbed the finding that a free and open-source license nonetheless has economic value. Jacobsen ultimately prevailed in 2010, and the Case established a new standard making terms and conditions under Artistic License 1.0 enforceable through copyright statutes and relevant precedents.

In response to the Request for Comments (RFC) process for improving the licensing position for Perl 6, Kuhn's draft was extensively rewritten by Roberta Cairney and Allison Randal for readability and legal clarity, with input from the Perl community. This resulted in the Artistic License 2.0, which has been approved as both a free software and open source license.

The Artistic license 2.0 is also notable for its excellent license compatibility with other FOSS licenses due to a relicensing clause, a property other licenses like the GPL lack.

It has been adopted by some of the Perl 6 implementations, the Mojolicious framework, NPM, and has been used by the Parrot virtual machine since version 0.4.13. It is also used by the SNEeSe emulator, which was formerly licensed under the Clarified Artistic License.

The OSI recommends that all developers and projects licensing their products with the Artistic License adopt Artistic License 2.0.


Afrikaans

Afrikaans ( , ) is a West Germanic language, spoken in South Africa, Namibia and (to a lesser extent) Botswana, Zambia and Zimbabwe. Estimates of the number of Afrikaans speakers range between 15 and 23 million. It evolved in the Dutch colonization in southern Africa from the Dutch vernacular used by Dutch settlers and people enslaved by them. Afrikaans gradually began to develop distinguishing characteristics during the course of the 18th century. 

Afrikaans is estimated to have 95% of the vocabulary of standard Dutch, with adopted words from other languages, including the Khoisan languages of Southern Africa. Differences with Dutch include a more analytic-type morphology and grammar, and some pronunciations. There is a large degree of mutual intelligibility between the two languages, especially in written form.

The name of the language comes directly from the Dutch word (now spelled ) meaning "African". It was previously referred to as "Cape Dutch" ("Kaap-Hollands"/"Kaap-Nederlands)," a term also used to refer to the early Cape settlers collectively, or the derogatory "kitchen Dutch" ("kombuistaal") from its use by slaves of colonial settlers "in the kitchen".

The Afrikaans language arose in the Dutch Cape Colony, through a gradual divergence from European Dutch dialects, during the course of the 18th century. As early as the mid-18th century and as recently as the mid-20th century, Afrikaans was known in standard Dutch as a "kitchen language" (), lacking the prestige accorded, for example, even by the educational system in Africa, to languages spoken outside Africa. Other early epithets setting apart ' ("Cape Dutch", i.e. Afrikaans) as putatively beneath official Dutch standards included ', ' and ' ("mutilated/broken/uncivilised Dutch"), as well as " ("incorrect Dutch").

Den Besten theorises that modern Standard Afrikaans derives from two sources:

Thus in his view Afrikaans is neither a creole nor a direct descendant of Dutch, but a fusion of two transmission pathways.

Most of the first settlers whose descendants today are the Afrikaners were from the United Provinces (now Netherlands), with up to one-sixth of the community of French Huguenot origin, and a seventh from Germany.

African and Asian workers, Cape Coloured children of European settlers and Khoikhoi women, and slaves contributed to the development of Afrikaans. The slave population was made up of people from East Africa, West Africa, India, Madagascar, and the Dutch East Indies (modern Indonesia). A number were also indigenous Khoisan people, who were valued as interpreters, domestic servants, and labourers. Many free and enslaved women married or cohabited with the male Dutch settlers. M. F. Valkhoff argued that 75% of children born to female slaves in the Dutch Cape Colony between 1652 and 1672 had a Dutch father. Sarah Grey Thomason and Terrence Kaufman argue that Afrikaans' development as a separate language was "heavily conditioned by nonwhites who learned Dutch imperfectly as a second language."

Beginning in about 1815, Afrikaans started to replace Malay as the language of instruction in Muslim schools in South Africa, written with the Arabic alphabet: see Arabic Afrikaans. Later, Afrikaans, now written with the Latin script, started to appear in newspapers and political and religious works in around 1850 (alongside the already established Dutch).

In 1875, a group of Afrikaans-speakers from the Cape formed the " ("Society for Real Afrikaners"), and published a number of books in Afrikaans including grammars, dictionaries, religious materials and histories.

Until the early 20th century, Afrikaans was considered a Dutch dialect, alongside Standard Dutch, which it eventually replaced as an official language. Before the Boer wars, "and indeed for some time afterwards, Afrikaans was regarded as inappropriate for educated discourse. Rather, Afrikaans was described derogatorily as 'a kitchen language' or 'a bastard jargon', suitable for communication mainly between the Boers and their servants."

In 1925, Afrikaans was recognised by the South African government as a distinct language, rather than simply a vernacular of Dutch. On 8 May 1925, twenty-three years after the Second Boer War ended, the Official Languages of the Union Act of 1925 was passed—mostly due to the efforts of the Afrikaans-language movement—at a joint sitting of the House of Assembly and the Senate, in which the Afrikaans language was declared a variety of Dutch. The Constitution of 1961 reversed the position of Afrikaans and Dutch, so that English and Afrikaans were the official languages, and Afrikaans was deemed to include Dutch. The Constitution of 1983 removed any mention of Dutch altogether.

The Afrikaans Language Monument is located on a hill overlooking Paarl in the Western Cape Province. Officially opened on 10 October 1975, it was erected on the 100th anniversary of the founding of the Society of Real Afrikaners, and the 50th anniversary of Afrikaans being declared an official language of South Africa in distinction to Dutch.

The earliest Afrikaans texts were some doggerel verse from 1795 and a dialogue transcribed by a Dutch traveller in 1825. Afrikaans used the Latin alphabet around this time, although the Cape Muslim community used the Arabic script. In 1861, L.H. Meurant published his " ("Conversation between Nicholas Truthsayer and John Doubter"), which is considered to be the first book published in Afrikaans.

The first grammar book was published in 1876; a bilingual dictionary was later published in 1902. The main modern Afrikaans dictionary in use is the ' (HAT). A new authoritative dictionary, called ' (WAT), was under development as of 2018. The official orthography of Afrikaans is the ", compiled by .

The Afrikaners primarily were Protestants, of the Dutch Reformed Church of the 17th century. Their religious practices were later influenced in South Africa by British ministries during the 1800s. A landmark in the development of the language was the translation of the whole Bible into Afrikaans. While significant advances had been made in the textual criticism of the Bible, especially the Greek New Testament, the 1933 translation followed the Textus Receptus and was closely akin to the . Before this, most Cape Dutch-Afrikaans speakers had to rely on the Dutch . This had its origins with the Synod of Dordrecht of 1618 and was thus in an archaic form of Dutch. This was hard for Dutch speakers to understand, and increasingly unintelligible for Afrikaans speakers.

C. P. Hoogehout, , and Stephanus Jacobus du Toit were the first Afrikaans Bible translators. Important landmarks in the translation of the Scriptures were in 1878 with C. P. Hoogehout's translation of the " (Gospel of Mark, lit. Gospel according to Mark); however, this translation was never published. The manuscript is to be found in the South African National Library, Cape Town.

The first official translation of the entire Bible into Afrikaans was in 1933 by J. D. du Toit, E. E. van Rooyen, J. D. Kestell, H. C. M. Fourie, and BB Keet. This monumental work established Afrikaans as ", that is "a pure and proper language" for religious purposes, especially amongst the deeply Calvinist Afrikaans religious community that previously had been sceptical of a Bible translation that varied from the Dutch version that they were used to.

In 1983, a fresh translation marked the 50th anniversary of the 1933 version. The final editing of this edition was done by E. P. Groenewald, A. H. van Zyl, P. A. Verhoef, J. L. Helberg and W. Kempen. This translation was influenced by Eugene Nida's theory of dynamic equivalence which focused on finding the nearest equivalent in the receptor language to the idea that the Greek, Hebrew or Aramaic wanted to convey.

A new translation, "Die Bybel: 'n Direkte Vertaling" was released in November 2020. It is the first truly ecumenical translation of the Bible in Afrikaans as translators from various churches, including the Roman Catholic and Anglican Churches, were involved.


Afrikaans descended from Dutch dialects in the 17th century. It belongs to a West Germanic sub-group, the Low Franconian languages. Other West Germanic languages related to Afrikaans are German, English, the Frisian languages, and the unstandardised languages Low German and Yiddish.

Afrikaans is also widely spoken in Namibia. Before independence, Afrikaans had equal status with German as an official language. Since independence in 1990, Afrikaans has had constitutional recognition as a national, but not official, language. There is a much smaller number of Afrikaans speakers among Zimbabwe's white minority, as most have left the country since 1980. Afrikaans was also a medium of instruction for schools in Bophuthatswana, an Apartheid-era Bantustan. Eldoret in Kenya was founded by Afrikaners.

In 1976, secondary-school pupils in Soweto began a rebellion in response to the government's decision that Afrikaans be used as the language of instruction for half the subjects taught in non-White schools (with English continuing for the other half). Although English is the mother tongue of only 8.2% of the population, it is the language most widely understood, and the second language of a majority of South Africans. Afrikaans is more widely spoken than English in the Northern and Western Cape provinces, several hundred kilometres from Soweto. The Black community's opposition to Afrikaans and preference for continuing English instruction was underlined when the government rescinded the policy one month after the uprising: 96% of Black schools chose English (over Afrikaans or native languages) as the language of instruction. Afrikaans-medium schools were also accused of using language policy to deter black African parents. Some of these parents, in part supported by provincial departments of education, initiated litigation which enabled enrolment with English as language of instruction. By 2006 there were 300 single-medium Afrikaans schools, compared to 2,500 in 1994, after most converted to dual-medium education. Due to Afrikaans being viewed as the "language of the white oppressor" by some, pressure has been increased to remove Afrikaans as a teaching language in South African universities, resulting in bloody student protests in 2015.

Under South Africa's Constitution of 1996, Afrikaans remains an official language, and has equal status to English and nine other languages. The new policy means that the use of Afrikaans is now often reduced in favour of English, or to accommodate the other official languages. In 1996, for example, the South African Broadcasting Corporation reduced the amount of television airtime in Afrikaans, while South African Airways dropped its Afrikaans name " from its livery. Similarly, South Africa's diplomatic missions overseas now display the name of the country only in English and their host country's language, and not in Afrikaans. Meanwhile, the constitution of the Western Cape, which went into effect in 1998, declares Afrikaans to be an official language of the province alongside English and Xhosa.

The Afrikaans-language general-interest family magazine " has the largest readership of any magazine in the country. 

When the British design magazine "Wallpaper" described Afrikaans as "one of the world's ugliest languages" in its September 2005 article about the monument, South African billionaire Johann Rupert (chairman of the Richemont Group), responded by withdrawing advertising for brands such as Cartier, Van Cleef & Arpels, Montblanc and Alfred Dunhill from the magazine. The author of the article, Bronwyn Davies, was an English-speaking South African.

An estimated 90 to 95% of the Afrikaans lexicon is ultimately of Dutch origin, and there are few lexical differences between the two languages. Afrikaans has a considerably more regular morphology, grammar, and spelling. There is a high degree of mutual intelligibility between the two languages, particularly in written form.

Afrikaans acquired some lexical and syntactical borrowings from other languages such as Malay, Khoisan languages, Portuguese, and Bantu languages, and Afrikaans has also been significantly influenced by South African English. Dutch speakers are confronted with fewer non-cognates when listening to Afrikaans than the other way round. Mutual intelligibility thus tends to be asymmetrical, as it is easier for Dutch speakers to understand Afrikaans than for Afrikaans speakers to understand Dutch.

In general, mutual intelligibility between Dutch and Afrikaans is far better than between Dutch and Frisian or between Danish and Swedish. The South African poet writer Breyten Breytenbach, attempting to visualise the language distance for Anglophones once remarked that the differences between (Standard) Dutch and Afrikaans are comparable to those between the Received Pronunciation and Southern American English.

Post-apartheid South Africa has seen a loss of preferential treatment by the government for Afrikaans, in terms of education, social events, media (TV and radio), and general status throughout the country, given that it now shares its place as official language with ten other languages. Nevertheless, Afrikaans remains more prevalent in the media – radio, newspapers and television – than any of the other official languages, except English. More than 300 book titles in Afrikaans are published annually. South African census figures suggest a growing number of speakers in all nine provinces, a total of 6.85 million in 2011 compared to 5.98 million a decade earlier. The South African Institute of Race Relations (SAIRR) projects that a growing majority will be Coloured Afrikaans speakers. Afrikaans speakers experience higher employment rates than other South African language groups, though half a million were unemployed.

Despite the challenges of demotion and emigration that it faces in South Africa, the Afrikaans vernacular remains competitive, being popular in DSTV pay channels and several internet sites, while generating high newspaper and music CD sales. A resurgence in Afrikaans popular music since the late 1990s has invigorated the language, especially among a younger generation of South Africans. A recent trend is the increased availability of pre-school educational CDs and DVDs. Such media also prove popular with the extensive Afrikaans-speaking emigrant communities who seek to retain language proficiency in a household context.

Afrikaans-language cinema showed signs of new vigour in the early 21st century. The 2007 film , the first full-length Afrikaans movie since in 1998, is seen as the dawn of a new era in Afrikaans cinema. Several short films have been created and more feature-length movies, such as and (both in 2008) have been produced, besides the 2011 Afrikaans-language film , which was the first Afrikaans film to screen at the Cannes Film Festival. The film was also released in 2011. The Afrikaans film industry started gaining international recognition via the likes of big Afrikaans Hollywood film stars, like Charlize Theron ("Monster") and Sharlto Copley ("District 9") promoting their mother tongue.

SABC3 announced early in 2009 that it would increase Afrikaans programming due to the "growing Afrikaans-language market and [their] need for working capital as Afrikaans advertising is the only advertising that sells in the current South African television market". In April 2009, SABC3 started screening several Afrikaans-language programmes. There is a groundswell movement within Afrikaans to be inclusive, and to promote itself along with the other indigenous official languages. In Namibia, the percentage of Afrikaans speakers declined from 11.4% (2001 Census) to 10.4% (2011 Census). The major concentrations are in Hardap (41.0%), ǁKaras (36.1%), Erongo (20.5%), Khomas (18.5%), Omaheke (10.0%), Otjozondjupa (9.4%), Kunene (4.2%), and Oshikoto (2.3%).

Many native speakers of Bantu languages and English also speak Afrikaans as a second language. It is widely taught in South African schools, with about 10.3 million second-language students. Even in KwaZulu-Natal (where there are relatively few Afrikaans home-speakers), the majority of pupils opt for Afrikaans as their first additional language because it is regarded as easier than Zulu.

Afrikaans is offered at many universities outside South Africa, including in the Netherlands, Belgium, Germany, Poland, Russia and the United States.

In Afrikaans grammar, there is no distinction between the infinitive and present forms of verbs, with the exception of the verbs 'to be' and 'to have':
In addition, verbs do not conjugate differently depending on the subject. For example,
Only a handful of Afrikaans verbs have a preterite, namely the auxiliary ' ("to be"), the modal verbs, and the verb ' ("to think"). The preterite of "" ("may") is rare in contemporary Afrikaans.

All other verbs use the perfect tense, het + past participle (ge-), for the past. Therefore, there is no distinction in Afrikaans between "I drank" and "I have drunk". (In colloquial German, the past tense is also often replaced with the perfect.)

When telling a longer story, Afrikaans speakers usually avoid the perfect and simply use the present tense, or historical present tense instead (as is possible, but less common, in English as well).

A particular feature of Afrikaans is its use of the double negative; it is classified in Afrikaans as "" and is something that is absent from the other West Germanic standard languages. For example,

Both French and San origins have been suggested for double negation in Afrikaans. While double negation is still found in Low Franconian dialects in West Flanders and in some "isolated" villages in the centre of the Netherlands (such as Garderen), it takes a different form, which is not found in Afrikaans. The following is an example:

The ' was the Middle Dutch way to negate but it has been suggested that since ' became highly non-voiced, or was needed to complement the '. With time the ' disappeared in most Dutch dialects.

The double negative construction has been fully grammaticalised in standard Afrikaans and its proper use follows a set of fairly complex rules as the examples below show:

A notable exception to this is the use of the negating grammar form that coincides with negating the English present participle. In this case there is only a single negation.

Certain words in Afrikaans would be contracted. For example, ', which literally means "must not", usually becomes '; although one does not have to write or say it like this, virtually all Afrikaans speakers will change the two words to " in the same way as "do not" is contracted to "don't" in English.

The Dutch word ' ("it" in English) does not correspond to ' in Afrikaans. The Dutch words corresponding to Afrikaans ' are ', ', ' and ".




Following early dialectal studies of Afrikaans, it was theorised that three main historical dialects probably existed after the Great Trek in the 1830s. These dialects are the Northern Cape, Western Cape, and Eastern Cape dialects. Northern Cape dialect may have resulted from contact between Dutch settlers and the Khoekhoe people between the Great Karoo and the Kunene, and Eastern Cape dialect between the Dutch and the Xhosa. Remnants of these dialects still remain in present-day Afrikaans, although the standardising effect of Standard Afrikaans has contributed to a great levelling of differences in modern times.

There is also a prison cant, known as Sabela, which is based on Afrikaans, yet heavily influenced by Zulu. This language is used as a secret language in prison and is taught to initiates.

A distinct dialect of Afrikaans is spoken by the 650-strong South African community of Argentina, in the region of Patagonia.

Due to the early settlement of a Cape Malay community in Cape Town, who are now known as Coloureds, numerous Classical Malay words were brought into Afrikaans. Some of these words entered Dutch via people arriving from what is now known as Indonesia as part of their colonial heritage. Malay words in Afrikaans include:

Some words originally came from Portuguese such as ' ("umbrella") from the Portuguese ', ' ("pen/cattle enclosure") from the Portuguese ' and ' ("corn", from '). Some of these words also exist in Dutch, like " "parasol", though usage is less common and meanings can slightly differ.


Some of these words also exist in Dutch, though with a more specific meaning: ' for example means "South-African tribal javelin" and ' means "South-African tribal blanket of animal hides".

Loanwords from Bantu languages in Afrikaans include the names of indigenous birds, such as ' and ', and indigenous plants, such as ' and '.

The revoking of the Edict of Nantes on 22 October 1685 was a milestone in the history of South Africa, for it marked the beginning of the great Huguenot exodus from France. It is estimated that between 250,000 and 300,000 Protestants left France between 1685 and 1700; out of these, according to Louvois, 100,000 had received military training. A measure of the calibre of these immigrants and of their acceptance by host countries (in particular South Africa) is given by H. V. Morton in his book: "In Search of South Africa" (London, 1948). The Huguenots were responsible for a great linguistic contribution to Afrikaans, particularly in terms of military terminology as many of them fought on the battlefields during the wars of the Great Trek.

Most of the words in this list are descendants from Dutch borrowings from French, Old French or Latin, and are not direct influences from French on Afrikaans.

The Afrikaans writing system is based on Dutch, using the 26 letters of the ISO basic Latin alphabet, plus 16 additional vowels with diacritics. The hyphen (e.g. in a compound like "see-eend" 'sea duck'), apostrophe (e.g. "ma's" 'mothers'), and a whitespace character (e.g. in multi-word units like "Dooie See" 'Dead Sea') is part of the orthography of words, while the indefinite article "ŉ" is a ligature. All the alphabet letters, including those with diacritics, have capital letters as allographs; the "ŉ" does not have a capital letter allograph. This means that Afrikaans has 88 graphemes with allographs in total.

In Afrikaans, many consonants are dropped from the earlier Dutch spelling. For example, ' ('only') in Dutch becomes ' in Afrikaans. Also, Afrikaans and some Dutch dialects make no distinction between and , having merged the latter into the former; while the word for "south" is written ' in Dutch, it is spelled ' in Afrikaans (as well as dialectal Dutch writings) to represent this merger. Similarly, the Dutch digraph ', normally pronounced as , corresponds to Afrikaans ', except where it replaces the Dutch suffix "" which is pronounced as , as in " > ".

Another difference is the indefinite article, ' in Afrikaans and in Dutch. "A book" is ' in Afrikaans, whereas it is either ' or ' in Dutch. This " is usually pronounced as just a weak vowel, , just like English "a".

The diminutive suffix in Afrikaans is ', ' or ', whereas in Dutch it is ' or ", hence a "bit" is ŉ in Afrikaans and in Dutch.

The letters "c", "q", "x", and "z" occur almost exclusively in borrowings from French, English, Greek and Latin. This is usually because words that had "c" and "ch" in the original Dutch are spelled with "k" and "g", respectively, in Afrikaans. Similarly original "qu" and "x" are most often spelt "kw" and "ks", respectively. For example, ' instead of "equatoriaal", and ' instead of "excuus".

The vowels with diacritics in non-loanword Afrikaans are: "á", "ä", "é", "è", "ê", "ë", "í", "î", "ï", "ó", "ô", "ö", "ú", "û", "ü", "ý". Diacritics are ignored when alphabetising, though they are still important, even when typing the diacritic forms may be difficult. For example, ' ("ate") instead of the 3 e's alongside each other: "*", which can never occur in Afrikaans, or ', which translates to "say", whereas ' is a possessive form. The acute's ("á", "é", "í", "ó", "ú, ý)" primary function is to place emphasis on a word (i.e. for emphatic reasons), by adding it to the emphasised syllable of the word. For example, "sál" ("will" (verb)), "néé" ('no'), "móét" ("must"), "hý" ("he"), "gewéét" ("knew"). The acute is only placed on the "i" if it is the only vowel in the emphasised word: "wil" ('want' (verb)) becomes "wíl", but "lui" ('lazy') becomes "lúi." Only a few non-loan words are spelled with acutes, e.g. "dié" ('this'), "ná" ('after'), "óf ... óf" ('either ... or'), "nóg ... nóg" ('neither ... nor'), etc. Only four non-loan words are spelled with the grave: ' ('yes?', 'right?', 'eh?'), "" ('here, take this!' or '[this is] yours!'), "hè" ('huh?', 'what?', 'eh?'), and "appèl" ('(formal) appeal' (noun)).

A few short words in Afrikaans take initial apostrophes. In modern Afrikaans, these words are always written in lower case (except if the entire line is uppercase), and if they occur at the beginning of a sentence, the next word is capitalised. Three examples of such apostrophed words are '. The last (the indefinite article) is the only apostrophed word that is common in modern written Afrikaans, since the other examples are shortened versions of other words (' and "", respectively) and are rarely found outside of a poetic context.

Here are a few examples:
The apostrophe and the following letter are regarded as two separate characters, and are never written using a single glyph, although a single character variant of the indefinite article appears in Unicode, .

For more on the pronunciation of the letters below, see "".

Psalm 23 1983 translation:

<poem style="margin-left: 1em; font-style: italic;" lang="af">
Die Here is my Herder, ek kom niks kort nie.
Hy laat my rus in groen weivelde. Hy bring my by waters waar daar vrede is.
Hy gee my nuwe krag. Hy lei my op die regte paaie tot eer van Sy naam.
Selfs al gaan ek deur donker dieptes, sal ek nie bang wees nie, want U is by my. In U hande is ek veilig.
</poem>

Psalm 23 1953 translation:

<poem style="margin-left: 1em; font-style: italic;" lang="af">
Die Here is my Herder, niks sal my ontbreek nie.
Hy laat my neerlê in groen weivelde; na waters waar rus is, lei Hy my heen.
Hy verkwik my siel; Hy lei my in die spore van geregtigheid, om sy Naam ontwil.
Al gaan ek ook in 'n dal van doodskaduwee, ek sal geen onheil vrees nie; want U is met my: u stok en u staf die vertroos my.
</poem>

Lord's Prayer (Afrikaans New Living translation)

<poem style="margin-left: 1em; font-style: italic;" lang="af">
Ons Vader in die hemel, laat U Naam geheilig word.
Laat U koningsheerskappy spoedig kom.
Laat U wil hier op aarde uitgevoer word soos in die hemel.
Gee ons die porsie brood wat ons vir vandag nodig het.
En vergeef ons ons sondeskuld soos ons ook óns skuldenaars vergewe het.
Bewaar ons sodat ons nie aan verleiding sal toegee nie; en bevry ons van die greep van die bose.
Want van U is die koninkryk,
en die krag,
en die heerlikheid,
tot in ewigheid.
Amen
</poem>

Lord's Prayer (Original translation):

<poem style="margin-left: 1em; font-style: italic;" lang="af">
Onse Vader wat in die hemel is,
laat U Naam geheilig word;
laat U koninkryk kom;
laat U wil geskied op die aarde,
net soos in die hemel.
Gee ons vandag ons daaglikse brood;
en vergeef ons ons skulde
soos ons ons skuldenaars vergewe
en laat ons nie in die versoeking nie
maar verlos ons van die bose
Want aan U behoort die koninkryk
en die krag
en die heerlikheid
tot in ewigheid.
Amen
</poem>





Aeolus

In Greek mythology, Aeolus or Aiolos (; , ) is a name shared by three mythical characters. These three personages are often difficult to tell apart, and even the ancient mythographers appear to have been perplexed about which Aeolus was which. Diodorus Siculus made an attempt to define each of these three (although it is clear that he also became muddled), and his opinion is followed here. 

All three men named Aeolus appear to be connected genealogically, although the precise relationship, especially regarding the second and third Aeolus, is often ambiguous as their identities seem to have been merged by many ancient writers.

Aeolus was also the name of the following minor characters:



ABC

ABC are the first three letters of the Latin script.

ABC or abc may also refer to:




























Alford plea

In United States law, an Alford plea, also called a Kennedy plea in West Virginia, an Alford guilty plea, and the Alford doctrine, is a guilty plea in criminal court, whereby a defendant in a criminal case does not admit to the criminal act and asserts innocence, even if the evidence presented by the prosecution would be likely to persuade a judge or jury to find the defendant guilty beyond a reasonable doubt. This can be caused by circumstantial evidence and testimony favoring the prosecution and difficulty finding evidence and witnesses that would aid the defense.

Alford pleas are legally permissible in nearly all U.S. federal and state courts, except in the state courts of Indiana, Michigan, and New Jersey, or in the courts of the United States Armed Forces.

The "Alford" guilty plea is named after the United States Supreme Court case of "North Carolina v. Alford" (1970). Henry Alford had been indicted on a charge of first-degree murder in 1963. Evidence in the case included testimony from witnesses that Alford himself had said, after the victim's death, that he had killed the individual. Court testimony showed that Alford and the victim had argued at the victim's house. Alford left the house, and afterwards the victim received a fatal gunshot wound when he opened the door responding to a knock.

Alford was faced with the possibility of capital punishment if convicted by a jury trial. The death penalty was the default sentence by North Carolina law at the time, if two requisites in the case were satisfied: the defendant had to have pleaded not guilty, and the jury did not instead recommend a life sentence. Had he pleaded guilty to first-degree murder, Alford would have had the possibility of a life sentence and would have avoided the death penalty, but he did not want to admit guilt. Nonetheless, Alford pleaded guilty to second-degree murder and said he was doing so to avoid a death sentence, were he to be convicted of first-degree murder, after attempting to contest that charge. Alford was sentenced to 30 years in prison after the trial judge accepted the plea bargain and ruled that the defendant had been adequately advised by his defense lawyer.

Alford appealed and requested a new trial, arguing he was forced into a guilty plea because he was afraid of receiving a death sentence. The Supreme Court of North Carolina ruled that the defendant had voluntarily entered the guilty plea with knowledge of what that meant. Following this ruling, Alford petitioned for a writ of "habeas corpus" in the United States District Court for the Middle District of North Carolina, which upheld the initial ruling, and subsequently to the United States Court of Appeals for the Fourth Circuit, which ruled that Alford's plea was not voluntary, because it was made under fear of the death penalty. "I just pleaded guilty because they said if I didn't, they would gas me for it," wrote Alford in one of his appeals.
The case was then appealed to the U.S. Supreme Court. Supreme Court Justice Byron White wrote the majority decision, which held that for the plea to be accepted, the defendant must have been advised by a competent lawyer who was able to inform the individual that his best decision in the case would be to enter a guilty plea. The Court ruled that the defendant can enter such a plea "when he concludes that his interests require a guilty plea and the record strongly indicates guilt." The Court allowed the guilty plea with a simultaneous protestation of innocence only because there was enough evidence to show that the prosecution had a strong case for a conviction and the defendant was entering such a plea to avoid this possible sentencing. The Court went on to note that even if the defendant could have shown that he would not have entered a guilty plea "but for" the rationale of receiving a lesser sentence, the plea itself would not have been ruled invalid. As evidence existed that could have supported Alford's conviction, the Supreme Court held that his guilty plea was allowable while the defendant himself still maintained that he was not guilty.

Alford died in prison in 1975.

The "Dictionary of Politics: Selected American and Foreign Political and Legal Terms" defines the term "Alford plea" as: "A plea under which a defendant may choose to plead guilty, not because of an admission to the crime, but because the prosecutor has sufficient evidence to place a charge and to obtain conviction in court. The plea is commonly used in local and state courts in the United States." According to "University of Richmond Law Review", "When offering an Alford plea, a defendant asserts his innocence but admits that sufficient evidence exists to convict him of the offense." "A Guide to Military Criminal Law" notes that under the Alford plea, "the defendant concedes that the prosecution has enough evidence to convict, but the defendant still refuses to admit guilt." The book "Plea Bargaining's Triumph: A History of Plea Bargaining in America" published by Stanford University Press defines the plea as one in "which the defendant adheres to his/her claim of innocence even while allowing that the government has enough evidence to prove his/her guilt beyond a reasonable doubt". According to the book "Gender, Crime, and Punishment" published by Yale University Press, "Under the Alford doctrine, a defendant does not admit guilt but admits that the state has sufficient evidence to find him or her guilty, should the case go to trial." "Webster's New World Law Dictionary" defines Alford plea as: "A guilty plea entered as part of a plea bargain by a criminal defendant who denies committing the crime or who does not actually admit his guilt. In federal courts, such plea may be accepted as long as there is evidence that the defendant is actually guilty."

The Alford guilty plea is "a plea of guilty containing a protestation of innocence". The defendant pleads guilty, but does not have to specifically admit to the guilt itself. The defendant maintains a claim of innocence, but agrees to the entry of a conviction in the charged crime. Upon receiving an Alford guilty plea from a defendant, the court may immediately pronounce the defendant guilty and impose sentence as if the defendant had otherwise been convicted of the crime. Sources disagree, as may differing states' laws, as to what category of plea the "Alford" plea falls under: Some sources state that the Alford guilty plea is a form of "nolo contendere", where the defendant in the case states "no contest" to the factual matter of the case as given in the charges outlined by the prosecution. Others hold that an "Alford" plea is simply one form of a guilty plea, and, as with other guilty pleas, the judge must see there is some factual basis for the plea.

Defendants can take advantage of the ability to use the Alford guilty plea, by admitting there is enough evidence to convict them of a higher crime, while at the same time pleading guilty to a lesser charge. Defendants usually enter an Alford guilty plea if they want to avoid a possible worse sentence were they to lose the case against them at trial. It affords defendants the ability to accept a plea bargain, while maintaining innocence.

This form of guilty plea has been frequently used in local and state courts in the United States, though it constitutes a small percentage of all plea bargains in the U.S. This form of plea is not allowed in courts of the United States military. In 2000, the United States Department of Justice noted, "In an Alford plea the defendant agrees to plead guilty because he or she realizes that there is little chance to win acquittal because of the strong evidence of guilt. About 17% of State inmates and 5% of Federal inmates submitted either an Alford plea or a no contest plea, regardless of the type of attorney. This difference reflects the relative readiness of State courts, compared to Federal courts, to accept an alternative plea."

In the 1995 case "State of Idaho v. Howry" before the Idaho Court of Appeals, the Court commented on the impact of the Alford guilty plea on later sentencing. The Court ruled, "Although an Alford plea allows a defendant to plead guilty amid assertions of innocence, it does not require a court to accept those assertions. The sentencing court may, of necessity, consider a broad range of information, including the evidence of the crime, the defendant's criminal history and the demeanor of the defendant, including the presence or absence of remorse." In the 1999 South Carolina Supreme Court case "State v. Gaines", the Court held that Alford guilty pleas were to be held valid even in the absence of a specific on-the-record ruling that the pleas were voluntary – provided that the sentencing judge acted appropriately in accordance with the rules for acceptance of a plea made voluntarily by the defendant. The Court held that a ruling that the plea was entered into voluntarily is implied by the act of sentencing.
In the 2006 case before the United States Court of Appeals for the Fifth Circuit, "Ballard v. Burton", Judge Carl E. Stewart writing for the Court held that an Alford guilty plea is a "variation of an ordinary guilty plea". In October 2008, the United States Department of Justice defined an Alford plea as: "the defendant maintains his or her innocence with respect to the charge to which he or she offers to plead guilty".

In March 2009, the Minnesota House of Representatives characterized the Alford plea as: "a form of a guilty plea in which the defendant asserts innocence but acknowledges on the record that the prosecutor could present enough evidence to prove guilt." The Minnesota Judicial Branch similarly states: "Alford Plea: A plea of guilty that may be accepted by a court even where the defendant does not admit guilt. In an Alford plea, defendant has to admit that he has reviewed the state's evidence, a reasonable jury could find him guilty, and he wants to take advantage of a plea offer that has been made. Court has discretion as to whether to accept this type of plea."

The U.S. Attorneys' Manual states that in the federal system, Alford pleas "should be avoided except in the most unusual circumstances, even if no plea agreement is involved and the plea would cover all pending charges." U.S. Attorneys are required to obtain the approval of an Assistant Attorney General with supervisory responsibility over the subject matter before accepting such a plea.

The Alford plea has received public attention for its use in resolving high-profile post-conviction proceedings for individuals who claim they were wrongfully convicted for crimes they did not commit. In 2011, the West Memphis Three—three men who had been convicted as teenagers of the 1993 murders of three children and sentenced to life in prison or, for one defendant, the death penalty—entered Alford pleas decades following their initial convictions. New evidence had come to light that might exonerate them, so the Arkansas Supreme Court ordered an evidentiary hearing to consider whether a new trial would be required. Instead of holding the hearing, the defendants and state prosecutors agreed that the court would vacate the prior convictions to permit the defendants to enter Alford pleas, be re-sentenced to "time served," and obtain immediate release from prison. As part of the plea deal, the men agreed not to sue the state seeking civil damages for their convictions and imprisonment.

Similarly, novelist Michael Peterson, who had been convicted in 2003 of murdering his wife, entered an Alford plea in 2017 to resolve the case against him. Peterson's case had been the subject of the 2004 documentary series "The Staircase" and other media scrutiny, and Peterson continued to challenge his conviction based on alleged law enforcement misconduct and judicial errors. He was eventually granted a new trial, but then agreed to enter an Alford plea to the lesser offense of voluntary manslaughter instead. The judge then imposed a sentence that, after being reduced to account for time already served, resulted in Peterson serving no additional time in prison. A 2022 scripted drama miniseries, also called "The Staircase", portrayed the events of the case, including the legal battle and Alford plea.

In his book "American Criminal Justice" (1972), Jonathan D. Casper comments on the Supreme Court decision, noting, "The "Alford" decision recognizes the plea-bargaining system, acknowledging that a man may maintain his innocence but still plead guilty in order to minimize his potential loss." Casper comments on the impact of the Supreme Court's decision to require evidence of guilt in such a plea: "By requiring that there be some evidence of guilt in such a situation, the decision attempts to protect the 'really' innocent from the temptations to which plea-bargaining and defense attorneys may subject them."

US Air Force attorney Steven E. Walburn argues in a 1998 article in "The Air Force Law Review" that this form of guilty plea should be adopted for usage by the United States military. "In fairness to an accused, if, after consultation with his defense counsel, he knowingly and intelligently determines that his best interest is served by an Alford-type guilty plea, he should be free to choose this path. The system should not force him to lie under oath, nor to go to trial with no promise of the ultimate outcome concerning guilt or punishment. We must trust the accused to make such an important decision for himself. The military provides an accused facing court-martial with a qualified defense attorney. Together, they are in the best position to properly weigh what the impact his decision, and the resulting conviction, will have upon himself and his family," writes Walburn. He emphasizes that when allowing these pleas, "trial counsel should establish as strong a factual basis as possible", in order to minimize the possible negative outcomes to "the public's perception of the administration of justice within the military".

Stephanos Bibas writes in a 2003 analysis for "Cornell Law Review" that Judge Frank H. Easterbrook and a majority of scholars "praise these pleas as efficient, constitutional means of resolving cases". Bibas notes that prominent plea bargain critic Albert Alschuler supports the use of this form of plea, writing, "He views them as a lesser evil, a way to empower defendants within a flawed system. As long as we have plea bargaining, he maintains, innocent defendants should be free to use these pleas to enter advantageous plea bargains without lying. And guilty defendants who are in denial should be empowered to use these pleas instead of being forced to stand trial." Bibas instead asserts that this form of plea is "unwise and should be abolished". Bibas argues, "These procedures may be constitutional and efficient, but they undermine key values served by admissions of guilt in open court. They undermine the procedural values of accuracy and public confidence in accuracy and fairness, by convicting innocent defendants and creating the perception that innocent defendants are being pressured into pleading guilty. More basically, they allow guilty defendants to avoid accepting responsibility for their wrongs."

Legal scholar Jim Drennan, an expert on the court system at the Institute of Government at the University of North Carolina at Chapel Hill, told the "Winston-Salem Journal" in a 2007 interview that the ability to use this form of guilty plea as an option in courts had a far-reaching effect throughout the United States. Drennan commented, "We have lots of laws, but human interaction creates unique circumstances and the law has to adapt." He said of the Supreme Court case, "They had to make a decision about what to do. One of the things the court has to do is figure out how to answer new questions, and that is what happened in this case."

Common criticisms of Alford pleas include: harm to victims who are denied justice, harm to society from lack of respect for the criminal justice system, the incentive for coercion, violating the right against self-incrimination, hindering rehabilitation by avoiding treatment, and the arbitrary nature in which they are utilized, allowing a person to say one thing when they mean another.



ABCD

ABCD is a list of the first four letters in the English alphabet. It may also refer to:





Anti-realism

In analytic philosophy, anti-realism is a position which encompasses many varieties such as metaphysical, mathematical, semantic, scientific, moral and epistemic. The term was first articulated by British philosopher Michael Dummett in an argument against a form of realism Dummett saw as 'colorless reductionism'.

In anti-realism, the truth of a statement rests on its demonstrability through internal logic mechanisms, such as the context principle or intuitionistic logic, in direct opposition to the realist notion that the truth of a statement rests on its correspondence to an external, independent reality. In anti-realism, this external reality is hypothetical and is not assumed. 

Anti-realism in its most general sense can be understood as being in contrast to a "generic realism", which holds that distinctive objects of a subject-matter exist and have properties independent of one's beliefs and conceptual schemes. The ways in which anti-realism rejects these type of claims can vary dramatically. Because this encompasses statements containing abstract ideal objects (i.e. mathematical objects), anti-realism may apply to a wide range of philosophical topics, from material objects to the theoretical entities of science, mathematical statements, mental states, events and processes, the past and the future.

One kind of metaphysical anti-realism maintains a skepticism about the physical world, arguing either: 1) that nothing exists outside the mind, or 2) that we would have no access to a mind-independent reality, even if it exists. The latter case often takes the form of a denial of the idea that we can have 'unconceptualised' experiences (see Myth of the Given). Conversely, most realists (specifically, indirect realists) hold that perceptions or sense data are caused by mind-independent objects. But this introduces the possibility of another kind of skepticism: since our understanding of causality is that the same effect can be produced by multiple causes, there is a lack of determinacy about what one is really perceiving, as in the brain in a vat scenario. The main alternative to this sort of metaphysical anti-realism is metaphysical realism.

On a more abstract level, model-theoretic anti-realist arguments hold that a given set of symbols in a theory can be mapped onto any number of sets of real-world objects—each set being a "model" of the theory—provided the relationship between the objects is the same (compare with symbol grounding.)

In ancient Greek philosophy, nominalist (anti-realist) doctrines about universals were proposed by the Stoics, especially Chrysippus. In early modern philosophy, conceptualist anti-realist doctrines about universals were proposed by thinkers like René Descartes, John Locke, Baruch Spinoza, Gottfried Wilhelm Leibniz, George Berkeley, and David Hume. In late modern philosophy, anti-realist doctrines about knowledge were proposed by the German idealist Georg Wilhelm Friedrich Hegel. Hegel was a proponent of what is now called inferentialism: he believed that the ground for the axioms and the foundation for the validity of the inferences are the right consequences and that the axioms do not explain the consequence. Kant and Hegel held conceptualist views about universals. In contemporary philosophy, anti-realism was revived in the form of empirio-criticism, logical positivism, semantic anti-realism and scientific instrumentalism (see below).

In the philosophy of mathematics, realism is the claim that mathematical entities such as 'number' have an observer-independent existence. Empiricism, which associates numbers with concrete physical objects, and Platonism, in which numbers are abstract, non-physical entities, are the preeminent forms of mathematical realism.

The "epistemic argument" against Platonism has been made by Paul Benacerraf and Hartry Field. Platonism posits that mathematical objects are "abstract" entities. By general agreement, abstract entities cannot interact causally with physical entities ("the truth-values of our mathematical assertions depend on facts involving platonic entities that reside in a realm outside of space-time") Whilst our knowledge of physical objects is based on our ability to perceive them, and therefore to causally interact with them, there is no parallel account of how mathematicians come to have knowledge of abstract objects.

Field developed his views into fictionalism. Benacerraf also developed the philosophy of mathematical structuralism, according to which there are no mathematical objects. Nonetheless, some versions of structuralism are compatible with some versions of realism.

Anti-realist arguments hinge on the idea that a satisfactory, naturalistic account of thought processes can be given for mathematical reasoning. One line of defense is to maintain that this is false, so that mathematical reasoning uses some special intuition that involves contact with the Platonic realm, as in the argument given by Sir Roger Penrose.

Another line of defense is to maintain that abstract objects are relevant to mathematical reasoning in a way that is non causal, and not analogous to perception. This argument is developed by Jerrold Katz in his 2000 book "Realistic Rationalism". In this book, he put forward a position called realistic rationalism, which combines metaphysical realism and rationalism.

A more radical defense is to deny the separation of physical world and the platonic world, i.e. the mathematical universe hypothesis (a variety of mathematicism). In that case, a mathematician's knowledge of mathematics is one mathematical object making contact with another.

The term "anti-realism" was introduced by Michael Dummett in his 1982 paper "Realism" in order to re-examine a number of classical philosophical disputes, involving such doctrines as nominalism, Platonic realism, idealism and phenomenalism. The novelty of Dummett's approach consisted in portraying these disputes as analogous to the dispute between intuitionism and Platonism in the philosophy of mathematics.

According to intuitionists (anti-realists with respect to mathematical objects), the truth of a mathematical statement consists in our ability to prove it. According to Platonic realists, the truth of a statement is proven in its correspondence to objective reality. Thus, intuitionists are ready to accept a statement of the form "P or Q" as true only if we can prove P or if we can prove Q. In particular, we cannot in general claim that "P or not P" is true (the law of excluded middle), since in some cases we may not be able to prove the statement "P" nor prove the statement "not P". Similarly, intuitionists object to the existence property for classical logic, where one can prove formula_1, without being able to produce any term formula_2 of which formula_3 holds.

Dummett argues that this notion of truth lies at the bottom of various classical forms of anti-realism, and uses it to re-interpret phenomenalism, claiming that it need not take the form of reductionism.

Dummett's writings on anti-realism draw heavily on the later writings of Ludwig Wittgenstein, concerning meaning and rule following, and can be seen as an attempt to integrate central ideas from the "Philosophical Investigations" into the constructive tradition of analytic philosophy deriving from Gottlob Frege.

In philosophy of science, anti-realism applies chiefly to claims about the non-reality of "unobservable" entities such as electrons or genes, which are not detectable with human senses.

One prominent variety of scientific anti-realism is instrumentalism, which takes a purely agnostic view towards the existence of unobservable entities, in which the unobservable entity X serves as an instrument to aid in the success of theory Y and does not require proof for the existence or non-existence of X.

In the philosophy of ethics, moral anti-realism (or moral irrealism) is a meta-ethical doctrine that there are no objective moral values or normative facts. It is usually defined in opposition to moral realism, which holds that there are objective moral values, such that a moral claim may be either true or false. Specifically the moral anti-realist is committed to denying at least one of the following three statements: 


Different version of moral anti-realism deny different statements: specifically, non-cognitivism denies the first claim, arguing that moral statements have no meaning or truth content, error theory denies the second claim, arguing that all moral statements are false, and ethical subjectivism denies the third claim, arguing that the truth of moral statements is mind dependent.

Examples of anti-realist moral theories might be:


There is a debate as to whether moral relativism is actually an anti-realist position. While many versions deny the metaphysical thesis, some do not, as one could imagine a system of morality which requires you to obey the written laws in your country. Such a system would be a version of moral relativism, as different individuals would be required to follow different laws, but the moral facts are physical facts about the world, not mental facts, so they are metaphysically ordinary. Thus, different versions of moral relativism might be considered anti-realist or realist.

Just as moral anti-realism asserts the nonexistence of normative facts, epistemic anti-realism asserts the nonexistence of facts in the domain of epistemology. Thus, the two are now sometimes grouped together as "metanormative anti-realism". Prominent defenders of epistemic anti-realism include Hartry Field, Simon Blackburn, Matthew Chrisman, and Allan Gibbard, among others.



Arsenal F.C.

The Arsenal Football Club, commonly known as Arsenal, is an English professional football club based in Islington, North London. Arsenal compete in the Premier League, the top flight of English football. In domestic football, Arsenal have won 13 league titles (including one unbeaten title), a record 14 FA Cups, two League Cups, 17 FA Community Shields and a Football League Centenary Trophy. In European football, they have one European Cup Winners' Cup and one Inter-Cities Fairs Cup. In terms of trophies won, it is the third-most successful club in English football.

Arsenal was the first club from the South of England to join the Football League in 1893, and they reached the First Division in 1904. Relegated only once, in 1913, they continue the longest streak in the top division, and have won the second-most top-flight matches in English football history. In the 1930s, Arsenal won five League Championships and two FA Cups, and another FA Cup and two Championships after the war. In 1970–71, they won their first League and FA Cup Double. Between 1989 and 2005, they won five League titles and five FA Cups, including two more Doubles. They completed the 20th century with the highest average league position. Between 1998 and 2017, Arsenal qualified for the UEFA Champions League for nineteen consecutive seasons.

In 1886, munitions workers at the Royal Arsenal in Woolwich founded the club as Dial Square. In 1913 the club crossed the city to Arsenal Stadium in Highbury, becoming close neighbours of Tottenham Hotspur, and creating the North London derby. Herbert Chapman, who changed the fortunes of Arsenal forever, won the club its first silverware, and his legacy led the club to dominate the 1930s. He helped introduce the WM formation, floodlights, and shirt numbers; he also added the white sleeves and brighter red to the club's jersey. Arsène Wenger is the club's longest-serving manager and has won the most trophies for it. He won a record seven FA Cups, and his title-winning team set an English record for the longest top-flight unbeaten league run at 49 games between 2003 and 2004, receiving the nickname The Invincibles.

In 2006, the club moved to the nearby Emirates Stadium. With an annual revenue of £367.1m in the 2021–22 season, Arsenal was estimated to be worth US$2.26 billion by "Forbes", making it the world's tenth most valuable football club, while it is one of the most followed on social media. The motto of the club is "Victoria Concordia Crescit", Latin for "Victory Through Harmony".

In October 1886, Scotsman David Danskin and fifteen fellow munitions workers in Woolwich formed Dial Square Football Club, named after a workshop at the heart of the Royal Arsenal complex. Each member contributed sixpence and Danskin also added three shillings to help form the club. Dial Square played their first match on 11 December 1886 against Eastern Wanderers and won 6–0. The club had renamed to Royal Arsenal by January 1887, and its first home was Plumstead Common, though they spent most of their time playing at the Manor Ground. Their first trophies were the Kent Senior Cup and London Charity Cup in 1889–90 and the London Senior Cup in 1890–91; these were the only county association trophies Arsenal won during their time in South East London. In 1891, Royal Arsenal became the first London club to turn professional.

Royal Arsenal renamed for a second time upon becoming a limited liability company in 1893. They registered their new name, Woolwich Arsenal, with the Football League when the club ascended later that year. Woolwich Arsenal was the first southern member of the Football League, starting out in the Second Division and reaching the First Division in 1904. Falling attendances, due to financial difficulties among the munitions workers and the arrival of more accessible football clubs elsewhere in the city, led the club close to bankruptcy by 1910. Businessmen Henry Norris and William Hall became involved in the club, and sought to move them elsewhere.

In 1913, soon after relegation back to the Second Division, the club moved across the river to the new Arsenal Stadium in Highbury. In 1919, the Football League controversially voted to promote The Arsenal, instead of relegated local rivals Tottenham Hotspur, into the newly enlarged First Division, despite only finishing fifth in the Second Division's last pre-war season of 1914–15. Later that year, The Arsenal started dropping "The" in official documents, gradually shifting its name for the final time towards Arsenal, as it is generally known today.
With a new home and First Division football, attendances were more than double those at the Manor Ground, and Arsenal's budget grew rapidly. With record-breaking spending and gate receipts, Arsenal quickly became known as the Bank of England club.

Arsenal's location and record-breaking salary offer lured star Huddersfield Town manager Herbert Chapman in 1925. Over the next five years, Chapman built a revolutionary new Arsenal. Firstly, he appointed an enduring new trainer Tom Whittaker who would one day rise to become a fabled Arsenal manager himself. With the help of player Charlie Buchan, implemented the nascent WM formation which would serve as a stable bedrock to his outfit. He also captured generational young talents such as Cliff Bastin and Eddie Hapgood, whilst also lavishing Highbury's high income on stars such as David Jack and Alex James.

Transformed, Chapman's Arsenal claimed their first national trophy, the FA Cup in 1930, and League Championships followed in 1930–31 and 1932–33. Chapman also presided over off the pitch changes: white sleeves and shirt numbers were added to the kit; a Tube station was named after the club; and the first of two opulent, Art Deco stands was completed, with some of the first floodlights in English football. Suddenly, in the middle of the 1933–34 season, Chapman died of pneumonia.

Chapman's death meant work was left to his colleagues Joe Shaw and George Allison, with both proving to be shrewd & consummate custodians of Chapman's excellent Arsenal team, seeing out a hat-trick of league wins with the 1933–34, 1934–35, and 1937–38 titles, and then furthermore winning the 1936 FA Cup.

World War II meant the Football League was suspended for seven years. While Arsenal were paraded by the nation as a symbol of solidarity with war efforts, the war took a huge toll on the team as the club had had more players killed than any top flight club. Furthermore, debt from reconstructing an ambitious North Bank Stand redevelopment greatly bled Arsenal's resources.

Despite this period of turbulence & churn, Arsenal returned to win the league in the second post-war season of 1947–48. This was Tom Whittaker's first season as manager, and meant the club equalled the champions of England record. Tom Whittaker, despite his disarming humble & modest disposition, was oft-referred to as the "brains" behind the charismatic Chapman's legendary Arsenal side. He gathered a successful & highly skilled Arsenal side in spite of greatly limited resources, with a fiery and expansive style that drove great fanfare at the time.

They won a third FA Cup in 1950, and then won a record-breaking seventh championship in 1952–53 making Arsenal the most successful team in English history at the time.

Arsenal were not to win the League or the FA Cup for another 18 years. The '53 Champions squad had aged, and the club failed to attract strong enough replacements. Although Arsenal were competitive during these years, their fortunes had waned; the club spent most of the 1950s and 1960s in mid-table mediocrity. Even former England captain Billy Wright could not bring the club any success as manager, in a stint between 1962 and 1966.

Arsenal tentatively appointed club physiotherapist Bertie Mee as acting manager in 1966. With new assistant Don Howe and new players such as Bob McNab and George Graham, Mee led Arsenal to their first League Cup finals, in 1967–68 and 1968–69. Next season saw a breakthrough, with Arsenal's first competitive European trophy, the 1969–70 Inter-Cities Fairs Cup. The season after, Arsenal achieved an even greater triumph with their first League and FA Cup double, and a new champions of England record. This marked a premature high point of the decade; the Double-winning side was soon broken up and the rest of the decade was characterised by a series of near misses, with Arsenal finishing as FA Cup runners up in 1972, and First Division runners-up in 1972–73.

Former player Terry Neill succeeded Mee in 1976. At the age of 34, he became the youngest Arsenal manager to date. With new signings like Malcolm Macdonald and Pat Jennings, and a crop of talent in the side like Liam Brady and Frank Stapleton, the club reached a trio of FA Cup finals (1978 FA Cup, 1979 FA Cup and 1980 FA Cup), and lost the 1980 European Cup Winners' Cup Final on penalties. The club's only trophy during this time was the 1979 FA Cup, achieved with a last-minute 3–2 victory over Manchester United, in a final is widely regarded as a classic.

One of Mee's double winners, George Graham, returned as manager in 1986, with Arsenal winning their first League Cup in 1987, Graham's first season in charge. New signings Nigel Winterburn, Lee Dixon and Steve Bould had joined the club by 1988 to complete the "famous Back Four", led by homegrown player Tony Adams. Graham's credo of prioritising defensive excellence seemingly clashed with the club's traditional expansive motif & the young player demographic at the club at the time, however they quickly gained a cult following after initial successes.

They immediately won the 1988 Football League Centenary Trophy, and followed it with the 1988–89 Football League title, snatched with a last-minute goal in the final game of the season against fellow title challengers Liverpool. Graham's Arsenal won another title in 1990–91, losing only one match, won the FA Cup and League Cup double in 1993, and the European Cup Winners' Cup in 1994. Graham's reputation was tarnished when he was found to have taken kickbacks from agent Rune Hauge for signing certain players, and he was dismissed in 1995. His replacement, Bruce Rioch, lasted for only one season, leaving the club after a dispute with the board of directors.

The club metamorphosed during the tenure of French manager Arsène Wenger, who was appointed in 1996. Attacking football, an overhaul of dietary and fitness practices, and efficiency with money defined his reign. Accumulating key players from Wenger's homeland, such as Patrick Vieira and Thierry Henry, Arsenal won a second League and Cup double in 1997–98 and a third in 2001–02. In addition, the club reached the final of the 1999–2000 UEFA Cup, were victorious in the 2003 and 2005 FA Cup finals, and won the Premier League in 2003–04 without losing a single match, an achievement which earned the side the nickname "The Invincibles". This feat came within a run of 49 league matches unbeaten from 7 May 2003 to 24 October 2004, a national record.

Arsenal finished in either first or second place in the league in eight of Wenger's first nine seasons at the club, although they never won the title in two consecutive seasons.
The club had never progressed beyond the quarter-finals of the Champions League until 2005–06; in that season they became the first club from London to reach the final in the competition's fifty-year history, but were beaten 2–1 by Barcelona. In July 2006, they moved into the Emirates Stadium, after 93 years at Highbury.
Arsenal reached the final of the 2007 and 2011 League Cups, losing 2–1 to Chelsea and Birmingham City respectively. The club had not gained a trophy since the 2005 FA Cup until, spearheaded by club record acquisition Mesut Özil, Arsenal beat Hull City in the 2014 FA Cup Final, coming back from a 2–0 deficit to win the match 3–2. A year later, Arsenal completed another victorious FA Cup campaign, and became the most successful club in the tournament's history by winning their 13th FA Cup in 2016–17. However, in that same season, Arsenal finished fifth in the league, the first time they had finished outside the top four since before Wenger arrived in 1996. In his 21st & final season, Arsenal under Arsene Wenger finished sixth & won the community shield. Wenger departed Arsenal following the end of the season, on 13 May 2018.

After conducting an overhaul in the club's operating model to coincide with Wenger's departure, Spaniard Unai Emery was named as the club's new head coach on 23 May 2018. He became the club's first ever 'head coach' and second manager from outside the United Kingdom. In Emery's first season, Arsenal finished fifth in the Premier League and as runner-up in the Europa League. On 29 November 2019, Emery was dismissed as manager and former player and assistant first team coach Freddie Ljungberg was appointed as interim head coach.

On 20 December 2019, Arsenal appointed former club captain Mikel Arteta as the new head coach. Arsenal finished the league season in eighth, their lowest finish since 1994–95, but beat Chelsea 2–1 to earn a record-extending 14th FA Cup win. After the season, Arteta's title was changed from head coach to manager. On 18 April 2021, Arsenal were announced as a founding club of the breakaway European competition The Super League; they withdrew from the competition two days later amid near-universal condemnation. Arsenal finished the 2020–21 season in eighth place once again, not qualifying for a European competition for the first time in 26 years. In the 2022–23 Premier League season, Arsenal returned to the Champions League by coming second to Manchester City. Arsenal led the league for most of the season, but suffered a series of losses at the end of the run, setting a record for most time spent on top of the table without actually winning the league.

Unveiled in 1888, Royal Arsenal's first crest featured three cannons viewed from above, pointing northwards, similar to the coat of arms of the Metropolitan Borough of Woolwich (nowadays transferred to the coat of arms of the Royal Borough of Greenwich). These can sometimes be mistaken for chimneys, but the presence of a carved lion's head and a cascabel on each are clear indicators that they are cannons. This was dropped after the move to Highbury in 1913, only to be reinstated in 1922, when the club adopted a crest featuring a single cannon, pointing eastwards, with the club's nickname, "The Gunners", inscribed alongside it; this crest only lasted until 1925, when the cannon was reversed to point westward and its barrel slimmed down.

In 1949, the club unveiled a modernised crest featuring the same style of cannon below the club's name, set in blackletter typography, and above the coat of arms of the Metropolitan Borough of Islington and a scroll inscribed with the club's newly adopted Latin motto, "Victoria Concordia Crescit" (VCC) – "victory comes from harmony" – coined by the club's programme editor Harry Homer. For the first time, the crest was rendered in colour, which varied slightly over the crest's lifespan, finally becoming red, gold and green. Because of the numerous revisions of the crest, Arsenal were unable to copyright it. Although the club had managed to register the crest as a trademark, and had fought (and eventually won) a long legal battle with a local street trader who sold "unofficial" Arsenal merchandise,
Arsenal eventually sought a more comprehensive legal protection. Therefore, in 2002 they introduced a new crest featuring more modern curved lines and a simplified style, which was copyrightable.
The cannon once again faces east and the club's name is written in a sans-serif typeface above the cannon. Green was replaced by dark blue. The new crest was criticised by some supporters; the Arsenal Independent Supporters' Association claimed that the club had ignored much of Arsenal's history and tradition with such a radical modern design, and that fans had not been properly consulted on the issue.
Until the 1960s, a badge was worn on the playing shirt only for high-profile matches such as FA Cup finals, usually in the form of a monogram of the club's initials in red on a white background.

The monogram theme was developed into an Art Deco-style badge on which the letters A and C framed a football rather than the letter F, the whole set within a hexagonal border. This early example of a corporate logo, introduced as part of Herbert Chapman's rebranding of the club in the 1930s, was used not only on Cup Final shirts but as a design feature throughout Highbury Stadium, including above the main entrance and inlaid in the floors.
From 1967, a white cannon was regularly worn on the shirts, until replaced by the club crest, sometimes with the addition of the nickname "The Gunners", in the 1990s.

In the 2011–12 season, Arsenal celebrated their 125th anniversary. The celebrations included a modified version of the current crest worn on their jerseys for the season. The crest was all white, surrounded by 15 oak leaves to the right and 15 laurel leaves to the left. The oak leaves represent the 15 founding members of the club who met at the Royal Oak pub. The 15 laurel leaves represent the design detail on the six pence pieces paid by the founding fathers to establish the club. The laurel leaves also represent strength. To complete the crest, 1886 and 2011 are shown on either sides of the motto "Forward" at the bottom of the crest.

For much of Arsenal's history, their home colours have been bright red shirts with white sleeves and white shorts, though this has not always been the case. The choice of red is in recognition of a charitable donation from Nottingham Forest, soon after Arsenal's foundation in 1886. Two of Dial Square's founding members, Fred Beardsley and Morris Bates, were former Forest players who had moved to Woolwich for work. As they put together the first team in the area, no kit could be found, so Beardsley and Bates wrote home for help and received a set of kit and a ball. The shirt was redcurrant, a dark shade of red, and was worn with white shorts and socks with blue and white hoops.

In 1933, Herbert Chapman, wanting his players to be more distinctly dressed, updated the kit, adding white sleeves and changing the shade to a brighter pillar box red. Two possibilities have been suggested for the origin of the white sleeves. One story reports that Chapman noticed a supporter in the stands wearing a red sleeveless sweater over a white shirt; another was that he was inspired by a similar outfit worn by the cartoonist Tom Webster, with whom Chapman played golf.
Regardless of which story is true, the red and white shirts have come to define Arsenal and the team have worn the combination ever since, aside from two seasons. The first was 1966–67, when Arsenal wore all-red shirts; this proved unpopular and the white sleeves returned the following season. The second was 2005–06, the last season that Arsenal played at Highbury, when the team wore commemorative redcurrant shirts similar to those worn in 1913, their first season in the stadium; the club reverted to their normal colours at the start of the next season. In the 2008–09 season, Arsenal replaced the traditional all-white sleeves with red sleeves with a broad white stripe.

Arsenal's home colours have been the inspiration for at least three other clubs. In 1909, Sparta Prague adopted a dark red kit like the one Arsenal wore at the time; in 1938, Hibernian adopted the design of the Arsenal shirt sleeves in their own green and white strip.
In 1941, Luis Robledo, an England-schooled founder of Santa Fe and a fan of Arsenal, selected the main colors for his newly created team. In 1920, Sporting Clube de Braga's manager returned from a game at Highbury and changed his team's green kit to a duplicate of Arsenal's red with white sleeves and shorts, giving rise to the team's nickname of "Os Arsenalistas".
These teams still wear those designs to this day.

For many years Arsenal's away colours were white or navy blue. However, in 1968 the FA banned navy shirts (they looked too similar to referees' black kit) so in the 1969–70 season, Arsenal introduced an away kit of yellow shirts with blue shorts. This kit was worn in the 1971 FA Cup Final as Arsenal beat Liverpool to secure the double for the first time in their history. The yellow and blue strip became almost as famous as their iconic red and white home kit. Arsenal reached the FA Cup final again the following year wearing the red and white home strip and were beaten by Leeds United. Arsenal then competed in three consecutive FA Cup finals between 1978 and 1980 wearing their "lucky" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83. The following season, Arsenal returned to the yellow and blue scheme, albeit with a darker shade of blue than before.

When Nike took over from Adidas as Arsenal's kit provider in 1994, Arsenal's away colours were again changed to two-tone blue shirts and shorts. Since the advent of the lucrative replica kit market, the away kits have been changed regularly, with Arsenal usually releasing both away and third choice kits. During this period the designs have been either all blue designs, or variations on the traditional yellow and blue, such as the metallic gold and navy strip used in the 2001–02 season, the yellow and dark grey used from 2005 to 2007, and the yellow and maroon of 2010 to 2013.
Until 2014, the away kit was changed every season, and the outgoing away kit became the third-choice kit if a new home kit was being introduced in the same year.

Since Puma began manufacturing Arsenal's kits in 2014, new home, away and third kits were released every single season. In the 2017–18 season, Puma released a new color scheme for the away and third kits. The away kit was a light blue, which fades to a darker blue near the bottom, while the third kit was black with red highlight. Puma returned to the original color scheme for the 2018–19 season.

From the 2019–20 season Arsenal's kits are manufactured by Adidas. In the 2020–21 season, Adidas unveiled the new away kit to 
mark the 15-year anniversary since leaving Highbury. The new away kit is white, with a marbled pattern all across to replicate the iconic marble hall in the East stand of Highbury.

Before joining the Football League, Arsenal played briefly on Plumstead Common, then at the Manor Ground in Plumstead, then spent three years between 1890 and 1893 at the nearby Invicta Ground. Upon joining the Football League in 1893, the club returned to the Manor Ground and installed stands and terracing, upgrading it from just a field. Arsenal continued to play their home games there for the next twenty years (with two exceptions in the 1894–95 season), until the move to north London in 1913.

Widely referred to as Highbury, Arsenal Stadium was the club's home from September 1913 until May 2006. The original stadium was designed by the renowned football architect Archibald Leitch, and had a design common to many football grounds in the UK at the time, with a single covered stand and three open-air banks of terracing. The entire stadium was given a massive overhaul in the 1930s: new Art Deco West and East stands were constructed, opening in 1932 and 1936 respectively, and a roof was added to the North Bank terrace, which was bombed during the Second World War and not restored until 1954.

Highbury could hold more than 60,000 spectators at its peak, and had a capacity of 57,000 until the early 1990s. The Taylor Report and Premier League regulations obliged Arsenal to convert Highbury to an all-seater stadium in time for the 1993–94 season, thus reducing the capacity to 38,419 seated spectators. This capacity had to be reduced further during Champions League matches to accommodate additional advertising boards, so much so that for two seasons, from 1998 to 2000, Arsenal played Champions League home matches at Wembley, which could house more than 70,000 spectators.
Expansion of Highbury was restricted because the East Stand had been designated as a Grade II listed building and the other three stands were close to residential properties. These limitations prevented the club from maximising matchday revenue during the 1990s and first decade of the 21st century, putting them in danger of being left behind in the football boom of that time. After considering various options, in 2000 Arsenal proposed building a new 60,361-capacity stadium at Ashburton Grove, since named the Emirates Stadium, about 500 metres south-west of Highbury. The project was initially delayed by red tape and rising costs, and construction was completed in July 2006, in time for the start of the 2006–07 season. The stadium was named after its sponsors, the airline company Emirates, with whom the club signed the largest sponsorship deal in English football history, worth around £100 million. Some fans referred to the ground as Ashburton Grove, or the Grove, as they did not agree with corporate sponsorship of stadium names. The stadium will be officially known as Emirates Stadium until at least 2028, and the airline will be the club's shirt sponsor until at least 2024. From the start of the 2010–11 season on, the stands of the stadium have been officially known as North Bank, East Stand, West Stand and Clock end. The capacity of the Emirates now stands at 60,704.

Arsenal's players train at the Shenley Training Centre in Hertfordshire, a purpose-built facility which opened in 1999.
Before that the club used facilities on a nearby site owned by the University College of London Students' Union. Until 1961 they had trained at Highbury.
Arsenal's Academy under-18 teams play their home matches at Shenley, while the reserves play their games at Meadow Park, which is also the home of Boreham Wood F.C. Both the Academy under-18 & the reserves occasionally play their big games at the Emirates in front of a crowd reduced to only the lower west stand.

Arsenal's fanbase are referred to as "Gooners" – the name derived from the club's nickname "The Gunners". Virtually all home matches sell out; in 2007–08 Arsenal had the second-highest average League attendance for an English club (60,070, which was 99.5% of available capacity), and, as of 2015, the third-highest all-time average attendance. Arsenal have the seventh highest average attendance of European football clubs only behind Borussia Dortmund, FC Barcelona, Manchester United, Real Madrid, Bayern Munich, and Schalke. The club's location, adjoining wealthy areas such as Canonbury and Barnsbury, mixed areas such as Islington, Holloway, Highbury, and the adjacent London Borough of Camden, and largely working-class areas such as Finsbury Park and Stoke Newington, has meant that Arsenal's supporters have come from a variety of social classes. Much of the Afro-Caribbean support comes from the neighbouring London Borough of Hackney and a large portion of the South Asian Arsenal supporters commute to the stadium from Wembley Park, North West of the capital. There was also traditionally a large Irish community that followed Arsenal, with the surrounding Islington and particularly the nearby Archway area having a large community of residents with Irish heritage. But Irish migration to North London is recently much lower than in the 1960s or 1970s.

Like all major English football clubs, Arsenal have a number of domestic supporters' clubs, including the Arsenal Football Supporters' Club, which works closely with the club, and the Arsenal Independent Supporters' Association, which maintains a more independent line. The Arsenal Supporters' Trust promotes greater participation in ownership of the club by fans. The club's supporters also publish fanzines such as "The Gooner", "Gunflash" and the satirical "Up The Arse!"

There have always been Arsenal supporters outside London, and since the advent of satellite television, a supporter's attachment to a football club has become less dependent on geography. Consequently, Arsenal have a significant number of fans from beyond London and all over the world; in 2007, 24 UK, 37 Irish and 49 other overseas supporters clubs were affiliated with the club. A 2011 report by SPORT+MARKT estimated Arsenal's global fanbase at 113 million. The club's social media activity was the fifth highest in world football during the 2014–15 season.

The team's anthem is "The Angel (North London Forever)" by Louis Dunford. The song is typically played at Arsenal home games before a match.

In addition to the usual English football chants, Arsenal's supporters sing "One-Nil to the Arsenal" (to the tune of "Go West") and also regularly sing "Who's that team they call the Arsenal", "Good Old Arsenal" (to the tune of "Rule, Britannia!") and "We're the North Bank/Clock End Highbury". The fans also chant "Boring, Boring Arsenal" in self-deprecating reference to Arsenal's reputation during the 1970s and 1980s as an overly defensive, cautious team.

Arsenal's longest-running and deepest rivalry is with their nearest major neighbour, Tottenham Hotspur; matches between the two are referred to as the North London derby. There also exists a rivalry between Arsenal and Chelsea. In addition, Arsenal and Manchester United developed a strong on-pitch rivalry in the late 1980s, which intensified in the early 2000s when both clubs were competing for the Premier League title. During the 2010s and now the 2020s, a competitive rivalry with Manchester City began during the Arteta era following a close title race in the 2022-23 Premier League season.

The club mascot is Gunnersaurus Rex, a smiling, 7-foot-tall green dinosaur, who first appeared at a home match against Manchester City in August 1994 (or 1993). He is based on a drawing by then 11-year-old Peter Lovell, whose design and another similar idea won a Junior Gunners contest; his official back story is that he hatched from an egg found during renovations at Highbury.

The same person, Jerry Quy, has been inside the suit from the start; in early October 2020, as part of cost-cutting brought about by the COVID-19 pandemic, the club made him redundant from that and his other part-time job in supporter liaison, together with 55 full-time employees, although they later said Gunnersaurus could return after spectators were allowed back in stadiums. An online fundraiser was begun for Quy, and Mesut Özil offered to pay his salary himself as long as he remains with Arsenal. In November 2020, in advance of COVID-19 regulations being relaxed to allow supporters to attend home games from 3 December, Arsenal announced that Gunnersaurus would return, to be played by a roster of people that could include Quy if he wished.

The largest shareholder on the Arsenal board is American sports tycoon Stan Kroenke. Kroenke first launched a bid for the club in April 2007, and faced competition for shares from Red and White Securities, which acquired its first shares from David Dein in August 2007. Red & White Securities was co-owned by Russian billionaire Alisher Usmanov and Iranian London-based financier Farhad Moshiri, though Usmanov bought Moshiri's stake in 2016. Kroenke came close to the 30% takeover threshold in November 2009, when he increased his holding to 18,594 shares (29.9%). In April 2011, Kroenke achieved a full takeover by purchasing the shareholdings of Nina Bracewell-Smith and Danny Fiszman, taking his shareholding to 62.89%. In May 2017, Kroenke owned 41,721 shares (67.05%) and Red & White Securities owned 18,695 shares (30.04%). In January 2018, Kroenke expanded his ownership by buying twenty-two more shares, taking his total ownership to 67.09%. In August 2018, Kroenke bought out Usmanov for £550m. Now owning more than 90% of the shares, he had the required stake to complete the buyout of the remaining shares and become the sole owner. There has been criticism of Arsenal's poor performance since Kroenke took over, which has been attributed to his ownership. Ivan Gazidis was the club's Chief executive from 2009 to 2018.

Arsenal's parent company, Arsenal Holdings plc, operates as an unlisted public limited company, whose ownership is considerably different from that of other football clubs. Only 62,219 shares in Arsenal have been issued, and they are not traded on a public exchange such as the FTSE or AIM; instead, they are traded relatively infrequently on the ICAP Securities and Derivatives Exchange, a specialist market. On 29 May 2017, a single share in Arsenal had a mid price of £18,000, which sets the club's market capitalisation value at approximately £1,119.9m. Most football clubs are not listed on an exchange, which makes direct comparisons of their values difficult. Consultants Brand Finance valued the club's brand and intangible assets at $703m in 2015, and consider Arsenal an AAA global brand. Business magazine Forbes valued Arsenal as a whole at $2.238 billion (£1.69 billion) in 2018, ranked third in English football. Research by the Henley Business School ranked Arsenal second in English football, modelling the club's value at £1.118 billion in 2015.

Arsenal's financial results for the 2019–20 season showed an after tax loss of £47.8m, due in part to the impact of the COVID-19 pandemic. The Deloitte Football Money League is a publication that homogenises and compares clubs' annual revenue. Deloitte put Arsenal's footballing revenue in 2019 at £392.7m (€445.6m), ranking Arsenal eleventh among world football clubs. Arsenal and Deloitte both listed the match day revenue generated in 2019 by the Emirates Stadium as €109.2m (£96.2m).

Partly due to their proximity to the Alexandra Palace transmitter, Arsenal have appeared in a number of media "firsts". On 22 January 1927, their match at Highbury against Sheffield United was the first English League match to be broadcast live on radio. A decade later, on 16 September 1937, an exhibition match between Arsenal's first team and the reserves was the first football match in the world to be televised live. Arsenal also featured in the first edition of the BBC's "Match of the Day", which screened highlights of their match against Liverpool at Anfield on 22 August 1964. Sky's coverage of Arsenal's January 2010 match against Manchester United was the first live public broadcast of a sports event on 3D television.

As one of the most successful teams in the country, Arsenal have often featured when football is depicted in the arts in Britain. They formed the backdrop to one of the earliest football-related novels, "The Arsenal Stadium Mystery" (1939), which was made into a film in the same year. The story centres on a friendly match between Arsenal and an amateur side, one of whose players is poisoned while playing. Many Arsenal players appeared as themselves in the film and manager George Allison was given a speaking part. The book "Fever Pitch" by Nick Hornby was an autobiographical account of Hornby's life and relationship with football and Arsenal in particular. Published in 1992, it formed part of the revival and rehabilitation of football in British society during the 1990s. The book was twice adapted for the cinema – the 1997 British film focuses on Arsenal's 1988–89 title win, and a 2005 American version features a fan of baseball's Boston Red Sox.

Arsenal have often been stereotyped as a defensive and "boring" side, especially during the 1970s and 1980s. In the 1997 film "The Full Monty" the principal characters move forward in a line and raise their hands, deliberately mimicking the Arsenal defence's offside trap, in an attempt to co-ordinate their striptease routine.
Fifteen years later an almost identical scene was included in the 2012 Disney science-fiction film "John Carter" (director and co-writer Andrew Stanton, a notable overseas supporter of the club), along with other visual cues and oblique dialogue hints and references to the club throughout the film.
Another film reference to the club's defence comes in the film "Plunkett & Macleane", in which two characters are named Dixon and Winterburn after Arsenal's long-serving full backs – the right-sided Lee Dixon and the left-sided Nigel Winterburn.

In August 2022, Amazon Prime Video released an eight-episode docuseries called "". It documented the club by spending time with the coaching staff and players behind the scenes both on and off the field throughout their 2021–22 season, in which they were the youngest team in the Premier League with an average starting age of 24 years and 308 days – more than a whole year younger than the next team.

In 1985, Arsenal founded a community scheme, "Arsenal in the Community", which offered sporting, social inclusion, educational and charitable projects. The club support a number of charitable causes directly and in 1992 established The Arsenal Charitable Trust, which by 2006 had raised more than £2 million for local causes. An ex-professional and celebrity football team associated with the club also raised money by playing charity matches. The club launched the Arsenal for Everyone initiative in 2008 as an annual celebration of the diversity of the Arsenal family. In the 2009–10 season Arsenal announced that they had raised a record breaking £818,897 for the Great Ormond Street Hospital Children's Charity. The original target was £500,000. In 2022, Arsenal and Adidas partnered up to launch the "No More Red" campaign to support the long-standing work being done by Arsenal in the Community to help keep young people safe from knife crime and youth violence. To promote the event, the club launched an exclusive all white kit that was not commercially available and only awarded to individuals who are making a positive difference in the community.

Save the Children has been Arsenal global charity partner since 2011 and have worked together in numerous projects to improve safety and well-being for vulnerable children in London and abroad. On 3 September 2016 The Arsenal Foundation has donated £1m to build football pitches for children in London, Indonesia, Iraq, Jordan and Somalia thanks to The Arsenal Foundation Legends Match against Milan Glorie at the Emirates Stadium. On 3 June 2018, Arsenal played Real Madrid in the Corazon Classic Match 2018 at the Bernabeu, where the proceeds went to Realtoo Real Madrid Foundation projects that are aimed at the most vulnerable children. In addition there will be a return meeting on 8 September 2018 at the Emirates stadium where proceeds will go towards the Arsenal foundation.

During 2007 in Pleiku, Vietnam, Arsenal partnered with the JMG Academy and the Hoang Anh Gia Lai Corporation to found a youth academy for the V.League 1 side Hoàng Anh Lai Lai, which saw a selection of Vietnam-based players train with Arsenal; the club ended their partnership with the club in 2017. Additionally, the club formally partnered with a variety of clubs overseas including Virginia based Richmond Strikers and Cairo based Wadi Degla.

Arsenal's tally of 13 League Championships is the third highest in English football, after Manchester United (20) and Liverpool (19),
and they were the first club to reach a seventh and an eighth League Championship. As of June 2020, they are one of seven teams, the others being Manchester United, Blackburn Rovers, Chelsea, Manchester City, Leicester City and Liverpool, to have won the Premier League since its formation in 1992.

They hold the highest number of FA Cup trophies, with 14. The club is one of only six clubs to have won the FA Cup twice in succession, in 2002 and 2003, and 2014 and 2015.
Arsenal have achieved three League and FA Cup "Doubles" (in 1971, 1998 and 2002), a feat only previously achieved by Manchester United (in 1994, 1996 and 1999).
They were the first side in English football to complete the FA Cup and League Cup double, in 1993.
Arsenal were also the first London club to reach the final of the UEFA Champions League, in 2006, losing the final 2–1 to Barcelona.

Arsenal have one of the best top-flight records in history, having finished below fourteenth only seven times. They have won the second most top flight league matches in English football, and have also accumulated the second most points, whether calculated by two points per win or by the contemporary points value. They have been in the top flight for the most consecutive seasons (98 as of 2023–24). Arsenal also have the highest average league finishing position for the 20th century, with an average league placement of 8.5.

Arsenal hold the record for the longest run of unbeaten League matches (49 between May 2003 and October 2004). This included all 38 matches of their title-winning 2003–04 season, when Arsenal became only the second club to finish a top-flight campaign unbeaten, after Preston North End (who played only 22 matches) in 1888–89. They also hold the record for the longest top flight win streak. Arsenal set a Champions League record during the 2005–06 season by going ten matches without conceding a goal, beating the previous best of seven set by AC Milan. They went a record total stretch of 995 minutes without letting an opponent score; the streak ended in the final, when Samuel Eto'o scored a 76th-minute equaliser for Barcelona.

David O'Leary holds the record for Arsenal appearances, having played 722 first-team matches between 1975 and 1993. Fellow centre half and former captain Tony Adams comes second, having played 669 times. The record for a goalkeeper is held by David Seaman, with 564 appearances. Thierry Henry is the club's top goalscorer with 228 goals in all competitions between 1999 and 2012,
having surpassed Ian Wright's total of 185 in October 2005.
Wright's record had stood since September 1997, when he overtook the longstanding total of 178 goals set by winger Cliff Bastin in 1939.
Henry also holds the club record for goals scored in the League, with 175, a record that had been held by Bastin until February 2006.
Declan Rice is the Arsenal record signing after a deal with West Ham United was completed in July 2023, for an initial £100 million. This easily surpassed the former record of £72 million for Nicolas Pepe.

Arsenal's record home attendance is 73,707, for a UEFA Champions League match against RC Lens on 25 November 1998 at Wembley Stadium, where the club formerly played home European matches because of the limits on Highbury's capacity. The record attendance for an Arsenal match at Highbury is 73,295, for a 0–0 draw against Sunderland on 9 March 1935, while that at Emirates Stadium is 60,161, for a 2–2 draw with Manchester United on 3 November 2007.

Arsenal's first ever silverware was won as the Royal Arsenal in 1890. The Kent Junior Cup, won by Royal Arsenal's reserves, was the club's first trophy, while the first team's first trophy came three weeks later when they won the Kent Senior Cup. Their first national senior honour came in 1930, when they won the FA Cup. The club enjoyed further success in the 1930s, winning another FA Cup and five Football League First Division titles. Arsenal won their first league and cup double in the 1970–71 season and twice repeated the feat, in 1997–98 and 2001–02, as well as winning a cup double of the FA Cup and League Cup in 1992–93. The 2003–04 season was the only 38-match league season unbeaten in English football history. A special gold version of the Premier League trophy was commissioned and presented to the club the following season.

When the FA Cup was the only national football association competition available to Arsenal, the other football association competitions were County Cups, and they made up many of the matches the club played during a season. Arsenal's first first-team trophy was a County Cup, the inaugural Kent Senior Cup. Arsenal became ineligible for the London Cups when the club turned professional in 1891, and rarely participated in County Cups after this. Due to the club's original location within the borders of both the London and Kent Football Associations, Arsenal competed in and won trophies organised by each.

During Arsenal's history, the club has participated in and won a variety of pre-season and friendly honours. These include Arsenal's own pre-season competition the Emirates Cup, begun in 2007. During the wars, previous competitions were widely suspended and the club had to participate in wartime competitions. During WWII, Arsenal won several of these.





American cuisine

American cuisine consists of the cooking style and traditional dishes prepared in the United States of America. It has been significantly influenced by Europeans, Indigenous Americans, Africans, Latin Americans, Asians, Pacific Islanders, and many other cultures and traditions. Principal influences on American cuisine are European, Native American, soul food, regional heritages including Cajun, Louisiana Creole, Pennsylvania Dutch, Mormon foodways, Texan, Tex-Mex, New Mexican, and Tlingit, and the cuisines of immigrant groups such as Chinese American, Italian American, Greek American and Mexican American. The large size of America and its long history of immigration have created an especially diverse cuisine that varies by region.

American cooking dates back to the traditions of the Native Americans, whose diet included a mix of farmed and hunted food, and varied widely across the continent. The Colonial period created a mix of new world and Old World cookery, and brought with it new crops and livestock. During the early 19th century, cooking was based mostly on what the agrarian population could grow, hunt, or raise on their land. With an increasing influx of immigrants, and a move to city life, American food further diversified in the later part of the 19th century. The 20th century saw a revolution in cooking as new technologies, the World Wars, a scientific understanding of food, and continued immigration combined to create a wide range of new foods. This has allowed for the current rich diversity in food dishes throughout the country. This was driven in part by the many chefs and television personalities who contributed to the rise of the culinary arts in the US.

Highlights of American cuisine include milkshakes, barbecue, and a wide range of fried foods. Many quintessential American dishes are unique takes on food originally from other culinary traditions, including pizza, hot dogs, and Tex-Mex. Regional highlights include a range of fish dishes in the coastal states, gumbo, and cheesesteak. American cuisine has specific foods that are eaten on holidays, such as a turkey at thanksgiving dinner or Christmas dinner. Modern American cuisine includes a focus on fast food, as well as take-out food, which is often ethnic. There is also a vibrant culinary scene in the country surrounding televised celebrity chefs.

Native Americans utilized a number of cooking methods in early American cuisine that have been blended with the methods of early Europeans to form the basis of what is now American cuisine. Nearly all regions and subregions of the present-day cuisine have roots in the foodways of Native Americans, who lived in tribes numbering in the thousands. Prior to 1600, native peoples lived off the land in very diverse bioregions and had done so for thousands of years, often living a nomadic life where their diet changed with the season.

Many practiced a form of agriculture revolving around the Three Sisters, the rotation of beans, maize, and squash as staples of their diet. In the East, this was documented as early as the 1620s in "Of Plimoth Plantation," evidenced by the pages William Bradford wrote regarding Squanto, who showed them the traditional regional method of burying a fish or eel in a mound with seeds for maize to improve the soil; this itself is also part of the widely practiced phenomenon of companion planting.

Wild game was equally a staple of nearly every tribe: generally, deer, elk, and bison were staples, as were rabbits and hare. The Cherokee of the Southern Appalachians used blowguns made of an indigenous type of bamboo to hunt squirrels.

Northern tribes like the Ojibwe of what is now the state of Michigan and the peoples of the Wabanaki of what is now the state of Maine would stalk and hunt moose, whereas their Southern counterparts, like the Choctaw or Catawba, hunted snapping turtles and other testudines, possums, and young alligators in the subtropical swamps of Louisiana and South Carolina.

Many tribes would preserve their meat in the form of pemmican, needed on long journeys or to survive harsh winters.

As with the hunted game, the biome in which one lived often dictated what was available to catch. For example, the Apache and Navajo peoples of the Southwest, whose territories each would have included swathes of New Mexico and Arizona, generally do not eat fish because in both cultures it is taboo, as well as often inconvenient. The Navajo believe that fish have a part in the story of creation, the Apache were in general afraid of water since they associated it with thunder, and the arid desert climate made fish a rarity.

However, in the culture of the Lenape, the tribe that originally lived in New Jersey, on the Delaware River, and the area that now comprises New York City, fish and shellfish were a staple in their diet and it was such a revered part of the culture that there is a documented and still-practiced harvest dance called the Fish Dance. Originally it would have been done to celebrate bringing in fish from places like the Delaware or Raritan River or the estuary around Manhattan Island and the completion of smoking them as a source of food for the winter ahead.

Eastern tribes would have eaten cod, particularly groups that spoke the Algonquian languages of New England as far south as present day Connecticut, winter flounder and other flatfish, species of herring like the alewife, shad, Atlantic herring, and Atlantic menhaden, They also would have consumed the Atlantic sturgeon and drum.

In the West, Pacific several species of sturgeon, like the white sturgeon and green sturgeon, olachen and several autochthonal fish of the "Oncorhynchus" family including the rainbow trout, cutthroat trout, coho salmon, kokanee salmon, and chinook salmon. The last makes an appearance in the accounts of Lewis and Clark as being fished for in the Columbia River Basin, and this species is named for a family of tribes of the Pacific Northwest, indicating its important role in that food culture.

Pacific gray whales and humpbacks were hunted by American Indians off the Northwest coast, especially by the Makah, and used for their meat and oil. Catfish was also popular among native people throughout the land, over many types of terrain.

Crustaceans included shrimp, lobster, crayfish, and dungeness crabs in the Northwest and shrimp, lobster and blue crabs in the East. Other shellfish include abalone and geoduck on the West Coast, while on the East Coast the surf clam, quahog, and the soft-shell clam. Oysters were eaten on both shores, as were mussels and periwinkles.

Early American natives used a number of cooking methods that have been blended with early European cooking methods to form the basis of American cuisine. Grilling meats was common. Spit roasting over a pit fire was common as well. Vegetables, especially root vegetables, were often cooked directly in the ashes of the fire.

As early Native Americans lacked pottery that could be used directly over a fire, they developed a technique many anthropologists call "stone boiling". They heated rocks in a fire, then added the rocks to a pot filled with water until it came to a boil to cook the meat or vegetables. In what is now the Southwestern United States, they also created adobe ovens, dubbed "hornos" by the Spanish, to bake products such as cornmeal bread. Other parts of America dug pit ovens, which were also used to steam foods by adding heated rocks or embers. One technique performed extensively by New England tribes was adding seaweed or corn husks on top of the layers of stones to steam fish and shellfish as well as vegetables. A later addition was potatoes, a garden plant that came to New England by the 18th century, added while still in skin with corn in husk, later to be referred to as a clambake by colonists.

The European settlement of the Americas introduced a number of ingredients, spices, herbs, and cooking styles to the continent.

When European colonists came to Virginia, Pennsylvania, Massachusetts, and any of the other English colonies on the eastern seaboard of North America, their initial attempts at survival included planting crops familiar to them from back home in England. In the same way, they farmed animals for clothing and meat. Through hardships and the eventual establishment of trade with England, the West Indies and other regions, the colonists were able to derive a cuisine similar to what they had previously consumed in Britain and Ireland, while also introducing local animals and plants to their diet. American colonists followed along the line of British cookery up until the Revolution, when a desire to distinguish themselves from Britain led Americans to create "American" styles of cookery.

In 1796, the first American cookbook was published, and others followed. There was a general disdain for French cuisine/French cookery, even with French Huguenot settlers in South Carolina and French-Canadian emigrants in America. One of the cookbooks that proliferated in the colonies was "The Art of Cookery Made Plain and Easy" (1747) by Hannah Glasse, who referred to "the blind folly of this age that would rather be imposed on by a French booby, than give encouragement to a good English cook!" Of the French recipes given in the text, she speaks out flagrantly against the dishes as she "... think[s] it an odd jumble of trash."

With the introduction of slavery, Africans were brought into the colonies. With them, came foods and ingredients such as bananas, peanuts, sweet potato, yams, and coffee, and cooking styles reminiscent of West African cuisines are still found in many dishes, especially in Southern cuisine.

The expulsion of the Acadians from Acadia led many of them to Louisiana, where they left a French influence in the diet of those settled in Louisiana, and among the Acadian Francophones who settled eastern Maine and parts of what is now northern Vermont at the same time they colonized New Brunswick.

Some of the Jews who fled from the Inquisition with other Sephardic Jews in the 15th century had previously settled in Recife, Brazil and the West Indies, where their cuisine was influenced by new local ingredients like molasses, rum, sugar, vanilla, chocolate, peppers, corn, tomatoes, kidney beans, string beans and turkey. In 1654, twenty three Sephardic Jews arrived in New Amsterdam bringing this cuisine with them to the early colonial United States. Early American Jewish cuisine was heavily influenced by this branch of Sephardic cuisine. Many of the recipes were bound up in observance of traditional holidays and remained true to their origins. These included dishes such as stew and fish fried in olive oil, beef and bean stews, almond puddings, and egg custards. The first kosher cookbook in America was the "Jewish Cookery Book" by Esther Levy, published in 1871 in Philadelphia and includes many of the traditional recipes.

The American colonial diet varied depending on the settled region in which someone lived. Local cuisine patterns had been established by the mid-18th century. The New England colonies were extremely similar in their dietary habits to those that many of them had brought from England. As many of the New Englanders were originally from England, game hunting was useful when they immigrated to the New World. Many of the northern colonists depended upon their ability to hunt, or upon others from whom they could purchase game. Hunting was the preferred method of protein acquisition, as opposed to animal husbandry, which required much more work to defend the kept animals against raids.

A striking difference for the colonists in New England compared to other regions was seasonality. While in the southern colonies, they could farm almost year-round, in the northern colonies, the growing seasons were very restricted. In addition, northern colonists' close proximity to the ocean gave them a bounty of fresh fish to add to their diet.

Wheat, the grain used to bake bread back in England, was almost impossible to grow, and imports of wheat were far from cost productive. Substitutes in cases such as this included cornmeal. The Johnnycake was a poor substitute to some for wheaten bread, but acceptance by both the northern and southern colonies seems evident.

Commonly hunted game included deer, bear, buffalo, and wild turkey. The larger muscles of the animals were roasted and served with currant sauce, while the other smaller portions went into soups, stews, sausages, pies, and pastries. In addition to the game, colonists' protein intake was supplemented by mutton.

The Spanish in Florida originally introduced sheep to the New World, but this development never quite reached the North, and there they were introduced by the Dutch and English. The keeping of sheep was a result of the English non-practice of animal husbandry. The animals provided wool when young and mutton upon maturity after wool production was no longer desirable. The forage-based diet for sheep that prevailed in the Colonies produced a characteristically strong, gamy flavor and a tougher consistency, which required aging and slow cooking to tenderize.

Fats and oils made from animals served to cook many colonial foods. Many homes had a sack made of deerskin filled with bear oil for cooking, while solidified bear fat resembled shortening.

Rendered pork fat made the most popular cooking medium, especially from the cooking of bacon. Pork fat was used more often in the southern colonies than the northern colonies as the Spanish introduced pigs earlier to the South. The colonists enjoyed butter in cooking as well, but it was rare prior to the American Revolution, as cattle were not yet plentiful.

Prior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items. Rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies.

Further into the interior, however, one would often find colonists consuming whiskey, as they did not have similar access to sugar cane. They did have ready access to corn and rye, which they used to produce their whiskey.

Until the Revolution, many considered whiskey to be a coarse alcohol unfit for human consumption, as many believed that it caused the poor to become raucous and unkempt drunkards. In addition to these alcohol-based products produced in America, imports were seen on merchant shelves, including wine and brandy.

In comparison to the northern colonies, the southern colonies were quite diverse in their agricultural diet. The uplands of Piedmont and the coastal lowlands made up the two main parts of the southern colonies.

The diet of the uplands often included wild game, cabbage, string beans, corn, squashes and white potatoes. People had biscuits as part of their breakfast, along with healthy portions of pork. The lowlands of Louisiana included a varied diet heavily influenced by the French, Spanish, Acadians, Germans, Native Americans, Africans and Caribbeans. Rice played a large part of the diet in Louisiana. In addition, unlike the uplands, the lowlands subsistence of protein came mostly from coastal seafood. Much of the diet involved the use of peppers, as it still does to this day.

During the 18th and 19th centuries, Americans developed many new foods. Some, such as Rocky Mountain oysters, stayed regional; some spread throughout the nation but with little international appeal, such as peanut butter (a core ingredient of the peanut butter and jelly sandwich); and some spread throughout the world, such as popcorn, cola, fried chicken, cornbread, unleavened muffins such as the poppyseed muffin, and brownies.

During the 1800s, American farms were mostly self-sufficient, but certain staples like salt, coffee, sugar, and baking soda would be purchased at the town general store. If the family did not grow wheat, then flour would also be purchased. Another luxury was canned salmon, which was sometimes eaten for Sunday dinner. Items purchased at the general store would be paid for with eggs, butter or some other food from the farm. Women were responsible for much of the processing of food like straining fresh milk, churning butter, making molasses from sorghum, grinding corn into cornmeal or cleaning whole chickens. Fresh picked apples were pressed into cider, which could be fermented to make apple cider vinegar. Fruits and vegetables were preserved by various means like canning, drying or pickling.

One contemporary writer from Michigan described October as cider season, when apple butter would be made. Her writings mention johnnycakes, and, as winter fare, buckwheat cakes.

Typical farmhouse fare included fried chicken, simmered green beans, boiled corn, chicken and dumplings, fried ham, boiled beans and beets, stewed tomatoes, potatoes, and coleslaw made of shredded cabbage. "Pon haus", similar to the scrapple of the Pennsylvania Dutch, was a typical breakfast dish among the Germans who had settled Indiana in the 19th century.

Pork scraps and corn meal were cooked into a thick porridge and molded in loaf pans. Once solidified, the mixture would be cut and fried. During the fall months, pork might be replaced with fried apples or potatoes. It was served with buttered biscuits, jam, jelly, milk gravy or sorghum syrup. Fruit butter might be made from apples, plums or peaches to accompany the meal.

The 20th century revolutionized American cooking, with the advent of many new technologies, and a continued influx of immigrants with unique food traditions.

At the universities, nutritionists and home economists taught a new scientific approach to food. In the early 1900s muckraking journalists raised public concern about the wholesomeness of industrialized food products that contained various preservatives and adulterants of unknown safety. From 1902 to 1912 Harvey Washington Wiley, a chemist at the U.S. Department of Agriculture, supervised "hygienic table trials" to test the safety of food additives and preservatives. His work contributed to the enactment of the Pure Food and Drug Act of 1906. He became the first commissioner of the FDA and later led the laboratories of "Good Housekeeping" Magazine.

During World War I the Progressives' moral advice about food conservation was emphasized in large-scale state and federal programs designed to educate housewives. Large-scale foreign aid during and after the war brought American standards to Europe.

From 1912 to the end of the 1930s researchers discovered and popularized the role of various vitamins and minerals in human health. Starting with iodized salt in 1924, commercially distributed food began to be fortified with vitamins and minerals. In 1932, milk began to be fortified with viosterol, a purified vitamin D2 product. Synthetic thiamin (vitamin B1) first became available after 1936 and bakers began voluntarily enriching bread with high-vitamin yeast or synthetic vitamins in the late 1930s.

The cookware of the period was made of cast iron and these were thoroughly seasoned with pork fat. Fried salt pork with gravy was an indulgent fat-laden dish often served with a side of boiled potatoes. In the Appalachian region a dish called "killed lettuce" was made with pokeweed, dandelion and assorted wild greens that were drizzled with hot bacon grease until wilted or "killed".

Pie could be served up to three times a day and many varieties were prepared depending on the season. During the spring months, pies would be made of rhubarb and strawberry; in summer peach, cherry, blackberry, blueberry, elderberry and grape; and in fall apple.

The staples of the urban diet were bread, dairy and canned goods. Dinner might be tomato bisque from a can topped with cream or a salad made of canned string beans and mayonnaise. Many preferred to purchase food at delicatessens, rather than attempt to prepare meals in the cramped kitchenettes.

German delicatessens in cities like New York and Milwaukee sold imported cold cuts, potato salads, "schmierkase", "wienerwurst", North Sea herring, assorted pickles (pickled cucumber) and other prepared foods. Jewish immigrants from Germany soon followed suit, replacing pork dishes with corned beef (salt-cured beef) and pastrami. Ice cream soda was served at soda fountains, along with various other early "soda water" recipes like the Garden Sass Sundae (rhubarb) or the Oh-Oh-Cindy Sundae (strawberry ice cream topped with chocolate syrup, chopped nuts, whipped cream and candied cherries).

During that same time frame, grain-feeding of cattle during low pasture months made milk increasingly available year-round. The invention of milking machines lowered production costs. Pasteurization, homogenization, evaporation, condensation, and refrigeration along with glass milk bottles, wax-paper cartons, and then plastic bottles made milk increasingly available and safe for urban consumers. Milk became a staple food item and an increasingly important ingredient in American cuisine. Examples include the root beer float and the milkshake.

Pork was a staple of the rural diet through the Southern and Midwestern United States. Lard was used for baking, frying and even as a seasoning.

Major railroads featured upscale cuisine in their dining cars. Restaurant chains emerged with standardized decor and menus, including the Fred Harvey restaurants along the route of the Santa Fe Railroad in the Southwest.

The Food and Nutrition Board of the National Academy of Science established the first set of "Recommended Dietary Allowances" in 1941. In 1943, the US War Foods Administration issued the War Food Order No. 1, which made enriched bread the temporary law of the land.

In 1945 George Stigler published an article on "The cost of subsistence" which described the so-called Stigler diet, his solution to the problem of providing a diet that met the RDA at a minimum cost.

The logistical requirements of the US military during WW2 and the Korean War spurred the development and growth of the processed foods industry in the US. These wars encouraged the production of shelf-stable ingredients processed on a vast industrial scale. Examples include powdered milk, powdered eggs, potato flakes, and frozen concentrated orange juice.

After the war, low-cost, highly processed foods became one of the foundational elements of an era of mass prosperity. Many companies in the American food industry developed new products requiring minimal preparation, such as frozen entrees. One such example is the TV dinner in which a multi-course meal was assembled in aluminum packaging in a food factory and flash frozen, then reheated at home in a thermal oven to be served while watching TV. Convenience foods of the era were designed to simplify home preparation.

One example is macaroni & cheese created using a powdered artificial cheese product that is reconstituted at home with fresh milk. Newspapers and magazines ran recipe columns, aided by research from corporate kitchens, which were major food manufacturers like General Mills, Campbell's, and Kraft Foods. For example, General Mills "Betty Crocker's Cookbook", first published in 1950, was a popular book in American homes.

Highly processed foods of the mid-20th century included novelty elements like multi-colored Jell-O using various chemical food colorings, prepared breakfast cereals marketed to children with large amounts of sugar and artificial colors (e.g. Froot Loops). Fruit-flavored punches made with artificial fruit flavorings (e.g. Tang, Hi-C). Mid-20th-century foods also added novelty packaging elements like spray cheese in an aerosol can, pimento-stuffed olives, and drink pouches.

The development of the microwave oven resulted in the creation of industrial food products and packaging intended to take advantage of the opportunities and overcome the unique challenges of that technology. Microwave popcorn is an example of such a product.

Throughout the second half of the 20th century the US commercial food system has become increasingly dependent on subsidized maize (corn) production to provide feed for livestock and ingredients for human foods such as high-fructose corn syrup. It is estimated that the typical American gets 70 percent of their carbon intake from maize (corn) sources.

The last half of the 20th century saw the development of controversial technological innovations intended to lower the cost of, improve the quality of, or increase the safety of commercial food including: food irradiation, genetically modified organisms, livestock treated with antibiotics/hormones, and concentrated animal feeding operations. Activists have raised concerns about the wholesomeness, safety, or humaneness of these innovations and recommend alternatives such as organic produce, veganism/vegetarianism, and locavore diets.

Fast-food restaurants with standardized product and franchised service models began to appear and spread with the development of the highway system. White Castle (1916) was one of the first examples. Franchising was introduced in 1921 by A&W Root Beer. The McDonald brothers created their "Speedee Service System" in 1948. Other examples include Burger King, KFC, Wendy's, Pizza Hut, Little Caesars, Domino's Pizza and Papa John's Pizza.

One signature characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. For example, spaghetti is Italian, while hot dogs are German; a popular meal, especially among young children, is spaghetti containing slices of hot dogs. Since the 1960s, Asian cooking has played a particularly large role in American fusion cuisine.

Some dishes that are typically considered American have their origins in other countries. American cooks and chefs have substantially altered these dishes over the years, to the degree that the dishes now enjoyed around the world are considered to be American. Hot dogs and hamburgers are both based on traditional German dishes, but in their modern popular form they can be reasonably considered American dishes.

Pizza is based on the traditional Italian dish, brought by Italian immigrants to the United States, but varies highly in style based on the region of development since its arrival. For example, Chicago style has focus on a thicker, taller crust, whereas a "New York Slice" is known to have a much thinner crust which can be folded. These different types of pizza can be advertised throughout the country and are generally recognizable and well-known, with some restaurants going so far as to import New York tap water from a thousand or more miles away to recreate the signature style in other regions.

Some dishes that Americans think of as being of "foreign" in origin and/or associated with a particular immigrant group were in fact invented in America and customized to American tastes. For example General Tso's chicken was invented by Chinese or Taiwanese chefs working in New York in the early 1970s. The dish is unknown in China, except for a distant resemblance to a much spicier dish from Hong Kong said to have influenced the American version. The fortune cookie was likewise invented in California in the early 1900s and is known in Asia only as an American style food.

A wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels like Food Network. Probably the best-known television chef was Child, who taught French cuisine in her weekly show, "The French Chef". By the beginning of the 21st century, regional variations in consumption of meat began to reduce, as more meat was consumed overall. Saying they eat too much protein, the "2015–2020 Dietary Guidelines for Americans" asked men and teenage boys to increase their consumption of underconsumed foods such as vegetables.

During the 1980s, upscale restaurants introduced a mixing of cuisines that contain Americanized styles of cooking with foreign elements commonly referred to as New American cuisine, a type of fusion cuisine combining flavors from the melting pot of traditional American cooking techniques with those from other cultures, sometimes adding molecular gastronomy components.

In the 21st century, vegan and vegetarian meals have increased in popularity, with more restaurants catering to vegans and vegetarians.

In the present day, the modern cuisine of the United States is very regional in nature. Excluding Alaska and Hawaii, the terrain spans from east to west and more than from north to south.

New England cuisine traces its roots to English cuisine and the Native American cuisine of the Abenaki, Narragansett, Niantic, Wabanaki, Wampanoag, and other native peoples. It also includes influences from French, Italian, and Portuguese cuisine, among others. It is characterized by the extensive use of potatoes, beans, dairy products and seafood. Corn, historically the main crop grown by Native American tribes in New England, continues to be grown in all New England states. It is traditionally used in hasty pudding, cornbread and corn chowder. Three prominent foodstuffs native to New England are maple syrup, cranberries and blueberries. Maine is the only state with a commercial wild blueberry industry, with 105 million pounds harvested in 2021.

Initial European colonists came from East Anglia in England. East Anglian cookery included dishes like suet puddings, soda breads, and a few shellfish delicacies, and would have been quite simple in contrast to the dishes prepared in contemporary London. Most of this cuisine was one-pot cookery, which developed into such dishes as succotash, chowder, baked beans, and others.

The most popular starches in New England cuisine include potatoes and cornmeal, and a few native breads like Anadama bread, johnnycakes, bulkie rolls, Parker House rolls, popovers, ployes, and New England brown bread. Because of the influence of New Englander health reformers, the most well known of whom is Sylvester Graham, this region is fairly conservative with its spices, but typical spices include nutmeg, ginger, cinnamon, cloves, and allspice, especially in desserts, and for savory foods, thyme, black pepper, sea salt, and sage. Typical condiments include maple syrup, grown from the native sugar maple, molasses, and cranberry sauce.

The fruits of the region include the "Vitis labrusca" grapes used in grape juice made by companies such as Welch's, along with jelly, Kosher wine by companies like Mogen David and Manischewitz along with other wineries that make higher quality wines. Though not anywhere near as productive a region as the top three apple-producing regions, apples have been a staple of New England foodways since at least the 1640s, and it is here that a very high amount of heirloom varieties are found, many of them gaining renewed interest as part of locavore movements and the re-emergence of cider as a beverage of choice. Apples from New England would include varieties imported from their earliest in Europe and a few natives, like Baldwin, Lady, Mother, Pomme Grise, Porter, Roxbury Russet, Rhode Island Greening, Sops of Wine, Hightop Sweet, Peck's Pleasant, Titus Pippin, Westfield-Seek-No-Further, and Duchess of Oldenburg. Beach plums a small native species with fruits the size of a pinball, are sought after in summer to make into a jam. Cranberries are another fruit indigenous to the region, often collected in autumn in huge flooded bogs. Thereafter they are juiced so they can be drunk fresh for breakfast, or dried and incorporated into salads and quickbreads.

Winter squashes like pumpkin and butternut squashes have been a staple for generations owing to their ability to keep for long periods over icy New England winters and being an excellent source of beta carotene; in summer, they are replaced with pattypan and zucchini, the latter brought to the region by immigrants from Southern Italy a century ago. Blueberries are a very common summertime treat owing to them being an important crop, and find their way into muffins, pies and pancakes.

Historically New England and the other original 13 colonies were major producers of hard cider and the only reason why this changed were that immigrants from Western and Central Europe preferred beer, especially lagers, to apple based alcohol. In more recent years cider has made a roaring comeback nationwide, with New England being the first to break out of the box and with many pomologists scouring the woods for abandoned apple trees and heirloom varieties to add to the cider press. Angry Orchard is a local commercial brand that began in New Hampshire but has since skyrocketed in sales, with other large marques following suit around the land.

Typical favorite desserts are quite diverse, and encompass hasty pudding, blueberry pie, whoopie pies, Boston cream pie, pumpkin pie, Joe Frogger cookies, hand-crafted ice cream, Hermit cookies, and the chocolate chip cookie, invented in Massachusetts in the 1930s.

New England is noted for having a heavy emphasis on seafood, a legacy inherited from coastal tribes like the Wampanoag and Narragansett, who equally used the rich fishing banks offshore for sustenance. Favorite fish include cod, salmon, winter flounder, haddock, striped bass, pollock, hake, bluefish, and, in southern New England, tautog. All of these are prepared numerous ways, such as frying cod for fish fingers, grilling bluefish over hot coals for summertime, smoking salmon or serving a whole poached one chilled for feasts with a dill sauce, or, on cold winter nights, serving haddock baked in casserole dish with a creamy sauce and crumbled breadcrumbs as a top so it forms a crust. Clam cakes, a savory fritter based on chopped clams, are a specialty of Rhode Island. Also, a hard shell clam is unique to Rhode Island called the Quahoag which is used in clear chowders. Farther inland, brook trout, largemouth bass, and herring are sought after, especially in the rivers and icy finger lakes in upper New England where New Englanders will fly fish for them in summertime.

Meat is present though not as prominent, and typically is either stewed in dishes like Yankee pot roast and New England boiled dinner or braised, as in a picnic ham; these dishes suit the weather better as summers are humid and hot but winters are raw and cold, getting below 0 °C for most of the winter and only just above it by March. The roasting of whole turkeys began here as a centerpiece for large American banquets, and like all other East Coast tribes, the Native American tribes of New England prized wild turkeys as a source of sustenance and later Anglophone settlers were enamored of cooking them using methods they knew from Europe: often that meant trussing the bird and spinning it on a string or spit roasting. Today turkey meat is a key ingredient in soups, and also a favorite in several sandwiches like the Pilgrim. For lunch, hot roast beef is sometimes chopped finely into small pieces and put on a roll with salami and American or provolone cheese to make a steak bomb. Bacon is often maple cured, and often bacon or salt pork drippings are an ingredient in corn chowder, a cousin of clam chowder. Veal consumption was prevalent in the North Atlantic States prior to World War II.

A variety of "linguiça" is favored as a breakfast food, introduced by Portuguese fishermen and Brazilian immigrants. Dairy farming and its resultant products figure strongly on the ingredient list, and homemade ice cream is a summertime staple of the region: it was a small seasonal roadside stand in Vermont that eventually became the internationally famous Ben and Jerry's ice cream. Vermont is known for producing farmhouse style cheeses, especially a type of cheddar. The recipe goes all the way back to colonial times when English settlers brought the recipe with them from England and found the rocky landscape eminently suitable to making the cheese. Today Vermont has more artisanal cheese makers per capita than any other state, and diversity is such that interest in goat's milk cheeses has become prominent.

Crustaceans and mollusks are also an essential ingredient in the regional cookery. Maine and Massachusetts, in more recent years, have taken to harvesting peekytoe crab and Jonah crab and making crab bisques, based on cream with 35% milkfat, and crabcakes out of them: often these were overlooked as bycatch of lobster pots by fishermen of the region, but in the past 30 years their popularity has firmly established them as a staple. They even appear on the menu as far south as to be out of the region in New York, where they are sold to four star restaurants in the form of cocktail claws. Whelks are eaten in salad, and lobster, which is indigenous to the coastal waters of the region and are a feature of many dishes, baked, boiled, roasted, and steamed, or simply eaten as a sandwich, chilled with mayonnaise and chopped celery in Maine and Massachusetts, or slathered with melted butter on Long Island and in Connecticut. Shellfish of all sorts are part of the diet, and shellfish of the coastal regions include little neck clams, sea scallops, blue mussels, oysters, soft shell clams, and razor shell clams. Much of this shellfish contributes to New England tradition, the clambake. The clambake as known today is a colonial interpretation of an American Indian tradition.

In summer, oysters and clams are dipped in batter and fried, often served in a basket with french fries, or commonly on a wheaten bun as a clam roll. Oysters are otherwise eaten chilled on a bed of crushed ice on the half shell with mignonette sauce, and are often branded on where they were harvested. Large quahogs are stuffed with breadcrumbs and seasoning and baked in their shells, and smaller ones often find their way into clam chowder. Other preparations include clams casino, clams on the half shell served stuffed with herbs like oregano and streaky bacon.

Southern New England, particularly along the coast, shares many specialties with the Mid-Atlantic, including especially dishes from Jewish and Italian-American cuisine. Coastal Connecticut is known for distinctive kinds of pizza, locally called apizza (pronounced locally as "abeetz"), differing in texture (thin and slightly blackened) and toppings (such as clams) from pizza further south in the so-called pizza belt, which stretches from New Haven, Connecticut southward through New York, New Jersey, and into Maryland.

The mid-Atlantic states comprise the states of New York, New Jersey, Delaware, Pennsylvania, and Northern Maryland. The oldest major settlement in this area of the country is found in the most populous city in the nation, New York, founded in 1625 by the Dutch. Today, it is a major cultural capital of the United States.

The influences on cuisine in this region are extremely eclectic owing to the fact that it has been and continues to be a gateway for international culture as well as a gateway for new immigrants. Going back to colonial times, each new group has left their mark on homegrown cuisine and in turn the cities in this region disperse trends to the wider United States. In addition, cities like New York and Philadelphia have had the past influence of Dutch, Italian, German, Irish, British, and Jewish cuisines, and that continues to this day. Baltimore has become the crossroads between North and South, a distinction it has held since the end of the Civil War.

A global power city, New York is well known for its diverse and cosmopolitan dining scene. Its restaurants compete fiercely for good reviews in the Food and Dining section of "The New York Times", online guides, and Zagat's, the last of which is widely considered the premier American dining guide, published yearly and headquartered in New York.
Many of the more complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak were born out of a need to entertain and impress the well-to-do in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel. Modern commercial American cream cheese was developed in 1872.

Since the first reference to an alcoholic mixed drink called a cocktail comes from New York State in 1803, it is not a surprise that there have been many cocktails invented in New York and the surrounding environs. Even today New York bars are noted for being highly influential in making national trends. Cosmopolitans, Long Island iced teas, Manhattans, Rob Roys, Tom Collins, Aviations, and Greyhounds were all invented in New York bars, and the gin martini was popularized in New York in speakeasies during the 1920s, as evidenced by its appearance in the works of New Yorker and American writer F. Scott Fitzgerald. Like its neighbor Philadelphia, many rare and unusual liquors and liqueurs often find their way into a mixologist's cupboard or restaurant wine list.

New York State is the third most productive area in the country for wine grapes, just behind California and Washington. It has AVA's near the Finger Lakes, the Catskills, and Long Island, and in the Hudson Valley has the second-most productive area in the country for growing apples, making it a center for hard cider production, just like New England. Pennsylvania has been growing rye since Germans began to emigrate to the area at the end of the 17th century and required a grain they knew from Germany. Therefore, overall it is not unusual to find New York grown Gewürtztraminer and Riesling, Pennsylvania rye whiskey, or marques of locally produced ciders like Original Sin on the same menu.
Since their formative years, New York, Philadelphia, and Baltimore have welcomed immigrants of every kind to their shores, and all three have been an important gateway through which new citizens to the general United States arrive. Traditionally natives have eaten cheek to jowl with newcomers for centuries as the newcomers would open new restaurants and small businesses and all the different groups would interact.

Even in colonial days this region was a very diverse mosaic of peoples, as settlers from Switzerland, Wales, England, Ulster, Wallonia, Holland, Gelderland, the British Channel Islands, and Sweden sought their fortune in this region. This is very evident in many signature dishes and local foods, all of which have evolved to become American dishes in their own right.

The original Dutch settlers of New York brought recipes they knew and understood from the Netherlands and their mark on local cuisine is still apparent today: in many quarters of New York their version of apple pie with a streusel top is still baked. In the colony of New Amsterdam, their predilection for waffles in time evolved into the American national recipe and forms part of a New York brunch. They also made coleslaw, originally a Dutch salad, but today accented with the later 18th-century introduction of mayonnaise.

The doughnut began its life originally as a New York pastry that arrived in the 18th century as the Dutch "olykoek", with later additions from other nations of Europe like the Italian "zeppole", the Jewish/Polish "pączki", and the German "Berliner" arriving in the 19th century to complete the variety found in modern doughnuts today.
Crab cakes were once a kind of English "croquette", but over time as spices have been added they and the Maryland crab feast became two of Baltimore's signature dishes. Fishing for blue crab is a favorite summer pastime in the waters off Maryland, New Jersey, and Delaware where they may grace the table at summer picnics.

Other mainstays of the region have been present since the early years of American history, like oysters from Cape May, the Chesapeake Bay, and Long Island, and lobster and tuna from the coastal waters found in New York and New Jersey. Philadelphia Pepper Pot, a tripe stew, was originally a British dish but today is a classic of home cooking in Pennsylvania alongside bookbinder soup, a type of turtle soup.

In the winter, New York pushcarts sell roasted chestnuts, a delicacy dating back to English Christmas traditions, and it was in New York and Pennsylvania that the earliest Christmas cookies were introduced: Germans introduced crunchy molasses-based gingerbread and sugar cookies in Pennsylvania, and the Dutch introduced cinnamon-based cookies, all of which have become part of the traditional Christmas meal.

Scrapple was originally a type of savory pudding that early Pennsylvania Germans made to preserve the offal of a pig slaughter. The Philadelphia soft pretzel was originally brought to Eastern Pennsylvania in the early 18th century, and later, 19th-century immigrants sold them to the masses from pushcarts to make them the city's best-known bread product, having evolved into its own unique recipe.
After the 1820s, new groups began to arrive and the character of the region began to change. There had been some Irish from Ulster prior to 1820, however largely they had been Protestants with somewhat different culture and (often) a different language than the explosion of emigrants that came to Castle Garden and Locust Point in Baltimore in their masses starting in the 1840s.

The Irish arrived in America in a rather woeful state, as Ireland at the time was often plagued by some of the worst poverty in Europe and often heavy disenfranchisement among the masses. Many of them arrived barely alive having ridden coffin ships to the New World, very sick with typhus and gaunt from prolonged starvation.

In addition, they were the first to face challenges other groups did not have: they were the first large wave of Catholics. They faced prejudice for their faith and the cities of Philadelphia, New York, and Baltimore were not always set up for their needs.

For example, Catholic bishops in the U.S. mandated until the 1960s that all Catholics were forbidden from eating red meat on Fridays and during Lent, and attending Mass sometimes conflicted with work as produce and meat markets would be open on high holy days; this was difficult for Irishmen supporting families since many worked as laborers.

Unsurprisingly, many Irishmen also found their fortunes working as longshoremen, which would have given their families access to fish and shellfish whenever a fisherman made berth, which was frequent on the busy docks of Baltimore and New York.

Though there had been some activity in Baltimore in founding a see earlier by the Carrolls, the Irish were the first major wave of Catholic worship in this region, and that meant bishops and cardinals sending away to Europe for wine. Wine, with water, is consecrated as part of the Catholic Mass.

Taverns had existed prior to their emigration to America in the region, though the Irish brought their particular brand of pub culture and founded some of the first saloons and bars that served Dublin style stout and red ale; they brought with them the knowledge of single-malt style whiskey and sold it.

The Irish were the first immigrant group to arrive in this region in massive millions, and these immigrants also founded some of the earliest saloons and bars in this region, of which McSorley's is a still operating example.
It was also in this region that the Irish introduced something that today is a very important festival in American culture that involves a large amount of food, drink, and merry making: Halloween. In England and Wales, where prior immigrants had come from, the feast of All Hallows Eve had died out in the Reformation, dismissed as superstition and excess having nothing to do with the Bible and often replaced with the festival of Guy Fawkes Night. Other immigrant groups like the Germans preferred to celebrate October 31 as Reformation Day, and after the American Revolution all of the above were less and less eager to celebrate the legacy of an English festival given they had fought against Great Britain for their independence.

The Catholicism of the Irish demanded attendance at church on November 1 and charity and deeds, not just faith, as a cornerstone of dogma, and many of their older traditions survived the Reformation and traveled with them. Naturally, they went door-to-door to collect victuals for masked parties as well as gave them out, like nuts to roast on the fire, whiskey, beer, or cider, and barmbracks; they also bobbed for apples and made dumb cakes. Later in the century they were joined by Scots going guising, children going door-to-door to ask for sweets and treats in costume.

From the Mid-Atlantic this trend spread to be nationwide and evolved into American children trick-or-treating on October 31 wearing costumes and their older counterparts having wild costume parties with various foods and drinks such as caramel apples, candy apples, dirt cakes, punch, cocktails, cider (both alcoholic and non,) pumpkin pie, candy corn, chocolate turtles, peanut brittle, taffy, tipsy cake, and copious buckets full of candy; children carving jack-o-lanterns and eating squash derived foods derive from Halloween's heritage as a harvest festival and from Irish and Scottish traditions of carving turnips and eating root vegetables at this time of year.

Bobbing for apples has survived to the present day as a Halloween party classic game, as has a variation on the parlor game of trying to grab an apple hanging from the ceiling blindfolded: it has evolved into trying to catch a donut in one's teeth.

Immigrants from Southern Europe, namely Sicily, Campania, Lazio, and Calabria, appeared between 1880 and 1960 in New York, New Jersey, Pennsylvania, and Eastern Maryland hoping to escape the extreme poverty and corruption endemic to Italy.

Typically none of them spoke English, but rather dialects of Italian and had a culture that was more closely tied to the village they were born in than the high culture only accessible to those who could afford it at this time; many could not read or write in any language.

They were employed in manual labor or factory work but it is because of them that dishes like spaghetti with meatballs, New York–style pizza, calzones, and baked ziti exist, and Americans of today are very familiar with semolina based pasta noodles.

Their native cuisine had less of an emphasis on meat, as evidenced by dishes they introduced like pasta e fagioli and minestrone, but the dishes they created in America often piled it on as a sign of wealth and newfound prosperity since for the first time even cheap cuts of it were affordable. The American recipe for lasagna is proof of this, as mostly it is derived from the Neapolitan version of the dish with large amounts of meat and cheese.
New York–style hot dogs came about with German-speaking emigrants from Austria and Germany, particularly with the frankfurter sausage and the smaller wiener sausage; Jews would also contribute here by introducing the kosher version of these sausages, made of beef rather than pork. Today, the New York–style hot dog with sauerkraut, mustard, and the optional cucumber pickle relish is such a part of the local fabric, that it is one of the favorite comestibles of New York and both the pork and the beef versions are beloved. Hot dogs are a typical street food sold year round in all but the most inclement weather from thousands of pushcarts.

As with all other stadiums in Major League Baseball they are an essential for New York Yankees and the New York Mets games though it is the local style of preparation that predominates without exception.

Hot dogs are also the focus of a televised eating contest on the Fourth of July in Coney Island, at Nathan's Famous, one of the earliest hot dog stands opened in the United States in 1916 by Nathan Handwerker. Handwerker was a Jewish man who emigrated from what is now Ukraine in 1912 and whose influence is felt today around the world.

Coney Island is most famous for being a traditional boardwalk amusement park and the site of the world's first rollercoaster, a precursor of modern theme parks. Hot dogs are a staple of amusement parks 100 years later.

A summertime treat, Italian ice, began its life as a sweeter adaptation of the Sicilian granita that was strictly lemon-flavored and brought to New York and Philadelphia. Its Hispanic counterpart, "piragua", is a common shaved-ice treat brought to New York by Puerto Ricans in the 1930s. Unlike the original dish which included flavors like tamarind, mango, coconut, "piragua" is evolving to include flavors like grape and cherry, fruits which are impossible to grow in the tropical Puerto Rican climate and get exported back to the island from New York.

Taylor Ham, a meat delicacy of New Jersey, first appeared around the time of the Civil War and today is often served for breakfast with eggs and cheese on a kaiser roll, a variant of a roll brought to the area by Austrians in the second half of the 19th century, now commonly used for sandwiches at lunchtime, often topped with poppyseeds. This breakfast meat is generally known as pork roll in southern New Jersey and Philadelphia, and Taylor Ham in northern New Jersey.
Other dishes came about during the early 20th century and have much to do with delicatessen fare, set up largely by Jewish immigrants from Eastern Europe who came to America incredibly poor, often illiterate in any other language but Yiddish, and often banished from mainstream society in their place of origin for centuries. Most often they were completely unable to partake in the outdoor food markets that the general population utilized as most of the food for sale was not kosher.

The influence of European Jewry before their destruction in the Holocaust on modern mid-Atlantic cooking remains strong and reinforced by their many descendants in the region. These currently form the largest concentration of Jews outside Tel Aviv and are very integrated into the local mainstream of New York in particular.

American-style pickles, now a common addition to hamburgers and sandwiches, were brought by Polish Jews, and Austro-Hungarian Jews brought a recipe for almond horns that now is a common regional cookie, diverting from the original recipe in dipping the ends in dark chocolate.

New York–style cheesecake has copious amounts of cream and eggs because animal rennet is not kosher and so could not be sold to a large number of the deli's clientele.

New York inherited its bagels and bialys from Jews, as well as Challah bread. Pastrami first entered the country via Romanian Jews, and is a feature of many sandwiches, often eaten on marble rye, a bread that was born in the mid-Atlantic.

Whitefish salad, lox, and matzoh ball soup are now standard fare made to order at local diners and delicatessens, but started their life as foods that made up a strict dietary code. Rugelach cookies and hamentashen are sweet staples still sold to the general public, but came to New York over a century ago with Ashkenazi Jews along with Jewish rye.
Many of their dishes passed into the mainstream enough that they became standard fare in diners by the end of the 20th century, a type of restaurant that is now the most common in the region, and the subject matter of the artist Edward Hopper.

In the past this sort of establishment was the haven of the short-order cook grilling or frying simple foods for the working man. Today typical service includes staples from this large region like beef on weck, Manhattan clam chowder, the club sandwich, Buffalo wings, Philadelphia cheesesteak, the black and white cookie, shoofly pie, snapper soup, Smith Island cake, blackout cake, grape pie, milkshakes, and the egg cream, a vanilla or chocolate fountain drink with a frothy top and fizzy taste.

As in Hopper's painting from 1942, many of these businesses are open 24 hours a day.

This region today comprises the states near the Great Lakes and also the Great Plains; much of it is prairie with very flat terrain. Winters are bitterly cold, windy, and wet.
Midwestern cuisine today is a very eclectic and odd mix and match of foodways, covering everything from Kansas City–style barbecue to the Chicago-style hot dog, though many of its classics are very simple, hearty fare.

This region was mostly untouched by European and American settlers until after the American Revolutionary War, and excepting Missouri and the heavily forested states near the Great Lakes, was mainly populated by nomadic tribes like the Sioux, Osage, Arapaho, and Cheyenne. As with most other American Indian tribes, these tribes consumed the Three Sisters of beans, maize, and squash, but also for thousands of years followed the herds of bison, hunting them on foot and later on horseback, typically using bow and arrow. There are buffalo jumps dating back nearly 10,000 years and several photographs and written accounts of trappers and homesteaders attesting to their dependence on the buffalo and to a lesser degree elk.

After nearly wiping out elk and bison, this region has taken to raising bison alongside cattle for their meat and at an enormous profit, making them into burgers and steaks.

Often that means harsh blizzards especially near the Great Lakes where Arctic winds blow off of Canada, where ice on rivers and lakes freezes thick enough for ice hockey, and for ice fishing for pike, walleye and panfish to be ubiquitous. In Minnesota, Wisconsin, and Michigan, they often become part of the local fish fry.

The primary meats here are beef and poultry, since the Midwest has been raising turkeys, chickens, and geese for over 150 years. Chickens have been common for so long that the Midwest has several native breeds that are prized for both backyard farming and for farmer's markets, such as the Buckeye and Wyandotte. One, Billina, appears as a character in the second book of the Oz series by L. Frank Baum.

Favorite fruits of the region include some native plants inherited from Native American tribes like the pawpaw, and American persimmons are also highly favored.

As in the American South, pawpaws are the region's largest native fruit, about the size of a mango, often found growing wild come September; they are made into preserves and cakes and command quite a price at farmer's markets in Chicago.

The American persimmon is often smaller than its Japanese cousin, about the size of a small plum, but in the Midwest and parts of the East it is the main ingredient in the steamed persimmon pudding, topped with "crème anglaise".

Other crops inherited from the Native Americans include wild rice, which grows on the banks of lakes and is a local favorite for fancy meals and today often used in stuffing for Thanksgiving.

Typical fruits of the region are cold-weather crops. Once it was thought that its winters were too harsh for apples, but a breeder in Minnesota produced the Wealthy apple and it became the third-most productive region for apple growing in the country, with local varieties comprising Wolf River, Enterprise, Melrose, Paula Red, Rome Beauty, Honeycrisp, and the Red Delicious.

Cherries are important to Michigan and Wisconsin grows many cranberries, a legacy of early-19th-century emigration of New England farmers. Crabapple jelly is a favorite condiment of the region.

The influence of German, Scandinavian, and Slavic peoples on the northern portion of the region is very strong; many emigrated to Wisconsin, Minnesota, Michigan, Ohio, and Illinois in the 19th century to take advantage of jobs in the meatpacking business as well as being homesteaders and tradesmen.

Bratwurst is a very common sausage eaten at tailgate parties for the Green Bay Packers, Chicago Bears, or Detroit Lions, often served boiled in lager beer with sauerkraut, different from many of the recipes currently found in Germany.

Polish sausage, in particular a locally invented type of kielbasa, is essential for sporting events in Chicago: Chicago today has approximately 200,000 Polish speakers and has had a similar population for over 100 years.

When Poles came to Chicago and surrounding cities from Europe, they brought with them long ropes of kielbasa, cabbage rolls, and pierogi. Poles that left Poland after the fall of the Berlin Wall and descendants of earlier immigrants still make them, and they remain common in local diners and delis.

Today alongside the pierogi, the sausage is served on a long roll with mustard like a hot dog or as a Maxwell Street Polish, a sandwich with caramelized onions. In Cleveland, the same sausage is served in the form of the Polish boy, a sandwich made of french fries, spicy barbecue sauce, and coleslaw.

Unlike cities in the East where the hot dog alone is traditional, fans of the Cleveland Indians, Detroit Tigers, Chicago Cubs, Chicago White Sox, and Milwaukee Brewers favor two or three different kinds of sausage sold in the pushcarts outside the stadium.

The hot dogs themselves tend to follow the Chicago style, with mustard and pickled vegetables.

In Cincinnati, where the Cincinnati Reds play, there is a competitor in Cincinnati chili. Invented by Macedonian immigrants, it includes spaghetti as its base, chili with a Mediterranean-inspired spice mix, and cheddar cheese; the chili itself is often a topping for local hot dogs at games.

In the Midwest and especially Minnesota, the tradition of the church potluck is a gathering where local foods reign, and has been since the era of the frontier; pioneers often needed to pool resources to have a celebration in the 19th century and that simply never changed.

Nowhere is this more clear than with the hotdish, a type of casserole believed to have derived from a Norwegian recipe, it is usually topped with potatoes or tater tots. Next to the hotdish at potlucks usually glorified rice is found, a kind of rice pudding mixed with crushed pineapple and maraschino cherries. Next to that is the booyah, a thick soup made of meat, vegetables, and seasonings that is meant to simmer on the stove for up to two days.

Lefse, traditionally a Scandinavian flatbread, has been handed down to descendants for over a hundred years and is common on the table. Behind that is venison, a popular meat around the Great Lakes and often eaten as steaks, sandwiches, and crown roasts for special events. Within Wisconsin, Minnesota and the Dakotas, tiger meat, a dish similar to steak tartare, is common.

Last on the table are the dessert bars and brownies, created originally in 1898 in Chicago, now a global food and international favorite.
Further south, barbecue has its own style in places in Kansas City and St. Louis different from the South and American West. Kansas City and St. Louis were and are important hubs for the railroad that connected the plains with the Great Lakes and cities farther east, like Philadelphia.

At the turn of the 19th century, the St. Louis area, Omaha, and Kansas City had huge stockyards, waystations for cattle and pigs on their way east to the cities of the coast and north to the Great Lakes. They all had large growing immigrant and migrant populations from Europe and the South respectively, so the region has developed unique styles of barbecue.

St. Louis–style barbecue favors a heavy emphasis on a sticky sweet barbecue sauce. Its standbys include the pork steak, a cut taken from the shoulder of the pig, grilled then slowly stewed in a pan over charcoal; crispy snoots, a cut from the cheek and nose of the pig that is fried up like cracklins and eaten dipped in sauce; pork spare ribs; and a mix of either beer-boiled bratwurst or grilled Italian sausage, flavored with fennel.

Dessert is usually something like gooey butter cake, invented in the city in the 1930s.

Kansas City–style barbecue uses several different kinds of meat, more than most styles of American barbecue—turkey, mutton, pork, and beef to name a few—but is distinct from St. Louis in that the barbecue sauce adds molasses in with the tomato-based recipe and typically has a more tart taste.

Traditionally, Kansas City uses a low-and-slow method of smoking the meat in addition to just stewing it in the sauce. It also favors using hickory wood for smoking and continual watering or layering of the sauce while cooking to form a glaze; with burnt ends this step is necessary to create the "bark" or charred outer layer of the brisket.

When referring to the American South as a region, typically it should indicate Southern Maryland and the states that were once part of the Old Confederacy, with the dividing line between the East and West jackknifing about 100 miles west of Dallas, Texas, and mostly south of the old Mason–Dixon line. Cities found in this area include New Orleans, Atlanta, Washington, D.C., Memphis, Charleston, and Charlotte with Houston, Texas being the largest. The Florida Panhandle is usually considered part of the South, but the Florida peninsula (especially its lower half) is not.

These states are much more closely tied to each other and have been part of U.S. territory for much longer than states much farther west than East Texas, and in the case of food, the influences and cooking styles are strictly separated as the terrain begins to change to prairie and desert from bayou and hardwood forest.
This section of the country has some of the oldest known U.S. foodways, with some recipes almost 400 years old.

Native American influences are still quite visible in the use of cornmeal as an essential staple and found in the Southern predilection for hunting wild game, in particular wild turkey, deer, woodcock, and various kinds of waterfowl; for example, coastal North Carolina is a place where hunters will seek tundra swan as a part of Christmas dinner; the original English and Scottish settlers would have rejoiced at this revelation owing to the fact that such was banned among the commoner class in what is now the United Kingdom, and naturally, their descendants have not forgotten.

Native Americans also consumed turtles and catfish, specifically the snapping turtle, the alligator snapping turtle, and blue catfish. Catfish are often caught with one's bare hands, gutted, breaded, and fried to make a Southern variation on English fish and chips and turtles are turned into stews and soups.

Native American tribes of the region such as the Cherokee or Choctaw often cultivated or gathered local plants like pawpaw, maypop and several sorts of squashes and corn as food. They also used spicebush and sassafras as spices, and the aforementioned fruits are still cultivated as food in the South.

Maize is to this day found in dishes for breakfast, lunch and dinner in the form of grits, hoecakes, baked cornbread, and spoonbread, and nuts like the hickory, black walnut and pecan are commonly included in desserts and pastries as varied as mince pies, pecan pie, pecan rolls and honey buns (both are types of sticky bun), and quick breads, which were themselves invented in the South during the American Civil War.

Peaches have been grown in this region since the 17th century and are a staple crop as well as a favorite fruit, with peach cobbler being a signature dessert.

European influence began soon after the settlement of Jamestown in 1607 and the earliest recipes emerged by the end of the 17th century. Specific influences from Europe were quite varied, and they remain traditional and essential to the modern cookery overall.

German speakers often settled in the Piedmont on small farms from the coast, and invented an American delicacy that is now nationally beloved, apple butter, based on their recipe for "apfelkraut", and later they introduced red cabbage and rye.

From the British Isles, an enormous amount of influence was bestowed upon the South, specifically foodways from 17th- and 18th-century Ulster, the borderlands between England and Scotland, the Scottish Highlands, portions of Wales, the West Midlands, the West Country, Black Country and Southern England. Settlers bound for America fled the tumult of the Civil War, Ulster and the Highland Clearances.

Often ships' manifests show their belongings nearly always included cookpots or bakestones and seed stock for plants like peaches, plums, and apples to grow orchards which they planted in their hundreds. Each group brought foods and ideas from their respective regions.

Settlers from Ireland and Scotland were well known for creating "peatreak" and "poitín", strong hard liquor based on fermenting potatoes or barley. In time they came up with a method for distilling a corn mash with added sugar and aging in charred barrels made of select hardwoods, which created a whiskey with a high proof. This gave birth to American whiskey and Kentucky bourbon, and its cousins moonshine and Everclear.

Closer to the coast, 18th-century recipes for English trifle turned into tipsy cakes, replacing the sherry with whiskey and their recipe for pound cake, brought to the South around the same time, still works with American baking units: one pound sugar, one pound eggs, one pound butter, one pound flour.

Pork is the popular choice in 80% of Southern style barbecue and features in other preparations like sausages and sandwiches. For most Southerners in the antebellum period, corn and pork were staples of the diet. Country sausage is an ingredient in the Southern breakfast dish of biscuits and gravy. Country ham is often served for breakfast and cured with salt or sugar and hickory-smoked.

Accompanying many meals is the southern style fluffy biscuit, where the leavening agent is baking soda and often includes buttermilk, and for breakfast they often accompany country ham, grits, and scrambled eggs.

Desserts in the South tend to be quite rich and very much a legacy of entertaining to impress guests, since a Southern housewife was (and to a degree still is) expected to show her hospitality by laying out as impressive a banquet as she is able to manage.

Desserts are vast and encompass Lane cake, sweet potato pie, peach cobbler, pecan pie, hummingbird cake, Jefferson Davis pie, peanut brittle, coconut cake, apple fritters, peanut cookies, Moravian spice cookies, chess pie, doberge cake, Lady Baltimore cake, bourbon balls, and caramel cake.
American-style sponge cakes tend to be the rule rather than the exception as is American buttercream, a place where Southern baking intersects with the rest of the United States. Nuts like pecan and hickory tend to be revered as garnishes for these desserts, and they make their way into local bakeries as fillings for chocolates.

In Louisiana, cooking methods have more in common with rustic French cuisines of the 17th and 18th century than anything ever found at the French court in Versailles or the bistros of 19th- and 20th-century Paris; this is especially true of Cajun cuisine.

Cajun French is more closely related to dialects spoken in Northern Maine, New Brunswick, and to a lesser degree Haiti than anything spoken in modern France, and likewise their terminology, methodology, and culture concerning food is much more closely related to the styles of these former French colonies even today.

Unlike other areas of the South, Cajuns were and still are largely Catholics and thus much of what they eat is seasonal; for example pork is an important component of the Cajun "boucherie" (a large community event where the hog is butchered, prepared with a fiery spice mix, and eaten snout to tail) but it is never consumed in the five weeks of Lent, when such would be forbidden.

Cajun cuisine tends to focus on what is locally available, historically because Cajuns were often poor, illiterate, independent farmers and not plantation owners but today it is because such is deeply imbedded in local culture.

"Boudin" is a type of sausage found only in this area of the country, and it is often by far more spicy than anything found in France or Belgium. "Chaudin" is unique to the area, and the method of cooking is comparable to the Scottish dish haggis: the stuffing includes onions, rice, bell peppers, spices, and pork sewn up in the stomach of a pig, and served in slices piping hot.

Crawfish are a staple of the Cajun grandmother's cookpot, as they are abundant in the bayous of Southern Louisiana and a main source of livelihood, as are blue crabs, shrimp, corn on the cob, and red potatoes, since these are the basic ingredients of the Louisiana crawfish boil.
New Orleans has been the capital of Creole culture since before Louisiana was a state. This culture is that of the colonial French and Spanish that evolved in the city of New Orleans, which was and still is quite distinct from the rural culture of Cajuns and dovetails with what would have been eaten in antebellum Louisiana plantation culture long ago.

Cooking to impress and show one's wealth was a staple of Creole culture, which often mixed French, Spanish, Italian, German, African, Caribbean and Native American cooking methods, producing rich dishes like oysters bienville, pompano en papillote, and even the muffaletta sandwich.

However, Louisiana Creole cuisine tends to diverge from the original ideas brought to the region in ingredients: profiteroles, for example, use a near identical choux pastry to that which is found in modern Paris but often use vanilla or chocolate ice cream rather than custard as the filling, pralines nearly always use pecan and not almonds, and bananas foster came about when New Orleans was a key port for the import of bananas from the Caribbean Sea.

Gumbos tend to be thickened with okra, or the leaves of the sassafrass tree. "Andouille" is often used, but not the "andouille" currently known in France, since French "andouille" uses tripe whereas Louisiana "andouille" is made from a Boston butt, usually inflected with pepper flakes, and smoked for hours over pecan wood.

Other ingredients that are native to Louisiana and not found in the cuisine of modern France would include rice, which has been a staple of both Creole and Cajun cooking for generations, and sugarcane, which has been grown in Louisiana since the early 1800s.
Ground cayenne pepper is a key spice of the region, as is the meat of the American alligator, something settlers learned from the Choctaws and Houma. The maypop plant has been a favorite of Southerners for 350 years; it gives its name to the Ocoee River in Tennessee, a legacy of the Cherokees, and in Southern Louisiana it is known as "liane de grenade", indicating its consumption by Cajuns. It is a close relative of the commercial passionfruit, similar in size, and is a common plant growing in gardens all over the South as a source of fresh summertime fruit.

West African influences came with enslaved peoples from Ghana, Benin, Mali, Congo, Angola, Sierra Leone, Nigeria, and other portions of the Gold Coast, and the mark Africans and their descendants, the African Americans, have made on Southern food is strong today and an essential addition to the Southern table.

Crops like okra, sorghum, sesame seeds, eggplant, and many different kinds of melons were brought with them from West Africa along with the incredibly important introduction of rice to the Carolinas and later to Texas and Louisiana, whence it became a staple grain of that region and still remains a staple in those areas today, found in dishes like Hoppin John, purloo, and Charleston red rice.

Like the poorer indentured servants that came to the South, slaves often got the leftovers of what was slaughtered for the consumption of the master of the plantation and so many recipes had to be adapted for offal, like pig's ears and fatback though other methods encouraged low and slow methods of cooking to tenderize the tougher cuts of meat, like braising, smoking, and pit roasting, the last of which was a method known to West Africans in the preparation of roasting goat.

Peanut soup is one of the oldest known recipes brought to Virginia by Africans and over time, through their descendants, it has become creamier and milder tasting than the original.

Certain portions of the South often have their own distinct subtypes of cuisine owing to local history and landscape. Floridian cuisine, for example, has a distinct way of cooking that includes different ingredients, especially south of Tampa and Orlando.

Spain had control of the state until the early 19th century and used the southern tip as an outpost to guard the Spanish Main beginning in the 1500s, but Florida kept and still maintains ties with the Caribbean Sea, including the Bahamas, Haiti, Cuba, Puerto Rico, the Dominican Republic, and Jamaica.

South of Tampa, there are and have been for a long time many speakers of Caribbean Spanish, Haitian French, Jamaican Patois, and Haitian Creole and each Caribbean culture has a strong hold on cooking methods and spices in Florida. In turn, each mixes and matches with the foodways of the Seminole tribe and Anglophone settlers. Thus, for almost 200 years, Floridian cooking has had a more tropical flavor than any other Southern state.

Allspice, a spice originally from Jamaica, is an ingredient found in spice mixes in summer barbecues along with ginger, garlic, scotch bonnet peppers, sea salt, and nutmeg; in Floridian cooking this is often a variant of Jamaican jerk spice. Coconuts are grown in the areas surrounding Miami and are shipped in daily through its port for consumption of the milk, meat, and water of the coconut.

Bananas are not just the yellow Cavendish variety found in supermarkets across America: in Florida they are available as "bananitos", "colorados", "plátanos", and "maduros". The first of these is a tiny miniature banana only about 4-5 inches (10–13 cm) in length and it is sweet. The second has a red peel and an apple-like aftertaste, and the third and fourth are used as a starch on nearly every Caribbean island as a side dish, baked or fried: all of the above are a staple of Florida outdoor markets when in season and all have been grown in the Caribbean for almost 400 years.

Mangoes are grown as a backyard plant in Southern Florida and otherwise are a favorite treat coming in many different shapes in sizes from "Nam Doc Mai", brought to Florida after the Vietnam War, to "Madame Francis", a mango from Haiti. Sweetsop and soursop are popular around Miami, but nearly unheard of in other areas of the South.

Citrus is a major crop of Florida, and features at many breakfast tables and many markets, with the height of the season near the first week of January. Hamlin oranges are the main cultivar planted, and from this crop the rest of the United States and to a lesser extent Europe gets orange juice. Other plantings would include grapefruits, tangerines, clementine oranges, limes, and even a few more rare ones, like cara cara navels, tangelos, and the Jamaican Ugli fruit. Tomatoes, bell peppers, habanero peppers, and figs, especially taken from the Florida strangler fig, complete the produce menu.

Blue crab, conch, Florida stone crab, red drum, dorado, and marlins tend to be local favorite ingredients. Dairy is available in this region, but it is less emphasized due to the year round warmth.

Traditional key lime pie, a dessert from the islands off the coast of Miami, is made with condensed milk to form the custard with the eye wateringly tart limes native to the Florida Keys in part because milk would spoil in an age before refrigeration.

Pork in this region tends to be roasted in methods similar to those found in Puerto Rico and Cuba, owing to mass emigration from those countries in the 20th century, especially in the counties surrounding Miami.

Orange blossom honey is a specialty of the state, and is widely available in farmer's markets. Caribbean lobster is a favorite special meal eagerly sought after by Floridians as it is found as far north as Fort Myers: spear diving and collecting them from reefs in the Florida Keys and near rocky shoals is a common practice of local scuba divers.

Ptarmigan, grouse, crow, blackbirds, dove, duck and other game fowl are consumed in the United States. In the American state of Arkansas, beaver tail stew is consumed in Cotton town. Squirrel, raccoon, possum, bear, muskrat, chipmunk, skunk, groundhog, pheasant, armadillo and rabbit are also consumed in the United States.

Cooking in the American West gets its influence from Native American and Hispanophone cultures, as well as later settlers that came in the 19th century: Texas, for example, has some influence from Germany in its choice of barbecue by using sausages.

Another instance can be found in the Northwestern region, which encompasses Oregon, Washington, and Northern California. All of the aforementioned rely on local seafood and a few classics of their own.

In New Mexico, Colorado, Nevada, Arizona, Utah, West Texas, and Southern California, Mexican flavors and influences are extremely common, especially from the Mexican states of Chihuahua, Baja California, and Sonora.

The Pacific Northwest as a region includes Alaska and the state of Washington near the Canada-US border and terminates near Sacramento, California. Here, the terrain is mostly temperate rainforest on the coast mixed with pine forest as one approaches the Canada-US border inland.

One of the core favorite foodstuffs is Pacific salmon, native to many of the larger rivers of the area and often smoked or grilled on cedar planks. In Alaska, wild game like ptarmigan and moose meat feature extensively since much of the state is wilderness.

Fresh fish like steelhead trout, Pacific cod, Pacific halibut, and pollock are fished for extensively and feature on the menu of many restaurants, as do a plethora of fresh berries and vegetables, like Cameo apples from Washington state, the headquarters of the U.S. apple industry, cherries from Oregon, blackberries, and marionberries, a feature of many pies. Hazelnuts are grown extensively in this region and are a feature of baking, such as in chocolate hazelnut pie, an Oregon favorite, and Almond Roca is a local candy.

Like its counterpart on the opposite coast to the East, there is a grand variety of shellfish in this region. Geoducks are a native species of giant clam that have incredibly long necks; they are eaten by the bucketful and shipped to Asia for millions of dollars as they are believed to be an aphrodisiac. Gaper clams are a favorite food, often grilled or steamed in a sauce.

Native California abalone is protected as a food source, and a traditional foodway predating settlement by whites, today featuring heavily in the cooking of fine restaurants as well as in home cooking, in mirin-flavored soups (the influence of Japanese cooking is strong in the region) noodle dishes and on the barbecue.

Native Olympia oysters are served on the half shell as well as the Kumamoto oyster, introduced by Japanese immigrants and a staple at dinner as an appetizer.

California mussels are a delicacy of the region, and have been a feature of the cooking for generations. There is evidence that Native American tribes consumed them up and down the California coast for centuries.

Crabs are a delicacy, and included in this are Alaskan king crab, red crab, yellow crab, and Dungeness crab. Californian and Oregonian sportsmen pursue the last three extensively using hoop nets, and prepare them in a multitude of ways.

Alaskan king crab, able to grow as large as 10 kg, is often served steamed for a whole table with lemon-butter sauce or put in chunks of salad with avocado, and native crabs are the base of dishes like the California roll, cioppino, a tomato-based fisherman's stew, and Crab Louie, another kind of salad native to San Francisco.

Favorite grains are mainly wheat, and the region is known for sourdough bread. Cheeses of the region include Humboldt Fog, Cougar Gold and Teleme.

The states of the Four Corners (Arizona, New Mexico, Colorado, and Utah) plus Nevada, Southern California, and West Texas make up a large chunk of the United States.

There is a distinct Hispanic accent to the cookery here, with each having cultural capitals in Albuquerque, Denver, Las Vegas, Los Angeles, Phoenix, Santa Fe, San Diego, and Tucson.

For centuries, prior to California's statehood in the 1850s, it was part of the Spanish Empire, namely Alta California (modern California), Santa Fe de Nuevo México (modern New Mexico), and Tejas (modern Texas). Today it is home of a large population of Native Americans, Hispanos, descendants of the American frontier, Asian Americans, and immigrants from Mexico and Latin America.

California, New Mexico, and Texas continue to hold their unique identities which is reflected in their distinct regional cuisines, the multiple cuisines of California, New Mexican cuisine, Texan cuisine, and Tex-Mex. Spanish is a commonly spoken secondary language here; the state of New Mexico has its own distinct dialect.

With the exception of Southern California, the signature meat is beef, since this is one of the two regions in which cowboys lived and modern cattle ranchers still eke out their living today. High-quality beefstock is a feature that has been present in the region for more than 200 years and the many cuts of beef are unique to the United States. These cuts of meat are different from the related Mexican cuisine over the border in that certain kind of offal, like "lengua" (tongue) "cabeza" (head) and "tripas" (tripe) are considered less desirable and are thus less emphasized. Typical cuts would include the ribs, brisket, sirloin, flank steak, skirt steak, and t-bone.
Historically, Spanish settlers that came to the region found it completely unsuitable to the mining operations that much older settlements in Mexico had to offer as their technology was not advanced enough to extract the silver that would later be found. They had no knowledge of the gold in California, which would not be found until 1848, and knew even less about the silver in Nevada, undiscovered until after the Civil War.

Instead, in order to make the pueblos prosper, they adapted the old rancho system of places like Andalusia in Spain and brought the earliest beefstock, among these were breeds that would go feral and become the Texas longhorn, and Navajo-Churro sheep, still used as breeding stock because they are easy to keep and well adapted to the extremely arid and hot climate, where temperatures easily exceed 38 °C.

Later, cowboys learned from their management practices, many of which still stand today, like the practical management of stock on horseback using the Western saddle.
Likewise, settlers learned the cooking methods of those who came before and local tribes as well, for example, portions of Arizona and New Mexico still use the aforementioned beehive shaped clay contraption called an "horno", an outdoor wood-fired oven both Native American tribes like the Navajo and Spaniards used for roasting meat, maize, and baking bread.

Meats that see frequent use are elk meat, a favorite in crown roasts and burgers, and nearer the Mexican border rattlesnake, often skinned and stewed.

The taste for alcohol tends toward light and clean flavors found in tequila, a staple of this region since the days of the Wild West and a staple in the bartender's arsenal for cocktails, especially in Las Vegas. In Utah, a state heavily populated by Mormons, alcohol is frowned upon by the Church of Jesus Christ of Latter-day Saints but still available in area bars in Salt Lake City, mainly consumed by the populations of Catholics and other Protestant denominations living there.

Introduction of agriculture was limited prior to the 20th century and the development of better irrigation techniques, but included the addition of peaches, a crop still celebrated by Native American tribes like the Havasupai, and oranges. Today in Arizona, Texas, and New Mexico the favored orange today is the Moro blood orange, which often finds its way into the local cuisine, like cakes and marmalade.

Pine nuts are a particular regional specialty and feature often in fine dining and cookies; in Nevada the Native American tribes that live there are by treaty given rights to exclusive harvest, and in New Mexico they reserve usage of the term "piñon" for certain species of indigenous pine nuts.

From Native Americans, Westerners learned the practice of eating cactus fruit from the myriad species of opuntia that occupy the Chihuahuan, Sonoran, and Mojave desert lands. In California, Spanish missionaries brought with them the mission fig, and today this fruit is a delicacy.
Cuisine in this region tends to have certain key ingredients: tomatoes, onions, black beans, pinto beans, rice, bell peppers, chile peppers, and cheese, in particular Monterey Jack, invented in Southern California in the 19th century and itself often further altered into pepper Jack where spicy jalapeño peppers are incorporated into the cheese to create a smoky taste.

Chili peppers play an important role in the cuisine, with a few native to the region. This is especially true with the region's distinct New Mexico chile pepper, still grown by Hispanos of New Mexico and Puebloans the most sought after of which come from the Hatch valley, Albuquerque's Central Rio Grande, Chimayo, and Pueblos.

In New Mexico, chile is eaten on a variety of foods, such as the green chile cheeseburger, made popular by fast food chains such as Blake's Lotaburger. Indeed, even national fast food chains operating in the state, such as McDonald's, offer locally grown chile on many of their menu items.

In the 20th century a few more recent additions have arrived like the poblano pepper, rocoto pepper, ghost pepper, thai chili pepper, and Korean pepper, the last three especially when discussing Southern California and its large population from East and South Asia.

Cornbread is consumed, however the recipe differs from ones in the East in that the batter is cooked in a cast-iron skillet.

Outdoor cooking is popular and still utilizes an old method settlers brought from the East with them, in which a cast-iron Dutch oven is covered with the coals of the fire and stacked or hung from a tripod: this is different from the earthenware pots of Mexico.

Tortillas are still made the traditional way here and form an important component of the spicy breakfast burrito, which contains ham, eggs, and salsa or "pico de gallo". They are also used for regular burritos, which contains any combination of marinated meats, vegetables, and piquant chilis; smothered burritos, often both containing and topped with New Mexico chile sauces; quesadillas, a much loved grilled dish where cheese and other ingredients are stuffed between two tortillas and served by the slice; and steak fajitas, where sliced skirt steak sizzles in a skillet with caramelized onions.
Unlike Mexico, tortillas of this region also may incorporate vegetables like spinach into the flatbread dough to make wraps, which were invented in Southern California. Food here tends to use pungent spices and condiments, typically "chili verde" sauce, various kinds of hot sauce, sriracha sauce, chili powder, cayenne pepper, white pepper, cumin, paprika, onion powder, thyme and black pepper. Nowhere is this fiery mix of spice more evident than in the dishes chili con carne, a meaty stew, and cowboy beans, both of which are a feature of regional cookoffs.

Southern California has several additions like five spice powder, rosemary, curry powder, kimchi, and lemongrass, with many of these brought by recent immigration to the region and often a feature of Southern California's fusion cuisine, popular in fine dining.

In Texas, the local barbecue is often entirely made up of beef brisket or large rib racks, where the meat is seasoned with a spice rub and cooked over coals of mesquite. In other portions of the state they smoke the meat and peppery sausages over high heat using pecan, apple, and oak wood and serve it with a side of pickled vegetables, a legacy of German and Czech settlers of the late 1800s.

California is home to Santa Maria–style barbecue, where the spices involved generally are black pepper, paprika, and garlic salt, and grill over the coals of coast live oak.
Native American additions may include Navajo frybread and corn on the cob, often roasted on the grill in its husk. A typical accompaniment or appetizer of all these states is the tortilla chip, which sometimes includes cornmeal from cultivars of corn that are blue or red in addition to the standard yellow of sweetcorn, and is served with salsa of varying hotness.

Tortilla chips also are an ingredient in the Tex Mex dish nachos, where these chips are loaded with any combination of ground beef, melted Monterey Jack, cheddar, or Colby cheese, guacamole, sour cream, and salsa, and Texas usually prefers a version of potato salad as a side dish.

For alcohol, a key ingredient is tequila: this spirit has been made on both sides of the US-Mexican border for generations, and in modern cuisine it is a must-have in a bartender's arsenal as well as an addition to dishes for sauteeing.

Southern California is focused more towards the coast and has had more contact with immigration from the West Pacific and Baja California, in addition to having the international city of Los Angeles as its capital. Here, the prime mode of transportation is by car.

Drive through fast food was invented in this area, but so was the concept of the gourmet burger movement, giving birth to chains like In-N-Out Burger, with many variations of burgers including chili, multiple patties, avocado, special sauces, and Angus or wagyu beef. Common accompaniments include thick milkshakes in various flavors like mint, chocolate, peanut butter, vanilla, strawberry, and mango.

Smoothies are a common breakfast item made with fresh fruit juice, yogurt, and crushed ice. "Agua fresca", a drink originated by Mexican immigrants, is a common hot-weather beverage sold in many supermarkets and at mom and pop stands, available in citrus, watermelon, and strawberry flavors; the California version usually served chilled without grain in it.
The weather in Southern California is such that the temperature rarely drops below in winter, thus, sun-loving crops like pistachios, kiwifruit, avocadoes, strawberries, and tomatoes are staple crops of the region, the last often dried in the sun and a feature of salads and sandwiches.

Olive oil is a staple cooking oil of the region and has been since the days of Junípero Serra; today the mission olive is a common tree growing in a Southern Californian's back garden. As a crop olives are increasingly a signature of the region along with Valencia oranges and Meyer lemons.

Soybeans, bok choy, Japanese persimmon, thai basil, Napa cabbage, nori, mandarin oranges, water chestnuts, and mung beans are other crops brought to the region from East Asia and are common additions to salads as the emphasis on fresh produce in both Southern and Northern California is strong.

Other vegetables and herbs have a distinct Mediterranean flavor which would include oregano, basil, summer squash, eggplant, and broccoli, with all of the above extensively available at farmers' markets all around Southern California.

Naturally, salads native to Southern California tend to be hearty affairs, like Cobb salad and Chinese chicken salad, and dressings like green goddess and ranch are a staple.

California-style pizza tends to have disparate ingredients with an emphasis on vegetables, with any combination of chili oil, prawns, eggs, chicken, shiitake mushrooms, olives, bell pepper, goat cheese, and feta cheese. Peanut noodles tend to include a sweet dressing with lo mein noodles and chopped peanuts.

Fresh fish and shellfish in Southern California tends to be expensive in restaurants, but every year since the end of WWII, the Pismo clam festival has taken place where the local population takes a large species of clam and bakes, stuffs, and roasts it as it is a regional delicacy.

Fishing for pacific species of octopus and the Humboldt squid are common, and both are a feature of East Asian and other L.A. fish markets. Lingcod is a coveted regional fish that is often caught in the autumn off the coast of San Diego and in the Channel Islands and often served baked. California sheephead are often grilled and are much sought after by spear fishermen and the immigrant Chinese population, in which case it is basket steamed.

Most revered of all in recent years is the California spiny lobster, a beast that can grow to 44 lb, and is a delicacy that now rivals the fishery for Dungeness crab in its importance.

Hawaii is often considered to be one of the most culturally diverse U.S. states, as well as being the only state with an Asian-majority population and one of the few places where United States territory extends into the tropics. As a result, Hawaiian cuisine borrows elements of a variety of cuisines, particularly those of Asian and Pacific-rim cultures, as well as traditional native Hawaiian and a few additions from the American mainland.

American influence in the last 150 years has brought cattle, goats, and sheep to the islands, introducing cheese, butter, and yogurt products, as well as crops like red cabbage.

Major Asian and Polynesian influences on modern Hawaiian cuisine are from Japan, Korea, Vietnam, China (especially near the Pearl River delta,) Samoa, and the Philippines. From Japan, the concept of serving raw fish as a meal with rice was introduced, as was soft tofu, setting the stage for the popular dish called poke.

From Korea, immigrants to Hawaii brought a love of spicy garlic marinades for meat and "kimchi". From China, their version of "char siu baau" became modern "manapua", a type of steamed pork bun with a spicy filling.

Filipinos brought vinegar, "bagoong", and "lumpia", and during the 20th century immigrants from American Samoa brought the open pit fire "umu" and the Vietnamese introduced lemongrass and fish sauce.

Each East Asian culture brought several different kinds of noodles, including udon, ramen, "mei fun", and "pho", and today these are common lunchtime meals.

Much of this cuisine mixes and melts into traditions like the "lu'au", whose traditional elaborate fare was once the prerogative of kings and queens but is today the subject of parties for both tourists and also private parties for the "‘ohana" (meaning family and close friends.)

Traditionally, women and men ate separately under the Hawaiian "kapu" system, a system of religious beliefs that honored the Hawaiian gods similar to the Maori "tapu" system, though in this case had some specific prohibitions towards females eating things like coconut, pork, turtle meat, and bananas as these were considered parts of the male gods. Punishment for violation could be severe, as a woman might endanger a man's "mana", or soul, by eating with him or otherwise by eating the forbidden food because doing so dishonored the male gods.

As the system broke down after 1810, introductions of foods from laborers on plantations began to be included at feasts and much cross pollination occurred, where Asian foodstuffs mixed with Polynesian foodstuffs like breadfruit, kukui nuts, and purple sweet potatoes.

Some notable Hawaiian fare includes seared ahi tuna, "opakapaka" (snapper) with passionfruit, Hawaiian island-raised lamb, beef and meat products, Hawaiian plate lunch, and Molokai shrimp. Seafood traditionally is caught fresh in Hawaiian waters, and particular delicacies are "ula poni", "papaikualoa", "‘opihi", and "‘opihi malihini", better known as Hawaiian spiny lobster, Kona crab, Hawaiian limpet, and abalone, the last brought over with Japanese immigrants.

Some cuisine also incorporates a broad variety of produce and locally grown agricultural products, including tomatoes, sweet Maui onions, taro, and macadamia nuts. Tropical fruits also play an important role in the cuisine as a flavoring in cocktails and in desserts, including local cultivars of bananas, sweetsop, mangoes, lychee, coconuts, papayas, and "lilikoi" (passionfruit). Pineapples have been an island staple since the 19th century and figure into many marinades and drinks.

The influence of ethnicity-specific cuisines like Italian cuisine and Mexican cuisine was present in the United States by World War I. There are recipes for Chilean meat pies, chicken chop suey, chow mein, Mexican pork pastries and Italian meatballs going back to at least the 1930s, but many of the recipes were Anglicized and they appeared relatively infrequently compared to Northern European recipes.

19th-century cookbooks bear evidence of diverse influences with some including recipes like Indian pickle, Italian pork and various curries. 19th-century literature shows knowledge of Jewish, Russian, Italian, Chinese and Greek-American cuisines, and foreign cookbooks continued to grow more detailed through World War I including recipes like Peruvian chicken, Mexican enchiladas, Chilean corn pudding and Hindustan chicken curry.

Louise Rice, author of "Dainty Dishes from Foreign Lands" describes the recipes in her book as "not wholly vegetarian" though noting at the time of publication in 1911 that most of the recipes would likely be new to average American cooks and likely contain higher proportions of vegetables to meat. She includes Italian pasta recipes like macaroni in milk, soups and polentas and German recipes like liver dumplings called "Leberknödel" and a variation of Sauerbraten.

The demand for ethnic foods in the United States reflects the nation's changing diversity as well as its development over time. According to the National Restaurant Association, 
Restaurant industry sales are expected to reach a record high of $476 billion in 2005, an increase of 4.9 percent over 2004... Driven by consumer demand, the ethnic food market reached record sales in 2002, and has emerged as the fastest growing category in the food and beverage product sector, according to USBX Advisory Services. Minorities in the U.S. spend a combined $142 billion on food and by 2010, America's ethnic population is expected to grow by 40 percent.

A movement began during the 1980s among popular leading chefs to reclaim America's ethnic foods within its regional traditions, where these trends originated. One of the earliest was Paul Prudhomme, who in 1984 began the introduction of his influential cookbook, "Paul Prodhomme's Louisiana Kitchen", by describing the over 200-year history of Creole and Cajun cooking; he aims to "preserve and expand the Louisiana tradition." Prodhomme's success quickly inspired other chefs. Norman Van Aken embraced a Floridian type cuisine fused with many ethnic and globalized elements in his "Feast of Sunlight" cookbook in 1988. California became swept up in the movement, then seemingly started to lead the trend itself, in, for example, the popular restaurant Chez Panisse in Berkeley.

Examples of the Chez Panisse phenomenon, chefs who embraced a new globalized cuisine, were celebrity chefs like Jeremiah Tower and Wolfgang Puck, both former colleagues at the restaurant. Puck went on to describe his belief in contemporary, new style American cuisine in the introduction to "The Wolfgang Puck Cookbook":

Another major breakthrough, whose originators were once thought to be crazy, is the mixing of ethnic cuisines. It is not at all uncommon to find raw fish listed next to tortillas on the same menu. Ethnic crossovers also occur when distinct elements meet in a single recipe. This country is, after all, a huge melting pot. Why should its cooking not illustrate the American transformation of diversity into unity?
Puck's former colleague, Jeremiah Tower became synonymous with California Cuisine and the overall American culinary revolution. Meanwhile, the restaurant that inspired both Puck and Tower became a distinguished establishment, popularizing its so called "mantra" in its book by Paul Bertolli and owner Alice Waters, "Chez Panisse Cooking", in 1988. Published well after the restaurants' founding in 1971, this new cookbook from the restaurant seemed to perfect the idea and philosophy that had developed over the years. The book embraced America's natural bounty, specifically that of California, while containing recipes that reflected Bertoli and Waters' appreciation of both northern Italian and French style foods.

While the earliest cuisine of the United States was influenced by Native Americans, the thirteen colonies, or the antebellum South, the overall culture of the nation, its gastronomy and the growing culinary arts became ever more influenced by its changing ethnic mix and immigrant patterns from the 18th and 19th centuries unto the present. Some of the ethnic groups that continued to influence the cuisine were here in prior years; others arrived more numerously during "The Great Transatlantic Migration" (of 1870–1914) or other mass migrations.

Some of the ethnic influences could be found across the nation after the American Civil War and into the continental expansion for most of the 19th century. Ethnic influences already in the nation at that time would include the following groups and their respective cuisines:

Mass migrations of immigrants to the United States came over time. Historians identify several waves of migration to the United States: one from 1815 to 1860, in which some five million English, Irish, German, Scandinavian, and others from northwestern Europe came to the United States; one from 1865 to 1890, in which some 10 million immigrants, also mainly from northwestern Europe, settled; and a third from 1890 to 1914, in which 15 million immigrants, mainly from central, eastern, and southern Europe (many Austrian, Hungarian, Turkish, Lithuanian, Russian, Jewish, Greek, Italian, and Romanian) settled in the United States.

Together with earlier arrivals to the United States (including the indigenous Native Americans, Hispanic and Latino Americans, particularly in the West, Southwest, and Texas; African Americans who came to the United States in the Atlantic slave trade; and early colonial migrants from Europe), these new waves of immigrants had a profound impact on national or regional cuisine. Some of these more prominent groups include the following:

Italian, Mexican and Chinese (Cantonese) cuisines have indeed joined the mainstream. These three cuisines have become so ingrained in the American culture that they are no longer foreign to the American palate. According to the study, more than nine out of 10 consumers are familiar with and have tried these foods, and about half report eating them frequently. The research also indicates that Italian, Mexican and Chinese (Cantonese) have become so adapted to such an extent that "authenticity" is no longer a concern to customers.
Contributions from these ethnic foods have become as common as traditional "American" fares such as hot dogs, hamburgers, beef steak, which are derived from German cuisine, (chicken-fried steak, for example, is a variation on German schnitzel), cherry pie, Coca-Cola, milkshakes, fried chicken (Fried chicken is of English, Scottish, and African influence), Pepsi, Dr Pepper and so on. Nowadays, Americans also have a ubiquitous consumption of foods like pizza and pasta, tacos and burritos to "General Tso's chicken" and fortune cookies. Fascination with these and other ethnic foods may also vary with region.

The United States has a large fast food industry. Major American fast food chains include McDonald's, Burger King, Wendy's, Domino's, Pizza Hut, KFC, Popeyes, Subway, Taco Bell, Arby's, Starbucks, Dunkin' Donuts, White Castle, In-N-Out Burger, Sonic Drive-In, Chick-fil-A, Church's Chicken, and Raising Cane's, among numerous other multinational, national, regional, and local chains. Traditional American fast food items are hamburgers, french fries, breaded chicken, and pizza, though several chains also offer items from different cuisines modified for American palates, such as tacos, pasta, and stir-fry. Many American fast food chains have expanded abroad to other countries, typically offering standard American fare alongside items adapted to appeal to regional tastes within their markets.

A classic American dessert is apple pie. Some other famous American desserts are banana split, Boston cream pie, key lime pie, and bananas foster. Other famous American desserts are chocolate chip cookies, pecan pie, carrot cake, banana pudding, S'more, black and white cookies, pumpkin pie, coconut cake, funnel cake, brownies and red velvet cake.

American chefs have been influential both in the food industry and in popular culture. Some important 19th-century American chefs include Charles Ranhofer of Delmonico's Restaurant in New York, and Bob Payton, who is credited with bringing American-style pizza to the UK. Later, chefs Charles Scotto, Louis Pacquet, John Massironi were founded the American Culinary Federation in 1930, taking after similar organizations across Europe. In the 1940s, Chef James Beard hosted the first nationally televised cooking show "I Love to Eat." His name is also carried by the foundation and prestigious cooking award recognizing excellence in the American cooking community. Since Beard, many chefs and cooking personalities have taken to television, and the success of the Cooking Channel and Food Network have contributed to the popularity of American cuisine. In 1946, the Culinary Institute of America was founded by Katharine Angel and Frances Roth. This would become the United States' most prestigious culinary school, where many of the most talented American chefs would study prior to successful careers. The American Culinary community has grown due to both restaurants and media, through the work of many talented chefs.

Notable American restaurant chefs include Samin Nosrat (Salt, Fat, Acid, Heat), Thomas Keller (The French Laundry), Charlie Trotter (Trotter's), Grant Achatz (Alinea), Alfred Portale (Portale), Paul Prudhomme (K-Paul's), Paul Bertolli (Oliveto), Jonathan Waxman (Barbuto), Mark Peel (Campanile), Frank Stitt (Bottega), Alice Waters (Chez Panisse), Wolfgang Puck (Spago), Patrick O'Connell (The Inn), Eric Ripert (Le Bernardin), Todd English (Olives) and Anthony Bourdain (Les Halles). Many of these chefs have received much critical acclaim, as Keller, Achatz, Ripert and O'Connell have all received three Michelin stars, the highest distinction which a restaurant can be given. Keller was given this award for The French Laundry, Achatz for Alinea, Ripert for Le Bernardin and O'Connell for The Inn at Little Washington.

Celebrity chefs have also helped to expand the culinary arts into popular culture, with chefs such as David Chang (Chef's Table), Alton Brown (Iron Chef America), Emeril Lagasse (Emeril Live), Cat Cora (Iron Chef America), Erik Davidson (Phat Erik's), Michael Symon (The Chew), Bobby Flay (Beat Bobby Flay), Ina Garten (Barefoot Contessa) and Guy Fieri (Diners, Drive-ins and Dives). Many of these celebrity chefs, such as David Chang, Emeril Lagasse and Bobby Flay began their careers in restaurants before branching out into television. The shows have a wide variety of formats, including cooking competitions, such as Iron Chef, documentaries, such as Anthony Bourdain's "", shows that take a look into restaurants, as Chef's Table does, and shows that teach cooking. The success of food television specifically in the United States has helped American Cuisine grow around the world.

Regional chefs are emerging as localized celebrity chefs with growing broader appeal, such as Peter Merriman (Hawaii Regional Cuisine), Roy Choi (Korean American Cuisine), Jerry Traunfeld, Alan Wong (Pacific Rim cuisine), Rick Bayless and Daniela Soto-Innes (traditional Mexican cuisine with modern interpretations), Norman Van Aken (New World Cuisine – fusion Latin, Caribbean, Asian, African and American), and Mark Miller (American Southwest cuisine).







Ahmad Shah Massoud

Ahmad Shah Massoud (Dari/Pashto: , ; September 2, 1953September 9, 2001) was an Afghan politician and military commander. He was a powerful guerrilla commander during the resistance against the Soviet occupation between 1979 and 1989. In the 1990s, he led the government's military wing against rival militias; after the Taliban takeover, he was the leading opposition commander against their regime until his assassination in 2001.

Massoud came from an ethnic Tajik, Sunni Muslim background in the Panjshir Valley of Northern Afghanistan. He began studying engineering at Polytechnical University of Kabul in the 1970s, where he became involved with religious anti-communist movements around Burhanuddin Rabbani, a leading Islamist. He participated in a failed uprising against Mohammed Daoud Khan's government. He later joined Rabbani's Jamiat-e Islami party. During the Soviet–Afghan War, his role as a powerful insurgent leader of the Afghan mujahideen earned him the nickname "Lion of Panjshir" () among his followers. Supported by Britain's MI6 and to a lesser extent by the U.S. Central Intelligence Agency (CIA), he successfully resisted the Soviets from taking the Panjshir Valley. In 1992, he signed the Peshawar Accord, a peace and power-sharing agreement, in the post-communist Islamic State of Afghanistan. He was appointed the Minister of Defense as well as the government's main military commander. His militia fought to defend Kabul against militias led by Gulbuddin Hekmatyar and other warlords who were bombing the city, as well as later against the Taliban, who laid siege to the capital in January 1995 after the city had seen fierce fighting with at least 60,000 civilians killed.

Following the rise of the Taliban in 1996, Massoud, who rejected the Taliban's fundamentalist interpretation of Islam, returned to armed opposition until he was forced to flee to Kulob, Tajikistan, strategically destroying the Salang Tunnel on his way north. He became the military and political leader of the United Islamic Front for the Salvation of Afghanistan or Northern Alliance, which by 2000 controlled only between 5 and 10 percent of the country. In 2001 he visited Europe and urged European Parliament leaders to pressure Pakistan on its support for the Taliban. He also asked for humanitarian aid to combat the Afghan people's gruesome conditions under the Taliban. On September 9, 2001, Massoud was injured in a suicide bombing by two al-Qaeda assassins, ordered personally by the al-Qaeda leader Osama bin Laden himself; he lost his life while en route to a hospital across the border in Tajikistan. Two days later, the September 11 attacks occurred in the United States, which ultimately led to the North Atlantic Treaty Organization invading Afghanistan and allying with Massoud's forces. The Northern Alliance eventually won the two-month-long war in December 2001, removing the Taliban from power.

Massoud has been described as one of the greatest guerrilla leaders of the 20th century and has been compared to Josip Broz Tito, Ho Chi Minh and Che Guevara. Massoud was posthumously named "National Hero" by the order of President Hamid Karzai after the Taliban were ousted from power. The date of Massoud's death, September 9, was observed as a national holiday known as "Massoud Day" until the Taliban takeover in August 2021. His followers call him "Amer Sāhib-e Shahīd" (), which translates to "(our) martyred commander". He has been posthumously honored by a plaque in France in 2021, and in the same year was awarded with the highest honor of Tajikistan.

Ahmad Shah Massoud was born in 1953 in the small village of Jangalak, Bazarak in the Panjshir Valley (now administered as part of the Panjshir Province), to a well-to-do family native to the Panjshir Valley. Massoud's name at birth was 'Ahmad Shah' after King Ahamad Shah Durrani, founder of the modern, unified state of Afghanistan, later taking the name 'Massoud' as a "nom de guerre" in 1974 when he joined the resistance movement against the forces of Daoud Khan. Massoud's father, Dost Mohammad, was a colonel in the Royal Afghan Army; his mother, Bibi Khorshaid has been described as a "modern-minded" woman who taught herself to read and write determined to educate her daughters no less than her sons.

Moving along with his father's postings, the adolescent Massoud attended primary school in Afghanistan's western city of Herat before his father was dispatched to Kabul. There, Massoud was sent to the renowned Franco-Afghan Lycée Esteqlal (lit. Independence High School) where he attained his proficiency in French. Massoud's experience at Lycée would be formative and, as he would later remark, was the happiest period of his life. At Lycée his classes were taught by French and Afghan tutors educated in France and the students donned Western jackets, neckties, trousers, skirts, scarves, and stockings. Although his knowledge of the French language would earn him greater affinity among French journalists and politicians, later conservative Islamist opponents such as Gulbuddin Hekmatyar and the Taliban would derogatorily dub him "The Frenchmen" or "The Parisian" suggestive of his sympathies to Western culture.

While at the Lycée, Massoud was described as an intellectually-gifted student, hard-working, religiously devout, and mature for his age with a particular interest in ethics, politics, universal justice. Friends and family recall an instance where Massoud, returning from school, came to the defense of a younger boy leaving the three bullies knocked-out on the pavement. More formatively, Massoud followed closely reports of the 1967 Six-Day War and the defiant statements of Arab leaders like Egyptian President Gamal Abdel Nasser. He later told researcher Peter DeNeufville that, at fourteen, the war left him determined to be a soldier and gave him a new regard for Pan-Islamism after hearing the stories told by Jordanian, Egyptian, and Syrian soldiers defending their homelands. Massoud refused repeated suggestions to apply for a scholarship to study in France expressing his desire to remain in Afghanistan and apply to the nation's military academy in Kabul.

By protest of his father and eldest brother, Massoud enrolled at Kabul Polytechnic Institute, then Kabul University's newest and most prestigious addition founded, financed, and operated by the Soviet Union. Massoud studied engineering and architecture but never attempted to learn Russian. There he found interest in politics, political Islam, and anti-Communism which often put him and his pious peers at odds with communist-inspired students.

In 1973, former Prime Minister Mohammed Daoud Khan was brought to power in a coup d'état backed by the People's Democratic Party of Afghanistan, and the Republic of Afghanistan was established. These developments gave rise to an Islamist movement opposed to the increasing communist and Soviet influence over Afghanistan. During that time, while studying at Kabul University, Massoud became involved with the Muslim Youth (Sazman-i Jawanan-i Musulman), the student branch of the Jamiat-e Islami (Islamic Society), whose chairman then was the professor Burhanuddin Rabbani. Kabul University was a center for political debate and activism during that time.

Infuriated by the arrogance of his communist peers and Russian professors, a physical altercation between Massoud and his Russian professor led Massoud to walk out of the university, and shortly after, Kabul. Two days later, Massoud and a number of fellow militant students traveled to Pakistan where, goaded by another trainee of the Pakistani Inter-Services Intelligence (ISI), Gulbaddin Hekmatyar, Massoud agreed to take part in a coup against Daoud with his forces rising up in the Panjshir and Hekmatyar's elsewhere. In July 1975, Massoud, with help from the Pakistani intelligence, led the first rebellion of Panjshir residents against the government of Daoud Khan. While the uprising in the Panjshir saw initial success, even taking the military garrison in Rokha, the promised support from Kabul never came and the rebellion was suppressed by Daoud Khan's forces sending Massoud back into Pakistan (after a day hiding in Jangalak) where he would attend a secret, paramilitary ISI training center in Cherat. Dissatisfied, Massoud left the center and returned to Peshawar where he committed himself to personal military studies. Massoud read Mao Tse-tung's writings on the Long March, of Che Guevara's career, the memoirs of General de Gualle, General Võ Nguyên Giáp, Sun Tzu's Art of War, and an unnamed handbook on counterterrorism by an American general which Massoud called "the most instructive of all".

After this failure, a "profound and long-lasting schism" within the Islamist movement began to emerge. The Islamic Society split between supporters of the more moderate forces around Massoud and Rabbani, who led the Jamiat-i Islami, and more radical Islamist elements surrounding Gulbuddin Hekmatyar, who founded the Hezb-i Islami. The conflict reached such a point that Hekmatyar reportedly tried to kill Massoud, then 22 years old.

The government of Mohammed Daoud Khan tried to scale back the communist People's Democratic Party of Afghanistan's influence, dismissing PDPA members from their government posts, appointing conservatives to replace them, and finally dissolved the PDPA, with the arrests of senior party members. On April 27, 1978, the PDPA and military units loyal to it killed Daoud Khan, his immediate family, and bodyguards in a violent coup, and seized control of the capital Kabul declaring the new Democratic Republic of Afghanistan (DRA). The new communist government, led by a revolutionary council, did not enjoy the support of the masses. It implemented a doctrine hostile to political dissent, whether inside or outside the party. The PDPA started reforms along Marxist–Leninist and Soviet lines. The reforms and the PDPA's affinity to the Soviet Union were met with strong resistance by the population, especially as the government attempted to enforce its Marxist policies by arresting or executing those who resisted. Between 50,000 and 100,000 people were estimated to have been arrested and killed by communist troops in the countryside alone. Due to the repression, large parts of the country, especially the rural areas, organized into open revolt against the PDPA government. By spring 1979, unrest had reached 24 out of 28 Afghan provinces, including major urban areas. Over half of the Afghan army either deserted or joined the insurrection.

With religious elders declaring a jihad against the government, in May 1979 Massoud prepared in Peshawar to oppose the new communist government in Panjshir. Along with twenty-four of his friends, Massoud took a bus to Bajaur and, with arms-smuggling Pashtun tribesmen, marched on foot into the Panjshir Valley. Massoud's group seized control over a number of government outposts in the Valley, entered the Shomali Plain to capture Gulbahar, and cut off the Salang Highway, the main supply route between Kabul and the Soviet border raising alarm in both Kabul and Moscow which brought upon Massoud and his group a government counterattack.

Believing that an uprising against the Soviet-backed communists would be supported by the people, Massoud, on July 6, 1979, started an insurrection in the Panjshir, which initially failed. Massoud decided to avoid conventional confrontation with the larger government forces and to wage a guerrilla war. He subsequently took full control of Panjshir, pushing out Afghan communist troops. Oliver Roy writes that in the following period, Massoud's "personal prestige and the efficiency of his military organization persuaded many local commanders to come and learn from him."

Following the 1979 Soviet invasion and occupation of Afghanistan, Massoud devised a strategic plan for expelling the invaders and overthrowing the communist regime. The first task was to establish a popularly based resistance force that had the loyalty of the people. The second phase was "active defense" of the Panjshir stronghold, while carrying out asymmetric warfare. In the third phase, the "strategic offensive", Massoud's forces would gain control of large parts of Northern Afghanistan. The fourth phase was the "general application" of Massoud's principles to the whole country, and the defeat of the Afghan communist government.

Massoud's mujahideen attacked the occupying Soviet forces, ambushing Soviet and Afghan communist convoys travelling through the Salang Pass, and causing fuel shortages in Kabul. The Soviets mounted a series of offensives against the Panjshir. Between 1980 and 1985, these offensives were conducted twice a year. Despite engaging more men and hardware on each occasion, the Soviets were unable to defeat Massoud's forces. In 1982, the Soviets began deploying major combat units in the Panjshir, numbering up to 30,000 men. Massoud pulled his troops back into subsidiary valleys, where they occupied fortified positions. When the Soviet columns advanced onto these positions, they fell into ambushes. When the Soviets withdrew, Afghan army garrisons took over their positions. Massoud and his mujahideen forces attacked and recaptured them one by one.

In 1983, the Soviets offered Massoud a temporary truce, which he accepted in order to rebuild his own forces and give the civilian population a break from Soviet attacks. He put the respite to good use. In this time he created the Shura-e Nazar (Supervisory Council), which subsequently united 130 commanders from 12 Afghan provinces in their fight against the Soviet army. This council existed outside the Peshawar parties, which were prone to internecine rivalry and bickering, and served to smooth out differences between resistance groups, due to political and ethnic divisions. It was the predecessor of what could have become a unified Islamic Afghan army.

Relations with the party headquarters in Peshawar were often strained, as Rabbani insisted on giving Massoud no more weapons and supplies than to other Jamiat commanders, even those who did little fighting. To compensate for this deficiency, Massoud relied on revenues drawn from exports of emeralds and lapis lazuli, that are traditionally exploited in Northern Afghanistan.

Regarding infighting among different mujahideen factions, following a Soviet truce, Massoud said in an interview:

Britain's MI6 having activated long-established networks of contacts in Pakistan were able to support Massoud, and soon became their key ally. MI6 sent an annual mission of two of their officers as well as military instructors to Massoud and his fighters. They also gave supplies to Massoud which included sniper rifles with silencers and mortars. As well as training Massoud's junior commanders, MI6 team's most important contribution was help with organisation and communication via radio equipment which was highly useful for Massoud to coordinate his forces and be warned of any impending Soviet attacks. The United States however provided Massoud with comparatively less support than other factions. Part of the reason was that it permitted its funding and arms distribution to be administered by Pakistan, which favored the rival mujahideen leader Gulbuddin Hekmatyar. In an interview, Massoud said, "We thought the CIA knew everything. But they didn't. They supported some bad people [meaning Hekmatyar]." Primary advocates for supporting Massoud were the US State Department's Edmund McWilliams and Peter Tomsen, who were on the ground in Afghanistan and Pakistan. Others included two Heritage Foundation foreign policy analysts, Michael Johns and James A. Phillips, both of whom championed Massoud as the Afghan resistance leader most worthy of U.S. support under the Reagan Doctrine. Thousands of foreign Islamic volunteers entered Afghanistan to fight with the mujahideen against the Soviet troops.

To organize support for the mujahideen, Massoud established an administrative system that enforced law and order ("nazm") in areas under his control. The Panjshir was divided into 22 bases ("qarargah") governed by a military commander and a civilian administrator, and each had a judge, a prosecutor and a public defender. Massoud's policies were implemented by different committees: an economic committee was charged with funding the war effort. The health committee provided health services, assisted by volunteers from foreign humanitarian non-governmental organizations, such as Aide médicale internationale. An education committee was charged with the training of the military and administrative cadre. A culture committee and a judiciary committee were also created.

This expansion prompted Babrak Karmal to demand that the Red Army resume their offensives, in order to crush the Panjshir groups. However, Massoud had received warning of the attack through Britain's GCHQ intelligence and he evacuated all 130,000 inhabitants from the valley into the Hindukush mountains, leaving the Soviet bombings to fall on empty ground and the Soviet battalions to face the mountains.

With the defeat of the Soviet-Afghan attacks, Massoud carried out the next phase of his strategic plan, expanding the resistance movement and liberating the northern provinces of Afghanistan. In August 1986, he captured Farkhar in Takhar Province. In November 1986, his forces overran the headquarters of the government's 20th division at Nahrin in Baghlan Province, scoring an important victory for the resistance. This expansion was also carried out through diplomatic means, as more mujahideen commanders were persuaded to adopt the Panjshir military system.

Despite almost constant attacks by the Red Army and the Afghan army, Massoud increased his military strength. Starting in 1980 with a force of less than 1,000 ill-equipped guerrillas, the Panjshir valley mujahideen grew to a 5,000-strong force by 1984. After expanding his influence outside the valley, Massoud increased his resistance forces to 13,000 fighters by 1989. The junior commanders were trained by Britain's SAS as well as private military contractors, some being sent as far as Oman and even SAS training grounds in the Scottish Highlands. These forces were divided into different types of units: the locals (mahalli) were tasked with static defense of villages and fortified positions. The best of the mahalli were formed into units called grup-i zarbati (shock troops), semi-mobile groups that acted as reserve forces for the defense of several strongholds. A different type of unit was the mobile group (grup-i-mutaharek), a lightly equipped commando-like formation numbering 33 men, whose mission was to carry out hit-and-run attacks outside the Panjshir, sometimes as far as 100 km from their base. These men were professional soldiers, well-paid and trained, and, from 1983 on, they provided an effective strike force against government outposts. Uniquely among the mujahideen, these groups wore uniforms, and their use of the "pakul" made this headwear emblematic of the Afghan resistance.

Massoud's military organization was an effective compromise between the traditional Afghan method of warfare and the modern principles of guerrilla warfare which he had learned from the works of Mao Zedong and Che Guevara. His forces were considered the most effective of all the various Afghan resistance movements.

The Soviet army and the Afghan communist army were mainly defeated by Massoud and his mujahideen in numerous small engagements between 1984 and 1988. After describing the Soviet Union's military engagement in Afghanistan as "a bleeding wound" in 1986, Soviet General Secretary Mikhail Gorbachev began a withdrawal of Soviet troops from the nation in May 1988. On February 15, 1989, in what was depicted as an improbable victory for the mujahideen, the last Soviet soldier left the nation.

After the departure of Soviet troops in 1989, the People's Democratic Party of Afghanistan regime, then headed by Mohammad Najibullah, held its own against the mujahideen. Backed by a massive influx of weapons from the Soviet Union, the Afghan armed forces reached a level of performance they had never reached under direct Soviet tutelage. They maintained control over all of Afghanistan's major cities. During late 1990, helped by hundreds of mujahideen forces, Massoud targeted the Tajik Supreme Soviet, trying to oust communism from the neighboring Tajikistan to further destabilize the dying Soviet Union, which would also impact the Afghan government. At that time, as per Asad Durrani, the director-general of the ISI during this period, Massoud's base camp was in Garam Chashma, in Pakistan. By 1992, after the collapse of the Soviet Union, the Afghan regime eventually began to crumble. Food and fuel shortages undermined the capacities of the government's army, and a resurgence of factionalism split the regime between Khalq and Parcham supporters.

A few days after Najibullah had lost control of the nation, his army commanders and governors arranged to turn over authority to resistance commanders and local warlords throughout the country. Joint councils ("shuras") were immediately established for local government, in which civil and military officials of the former government were usually included. In many cases, prior arrangements for transferring regional and local authority had been made between foes.

Collusions between military leaders quickly brought down the Kabul government. In mid-January 1992, within three weeks of the demise of the Soviet Union, Massoud was aware of conflict within the government's northern command. General Abdul Momim, in charge of the Hairatan border crossing at the northern end of Kabul's supply highway, and other non-Pashtun generals based in Mazar-i-Sharif, feared removal by Najibullah and replacement by Pashtun officers. When the generals rebelled, Abdul Rashid Dostum, who held general rank as head of the Jowzjani militia, also based in Mazar-i-Sharif, took over.

He and Massoud reached a political agreement, together with another major militia leader, Sayyed Mansour, of the Ismaili community based in Baghlan Province. These northern allies consolidated their position in Mazar-i-Sharif on March 21. Their coalition covered nine provinces in the north and northeast. As turmoil developed within the government in Kabul, no government force stood between the northern allies and the major air force base at Bagram, some seventy kilometers north of Kabul. By mid-April 1992, the Afghan air force command at Bagram had capitulated to Massoud. On March 18, 1992, Najibullah decided to resign. On April 17, as his government fell, he tried to escape but was stopped at Kabul Airport by Dostum's forces. He took refuge at the United Nations mission, where he remained unharmed until 1996, while Massoud controlled the area surrounding the mission.

Senior communist generals and officials of the Najibullah administration acted as a transitional authority to transfer power to Ahmad Shah Massoud's alliance. The Kabul interim authority invited Massoud to enter Kabul as the new Head of State, but he held back. Massoud ordered his forces, positioned to the north of Kabul, not to enter the capital until a political solution was in place. He called on all the senior Afghan party leaders, many then based in exile in Peshawar, to work out a political settlement acceptable to all sides and parties.

With United Nations support, most Afghan political parties decided to appoint a legitimate national government to succeed communist rule, through an elite settlement. While the external Afghan party leaders were residing in Peshawar, the military situation around Kabul involving the internal commanders was tense. A 1991 UN peace process brought about some negotiations, but the attempted elite settlement did not develop. In April 1992, resistance leaders in Peshawar tried to negotiate a settlement. Massoud supported the Peshawar process of establishing a broad coalition government inclusive of all resistance parties, but Hekmatyar sought to become the sole ruler of Afghanistan, stating, "In our country coalition government is impossible because, this way or another, it is going to be weak and incapable of stabilizing the situation in Afghanistan."

Massoud wrote:

All the parties had participated in the war, in jihad in Afghanistan, so they had to have their share in the government, and in the formation of the government. Afghanistan is made up of different nationalities. We were worried about a national conflict between different tribes and different nationalities. In order to give everybody their own rights and also to avoid bloodshed in Kabul, we left the word to the parties so they should decide about the country as a whole. We talked about it for a temporary stage and then after that the ground should be prepared for a general election.

A recorded radio communication between the two leaders showed the divide as Massoud asked Hekmatyar:

The Kabul regime is ready to surrender, so instead of the fighting we should gather. ... The leaders are meeting in Peshawar. ... The troops should not enter Kabul, they should enter later on as part of the government.

Hekmatyar's response:

We will march into Kabul with our naked sword. No one can stop us. ... Why should we meet the leaders?"
Massoud answered:

"It seems to me that you don't want to join the leaders in Peshawar nor stop your threat, and you are planning to enter Kabul ... in that case I must defend the people.

At that point Osama bin Laden, trying to mediate, urged Hekmatyar to "go back with your brothers" and to accept a compromise. Bin Laden reportedly "hated Ahmad Shah Massoud". Bin Laden was involved in ideological and personal disputes with Massoud and had sided with Gulbuddin Hekmatyar against Massoud in the inner-Afghan conflict since the late 1980s. But Hekmatyar refused to accept a compromise, confident that he would be able to gain sole power in Afghanistan.

On April 24, 1992, the leaders in Peshawar agreed on and signed the Peshawar Accord, establishing the post-communist Islamic State of Afghanistan – which was a stillborn 'state' with a paralyzed 'government' right from its inception, until its final succumbing in September 1996. The creation of the Islamic State was welcomed though by the General Assembly of the United Nations and the Islamic State of Afghanistan was recognized as the legitimate entity representing Afghanistan until June 2002, when its successor, the Islamic Republic of Afghanistan, was established under the interim government of Hamid Karzai. Under the 1992 Peshawar Accord, the Defense Ministry was given to Massoud while the Prime Ministership was given to Hekmatyar. Hekmatyar refused to sign. With the exception of Hekmatyar's Hezb-e Islami, all of the other Peshawar resistance parties were unified under this peace and power-sharing accord in April 1992.

Although repeatedly offered the position of prime minister, Gulbuddin Hekmatyar refused to recognize the peace and power-sharing agreement. His Hezb-e Islami militia initiated a massive bombardment campaign against the Islamic State and the capital city Kabul. Gulbuddin Hekmatyar received operational, financial and military support from neighboring Pakistan. The Director of the Centre for Arab and Islamic Studies at the Australian National University, Amin Saikal, writes in "Modern Afghanistan: A History of Struggle and Survival" that without Pakistan's support, Hekmatyar "would not have been able to target and destroy half of Kabul." Saikal states that Pakistan wanted to install a favorable regime under Hekmatyar in Kabul so that it could use Afghan territory for access to Central Asia.

Hekmatyar's rocket bombardments and the parallel escalation of violent conflict between two militias, Ittihad and Wahdat, which had entered some suburbs of Kabul, led to a breakdown in law and order. Shia Iran and Sunni Wahabbi Saudi Arabia, as competitors for regional hegemony, encouraged conflict between the Ittihad and Wahdat factions. On the one side was the Shia Hazara Hezb-i Wahdat of Abdul Ali Mazari and on the other side, the Sunni Pashtun Ittihad-i Islami of Abdul Rasul Sayyaf.

According to Human Rights Watch, Iran was strongly supporting the Hezb-i Wahdat forces, with Iranian intelligence officials providing direct orders, while Saudi Arabia supported Sayyaf and his Ittihad-i Islami faction to maximize Wahhabi influence. Kabul descended into lawlessness and chaos, as described in reports by Human Rights Watch and the Afghanistan Justice Project. Massoud's Jamiat commanders, the interim government, and the International Committee of the Red Cross (ICRC) repeatedly tried to negotiate ceasefires, which broke down in only a few days. Another militia, the Junbish-i Milli of former communist general Abdul Rashid Dostum, was backed by Uzbekistan. Uzbek president Islam Karimov was keen to see Dostum controlling as much of Afghanistan as possible, especially in the north. Dostum repeatedly changed allegiances.

The Afghanistan Justice Project (AJP) says, that "while [Hekmatyar's anti-government] Hizb-i Islami is frequently named as foremost among the factions responsible for the deaths and destruction in the bombardment of Kabul, it was not the only perpetrator of these violations." According to the AJP, "the scale of the bombardment and kinds of weapons used represented disproportionate use of force" in a capital city with primarily residential areas by all the factions involved – including the government forces. Crimes were committed by individuals within the different armed factions. Gulbuddin Hekmatyar released 10,000 dangerous criminals from the main prisons into the streets of Kabul to destabilize the city and cut off Kabul from water, food and energy supplies. The Iran-controlled Wahdat of Abdul Ali Mazari, as well as the Ittihad of Abdul Rasul Sayyaf supported by Saudi Arabia, targeted civilians of the 'opposite side' in systematic atrocities. Abdul Rashid Dostum allowed crimes as a perceived payment for his troops.

"The major criticism of Massoud's human rights record" is the escalation of the Afshar military operation in 1993. A report by the Afghanistan Justice Project describes Massoud as failing to prevent atrocities carried out by his forces and those of their factional ally, Ittihad-i Islami, against civilians on taking the suburb of Afshar during a military operation against an anti-state militia allied to Gulbuddin Hekmatyar. They shelled residential areas in the capital city in February 1993. Critics said that Massoud should have foreseen these problems. A meeting convened by Massoud on the next day ordered a halt to killing and looting, but it failed to stop abuses. Human Rights Watch, in a report based largely on the material collected by the Afghanistan Justice Project, concurs that Massoud's Jamiat forces bear a share of the responsibility for human rights abuses throughout the war, including the indiscriminate targeting of civilians in Afshar, and that Massoud was personally implicated in some of these abuses. Roy Gutman has argued that the witness reports about Afshar cited in the AJP report implicated only the Ittihad forces, and that these had not been under Massoud's direct command.

Anthony Davis, who studied and observed Massoud's forces from 1981 to 2001, reported that during the observed period, there was "no pattern of repeated killings of enemy civilians or military prisoners" by Massoud's forces. Edward Girardet, who covered Afghanistan for over three decades, was also in Kabul during the war. He states that while Massoud was able to control most of his commanders well during the anti-Soviet and anti-Taliban resistance, he was not able to control every commander in Kabul. According to this and similar testimonies, this was due to a breakdown of law and order in Kabul and a war on multiple fronts, which they say, Massoud personally had done all in his power to prevent:

In 1993, Massoud created the Cooperative Mohammad Ghazali Culture Foundation ("Bonyad-e Farhangi wa Ta'wani Mohammad-e Ghazali") to further humanitarian assistance and politically independent Afghan culture. The Ghazali Foundation provided free medical services during some days of the week to residents of Kabul who were unable to pay for medical treatment. The Ghazali Foundation's department for distribution of auxiliary goods was the first partner of the Red Cross. The Ghazali Foundation's department of family consultation was a free advisory board, which was accessible seven days a week for the indigent. Although Massoud was responsible for the financing of the foundation, he did not interfere with its cultural work. A council led the foundation and a jury, consisting of impartial university lecturers, decided on the works of artists. The Ghazali foundation enabled Afghan artists to exhibit their works at different places in Kabul, and numerous artists and authors were honoured for their works; some of them neither proponents of Massoud nor the Islamic State government.

In March 1993, Massoud resigned his government position in exchange for peace, as requested by Hekmatyar, who considered him as a personal rival. According to the Islamabad Accord, Burhanuddin Rabbani, belonging to the same party as Massoud, remained president, while Gulbuddin Hekmatyar took the long-offered position of prime minister. Two days after the Islamabad Accord was put into effect, however, Hekmatyar's allies of Hezb-e Wahdat renewed rocket attacks in Kabul.

Both the Wahhabi Pashtun Ittehad-i Islami of Abdul Rasul Sayyaf backed by Saudi Arabia and the Shia Hazara Hezb-e Wahdat supported by Iran remained involved in heavy fighting against each other. Hekmatyar was afraid to enter Kabul proper, and chaired only one cabinet meeting. The author Roy Gutman of the United States Institute of Peace wrote in "How We Missed the Story: Osama bin Laden, the Taliban, and the Hijacking of Afghanistan":

Hekmatyar had become prime minister ... But after chairing one cabinet meeting, Hekmatyar never returned to the capital, fearing, perhaps, a lynching by Kabulis infuriated over his role in destroying their city. Even his close aides were embarrassed. Hekmatyar spokesman Qutbuddin Helal was still setting up shop in the prime minister's palace when the city came under Hezb[-i Islami] rocket fire late that month. "We are here in Kabul and he is rocketing us. Now we have to leave. We can't do anything," he told Massoud aides.

Hekmatyar, who was generally opposed to coalition government and struggled for undisputed power, had conflicts with other parties over the selection of cabinet members. His forces started major attacks against Kabul for one month. The President, Burhanuddin Rabbani, was attacked when he attempted to meet Hekmatyar. Massoud resumed his responsibilities as minister of defense.

In May 1993, a new effort was made to reinstate the Islamabad Accord. In August, Massoud reached out to Hekmatyar in an attempt to broaden the government. By the end of 1993, however, Hekmatyar and the former communist general and militia leader, Abdul Rashid Dostum, were involved in secret negotiations encouraged by Pakistan's secret Inter-Services Intelligence, Iran's intelligence service, and Uzbekistan's Karimov administration. They planned a coup to oust the Rabbani administration and to attack Massoud in his northern areas.

In January 1994, Hekmatyar and Dostum mounted a bombardment campaign against the capital and attacked Massoud's core areas in the northeast. Amin Saikal writes, Hekmatyar had the following objectives in all his operations:

The first was to make sure that Rabbani and Massoud were not allowed to consolidate power, build a credible administration, or expand their territorial control, so that the country would remain divided into small fiefdoms, run by various Muajhideen leaders and local warlords or a council of such elements, with only some of them allied to Kabul. The second was to ensure the Rabbani government acquired no capacity to dispense patronage, and to dissuade the Kabul population from giving more than limited support to the government. The third was to make Kabul an unsafe city for representatives of the international community and to prevent the Rabbani government from attracting the international support needed to begin the post-war reconstruction of Afghanistan and generate a level of economic activity which would enhance its credibility and popularity.

By mid-1994, Hekmatyar and Dostum were on the defensive in Kabul against Islamic State forces led by Massoud. Southern Afghanistan had been neither under the control of foreign-backed militias nor of the government in Kabul, but was ruled by local Pashtun leaders, such as Gul Agha Sherzai, and their militias. In 1994, the Taliban (a movement originating from Jamiat Ulema-e-Islam-run religious schools for Afghan refugees in Pakistan) also developed in Afghanistan as a politico-religious force, reportedly in opposition to the tyranny of the local governor. When the Taliban took control of Kandahar in 1994, they forced the surrender of dozens of local Pashtun leaders who had presided over a situation of complete lawlessness and atrocities. In 1994, the Taliban took power in several provinces in southern and central Afghanistan.

Hizb-i Islami had bombarded Kabul from January 1994 until February 1995 when the Taliban expelled Hizb from its Charasiab headquarters, after which the Taliban relaunched the bombardment of Kabul and started to besiege the town.

By early 1995, Massoud initiated a nationwide political process with the goal of national consolidation and democratic elections. He arranged a conference in three parts uniting political and cultural personalities, governors, commanders, clergymen and representatives, in order to reach a lasting agreement. Massoud's favourite for candidacy to the presidency was Dr. Mohammad Yusuf, the first democratic prime minister under Zahir Shah, the former king. In the first meeting representatives from 15 different Afghan provinces met, in the second meeting there were already 25 provinces participating.

Massoud also invited the Taliban to join the peace process wanting them to be a partner in providing stability to Afghanistan during such a process. But the Taliban, which had emerged over the course of 1994 in southern Afghanistan, were already at the doors of the capital city. Against the advice of his security personnel, Massoud went to talk to some Taliban leaders in Maidan Shar, Taliban territory. The Taliban declined to join the peace process leading towards general elections. When Massoud returned to Kabul unharmed, the Taliban leader who had received him as his guest paid with his life: he was killed by other senior Taliban for failing to assassinate Massoud while the possibility had presented itself. The Taliban, placing Kabul under a two-year siege and bombardment campaign from early 1995 onwards, in later years committed massacres against civilians, compared by United Nations observers to those that happened during the War in Bosnia.

Neighboring Pakistan exerted strong influence over the Taliban. A publication with the George Washington University describes: "Initially, the Pakistanis supported ... Gulbuddin Hekmatyar ... When Hekmatyar failed to deliver for Pakistan, the administration began to support a new movement of religious students known as the Taliban." Many analysts like Amin Saikal describe the Taliban as developing into a proxy force for Pakistan's regional interests. The Taliban started shelling Kabul in early 1995 but were defeated by forces of the Islamic State government under Ahmad Shah Massoud. Amnesty International, referring to the Taliban offensive, wrote in a 1995 report:

The Taliban's early victories in 1994 were followed by a series of defeats that resulted in heavy losses. The Taliban's first major offensive against the important western city of Herat, under the rule of Islamic state ally Ismail Khan, in February 1995 was defeated when Massoud airlifted 2,000 of his own core forces from Kabul to help defend Herat. Ahmed Rashid writes: "The Taliban had now been decisively pushed back on two fronts by the government and their political and military leadership was in disarray. Their image as potential peacemakers was badly dented, for in the eyes of many Afghans they had become nothing more than just another warlord party." International observers already speculated that the Taliban as a country-wide organization might have "run its course".

Mullah Omar, however, consolidated his control of the Taliban and with foreign help rebuilt and re-equipped his forces. Pakistan increased its support to the Taliban. Its military advisers oversaw the restructuring of Taliban forces. The country provided armored pick-up trucks and other military equipment. Saudi Arabia provided the funding. Furthermore, there was a massive influx of 25,000 new Taliban fighters, many of them recruited in Pakistan. This enabled the Taliban to capture Herat to the west of Kabul in a surprise attack against the forces of Ismail Khan in September 1995. A nearly one-year siege and bombardment campaign against Kabul, however, was again defeated by Massoud's forces.

Massoud and Rabbani meanwhile kept working on an internal Afghan peace process – successfully. By February 1996, all of Afghanistan's armed factions – except for the Taliban – had agreed to take part in the peace process and to set up a peace council to elect a new interim president. Many Pashtun areas under Taliban control had representatives also advocating for a peace agreement with the Islamic State government. But Taliban leader Mullah Omar and the Kandaharis surrounding him wanted to expand the war. At that point the Taliban leadership and their foreign supporters decided they needed to act quickly before the government could consolidate the new understanding between the parties. The Taliban moved against Jalalabad, under the control of the Pashtun Jalalabad Shura, to the east of Kabul. Part of the Jalalabad Shura was bribed with millions of dollars by the Taliban's foreign sponsors, especially Saudi Arabia, to vacate their positions. The Taliban's battle for Jalalabad was directed by Pakistani military advisers. Hundreds of Taliban crossed the Afghan-Pakistani border moving on Jalalabad from Pakistan and thereby suddenly placed to the east of Kabul. This left the capital city Kabul "wide open" to many sides as Ismail Khan had been defeated to the west, Gulbuddin Hekmatyar had vacated his positions to the south and the fall and surrender of Jalalabad had suddenly opened a new front to the east.
At that point Massoud decided to conduct a strategic retreat through a northern corridor, according to Ahmed Rashid, "knowing he could not defend [Kabul] from attacks coming from all four points of the compass. Nor did he want to lose the support of Kabul's population by fighting for the city and causing more bloodshed." On September 26, 1996, as the Taliban with military support by Pakistan and financial support by Saudi Arabia prepared for another major offensive, Massoud ordered a full retreat from Kabul. The Taliban marched into Kabul on September 27, 1996, and established the Islamic Emirate of Afghanistan. Massoud and his troops retreated to the northeast of Afghanistan which became the base for the still internationally recognized Islamic State of Afghanistan.

Ahmad Shah Massoud created the United Front (Northern Alliance) against the Taliban advance. The United Front included forces and leaders from different political backgrounds as well as from all ethnicities of Afghanistan. From the Taliban conquest in 1996 until November 2001, the United Front controlled territory in which roughly 30% of Afghanistan's population was living, in provinces such as Badakhshan, Kapisa, Takhar and parts of Parwan, Kunar, Nuristan, Laghman, Samangan, Kunduz, Ghōr and Bamyan.

Meanwhile, the Taliban imposed their repressive regime in the parts of Afghanistan under their control. Hundreds of thousands of people fled to Northern Alliance territory, Pakistan and Iran. Massoud's soldiers held some 1,200 Taliban prisoners in the Panjshir Valley, 122 of them foreign Muslims who had come to Afghanistan to fight a jihad. In 1998, after the defeat of Abdul Rashid Dostum's faction in Mazar-i-Sharif, Ahmad Shah Massoud remained the only main leader of the United Front in Afghanistan and the only leader who was able to defend vast parts of his area against the Taliban. Most major leaders including the Islamic State's President Burhanuddin Rabbani, Abdul Rashid Dostum, and others, were living in exile. During this time, commentators remarked that "The only thing standing in the way of future Taliban massacres is Ahmad Shah Massoud."

Massoud stated that the Taliban repeatedly offered him a position of power to make him stop his resistance. He declined, declaring the differences between their ideology and his own pro-democratic outlook on society to be insurmountable.

Massoud wanted to convince the Taliban to join a political process leading towards democratic elections in a foreseeable future. He also predicted that without assistance from Pakistan and external extremist groups, the Taliban would lose their hold on power.

In early 2001, the United Front employed a new strategy of local military pressure and global political appeals. Resentment was increasingly gathering against Taliban rule from the bottom of Afghan society including the Pashtun areas. At the same time, Massoud was very wary not to revive the failed Kabul government of the early 1990s. Already in 1999 the United Front leadership ordered the training of police forces specifically to keep order and protect the civilian population in case the United Front would be successful.

From 1999 onwards, a renewed process was set into motion by the Tajik Ahmad Shah Massoud and the Pashtun Abdul Haq to unite all the ethnicities of Afghanistan. Massoud united the Tajiks, Hazara and Uzbeks as well as several Pashtun commanders under his United Front. Besides meeting with Pashtun tribal leaders and acting as a point of reference, Abdul Haq received increasing numbers of Pashtun Taliban themselves who were secretly approaching him. Some commanders who had worked for the Taliban military apparatus agreed to the plan to topple the Taliban regime as the Taliban lost support even among the Pashtuns. Senior diplomat and Afghanistan expert Peter Tomsen wrote that ""[t]he 'Lion of Kabul' [Abdul Haq] and the 'Lion of Panjshir' [Ahmad Shah Massoud] would make a formidable anti-Taliban team if they combined forces. Haq, Massoud, and Karzai, Afghanistan's three leading moderates, could transcend the Pashtun – non-Pashtun, north-south divide."" Steve Coll referred to this plan as a "grand Pashtun-Tajik alliance". The senior Hazara and Uzbek leaders took part in the process just like later Afghan president Hamid Karzai. They agreed to work under the banner of the exiled Afghan king Zahir Shah in Rome.

In November 2000, leaders from all ethnic groups were brought together in Massoud's headquarters in northern Afghanistan, travelling from other parts of Afghanistan, Europe, the United States, Pakistan and India to discuss a Loya Jirga for a settlement of Afghanistan's problems and to discuss the establishment of a post-Taliban government. In September 2001, an international official who met with representatives of the alliance remarked, ""It's crazy that you have this today ... Pashtuns, Tajiks, Uzbeks, Hazara ... They were all ready to buy in to the process"."

In early 2001, Ahmad Shah Massoud with leaders from all ethnicities of Afghanistan addressed the European Parliament in Brussels, asking the international community to provide humanitarian aid to the people of Afghanistan. He stated that the Taliban and al-Qaeda had introduced "a very wrong perception of Islam" and that without the support of Pakistan and Bin Laden the Taliban would not be able to sustain their military campaign for up to a year. On that visit to Europe, he also warned the U.S. about Bin Laden.

Life in the areas under direct control of Massoud was different from the life in the areas under Taliban or Dostum's control. In contrast to the time of chaos in which all structures had collapsed in Kabul, Massoud was able to control most of the troops under his direct command well during the period starting in late 1996. Massoud always controlled the Panjshir, Takhar, parts of Parwan and Badakhshan during the war. Some other provinces (notably Kunduz, Baghlan, Nuristan and the north of Kabul) were captured by his forces from the Taliban and lost again from time to time as the frontlines varied.

Massoud created democratic institutions which were structured into several committees: political, health, education and economic. Still, many people came to him personally when they had a dispute or problem and asked him to solve their problems.

In September 2000, Massoud signed the Declaration of the Essential Rights of Afghan Women drafted by Afghan women. The declaration established gender equality in front of the law and the right of women to political participation, education, work, freedom of movement and speech. In the areas of Massoud, women and girls did not have to wear the Afghan burqa by law. They were allowed to work and to go to school. Although it was a time of war, girls' schools were operating in some districts. In at least two known instances, Massoud personally intervened against cases of forced marriage in favour of the women to make their own choice.

While it was Massoud's stated personal conviction that men and women are equal and should enjoy the same rights, he also had to deal with Afghan traditions which he said would need a generation or more to overcome. In his opinion, that could only be achieved through education. Author Pepe Escobar wrote in "Asia Times":
Humayun Tandar, who took part as an Afghan diplomat in the 2001 International Conference on Afghanistan in Bonn, said that "strictures of language, ethnicity, region were [also] stifling for Massoud. That is why ... he wanted to create a unity which could surpass the situation in which we found ourselves and still find ourselves to this day." This applied also to strictures of religion. Jean-José Puig describes how Massoud often led prayers before a meal or at times asked his fellow Muslims to lead the prayer but also did not hesitate to ask the Jewish Princeton Professor Michael Barry or his Christian friend Jean-José Puig: "Jean-José, we believe in the same God. Please, tell us the prayer before lunch or dinner in your own language."

U.S. policy regarding Massoud, the Taliban and Afghanistan remained ambiguous and differed between the various U.S. government agencies.

In 1997, U.S. State Department's Robin Raphel suggested to Massoud he should surrender to the Taliban. He soundly rejected the proposal.

At one point in the war, in 1997, two top foreign policy officials in the Clinton administration flew to northern Afghanistan in an attempt to convince Massoud not to take advantage of a strategic opportunity to make crucial gains against the Taliban.

In 1998, a U.S. Defense Intelligence Agency analyst, Julie Sirrs, visited Massoud's territories privately, having previously been denied official permission to do so by her agency. She reported that Massoud had conveyed warnings about strengthened ties between the Taliban and foreign Islamist terrorists. Returning home, she was sacked from her agency for insubordination, because at that time the U.S. administration had no trust in Massoud.

In the meantime, the only collaboration between Massoud and another U.S. intelligence service, the Central Intelligence Agency (CIA), consisted of an effort to trace Osama bin Laden following the 1998 embassy bombings. The U.S. and the European Union provided no support to Massoud for the fight against the Taliban.

A change of policy, lobbied for by CIA officers on the ground who had visited the area of Massoud, regarding support to Massoud, was underway in the course of 2001. According to Steve Coll's book "Ghost Wars" (who won the 2005 Pulitzer Prize for General Non-Fiction):

U.S. Congressman Dana Rohrabacher also recalled:

[B]etween Bush's inauguration and 9/11, I met with the new national security staff on 3 occasions, including one meeting with Condoleezza Rice to discuss Afghanistan. There were, in fact, signs noted in an overview story in The Washington Post about a month ago that some steps were being made to break away from the previous administration's Afghan policy.

CIA lawyers, working with officers in the Near East Division and Counterterrorist Center, began to draft a formal, legal presidential finding for Bush's signature authorizing a new covert action program in Afghanistan, the first in a decade that sought to influence the course of the Afghan war in favour of Massoud. This change in policy was finalized in August 2001 when it was too late.

After Pakistan had funded, directed and supported the Taliban's rise to power in Afghanistan, Massoud and the United Front received some assistance from India. The assistance provided by India was extensive, including uniforms, ordnance, mortars, small armaments, refurbished Kalashnikovs, combat and winter clothes, as well as funds. India was particularly concerned about Pakistan's Taliban strategy and the Islamic militancy in its neighborhood; it provided U.S.$70 million in aid including two Mi-17 helicopters, three additional helicopters in 2000 and US$8 million worth of high-altitude equipment in 2001. Also In the 1990s, India had run a field hospital at Farkor on the Tajik-Afghan border to treat wounded fighters from the then Northern Alliance that was battling the Taliban regime in Afghanistan.
It was at the very same hospital that the Northern Alliance leader Ahmed Shah Masood was pronounced dead after being assassinated just two days before the 9/11 terror strikes in 2001. Furthermore, the alliance supposedly also received minor aid from Tajikistan, Russia and Iran because of their opposition to the Taliban and the Pakistani control over the Taliban's Emirate. Their support, however, remained limited to the most needed things. Meanwhile, Pakistan engaged up to 28,000 Pakistani nationals and regular Pakistani army troops to fight alongside the Taliban and Al Qaeda forces against Massoud.

In April 2001, the president of the European Parliament, Nicole Fontaine (who called Massoud the "pole of liberty in Afghanistan"), invited Massoud with the support of French and Belgian politicians to address the European Parliament in Brussels, Belgium. In his speech, he asked for humanitarian aid for the people of Afghanistan. Massoud further went on to warn that his intelligence agents had gained limited knowledge about a large-scale terrorist attack on U.S. soil being imminent.

Massoud, then aged 48, was the target of an assassination plot in Khwājah Bahā ud Dīn (Khvājeh Bahāuḏḏīn), Takhar Province in northeastern Afghanistan on September 9, 2001. The attackers' names were alternately given as Dahmane Abd al-Sattar, husband of Malika El Aroud, and Bouraoui el-Ouaer; or 34-year-old Karim Touzani and 26-year-old Kacem Bakkali.

The attackers claimed to be Belgians originally from Morocco. According to "Le Monde" they transited through the municipality of Molenbeek. Their passports turned out to be stolen and their nationality was later determined to be Tunisian. Waiting for almost three weeks (during which they also interviewed Burhanuddin Rabbani and Abdul Rasul Sayyaf) for an interview opportunity, on September 8, 2001, an aide to Massoud recalls the would-be suicide attackers "were so worried" and threatened to leave if the interview did not happen in the next 24 hours (until September 10, 2001). They were finally granted an interview. During the interview, they set off a bomb composed of explosives hidden in the camera and in a battery-pack belt. Massoud died in a helicopter that was taking him to an Indian military field hospital at Farkhor in nearby Tajikistan. The explosion also killed Mohammed Asim Suhail, a United Front official, while Mohammad Fahim Dashty and Massoud Khalili were injured. One of the suicide attackers, Bouraoui, was killed by the explosion, while Dahmane Abd al-Sattar was captured and shot while trying to escape.

Despite initial denials by the United Front, news of Massoud's death was reported almost immediately, appearing on the BBC, and in European and North American newspapers on September 10, 2001. On September 16, the United Front officially announced that Massoud had died of injuries in the suicide attack. Massoud was buried in his home village of Bazarak in the Panjshir Valley. The funeral, although in a remote rural area, was attended by hundreds of thousands of people.

Massoud had survived assassination attempts over a period of 26 years, including attempts made by al-Qaeda, the Taliban, the Pakistani ISI and before them the Soviet KGB, the Afghan communist KHAD and Hekmatyar. The first attempt on Massoud's life was carried out by Hekmatyar and two Pakistani ISI agents in 1975 when Massoud was 22 years old. In early 2001, al-Qaeda would-be assassins were captured by Massoud's forces while trying to enter his territory.

The assassination of Massoud is considered to have a strong connection to the September 11 attacks in 2001 on U.S. soil, which killed nearly 3,000 people. It appeared to have been the major terrorist attack which Massoud had warned against in his speech to the European Parliament several months earlier. Al-Qaeda's motive for the assassination is believed to have been to secure the Taliban's support of Osama bin Laden after the planned attacks. By eliminating Massoud, it was expected that the remaining anti-Taliban forces in Afghanistan would collapse, allowing the Taliban to solidify their control over the country. Bin Laden thought that this, in turn, would make the Taliban indebted to him, and he believed their support would be crucial for his planned war against the United States.

In late 2001, a computer was seized that was stolen from an office used by al-Qaeda immediately after the fall of Kabul in November. This computer was mainly used by Aiman al-Zawahri and contained the letter with the interview request for Massoud. The two assassins had completed military training in training camps in Afghanistan at the end of 2000 and were selected for the suicide mission in the spring or early summer of the following year. The Afghan publicist Waheed Muzhda, who worked for the Taliban in the Foreign Ministry, confirmed the two assassins met with al-Qaeda officials in Kandahar and bin Laden and al-Zawahri saw them off when they left. Following the assassination, bin Laden had an emissary deliver Dahmane Abd al-Sattar's widow a letter with $500 in an envelope to settle a debt. An al-Qaeda magazine in Saudi Arabia later published a biography of Youssef al-Aayyiri, who headed al-Qaeda's operations in Saudi Arabia from 2002, which described al-Qaeda's involvement in Massoud's assassination. Osama bin Laden commissioned the assassination attempt to appease the Taliban because of the imminent terrorist attacks in the US, which would cause serious problems for the Taliban.

The Taliban denied any involvement in Massoud's assassination, and it is very unlikely that they were privy to the assassination plans. There were a few minor attacks by the Taliban after the attack, but no major offensive.

In April 2003, the Karzai administration created a commission to investigate the assassination of Massoud. In 2003, French investigators and the FBI were able to trace the provenance of the camera used in the assassination, which had been stolen in France some time earlier.

Massoud was the only chief Afghan leader who never left Afghanistan in the fight against the Soviet Union and later in the fight against the Taliban Emirate. In the areas under his direct control, such as Panjshir, some parts of Parwan and Takhar, Massoud established democratic institutions. One refugee who cramped his family of 27 into an old jeep to flee from the Taliban to the area of Massoud described Massoud's territory in 1997 as "the last tolerant corner of Afghanistan".<ref name="Journeyman Picture/ABC Australia"></ref>

One man holds a greater political punch than all 18 living [Afghan] presidential candidates combined. Though already dead for three years... Since his death on September 9, 2001 at the hands of two al Qaeda-linked Islamic radicals, Massoud has been transformed from mujahedin to national heroif not saint. Pictures of Massoud, the Afghan mujahedin who battled the Soviets, other warlords, and the Taliban for more than 20 years, vastly outnumber those of any other Afghan including those of Karzai.

Today Panjshir, the home of Massoud,

is arguably the most peaceful place in the entire country. A small US military reconstruction team is based here, but there are none of the signs of foreign occupation that exist elsewhere. Even Afghan soldiers are few and far between. Instead, the people like to boast about how they keep their own security.

The road near the Afghanistan Embassy is a "symbol of ties" that binds the two nations that have always "enjoyed excellent relations".

His friend Abdullah Abdullah said that Massoud was different from the other guerilla leaders. "He is a hero who led a clear struggle for the values of the people".

In a 2001 mourning ceremony at Moscow to honour the memory of Ahmad Shah Massoud, one-time foe Colonel Abdul Qadir stated: "Though Massoud and I used to be enemies, I am sure he deserves great respect as an outstanding military leader and, first of all, as a patriot of his country".

Massoud's byname, "Lion of Panjshir" (, "Shir-e-Panjshir"), earned for his role during the Soviet occupation, is a rhyming play on words in Persian, as the name of the valley means "five lions".

"The Wall Street Journal" referred to Massoud as "The Afghan Who Won the Cold War", referring to the global significance of the Soviet defeat in Afghanistan for the subsequent collapse of the Eastern Bloc.

In 2007, the government of India decided to name a road in New Delhi's Chanakyapuri district after Massoud.

In February 2021, the Council of Paris in France honored Massoud by installing a plaque in the 8th arrondissement of Paris. The decision reflected Massoud's unique connections with France. In March 2021, the Mayor of Paris named a pathway in the Champs-Élysées gardens after Massoud. The ceremony was attended by Massoud's son and former president Hamid Karzai.


Although Pakistan were supporting the mujahideen groups during the Soviet-Afghan War, Ahmad Shah Massoud increasingly distrusted the Pakistanis and eventually kept his distance from them. In a 1999 interview, Massoud says "They [Pakistan] are trying to turn us into a colony. Without them there would be no war".

In the spring 2001, Ahmad Shah Massoud addressed the European Parliament in Brussels, saying that Pakistan was behind the situation in Afghanistan. He also said that he believed that, without the support of Pakistan, Osama bin Laden, and Saudi Arabia, the Taliban would not be able to sustain their military campaign for up to a year. He said the Afghan population was ready to rise against them. Addressing the United States specifically, he warned that should the U.S. not work for peace in Afghanistan and put pressure on Pakistan to cease their support to the Taliban, the problems of Afghanistan would soon become the problems of the U.S. and the world.

Declassified Defense Intelligence Agency (DIA) documents from November 2001 show that Massoud had gained "limited knowledge... regarding the intentions of al-Qaeda to perform a terrorist act against the U.S. on a scale larger than the 1998 bombing of the U.S. embassies in Kenya and Tanzania." They noted that he warned about such attacks.

In September 2019, his son Ahmad Massoud was declared as his successor. Following the 2021 Taliban offensive and the Fall of Kabul, Massoud allied with self-proclaimed acting president Amrullah Saleh and established the National Resistance Front of Afghanistan to the Taliban in the Panjshir Valley. Massoud called for West's support to resist the Taliban.

Massoud was married to Sediqa Massoud. They had one son, Ahmad Massoud (born in 1989) and five daughters (Fatima born in 1992, Mariam born in 1993, Ayesha born in 1995, Zohra born in 1996 and Nasrine born in 1998). In 2005 Sediqa Massoud published a personal account on her life with Massoud (co-authored by two women's rights activists and friends of Sediqa Massoud, Chékéba Hachemi and ) called ""Pour l'amour de Massoud"" (For the love of Massoud), in which she describes a decent and loving husband.

Massoud liked reading and had a library of 3,000 books at his home in Panjshir. He used to read the works of revolutionaries Mao Zedong and Che Guevara, and was a great admirer of Charles de Gaulle, founder of the French Fifth Republic. Massoud said his favorite author was Victor Hugo and he was also a fan of classical Persian poetry, including the works of Bidel and Hafez. He was keen at playing football and chess.

Massoud's reputation for fearlessness is illustrated by a story about him told in Afghanistan, which cannot be confirmed. Once, while inspecting the front lines with a deputy, Massoud's driver had become lost and driven into the middle of a Taliban encampment. In tremendous peril, since he was recognized immediately, Massoud demanded to see the Taliban commander, making polite conversation for just long enough to bluff that he had arrived intentionally and not accidentally. The confused Taliban allowed him to leave.

Massoud's family since his death have had a great deal of prestige in the politics of Afghanistan. One of his six brothers, Ahmad Zia Massoud, was the Vice President of Afghanistan from 2004 until 2009 under the first democratically elected government of Afghanistan. Unsuccessful attempts have been made on the life of Ahmad Zia Massoud in 2004 and late 2009. The Associated Press reported that eight Afghans died in the attempt on Ahmad Zia Massoud's life. Ahmad Zia Massoud leads the National Front of Afghanistan (a United Front group). Another brother, Ahmad Wali Massoud, was Afghanistan's Ambassador to the United Kingdom from 2002 to 2006. He was a member of Abdullah Abdullah's National Coalition of Afghanistan (another United Front group).







Atlantis

Atlantis () is a fictional island mentioned in Plato's works "Timaeus" and "Critias" as part of an allegory on the hubris of nations. In the story, Atlantis is described as a naval empire that ruled all Western parts of the known world, making it the literary counter-image of the Achaemenid Empire. After an ill-fated attempt to conquer "Ancient Athens," Atlantis falls out of favor with the deities and submerges into the Atlantic Ocean. Since Plato describes Athens as resembling his ideal state in the "Republic", the Atlantis story is meant to bear witness to the superiority of his concept of a state.

Despite its minor importance in Plato's work, the Atlantis story has had a considerable impact on literature. The allegorical aspect of Atlantis was taken up in utopian works of several Renaissance writers, such as Francis Bacon's "New Atlantis" and Thomas More's "Utopia". On the other hand, nineteenth-century amateur scholars misinterpreted Plato's narrative as historical tradition, most famously Ignatius L. Donnelly in his "". Plato's vague indications of the time of the events (more than 9,000 years before his time) and the alleged location of Atlantis ("beyond the Pillars of Hercules") gave rise to much pseudoscientific speculation. As a consequence, Atlantis has become a byword for any and all supposed advanced prehistoric lost civilizations and continues to inspire contemporary fiction, from comic books to films.

While present-day philologists and classicists agree on the story's fictional nature, there is still debate on what served as its inspiration. Plato is known to have freely borrowed some of his allegories and metaphors from older traditions, as he did with the story of Gyges. This led a number of scholars to suggest possible inspiration of Atlantis from Egyptian records of the Thera eruption, the Sea Peoples invasion, or the Trojan War. Others have rejected this chain of tradition as implausible and insist that Plato created an entirely fictional account, drawing loose inspiration from contemporary events such as the failed Athenian invasion of Sicily in 415–413 BC or the destruction of Helike in 373 BC.

The only primary sources for Atlantis are Plato's dialogues "Timaeus" and "Critias"; all other mentions of the island are based on them. The dialogues claim to quote Solon, who visited Egypt between 590 and 580 BC; they state that he translated Egyptian records of Atlantis. Plato introduced Atlantis in "Timaeus", written in 360 BC:

The four people appearing in those two dialogues are the politicians Critias and Hermocrates as well as the philosophers Socrates and Timaeus of Locri, although only Critias speaks of Atlantis. In his works Plato makes extensive use of the Socratic method in order to discuss contrary positions within the context of a supposition.

The "Timaeus" begins with an introduction, followed by an account of the creations and structure of the universe and ancient civilizations. In the introduction, Socrates muses about the perfect society, described in Plato's "Republic" (), and wonders if he and his guests might recollect a story which exemplifies such a society. Critias mentions a tale he considered to be historical, that would make the perfect example, and he then follows by describing Atlantis as is recorded in the "Critias". In his account, ancient Athens seems to represent the "perfect society" and Atlantis its opponent, representing the very antithesis of the "perfect" traits described in the "Republic".

According to Critias, the Hellenic deities of old divided the land so that each deity might have their own lot; Poseidon was appropriately, and to his liking, bequeathed the island of Atlantis. The island was larger than Ancient Libya and Asia Minor combined, but it was later sunk by an earthquake and became an impassable mud shoal, inhibiting travel to any part of the ocean. Plato asserted that the Egyptians described Atlantis as an island consisting mostly of mountains in the northern portions and along the shore and encompassing a great plain in an oblong shape in the south "extending in one direction three thousand "stadia" [about 555 km; 345 mi], but across the center inland it was two thousand stadia [about 370 km; 230 mi]." Fifty stadia [9 km; 6 mi] from the coast was a mountain that was low on all sides ... broke it off all round about ... the central island itself was five stades in diameter [about 0.92 km; 0.57 mi].

In Plato's metaphorical tale, Poseidon fell in love with Cleito, the daughter of Evenor and Leucippe, who bore him five pairs of male twins. The eldest of these, Atlas, was made rightful king of the entire island and the ocean (called the Atlantic Ocean in his honor), and was given the mountain of his birth and the surrounding area as his fiefdom. Atlas's twin Gadeirus, or Eumelus in Greek, was given the extremity of the island toward the pillars of Hercules. The other four pairs of twins—Ampheres and Evaemon, Mneseus and Autochthon, Elasippus and Mestor, and Azaes and Diaprepes—were also given "rule over many men, and a large territory."

Poseidon carved the mountain where his love dwelt into a palace and enclosed it with three circular moats of increasing width, varying from one to three stadia and separated by rings of land proportional in size. The Atlanteans then built bridges northward from the mountain, making a route to the rest of the island. They dug a great canal to the sea, and alongside the bridges carved tunnels into the rings of rock so that ships could pass into the city around the mountain; they carved docks from the rock walls of the moats. Every passage to the city was guarded by gates and towers, and a wall surrounded each ring of the city. The walls were constructed of red, white, and black rock, quarried from the moats, and were covered with brass, tin, and the precious metal orichalcum, respectively.

According to Critias, 9,000 years before his lifetime a war took place between those outside the Pillars of Hercules at the Strait of Gibraltar and those who dwelt within them. The Atlanteans had conquered the parts of Libya within the Pillars of Hercules, as far as Egypt, and the European continent as far as Tyrrhenia, and had subjected its people to slavery. The Athenians led an alliance of resistors against the Atlantean empire, and as the alliance disintegrated, prevailed alone against the empire, liberating the occupied lands.

The logographer Hellanicus of Lesbos wrote an earlier work entitled "Atlantis", of which only a few fragments survive. Hellanicus' work appears to have been a genealogical one concerning the daughters of Atlas (Ἀτλαντὶς in Greek means "of Atlas"), but some authors have suggested a possible connection with Plato's island. John V. Luce notes that when Plato writes about the genealogy of Atlantis's kings, he writes in the same style as Hellanicus, suggesting a similarity between a fragment of Hellanicus's work and an account in the "Critias". Rodney Castleden suggests that Plato may have borrowed his title from Hellanicus, who may have based his work on an earlier work about Atlantis.

Castleden has pointed out that Plato wrote of Atlantis in 359 BC, when he returned to Athens from Sicily. He notes a number of parallels between the physical organisation and fortifications of Syracuse and Plato's description of Atlantis. Gunnar Rudberg was the first who elaborated upon the idea that Plato's attempt to realize his political ideas in the city of Syracuse could have heavily inspired the Atlantis account.

Some ancient writers viewed Atlantis as fictional or metaphorical myth; others believed it to be real. Aristotle believed that Plato, his teacher, had invented the island to teach philosophy. The philosopher Crantor, a student of Plato's student Xenocrates, is cited often as an example of a writer who thought the story to be historical fact. His work, a commentary on "Timaeus", is lost, but Proclus, a Neoplatonist of the fifth century AD, reports on it. The passage in question has been represented in the modern literature either as claiming that Crantor visited Egypt, had conversations with priests, and saw hieroglyphs confirming the story, or, as claiming that he learned about them from other visitors to Egypt. Proclus wrote:

The next sentence is often translated "Crantor adds, that this is testified by the prophets of the Egyptians, who assert that these particulars [which are narrated by Plato] are written on pillars which are still preserved." But in the original, the sentence starts not with the name Crantor but with the ambiguous "He"; whether this referred to Crantor or to Plato is the subject of considerable debate. Proponents of both Atlantis as a metaphorical myth and Atlantis as history have argued that the pronoun refers to Crantor.

Alan Cameron argues that the pronoun should be interpreted as referring to Plato, and that, when Proclus writes that "we must bear in mind concerning this whole feat of the Athenians, that it is neither a mere myth nor unadorned history, although some take it as history and others as myth", he is treating "Crantor's view as mere personal opinion, nothing more; in fact he first quotes and then dismisses it as representing one of the two unacceptable extremes".

Cameron also points out that whether "he" refers to Plato or to Crantor, the statement does not support conclusions such as Otto Muck's "Crantor came to Sais and saw there in the temple of Neith the column, completely covered with hieroglyphs, on which the history of Atlantis was recorded. Scholars translated it for him, and he testified that their account fully agreed with Plato's account of Atlantis" or J. V. Luce's suggestion that Crantor sent "a special enquiry to Egypt" and that he may simply be referring to Plato's own claims.

Another passage from the commentary by Proclus on the "Timaeus" gives a description of the geography of Atlantis: 

Marcellus remains unidentified.

Other ancient historians and philosophers who believed in the existence of Atlantis were Strabo and Posidonius. Some have theorized that, before the sixth century BC, the "Pillars of Hercules" may have applied to mountains on either side of the Gulf of Laconia, and also may have been part of the pillar cult of the Aegean. The mountains stood at either side of the southernmost gulf in Greece, the largest in the Peloponnese, and it opens onto the Mediterranean Sea. This would have placed Atlantis in the Mediterranean, lending credence to many details in Plato's discussion.

The fourth-century historian Ammianus Marcellinus, relying on a lost work by Timagenes, a historian writing in the first century BC, writes that the Druids of Gaul said that part of the inhabitants of Gaul had migrated there from distant islands. Some have understood Ammianus's testimony as a claim that at the time of Atlantis's sinking into the sea, its inhabitants fled to western Europe; but Ammianus, in fact, says that "the Drasidae (Druids) recall that a part of the population is indigenous but others also migrated in from islands and lands beyond the Rhine" ("Res Gestae" 15.9), an indication that the immigrants came to Gaul from the north (Britain, the Netherlands, or Germany), not from a theorized location in the Atlantic Ocean to the south-west. Instead, the Celts who dwelled along the ocean were reported to venerate twin gods, (Dioscori), who appeared to them coming from that ocean.

During the early first century, the Hellenistic Jewish philosopher Philo wrote about the destruction of Atlantis in his "On the Eternity of the World", xxvi. 141, in a longer passage allegedly citing Aristotle's successor Theophrastus:

The theologian Joseph Barber Lightfoot ("Apostolic Fathers", 1885, II, p. 84) noted on this passage: "Clement may possibly be referring to some known, but hardly accessible land, lying without the pillars of Hercules. But more probably he contemplated some unknown land in the far west beyond the ocean, like the fabled Atlantis of Plato ..."

Other early Christian writers wrote about Atlantis, although they had mixed views on whether it once existed or was an untrustworthy myth of pagan origin. Tertullian believed Atlantis was once real and wrote that in the Atlantic Ocean once existed "[the isle] that was equal in size to Libya or Asia" referring to Plato's geographical description of Atlantis. The early Christian apologist writer Arnobius also believed Atlantis once existed, but blamed its destruction on pagans.

Cosmas Indicopleustes in the sixth century wrote of Atlantis in his "Christian Topography" in an attempt to prove his theory that the world was flat and surrounded by water:

Aside from Plato's original account, modern interpretations regarding Atlantis are an amalgamation of diverse, speculative movements that began in the sixteenth century, when scholars began to identify Atlantis with the New World. Francisco Lopez de Gomara was the first to state that Plato was referring to America, as did Francis Bacon and Alexander von Humboldt; Janus Joannes Bircherod said in 1663 "orbe novo non-novo" ("the New World is not new"). Athanasius Kircher accepted Plato's account as literally true, describing Atlantis as a small continent in the Atlantic Ocean.

Contemporary perceptions of Atlantis share roots with Mayanism, which can be traced to the beginning of the Modern Age, when European imaginations were fueled by their initial encounters with the indigenous peoples of the Americas. From this era sprang apocalyptic and utopian visions that would inspire many subsequent generations of theorists.

Most of these interpretations are considered pseudohistory, pseudoscience, or pseudoarchaeology, as they have presented their works as academic or scientific, but lack the standards or criteria.

The Flemish cartographer and geographer Abraham Ortelius is believed to have been the first person to imagine that the continents were joined before drifting to their present positions. In the 1596 edition of his "Thesaurus Geographicus" he wrote: "Unless it be a fable, the island of Gadir or Gades will be the remaining part of the island of Atlantis or America, which was not sunk (as Plato reports in the "Timaeus") so much as torn away from Europe and Africa by earthquakes and flood... The traces of the ruptures are shown by the projections of Europe and Africa and the indentations of America in the parts of the coasts of these three said lands that face each other to anyone who, using a map of the world, carefully considered them. So that anyone may say with Strabo in Book 2, that what Plato says of the island of Atlantis on the authority of Solon is not a figment."

The term "utopia" (from "no place") was coined by Sir Thomas More in his sixteenth-century work of fiction "Utopia". Inspired by Plato's Atlantis and travelers' accounts of the Americas, More described an imaginary land set in the New World. His idealistic vision established a connection between the Americas and utopian societies, a theme that Bacon discussed in "The New Atlantis" (). A character in the narrative gives a history of Atlantis that is similar to Plato's and places Atlantis in America. People had begun believing that the Mayan and Aztec ruins could possibly be the remnants of Atlantis.

Much speculation began as to the origins of the Maya, which led to a variety of narratives and publications that tried to rationalize the discoveries within the context of the Bible and that had undertones of racism in their connections between the Old and New World. The Europeans believed the indigenous people to be inferior and incapable of building that which was now in ruins and by sharing a common history, they insinuated that another race must have been responsible.

In the middle and late nineteenth century, several renowned Mesoamerican scholars, starting with Charles Étienne Brasseur de Bourbourg, and including Edward Herbert Thompson and Augustus Le Plongeon, formally proposed that Atlantis was somehow related to Mayan and Aztec culture.

The French scholar Brasseur de Bourbourg traveled extensively through Mesoamerica in the mid-1800s, and was renowned for his translations of Mayan texts, most notably the sacred book Popol Vuh, as well as a comprehensive history of the region. Soon after these publications, however, Brasseur de Bourbourg lost his academic credibility, due to his claim that the Maya peoples had descended from the Toltecs, people he believed were the surviving population of the racially superior civilization of Atlantis. His work combined with the skillful, romantic illustrations of Jean Frederic Waldeck, which visually alluded to Egypt and other aspects of the Old World, created an authoritative fantasy that excited much interest in the connections between worlds.

Inspired by Brasseur de Bourbourg's diffusion theories, the pseudoarchaeologist Augustus Le Plongeon traveled to Mesoamerica and performed some of the first excavations of many famous Mayan ruins. Le Plongeon invented narratives, such as the kingdom of Mu saga, which romantically drew connections to him, his wife Alice, and Egyptian deities Osiris and Isis, as well as to Heinrich Schliemann, who had just discovered the ancient city of Troy from Homer's epic poetry (that had been described as merely mythical). He also believed that he had found connections between the Greek and Mayan languages, which produced a narrative of the destruction of Atlantis.

The 1882 publication of "" by Ignatius L. Donnelly stimulated much popular interest in Atlantis. He was greatly inspired by early works in Mayanism, and like them, attempted to establish that all known ancient civilizations were descended from Atlantis, which he saw as a technologically sophisticated, more advanced culture. Donnelly drew parallels between creation stories in the Old and New Worlds, attributing the connections to Atlantis, where he believed the Biblical Garden of Eden existed. As implied by the title of his book, he also believed that Atlantis was destroyed by the Great Flood mentioned in the Bible.

Donnelly is credited as the "father of the nineteenth century Atlantis revival" and is the reason the myth endures today. He unintentionally promoted an alternative method of inquiry to history and science, and the idea that myths contain hidden information that opens them to "ingenious" interpretation by people who believe they have new or special insight.

Helena Petrovna Blavatsky, the founder of the Theosophists, took up Donnelly's interpretations when she wrote "The Secret Doctrine" (1888), which she claimed was originally dictated in Atlantis. She maintained that the Atlanteans were cultural heroes (contrary to Plato, who describes them mainly as a military threat). She believed in a form of racial evolution (as opposed to primate evolution). In her process of evolution the Atlanteans were the fourth "root race", which were succeeded by the fifth, the "Aryan race", which she identified with the modern human race.

In her book, Blavatsky reported that the civilization of Atlantis reached its peak between 1,000,000 and 900,000 years ago, but destroyed itself through internal warfare brought about by the dangerous use of psychic and supernatural powers of the inhabitants. Rudolf Steiner, the founder of anthroposophy and Waldorf Schools, along with other well known Theosophists, such as Annie Besant, also wrote of cultural evolution in much the same vein. Other occultists followed the same lead, at least to the point of tracing the lineage of occult practices back to Atlantis. Among the most famous is Dion Fortune in her "Esoteric Orders and Their Work".

Drawing on the ideas of Rudolf Steiner and Hanns Hörbiger, Egon Friedell started his book "", and thus his historical analysis of antiquity, with the ancient culture of Atlantis. The book was published in 1940.

Blavatsky was also inspired by the work of the 18th-century astronomer Jean-Sylvain Bailly, who had "Orientalized" the Atlantis myth in his mythical continent of Hyperborea, a reference to Greek myths featuring a Northern European region of the same name, home to a giant, godlike race. Dan Edelstein claims that her reshaping of this theory in "The Secret Doctrine" provided the Nazis with a mythological precedent and a pretext for their ideological platform and their subsequent genocide. However, Blavatsky's writings mention that the Atlantean were in fact olive-skinned peoples with Mongoloid traits who were the ancestors of modern Native Americans, Mongolians, and Malayans.

The idea that the Atlanteans were Hyperborean, Nordic supermen who originated in the Northern Atlantic or even in the far North, was popular in the German ariosophic movement around 1900, propagated by Guido von List and others. It gave its name to the "Thule Gesellschaft", an antisemite Münich lodge, which preceded the German Nazi Party (see Thule). The scholars (1920) and Herman Wirth (1928) were the first to speak of a "Nordic-Atlantean" or "Aryan-Nordic" master race that spread from Atlantis over the Northern Hemisphere and beyond. The Hyperboreans were contrasted with the Jewish people. Party ideologist Alfred Rosenberg (in "The Myth of the Twentieth Century", 1930) and SS-leader Heinrich Himmler made it part of the official doctrine. The idea was followed up by the adherents of Esoteric Nazism such as Julius Evola (1934) and, more recently, Miguel Serrano (1978).

The idea of Atlantis as the homeland of the Caucasian race would contradict the beliefs of older Esoteric and Theosophic groups, which taught that the Atlanteans were non-Caucasian brown-skinned peoples. Modern Esoteric groups, including the Theosophic Society, do not consider Atlantean society to have been superior or Utopian—they rather consider it a lower stage of evolution.

The clairvoyant Edgar Cayce spoke frequently of Atlantis. During his "life readings", he claimed that many of his subjects were reincarnations of people who had lived there. By tapping into their collective consciousness, the "Akashic Records" (a term borrowed from Theosophy), Cayce declared that he was able to give detailed descriptions of the lost continent. He also asserted that Atlantis would "rise" again in the 1960s (sparking much popularity of the myth in that decade) and that there is a "Hall of Records" beneath the Egyptian Sphinx which holds the historical texts of Atlantis.

As continental drift became widely accepted during the 1960s, and the increased understanding of plate tectonics demonstrated the impossibility of a lost continent in the geologically recent past, most "Lost Continent" theories of Atlantis began to wane in popularity.

Plato scholar Julia Annas, Regents Professor of Philosophy at the University of Arizona, had this to say on the matter:

One of the proposed explanations for the historical context of the Atlantis story is that it serves as Plato's warning to his fellow citizens against their striving for naval power.

Kenneth Feder points out that Critias's story in the "Timaeus" provides a major clue. In the dialogue, Critias says, referring to Socrates' hypothetical society:

Feder quotes A. E. Taylor, who wrote, "We could not be told much more plainly that the whole narrative of Solon's conversation with the priests and his intention of writing the poem about Atlantis are an invention of Plato's fancy."

Since Donnelly's day, there have been dozens of locations proposed for Atlantis, to the point where the name has become a generic concept, divorced from the specifics of Plato's account. This is reflected in the fact that many proposed sites are not within the Atlantic at all. Few today are scholarly or archaeological hypotheses, while others have been made by psychic (e.g., Edgar Cayce) or other pseudoscientific means. (The Atlantis researchers Jacques Collina-Girard and Georgeos Díaz-Montexano, for instance, each claim the other's hypothesis is pseudoscience.) Many of the proposed sites share some of the characteristics of the Atlantis story (water, catastrophic end, relevant time period), but none has been demonstrated to be a true historical Atlantis.

Most of the historically proposed locations are in or near the Mediterranean Sea: islands such as Sardinia, Crete, Santorini (Thera), Sicily, Cyprus, and Malta; land-based cities or states such as Troy, Tartessos, and Tantalis (in the province of Manisa, Turkey); Israel-Sinai or Canaan; and northwestern Africa, including the Richat Structure in Mauritania.

The Thera eruption, dated to the seventeenth or sixteenth century BC, caused a large tsunami that some experts hypothesize devastated the Minoan civilization on the nearby island of Crete, further leading some to believe that this may have been the catastrophe that inspired the story. In the area of the Black Sea the following locations have been proposed: Bosporus and Ancomah (a legendary place near Trabzon).

Others have noted that, before the sixth century BC, the mountains on either side of the Laconian Gulf were called the "Pillars of Hercules", and they could be the geographical location being described in ancient reports upon which Plato was basing his story. The mountains stood at either side of the southernmost gulf in Greece, the largest in the Peloponnese, and that gulf opens onto the Mediterranean Sea. 

If from the beginning of discussions, misinterpretation of Gibraltar as the location rather than being at the Gulf of Laconia, would lend itself to many erroneous concepts regarding the location of Atlantis. Plato may have not been aware of the difference. The Laconian pillars open to the south toward Crete and beyond which is Egypt. The Thera eruption and the Late Bronze Age collapse affected that area and might have been the devastation to which the sources used by Plato referred. Significant events such as these would have been likely material for tales passed from one generation to another for almost a thousand years.

The location of Atlantis in the Atlantic Ocean has a certain appeal given the closely related names. Popular culture often places Atlantis there, perpetuating the original Platonic setting as they understand it. The Canary Islands and Madeira Islands have been identified as a possible location, west of the Straits of Gibraltar, but in relative proximity to the Mediterranean Sea. Detailed studies of their geomorphology and geology have demonstrated, however, that they have been steadily uplifted, without any significant periods of subsidence, over the last four million years, by geologic processes such as erosional unloading, gravitational unloading, lithospheric flexure induced by adjacent islands, and volcanic underplating.

Various islands or island groups in the Atlantic were also identified as possible locations, notably the Azores. Similarly, cores of sediment covering the ocean bottom surrounding the Azores and other evidence demonstrate that it has been an undersea plateau for millions of years. The area is known for its volcanism however, which is associated with rifting along the Azores Triple Junction. The spread of the crust along the existing faults and fractures has produced many volcanic and seismic events. 

The area is supported by a buoyant upwelling in the deeper mantle, which some associate with an Azores hotspot. Most of the volcanic activity has occurred primarily along the Terceira Rift. From the beginning of the islands' settlement, around the 15th century, there have been about 30 volcanic eruptions (terrestrial and submarine) as well as numerous, powerful earthquakes. The island of São Miguel in the Azores is the site of the Sete Cidades volcano and caldera, which are the byproducts of historical volcanic activity in the Azores.

The submerged island of Spartel near the Strait of Gibraltar has also been suggested.

Several hypotheses place the sunken island in northern Europe, including Doggerland in the North Sea, and Sweden (by Olof Rudbeck in "Atland", 1672–1702). Doggerland, as well as Viking Bergen Island, is thought to have been flooded by a megatsunami following the Storegga Slide of c. 6100 BC. Some have proposed the Celtic Shelf as a possible location, and that there is a link to Ireland. In 2004, Swedish physiographist Ulf Erlingsson proposed that the legend of Atlantis was based on Stone Age Ireland. He later stated that he does not believe that Atlantis ever existed but maintained that his hypothesis that its description matches Ireland's geography has a 99.8% probability. The director of the National Museum of Ireland commented that there was no archaeology supporting this.

In 2011, a team, working on a documentary for the National Geographic Channel, led by Professor Richard Freund from the University of Hartford, claimed to have found possible evidence of Atlantis in southwestern Andalusia. The team identified its possible location within the marshlands of the Doñana National Park, in the area that once was the Lacus Ligustinus, between the Huelva, Cádiz, and Seville provinces, and they speculated that Atlantis had been destroyed by a tsunami, extrapolating results from a previous study by Spanish researchers, published four years earlier.

Spanish scientists have dismissed Freund's speculations, claiming that he sensationalised their work. The anthropologist Juan Villarías-Robles, who works with the Spanish National Research Council, said, "Richard Freund was a newcomer to our project and appeared to be involved in his own very controversial issue concerning King Solomon's search for ivory and gold in Tartessos, the well documented settlement in the Doñana area established in the first millennium BC", and described Freund's claims as "fanciful".

A similar theory had previously been put forward by a German researcher, Rainer W. Kühne, that is based only on satellite imagery and places Atlantis in the Marismas de Hinojos, north of the city of Cádiz. Before that, the historian Adolf Schulten had stated in the 1920s that Plato had used Tartessos as the basis for his Atlantis myth.

Several writers, such as Flavio Barbiero as early as 1974, have speculated that Antarctica is the site of Atlantis. A number of claims involve the Caribbean, such as an alleged underwater formation off the Guanahacabibes Peninsula in Cuba. The adjacent Bahamas or the folkloric Bermuda Triangle have been proposed as well. Areas in the Pacific and Indian Oceans have also been proposed, including Indonesia (i.e. Sundaland). The stories of a lost continent off the coast of India, named "Kumari Kandam", have inspired some to draw parallels to Atlantis.

In order to give his account of Atlantis verisimilitude, Plato mentions that the story was heard by Solon in Egypt, and transmitted orally over several generations through the family of Dropides, until it reached Critias, a dialogue speaker in "Timaeus" and "Critias". Solon had supposedly tried to adapt the Atlantis oral tradition into a poem (that if published, was to be greater than the works of Hesiod and Homer). While it was never completed, Solon passed on the story to Dropides. Modern classicists deny the existence of Solon's Atlantis poem and the story as an oral tradition. 

Instead, Plato is thought to be the sole inventor or fabricator.
Hellanicus of Lesbos used the word "Atlantis" as the title for a poem published before Plato, a fragment of which may be Oxyrhynchus Papyrus 11, 1359. This work only describes the Atlantides, the daughters of Atlas, and has no relation to Plato's Atlantis account.

In the new era, the third century AD Neoplatonist Zoticus wrote an epic poem based on Plato's account of Atlantis. Plato's work may already have inspired parodic imitation, however. Writing only a few decades after the "Timaeus" and "Critias", the historian Theopompus of Chios wrote of a land beyond the ocean known as Meropis. This description was included in Book 8 of his "Philippica", which contains a dialogue between Silenus and King Midas. Silenus describes the Meropids, a race of men who grow to twice normal size, and inhabit two cities on the island of Meropis: "Eusebes" (, "Pious-town") and "Machimos" (, "Fighting-town").

He also reports that an army of ten million soldiers crossed the ocean to conquer Hyperborea, but abandoned this proposal when they realized that the Hyperboreans were the luckiest people on earth. Heinz-Günther Nesselrath has argued that these and other details of Silenus' story are meant as imitation and exaggeration of the Atlantis story, by parody, for the purpose of exposing Plato's ideas to ridicule.

The creation of Utopian and dystopian fictions was renewed after the Renaissance, most notably in Francis Bacon's "New Atlantis" (1627), the description of an ideal society that he located off the western coast of America. Thomas Heyrick (1649–1694) followed him with "The New Atlantis" (1687), a satirical poem in three parts. His new continent of uncertain location, perhaps even a floating island either in the sea or the sky, serves as background for his exposure of what he described in a second edition as "A True Character of Popery and Jesuitism".

The title of "The New Atalantis" by Delarivier Manley (1709), distinguished from the two others by the single letter, is an equally dystopian work but set this time on a fictional Mediterranean island. In it sexual violence and exploitation is made a metaphor for the hypocritical behaviour of politicians in their dealings with the general public. In Manley's case, the target of satire was the Whig Party, while in David Maclean Parry's "The Scarlet Empire" (1906) it is Socialism as practised in foundered Atlantis. It was followed in Russia by Velimir Khlebnikov's poem "The Fall of Atlantis" ("Gibel' Atlantidy", 1912), which is set in a future rationalist dystopia that has discovered the secret of immortality and is so dedicated to progress that it has lost touch with the past. When the high priest of this ideology is tempted by a slave girl into an act of irrationality, he murders her and precipitates a second flood, above which her severed head floats vengefully among the stars.

A slightly later work, "The Ancient of Atlantis" (Boston, 1915) by Albert Armstrong Manship, expounds the Atlantean wisdom that is to redeem the earth. Its three parts consist of a verse narrative of the life and training of an Atlantean wise one, followed by his Utopian moral teachings and then a psychic drama set in modern times in which a reincarnated child embodying the lost wisdom is reborn on earth.

In Hispanic eyes, Atlantis had a more intimate interpretation. The land had been a colonial power which, although it had brought civilization to ancient Europe, had also enslaved its peoples. Its tyrannical fall from grace had contributed to the fate that had overtaken it, but now its disappearance had unbalanced the world. This was the point of view of Jacint Verdaguer's vast mythological epic "L'Atlantida" (1877). After the sinking of the former continent, Hercules travels east across the Atlantic to found the city of Barcelona and then departs westward again to the Hesperides. The story is told by a hermit to a shipwrecked mariner, who is inspired to follow in his tracks and so "call the New World into existence to redress the balance of the Old". This mariner, of course, was Christopher Columbus.

Verdaguer's poem was written in Catalan, but was widely translated in both Europe and Hispano-America. One response was the similarly entitled Argentinian "Atlantida" of Olegario Víctor Andrade (1881), which sees in "Enchanted Atlantis that Plato foresaw, a golden promise to the fruitful race" of Latins. The bad example of the colonising world remains, however. José Juan Tablada characterises its threat in his "De Atlántida" (1894) through the beguiling picture of the lost world populated by the underwater creatures of Classical myth, among whom is the Siren of its final stanza with

There is a similar ambivalence in Janus Djurhuus' six-stanza "Atlantis" (1917), where a celebration of the Faroese linguistic revival grants it an ancient pedigree by linking Greek to Norse legend. In the poem a female figure rising from the sea against a background of Classical palaces is recognised as a priestess of Atlantis. The poet recalls "that the Faroes lie there in the north Atlantic Ocean/ where before lay the poet-dreamt lands," but also that in Norse belief, such a figure only appears to those about to drown.

The fact that Atlantis is a lost land has made of it a metaphor for something no longer attainable. For the American poet Edith Willis Linn Forbes, "The Lost Atlantis" stands for idealisation of the past; the present moment can only be treasured once that is realised. Ella Wheeler Wilcox finds the location of "The Lost Land" (1910) in one's carefree youthful past. Similarly, for the Irish poet Eavan Boland in "Atlantis, a lost sonnet" (2007), the idea was defined when "the old fable-makers searched hard for a word/ to convey that what is gone is gone forever".

For some male poets too, the idea of Atlantis is constructed from what cannot be obtained. Charles Bewley in his Newdigate Prize poem (1910) thinks it grows from dissatisfaction with one's condition,

in a dream of Atlantis. Similarly for the Australian Gary Catalano in a 1982 prose poem, it is "a vision that sank under the weight of its own perfection". W. H. Auden, however, suggests a way out of such frustration through the metaphor of journeying toward Atlantis in his poem of 1941. While travelling, he advises the one setting out, you will meet with many definitions of the goal in view, only realising at the end that the way has all the time led inward.

A few late-19th century verse narratives complement the genre fiction that was beginning to be written at the same period. Two of them report the disaster that overtook the continent as related by long-lived survivors. In Frederick Tennyson's "Atlantis" (1888), an ancient Greek mariner sails west and discovers an inhabited island which is all that remains of the former kingdom. He learns of its end and views the shattered remnant of its former glory, from which a few had escaped to set up the Mediterranean civilisations. In the second, "Mona, Queen of Lost Atlantis: An Idyllic Re-embodiment of Long Forgotten History" (Los Angeles CA 1925) by James Logue Dryden (1840–1925), the story is told in a series of visions. A Seer is taken to Mona's burial chamber in the ruins of Atlantis, where she revives and describes the catastrophe. There follows a survey of the lost civilisations of Hyperborea and Lemuria as well as Atlantis, accompanied by much spiritualist lore.

William Walton Hoskins (1856–1919) admits to the readers of his "Atlantis and other poems" (Cleveland OH, 1881), that he is only 24. Its melodramatic plot concerns the poisoning of the descendant of god-born kings. The usurping poisoner is poisoned in his turn, following which the continent is swallowed in the waves. Asian gods people the landscape of "The Lost Island" (Ottawa 1889) by Edward Taylor Fletcher (1816–97). An angel foresees impending catastrophe and that the people will be allowed to escape if their semi-divine rulers will sacrifice themselves. A final example, Edward N. Beecher's "The Lost Atlantis or The Great Deluge of All" (Cleveland OH, 1898) is just a doggerel vehicle for its author's opinions: that the continent was the location of the Garden of Eden; that Darwin's theory of evolution is correct, as are Donnelly's views.

Atlantis was to become a theme in Russia following the 1890s, taken up in unfinished poems by Valery Bryusov and Konstantin Balmont, as well as in a drama by the schoolgirl Larissa Reisner. One other long narrative poem was published in New York by George V. Golokhvastoff. His 250-page "The Fall of Atlantis" (1938) records how a high priest, distressed by the prevailing degeneracy of the ruling classes, seeks to create an androgynous being from royal twins as a means to overcome this polarity. When he is unable to control the forces unleashed by his occult ceremony, the continent is destroyed.

The Spanish composer Manuel de Falla worked on a dramatic cantata based on Verdaguer's "L'Atlántida", during the last 20 years of his life. The name has been affixed to symphonies by Jānis Ivanovs (Symphony 4, 1941), Richard Nanes, and Vaclav Buzek (2009). There was also the symphonic celebration of Alan Hovhaness: "Fanfare for the New Atlantis" (Op. 281, 1975).

The Bohemian-American composer and arranger Vincent Frank Safranek wrote "Atlantis (The Lost Continent) Suite in Four Parts"; I. Nocturne and Morning Hymn of Praise, II. A Court Function, III. "I Love Thee" (The Prince and Aana), IV. The Destruction of Atlantis, for military (concert) band in 1913.

The opera "Der Kaiser von Atlantis" ("The Emperor of Atlantis") was written in 1943 by Viktor Ullmann with a libretto by Petr Kien, while they were both inmates at the Nazi concentration camp of Theresienstadt. The Nazis did not allow it to be performed, assuming the opera's reference to an Emperor of Atlantis to be a satire on Hitler. Though Ullmann and Kiel were murdered in Auschwitz, the manuscript survived and was performed for the first time in 1975 in Amsterdam.

Paintings of the submersion of Atlantis are comparatively rare. In the seventeenth century there was François de Nomé's "The Fall of Atlantis", which shows a tidal wave surging toward a Baroque city frontage. The style of architecture apart, it is not very different from Nicholas Roerich's "The Last of Atlantis" of 1928.

The most dramatic depiction of the catastrophe was Léon Bakst's "Ancient Terror" ("Terror Antiquus", 1908), although it does not name Atlantis directly. It is a mountain-top view of a rocky bay breached by the sea, which is washing inland about the tall structures of an ancient city. A streak of lightning crosses the upper half of the painting, while below it rises the impassive figure of an enigmatic goddess who holds a blue dove between her breasts. Vyacheslav Ivanov identified the subject as Atlantis in a public lecture on the painting given in 1909, the year it was first exhibited, and he has been followed by other commentators in the years since.

Sculptures referencing Atlantis have often been stylized single figures. One of the earliest was Einar Jónsson's "The King of Atlantis" (1919–1922), now in the garden of his museum in Reykjavík. It represents a single figure, clad in a belted skirt and wearing a large triangular helmet, who sits on an ornate throne supported between two young bulls. The walking female entitled "Atlantis" (1946) by Ivan Meštrović was from a series inspired by ancient Greek figures with the symbolical meaning of unjustified suffering.

In the case of the Brussels fountain feature known as "The Man of Atlantis" (2003) by the Belgian sculptor , the 4-metre tall figure wearing a diving suit steps from a plinth into the spray. It looks light-hearted but the artist's comment on it makes a serious point: "Because habitable land will be scarce, it is no longer improbable that we will return to the water in the long term. As a result, a portion of the population will mutate into fish-like creatures. Global warming and rising water levels are practical problems for the world in general and here in the Netherlands in particular".
Robert Smithson's "Hypothetical Continent – Map of Broken Clear Glass: Atlantis" was first created as a photographical project in Loveladies, New Jersey, in 1969, and then recreated as a gallery installation of broken glass. On this he commented that he liked "landscapes that suggest prehistory", and this is borne out by the original conceptual drawing of the work that includes an inset map of the continent sited off the coast of Africa and at the straits into the Mediterranean.

Mythology:

Underwater geography:

Other:


Ancient sources

Modern sources

Autobiography

An autobiography, sometimes informally called an autobio, is a self-written biography of one's own life.

The word "autobiography" was first used deprecatingly by William Taylor in 1797 in the English periodical "The Monthly Review", when he suggested the word as a hybrid, but condemned it as "pedantic". However, its next recorded use was in its present sense, by Robert Southey in 1809. Despite only being named early in the nineteenth century, first-person autobiographical writing originates in antiquity. Roy Pascal differentiates autobiography from the periodic self-reflective mode of journal or diary writing by noting that "[autobiography] is a review of a life from a particular moment in time, while the diary, however reflective it may be, moves through a series of moments in time". Autobiography thus takes stock of the autobiographer's life from the moment of composition. While biographers generally rely on a wide variety of documents and viewpoints, autobiography may be based entirely on the writer's memory. The memoir form is closely associated with autobiography but it tends, as Pascal claims, to focus less on the self and more on others during the autobiographer's review of their own life.

Autobiographical works are by nature subjective. The inability—or unwillingness—of the author to accurately recall memories has in certain cases resulted in misleading or incorrect information. Some sociologists and psychologists have noted that autobiography offers the author the ability to recreate history.

Spiritual autobiography is an account of an author's struggle or journey towards God, followed by conversion a religious conversion, often interrupted by moments of regression. The author re-frames their life as a demonstration of divine intention through encounters with the Divine. The earliest example of a spiritual autobiography is Augustine's "Confessions" though the tradition has expanded to include other religious traditions in works such as Mohandas Gandhi's "An Autobiography" and "Black Elk Speaks". "Deliverance from Error" by Al-Ghazali is another example. The spiritual autobiography often serves as an endorsement of the writer's religion.

A memoir is slightly different in character from an autobiography. While an autobiography typically focuses on the "life and times" of the writer, a memoir has a narrower, more intimate focus on the author's memories, feelings and emotions. Memoirs have often been written by politicians or military leaders as a way to record and publish an account of their public exploits. One early example is that of Julius Caesar's "Commentarii de Bello Gallico", also known as "Commentaries on the Gallic Wars". In the work, Caesar describes the battles that took place during the nine years that he spent fighting local armies in the Gallic Wars. His second memoir, "Commentarii de Bello Civili" (or "Commentaries on the Civil War") is an account of the events that took place between 49 and 48 BC in the civil war against Gnaeus Pompeius and the Senate.

Leonor López de Córdoba (1362–1420) wrote what is supposed to be the first autobiography in Spanish. The English Civil War (1642–1651) provoked a number of examples of this genre, including works by Sir Edmund Ludlow and Sir John Reresby. French examples from the same period include the memoirs of Cardinal de Retz (1614–1679) and the Duc de Saint-Simon.

The term "fictional autobiography" signifies novels about a fictional character written as though the character were writing their own autobiography, meaning that the character is the first-person narrator and that the novel addresses both internal and external experiences of the character. Daniel Defoe's "Moll Flanders" is an early example. Charles Dickens' "David Copperfield" is another such classic, and J.D. Salinger's "The Catcher in the Rye" is a well-known modern example of fictional autobiography. Charlotte Brontë's "Jane Eyre" is yet another example of fictional autobiography, as noted on the front page of the original version. The term may also apply to works of fiction purporting to be autobiographies of real characters, e.g., Robert Nye's "Memoirs of Lord Byron".

In antiquity such works were typically entitled "apologia", purporting to be self-justification rather than self-documentation. The title of John Henry Newman's 1864 Christian confessional work "Apologia Pro Vita Sua" refers to this tradition.

The historian Flavius Josephus introduces his autobiography "Josephi Vita" () with self-praise, which is followed by a justification of his actions as a Jewish rebel commander of Galilee.

The rhetor Libanius (–394) framed his life memoir "Oration I" (begun in 374) as one of his orations, not of a public kind, but of a literary kind that would not be read aloud in privacy.

Augustine of Hippo (354–430) applied the title "Confessions" to his autobiographical work, and Jean-Jacques Rousseau used the same title in the 18th century, initiating the chain of confessional and sometimes racy and highly self-critical autobiographies of the Romantic era and beyond. Augustine's was arguably the first Western autobiography ever written, and became an influential model for Christian writers throughout the Middle Ages. It tells of the hedonistic lifestyle Augustine lived for a time within his youth, associating with young men who boasted of their sexual exploits; his following and leaving of the anti-sex and anti-marriage Manichaeism in attempts to seek sexual morality; and his subsequent return to Christianity due to his embracement of Skepticism and the New Academy movement (developing the view that sex is good, and that virginity is better, comparing the former to silver and the latter to gold; Augustine's views subsequently strongly influenced Western theology). "Confessions" is considered one of the great masterpieces of western literature.

Peter Abelard's 12th-century "Historia Calamitatum" is in the spirit of Augustine's "Confessions", an outstanding autobiographical document of its period.

In the 15th century, Leonor López de Córdoba, a Spanish noblewoman, wrote her "Memorias", which may be the first autobiography in Castillian.

Zāhir ud-Dīn Mohammad Bābur, who founded the Mughal dynasty of South Asia kept a journal "Bāburnāma" (Chagatai/; literally: ""Book of Babur"" or ""Letters of Babur"") which was written between 1493 and 1529.

One of the first great autobiographies of the Renaissance is that of the sculptor and goldsmith Benvenuto Cellini (1500–1571), written between 1556 and 1558, and entitled by him simply "Vita" (Italian: "Life"). He declares at the start: "No matter what sort he is, everyone who has to his credit what are or really seem great achievements, if he cares for truth and goodness, ought to write the story of his own life in his own hand; but no one should venture on such a splendid undertaking before he is over forty." These criteria for autobiography generally persisted until recent times, and most serious autobiographies of the next three hundred years conformed to them.

Another autobiography of the period is "De vita propria", by the Italian mathematician, physician and astrologer Gerolamo Cardano (1574).

One of the first autobiographies written in an Indian language was "Ardhakathānaka", written by Banarasidas, who was a Shrimal Jain businessman and poet of Mughal India. The poetic autobiography "Ardhakathānaka" (The Half Story), was composed in Braj Bhasa, an early dialect of Hindi linked with the region around Mathura.In his autobiography, he describes his transition from an unruly youth, to a religious realization by the time the work was composed. The work also is notable for many details of life in Mughal times. 

The earliest known autobiography written in English is the "Book of Margery Kempe", written in 1438. Following in the earlier tradition of a life story told as an act of Christian witness, the book describes Margery Kempe's pilgrimages to the Holy Land and Rome, her attempts to negotiate a celibate marriage with her husband, and most of all her religious experiences as a Christian mystic. Extracts from the book were published in the early sixteenth century but the whole text was published for the first time only in 1936.

Possibly the first publicly available autobiography written in English was Captain John Smith's autobiography published in 1630 which was regarded by many as not much more than a collection of tall tales told by someone of doubtful veracity. This changed with the publication of Philip Barbour's definitive biography in 1964 which, amongst other things, established independent factual bases for many of Smith's "tall tales", many of which could not have been known by Smith at the time of writing unless he was actually present at the events recounted.

Other notable English autobiographies of the 17th century include those of Lord Herbert of Cherbury (1643, published 1764) and John Bunyan ("Grace Abounding to the Chief of Sinners", 1666).

Jarena Lee (1783–1864) was the first African American woman to have a published biography in the United States.

Following the trend of Romanticism, which greatly emphasized the role and the nature of the individual, and in the footsteps of Jean-Jacques Rousseau's "Confessions", a more intimate form of autobiography, exploring the subject's emotions, came into fashion. Stendhal's autobiographical writings of the 1830s, "The Life of Henry Brulard" and "Memoirs of an Egotist", are both avowedly influenced by Rousseau. An English example is William Hazlitt's "Liber Amoris" (1823), a painful examination of the writer's love-life.

With the rise of education, cheap newspapers and cheap printing, modern concepts of fame and celebrity began to develop, and the beneficiaries of this were not slow to cash in on this by producing autobiographies. It became the expectation—rather than the exception—that those in the public eye should write about themselves—not only writers such as Charles Dickens (who also incorporated autobiographical elements in his novels) and Anthony Trollope, but also politicians (e.g. Henry Brooks Adams), philosophers (e.g. John Stuart Mill), churchmen such as Cardinal Newman, and entertainers such as P. T. Barnum. Increasingly, in accordance with romantic taste, these accounts also began to deal, amongst other topics, with aspects of childhood and upbringing—far removed from the principles of "Cellinian" autobiography.
From the 17th century onwards, "scandalous memoirs" by supposed libertines, serving a public taste for titillation, have been frequently published. Typically pseudonymous, they were (and are) largely works of fiction written by ghostwriters. So-called "autobiographies" of modern professional athletes and media celebrities—and to a lesser extent about politicians—generally written by a ghostwriter, are routinely published. Some celebrities, such as Naomi Campbell, admit to not having read their "autobiographies". Some sensationalist autobiographies such as James Frey's "A Million Little Pieces" have been publicly exposed as having embellished or fictionalized significant details of the authors' lives.

Autobiography has become an increasingly popular and widely accessible form. "A Fortunate Life" by Albert Facey (1979) has become an Australian literary classic. With the critical and commercial success in the United States of such memoirs as "Angela’s Ashes" and "The Color of Water", more and more people have been encouraged to try their hand at this genre. Maggie Nelson's book "The Argonauts" is one of the recent autobiographies. Maggie Nelson calls it autotheory—a combination of autobiography and critical theory.

A genre where the "claim for truth" overlaps with fictional elements though the work still purports to be autobiographical is autofiction.



Arcadius

Arcadius ( ; 377 – 1 May 408) was Roman emperor from 383 to his death in 408. He was the eldest son of the "Augustus" Theodosius I () and his first wife Aelia Flaccilla, and the brother of Honorius (). Arcadius ruled the eastern half of the empire from 395, when their father died, while Honorius ruled the west. A weak ruler, his reign was dominated by a series of powerful ministers and by his wife, Aelia Eudoxia.

Arcadius was born in 377 in Hispania, the eldest son of Theodosius I and Aelia Flaccilla, and brother of Honorius. On 16 January 383, his father declared the five-year-old Arcadius an Augustus and co-ruler for the eastern half of the Empire. Ten years later a corresponding declaration made Honorius Augustus of the western half. Arcadius passed his early years under the tutelage of the rhetorician Themistius and Arsenius Zonaras, a monk.

Both of Theodosius' sons were young and inexperienced, susceptible to being dominated by ambitious subordinates. In 394 Arcadius briefly exercised independent power with the help of his advisors in Constantinople, when his father Theodosius went west to fight Arbogastes and Eugenius. Theodosius died on 17 January 395, and Arcadius, still aged only 17, fell under the influence of the praetorian prefect of the East, Rufinus. Honorius, aged 10, was consigned to the guardianship of the "magister militum" Stilicho. Rufinus ambitiously sought to marry his daughter to Arcadius and thereby gain the prestige of being the emperor's father-in-law. However, when the prefect was called away to business in Antioch (where according to Zosimus, Rufinus had Lucianus, the "comes orientis", flogged to death with whips loaded with lead), Arcadius was shown a painting of Aelia Eudoxia, the daughter of the deceased Frankish "magister militum per orientem", Bauto. Seeing the young emperor's interest in Eudoxia, Eutropius, the eunuch "praepositus sacri cubiculi", arranged for the two to meet. Arcadius fell in love and a marriage was quickly arranged, with the ceremony performed on 27 April 395. According to Zosimus, Rufinus assumed that his daughter was still to be the bride, only discovering otherwise when the nuptial procession went to Eudoxia's residence rather than his own. The rise of Eudoxia, facilitated by a general who was a rival of Rufinus, demonstrates the shifting of the centres of power in the eastern court. Such jostling for influence over the malleable emperor would be a recurring feature of Arcadius's reign.
The first crisis facing the young Arcadius was the Gothic revolt in 395, under the command of Alaric I (), who sought to take advantage of the accession of two inexperienced Roman emperors. As Alaric marched towards Constantinople, plundering Macedonia and Thrace, the eastern court could offer no response, as the majority of its army had gone to Italy with Theodosius and was now in the hands of Stilicho. Perhaps sensing an opportunity to exercise power in the eastern half of the empire as well, Stilicho declared that Theodosius had made him guardian over both his sons. He traveled eastward, ostensibly to face Alaric, leading both his own forces and the Gothic mercenaries whom Theodosius had taken west in the civil war with Eugenius. Arcadius and Rufinus felt more threatened by Stilicho than by Alaric; upon landing in Thessaly Stilicho received an imperial order to send along the eastern regiments, but himself to proceed no further. Stilicho complied, falling back to Salona while Gainas led the mercenaries to Constantinople. Arcadius and his entourage received Gainas in the Campus Martius, a parade ground adjacent to the city, on 27 November 395. There Rufinus was suddenly assassinated by the Goths, on the orders of Stilicho and possibly with the support of Eutropius. The murder certainly created an opportunity for Eutropius and for Arcadius' wife, Eudoxia, who took Rufinus' place as advisors and guardians of the emperor.

While Eutropius consolidated his hold on power in the capital, the distracted government still failed to react to the presence of Alaric in Greece. At first Eutropius may have coordinated with Stilicho around the defence of Illyricum; by 397, when Stilicho personally led a blockade that compelled Alaric to retreat into Epirus, the atmosphere of the eastern court had changed. As neither Arcadius nor Eutropius was keen to have Stilicho intervening in the affairs of the eastern empire, they provided no further military aid to Stilicho, who then abandoned the blockade of the Visigoths. At Eutropius's urging, Arcadius declared Stilicho to be a "hostis publicus", and came to an arrangement with Alaric, making him "magister militum per Illyricum". At around the same time, the eastern court persuaded Gildo, the "magister utriusque militiae per Africam", to transfer his allegiance from Honorius to Arcadius, causing relations between the two imperial courts to deteriorate further.

Eutropius' influence lasted four years, during which time he sought to marginalise the military and promote the civilian offices within the bureaucracy. He brought to trial two prominent military officers, Timasius and Abundantius. He also had Arcadius introduce two administrative innovations: the running of the "cursus publicus" (office of postmaster general) and the office in charge of manufacturing military equipment was transferred from the praetorian prefects to the "magister officiorum" (master of offices). Secondly, the role that Eutropius held, the "praepositus sacri cubiculi" (grand chamberlain) was given the rank of "illustris", and therefore equal in rank to the praetorian prefects. In the autumn of 397 he issued a law in Arcadius's name, targeting the Roman military, where any conspiracy involving soldiers or the barbarian regiments against persons holding the rank of "illustris" was considered to be treason, with the conspirators to be sentenced to death, and their descendants to be deprived of citizenship.

In 398, Eutropius led a successful campaign against the Huns in Roman Armenia. The following year he convinced Arcadius to grant him the consulship, triggering protests across the empire. For traditionalists, the granting of the consulship to a eunuch and former slave was an insult to the Roman system and other contemporary Romans, and the western court refused to recognize him as consul. The crisis escalated when the Ostrogoths who had been settled in Asia Minor by Theodosius I revolted, demanding the removal of Eutropius. 
The emperor sent two forces against Tribigild, the rebel leader; the first, under an officer named Leo, was defeated. The second force was commanded by Gainas, rival of Eutropius in the Eastern court. He returned to Arcadius and argued that the Ostrogoths could not be defeated, and that it would be sensible to accede to their demand. Arcadius viewed this proposal with displeasure, but was convinced to support it by Eudoxia, who wished to take Eutropius’ place as the main influence upon the emperor. Arcadius therefore dismissed Eutropius and sent him into exile (17 August 399), before recalling him to face trial and execution during the autumn of 399. The imperial edict issued by Arcadius detailing Eutropius's banishment survives:

The Emperors Arcadius and Honorius, Augusti, to Aurelian, Praetorian Prefect. We have added to our treasury all the property of Eutropius, who was formerly the "Praepositus sacri cubiculi", having stripped him of his splendour, and delivered the consulate from the foul stain of his tenure, and from the recollection of his name and the base filth thereof ; so that, all his acts having been repealed, all time may be dumb concerning him ; and that the blot of our age may not appear by the mention of him ; and that those who by their valour and wounds extend the Roman borders or guard the same by equity in the maintenance of law, may not groan over the fact that the divine reward of consulship has been befouled and defiled by a filthy monster. Let him learn that he has been deprived of the rank of the patriciate and all lower dignities that he stained with the perversity of his character. That all the statues, all the images —whether of bronze or marble, or painted in colours, or of any other material used in art—we command to be abolished in all cities, towns, private and public places, that they may not, as a brand of infamy on our age, pollute the gaze of beholders. Accordingly under the conduct of faithful guards let him be taken to the island of Cyprus, whither let your sublimity know that he has been banished ; so that therein guarded with most watchful diligence he may be unable to work confusion with his mad designs.

With Eutropius' fall from power, Gainas sought to take advantage of Arcadius's current predicament. He joined the rebel Ostrogoths, and, in a face to face meeting with Arcadius, forced the emperor to make him "magister militum praesentalis" and Consul designate for 401. Arcadius also acquiesced when Gainas asked for the dismissal of further officials, such as the urban prefect Aurelianus, as well as a place for settlement for his troops in Thrace. However, Arcadius refused to agree to Gainas's demand for an Arian church in Constantinople for his Gothic mercenaries, following the advice of John Chrysostom, the Archbishop of Constantinople.

By July 400, the actions of Gainas had irritated a significant portion of the population of Constantinople to the point that a general riot broke out in the capital. Although Gainas had stationed his troops outside of the capital walls, he was either unable or unwilling to bring them into the capital when many Goths in the city were hunted down and attacked. As many as 7,000 Goths were killed in the rioting; those who took refuge in a church were stoned and burned to death, after they received the emperor's permission, nor was it condemned by the Archbishop of Constantinople, John Chrysostom.

Although initially staying his hand (probably through the intervention of the new Praetorian Prefect of the East Caesarius), Gainas eventually withdrew with his Gothic mercenaries into Thrace and rebelled against Arcadius. He attempted to take his forces across the Hellespont into Asia, but was intercepted and defeated by Fravitta, another Goth who held the position of "magister militum praesentalis". Following his defeat, Gainas fled to the Danube with his remaining followers, but was ultimately defeated and killed by Uldin the Hun in Thrace.

With the fall of Gainas, the next conflict emerged between Eudoxia and John Chrysostom. The Archbishop was a stern, ascetic individual, who was a vocal critic of all displays of extravagant wealth. But his ire tended to focus especially on wealthy women, and their use of clothing, jewellery and makeup as being vain and frivolous. Eudoxia assumed that Chrysostom's denunciations of extravagance in feminine dress were aimed at her. As the tensions between the two escalated, Chrysostom, who felt that Eudoxia had used her imperial connections to obtain the possessions of the wife of a condemned senator, preached a sermon in 401 in which Eudoxia was openly called Jezebel, the infamous wife of the Israelite king Ahab. Eudoxia retaliated by supporting Bishop Severian of Gabala in his conflict with Chrysostom. As Chrysostom was very popular in the capital, riots erupted in favour of the Archbishop, forcing Arcadius and Eudoxia to publicly back down and beg Chrysostom to revoke Severian's excommunication.

Then in 403, Eudoxia saw another chance to strike against the Archbishop, when she threw her support behind Theophilus of Alexandria who presided over a synod in 403 (the Synod of the Oak) to charge Chrysostom with heresy. Although Arcadius originally supported Chrysostom, the Archbishop's decision not to participate caused Arcadius to change his mind and support Theophilus, resulting in Chrysostom's deposition and banishment. He was called back by Arcadius almost immediately, as the people started rioting over his departure, even threatening to burn the imperial palace. There was an earthquake the night of his arrest, which Eudoxia took for a sign of God's anger, prompting her to ask Arcadius for John's reinstatement.

Peace was short-lived. In September 403 a silver statue of Eudoxia was erected in the Augustaion, near the "Magna Ecclesia" church. Chrysostom, who was conducting a mass at the time, denounced the noisy dedication ceremonies as pagan and spoke against the Empress in harsh terms: "Again Herodias raves; again she is troubled; she dances again; and again desires to receive John's head in a charger", an allusion to the events surrounding the death of John the Baptist. This time Arcadius was unwilling to overlook the insult to his wife; a new synod was called in early 404 where Chrysostom was condemned. Arcadius hesitated until Easter to enforce the sentence, but Chrysostom refused to go, even after Arcadius sent in a squad of soldiers to escort him into exile. Arcadius procrastinated, but by 20 June 404, the emperor finally managed to get the Archbishop to submit, and he was taken away to his place of banishment, this time to Abkhazia in the Caucasus. Eudoxia did not get to enjoy her victory for long, dying later that year.

With the death of Eudoxia, Arcadius once again fell under the domination of a member of his court, this time the competent Anthemius, the Praetorian Prefect. He would rule in Arcadius's name for the final four years of his reign, seeking to repair the harm done by his predecessors. He attempted to heal the divisions of the past decade by trying to make peace with Stilicho in the West. Stilicho, however, had lost patience with the eastern court, and in 407 encouraged Alaric and the Visigoths to seize the Praetorian prefecture of Illyricum and hand it over to the western empire. Stilicho's plan failed, and soon after, on 1 May 408, Arcadius died. He was succeeded by his young son, Theodosius.

Like Constantine the Great and several of his successors, he was buried in the Church of the Holy Apostles, in a porphyry sarcophagus that was described in the 10th century by Constantine VII Porphyrogenitus in the "De Ceremoniis".

In noting the character of Arcadius, the historian J. B. Bury described him and his abilities thus: He was of short stature, of dark complexion, thin and inactive, and the dullness of his wit was betrayed by his speech and by his sleepy, drooping eyes. His mental deficiency and the weakness of his character made it inevitable that he should be governed by the strong personalities of his court.

Traditional interpretations of the reign of Arcadius have revolved around his weakness as an Emperor, and the formulation of policy by prominent individuals (and the court parties that formed and regrouped round them) towards curtailing the increasing influence of barbarians in the military, which in Constantinople at this period meant the Goths. Scholars such as the historian J. B. Bury spoke of a group in Arcadius's court with Germanic interests and, opposed to them, a Roman faction. So when interpreting the revolt of Gainas and the massacre of the Goths in Constantinople in 400, the episode has been traditionally interpreted by scholars such as Otto Seeck as a violent anti-barbarian reaction that functioned to stabilize the East and prevent the rise of all powerful Romanised barbarian military leaders such as Stilicho in the West - what has been termed the victory of anti-Germanism in the eastern empire.

The main source of this interpretation has been the works Synesius of Cyrene, specifically "Aegyptus sive de providentia" and "De regno". Both works have traditionally been interpreted to support the thesis that there were anti-barbarian and pro-barbarian groups, with the Praetorian Prefect Aurelianus being the leader of the anti-barbarian faction. Recent scholarly research has revised this interpretation, and has instead favoured the interaction of personal ambition and enmities among the principal participants as being the leading cause for the court intrigue throughout Arcadius's reign. The gradual decline of the use of Gothic mercenaries in the eastern empire's armies that began in the reign of Arcadius was driven by recruitment issues, as the regions beyond the Danube were made inaccessible by the Huns, forcing the empire to seek recruitment in Asia Minor. The current consensus can be summarised by the historian Thomas S. Burns: "Despite much civilian distrust and outright hatred of the army and the barbarians in it, there were no anti-barbarian or pro-barbarian parties at the court."

With respect to Arcadius himself, as emperor was more concerned with appearing to be a pious Christian than he was with political or military matters. Not being a military leader, he began to promote a new type of imperial victory through images, not via the traditional military achievements, but focusing on his piety. Arcadius's reign saw the growing push towards the outright abolishment of paganism. On 13 July 399, Arcadius issued an edict ordering that all remaining non-Christian temples should be immediately demolished.

In terms of buildings and monuments, a new forum was built in the name of Arcadius, on the seventh hill of Constantinople, the "Xērolophos", in which a column was begun to commemorate his 'victory' over Gainas (although the column was only completed after Arcadius' death by Theodosius II). The Pentelic marble portrait head of Arcadius (now in the Istanbul Archaeology Museum) was discovered in Istanbul close to the Forum Tauri, in June 1949, in excavating foundations for new buildings of the university at Beyazit. The neck was designed to be inserted in a torso, but no statue, base or inscription was found. The diadem is a fillet with rows of pearls along its edges and a rectangular stone set about with pearls over the young Emperor's forehead.

A more nuanced assessment of Arcadius's reign was provided by Warren Treadgold: By failing to reign, Arcadius had allowed a good deal of maladministration. But by continuing to reign—so harmlessly that nobody had taken the trouble to depose him—he had maintained legal continuity during a troubled time.

Arcadius had four children with Eudoxia: three daughters, Pulcheria, Arcadia and Marina, and one son, Theodosius, the future Emperor Theodosius II.



